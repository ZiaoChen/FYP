Title,Abstract,Keywords
QoS-aware middleware for Web services composition,"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.","Middleware,
Web services,
Quality of service,
Linear programming,
Web and internet services,
Financial management,
Availability,
Computer science,
Computer Society,
Online Communities/Technical Collaboration"
An introduction to biometric recognition,"A wide variety of systems requires reliable personal recognition schemes to either confirm or determine the identity of an individual requesting their services. The purpose of such schemes is to ensure that the rendered services are accessed only by a legitimate user and no one else. Examples of such applications include secure access to buildings, computer systems, laptops, cellular phones, and ATMs. In the absence of robust personal recognition schemes, these systems are vulnerable to the wiles of an impostor. Biometric recognition, or, simply, biometrics, refers to the automatic recognition of individuals based on their physiological and/or behavioral characteristics. By using biometrics, it is possible to confirm or establish an individual's identity based on ""who she is"", rather than by ""what she possesses"" (e.g., an ID card) or ""what she remembers"" (e.g., a password). We give a brief overview of the field of biometrics and summarize some of its advantages, disadvantages, strengths, limitations, and related privacy concerns.","Biometrics,
Character recognition,
Fingerprint recognition,
Robustness,
Privacy,
Humans,
Law enforcement,
Databases,
Computer science,
Pattern recognition"
Handling multiple objectives with particle swarm optimization,"This paper presents an approach in which Pareto dominance is incorporated into particle swarm optimization (PSO) in order to allow this heuristic to handle problems with several objective functions. Unlike other current proposals to extend PSO to solve multiobjective optimization problems, our algorithm uses a secondary (i.e., external) repository of particles that is later used by other particles to guide their own flight. We also incorporate a special mutation operator that enriches the exploratory capabilities of our algorithm. The proposed approach is validated using several test functions and metrics taken from the standard literature on evolutionary multiobjective optimization. Results indicate that the approach is highly competitive and that can be considered a viable alternative to solve multiobjective optimization problems.","Particle swarm optimization,
Proposals,
Computer science,
Genetic mutations,
Testing,
Scholarships,
Constraint optimization,
Evolutionary computation,
Data structures"
Probabilistic independent component analysis for functional magnetic resonance imaging,"We present an integrated approach to probabilistic independent component analysis (ICA) for functional MRI (FMRI) data that allows for nonsquare mixing in the presence of Gaussian noise. In order to avoid overfitting, we employ objective estimation of the amount of Gaussian noise through Bayesian analysis of the true dimensionality of the data, i.e., the number of activation and non-Gaussian noise sources. This enables us to carry out probabilistic modeling and achieves an asymptotically unique decomposition of the data. It reduces problems of interpretation, as each final independent component is now much more likely to be due to only one physical or physiological process. We also describe other improvements to standard ICA, such as temporal prewhitening and variance normalization of timeseries, the latter being particularly useful in the context of dimensionality reduction when weak activation is present. We discuss the use of prior information about the spatiotemporal nature of the source processes, and an alternative-hypothesis testing approach for inference, using Gaussian mixture models. The performance of our approach is illustrated and evaluated on real and artificial FMRI data, and compared to the spatio-temporal accuracy of results obtained from classical ICA and GLM analyses.","Independent component analysis,
Magnetic resonance imaging,
Testing,
Magnetic analysis,
Gaussian noise,
Signal analysis,
Image analysis,
Signal processing,
Biomedical imaging,
Noise reduction"
A Cooperative approach to particle swarm optimization,"The particle swarm optimizer (PSO) is a stochastic, population-based optimization technique that can be applied to a wide range of problems, including neural network training. This paper presents a variation on the traditional PSO algorithm, called the cooperative particle swarm optimizer, or CPSO, employing cooperative behavior to significantly improve the performance of the original algorithm. This is achieved by using multiple swarms to optimize different components of the solution vector cooperatively. Application of the new PSO algorithm on several benchmark optimization problems shows a marked improvement in performance over the traditional PSO.",
Ridge-based vessel segmentation in color images of the retina,"A method is presented for automated segmentation of vessels in two-dimensional color images of the retina. This method can be used in computer analyses of retinal images, e.g., in automated screening for diabetic retinopathy. The system is based on extraction of image ridges, which coincide approximately with vessel centerlines. The ridges are used to compose primitives in the form of line elements. With the line elements an image is partitioned into patches by assigning each image pixel to the closest line element. Every line element constitutes a local coordinate frame for its corresponding patch. For every pixel, feature vectors are computed that make use of properties of the patches and the line elements. The feature vectors are classified using a kNN-classifier and sequential forward feature selection. The algorithm was tested on a database consisting of 40 manually labeled images. The method achieves an area under the receiver operating characteristic curve of 0.952. The method is compared with two recently published rule-based methods of Hoover et al. and Jiang et al. . The results show that our method is significantly better than the two rule-based methods (p<0.01). The accuracy of our method is 0.944 versus 0.947 for a second observer.",
Contiki - a lightweight and flexible operating system for tiny networked sensors,"Wireless sensor networks are composed of large numbers of tiny networked devices that communicate untethered. For large scale networks, it is important to be able to download code into the network dynamically. We present Contiki, a lightweight operating system with support for dynamic loading and replacement of individual programs and services. Contiki is built around an event-driven kernel but provides optional preemptive multithreading that can be applied to individual processes. We show that dynamic loading and unloading is feasible in a resource constrained environment, while keeping the base system lightweight and compact.","Operating systems,
Sensor systems,
Wireless sensor networks,
Kernel,
Microcontrollers,
Libraries,
Large-scale systems,
Read-write memory,
Computer science,
Multithreading"
Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation,"Characterizing the performance of image segmentation approaches has been a persistent challenge. Performance analysis is important since segmentation algorithms often have limited accuracy and precision. Interactive drawing of the desired segmentation by human raters has often been the only acceptable approach, and yet suffers from intra-rater and inter-rater variability. Automated algorithms have been sought in order to remove the variability introduced by raters, but such algorithms must be assessed to ensure they are suitable for the task. The performance of raters (human or algorithmic) generating segmentations of medical images has been difficult to quantify because of the difficulty of obtaining or estimating a known true segmentation for clinical data. Although physical and digital phantoms can be constructed for which ground truth is known or readily estimated, such phantoms do not fully reflect clinical images due to the difficulty of constructing phantoms which reproduce the full range of imaging characteristics and normal and pathological anatomical variability observed in clinical data. Comparison to a collection of segmentations by raters is an attractive alternative since it can be carried out directly on the relevant clinical imaging data. However, the most appropriate measure or set of measures with which to compare such segmentations has not been clarified and several measures are used in practice. We present here an expectation-maximization algorithm for simultaneous truth and performance level estimation (STAPLE). The algorithm considers a collection of segmentations and computes a probabilistic estimate of the true segmentation and a measure of the performance level represented by each segmentation. The source of each segmentation in the collection may be an appropriately trained human rater or raters, or may be an automated segmentation algorithm. The probabilistic estimate of the true segmentation is formed by estimating an optimal combination of the segmentations, weighting each segmentation depending upon the estimated performance level, and incorporating a prior model for the spatial distribution of structures being segmented as well as spatial homogeneity constraints. STAPLE is straightforward to apply to clinical imaging data, it readily enables assessment of the performance of an automated image segmentation algorithm, and enables direct comparison of human rater and algorithm performance.","Image segmentation,
Biomedical imaging,
Humans,
Radiology,
Hospitals,
Imaging phantoms,
Performance analysis,
Computer science,
Artificial intelligence,
Pathology"
The Princeton Shape Benchmark,"In recent years, many shape representations and geometric algorithms have been proposed for matching 3D shapes. Usually, each algorithm is tested on a different (small) database of 3D models, and thus no direct comparison is available for competing methods. We describe the Princeton Shape Benchmark (PSB), a publicly available database of polygonal models collected from the World Wide Web and a suite of tools for comparing shape matching and classification algorithms. One feature of the benchmark is that it provides multiple semantic labels for each 3D model. For instance, it includes one classification of the 3D models based on function, another that considers function and form, and others based on how the object was constructed (e.g., man-made versus natural objects). We find that experiments with these classifications can expose different properties of shape-based retrieval algorithms. For example, out of 12 shape descriptors tested, extended Gaussian images by B. Horn (1984) performed best for distinguishing man-made from natural objects, while they performed among the worst for distinguishing specific object types. Based on experiments with several different shape descriptors, we conclude that no single descriptor is best for all classifications, and thus the main contribution of this paper is to provide a framework to determine the conditions under which each descriptor performs best.",
Robust image segmentation using FCM with spatial constraints based on new kernel-induced distance measure,"Fuzzy c-means clustering (FCM) with spatial constraints (FCM/spl I.bar/S) is an effective algorithm suitable for image segmentation. Its effectiveness contributes not only to the introduction of fuzziness for belongingness of each pixel but also to exploitation of spatial contextual information. Although the contextual information can raise its insensitivity to noise to some extent, FCM/spl I.bar/S still lacks enough robustness to noise and outliers and is not suitable for revealing non-Euclidean structure of the input data due to the use of Euclidean distance (L/sub 2/ norm). In this paper, to overcome the above problems, we first propose two variants, FCM/spl I.bar/S/sub 1/ and FCM/spl I.bar/S/sub 2/, of FCM/spl I.bar/S to aim at simplifying its computation and then extend them, including FCM/spl I.bar/S, to corresponding robust kernelized versions KFCM/spl I.bar/S, KFCM/spl I.bar/S/sub 1/ and KFCM/spl I.bar/S/sub 2/ by the kernel methods. Our main motives of using the kernel methods consist in: inducing a class of robust non-Euclidean distance measures for the original data space to derive new objective functions and thus clustering the non-Euclidean structures in data; enhancing robustness of the original clustering algorithms to noise and outliers, and still retaining computational simplicity. The experiments on the artificial and real-world datasets show that our proposed algorithms, especially with spatial constraints, are more effective.",
PCA-SIFT: a more distinctive representation for local image descriptors,"Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.",
The similarity metric,"A new class of distances appropriate for measuring similarity relations between sequences, say one type of similarity per distance, is studied. We propose a new ""normalized information distance,"" based on the noncomputable notion of Kolmogorov complexity, and show that it is in this class and it minorizes every computable distance in the class (that is, it is universal in that it discovers all computable similarities). We demonstrate that it is a metric and call it the similarity metric . This theory forms the foundation for a new practical tool. To evidence generality and robustness, we give two distinctive applications in widely divergent areas using standard compression programs like gzip and GenCompress. First, we compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we fully automatically compute the language tree of 52 different languages.",
Canonical Correlation Analysis: An Overview with Application to Learning Methods,"We present a general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text. The semantic space provides a common representation and enables a comparison between the text and images. In the experiments, we look at two approaches of retrieving images based on only their content from a text query. We compare orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model.",
Biometric cryptosystems: issues and challenges,"In traditional cryptosystems, user authentication is based on possession of secret keys; the method falls apart if the keys are not kept secret (i.e., shared with non-legitimate users). Further, keys can be forgotten, lost, or stolen and, thus, cannot provide non-repudiation. Current authentication systems based on physiological and behavioral characteristics of persons (known as biometrics), such as fingerprints, inherently provide solutions to many of these problems and may replace the authentication component of traditional cryptosystems. We present various methods that monolithically bind a cryptographic key with the biometric template of a user stored in the database in such a way that the key cannot be revealed without a successful biometric authentication. We assess the performance of one of these biometric key binding/generation algorithms using the fingerprint biometric. We illustrate the challenges involved in biometric key generation primarily due to drastic acquisition variations in the representation of a biometric identifier and the imperfect nature of biometric feature extraction and matching algorithms. We elaborate on the suitability of these algorithms for digital rights management systems.","Biometrics,
Cryptography,
Authentication,
Fingerprint recognition,
Security,
Databases,
Feature extraction,
Entropy,
Privacy,
Computer science"
Improved watershed transform for medical image segmentation using prior information,"The watershed transform has interesting properties that make it useful for many different image segmentation applications: it is simple and intuitive, can be parallelized, and always produces a complete division of the image. However, when applied to medical image analysis, it has important drawbacks (oversegmentation, sensitivity to noise, poor detection of thin or low signal to noise ratio structures). We present an improvement to the watershed transform that enables the introduction of prior information in its calculation. We propose to introduce this information via the use of a previous probability calculation. Furthermore, we introduce a method to combine the watershed transform and atlas registration, through the use of markers. We have applied our new algorithm to two challenging applications: knee cartilage and gray matter/white matter segmentation in MR images. Numerical validation of the results is provided, demonstrating the strength of the algorithm for medical image segmentation.","Biomedical imaging,
Image segmentation,
Signal to noise ratio,
Morphological operations,
Surgery,
Hospitals,
Filters,
Image analysis,
Medical signal detection,
Probability"
Load forecasting using support vector Machines: a study on EUNITE competition 2001,"Load forecasting is usually made by constructing models on relative information, such as climate and previous load demand data. In 2001, EUNITE network organized a competition aiming at mid-term load forecasting (predicting daily maximum load of the next 31 days). During the competition we proposed a support vector machine (SVM) model, which was the winning entry, to solve the problem. In this paper, we discuss in detail how SVM, a new learning technique, is successfully applied to load forecasting. In addition, motivated by the competition results and the approaches by other participants, more experiments and deeper analyses are conducted and presented here. Some important conclusions from the results are that temperature (or other types of climate information) might not be useful in such a mid-term load forecasting problem and that the introduction of time-series concept may improve the forecasting.","Load forecasting,
Support vector machines,
Temperature,
Demand forecasting,
Predictive models,
Learning systems,
Data analysis,
Power industry,
Computer science,
Intelligent networks"
Improved MDS-based localization,"It is often useful to know the geographic positions of nodes in a communications network, but adding GPS receivers or other sophisticated sensors to every node can be expensive. MDS-MAP is a recent localization method based on multidimensional scaling (MDS). It uses connectivity information - who is within communications range of whom - to derive the locations of the nodes in the network, and can take advantage of additional data, such as estimated distances between neighbors or known positions for certain anchor nodes, if they are available. However, MDS-MAP is an inherently centralized algorithm and is therefore of limited utility in many applications. In this paper, we present a new variant of the MDS-MAP method, which we call MDS-MAP(P) standing for MDS-MAP using patches of relative maps, that can be executed in a distributed fashion. Using extensive simulations, we show that the new algorithm not only preserves the good performance of the original method on relatively uniform layouts, but also performs much better than the original on irregularly-shaped networks. The main idea is to build a local map at each node of the immediate vicinity and then merge these maps together to form a global map. This approach works much better for topologies in which the shortest path distance between two nodes does not correspond well to their Euclidean distance. We also discuss an optional refinement step that improves solution quality even further at the expense of additional computation.",
Identification of humans using gait,"We propose a view-based approach to recognize humans from their gait. Two different image features have been considered: the width of the outer contour of the binarized silhouette of the walking person and the entire binary silhouette itself. To obtain the observation vector from the image features, we employ two different methods. In the first method, referred to as the indirect approach, the high-dimensional image feature is transformed to a lower dimensional space by generating what we call the frame to exemplar (FED) distance. The FED vector captures both structural and dynamic traits of each individual. For compact and effective gait representation and recognition, the gait information in the FED vector sequences is captured in a hidden Markov model (HMM). In the second method, referred to as the direct approach, we work with the feature vector directly (as opposed to computing the FED) and train an HMM. We estimate the HMM parameters (specifically the observation probability B) based on the distance between the exemplars and the image features. In this way, we avoid learning high-dimensional probability density functions. The statistical nature of the HMM lends overall robustness to representation and recognition. The performance of the methods is illustrated using several databases.",
Variable step-size NLMS and affine projection algorithms,This letter proposes two new variable step-size algorithms for normalized least mean square and affine projection. The proposed schemes lead to faster convergence rate and lower misadjustment error.,"Projection algorithms,
Convergence,
Adaptive filters,
Least squares approximation,
Gaussian noise,
Mean square error methods,
Autocorrelation,
Adaptive systems,
Computer science education,
Educational programs"
Comparing different classifiers for automatic age estimation,"We describe a quantitative evaluation of the performance of different classifiers in the task of automatic age estimation. In this context, we generate a statistical model of facial appearance, which is subsequently used as the basis for obtaining a compact parametric description of face images. The aim of our work is to design classifiers that accept the model-based representation of unseen images and produce an estimate of the age of the person in the corresponding face image. For this application, we have tested different classifiers: a classifier based on the use of quadratic functions for modeling the relationship between face model parameters and age, a shortest distance classifier, and artificial neural network based classifiers. We also describe variations to the basic method where we use age-specific and/or appearance specific age estimation methods. In this context, we use age estimation classifiers for each age group and/or classifiers for different clusters of subjects within our training set. In those cases, part of the classification procedure is devoted to choosing the most appropriate classifier for the subject/age range in question, so that more accurate age estimates can be obtained. We also present comparative results concerning the performance of humans and computers in the task of age estimation. Our results indicate that machines can estimate the age of a person almost as reliably as humans.",
Principal geodesic analysis for the study of nonlinear statistics of shape,"A primary goal of statistical shape analysis is to describe the variability of a population of geometric objects. A standard technique for computing such descriptions is principal component analysis. However, principal component analysis is limited in that it only works for data lying in a Euclidean vector space. While this is certainly sufficient for geometric models that are parameterized by a set of landmarks or a dense collection of boundary points, it does not handle more complex representations of shape. We have been developing representations of geometry based on the medial axis description or m-rep. While the medial representation provides a rich language for variability in terms of bending, twisting, and widening, the medial parameters are not elements of a Euclidean vector space. They are in fact elements of a nonlinear Riemannian symmetric space. In this paper, we develop the method of principal geodesic analysis, a generalization of principal component analysis to the manifold setting. We demonstrate its use in describing the variability of medially-defined anatomical objects. Results of applying this framework on a population of hippocampi in a schizophrenia study are presented.",
The impact of technology scaling on lifetime reliability,"The relentless scaling of CMOS technology has provided a steady increase in processor performance for the past three decades. However, increased power densities (hence temperatures) and other scaling effects have an adverse impact on long-term processor lifetime reliability. This paper represents a first attempt at quantifying the impact of scaling on lifetime reliability due to intrinsic hard errors, taking workload characteristics into consideration. For our quantitative evaluation, we use RAMP (Srinivasan et al., 2004), a previously proposed industrial-strength model that provides reliability estimates for a workload, but for a given technology. We extend RAMP by adding scaling specific parameters to enable workload-dependent lifetime reliability evaluation at different technologies. We show that (1) scaling has a significant impact on processor hard failure rates - on average, with SPEC benchmarks, we find the failure rate of a scaled 65nm processor to be 316% higher than a similarly pipelined 180nm processor; (2) time-dependent dielectric breakdown and electromigration have the largest increases; and (3) with scaling, the difference in reliability from running at worst-case vs. typical workload operating conditions increases significantly, as does the difference from running different workloads. Our results imply that leveraging a single microarchitecture design for multiple remaps across a few technology generations will become increasingly difficult, and motivate a need for workload specific, microarchitectural lifetime reliability awareness at an early design stage.","CMOS technology,
Dynamic voltage scaling,
Threshold voltage,
CMOS process,
Dielectric breakdown,
Electromigration,
Microarchitecture,
Thermal stresses,
Rivers,
Computer science"
Mapping and localization with RFID technology,"We analyze whether radio frequency identification (RFID) technology can be used to improve the localization of mobile robots and persons in their environment. In particular we study the problem of localizing RFID tags with a mobile platform that is equipped with a pair of RFID antennas. We present a probabilistic measurement model for RFID readers that allow us to accurately localize RFID tags in the environment. We also demonstrate how such maps can be used to localize a robot and persons in their environment. Finally, we present experiments illustrating that the computational requirements for global robot localization can be reduced strongly by fusing RFID information with laser data.","Radiofrequency identification,
Mobile robots,
RFID tags,
Laser modes,
Mobile antennas,
Computer science,
Antenna measurements,
Robot sensing systems,
Robot localization,
Circuits"
Genus zero surface conformal mapping and its application to brain surface mapping,"We developed a general method for global conformal parameterizations based on the structure of the cohomology group of holomorphic one-forms for surfaces with or without boundaries (Gu and Yau, 2002), (Gu and Yau, 2003). For genus zero surfaces, our algorithm can find a unique mapping between any two genus zero manifolds by minimizing the harmonic energy of the map. In this paper, we apply the algorithm to the cortical surface matching problem. We use a mesh structure to represent the brain surface. Further constraints are added to ensure that the conformal map is unique. Empirical tests on magnetic resonance imaging (MRI) data show that the mappings preserve angular relationships, are stable in MRIs acquired at different times, and are robust to differences in data triangulation, and resolution. Compared with other brain surface conformal mapping algorithms, our algorithm is more stable and has good extensibility.",
Nonparametric weighted feature extraction for classification,"In this paper, a new nonparametric feature extraction method is proposed for high-dimensional multiclass pattern recognition problems. It is based on a nonparametric extension of scatter matrices. There are at least two advantages to using the proposed nonparametric scatter matrices. First, they are generally of full rank. This provides the ability to specify the number of extracted features desired and to reduce the effect of the singularity problem. This is in contrast to parametric discriminant analysis, which usually only can extract L-1 (number of classes minus one) features. In a real situation, this may not be enough. Second, the nonparametric nature of scatter matrices reduces the effects of outliers and works well even for nonnormal datasets. The new method provides greater weight to samples near the expected decision boundary. This tends to provide for increased classification accuracy.","Feature extraction,
Covariance matrix,
Scattering,
Pattern recognition,
Availability,
Labeling,
Focusing,
Mathematics,
Computer science education,
Hyperspectral imaging"
Energy-efficient area monitoring for sensor networks,"The nodes in sensor networks must self-organize to monitor the target area as long as possible. Researchers at the Fundamental Computer Science Laboratory of Lille are developing strategies for selecting and updating an energy-efficient connected active sensor set that extends the network lifetime. We report on their work to optimize energy consumption in three separate problems: area coverage, request spreading, and data aggregation.",
Predicting source code changes by mining change history,"Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.","History,
Data mining,
Association rules,
Pattern analysis,
Computer Society,
Software systems,
Computer languages,
Frequency,
Programming profession,
Computer science"
Sharing features: efficient boosting procedures for multiclass object detection,"We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges.",
Constrained coverage for mobile sensor networks,"We consider the problem of self-deployment of a mobile sensor network. We are interested in a deployment strategy that maximizes the area coverage of the network with the constraint that each of the nodes has at least K neighbors, where K is a user-specified parameter. We propose an algorithm based on artificial potential fields which is distributed, scalable and does not require a prior map of the environment. Simulations establish that the resulting networks have the required degree with a high probability, are well connected and achieve good coverage. We present analytical results for the coverage achievable by uniform random and symmetrically tiled network configurations and use these to evaluate the performance of our algorithm.","Mobile robots,
Sensor systems,
Robot sensing systems,
Embedded system,
Taxonomy,
Laboratories,
Computer science,
Mobile computing,
Global Positioning System,
Fires"
The limits of localization using signal strength: a comparative study,"We characterize the fundamental limits of localization using signal strength in indoor environments. Signal strength approaches are attractive because they are widely applicable to wireless sensor networks and do not require additional localization hardware. We show that although a broad spectrum of algorithms can trade accuracy for precision, none has a significant advantage in localization performance. We found that using commodity 802.11 technology over a range of algorithms, approaches and environments, one can expect a median localization error of 10 ft and 97th percentile of 30 ft. We present strong evidence that these limitations are fundamental and that they are unlikely to transcend without a fundamentally more complex environmental models or additional localization infrastructure.","Hardware,
Wireless LAN,
Indoor environments,
Sampling methods,
Computer science,
Wireless sensor networks,
Level control,
Transfer functions,
Monitoring,
Routing"
On the representation of intuitionistic fuzzy t-norms and t-conorms,"Intuitionistic fuzzy sets form an extension of fuzzy sets: while fuzzy sets give a degree to which an element belongs to a set, intuitionistic fuzzy sets give both a membership degree and a nonmembership degree. The only constraint on those two degrees is that their sum must be smaller than or equal to 1. In fuzzy set theory, an important class of triangular norms and conorms is the class of continuous Archimedean nilpotent triangular norms and conorms. It has been shown that for such t-norms T there exists a permutation /spl phi/ of [0,1] such that T is the /spl phi/-transform of the Lukasiewicz t-norm. In this paper we introduce the notion of intuitionistic fuzzy t-norm and t-conorm, and investigate under which conditions a similar representation theorem can be obtained.","Fuzzy sets,
Fuzzy set theory,
Uncertainty,
Fuzzy reasoning,
Fuzzy systems,
Set theory,
Mathematics,
Computer science,
Mathematical model,
Databases"
Detection of optic disc in retinal images by means of a geometrical model of vessel structure,"We present here a new method to identify the position of the optic disc (OD) in retinal fundus images. The method is based on the preliminary detection of the main retinal vessels. All retinal vessels originate from the OD and their path follows a similar directional pattern (parabolic course) in all images. To describe the general direction of retinal vessels at any given position in the image, a geometrical parametric model was proposed, where two of the model parameters are the coordinates of the OD center. Using as experimental data samples of vessel centerline points and corresponding vessel directions, provided by any vessel identification procedure, model parameters were identified by means of a simulated annealing optimization technique. These estimated values provide the coordinates of the center of OD. A Matlab/spl reg/ prototype implementing this method was developed. An evaluation of the proposed procedure was performed using the set of 81 images from the STARE project, containing images from both normal and pathological subjects. The OD position was correctly identified in 79 out of 81 images (98%), even in rather difficult pathological situations.","Optical detectors,
Geometrical optics,
Structural discs,
Retina,
Solid modeling,
Mathematical model,
Retinal vessels,
Pathology,
Parametric statistics,
Simulated annealing"
Binary increase congestion control (BIC) for fast long-distance networks,"High-speed networks with large delays present a unique environment where TCP may have a problem utilizing the full bandwidth. Several congestion control proposals have been suggested to remedy this problem. The existing protocols consider mainly two properties: TCP friendliness and bandwidth scalability. That is, a protocol should not take away too much bandwidth from standard TCP flows while utilizing the full bandwidth of high-speed networks. This work presents another important constraint, namely, RTT (round trip time) unfairness where competing flows with different RTTs may consume vastly unfair bandwidth shares. Existing schemes have a severe RTT unfairness problem because the congestion window increase rate gets larger as the window grows ironically the very reason that makes them more scalable. RTT unfairness for high-speed networks occurs distinctly with drop tail routers for flows with large congestion windows where packet loss can be highly synchronized. After identifying the RTT unfairness problem of existing protocols, This work presents a new congestion control scheme that alleviates RTT unfairness while supporting TCP friendliness and bandwidth scalability. The proposed congestion control algorithm uses two window size control policies called additive increase and binary search increase. When the congestion window is large, additive increase with a large increment ensures square RTT unfairness as well as good scalability. Under small congestion windows, binary search increase supports TCP friendliness. The simulation results confirm these properties of the protocol.","Bandwidth,
Scalability,
High-speed networks,
Access protocols,
Remote monitoring,
Computer science,
Bit error rate,
Network interfaces,
Tail,
Internet"
Detecting unusual activity in video,"We present an unsupervised technique for detecting unusual activity in a large video set using many simple features. No complex activity models and no supervised feature selections are used. We divide the video into equal length segments and classify the extracted features into prototypes, from which a prototype-segment co-occurrence matrix is computed. Motivated by a similar problem in document-keyword analysis, we seek a correspondence relationship between prototypes and video segments which satisfies the transitive closure constraint. We show that an important sub-family of correspondence functions can be reduced to co-embedding prototypes and segments to N-D Euclidean space. We prove that an efficient, globally optimal algorithm exists for the co-embedding problem. Experiments on various real-life videos have validated our approach.","Event detection,
Feature extraction,
Prototypes,
Hidden Markov models,
Image segmentation,
Object detection,
Computer science,
Information science,
Layout,
Surveillance"
Fast portscan detection using sequential hypothesis testing,"Attackers routinely perform random portscans of IP addresses to find vulnerable servers to compromise. Network intrusion detection systems (NIDS) attempt to detect such behavior and flag these portscanners as malicious. An important need in such systems is prompt response: the sooner a NIDS detects malice, the lower the resulting damage. At the same time, a NIDS should not falsely implicate benign remote hosts as malicious. Balancing the goals of promptness and accuracy in detecting malicious scanners is a delicate and difficult task. We develop a connection between this problem and the theory of sequential hypothesis testing and show that one can model accesses to local IP addresses as a random walk on one of two stochastic processes, corresponding respectively to the access patterns of benign remote hosts and malicious ones. The detection problem then becomes one of observing a particular trajectory and inferring from it the most likely classification for the remote host. We use this insight to develop TRW (Threshold Random Walk), an online detection algorithm that identifies malicious remote hosts. Using an analysis of traces from two qualitatively different sites, we show that TRW requires a much smaller number of connection attempts (4 or 5 in practice) to detect malicious activity compared to previous schemes, while also providing theoretical bounds on the low (and configurable) probabilities of missed detection and false alarms. In summary, TRW performs significantly faster and also more accurately than other current solutions.","Sequential analysis,
Intrusion detection,
Web server,
Probes,
Laboratories,
Internet,
Network servers,
Reconnaissance,
Search engines,
Computer science"
Single-ISA heterogeneous multi-core architectures for multithreaded workload performance,"A single-ISA heterogeneous multi-core architecture is a chip multiprocessor composed of cores of varying size, performance, and complexity. This paper demonstrates that this architecture can provide significantly higher performance in the same area than a conventional chip multiprocessor. It does so by matching the various jobs of a diverse workload to the various cores. This type of architecture covers a spectrum of workloads particularly well, providing high single-thread performance when thread parallelism is low, and high throughput when thread parallelism is high. This paper examines two such architectures in detail, demonstrating dynamic core assignment policies that provide significant performance gains over naive assignment, and even outperform the best static assignment. It examines policies for heterogeneous architectures both with and without multithreading cores. One heterogeneous architecture we examine outperforms the comparable-area homogeneous architecture by up to 63%, and our best core assignment strategy achieves up to 31% speedup over a naive policy.","Computer architecture,
Throughput,
Yarn,
Parallel processing,
Performance gain,
Multithreading,
Delay,
Computer science,
Milling machines,
Microprocessors"
Environmental sensor networks,"The developments in wireless network technology and miniaturization makes it possible to realistically monitor the natural environment. Within the field of environmental sensor networks, domain knowledge is an essential fourth component. Before designing and installing any system, it is necessary to understand its physical environment and deployment in detail. Sensor networks are designed to transmit data from an array of sensors to a server data repository. They do not necessarily use a simple one way data stream over a communication network rather elements of the system decide what data to pass on, using local area summaries and filtering to minimize power use while maximizing the information content. The Envisense Glacs Web project is developing a monitoring system for a glacial environment. Monitoring the ice caps and glaciers provides valuable information about the global warming and climate change.","Intelligent sensors,
Remote monitoring,
Sensor phenomena and characterization,
Sensor arrays,
Vehicle detection,
Wireless sensor networks,
Network servers,
Kirk field collapse effect,
Condition monitoring,
Sensor systems"
A peer-to-peer architecture for media streaming,"Given that the Internet does not widely support Internet protocol multicast while content-distribution-network technologies are costly, the concept of peer-to-peer could be a promising start for enabling large-scale streaming systems. In our so-called Zigzag approach, we propose a method for clustering peers into a hierarchy called the administrative organization for easy management, and a method for building the multicast tree atop this hierarchy for efficient content transmission. In Zigzag, the multicast tree has a height logarithmic with the number of clients, and a node degree bounded by a constant. This helps reduce the number of processing hops on the delivery path to a client while avoiding network bottlenecks. Consequently, the end-to-end delay is kept small. Although one could build a tree satisfying such properties easily, an efficient control protocol between the nodes must be in place to maintain the tree under the effects of network dynamics. Zigzag handles such situations gracefully, requiring a constant amortized worst-case control overhead. Especially, failure recovery is done regionally with impact on, at most, a constant number of existing clients and with mostly no burden on the server.",
A technique to build a secret key in integrated circuits for identification and authentication applications,"This paper describes a technique that exploits the statistical delay variations of wires and transistors across ICs to build a secret key unique to each IC. To explore its feasibility, we fabricated a candidate circuit to generate a response based on its delay characteristics. We show that there exists enough delay variation across ICs implementing, the proposed circuit to identify individual ICs. Further. the circuit, functions reliably over a practical range of environmental variation such as temperature and voltage.",
An approach to multimodal biomedical image registration utilizing particle swarm optimization,"Biomedical image registration, or geometric alignment of two-dimensional and/or three-dimensional (3D) image data, is becoming increasingly important in diagnosis, treatment planning, functional studies, computer-guided therapies, and in biomedical research. Registration based on intensity values usually requires optimization of some similarity metric between the images. Local optimization techniques frequently fail because functions of these metrics with respect to transformation parameters are generally nonconvex and irregular and, therefore, global methods are often required. In this paper, a new evolutionary approach, particle swarm optimization, is adapted for single-slice 3D-to-3D biomedical image registration. A new hybrid particle swarm technique is proposed that incorporates initial user guidance. Multimodal registrations with initial orientations far from the ground truth were performed on three volumes from different modalities. Results of optimizing the normalized mutual information similarity metric were compared with various evolutionary strategies. The hybrid particle swarm technique produced more accurate registrations than the evolutionary strategies in many cases, with comparable convergence. These results demonstrate that particle swarm approaches, along with evolutionary techniques and local methods, are useful in image registration, and emphasize the need for hybrid approaches for difficult registration problems.",
Measurement of the flux and energy spectrum of cosmic-ray induced neutrons on the ground,"New ground-based measurements of the cosmic-ray induced neutron flux and its energy distribution have been made at several locations across the United States using an extended-energy Bonner sphere spectrometer. The data cover over twelve decades of neutron energy, from meV to GeV. An expression to scale the flux to other locations has been developed from a fit to the altitude dependence of our measurements and an expression from the literature for the geomagnetic and solar-activity dependence of neutron monitor rates. In addition, an analytic expression is provided which fits the neutron spectrum above about 0.4 MeV. The neutron flux is important for estimating the soft-error rate in computer memories and recent computer logic devices.",
Locating and bypassing routing holes in sensor networks,"Many algorithms for routing in sensor networks exploit greedy forwarding strategies to get packets to their destinations. We study a fundamental difficulty such strategies face: the ""local minimum phenomena"" that can cause packets to get stuck. We give a definition of stuck nodes where packets may get stuck in greedy multi-hop forwarding, and develop a local rule, the TENT rule, for each node in the network to test whether a packet can get stuck at that node. To help the packets get out of stuck nodes, we describe a distributed algorithm, BOUNDHOLE, to build routes around holes, which are connected regions of the network with boundaries consisting of all the stuck nodes. We show that these hole-surrounding routes can be used in many applications such as geographic routing, path migration, information storage mechanisms and identification of regions of interest.","Intelligent networks,
Electronic mail,
Wireless sensor networks,
Computer science,
Network topology,
Routing protocols,
Sensor phenomena and characterization,
Spread spectrum communication,
Testing,
Distributed algorithms"
PROBMELA: a modeling language for communicating probabilistic processes,"Building automated tools to address the analysis of reactive probabilistic systems requires a simple, but expressive input language with a formal semantics based on a probabilistic operational model that can serve as starting point for verification algorithms. We introduce for probabilistic parallel programs with shared variables, message passing via synchronous and (perfect or lossy) fifo channels and atomic regions and provide a structured operational semantics. Applied to finite-state systems, the semantics can serve as basis for the algorithmic generation of a Markov decision process that models the stepwise behavior of the given system.",
Deterministic memory-efficient string matching algorithms for intrusion detection,"Intrusion detection systems (IDSs) have become widely recognized as powerful tools for identifying, deterring and deflecting malicious attacks over the network. Essential to almost every intrusion detection system is the ability to search through packets and identify content that matches known attacks. Space and time efficient string matching algorithms are therefore important for identifying these packets at line rate. We examine string matching algorithms and their use for intrusion detection, in particular, we focus our efforts on providing worst-case performance that is amenable to hardware implementation. We contribute modifications to the Aho-Corasick string-matching algorithm that drastically reduce the amount of memory required and improve its performance on hardware implementations. We also show that these modifications do not drastically affect software performance on commodity processors, and therefore may be worth considering in these cases as well.","Intrusion detection,
Computer science,
Hardware,
Web server,
Computer crime,
Protection,
Telecommunication traffic,
Power engineering and energy,
Software performance,
Internet"
An information retrieval approach to concept location in source code,"Concept location identifies parts of a software system that implement a specific concept that originates from the problem or the solution domain. Concept location is a very common software engineering activity that directly supports software maintenance and evolution tasks such as incremental change and reverse engineering. This work addresses the problem of concept location using an advanced information retrieval method, Latent Semantic Indexing (LSI). LSI is used to map concepts expressed in natural language by the programmer to the relevant parts of the source code. Results of a case study on NCSA Mosaic are presented and compared with previously published results of other static methods for concept location.","Information retrieval,
Programming profession,
Software maintenance,
Natural languages,
Documentation,
Computer science,
Software systems,
Reverse engineering,
Large scale integration,
Pattern matching"
On network correlated data gathering,"We consider the problem of correlated data gathering by a network with a sink node and a tree communication structure, where the goal is to minimize the total transmission cost of transporting the information collected by the nodes, to the sink node. Two coding strategies are analyzed: a Slepian-Wolf model where optimal coding is complex and transmission optimization is simple, and a joint entropy coding model with explicit communication where coding is simple and transmission optimization is difficult. This problem requires a joint optimization of the rate allocation at the nodes and of the transmission structure. For the Slepian-Wolf setting, we derive a closed form solution and an efficient distributed approximation algorithm with a good performance. For the explicit communication case, we prove that building an optimal data gathering tree is NP-complete and we propose various distributed approximation algorithms.","Tree graphs,
Approximation algorithms,
Cost function,
Base stations,
Laboratories,
Computer science,
Entropy coding,
Closed-form solution,
Buildings,
Graph theory"
Detection strategies: metrics-based rules for detecting design flaws,"In order to support the maintenance of an object-oriented software system, the quality of its design must be evaluated using adequate quantification means. In spite of the current extensive use of metrics, if used in isolation metrics are oftentimes too fine grained to quantify comprehensively an investigated design aspect (e.g., distribution of system's intelligence among classes). To help developers and maintainers detect and localize design problems in a system, we propose a novel mechanism - called detection strategy - for formulating metrics-based rules that capture deviations from good design principles and heuristics. Using detection strategies an engineer can directly localize classes or methods affected by a particular design flaw (e.g., God Class), rather than having to infer the real design problem from a large set of abnormal metric values. We have defined such detection strategies for capturing around ten important flaws of object-oriented design found in the literature and validated the approach experimentally on multiple large-scale case-studies.",
Surface normal overlap: a computer-aided detection algorithm with application to colonic polyps and lung nodules in helical CT,We developed a novel computer-aided detection (CAD) algorithm called the surface normal overlap method that we applied to colonic polyp detection and lung nodule detection in helical computed tomography (CT) images. We demonstrate some of the theoretical aspects of this algorithm using a statistical shape model. The algorithm was then optimized on simulated CT data and evaluated using a per-lesion cross-validation on 8 CT colonography datasets and on 8 chest CT datasets. It is able to achieve 100% sensitivity for colonic polyps 10 mm and larger at 7.0 false positives (FPs)/dataset and 90% sensitivity for solid lung nodules 6 mm and larger at 5.6 FP/dataset.,
Internet indirection infrastructure,"Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes a general, overlay-based Internet Indirection Infrastructure (i3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows i3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.",
Haptic rendering: introductory concepts,"Haptic rendering allows users to ""feel"" virtual objects in a simulated environment. We survey current haptic systems and discuss some basic haptic-rendering algorithms. In the past decade we've seen an enormous increase in interest in the science of haptics. Haptics broadly refers to touch interactions (physical contact) that occur for the purpose of perception or manipulation of objects. These interactions can be between a human hand and a real object; a robot end-effector and a real object; a human hand and a simulated object (via haptic interface devices); or a variety of combinations of human and machine interactions with real, remote, or virtual objects. Rendering refers to the process by which desired sensory stimuli are imposed on the user to convey information about a virtual haptic object.","Haptic interfaces,
Computer displays,
Humans,
Computational modeling,
Computer graphics,
Computer simulation,
Engines,
Feedback,
Auditory displays,
Robots"
Inferring 3D body pose from silhouettes using activity manifold learning,"We aim to infer 3D body pose directly from human silhouettes. Given a visual input (silhouette), the objective is to recover the intrinsic body configuration, recover the viewpoint, reconstruct the input and detect any spatial or temporal outliers. In order to recover intrinsic body configuration (pose) from the visual input (silhouette), we explicitly learn view-based representations of activity manifolds as well as learn mapping functions between such central representations and both the visual input space and the 3D body pose space. The body pose can be recovered in a closed form in two steps by projecting the visual input to the learned representations of the activity manifold, i.e., finding the point on the learned manifold representation corresponding to the visual input, followed by interpolating 3D pose.",
Learning object detection from a small number of examples: the importance of good features,"Face detection systems have recently achieved high detection rates and real-time performance. However, these methods usually rely on a huge training database (around 5,000 positive examples for good performance). While such huge databases may be feasible for building a system that detects a single object, it is obviously problematic for scenarios where multiple objects (or multiple views of a single object) need to be detected. Indeed, even for multi-viewface detection the performance of existing systems is far from satisfactory. In this work we focus on the problem of learning to detect objects from a small training database. We show that performance depends crucially on the features that are used to represent the objects. Specifically, we show that using local edge orientation histograms (EOH) as features can significantly improve performance compared to the standard linear features used in existing systems. For frontal faces, local orientation histograms enable state of the art performance using only a few hundred training examples. For profile view faces, local orientation histograms enable learning a system that seems to outperform the state of the art in real-time systems even with a small number of training examples.",
Modeling complex systems using fuzzy cognitive maps,This research deals with the soft computing methodology of fuzzy cognitive map (FCM). Here a mathematical description of FCM is presented and a new methodology based on fuzzy logic techniques for developing the FCM is examined. The capability and usefulness of FCM in modeling complex systems and the application of FCM to modeling and describing the behavior of a heat exchanger system is presented. The applicability of FCM to model the supervisor of complex systems is discussed and the FCM-supervisor for evaluating the performance of a system is constructed; simulation results are presented and discussed.,
Sensor positioning in wireless ad-hoc sensor networks using multidimensional scaling,"Sensor Positioning is a fundamental and crucial issue for sensor network operation and management. In the paper, we first study some situations where most existing sensor positioning methods tend to fail to perform well, an example being when the topology of a sensor network is anisotropic. Then, we explore the idea of using dimensionality reduction techniques to estimate sensors coordinates in two (or three) dimensional space, and we propose a distributed sensor positioning method based on multidimensional scaling technique to deal with these challenging conditions. Multidimensional scaling and coordinate alignment techniques are applied to recover positions of adjacent sensors. The estimated positions of the anchors are compared with their true physical positions and corrected. The positions of other sensors are corrected accordingly. With iterative adjustment, our method can overcome adverse network and terrain conditions, and generate accurate sensor position. We also propose an on demand sensor positioning method based on the above method.",
A similarity learning approach to content-based image retrieval: application to digital mammography,"In this paper, we describe an approach to content-based retrieval of medical images from a database, and provide a preliminary demonstration of our approach as applied to retrieval of digital mammograms. Content-based image retrieval (CBIR) refers to the retrieval of images from a database using information derived from the images themselves, rather than solely from accompanying text indices. In the medical-imaging context, the ultimate aim of CBIR is to provide radiologists with a diagnostic aid in the form of a display of relevant past cases, along with proven pathology and other suitable information. CBIR may also be useful as a training tool for medical students and residents. The goal of information retrieval is to recall from a database information that is relevant to the user's query. The most challenging aspect of CBIR is the definition of relevance (similarity), which is used to guide the retrieval machine. In this paper, we pursue a new approach, in which similarity is learned from training examples provided by human observers. Specifically, we explore the use of neural networks and support vector machines to predict the user's notion of similarity. Within this framework we propose using a hierarchal learning approach, which consists of a cascade of a binary classifier and a regression module to optimize retrieval effectiveness and efficiency. We also explore how to incorporate online human interaction to achieve relevance feedback in this learning framework. Our experiments are based on a database consisting of 76 mammograms, all of which contain clustered microcalcifications (MCs). Our goal is to retrieve mammogram images containing similar MC clusters to that in a query. The performance of the retrieval system is evaluated using precision-recall curves computed using a cross-validation procedure. Our experimental results demonstrate that: 1) the learning framework can accurately predict the perceptual similarity reported by human observers, thereby serving as a basis for CBIR; 2) the learning-based framework can significantly outperform a simple distance-based similarity metric; 3) the use of the hierarchical two-stage network can improve retrieval performance; and 4) relevance feedback can be effectively incorporated into this learning framework to achieve improvement in retrieval precision based on online interaction with users; and 5) the retrieved images by the network can have predicting value for the disease condition of the query.",
Evidence-based software engineering,"Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.","Software engineering,
Australia,
Laboratories,
Computer science,
Costs,
Psychiatry,
Medical services,
Best practices,
Technological innovation,
Psychology"
A key management scheme for wireless sensor networks using deployment knowledge,"To achieve security in wireless sensor networks, it is important to he able to encrypt messages sent among sensor nodes. Keys for encryption purposes must he agreed upon by communicating nodes. Due to resource constraints, achieving such key agreement in wireless sensor networks is nontrivial. Many key agreement schemes used in general networks, such as Diffie-Hellman and public-key based schemes, are not suitable for wireless sensor networks. Pre-distribution of secret keys for all pairs of nodes is not viable due to the large amount of memory used when the network size is large. Recently, a random key pre-distribution scheme and its improvements have been proposed. A common assumption made by these random key pre-distribution schemes is that no deployment knowledge is available. Noticing that in many practical scenarios, certain deployment knowledge may be available a priori, we propose a novel random key pre-distribution scheme that exploits deployment knowledge and avoids unnecessary key assignments. We show that the performance (including connectivity, memory usage, and network resilience against node capture) of sensor networks can he substantially improved with the use of our proposed scheme. The scheme and its detailed performance evaluation are presented in this paper.","Knowledge management,
Wireless sensor networks,
Intelligent sensors,
Information security,
Cryptography,
Computer science,
Public key,
Resilience,
Patient monitoring,
Computer network management"
Weaknesses and improvements of an efficient password based remote user authentication scheme using smart cards,"Recently, Chien et al. proposed an efficient remote authentication scheme using smart cards. However, we find that their scheme is vulnerable to a reflection attack and an insider attack. In addition, their scheme lacks reparability. Herein, we first show the weaknesses of Chien et al.'s scheme, and then propose an improved scheme with better security strength.",
The architecture of the Earth System Modeling Framework,"The Earth System Modeling Framework (ESMF) project is developing a standard software platform for Earth system models. The standard, which defines a component architecture and a support infrastructure, is being developed under open-software practices. Target applications range from operational numerical weather prediction to climate-system change and predictability studies.",
MultiNet: connecting to multiple IEEE 802.11 networks using a single wireless card,"There are a number of scenarios where it is desirable to have a wireless device connect to multiple networks simultaneously. Currently, this is possible only by using multiple wireless network cards in the device. Unfortunately, using multiple wireless cards causes excessive energy drain and consequent reduction of lifetime in battery operated devices. We propose a software based approach, called MultiNet, that facilitates simultaneous connections to multiple networks by virtualizing a single wireless card. The wireless card is virtualized by introducing an intermediate layer below IP, which continuously switches the card across multiple networks. The goal of the switching algorithm is to he transparent to the user who sees her machine as being connected to multiple networks. We present the design, implementation, and performance of the MultiNet system. We analyze and evaluate buffering and switching algorithms in terms of delay and energy consumption. Our system is agnostic of the upper layer protocols, and works well over popular IEEE 802.11 wireless LAN cards.","Joining processes,
Protocols,
Wireless LAN,
Switches,
Wireless networks,
Delay,
Packet switching,
Traffic control,
Computer science,
Energy consumption"
Discovery of policy anomalies in distributed firewalls,"Firewalls are core elements in network security. However, managing firewall rules, particularly in multi-firewall enterprise networks, has become a complex and error-prone task. Firewall filtering rules have to be written, ordered and distributed carefully in order to avoid firewall policy anomalies that might cause network vulnerability. Therefore, inserting or modifying filtering rules in any firewall requires thorough intra- and inter-firewall analysis to determine the proper rule placement and ordering in the firewalls. We identify all anomalies that could exist in a single- or multi-firewall environment. We also present a set of techniques and algorithms to automatically discover policy anomalies in centralized and distributed legacy firewalls. These techniques are implemented in a software tool called the ""Firewall Policy Advisor"" that simplifies the management of filtering rules and maintains the security of next-generation firewalls.","Filtering,
Software tools,
Telecommunication traffic,
Intelligent networks,
Multimedia systems,
Laboratories,
Computer science,
Management information systems,
Computer security,
Information security"
Deafness: a MAC problem in ad hoc networks when using directional antennas,"This work addresses deafness - a problem that appears when MAC protocols are designed using directional antennas. Briefly, deafness is caused when a transmitter fails to communicate to its intended receiver, because the receiver is beamformed towards a direction away from the transmitter. Existing CSMA/CA protocols rely on the assumption that congestion is the predominant cause of communication failure, and adopt backoff schemes to handle congestion. While this may be appropriate for omnidirectional antennas, for directional antennas, both deafness and congestion can be the reason for communication failures. An appropriate directional MAC protocol needs to classify the actual cause of failure, and react accordingly. This paper quantifies the impact of deafness on directional medium access control, and proposes a tone-based mechanism as one way of addressing deafness. The tone-based mechanism, ToneDMAC, assumes congestion as the default reason for communication failures, and applies a corrective measure whenever the cause is deafness. Simulation results indicate that ToneDMAC can alleviate deafness, and perform better than existing directional MAC protocols.","Deafness,
Intelligent networks,
Ad hoc networks,
Directional antennas,
Media Access Protocol,
Transmitters,
Array signal processing,
Computer science,
Computer networks,
Design engineering"
BIDE: efficient mining of frequent closed sequences,"Previous studies have presented convincing arguments that a frequent pattern mining algorithm should not mine all frequent patterns but only the closed ones because the latter leads to not only more compact yet complete result set but also better efficiency. However, most of the previously developed closed pattern mining algorithms work under the candidate maintenance-and-test paradigm which is inherently costly in both runtime and space usage when the support threshold is low or the patterns become long. We present, BIDE, an efficient algorithm for mining frequent closed sequences without candidate maintenance. We adopt a novel sequence closure checking scheme called bidirectional extension, and prunes the search space more deeply compared to the previous algorithms by using the BackScan pruning method and the Scan-Skip optimization technique. A thorough performance study with both sparse and dense real-life data sets has demonstrated that BIDE significantly outperforms the previous algorithms: it consumes order(s) of magnitude less memory and can be more than an order of magnitude faster. It is also linearly scalable in terms of database size.",
AODV routing protocol implementation design,"To date, the majority of ad hoc routing protocol research has been done using simulation only. One of the most motivating reasons to use simulation is the difficulty of creating a real implementation. In a simulator, the code is contained within a single logical component, which is clearly defined and accessible. On the other hand, creating an implementation requires use of a system with many components, including many that have little or no documentation. The implementation developer must understand not only the routing protocol, but all the system components and their complex interactions. Further, since ad hoc routing protocols are significantly different from traditional routing protocols, a new set of features must be introduced to support the routing protocol. In this paper we describe the event triggers required for AODV operation, the design possibilities and the decisions for our ad hoc on-demand distance vector (AODV) routing protocol implementation, AODV-UCSB. This paper is meant to aid researchers in developing their own on-demand ad hoc routing protocols and assist users in determining the implementation design that best fits their needs.","Routing protocols,
Computational modeling,
Operating systems,
Computer science,
Computer simulation,
Documentation,
Mobile ad hoc networks,
Logic testing,
Performance evaluation,
Control system synthesis"
Fully Bayesian spatio-temporal modeling of FMRI data,"We present a fully Bayesian approach to modeling in functional magnetic resonance imaging (FMRI), incorporating spatio-temporal noise modeling and haemodynamic response function (HRF) modeling. A fully Bayesian approach allows for the uncertainties in the noise and signal modeling to be incorporated together to provide full posterior distributions of the HRF parameters. The noise modeling is achieved via a nonseparable space-time vector autoregressive process. Previous FMRI noise models have either been purely temporal, separable or modeling deterministic trends. The specific form of the noise process is determined using model selection techniques. Notably, this results in the need for a spatially nonstationary and temporally stationary spatial component. Within the same full model, we also investigate the variation of the HRF in different areas of the activation, and for different experimental stimuli. We propose a novel HRF model made up of half-cosines, which allows distinct combinations of parameters to represent characteristics of interest. In addition, to adaptively avoid over-fitting we propose the use of automatic relevance determination priors to force certain parameters in the model to zero with high precision if there is no evidence to support them in the data. We apply the model to three datasets and observe matter-type dependence of the spatial and temporal noise, and a negative correlation between activation height and HRF time to main peak (although we suggest that this apparent correlation may be due to a number of different effects).",
A nonstationary Poisson view of Internet traffic,"Since the identification of long-range dependence in network traffic ten years ago, its consistent appearance across numerous measurement studies has largely discredited Poisson-based models. However, since that original data set was collected, both link speeds and the number of Internet-connected hosts have increased by more than three orders of magnitude. Thus, we now revisit the Poisson assumption, by studying a combination of historical traces and new measurements obtained from a major backbone link belonging to a Tier 1 ISP. We show that unlike the older data sets, current network traffic can be well represented by the Poisson model for sub-second time scales. At multisecond scales, we find a distinctive piecewise-linear nonstationarity, together with evidence of long-range dependence. Combining our observations across both time scales leads to a time-dependent Poisson characterization of network traffic that, when viewed across very long time scales, exhibits the observed long-range dependence. This traffic characterization reconciliates the seemingly contradicting observations of Poisson and long-memory traffic characteristics. It also seems to be in general agreement with recent theoretical models for large-scale traffic aggregation",
Differentiation and characterization of rat mammary fibroadenomas and 4T1 mouse carcinomas using quantitative ultrasound imaging,"Scatterer properties like the average effective scatterer diameter and acoustic concentration were determined in vivo using a quantitative ultrasound (QUS) technique from two tumor phenotypes grown in animal models. These tumor models included spontaneously occurring mammary fibroadenomas in rats and transplanted 4T1 mammary carcinomas in mice. The scatterer properties of average scatterer diameter and acoustic concentration were estimated using a Gaussian form factor from the backscattered ultrasound measured from both types of tumors. QUS images of the tumors were constructed utilizing estimated scatterer properties from regions in the tumors. The QUS images showed a clear distinction between the two types of tumors and a statistically significant difference existed between their estimated scatterer properties. The average scatterer diameter and acoustic concentration for the mammary fibroadenomas were estimated to be 105/spl plusmn/25 /spl mu/m and -15.6/spl plusmn/5 dB (mm/sup -3/), respectively. The average scatterer diameter and acoustic concentration for the carcinomas was estimated to be 28/spl plusmn/4.6 /spl mu/m and 10.6/spl plusmn/6.9 dB (mm/sup -3/), respectively. The distinctions in the scattering properties are clearly seen in the QUS images of the tumors and indicate that QUS imaging can be useful in differentiating between different types of mammary tumors.","Ultrasonic imaging,
Mice,
Acoustic scattering,
Neoplasms,
Acoustic imaging,
In vivo,
Animals,
Rats,
Acoustic measurements,
Ultrasonic variables measurement"
Design and implementation of low-profile contactless battery charger using planar printed circuit board windings as energy transfer device,"This paper paper presents the practical details involved in the design and implementation of a contactless battery charger that employs a pair of neighboring printed circuit board (PCB) windings as a contactless energy transfer device. A prototype contactless battery charger developed for application with cellular phones is used as an example to address the design considerations for the PCB windings and energy transfer circuit, plus demonstrates the performance of the contactless charger adapted to a practical application system.","Batteries,
Printed circuits,
Energy exchange,
Cellular phones,
Coupling circuits,
Contacts,
Prototypes,
Spirals,
Insulated gate bipolar transistors,
Computer science"
Optic nerve head segmentation,"Reliable and efficient optic disk localization and segmentation are important tasks in automated retinal screening. General-purpose edge detection algorithms often fail to segment the optic disk due to fuzzy boundaries, inconsistent image contrast or missing edge features. This paper presents an algorithm for the localization and segmentation of the optic nerve head boundary in low-resolution images (about 20 /spl mu//pixel). Optic disk localization is achieved using specialized template matching, and segmentation by a deformable contour model. The latter uses a global elliptical model and a local deformable model with variable edge-strength dependent stiffness. The algorithm is evaluated against a randomly selected database of 100 images from a diabetic screening programme. Ten images were classified as unusable; the others were of variable quality. The localization algorithm succeeded on all bar one usable image; the contour estimation algorithm was qualitatively assessed by an ophthalmologist as having Excellent-Fair performance in 83% of cases, and performs well even on blurred images.",
Context-aware support for computer-supported ubiquitous learning,"This paper describes a computer supported ubiquitous learning environment for language learning. This paper proposes two systems. The first is context-aware language-learning support system for Japanese polite expressions learning, which is called JAPELAS (Japanese polite expressions learning assisting system). This system provides learner the appropriate polite expressions deriving the learner's situation and personal information. The second system is called TANGO (Tag Added learNinG Objects) system, which detects the objects around learner using RFID tags, and provides the learner the educational information. This paper describes the preliminary evaluation of those two systems.",
Motion gradient vector flow: an external force for tracking rolling leukocytes with shape and size constrained active contours,"Recording rolling leukocyte velocities from intravital microscopic video imagery is a critical task in inflammation research and drug validation. Since manual tracking is excessively time consuming, an automated method is desired. This paper illustrates an active contour based automated tracking method, where we propose a novel external force to guide the active contour that takes the hemodynamic flow direction into account. The construction of the proposed force field, referred to as motion gradient vector flow (MGVF), is accomplished by minimizing an energy functional involving the motion direction, and the image gradient magnitude. The tracking experiments demonstrate that MGVF can be used to track both slow- and fast-rolling leukocytes, thus extending the capture range of previously designed cell tracking techniques.","Tracking,
White blood cells,
Shape,
Active contours,
Microscopy,
Animals,
In vivo,
Boundary conditions,
Drugs,
Hemodynamics"
Shared information and program plagiarism detection,"A fundamental question in information theory and in computer science is how to measure similarity or the amount of shared information between two sequences. We have proposed a metric, based on Kolmogorov complexity, to answer this question and have proven it to be universal. We apply this metric in measuring the amount of shared information between two computer programs, to enable plagiarism detection. We have designed and implemented a practical system SID (Software Integrity Diagnosis system) that approximates this metric by a heuristic compression algorithm. Experimental results demonstrate that SID has clear advantages over other plagiarism detection systems. SID system server is online at http://software.bioinformatics.uwaterloo.ca/SID/.",
A constrained variational principle for direct estimation and smoothing of the diffusion tensor field from complex DWI,"In this paper, we present a novel constrained variational principle for simultaneous smoothing and estimation of the diffusion tensor field from complex valued diffusion-weighted images (DWI). The constrained variational principle involves the minimization of a regularization term of L/sup p/ norms, subject to a nonlinear inequality constraint on the data. The data term we employ is the original Stejskal-Tanner equation instead of the linearized version usually employed in literature. The complex valued nonlinear form leads to a more accurate (when compared to the linearized version) estimate of the tensor field. The inequality constraint requires that the nonlinear least squares data term be bounded from above by a known tolerance factor. Finally, in order to accommodate the positive definite constraint on the diffusion tensor, it is expressed in terms of Cholesky factors and estimated. The constrained variational principle is solved using the augmented Lagrangian technique in conjunction with the limited memory quasi-Newton method. Experiments with complex-valued synthetic and real data are shown to depict the performance of our tensor field estimation and smoothing algorithm.","Smoothing methods,
Tensile stress,
Diffusion tensor imaging,
Magnetic resonance imaging,
Anisotropic magnetoresistance,
Information science,
Nonlinear equations,
Least squares methods,
Lagrangian functions,
Anatomy"
A study of the coverage of large-scale sensor networks,"We study the coverage properties of large-scale sensor networks. Three coverage measures are defined to characterize the fraction of the area covered by sensors (area coverage), the fraction of sensors that can be removed without reducing the covered area (node coverage), and the capability of the sensor network to detect objects moving in the network (detectability), respectively. We approach the coverage problem from a theoretical perspective and explore the fundamental limits of the coverage of a large-scale sensor network. We characterize the asymptotic behavior of the coverage measures for a variety of sensor network scenarios. We find that the coverage of a sensor network exhibits different behaviors for different network configuration and parameters. Based on the analytical characterizations of the network coverage, we further discuss the implications to network planning and protocol performance of sensor networks.","Large-scale systems,
Sensor phenomena and characterization,
Object detection,
Computer science,
Area measurement,
Protocols,
Monitoring,
Energy consumption,
Intrusion detection,
Sensor systems and applications"
RFID in robot-assisted indoor navigation for the visually impaired,We describe how radio frequency identification (RFID) can be used in robot-assisted indoor navigation for the visually impaired. We present a robotic guide for the visually impaired that was deployed and tested both with and without visually unpaired participants in two indoor environments. We describe how we modified the standard potential fields algorithms to achieve navigation at moderate walking speeds and to avoid oscillation in narrow spaces. The experiments illustrate that passive RFID tags deployed in the environment can act as reliable stimuli that trigger local navigation behaviors to achieve global navigation objectives.,"Radiofrequency identification,
Cognitive robotics,
Dogs,
Robot sensing systems,
Computer science,
Radio navigation,
Orbital robotics,
Testing,
Legged locomotion,
Passive RFID tags"
Nonnegative matrix factorization for rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain,"We present an algorithm for blindly recovering constituent source spectra from magnetic resonance (MR) chemical shift imaging (CSI) of the human brain. The algorithm, which we call constrained nonnegative matrix factorization (cNMF), does not enforce independence or sparsity, instead only requiring the source and mixing matrices to be nonnegative. It is based on the nonnegative matrix factorization (NMF) algorithm, extending it to include a constraint on the positivity of the amplitudes of the recovered spectra. This constraint enables recovery of physically meaningful spectra even in the presence of noise that causes a significant number of the observation amplitudes to be negative. We demonstrate and characterize the algorithm's performance using /sup 31/P volumetric brain data, comparing the results with two different blind source separation methods: Bayesian spectral decomposition (BSD) and nonnegative sparse coding (NNSC). We then incorporate the cNMF algorithm into a hierarchical decomposition framework, showing that it can be used to recover tissue-specific spectra given a processing hierarchy that proceeds coarse-to-fine. We demonstrate the hierarchical procedure on /sup 1/H brain data and conclude that the computational efficiency of the algorithm makes it well-suited for use in diagnostic work-up.",
Experimental fluorescence tomography of tissues with noncontact measurements,"Noncontact optical measurements from diffuse media could facilitate the use of large detector arrays at multiple angles that are well suited for diffuse optical tomography applications. Such imaging strategy could eliminate the need for individual fibers in contact with tissue, restricted geometries, and matching fluids. Thus, it could significantly improve experimental procedures and enhance our ability to visualize functional and molecular processes in vivo. In this paper, we describe the experimental implementation of this novel concept and demonstrate capacity to perform small animal imaging.","Fluorescence,
Tomography,
Biomedical optical imaging,
Optical imaging,
Optical scattering,
Biomedical imaging,
Molecular imaging,
Biomedical measurements,
Geometrical optics,
Optical arrays"
Atlas-based segmentation of pathological MR brain images using a model of lesion growth,"We propose a method for brain atlas deformation in the presence of large space-occupying tumors, based on an a priori model of lesion growth that assumes radial expansion of the lesion from its starting point. Our approach involves three steps. First, an affine registration brings the atlas and the patient into global correspondence. Then, the seeding of a synthetic tumor into the brain atlas provides a template for the lesion. The last step is the deformation of the seeded atlas, combining a method derived from optical flow principles and a model of lesion growth. Results show that a good registration is performed and that the method can be applied to automatic segmentation of structures and substructures in brains with gross deformation, with important medical applications in neurosurgery, radiosurgery, and radiotherapy.","Image segmentation,
Pathology,
Brain modeling,
Lesions,
Neoplasms,
Deformable models,
Biomedical optical imaging,
Image motion analysis,
Medical services,
Biomedical equipment"
Autonomous deployment and repair of a sensor network using an unmanned aerial vehicle,"We describe a sensor network deployment method using autonomous flying robots. Such networks are suitable for tasks such as large-scale environmental monitoring or for command and control in emergency situations. We describe in detail the algorithms used for deployment and for measuring network connectivity and provide experimental data we collected from field trials. A particular focus is on determining gaps in connectivity of the deployed network and generating a plan for a second, repair, pass to complete the connectivity. This project is the result of a collaboration between three robotics labs (CSIRO, USC, and Dartmouth.).","Unmanned aerial vehicles,
Helicopters,
Robot sensing systems,
Mobile robots,
Network topology,
Remotely operated vehicles,
Computer science,
Monitoring,
Ad hoc networks,
Aircraft navigation"
Ambient intelligence: a multimedia perspective,"Ambient intelligence opens up a world of unprecedented experiences. The interaction of people with electronic devices will change as context awareness, natural interfaces, and ubiquitous availability of information come to fruition. Ambient intelligence is going to impose major challenges on multimedia research. Distributed multimedia applications and their processing on embedded static and mobile platforms will play a major role in the development of ambient-intelligent environments. The requirements that ambient-intelligent multimedia applications impose on the mechanisms users apply to interact with media call for paradigms substantially different from contemporary interaction concepts. The complexity of media will continually increase in terms of volume and functionality, thus introducing a need for simplicity and ease of use. Therefore, the massively distributed, integrated use of media will require replacing well-known interaction vehicles, such as remote control and menu-driven search and control, with novel more intuitive, and natural concepts. This article reviews the concept of ambient intelligence and elaborates on its relation with multimedia. (The ""Advances in media processing"" sidebar gives insight into the developments that have set the stage for this new step forward.) The emphasis is on qualitative aspects, highlighting those elements that play a role in realizing ambient intelligence. Multimedia processing techniques and applications are key to realizing ambient intelligence, and they introduce major challenges to the design and implementation of both media-processing platforms and multimedia applications. Technology will not be the limiting factor in realizing ambient intelligence. The ingredients to let the computers disappear are already available, but the true success of the paradigm will depend on the ability to develop concepts that allow natural interaction with digital environments. We must build these digital environments with the invisible technology of the forthcoming century. The role of intelligent algorithms in this respect is apparent because it is the key enabling factor for realizing natural interaction.","Ambient intelligence,
Pervasive computing,
Ubiquitous computing,
Materials science and technology,
Computer vision,
Oxygen,
Productivity,
Displays,
Context-aware services,
Context"
Analysis of 3-D myocardial motion in tagged MR images using nonrigid image registration,"Tagged magnetic resonance imaging (MRI) is unique in its ability to noninvasively image the motion and deformation of the heart in vivo, but one of the fundamental reasons limiting its use in the clinical environment is the absence of automated tools to derive clinically useful information from tagged MR images. In this paper, we present a novel and fully automated technique based on nonrigid image registration using multilevel free-form deformations (MFFDs) for the analysis of myocardial motion using tagged MRI. The novel aspect of our technique is its integrated nature for tag localization and deformation field reconstruction using image registration and voxel based similarity measures. To extract the motion field within the myocardium during systole we register a sequence of images taken during systole to a set of reference images taken at end-diastole, maximizing the normalized mutual information between the images. We use both short-axis and long-axis images of the heart to estimate the full four-dimensional motion field within the myocardium. We also present validation results from data acquired from twelve volunteers.","Image analysis,
Image motion analysis,
Myocardium,
Motion analysis,
Image registration,
Magnetic resonance imaging,
Heart,
Magnetic analysis,
In vivo,
Image reconstruction"
Measurement of trabecular bone thickness in the limited resolution regime of in vivo MRI by fuzzy distance transform,"Trabecular or cancellous bone, the type of bone found in the vertebrae and near the joints of long bones, consists of a network of plates and struts. Accurate measurement of trabecular thickness is of significant interest, for example, to assess the effectiveness of anabolic (bone forming) agents of patients with osteoporosis. Here, we introduce a new fuzzy distance transform (FDT)-based thickness computation method that obviates binary segmentation and that can effectively deal with images acquired at a voxel size comparable to the typical trabecular bone thickness. The method's robustness is shown on the basis of /spl mu/-CT images of human trabecular bone, resampled at progressively coarser resolution and after application of rotation and addition of noise as a means to simulate the in vivo situation. Reproducibility of the method is demonstrated with /spl mu/-CT images by comparing histograms of thickness within and between data sets and with /spl mu/-MRI volume data sets of human volunteers imaged repeatedly. Finally, with in vivo /spl mu/-MR images from a prior study in rabbits subjected to corticosteroid exposure, it is demonstrated that short-term treatment resulting in trabecular thinning can be quantified with the new method.","Thickness measurement,
Cancellous bone,
In vivo,
Magnetic resonance imaging,
Humans,
Spine,
Joints,
Osteoporosis,
Image segmentation,
Noise robustness"
A hybrid heuristic for DAG scheduling on heterogeneous systems,"Summary form only given. This paper is motivated by the observation that different methods to compute the weights of nodes and edges when scheduling DAGs onto heterogeneous machines may lead to significant variations in the generated schedule. To minimize such variations, we present a novel heuristic for DAG scheduling, which is based upon solving a series of independent task scheduling problems. A novel heuristic for the latter problem is also included. Both heuristics compare favourably with other related heuristics.","Processor scheduling,
Computer science,
Costs,
Distributed processing"
Multiscale vessel tracking,A method is presented that uses a vectorial multiscale feature image for wave front propagation between two or more user defined points to retrieve the central axis of tubular objects in digital images. Its implicit scale selection mechanism makes the method more robust to overlap and to the presence of adjacent structures than conventional techniques that propagate a wave front over a scalar image representing the maximum of a range of filters. The method is shown to retain its potential to cope with severe stenoses or imaging artifacts and objects with varying widths in simulated and actual two-dimensional angiographic images.,"Filters,
Costs,
Image retrieval,
Image analysis,
Iterative methods,
Biomedical imaging,
Eigenvalues and eigenfunctions,
Digital images,
Robustness,
Medical simulation"
A categorical semantics of quantum protocols,"Particular focus in this paper is on quantum information protocols, which exploit quantum-mechanical effects in an essential way. The particular examples we shall use to illustrate our approach will be teleportation (Benett et al., 1993), logic-gate teleportation (Gottesman and Chuang,1999), and entanglement swapping (Zukowski et al., 1993). The ideas illustrated in these protocols form the basis for novel and potentially very important applications to secure and fault-tolerant communication and computation (2001,1999,2000).","Protocols,
Teleportation,
Performance evaluation,
Quantum computing,
Quantum entanglement,
Quantum mechanics,
Laboratories,
Communication channels,
Force measurement,
Measurement standards"
Improved possibilistic C-means clustering algorithms,"A possibilistic approach was proposed in a previous paper for C-means clustering, and two algorithms realizing this approach were reported in two previous papers. Although the possibilistic approach is sound, these two algorithms tend to find identical clusters. In this paper, we modify and improve these algorithms to overcome their shortcoming. The numerical results demonstrate that the improved algorithms can determine proper clusters and they can realize the advantages of the possibilistic approach.","Clustering algorithms,
Partitioning algorithms,
Robustness,
Acoustic noise,
Research and development,
Information science,
Computer science,
Prototypes"
Fully automatic luminal contour segmentation in intracoronary ultrasound imaging-a statistical approach,"In this paper, a fully automatic method for luminal contour segmentation in intracoronary ultrasound imaging is introduced. Its principle is based on a contour with a priori properties that evolves according to the statistics of the ultrasound texture brightness, which is generally Rayleigh distributed. The main interest of the technique is its fully automatic character. This is insured by an initial contour that is not set by the user, like in classical snake-based algorithms, but estimated and, thus, adapted to each image. Its estimation combines two pieces of information extracted from the a posteriori probability function of the contour position: the function maximum location (or maximum a posteriori estimator) and the first zero-crossing of its derivative. Then, starting from the initial contour, a region of interest is automatically selected and the process iterated until the contour evolution can be ignored. In vivo coronary images from 15 patients, acquired with the 20-MHz central frequency Jomed Invision ultrasound scanner, were segmented with the developed method. Automatic contours were compared to those manually drawn by two physicians in terms of mean absolute difference. The results demonstrate that the error between automatic contours and the average of manual ones is of small amplitude, and only very slightly higher (0.099/spl plusmn/0.032 mm) than the interexpert error (0.097/spl plusmn/0.027 mm).","Image segmentation,
Ultrasonic imaging,
Arteries,
Biomedical imaging,
Statistical distributions,
Area measurement,
Biomedical measurements,
Ultrasonic variables measurement,
Biomedical engineering,
Thorax"
Quantitative weights and aggregation,"Based on the strong idempotency of aggregation operators, quantitative weights are incorporated into the fusion process. Our general method is compared with some previous specific cases. More details about weighted aggregation based on some penalty function is given. Further, weighted integral based aggregation linked to quantifier-based fuzzy measures is investigated, especially weighted OWA operators. Several examples are included.","Decision making,
Frequency,
Fuzzy systems,
Open wireless architecture,
Knowledge based systems,
Fuzzy control,
Costs,
Computer science,
Mathematics,
Information theory"
Visual tracking using learned linear subspaces,"This paper presents a simple but robust visual tracking algorithm based on representing the appearances of objects using affine warps of learned linear subspaces of the image space. The tracker adaptively updates this subspace while tracking by finding a linear subspace that best approximates the observations made in the previous frames. Instead of the traditional L/sup 2/-reconstruction error norm which leads to subspace estimation using PCA or SVD, we argue that a variant of it, the uniform L/sup 2/-reconstruction error norm, is the right one for tracking. Under this framework we provide a simple and a computationally inexpensive algorithm for finding a subspace whose uniform L/sup 2/-reconstruction error norm for a given collection of data samples is below some threshold, and a simple tracking algorithm is an immediate consequence. We show experimental results on a variety of image sequences of people and man-made objects moving under challenging imaging conditions, which include drastic illumination variation, partial occlusion and extreme pose variation.","Target tracking,
Robustness,
Principal component analysis,
Computer science,
Image sequences,
Lighting,
Algorithm design and analysis,
Image reconstruction,
Shape,
Reflectivity"
Robust interfaces for mixed-timing systems,"This paper presents several low-latency mixed-timing FIFO (first-in-first-out) interfaces designs that interface systems on a chip working at different speeds. The connected systems can be either synchronous or asynchronous. The designs are then adapted to work between systems with very long interconnect delays, by migrating a single-clock solution by Carloni et al. (1999, 2000, and 2001) (for ""latency-insensitive"" protocols) to mixed-timing domains. The new designs can be made arbitrarily robust with regard to metastability and interface operating speeds. Initial simulations for both latency and throughput are promising.","Robustness,
Timing,
Delay,
Very large scale integration,
Clocks,
Metastasis,
Integrated circuit interconnections,
Computer science,
Frequency synchronization,
Protocols"
Shot clustering techniques for story browsing,"Automatic video segmentation is the first and necessary step for organizing a long video file into several smaller units. The smallest basic unit is a shot. Relevant shots are typically grouped into a high-level unit called a scene. Each scene is part of a story. Browsing these scenes unfolds the entire story of a film, enabling users to locate their desired video segments quickly and efficiently. Existing scene definitions are rather broad, making it difficult to compare the performance of existing techniques and to develop a better one. This paper introduces a stricter scene definition for narrative films and presents ShotWeave, a novel technique for clustering relevant shots into a scene using the stricter definition. The crux of ShotWeave is its feature extraction and comparison. Visual features are extracted from selected regions of representative frames of shots. These regions capture essential information needed to maintain viewers' thought in the presence of shot breaks. The new feature comparison is developed based on common continuity-editing techniques used in film making. Experiments were performed on full-length films with a wide range of camera motions and a complex composition of shots. The experimental results show that ShotWeave outperforms two recent techniques utilizing global visual features in terms of segmentation accuracy and time.","Layout,
Feature extraction,
Cameras,
Content based retrieval,
Computer science,
Organizing,
Data mining,
Indexing,
Internetworking,
Computer aided instruction"
An evolutionary algorithm for large traveling salesman problems,"This work proposes an evolutionary algorithm, called the heterogeneous selection evolutionary algorithm (HeSEA), for solving large traveling salesman problems (TSP). The strengths and limitations of numerous well-known genetic operators are first analyzed, along with local search methods for TSPs from their solution qualities and mechanisms for preserving and adding edges. Based on this analysis, a new approach, HeSEA is proposed which integrates edge assembly crossover (EAX) and Lin-Kernighan (LK) local search, through family competition and heterogeneous pairing selection. This study demonstrates experimentally that EAX and LK can compensate for each other's disadvantages. Family competition and heterogeneous pairing selections are used to maintain the diversity of the population, which is especially useful for evolutionary algorithms in solving large TSPs. The proposed method was evaluated on 16 well-known TSPs in which the numbers of cities range from 318 to 13 509. Experimental results indicate that HeSEA performs well and is very competitive with other approaches. The proposed method can determine the optimum path when the number of cities is under 10 000 and the mean solution quality is within 0.0074% above the optimum for each test problem. These findings imply that the proposed method can find tours robustly with a fixed small population and a limited family competition length in reasonable time, when used to solve large TSPs.","Evolutionary computation,
Traveling salesman problems,
Cities and towns,
Bioinformatics,
Genetics,
Search methods,
Assembly,
Computer science,
Optimization methods,
Testing"
Application of anomaly detection algorithms for detecting SYN flooding attacks,"We investigate statistical anomaly detection algorithms for detecting SYN flooding, which is the most common type of denial of service (DoS) attack. The two algorithms considered are an adaptive threshold algorithm and a particular application of the cumulative sum (CUSUM) algorithm for change point detection. The performance is investigated in terms of the detection probability, the false alarm ratio, and the detection delay. Particular emphasis is on investigating the tradeoffs among these metrics and how they are affected by the parameters of the algorithm and the characteristics of the attacks. Such an investigation can provide guidelines to effectively tune the parameters of the detection algorithm to achieve specific performance requirements in terms of the above metrics.","Detection algorithms,
Floods,
Computer crime,
Change detection algorithms,
Computer science,
Delay,
TCPIP,
Application software,
Guidelines,
Web and internet services"
Prediction-based strategies for energy saving in object tracking sensor networks,"In order to fully realize the potential of sensor networks, energy awareness should be incorporated into every stage of the network design and operation. In this paper, we address the energy management issue in a sensor network killer application - object tracking sensor networks (OTSNs). Based on the fact that the movements of the tracked objects are sometimes predictable, we propose a prediction-based energy saving scheme, called PES, to reduce the energy consumption for object tracking under acceptable conditions. We compare PES against the basic schemes we proposed in the paper to explore the conditions under which PES is most desired. We also test the effect of some parameters related to the system workload, object moving behavior and sensing operations on PES through extensive simulation. Our results show that PES can save significant energy under various conditions.","Intelligent networks,
Energy consumption,
Energy management,
Tracking,
Wireless sensor networks,
Cost function,
Computer science,
Design engineering,
Power engineering and energy,
Electronic mail"
Precise service level agreements,"SLAng is an XML language for defining service level agreements, the part of a contract between the client and provider of an Internet service that describes the quality attributes that the service is required to possess. We define the semantics of SLAng precisely by modelling the syntax of the language in UML, then relating the language model to a model that describes the structure and behaviour of services. The presence of SLAng elements imposes behavioural constraints on service elements, and the precise definition of these constraints using OCL constitutes the semantic description of the language. We use the semantics to define a notion of SLA compatibility, and an extension to UML that enables the modelling of service situations as a precursor to analysis, implementation and provisioning activities.","Unified modeling language,
Web and internet services,
Contracts,
Web services,
Application specific processors,
Computer science,
Educational institutions,
XML,
Mission critical systems,
Supply chain management"
Making one object look like another: controlling appearance using a projector-camera system,"We present a method for controlling the appearance of an arbitrary 3D object using a projector and a camera. Our goal is to make one object look like another by projecting a carefully determined compensation image onto the object. The determination of the appropriate compensation image requires accounting for spatial variation in the object's reflectance, the effects of environmental lighting, and the spectral responses, spatially varying fall-offs, and non-linear responses in the projector-camera system. Addressing each of these effects, we present a compensation method which calls for the estimation of only a small number of parameters, as part of a novel off-line radiometric calibration. This calibration is accomplished by projecting and acquiring a minimal set of 6 images, irrespective of the object. Results of the calibration are then used on-line to compensate each input image prior to projection. Several experimental results are shown that demonstrate the ability of this method to control the appearance of everyday objects. Our method has direct applications in several areas including smart environments, product design and presentation, adaptive camouflages, interactive education and entertainment.","Control systems,
Cameras,
Calibration,
Reflectivity,
Computer science,
Radiometry,
Lighting,
Computer displays,
Color,
Product design"
Anatomical-based FDG-PET reconstruction for the detection of hypo-metabolic regions in epilepsy,"Positron emission tomography (PET) of the cerebral glucose metabolism has shown to be useful in the presurgical evaluation of patients with epilepsy. Between seizures, PET images using fluorodeoxyglucose (FDG) show a decreased glucose metabolism in areas of the gray matter (GM) tissue that are associated with the epileptogenic region. However, detection of subtle hypo-metabolic regions is limited by noise in the projection data and the relatively small thickness of the GM tissue compared to the spatial resolution of the PET system. Therefore, we present an iterative maximum-a-posteriori based reconstruction algorithm, dedicated to the detection of hypo-metabolic regions in FDG-PET images of the brain of epilepsy patients. Anatomical information, derived from magnetic resonance imaging data, and pathophysiological knowledge was included in the reconstruction algorithm. Two Monte Carlo based brain software phantom experiments were used to examine the performance of the algorithm. In the first experiment, we used perfect, and in the second, imperfect anatomical knowledge during the reconstruction process. In both experiments, we measured signal-to-noise ratio (SNR), root mean squared (rms) bias and rms standard deviation. For both experiments, bias was reduced at matched noise levels, when compared to post-smoothed maximum-likelihood expectation-maximization (ML-EM) and maximum a posteriori reconstruction without anatomical priors. The SNR was similar to that of ML-EM with optimal post-smoothing, although the parameters of the prior distributions were not optimized. We can conclude that the use of anatomical information combined with prior information about the underlying pathology is very promising for the detection of subtle hypo-metabolic regions in the brain of patients with epilepsy.","Epilepsy,
Positron emission tomography,
Image reconstruction,
Signal to noise ratio,
Sugar,
Biochemistry,
Reconstruction algorithms,
Spatial resolution,
Magnetic resonance imaging,
Monte Carlo methods"
Robust real-time myocardial border tracking for echocardiography: an information fusion approach,"Ultrasound is a main noninvasive modality for the assessment of the heart function. Wall tracking from ultrasound data is, however, inherently difficult due to weak echoes, clutter, poor signal-to-noise ratio, and signal dropouts. To cope with these artifacts, pretrained shape models can be applied to constrain the tracking. However, existing methods for incorporating subspace shape constraints in myocardial border tracking use only partial information from the model distribution, and do not exploit spatially varying uncertainties from feature tracking. In this paper, we propose a complete fusion formulation in the information space for robust shape tracking, optimally resolving uncertainties from the system dynamics, heteroscedastic measurement noise, and subspace shape model. We also exploit information from the ground truth initialization where this is available. The new framework is applied for tracking of myocardial borders in very noisy echocardiography sequences. Numerous myocardium tracking experiments validate the theory and show the potential of very accurate wall motion measurements. The proposed framework outperforms the traditional shape-space-constrained tracking algorithm by a significant margin. Due to the optimal fusion of different sources of uncertainties, robust performance is observed even for the most challenging cases.","Myocardium,
Echocardiography,
Shape measurement,
Ultrasonic imaging,
Noise shaping,
Heart,
Signal to noise ratio,
Subspace constraints,
Noise robustness,
Spatial resolution"
Exploiting the transients of adaptation for RoQ attacks on Internet resources,"We expose an unorthodox adversarial attack that exploits the transients of a system's adaptive behavior, as opposed to its limited steady-state capacity. We show that a well orchestrated attack could introduce significant inefficiencies that could potentially deprive a network element from much of its capacity, or significantly reduce its service quality, while evading detection by consuming an unsuspicious, small fraction of that element's hijacked capacity. This type of attack stands in sharp contrast to traditional brute-force, sustained high-rate DoS attacks, as well as recently proposed attacks that exploit specific protocol settings such as TCP timeouts. We exemplify what we term as reduction of quality (RoQ) attacks by exposing the vulnerabilities of common adaptation mechanisms. We develop control-theoretic models and associated metrics to quantify these vulnerabilities. We present numerical and simulation results, which we validate with observations from real Internet experiments. Our findings motivate the need for the development of adaptation mechanisms that are resilient to these new forms of attacks.","Computer crime,
Web and internet services,
Steady-state,
Protocols,
Computer science,
Adaptive systems,
Numerical simulation,
Computer crashes,
Costs,
Counting circuits"
Estimation of displacement vectors and strain tensors in elastography using angular insonifications,"In current practice, only one out of three components of the tissue displacement vector and one of nine components of the strain tensor are accurately estimated and imaged in ultrasound elastography. Since, only the axial component of both the displacement and strain are imaged, other important elastic parameters, such as shear strains and the Poisson's ratio, also are not imaged. Moreover, reconstruction of the Young's modulus would be significantly improved if all components of the strain tensor were available. In this paper, we describe a new method for estimating all the components of the tissue displacement vector following a quasi-static compression. The method uses displacements estimated from radiofrequency echo-signals along multiple ultrasound beam insonification directions. At each spatial location in the compressed medium, orthogonal tissue displacements in both the axial and lateral direction with respect to the direction of the applied compression are estimated by curve fitting angular displacement vector data calculated for all insonification directions. Following displacement estimation in orthogonal directions, components of the corresponding normal and shear strain tensors are estimated. Simulation and experimental results demonstrate the utility of this technique for the computation of the normal and shear strain tensors.","Capacitive sensors,
Tensile stress,
Ultrasonic imaging,
Elasticity,
Biomedical imaging,
Medical diagnostic imaging,
Physics,
Image reconstruction,
Image coding,
Acoustic beams"
Adaptive replication in peer-to-peer systems,"Peer-to-peer systems can be used to form a low-latency decentralized data delivery system. Structured peer-to-peer systems provide both low latency and excellent load balance with uniform query and data distributions. Under the more common skewed access distributions, however, individual nodes are easily overloaded, resulting in poor global performance and lost messages. This paper describes a lightweight, adaptive, and system-neutral replication protocol, called LAR, that maintains low access latencies and good load balance even under highly skewed demand. We apply LAR to Chord and show that it has lower overhead and better performance than existing replication strategies.","Peer to peer computing,
Delay,
Routing,
Adaptive systems,
Access protocols,
Streaming media,
Workstations,
Topology,
Computer science,
Educational institutions"
A utilization bound for aperiodic tasks and priority driven scheduling,"Real-time scheduling theory offers constant-time schedulability tests for periodic and sporadic tasks based on utilization bounds. Unfortunately, the periodicity or the minimal interarrival-time assumptions underlying these bounds make them inapplicable to a vast range of aperiodic workloads such as those seen by network routers, Web servers, and event-driven systems. This paper makes several important contributions toward real-time scheduling theory and schedulability analysis. We derive the first known bound for schedulability of aperiodic tasks. The bound is based on a utilization-like metric we call synthetic utilization, which allows implementing constant-time schedulability tests at admission control time. We prove that the synthetic utilization bound for deadline-monotonic scheduling of aperiodic tasks is 1/1+/spl radic/1/2. We also show that no other time-independent scheduling policy can have a higher schedulability bound. Similarly, we show that EDF has a bound of 1 and that no dynamic-priority policy has a higher bound. We assess the performance of the derived bound and conclude that it is very efficient in hit-ratio maximization.","Processor scheduling,
Real time systems"
A novel multiplexer-based low-power full adder,"The 1-bit full adder circuit is a very important component in the design of application specific integrated circuits. This paper presents a novel low-power multiplexer-based 1-bit full adder that uses 12 transistors (MBA-12T). In addition to reduced transition activity and charge recycling capability, this circuit has no direct connections to the power-supply nodes, leading to a noticeable reduction in short-current power consumption. Intensive HSPICE simulation shows that the new adder has more than 26% in power savings over conventional 28-transistor CMOS adder and it consumes 23% less power than 10-transistor adders (SERF and 10T ) and is 64% faster.","Adders,
CMOS logic circuits,
Multiplexing,
Circuit simulation,
Testing,
Computer science,
Application specific integrated circuits,
Recycling,
Energy consumption,
Very large scale integration"
Plane waves in FDTD simulations and a nearly perfect total-field/scattered-field boundary,"The total-field/scattered-field (TFSF) boundary has been successfully used for a number of years to introduce energy into finite-difference time-domain (FDTD) grids. If the propagation of the incident field is grid-aligned, a perfect TFSF implementation can be realized by using an auxiliary one-dimensional FDTD simulation which models propagation of the incident field. Here ""perfect"" implies the incident field propagation exactly matches the way in which the field propagates in the FDTD grid. However, for propagation which is not grid-aligned, no similarly perfect implementation has previously been presented. This work provides a framework for a perfect TFSF boundary for pulsed plane waves which do not propagate in a grid-aligned fashion. To achieve this, homogeneous plane-wave propagation is rigorously quantified. Using this knowledge and a specification of the desired incident field, the dispersion relation is used to ascertain the incident field at any point in the grid. It is required to account for, unlike in the continuous world, the electric field, the magnetic field, and the wavenumber vector not forming a mutually orthogonal set. Group velocity is also considered because of its relevance to the implementation.","Finite difference methods,
Time domain analysis,
Scattering,
Dispersion,
Magnetic fields,
Equations,
Perfectly matched layers,
Nonhomogeneous media,
Computer science"
Tooth segmentation of dental study models using range images,"The accurate segmentation of the teeth from the digitized representation of a dental study model is an important component in computer-based algorithms for orthodontic feature detection and measurement and in the simulation of orthodontic procedures such as tooth rearrangement. This paper presents an automated method for tooth segmentation from the three-dimensional (3-D) digitized image captured by a laser scanner. We avoid the complexity of directly processing 3-D mesh data by proposing the innovative idea of detecting features on two range images computed from the 3-D image. The dental arch is first obtained from the plan-view range image. Using the arch as the reference, a panoramic range image of the dental model can be computed. The interstices between the teeth are detected separately in the two range images, and results from both views are combined for a determination of interstice locations and orientations. Finally, the teeth are separated from the gums by delineating the gum margin. The algorithm was tested on 34 dental models representing a variety of malocclusions and was found to be robust and accurate.","Teeth,
Image segmentation,
Dentistry,
Application software,
Computer aided manufacturing,
Computer vision,
Computational modeling,
Design automation,
Image restoration,
Kelvin"
Feature selection in MLPs and SVMs based on maximum output information,"This paper presents feature selection algorithms for multilayer perceptrons (MLPs) and multiclass support vector machines (SVMs), using mutual information between class labels and classifier outputs, as an objective function. This objective function involves inexpensive computation of information measures only on discrete variables; provides immunity to prior class probabilities; and brackets the probability of error of the classifier. The maximum output information (MOI) algorithms employ this function for feature subset selection by greedy elimination and directed search. The output of the MOI algorithms is a feature subset of user-defined size and an associated trained classifier (MLP/SVM). These algorithms compare favorably with a number of other methods in terms of performance on various artificial and real-world data sets.",
An intrusion detection tool for AODV-based ad hoc wireless networks,"Mobile ad hoc network routing protocols are highly susceptible to subversion. Previous research in securing these protocols has typically used techniques based on encryption and redundant transmission. These techniques prevent a range of attacks against routing protocols but are expensive to deploy on energy-constrained wireless devices. Experience in securing wired networks has demonstrated that, in addition to intrusion prevention techniques, it is useful to deploy intrusion detection techniques as a second line of defense. In this paper, we discuss some of the threats to wireless ad hoc networks, and, specifically, some attacks against the AODV routing protocol. We also present a tool aimed at real-time detection of these attacks. The tool monitors network packets to detect local and distributed attacks within its radio range. Experiments show that the tool provides effective intrusion detection functionality while using only a limited amount of resources.","Intrusion detection,
Wireless networks,
Mobile ad hoc networks,
Ad hoc networks,
Routing protocols,
Computational modeling,
Computer science,
Cryptography,
Communication system security,
Network topology"
MAPGEN: mixed-initiative planning and scheduling for the Mars Exploration Rover mission,"The Mars Exploration Rover mission is one of NASA's most ambitious science missions to date. Launched in the summer of 2003, each rover carries instruments for conducting remote and in site observations to elucidate the planet's past climate, water activity, and habitability. Science is MER's primary driver, so making best use of the scientific instruments, within the available resources, is a crucial aspect of the mission. To address this criticality, the MER project team selected MAPGEN (mixed initiative activity plan generator) as an activity-planning tool. MAPGEN combines two existing systems, each with a strong heritage: the APGEN activity-planning tool from the Jet Propulsion Laboratory and the Europa planning and scheduling system from NASA Ames Research Center. We discuss the issues arising from combining these tools in this mission's context. MAPGEN is the first AI-based system to control a space platform on another planet's surface.","Mars,
Dictionaries,
Propulsion,
Graphical user interfaces,
NASA,
Laboratories,
Planets,
Artificial intelligence,
Technology planning,
Intelligent systems"
Three-dimensional model based face recognition,"The performance of face recognition systems that use two-dimensional (2D) images is dependent on consistent conditions such as lighting, pose and facial expression. We are developing a multi-view face recognition system that utilizes three-dimensional (3D) information about the face to make the system more robust to these variations. This work describes a procedure for constructing a database of 3D face models and matching this database to 2.5D face scans which are captured from different views, using coordinate system invariant properties of the facial surface. 2.5D is a simplified 3D (x, y, z) surface representation that contains at most one depth value (z direction) for every point in the (x, y) plane. A robust similarity metric is defined for matching, based on an iterative closest point (ICP) registration process. Results are given for matching a database of 18 3D face models with 113 2.5D face scans.","Face recognition,
Face detection,
Robustness,
Image databases,
Humans,
Image sensors,
Shape,
Testing,
Computer science,
Contracts"
Shape from moments - an estimation theory perspective,"This paper discusses the problem of recovering a planar polygon from its measured complex moments. These moments correspond to an indicator function defined over the polygon's support. Previous work on this problem gave necessary and sufficient conditions for such successful recovery process and focused mainly on the case of exact measurements being given. In this paper, we extend these results and treat the same problem in the case where a longer than necessary series of noise corrupted moments is given. Similar to methods found in array processing, system identification, and signal processing, we discuss a set of possible estimation procedures that are based on the Prony and the Pencil methods, relate them one to the other, and compare them through simulations. We then present an improvement over these methods based on the direct use of the maximum-likelihood estimator, exploiting the above methods as initialization. Finally, we show how regularization and, thus, maximum a posteriori probability estimator could be applied to reflect prior knowledge about the recovered polygon.","Shape,
Estimation theory,
Array signal processing,
Noise shaping,
Sufficient conditions,
Inverse problems,
Computer science,
Direction of arrival estimation,
System identification,
Eigenvalues and eigenfunctions"
On real-time capacity limits of multihop wireless sensor networks,"Multihop wireless sensor networks have recently emerged as an important embedded computing platform. This paper defines a quantitative notion of real-time capacity of a wireless network. Real-time capacity describes how much real-time data the network can transfer by their deadlines. A capacity bound is derived that can be used as a sufficient schedulability condition for a class of fixed-priority packet scheduling algorithms. Using this bound, a designer can perform capacity planning prior to network deployment to ensure satisfaction of applications' real-time requirements.","Spread spectrum communication,
Wireless sensor networks,
Throughput,
Wireless networks,
Scheduling algorithm,
Capacity planning,
Computer science,
Embedded computing,
Processor scheduling,
Data communication"
Interactive volume segmentation with differential image foresting transforms,"The absence of object information very often asks for considerable human assistance in medical image segmentation. Many interactive two-dimensional and three-dimensional (3-D) segmentation methods have been proposed, but their response time to user's actions should be considerably reduced to make them viable from the practical point of view. We circumvent this problem in the framework of the image foresting transform (IFT)-a general tool for the design of image operators based on connectivity-by introducing a new algorithm (DIFT) to compute sequences of IFTs in a differential way. We instantiate the DIFT algorithm for watershed-based and fuzzy-connected segmentations under two paradigms (single-object and multiple-object) and evaluate the efficiency gains of both approaches with respect to their linear-time implementation based on the nondifferential IFT. We show that the DIFT algorithm provides efficiency gains from 10 to 17, reducing the user's waiting time for segmentation with 3-D visualization on a common PC from 19-36 s to 2-3 s. We also show that the multiple-object approach is more efficient than the single-object paradigm for both segmentation methods.","Image segmentation,
Layout,
Biomedical imaging,
Visualization,
Humans,
Image sequence analysis,
Delay,
Algorithm design and analysis,
Computer applications,
Image processing"
Towards a theory of context spaces,"We propose initial work on a conceptual framework for context-aware systems, towards a general context model to aid thinking, describing, manipulating and utilizing context. Much work on context-aware computing have utilized context in specialized applications, with application-specific software (and perhaps hardware) and specialized context representation and manipulation. Context-aware systems are becoming increasingly important, and emerging research has begun to look at context-aware systems more generally, independently of specific applications, including context middleware and toolkits and ontologies for describing context.","Visualization,
Context modeling,
Application software,
Ontologies,
Software engineering,
Computer science,
Australia,
Context-aware services,
Hardware,
Middleware"
The /spl phi/ accrual failure detector,"The detection of failures is a fundamental issue for fault-tolerance in distributed systems. Recently, many people have come to realize that failure detection ought to be provided as some form of generic service, similar to IP address lookup or time synchronization. However, this has not been successful so far; one of the reasons being the fact that classical failure detectors were not designed to satisfy several application requirements simultaneously. We present a novel abstraction, called accrual failure detectors, that emphasizes flexibility and expressiveness and can serve as a basic building block to implementing failure detectors in distributed systems. Instead of providing information of a binary nature (trust vs. suspect), accrual failure detectors output a suspicion level on a continuous scale. The principal merit of this approach is that it favors a nearly complete decoupling between application requirements and the monitoring of the environment. In this paper, we describe an implementation of such an accrual failure detector, that we call the /spl phi/ failure detector. The particularity of the /spl phi/ failure detector is that it dynamically adjusts to current network conditions the scale on which the suspicion level is expressed. We analyzed the behavior of our /spl phi/ failure detector over an intercontinental communication link over a week. Our experimental results show that if performs equally well as other known adaptive failure detection mechanisms, with an improved flexibility.","Detectors,
Computer crashes,
Condition monitoring,
Fault detection,
Quality of service,
Information science,
Failure analysis,
Fault tolerant systems,
H infinity control,
Event detection"
Towards a shape model of white matter fiber bundles using diffusion tensor MRI,"White matter fiber bundles of the human brain form a spatial pattern defined by the anatomical and functional architecture. Human brain atlases provide names for individual tracts and document that these patterns are comparable across subjects. Tractography applied to the tensor field in diffusion tensor imaging (DTI) results in sets of streamlines which can be associated with major fiber tracts. Comparison of fiber tract properties across subjects requires comparison at corresponding anatomical locations. As an alternative to linear and nonlinear registration of DTI images and voxel-based analysis, we propose a novel methodology that models the shape of white matter tracts. A clustering uses similarity of adjacent curves and an iterative processing scheme to group sets of curves to bundles and to reject outliers. Unlike previous work which models fiber tracts as sets of curves centered around a spine, we extend the notion of bundling towards a more general representation of manifolds. We describe tracts, represented as sets of curves of similar shape, by a shape prototype swept along a space trajectory. This approach can naturally describe white matter structures observed either as bundles dispersing towards the cortex or tracts defined as dense patterns of parallel fibers forming manifolds. Curves are parameterized by arc-length and represented by intrinsic local shape properties (curvature and torsion). Feasibility is demonstrated by modeling the left and right cortico-spinal tracts and a part of the transversal callosal tract.","Shape,
Tensile stress,
Magnetic resonance imaging,
Diffusion tensor imaging,
Prototypes,
Humans,
Image analysis,
Computer science,
Psychiatry,
Computer architecture"
"Computational modeling in biohydrodynamics: trends, challenges, and recent advances","Computational modeling is assuming increased significance in the area of biohydrodynamics. This trend has been enabled primarily by the widespread availability of powerful computers, as well as the induction of novel numerical and modeling approaches. However, despite these recent advances, computational modeling of flows in complex biohydrodynamic configurations remains a challenging proposition. This is due to a multitude of factors, including the need to handle a wide range of flow conditions (laminar, transitional, and turbulent), the ubiquity of two-way coupled interaction between the fluid and moving/deformable structures, and, finally, the requirement of accurately resolving unsteady flow features. Recently, as part of an Office of Naval Research sponsored review, the objective of which was to distill the science related to biology-based hydrodynamics for maneuvering and propulsion, an extensive survey of computational biohydrodynamics was undertaken. The key findings of this survey are reported in this paper.","Computational modeling,
Testing,
Marine animals,
Sea measurements,
Propulsion,
Motion control,
Instruments,
Fluid flow measurement,
Particle measurements,
Numerical models"
Point-based probabilistic surfaces to show surface uncertainty,"Efficient and informative visualization of surfaces with uncertainties is an important topic with many applications in science and engineering. In these applications, the correct course of action may depend not only on the location of a boundary, but on the precision with which that location is known. Examples include environmental pollution borderline detection, oil basin edge characterization, or discrimination between cancerous and healthy tissue in medicine. We present a method for producing visualizations of surfaces with uncertainties using points as display primitives. Our approach is to render the surface as a collection of points and to displace each point from its original location along the surface normal by an amount proportional to the uncertainty at that point. This approach can be used in combination with other techniques such as pseudocoloring to produce efficient and revealing visualizations. The basic approach is sufficiently flexible to allow natural extensions; we show incorporation of expressive modulation of opacity, change of the stroke primitive, and addition of an underlying polygonal model. The method is used to visualize real and simulated tumor formations with uncertainty of tumor boundaries. The point-based technique is compared to pseudocoloring for a position estimation task in a preliminary user study.","Uncertainty,
Surface contamination,
Neoplasms,
Data visualization,
Application software,
Displays,
Pollution measurement,
Crystallography,
Computer Society,
Oil pollution"
An event stream driven approximation for the analysis of real-time systems,"This paper presents a new approach to understand the event stream model. Additionally a new approximation algorithm for the feasibility test of the sporadic and the generalized multiframe task system scheduled by earliest deadline first is presented. The new algorithm has polynomial complexity to solve the problem of schedulability analysis. The approximation error of the algorithm is bounded. In contrary to earlier work, where the error depends on the different deadlines of the tasks, the error of our algorithm depends only on the capacity of the chosen processor. It guarantees the acceptances on a processor with slightly higher capacity than the unknown optimal processor. While the algorithm is scalable and the run-time depends on the chosen error, a trade-off between running time and error is possible.","Real time systems,
System testing,
Scheduling algorithm,
Processor scheduling,
Approximation algorithms,
Polynomials,
Computer science,
Algorithm design and analysis,
Approximation error,
Runtime"
Validation and verification of simulation models,"In this paper we discuss validation and verification of simulation models. Four different approaches to deciding model validity are described; two different paradigms that relate validation and verification to the model development process are presented; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are discussed; a way to document results is given; a recommended procedure for model validation is presented; and accreditation is briefly discussed.","Computational modeling,
Performance evaluation,
Computer simulation,
Educational institutions,
Data engineering,
Computer science,
Accreditation,
Testing,
Large-scale systems,
Costs"
Authenticating query results in edge computing,"Edge computing pushes application logic and the underlying data to the edge of the network, with the aim of improving availability and scalability. As the edge servers are not necessarily secure, there must be provisions for validating their outputs. This paper proposes a mechanism that creates a verification object (VO) for checking the integrity of each query result produced by an edge server - that values in the result tuples are not tampered with, and that no spurious tuples are introduced. The primary advantages of our proposed mechanism are that the VO is independent of the database size, and that relational operations can still be fulfilled by the edge servers. These advantages reduce transmission load and processing at the clients. We also show how insert and delete transactions can be supported.","Network servers,
Logic,
Relational databases,
Computer networks,
Availability,
Scalability,
Web services,
Intelligent networks,
Computer science,
Drives"
Slurpie: a cooperative bulk data transfer protocol,"We present Slurpie: a peer-to-peer protocol for bulk data transfer. Slurpie is specifically designed to reduce client download times for large, popular files, and to reduce load on servers that serve these files. Slurpie employs a novel adaptive downloading strategy to increase client performance, and employs a randomized backoff strategy to precisely control load on the server. We describe a full implementation of the Slurpie protocol, and present results from both controlled local-area and wide-area testbeds. Our results show that Slurpie clients improve performance as the size of the network increases, and the server is completely insulated from large flash crowds entering the Slurpie network.","Protocols,
Network servers,
Peer to peer computing,
File servers,
Bandwidth,
Web server,
Computer science,
Educational institutions,
Testing,
Insulation"
Maximum matchings via Gaussian elimination,"We present randomized algorithms for finding maximum matchings in general and bipartite graphs. Both algorithms have running time O(n/sup w/), where w is the exponent of the best known matrix multiplication algorithm. Since w < 2.38, these algorithms break through the O(n/sup 2.5/) barrier for the matching problem. They both have a very simple implementation in time O(n/sup 3/) and the only non-trivial element of the O(n/sup w/) bipartite matching algorithm is the fast matrix multiplication algorithm. Our results resolve a long-standing open question of whether Lovasz's randomized technique of testing graphs for perfect matching in time O(n/sup w/) can be extended to an algorithm that actually constructs a perfect matching.","Partitioning algorithms,
Bipartite graph,
Informatics,
Testing,
Polynomials,
Matrix decomposition,
Computer science"
A particle swarm model for tracking multiple peaks in a dynamic environment using speciation,"A particle swarm optimisation model for tracking multiple peaks in a continuously varying dynamic environment is described. To achieve this, a form of speciation allowing development of parallel subpopulations is used. The model employs a mechanism to encourage simultaneous tracking of multiple peaks by preventing overcrowding at peaks. Possible metrics for evaluating the performance of algorithms in dynamic, multimodal environments are put forward. Results are appraised in terms of the proposed metrics, showing that the technique is capable of tracking multiple peaks and that its performance is enhanced by preventing overcrowding. Directions for further research suggested by these results are put forward.","Particle swarm optimization,
Particle tracking,
Genetic algorithms,
Computer science,
Information technology,
Australia,
Heuristic algorithms,
Appraisal,
Equations,
Shape"
Forecasting Intraday stock price trends with text mining techniques,"In this paper, we describe NewsCATS (news categorization and trading system), a system implemented to predict stock price trends for the time immediately after the publication of press releases. NewsCATS consists mainly of three components. The first component retrieves relevant information from press releases through the application of text preprocessing techniques. The second component sorts the press releases into predefined categories. Finally, appropriate trading strategies are derived by the third component by means of the earlier categorization. The findings indicate that a categorization of press releases is able to provide additional information that can be used to forecast stock price trends, but that an adequate trading strategy is essential for the results of the categorization to be fully exploited.","Text mining,
Data mining,
Information security,
Information systems,
Information retrieval,
Computer security,
Computer science,
Mathematics,
Retirement,
Board of Directors"
Detecting and modeling doors with mobile robots,"We describe a probabilistic framework for detection and modeling of doors from sensor data acquired in corridor environments with mobile robots. The framework captures shape, color, and motion properties of door and wall objects. The probabilistic model is optimized with a version of the expectation maximization algorithm, which segments the environment into door and wall objects and learns their properties. The framework allows the robot to generalize the properties of detected object instances to new object instances. We demonstrate the algorithm on real-world data acquired by a Pioneer robot equipped with a laser range finder and an omni-directional camera. Our results show that our algorithm reliably segments the environment into walls and doors, finding both doors that move and doors that do not move. We show that our approach achieves better results than models that only capture behavior, or only capture appearance.",
Estimating joint contact areas and ligament lengths from bone kinematics and surfaces,"We present a novel method for modeling contact areas and ligament lengths in articulations. Our approach uses volume images generated by computed tomography and allows the in vivo and noninvasive study of articulations. In our method, bones are modeled both implicitly (scalar distance fields) and parametrically (manifold surfaces). Using this double representation, we compute interbone distances and estimate joint contact areas. Using the same types of representation, we model ligament paths; in our model, the ligaments are approximated by the shortest paths in a three-dimensional space with bone obstacles. We demonstrate the method by applying our contact area and ligament model to the distal radioulnar joints of a volunteer diagnosed with malunited distal radius fracture in one forearm. Our approach highlights focal changes in the articulation at the distal radioulnar joint (location and area of bone contact) and potential soft-tissue constraints (increased ""length"" of the distal ligaments and ligament-bone impingement in the injured forearm). Results suggest that the method could be useful in the study of normal and injured anatomy and kinematics of complex joints.","Joints,
Ligaments,
Bones,
Kinematics,
Computed tomography,
In vivo,
Computer science,
Degenerative diseases,
Biological tissues,
In vitro"
Generating tests from counterexamples,"We have extended the software model checker BLAST to automatically generate test suites that guarantee full coverage with respect to a given predicate. More precisely, given a C program and a target predicate p, BLAST determines the set L of program locations which program execution can reach with p true, and automatically generates a set of test vectors that exhibit the truth of p at all locations in L. We have used BLAST to generate test suites and to detect dead code in C programs with up to 30 K lines of code. The analysis and test vector generation is fully automatic (no user intervention) and exact (no false positives).","Automatic testing,
Software testing,
Computer science,
Computerized monitoring,
Condition monitoring,
Automata,
Safety,
Security,
Concrete,
Linux"
Location-aware topology matching in P2P systems,"Peer-to-peer (P2P) computing has emerged as a popular model aiming at further utilizing Internet information and resources, complementing the available client-server services. However, the mechanism of peers randomly choosing logical neighbors without any knowledge about underlying physical topology can cause a serious topology mismatching between the P2P overlay network and the physical underlying network. The topology mismatching problem brings a great stress in the Internet infrastructure and greatly limits the performance gain from various search or routing techniques. Meanwhile, due to the inefficient overlay topology, the flooding-based search mechanisms cause a large volume of unnecessary traffic. Aiming at alleviating the mismatching problem and reducing the unnecessary traffic, we propose a location-aware topology matching (LTM) technique, an algorithm of building an efficient overlay by disconnecting low productive connections and choosing physically closer nodes as logical neighbors while still retaining the search scope and reducing response time for queries. LTM is scalable and completely distributed in the sense that it does not require any global knowledge of the whole overlay network when each node is optimizing the organization of its logical neighbors. The effectiveness of LTM is demonstrated through simulation studies.","Network topology,
Peer to peer computing,
Telecommunication traffic,
Computer science,
Internet,
File servers,
Educational institutions,
Stress,
IP networks,
Computer crime"
Classifying human-robot interaction: an updated taxonomy,"This paper extends taxonomy of human-robot interaction (HRI) introduced in 2002 to include additional categories as well as updates to the categories from the original taxonomy. New classifications include measures of the social nature of the task (human interaction roles and human-robot physical proximity), task type, and robot morphology.","Taxonomy,
Collaboration,
Robot sensing systems,
Human robot interaction,
Collaborative work,
Application software,
Control systems,
Sensor systems,
Computer science,
Teleconferencing"
A scalable logical coordinates framework for routing in wireless sensor networks,"Routing is one of the key challenges in sensor networks that directly affects the information throughput and energy expenditure. Geographic routing is the most scalable routing scheme for statically placed nodes in that it uses only a constant amount of per-node state regardless of network size. The location information needed for this scheme, however, is not easy to compute accurately using current localization algorithms. In this paper, we propose a novel logical coordinate framework that encodes connectivity information for routing purposes without the benefit of geographic knowledge, while retaining the constant-state advantage of geographic routing. In addition to efficiency in the absence of geographic knowledge, our scheme has two important advantages: (i) it improves robustness in the presence of voids compared to other logical coordinate frameworks, and (ii) it allows inferring bounds on route hop count from the logical coordinates of the source and destination nodes, which makes it a candidate for use in soft real-time systems. The scheme is evaluated in simulation demonstrating the advantages of the new protocol.","Intelligent networks,
Wireless sensor networks,
Routing protocols,
Real time systems,
Computer science,
Electronic mail,
Throughput,
Robustness,
Meteorology,
Humans"
PRISM 2.0: a tool for probabilistic model checking,"This paper gives a brief overview of version 2.0 of PRISM, a tool for the automatic formal verification of probabilistic systems, and some of the case studies to which it has already been applied.","Clocks,
Algebra,
Computer science,
Formal verification,
Formal specifications,
Probabilistic logic,
Probability distribution,
Delay,
Concurrent computing,
Automata"
Boosting nested cascade detector for multi-view face detection,"A novel nested cascade detector for multi-view face detection is presented. This nested cascade is learned by Schapire and Singer's to improved boosting algorithms that use real-valued confidence-rated weak classifiers (Schapire, R. E. and Singer, Y, 1999), where we use confidence-rated look-up-table (LUT) weak classifiers based on Haar features. Experiments show the system performance is significantly improved compared with previous methods.","Face detection,
Boosting,
Detectors,
Table lookup,
Artificial intelligence,
Computer science,
Laboratories,
System performance,
Bayesian methods,
Real time systems"
Learning with case-injected genetic algorithms,"This paper presents a new approach to acquiring and using problem specific knowledge during a genetic algorithm (GA) search. A GA augmented with a case-based memory of past problem solving attempts learns to obtain better performance over time on sets of similar problems. Rather than starting anew on each problem, we periodically inject a GA's population with appropriate intermediate solutions to similar previously solved problems. Perhaps, counterintuitively, simply injecting solutions to previously solved problems does not produce very good results. We provide a framework for evaluating this GA-based machine-learning system and show experimental results on a set of design and optimization problems. These results demonstrate the performance gains from our approach and indicate that our system learns to take less time to provide quality solutions to a new problem as it gains experience from solving other similar problems in design and optimization.",
Registration-based interpolation,A method is presented to interpolate between neighboring slices in a grey-scale tomographic data set. Spatial correspondence between adjacent slices is established using a nonrigid registration algorithm based on B-splines which optimizes the normalized mutual information similarity measure. Linear interpolation of the image intensities is then carried out along the directions calculated by the registration algorithm. The registration-based method is compared to both standard linear interpolation and shape-based interpolation in 20 tomographic data sets. Results show that the proposed method statistically significantly outperforms both linear and shape-based interpolation.,"Interpolation,
Biomedical imaging,
Tomography,
Spline,
Image segmentation,
Data mining,
Associate members,
Mutual information,
Biomedical measurements,
Image processing"
Comparison between the unscented Kalman filter and the extended Kalman filter for the position estimation module of an integrated navigation information system,"An integrated navigation information system must know continuously the current position with a good precision. The required performance of the positioning module is achieved by using a cluster of heterogeneous sensors whose measurements are fused. The most popular data fusion method for positioning problems is the extended Kalman filter. The extended Kalman filter is a variation of the Kalman filter used to solve non-linear problems. Recently, an improvement to the extended Kalman filter has been proposed, the unscented Kalman filter. This paper describes an empirical analysis evaluating the performances of the unscented Kalman filter and comparing them with the extended Kalman filter's performances.","Navigation,
Information systems,
Kalman filters,
Vehicles,
Global Positioning System,
Sensor fusion,
Gaussian noise,
Electrical engineering,
Computer science,
Position measurement"
Real-Coded Memetic Algorithms with Crossover Hill-Climbing,"This paper presents a real-coded memetic algorithm that applies a crossover hill-climbing to solutions produced by the genetic operators. On the one hand, the memetic algorithm provides global search (reliability) by means of the promotion of high levels of population diversity. On the other, the crossover hill-climbing exploits the self-adaptive capacity of real-parameter crossover operators with the aim of producing an effective local tuning on the solutions (accuracy). An important aspect of the memetic algorithm proposed is that it adaptively assigns different local search probabilities to individuals. It was observed that the algorithm adjusts the global/local search balance according to the particularities of each problem instance. Experimental results show that, for a wide range of problems, the method we propose here consistently outperforms other real-coded memetic algorithms which appeared in the literature.","crossover hill-climbing,
Memetic algorithms,
real-coding,
steady-stated genetic algorithms"
Connected K-coverage problem in sensor networks,"In overdeployed sensor networks, one approach to conserve energy is to keep only a small subset of sensors active at any instant. In this article, we consider the problem of selecting a minimum size connected K-cover, which is defined as a set of sensors M such that each point in the sensor network is ""covered"" by at least K different sensors in M, and the communication graph induced by M is connected. For the above optimization problem, we design a centralized approximation algorithm that delivers a near-optimal (within a factor of O(lg n)) solution, and present a distributed version of the algorithm. We also present a communication-efficient localized distributed algorithm which is empirically shown to perform well","Intelligent networks,
Sensor phenomena and characterization,
Signal processing algorithms,
Algorithm design and analysis,
Approximation algorithms,
Wireless sensor networks,
Computer science,
Design optimization,
Distributed algorithms,
Monitoring"
Interactive Visualization of Small World Graphs,"Many real world graphs have small world characteristics, that is, they have a small diameter compared to the number of nodes and exhibit a local cluster structure. Examples are social networks, software structures, bibliographic references and biological neural nets. Their high connectivity makes both finding a pleasing layout and a suitable clustering hard. In this paper we present a method to create scalable, interactive visualizations of small world graphs, allowing the user to inspect local clusters while maintaining a global overview of the entire structure. The visualization method uses a combination of both semantical and geometrical distortions, while the layout is generated by a spring embedder algorithm using recently developed force model. We use a cross referenced database of 500 artists as a running example","Visualization,
Social network services,
Clustering algorithms,
Spatial databases,
Visual databases,
Mathematics,
Neural networks,
Springs,
Computer science,
Solid modeling"
Tomographic image reconstruction based on a content-adaptive mesh model,"In this paper, we explore the use of a content-adaptive mesh model (CAMM) for tomographic image reconstruction. In the proposed framework, the image to be reconstructed is first represented by a mesh model, an efficient image description based on nonuniform sampling. In the CAMM, image samples (represented as mesh nodes) are placed most densely in image regions having fine detail. Tomographic image reconstruction in the mesh domain is performed by maximum-likelihood (ML) or maximum a posteriori (MAP) estimation of the nodal values from the measured data. A CAMM greatly reduces the number of unknown parameters to be determined, leading to improved image quality and reduced computation time. We demonstrated the method in our experiments using simulated gated single photon emission computed tomography (SPECT) cardiac-perfusion images. A channelized Hotelling observer (CHO) was used to evaluate the detectability of perfusion defects in the reconstructed images, a task-based measure of image quality. A minimum description length (MDL) criterion was also used to evaluate the effect of the representation size. In our application, both MDL and CHO suggested that the optimal number of mesh nodes is roughly five to seven times smaller than the number of projection bins. When compared to several commonly used methods for image reconstruction, the proposed approach achieved the best performance, in terms of defect detection and computation time. The research described in this paper establishes a foundation for future development of a (four-dimensional) space-time reconstruction framework for image sequences in which a built-in deformable mesh model is used to track the image motion.","Image reconstruction,
Maximum likelihood estimation,
Image quality,
Nonuniform sampling,
Performance evaluation,
Maximum likelihood detection,
Computational modeling,
Single photon emission computed tomography,
Image sequences,
Deformable models"
Kolmogorov's structure functions and model selection,"In 1974, Kolmogorov proposed a nonprobabilistic approach to statistics and model selection. Let data be finite binary strings and models be finite sets of binary strings. Consider model classes consisting of models of given maximal (Kolmogorov) complexity. The ""structure function"" of the given data expresses the relation between the complexity level constraint on a model class and the least log-cardinality of a model in the class containing the data. We show that the structure function determines all stochastic properties of the data: for every constrained model class it determines the individual best fitting model in the class irrespective of whether the ""true"" model is in the model class considered or not. In this setting, this happens with certainty, rather than with high probability as is in the classical case. We precisely quantify the goodness-of-fit of an individual model with respect to individual data. We show that-within the obvious constraints-every graph is realized by the structure function of some data. We determine the (un)computability properties of the various functions contemplated and of the ""algorithmic minimal sufficient statistic."".","Statistics,
Mathematics,
Computer science,
Stochastic processes,
Predictive models,
Technological innovation,
Engineering profession,
Region 8,
Logic,
Source coding"
Structures for phase classification,"Most programs are repetitive, where similar behavior can be seen at different execution times. Proposed algorithms automatically group these similar intervals of execution into phases, where all he intervals in a phase have homogeneous behavior and similar resource requirements. In this paper we examine different program structures for capturing phase behavior. The goal is to compare the size and accuracy of these structures for performing phase classification. We focus on profiling the frequency of program level structures that are independent from underlying architecture performance metrics. This allows the phase classification to be used across different hardware designs that support the same instruction set (ISA). We compare using basic blocks, loop branches, procedures, opcodes, register usage, and memory address information for guiding phase classification. We compare these different structures in terms of their ability to create homogeneous phases, and evaluate the accuracy of using these structures to pick simulation points for SimPoint.","Frequency,
Atomic measurements,
Computer science,
Hardware,
Instruction sets,
Phase detection,
Registers,
Analytical models,
Performance analysis,
Statistics"
Pulmonary airways: 3-D reconstruction from multislice CT and clinical investigation,"In the framework of computer-aided diagnosis, this paper proposes a novel functionality for the computerized tomography (CT)-based investigation of the pulmonary airways. It relies on an energy-based three-dimensional (3-D) reconstruction of the bronchial tree from multislice CT acquisitions, up to the sixth- to seventh-order subdivisions. Global and local analysis of the reconstructed airways is possible by means of specific visualization modalities, respectively, the CT bronchography and the virtual bronchoscopy. The originality of the 3-D reconstruction approach consists in combining axial and radial propagation potentials to control the growth of a subset of low-order airways extracted from the CT volume by means of a robust mathematical morphology operator-the selective marking and depth constrained (SMDC) connection cost. The proposed approach proved to be robust with respect to a large spectrum of airway pathologies, including even severe stenosis (bronchial lumen obstruction/collapse). Validated by expert radiologists, examples of airway 3-D reconstructions are presented and discussed for both normal and pathological cases. They highlight the interest in considering CT bronchography and virtual bronchoscopy as complementary tools for clinical diagnosis and follow-up of airway diseases.",
Actuating a simple 3D passive dynamic walker,"The passive dynamic walker described in this paper is a robot with a minimal number of degrees of freedom which is still capable of stable 3D dynamic walking. First, we present the reduced-order dynamic models used to tune the characteristics of the robot's passive gait. Our sagittal plane model is closely related to the compass gait model, but the steady state trajectory passively converges from a much larger range of initial conditions. We then experimentally quantify the stability of the mechanical device. Finally, we present an actuated version of the robot and some preliminary active control strategies. The control problem for the actuated version of the robot is interesting because although it is theoretically challenging (4 degrees of under-actuation), the mechanical design of the robot made it relatively easy to create controllers which allowed the robot to walk stably on flat terrain and even up a small slope.","Legged locomotion,
Robots,
Control systems,
Leg,
Energy efficiency,
Actuators,
Computer science,
Artificial intelligence,
Mechanical engineering,
Cognitive robotics"
Adaptive clustering ensembles,"Clustering ensembles combine multiple partitions of the given data into a single clustering solution of better quality. Inspired by the success of supervised boosting algorithms, we devise an adaptive scheme for integration of multiple non-independent clusterings. Individual partitions in the ensemble are sequentially generated by clustering specially selected subsamples of the given data set. The sampling probability for each data point dynamically depends on the consistency of its previous assignments in the ensemble. New subsamples are drawn to increasingly focus on the problematic regions of the input feature space. A measure of a data point's clustering consistency is defined to guide this adaptation. An empirical study compares the performance of adaptive and regular clustering ensembles using different consensus functions on a number of data sets. Experimental results demonstrate improved accuracy for some clustering structures.",
A definition for information system survivability,"Society has become dependent on information systems. As networks develop into large-scale systems, often critical to personal and business operations, survivability of these systems is imperative. While these systems continue to emerge and grow, answers to questions like: ""What does survivability mean?"", ""How is survivability being measured?"", and ""How is survivability computed?"" become very important. This paper summarizes the standard or lack of standard methods for defining and computing survivability while providing an easy to reference baseline of the current state. It also provides a template for defining survivability to facilitate subsequent research into computational quality attributes by using standard definitions. Where there are gaps or inconsistencies in current research and practice, assessments can be made to continue research and development in the areas most needed to develop taxonomy of survivability.","Information systems,
Computer networks,
Distributed computing,
Research and development,
Taxonomy,
Computer industry,
High-speed networks,
Information security,
Large-scale systems,
Guidelines"
Three-dimensional path planning for virtual bronchoscopy,"Multidetector computed-tomography (MDCT) scanners provide large high-resolution three-dimensional (3-D) images of the chest. MDCT scanning, when used in tandem with bronchoscopy, provides a state-of-the-art approach for lung-cancer assessment. We have been building and validating a lung-cancer assessment system, which enables virtual-bronchoscopic 3-D MDCT image analysis and follow-on image-guided bronchoscopy. A suitable path planning method is needed, however, for using this system. We describe a rapid, robust method for computing a set of 3-D airway-tree paths from MDCT images. The method first defines the skeleton of a given segmented 3-D chest image and then performs a multistage refinement of the skeleton to arrive at a final tree structure. The tree consists of a series of paths and branch structural data, suitable for quantitative airway analysis and smooth virtual navigation. A comparison of the method to a previously devised path-planning approach, using a set of human MDCT images, illustrates the efficacy of the method. Results are also presented for human lung-cancer assessment and the guidance of bronchoscopy.","Path planning,
Bronchoscopy,
Image analysis,
Navigation,
Computed tomography,
High-resolution imaging,
Skeleton,
Humans,
Biomedical imaging,
Cities and towns"
A constraint-handling mechanism for particle swarm optimization,"This work presents a simple mechanism to handle constraints with a particle swarm optimization algorithm. Our proposal uses a simple criterion based on closeness of a particle to the feasible region in order to select a leader. Additionally, our algorithm incorporates a turbulence operator that improves the exploratory capabilities of our particle swarm optimization algorithm. Despite its relative simplicity, our comparison of results indicates that the proposed approach is highly competitive with respect to three constraint-handling techniques representative of the state-of-the-art in the area.",
An adaptive mobility-aware MAC protocol for sensor networks (MS-MAC),"Most of the MAC protocols proposed for wireless sensor networks assume sensors to be stationary after deployment. This usually provides very bad network performance in scenarios involving mobile sensors. Handling mobility in wireless sensor networks in an energy-efficient way is a new challenge. Techniques developed for other mobile networks, such as mobile phone or mobile ad hoc networks, cannot be applicable, as in these networks, energy is not a very critical resource. The paper presents a new adaptive mobility-aware MAC protocol for sensor networks (MS-MAC). The protocol uses any change in received signal level as an indication of mobility and, when necessary, triggers the mobility handling mechanism. In this way, the new mobility-aware MAC protocol can work in a very energy-efficient way when the network is stationary, whereas it can maintain some level of network performance when there are mobile sensors.",
Transition invariants,Proof rules for program verification rely on auxiliary assertions. We propose a (sound and relatively complete) proof rule whose auxiliary assertions are transition invariants. A transition invariant of a program is a binary relation over program states that contains the transitive closure of the transition relation of the program. A relation is disjunctively well-founded if it is a finite union of well-founded relations. We characterize the validity of termination or another liveness property by the existence of a disjunctively well-founded transition invariant. The main contribution of our proof rule lies in its potential for automation via abstract interpretation.,
A survey of approaches to three-dimensional face recognition,"The vast majority of face recognition research has focused on the use of two-dimensional intensity images, and is covered in existing survey papers. This survey focuses on face recognition using three-dimensional data, either alone or in combination with two-dimensional intensity images. Challenges involved in developing more accurate three-dimensional face recognition are identified.","Face recognition,
Image segmentation,
Probes,
Shape,
Biometrics,
Fingerprint recognition,
Iris,
Surface cleaning,
Computer science,
Testing"
Cache-aware scratchpad allocation algorithm,"In the context of portable embedded systems, reducing energy is one of the prime objectives. Most high-end embedded microprocessors include onchip instruction and data caches, along with a small energy efficient scratchpad. Previous approaches for utilizing scratchpad did not consider caches and hence fail for the au courant architecture. In the presented work, we use the scratchpad for storing instructions and propose a generic cache aware scratchpad allocation (CASA) algorithm. We report an average reduction of 8-29% in instruction memory energy consumption compared to a previously published technique for benchmarks from the mediabench suite. The scratchpad in the presented architecture is similar to a preloaded loop cache. Comparing the energy consumption of our approach against preloaded loop caches, we report average energy savings of 20-44%.","Embedded system,
Energy efficiency,
Energy consumption,
Microprocessors,
Costs,
Computer science,
Batteries,
Computer aided instruction,
Performance analysis,
Design automation"
Software productivity measurement using multiple size measures,"Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p<0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an example of its use for Web application projects. We also explain the relationship between effort prediction models and productivity models.","Size measurement,
Software measurement,
Productivity,
Predictive models,
Costs,
Equations,
Computer science,
Computer Society,
Application software,
Production"
A new actuation approach for human friendly robot design,"Many successful robotic manipulator designs have been introduced. However, there remains the challenge of designing a manipulator that possesses the inherent safety characteristics necessary for human-centered robotics. In this paper, we present a new actuation approach that has the requisite characteristics for inherent safety while maintaining the performance expected of modern designs. By drastically reducing the effective impedance of the manipulator while maintaining high frequency torque capability, we show that the competing design requirements of performance and safety can be successfully integrated into a single manipulation system.","Manipulators,
Safety,
Acceleration,
Impedance,
Torque,
Human robot interaction,
Service robots,
Brain injuries,
Robot sensing systems,
Computer science"
Algorithmic mechanism design for load balancing in distributed systems,"Computational grids are promising next-generation computing platforms for large-scale problems in science and engineering. Grids are large-scale computing systems composed of geographically distributed resources (computers, storage etc.) owned by self interested agents or organizations. These agents may manipulate the resource allocation algorithm in their own benefit, and their selfish behavior may lead to severe performance degradation and poor efficiency. In this paper, we investigate the problem of designing protocols for resource allocation involving selfish agents. Solving this kind of problems is the object of mechanism design theory. Using this theory, we design a truthful mechanism for solving the static load balancing problem in heterogeneous distributed systems. We prove that using the optimal allocation algorithm the output function admits a truthful payment scheme satisfying voluntary participation. We derive a protocol that implements our mechanism and present experiments to show its effectiveness.","Algorithm design and analysis,
Load management,
Cost accounting,
Distributed computing,
Grid computing,
Protocols,
Game theory,
Large-scale systems,
Resource management,
Degradation"
Motion planning using dynamic roadmaps,"We evaluate the use of dynamic roadmaps for online motion planning in changing environments. When changes are detected in the workspace, the validity state of affected edges and nodes of a precompiled roadmap are updated accordingly. We concentrate in this paper on analyzing the tradeoffs between maintaining dynamic roadmaps and applying an on-line bidirectional rapidly-exploring random tree (RRT) planner alone, which requires no preprocessing or maintenance. We ground the analysis in several benchmarks in virtual environments with randomly moving obstacles. Different robotics structures are used, including a 17 degrees of freedom model of NASA's Robonaut humanoid. Our results show that dynamic roadmaps can be both faster and more capable for planning difficult motions than using on-line planning alone. In particular, we investigate its scalability to 3D workspaces and higher dimensional configurations spaces, as our main interest is the application of the method to interactive domains involving humanoids.","Motion planning,
Humanoid robots,
Humans,
Virtual environment,
Orbital robotics,
Manipulator dynamics,
Computer science,
Scalability,
Robot sensing systems,
Signal mapping"
Protection and guarantee for voice and video traffic in IEEE 802.11e wireless LANs,"In order to support multimedia applications such as voice and video over the wireless medium, a contention-based channel access function, called enhanced distributed coordination function (EDCF), is being developed in the emerging standard IEEE 802.11e. In EDCF, differentiated services are provided for different traffic classes. In this paper, we propose a two-level protection and guarantee mechanism for voice and video traffic in IEEE 802.11e wireless LANs. In the first-level protection, the existing voice and video flows are protected from the new and other existing voice and video flows. In the second-level protection, the voice and video flows are protected from the best-effort data traffic. For each protection level, a couple of protection mechanisms are proposed. Extensive simulation results show that the proposed two-level protection and guarantee mechanism is very effective in terms of protecting and guaranteeing existing voice and video flows as well as fully utilizing the channel capacity","Protection,
Wireless LAN,
Local area networks,
Electronic mail,
Quality of service,
Computer science,
Admission control,
Media Access Protocol,
Application software,
Standards development"
Improving the use of vibro-acoustography for brachytherapy metal seed imaging: A feasibility study,"Vibro-acoustography method is explored for detecting and imaging brachytherapy metal seeds in gel phantoms. In a previous paper, we have shown that some immersed objects' resonance frequencies could be detected by vibro-acoustography. Here, we use this idea to optimize the vibro-acoustic excitation of two different sized brass seeds implanted in an agar gel phantom. In the experiments, the best excitation vibration frequencies were determined either by calculating fundamental resonance frequencies for each of the seeds or the experimental optimal resonance frequency of the gel. The resulting vibro-acoustography images demonstrate remarkable contrast in acoustic emission amplitude compared with images obtained at nonresonance frequencies. Results suggest the possible application of vibro-acoustography for directing prostate brachytherapy seed implantation treatment.","Ultrasonic imaging,
Magnetic resonance imaging,
Brachytherapy,
Resonant frequency,
Acoustic emission,
Radiography,
Imaging phantoms,
Implants,
Biomedical imaging,
Glands"
Developing trust in large-scale peer-to-peer systems,"In peer-to-peer (P2P) systems, peers often must interact with unknown or unfamiliar peers without the benefit of trusted third parties or authorities to mediate the interactions. A peer needs reputation mechanisms to incorporate the knowledge of others to decide whether to trust another party in P2P systems. This paper discusses the design of reputation mechanisms and proposes a distributed reputation mechanism to detect malicious or unreliable peers in P2P systems. It illustrates the process for rating gathering and aggregation and presents some experimental results to evaluate the proposed approach. Moreover, it considers how to effectively aggregate noisy (dishonest or inaccurate) ratings from independent or collusive peers using weighted majority techniques. Furthermore, it analyzes some possible attacks on reputation mechanisms and shows how to defend against such attacks.","Large-scale systems,
Peer to peer computing,
Computer science,
Electronic commerce,
Web services,
Quality of service,
Aggregates,
Security,
Authentication,
Certification"
A middleware for building context-aware mobile services,"Computing becomes increasingly mobile and pervasive today; these changes imply that applications and services must be aware and adapt to highly dynamic environments. Today, building context-aware mobile services is a complex and time-consuming task. We present a service-oriented context-aware middleware (SOCAM) architecture for the building and rapid prototyping of context-aware mobile services. We propose an ontology-based approach to model various contexts. Our context model supports semantic representation, context reasoning and context knowledge sharing. We take a service-oriented approach to build our middleware which supports tasks including acquiring, discovering, interpreting, accessing various contexts and interoperability between different context-aware systems.","Middleware,
Context-aware services,
Mobile computing,
Context awareness,
Buildings,
Ontologies,
Context modeling,
Prototypes,
Computer science,
Application software"
A co-phase matrix to guide simultaneous multithreading simulation,"Several commercial processors have architectures that include support for simultaneous multithreading (SMT), yet there is still not a validated methodology for estimating the performance of an SMT machine that does not rely on full program simulation. To create an efficient sampling approach for SMT we must determine how far to fast-forward each individual thread between samples. The fast-forwarding distance for each thread will vary according to execution phases, thread interactions and changes to the architectural configuration. We examine using individual program phase information to guide SMT simulation. This is accomplished by creating what we call a co-phase matrix. The co-phase matrix is populated by collecting samples of the programs' phase combinations, and is used to guide fastforwarding between samples. We show for 28 pairs of SPEC programs that using the co-phase matrix provides an average error rate of 4% while requiring that only 1% of the full simulation be performed. The methods are also validated using a variety of architectural configurations and four-threaded workloads.","Multithreading,
Surface-mount technology,
Yarn,
Computational modeling,
Computer architecture,
Computer science,
Computer simulation,
Error analysis,
Modems,
Force measurement"
A new research challenge: persuasive technology to motivate healthy aging,"Healthcare systems in developed countries are experiencing severe financial stress as age demographics shift upward, leading to a larger percentage of older adults needing care. One way to potentially reduce or slow spiraling medical costs is to use technology, not only to cure sickness, but also to promote wellness throughout all stages of life, thereby avoiding or deferring expensive medical treatments. Ubiquitous computing and context-aware algorithms offer a new healthcare opportunity and a new set of research challenges: exploiting emerging consumer electronic devices to motivate healthy behavior as people age by presenting ""just-in-time"" information at points of decision and behavior",
Simplifying flexible isosurfaces using local geometric measures,"The contour tree, an abstraction of a scalar field that encodes the nesting relationships of isosurfaces, can be used to accelerate isosurface extraction, to identify important isovalues for volume-rendering transfer functions, and to guide exploratory visualization through a flexible isosurface interface. Many real-world data sets produce unmanageably large contour trees which require meaningful simplification. We define local geometric measures for individual contours, such as surface area and contained volume, and provide an algorithm to compute these measures in a contour tree. We then use these geometric measures to simplify the contour trees, suppressing minor topological features of the data. We combine this with a flexible isosurface interface to allow users to explore individual contours of a dataset interactively.","Isosurfaces,
Surface topography,
Computer science,
Skull,
Data visualization,
Rendering (computer graphics),
Lakes,
Blood vessels,
Sockets,
Data mining"
Performance evaluation of an optimal cache replacement policy for wireless data dissemination,"Data caching at mobile clients is an important technique for improving the performance of wireless data dissemination systems. However, variable data sizes, data updates, limited client resources, and frequent client disconnections make cache management a challenge. We propose a gain-based cache replacement policy, Min-SAUD, for wireless data dissemination when cache consistency must be enforced before a cached item is used. Min-SAUD considers several factors that affect cache performance, namely, access probability, update frequency, data size, retrieval delay, and cache validation cost. The paper employs stretch as the major performance metric since it accounts for the data service time and, thus, is fair when items have different sizes. We prove that Min-SAUD achieves optimal stretch under some standard assumptions. Moreover, a series of simulation experiments have been conducted to thoroughly evaluate the performance of Min-SAUD under various system configurations. The simulation results show that, in most cases, the Min-SAUD replacement policy substantially outperforms two existing policies, namely, LRU and SAIU.","Information retrieval,
Delay,
Mobile computing,
Broadcasting,
Computer science,
Frequency,
Costs,
Computational modeling,
Application software,
Resource management"
TrafficView: a scalable traffic monitoring system,"Vehicles are part of people's life in modern society, into which more and more high-tech devices are integrated, and a common platform for inter-vehicle communication is necessary to realize an intelligent transportation system supporting safe driving, dynamic route scheduling, emergency message dissemination, and traffic condition monitoring. TrafficView, which is a part of the e-Road project, defines a framework to disseminate and gather information about the vehicles on the road. Using such a system will provide a vehicle driver with road traffic information, which helps driving in situations such as foggy weather, or finding an optimal route in a trip several miles long. This paper describes the basic design of TrafficView and different algorithms used in the system.","Road vehicles,
Pervasive computing,
Vehicle driving,
Vehicle dynamics,
Condition monitoring,
Computer science,
Dynamic scheduling,
Intelligent transportation systems,
Real time systems,
Road accidents"
Domain-specific Web search with keyword spices,"Domain-specific Web search engines are effective tools for reducing the difficulty experienced when acquiring information from the Web. Existing methods for building domain-specific Web search engines require human expertise or specific facilities. However, we can build a domain-specific search engine simply by adding domain-specific keywords, called ""keyword spices,"" to the user's input query and forwarding it to a general-purpose Web search engine. Keyword spices can be effectively discovered from Web documents using machine learning technologies. The paper describes domain-specific Web search engines that use keyword spices for locating recipes, restaurants, and used cars.","Web search,
SPICE,
Search engines,
Crawlers,
Machine learning,
Computer science,
Filters,
Filtering,
Humans,
Decision trees"
Managing Duplicated Code with Linked Editing,"We present linked editing, a novel, lightweight editor-based technique for managing duplicated source code. Linked editing is implemented in a prototype editor called Codelink. We argue that the use of programming abstractions like functions and macros - the traditional solution to duplicated code - has inherent cognitive costs, leading programmers to chronically copy and paste code instead. Our user study compares functional abstraction with linked editing and shows that linked editing can give the benefits of abstraction with orders of magnitude decrease in programming time",
Energy-aware node placement in wireless sensor networks,"One of the main design issues for wireless sensor networks is the sensor placement problem. We formulate a constrained multivariable nonlinear programming problem to determine both the locations of the sensor nodes and data transmission pattern. Our two objectives are to maximize the network lifetime and to minimize the application-specific total cost, given a fixed number of sensor nodes in a region with a certain coverage requirement. We first study a linear network, and find optimal placement strategies numerically. Through numerical results, we show that the optimal node placement strategies provide significant benefit over a commonly used uniform placement scheme. Furthermore, we also present a performance bound as a benchmark. Lastly, we extend the results to a more sophisticated planar network, and use numerical results to evaluate the performance of the proposed strategies.","Intelligent networks,
Wireless sensor networks,
Energy consumption,
Telecommunication traffic,
Power system modeling,
Electronic mail,
Data communication,
Costs,
Monitoring,
Computer science"
Loopback recovery from double-link failures in optical mesh networks,"Network survivability is a crucial requirement in high-speed optical networks. Typical approaches of providing survivability have considered the failure of a single component such as a link or a node. We motivate the need for considering double-link failures and present three loopback methods for handling such failures. In the first two methods, two edge-disjoint backup paths are computed for each link for rerouting traffic when a pair of links fails. These methods require the identification of the failed links before recovery can be completed. The third method requires the precomputation of a single backup path and does not require link identification before recovery. An algorithm that precomputes backup paths for links in order to tolerate double-link failures is then presented. Numerical results comparing the performance of our algorithm with other approaches suggest that it is possible to achieve almost 100% recovery from double-link failures with a moderate increase in backup capacity. A remarkable feature of our approach is that it is possible to trade off capacity for restorability by choosing a subset of double-link failures and designing backup paths using our algorithm for only those failure scenarios.","Intelligent networks,
Optical fiber networks,
Mesh networks,
Wavelength division multiplexing,
High speed optical techniques,
Protection,
Wavelength routing,
Computer science,
Optical switches,
Circuit faults"
Location-centric isolation of misbehavior and trust routing in energy-constrained sensor networks,"We propose a novel, location-centric, architecture for isolating misbehavior and establishing trust routing in sensor networks. Our scheme fits the data-centric nature of sensor networks and is suitable for use in energy-constrained networks. Much of our protocols operate in the sinks relieving the sensors from a lot of functionality. Our protocols select trusted paths that do not include misbehaving nodes, by identifying insecure locations and routing around them efficiently via detour points using embedded blacklists and modified geographic or trajectory routing. For insecure location discovery we propose efficient one-shot probing. Cheat-proofing is achieved using location correlation to remove false reporting. Our simulations show how our scheme effectively increases the throughput and energy-efficiency of a sensor network and alleviates the effect of route infection in geographic routing.","Intelligent networks,
Routing protocols,
Network topology,
Wireless sensor networks,
Computer science,
Computer architecture,
Throughput,
Energy efficiency,
Collaborative work,
Broadcasting"
Variable packet size buffered crossbar (CICQ) switches,"One of the most widely used architectures for packet switches is the crossbar. A special version of it is the buffered crossbar, where small buffers are associated with the crosspoints; this simplifies scheduling and improves its efficiency and QoS capabilities to the point where the switch needs no internal speedup. Furthermore, by supporting variable length packets throughout a buffered crossbar: (a) there is no need for segmentation and reassembly (SAR) circuits; (b) no speedup is necessary to support SAR; and (c) synchronization between the input and output clock domains is simplified. In turn, the lack of SAR and speedup mean that no output queues are needed, either. In this paper we present an architecture, a chip layout and cost analysis, and a performance evaluation of such a 300 Gbps buffered crossbar operating on variable-size packets. The proposed organization is simple yet powerful, it can be implemented using modern technology, and, as the performance results demonstrate, it clearly outperforms unbuffered crossbars.","Packet switching,
Switches,
Scheduling,
Costs,
Synchronization,
Throughput,
Computer science,
Clocks,
Quality of service,
Energy consumption"
Speaker association with signal-level audiovisual fusion,"Audio and visual signals arriving from a common source are detected using a signal-level fusion technique. A probabilistic multimodal generation model is introduced and used to derive an information theoretic measure of cross-modal correspondence. Nonparametric statistical density modeling techniques can characterize the mutual information between signals from different domains. By comparing the mutual information between different pairs of signals, it is possible to identify which person is speaking a given utterance and discount errant motion or audio from other utterances or nonspeech events.","Mutual information,
Signal processing,
Speech recognition,
Signal detection,
Fusion power generation,
Databases,
Microphones,
Telephony,
Telephone sets,
Computer science"
Precise minimax redundancy and regret,"Recent years have seen a resurgence of interest in redundancy of lossless coding. The redundancy (regret) of universal fixed-to-variable length coding for a class of sources determines by how much the actual code length exceeds the optimal (ideal over the class) code length. In a minimax scenario one finds the best code for the worst source either in the worst case (called also maximal minimax) or on average. We first study the worst case minimax redundancy over a class of stationary ergodic sources and replace Shtarkov's bound by an exact formula. Among others, we prove that a generalized Shannon code minimizes the worst case redundancy, derive asymptotically its redundancy, and establish some general properties. This allows us to obtain precise redundancy for memoryless, Markov, and renewal sources. For example, we present the exact constant of the redundancy for memoryless and Markov sources by showing that the integer nature of coding contributes log(logm/(m-1))/logm+o(1) where m is the size of the alphabet. Then we deal with the average minimax redundancy and regret. Our approach here is orthogonal to most recent research in this area since we aspire to show that asymptotically the average minimax redundancy is equivalent to the worst case minimax redundancy for some classes of sources. After formulating some general bounds relating these two redundancies, we prove our assertion for memoryless and Markov sources. Nevertheless, we provide evidence that maximal redundancy of renewal processes does not have the same leading term as the average minimax redundancy (however, our general results show that maximal and average regrets are asymptotically equivalent).","Minimax techniques,
Information theory,
Entropy,
Computer science,
Source coding,
Maximum likelihood decoding"
Multiobjective data clustering,"Conventional clustering algorithms utilize a single criterion that may not conform to the diverse shapes of the underlying clusters. We offer a new clustering approach that uses multiple clustering objective functions simultaneously. The proposed multiobjective clustering is a two-step process. It includes detection of clusters by a set of candidate objective functions as well as their integration into the target partition. A key ingredient of the approach is a cluster goodness junction that evaluates the utility of multiple clusters using re-sampling techniques. Multiobjective data clustering is obtained as a solution to a discrete optimization problem in the space of clusters. At meta-level, our algorithm incorporates conflict resolution techniques along with the natural data constraints. An empirical study on a number of artificial and real-world data sets demonstrates that multiobjective data clustering leads to valid and robust data partitions.",
Value Webs: using ontologies to bundle real-world services,"Real-world services - that is, nonsoftware-based services - differ significantly from Web services, usually defined as software functionality accessible and configurable over the Web. Because of the economic, social, and business importance of the service concept in general, we believe it's necessary to rethink what this concept means in an ontological and computational sense. We deal about the OBELIX (ontology-based electronic integration of complex products and value chains) project has therefore developed a generic component-based ontology for real-world services. This OBELIX service ontology is first of all a formalization of concepts that represent the consensus in the business science literature on service management and marketing. We express our service ontology in a graphical, network-style representation, and we've developed support tools that facilitate end-user modeling of services. Then, automated knowledge-based configuration methods let business designers and analysts analyze service bundles. We've tested our ontology, methods, and tools on applications in real-world case studies of different industry sectors.","Ontologies,
Semantic Web,
Business,
Web services,
Computer science,
Intelligent systems,
Environmental economics,
Software,
Context-aware services,
Grounding"
Explicit construction of families of LDPC codes with no 4-cycles,"Low-density parity-check (LDPC) codes are serious contenders to turbo codes in terms of decoding performance. One of the main problems is to give an explicit construction of such codes whose Tanner graphs have known girth. For a prime power q and m/spl ges/2, Lazebnik and Ustimenko construct a q-regular bipartite graph D(m,q) on 2q/sup m/ vertices, which has girth at least 2/spl lceil/m/2/spl rceil/+4. We regard these graphs as Tanner graphs of binary codes LU(m,q). We can determine the dimension and minimum weight of LU(2,q), and show that the weight of its minimum stopping set is at least q+2 for q odd and exactly q+2 for q even. We know that D(2,q) has girth 6 and diameter 4, whereas D(3,q) has girth 8 and diameter 6. We prove that for an odd prime p, LU(3,p) is a [p/sup 3/,k] code with k/spl ges/(p/sup 3/-2p/sup 2/+3p-2)/2. We show that the minimum weight and the weight of the minimum stopping set of LU(3,q) are at least 2q and they are exactly 2q for many LU(3,q) codes. We find some interesting LDPC codes by our partial row construction. We also give simulation results for some of our codes.","Parity check codes,
Turbo codes,
Bipartite graph,
Binary codes,
Iterative decoding,
Belief propagation,
Computer science,
Mathematics,
Iterative algorithms,
Application software"
Finding latent code errors via machine learning over program executions,"This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.","Machine learning,
Testing,
Programming profession,
Computer errors,
Support vector machines,
Support vector machine classification,
Decision trees,
Classification tree analysis,
Laboratories,
Computer science"
Embedded hardware face detection,"Face detection is the first step towards face recognition and is a vital task in surveillance and security applications. Current software implementations of face detection algorithms lack the computational ability to support detection in real time video streams. Consequently, this work focuses on the design of special-purpose hardware for performing rotation invariant face detection. The synthesized design using 160 nm technology is found to operate at 409.5 kHz providing a throughput of 424 frames per second and consumes 7 Watts of power. The synthesized design provided 75% accuracy in detecting faces from a set of 55 images that is competitive with existing software implementations that provide around 80-85% accuracy.","Hardware,
Face detection,
Face recognition,
Software algorithms,
Security,
Robot vision systems,
Cameras,
Field programmable gate arrays,
Real time systems,
Computer science"
TDMA service for sensor networks,"Sensors networks are often constrained by limited power and limited communication range. If a sensor receives two messages simultaneously then they collide and both messages become incomprehensible. In this paper, we present a simple time division multiple access (TDMA) algorithms for assigning time slots to sensors and show that it provides a significant reduction in the number of collisions incurred during communication. We present TDMA algorithms customized for different communication patterns, namely, broadcast, convergecast and local gossip, that occur commonly in sensor networks. Our algorithms are self-stabilizing, i.e., TDMA is restored even if the system reaches an arbitrary state where the sensors are corrupted or improperly initialized.","Time division multiple access,
Multiaccess communication,
Broadcasting,
Access protocols,
Sensor systems,
Object detection,
Frequency division multiaccess,
Software engineering,
Laboratories,
Computer science"
Using cognitive artifacts to understand distributed cognition,"Studies of patient safety have identified gaps in current work including the need for research about communication and information sharing among healthcare providers. They have also encouraged the use of decision support tools to improve human performance. Distributed cognition is the shared awareness of goals, plans, and details that no single individual grasps. Cognitive artifacts are objects such as: schedules, display boards, lists, and worksheets that form part of a distributed cognition. Cognitive artifacts that are related to operating room (OR) scheduling include: the availabilities sheet, master schedule, OR graph, and OR board. All provide a ""way in"" to understand how teams in the acute care setting dynamically plan and manage the balance between demand for care and the resources available to provide it. This work has import for the way that information technology supports the organization, management, and use of healthcare resources. Better computer-supported cognitive artifacts will benefit patient safety by making teamwork processes, planning, communications, and resource management more resilient.","Cognition,
Scheduling,
Resource management,
Medical services,
Health and safety,
Humans,
Displays,
Availability,
Information technology,
Technology management"
Reducing Energy Consumption of Disk Storage Using Power-Aware Cache Management,"Reducing energy consumption is an important issue for data centers. Among the various components of a data center, storage is one of the biggest consumers of energy. Previous studies have shown that the average idle period for a server disk in a data center is very small compared to the time taken to spin down and spin up. This significantly limits the effectiveness of disk power management schemes. This paper proposes several power-aware storage cache management algorithms that provide more opportunities for the underlying disk power management schemes to save energy. More specifically, we present an off-line power-aware greedy algorithm that is more energy-efficient than Belady’s off-line algorithm (which minimizes cache misses only). We also propose an online power-aware cache replacement algorithm. Our trace-driven simulations show that, compared to LRU, our algorithm saves 16% more disk energy and provides 50% better average response time for OLTP I/O workloads. We have also investigated the effects of four storage cache write policies on disk energy consumption.","Energy consumption,
Cache storage,
Energy storage,
Energy management,
Power system management,
Costs,
Computer science,
Greedy algorithms,
Energy efficiency,
Delay"
Generating test cases from UML activity diagram based on Gray-box method,"Test case generation is the most important part of the testing efforts, the automation of specification based test case generation needs formal or semi-formal specifications. As a semi-formal modelling language, UML is widely used to describe analysis and design specifications by both academia and industry, thus UML models become the sources of test generation naturally. Test cases are usually generated from the requirement or the code while the design is seldom concerned, this paper proposes an approach to generate test cases directly from UML activity diagram using Gray-box method, where the design is reused to avoid the cost of test model creation. In this approach, test scenarios are directly derived from the activity diagram modelling an operation. Then all the information for test case generation, i.e. input/output sequence and parameters, the constraint conditions and expected object method sequence, is extracted from each test scenario. At last, the possible values of all the input/output parameters could be generated by applying category-partition method, and test suite could be systematically generated to find the inconsistency between the implementation and the design. A prototype tool named UMLTGF has been developed to support the above process.",
Six Learning Barriers in End-User Programming Systems,"As programming skills increase in demand and utility, the learnability of end-user programming systems is of utmost importance. However, research on learning barriers in programming systems has primarily focused on languages, overlooking potential barriers in the environment and accompanying libraries. To address this, a study of beginning programmers learning Visual Basic.NET was performed. This identified six types of barriers: design, selection, coordination, use, understanding, and information. These barriers inspire a new metaphor of computation, which provides a more learner-centric view of programming system design","Libraries,
Programming profession,
Feedback,
Utility programs,
Robot programming,
Computer languages,
Robot control,
Manufacturing,
Robot kinematics,
Prototypes"
Resource Management for Rapid Application Turnaround on Enterprise Desktop Grids,"Desktop grids are popular platforms for high throughput applications, but due their inherent resource volatility it is difficult to exploit them for applications that require rapid turnaround. Efficient desktop grid execution of short-lived applications is an attractive proposition and we claim that it is achievable via intelligent resource selection. We propose three general techniques for resource selection: resource prioritization, resource exclusion, and task duplication. We use these techniques to instantiate several scheduling heuristics. We evaluate these heuristics through trace-driven simulations of four representative desktop grid configurations. We find that ranking desk-top resources according to their clock rates, without taking into account their availability history, is surprisingly effective in practice. Our main result is that a heuristic that uses the appropriate combination of resource prioritization, resource exclusion, and task replication achieves performance within a factor of 1.7 of optimal.","Resource management,
Application software,
Throughput,
Grid computing,
Job shop scheduling,
Processor scheduling,
Measurement,
Computer science,
Supercomputers,
Clocks"
Discovery of Web services in a federated registry environment,"The potential of a large scale growth of private and semi-private registries is creating the need for an infrastructure which can support discovery and publication over a group of autonomous registries. Recent versions of UDDI have made changes to accommodate interactions between distributed registries. In this paper, we discuss METEOR-S Web service Discovery Infrastructure, which provides an ontology-based infrastructure to access a group of registries that are divided based on business domains and grouped into federations. We also discuss how Web service discovery is carried out within a federation.","Web services,
Ontologies,
Manufacturing,
Computer science,
Large-scale systems,
Peer to peer computing,
Data privacy,
Project management,
Semantic Web,
IEEE services"
Probability grid: a location estimation scheme for wireless sensor networks,"Location information is of paramount importance for wireless sensor networks (WSN). The accuracy of the collected data can significantly be affected by an imprecise positioning of the event of interest. Despite the importance of location information, real system implementations that do not use specialized hardware for localization purposes have not been successful. In this paper, we propose a location estimation scheme that uses a probabilistic approach for estimating the location of a node in a sensor network. Our localization scheme makes use of additional knowledge of topology deployment. We assume a sensor network is deployed in a controlled manner, where the goal of the deployment is to form a grid topology. We evaluate our localization scheme through simulations, showing localization errors as low as 3% of radio range. We outperform similar localization schemes by obtaining 50% less error in localization. We also evaluate our localization solution and the DV-hop scheme in a real implementation, obtaining an average error in location of 79% of radio range, outperforming DV-hop by approximately 40%. We analyze the significant differences in performance between simulations and a real implementation and stress the importance of further evaluations of real implementations. The result is an effective and realistic protocol that works in an actual implementation, under certain assumptions, because it exploits deployment information.","Wireless sensor networks,
Network topology,
Hardware,
Acoustic sensors,
Sensor systems,
Radio communication,
Computer science,
Performance analysis,
Analytical models,
Stress"
Digital subtraction bowel cleansing for CT colonography using morphological and linear filtration methods,"We describe a method to perform postacquisition processing of computed tomography colonography (virtual colonoscopy) datasets that results in electronic removal of opacified, ingested bowel contents while reconstructing natural appearing boundaries of colon lumen and thereby permitting 3D (three-dimensional) visual analyses of the resulting colon models.","Virtual colonoscopy,
Filtration,
Colon,
Colonography,
Computed tomography,
Colonic polyps,
Cancer,
Pixel,
Image reconstruction,
Image segmentation"
Estimation bounds for localization,"The localization problem is fundamentally important for sensor networks. We study the Cramer-Rao lower bound (CRB) for two kinds of localization based on noisy range measurements. The first is anchored localization in which we know true positions of at least 3 nodes. We show some basic invariances of the CRB in this case and derive lower and upper bounds on the CRB which can be computed using only local information. The second is anchor-free localization where no absolute positions are known. Although the Fisher information matrix is singular, we derive a CRB-like bound on the total estimation variance. Finally, for both cases we discuss how the bounds scale to large networks under different models of wireless signal propagation.",
Extended multipoint relays to determine connected dominating sets in MANETs,"MPR (multipoint relays) (A. Qayyum et al., Jan. 2002) provides a localized and optimized way of broadcasting messages in a mobile ad hoc network (MANET). Using 2-hop neighborhood information, each node determines a small set of forward neighbors to relay messages. Selected forward nodes form a connected dominating set (CDS) to ensure full coverage. Adjih, Jacquet, and Viennot (2002) later proposed a novel localized algorithm to construct a small CDS based on the original MPR without any broadcast information. Such an approach is called source-independent or broadcast-independent. In this paper, we provide several extensions of the source-independent MPR to generate a smaller CDS using 3-hop neighborhood information to cover each node's 2-hop neighbor set. In addition, we extend the notion of coverage in the original MPR. We show that the extended MPR has a constant local approximation ratio compared with a logarithmic local ratio in the original MPR. The effectiveness of our approach is confirmed through a simulation study.",
Motion history for facial action detection in video,"Enabling computer systems to recognize human facial expressions is a challenging research problem with many applications in behavioral science, medicine, security, and human-machine interaction. Instead of being another approach to automatic detection of prototypic facial expressions of emotion, this work attempts to analyze subtle changes in facial behavior by recognizing facial action units (AU, i.e. atomic facial signals) that produce expressions. This work proposes AU recognition based upon multilevel motion history images (MMHI), which can be seen as an extension to temporal templates introduced by Bobick and Davis. By recording motion history at multiple time Intervals (i.e., multilevel MHI) instead of recording it once for the entire image sequence, we overcome the problem of self-occlusion which is inherent to temporal templates original definition. For automatic classification of an input MMHI-represented face video in terms of 21 AU classes, two approaches are compared: a sparse network of Winnows (SNoW) and a standard kNearest neighbour (kNN) classifier. The system was tested on two different databases, the MMI-Face-DB developed by the authors and the Cohn-Kanade face database.","History,
Face detection,
Motion detection,
Gold,
Face recognition,
Video recording,
Databases,
Humans,
Application software,
Behavioral science"
A peer-to-peer framework for caching range queries,"Peer-to-peer systems are mainly used for object sharing although they can provide the infrastructure for many other applications. We extend the idea of object sharing to data sharing on a peer-to-peer system. We propose a method, which is based on the multidimensional CAN system, for efficiently evaluating range queries. The answers of the range queries are cached at the peers and are used to answer future range queries. The scalability and efficiency of our design is shown through simulation.","Peer to peer computing,
Routing,
Network servers,
Computer science,
Application software,
Multidimensional systems,
Scalability,
Internet,
Fault tolerance,
Floods"
Autonomous local path planning for a mobile robot using a genetic algorithm,This work presents results of our work in development of a genetic algorithm based path-planning algorithm for local obstacle avoidance (local feasible path) of a mobile robot in a given search space. The method tries to find not only a valid path but also an optimal one. The objectives are to minimize the length of the path and the number of turns. The proposed path-planning method allows a free movement of the robot in any direction so that the path-planner can handle complicated search spaces.,
Noisy optimization problems - a particular challenge for differential evolution?,"The popularity of search heuristics has lead to numerous new approaches in the last two decades. Since algorithm performance is problem dependent and parameter sensitive, it is difficult to consider any single approach as of greatest utility overall problems. In contrast, differential evolution (DE) is a numerical optimization approach that requires hardly any parameter tuning and is very efficient and reliable on both benchmark and real-world problems. However, the results presented in this paper demonstrate that standard methods of evolutionary optimization are able to outperform DE on noisy problems when the fitness of candidate solutions approaches the fitness variance caused by the noise.","Search problems,
Computer science,
Intelligent systems,
Optimization methods,
Evolutionary computation,
Convergence,
Parameter estimation,
Data engineering,
Reliability engineering,
Systems engineering and theory"
An end-to-end adaptation protocol for layered video multicast using optimal rate allocation,"Layered transmission is a promising solution to video multicast over the heterogeneous Internet. However, since the number of layers is practically limited, noticeable mismatches would occur between the coarse-grained layer subscription levels and the heterogeneous and dynamic rate requirements from the receivers. In this paper, we show that such mismatch can be effectively reduced using a dynamic and fine-grained layer rate allocation on the sender's side. Specifically, we study the optimization criteria for rate allocation, and propose a metric called application-aware fairness index. This metric takes into consideration 1) the nonlinear relation between the perceived video quality and the delivered rate and 2) the degree of satisfaction for receivers with heterogeneous bandwidth requirements. We formulate the rate allocation into an optimization problem with the objective of maximizing the expected fairness index for all receivers in a multicast session. We then derive an efficient and scalable solution, and demonstrate that it can be seamlessly integrated into an end-to-end adaptation protocol, called hybrid adaptation layered multicast (HALM). This protocol takes advantage of the emerging fine-grained layered coding, and is fully compatible with the best-effort Internet infrastructure. Simulation and numerical results show that HALM noticeably improves the degree of fairness, and interacts with TCP traffic better than static allocation based protocols. More important, increasing the number of layers in HALM generally improves the degree of fairness; it is sufficient to obtain satisfactory performance with a small number of layers (three to five layers).","Multicast protocols,
Internet,
Video compression,
Bandwidth,
Computer science,
Subscriptions,
Numerical simulation,
Traffic control,
Teleconferencing,
Councils"
A large deviations perspective on ordinal optimization,"We consider the problem of optimal allocation of computing budget to maximize the probability of correct selection in the ordinal optimization setting. This problem has been studied in the literature in an approximate mathematical framework under the assumption that the underlying random variables have a Gaussian distribution. We use the large deviations theory to develop a mathematically rigorous framework for determining the optimal allocation of computing resources even when the underlying variables have general, nonGaussian distributions. Further, in a simple setting we show that when there exists an indifference zone, quick stopping rules may be developed that exploit the exponential decay rates of the probability of false selection. In practice, the distributions of the underlying variables are estimated from generated samples leading to performance degradation due to estimation errors. On a positive note, we show that the corresponding estimates of optimal allocations converge to their true values as the number of samples used for estimation increases to infinity.","Random variables,
Gaussian distribution,
Computational modeling,
Engineering management,
Computer science,
Resource management,
Distributed computing,
Degradation,
Estimation error,
H infinity control"
On the advantage of network coding for improving network throughput,"Given a data network with link capacities, we consider the throughput of the network for a multicast session involving a source node and a given set of terminals. It is known that network coding can improve the throughput of the network. We study the coding advantage, i.e. the ratio of the throughput using network coding to that without using network coding. We show that the maximum coding advantage for a given network is equal to the integrality gap of certain linear programming (LP) formulations for a Steiner tree. This holds for both directed as well as undirected networks. For directed networks, the coding advantage is equal to the integrality gap of the directed Steiner tree LP formulation; for undirected networks, the coding advantage is equal to the integrality gap of the bidirected cut LP formulation for the Steiner tree. This relates the coding advantage to well studied notions in combinatorial optimization. Further, this connection improves the known bounds on the coding advantage for both undirected as well directed networks.","Network coding,
Throughput,
Linear programming,
Computer networks,
Engineering profession,
Computer science,
Decoding,
US Department of Energy,
Polynomials"
Supporting software evolution through dynamically retrieving traces to UML artifacts,"The ability to trace new and changed requirements to their impacted components provides critical support for managing change in an evolving software system. Unfortunately numerous studies have shown the difficulties of maintaining links using traditional traceability methods. Information retrieval techniques can be used to dynamically generate traces and alleviate the need to maintain explicit links, however prior work in this area has focused primarily on establishing intra-requirement links or links between requirements and code. We compare several retrieval techniques for generating links between requirements, code, and UML models. Tracing to UML elements provides a higher perspective on the proposed change than would be possible if links were generated directly to the code and supports the growing trend towards model driven development. Our experiment returned better results for establishing links to UML artifacts than to code, suggesting the usefulness of establishing links to code via UML artifacts. We conclude the paper by discussing the implications of this approach for managing the evolution of a software system.",
The utility of explicit rate-based flow control in mobile ad hoc networks,"Flow control in a mobile ad hoc network (MANET) must face many new challenges such as frequent rerouting and bandwidth variation of the wireless links. TCP's implicit AIMD flow control performs poorly in this environment, because it often cannot keep up with the dynamics of the network. This paper explores the potential utility of explicit flow control in the MANET domain. To this end, we propose an end-to-end rate-based flow control scheme (called EXACT), where a flow's allowed rate is explicitly conveyed from intermediate routers to the end-hosts in each data packet's special control header. As a result, EXACT reacts quickly and precisely to re-routing and bandwidth variation, which makes it especially suitable for a dynamic MANET network and also discusses several supporting mechanisms required for such a scheme at the MAC and the transport layers. By ns-2 simulations, we show that EXACT outperforms TCP in terms of fairness and efficiency, especially in a highly dynamic MANET environment.","Intelligent networks,
Mobile ad hoc networks,
Bandwidth,
Peer to peer computing,
Internet,
Feedback,
Computer science,
Wireless communication,
Spread spectrum communication,
Communication system control"
N-gram-based detection of new malicious code,"The current commercial anti-virus software detects a virus only after the virus has appeared and caused damage. Motivated by the standard signature-based technique for detecting viruses, and a recent successful text classification method, we explore the idea of automatically detecting new malicious code using the collected dataset of the benign and malicious code. We obtained accuracy of 100% in the training data, and 98% in 3-fold cross-validation.","Viruses (medical),
Electronic mail,
Text categorization,
Computer viruses,
Frequency,
Privacy,
Computer security,
Laboratories,
Computer science,
Code standards"
An interacting multiple model probabilistic data association filter for cavity boundary extraction from ultrasound images,"This paper presents a novel segmentation technique for extracting cavity contours from ultrasound images. The problem is first discretized by projecting equispaced radii from an arbitrary seed point inside the cavity toward its boundary. The distance of the cavity boundary from the seed point is modeled by the trajectory of a moving object. The motion of this moving object is assumed to be governed by a finite set of dynamical models subject to uncertainty. Candidate edge points obtained along each radius include the measurement of the object position and some false returns. The modeling approach enables us to use the interacting multiple model estimator along with a probabilistic data association filter, for contour extraction. The convergence rate of the method is very fast because it does not employ any numerical optimization. The robustness and accuracy of the method are demonstrated by segmenting contours from a series of ultrasound images. The results are validated through comparison with manual segmentations performed by an expert. An application of the method in segmenting bone contours from computed tomography images is also presented.","Filters,
Data mining,
Ultrasonic imaging,
Image segmentation,
Position measurement,
Convergence of numerical methods,
Optimization methods,
Robustness,
Bones,
Computed tomography"
Imaging mass lesions by vibro-acoustography: modeling and experiments,"Vibro-acoustography is a recently developed imaging method based on the dynamic response of to low-frequency vibration produced by of ultrasound radiation force. The main differentiating feature of this method is that the image includes information about the dynamic properties of the object at the frequency of the vibration, which is normally much lower than the ultrasound frequency. Such information is not available from conventional ultrasound imaging. The purpose of this study is to evaluate the performance of vibro-acoustography in imaging mass lesions in soft tissue. Such lesions normally have elastic properties that are different from the surrounding tissue. Here, we first present a brief formulation of image formation in vibro-acoustography. Then we study vibro-acoustography of solid masses through computer simulation and in vitro experiments. Experiments are conducted on excised fixed liver tissues. Resulting images show lesions with enhanced boundary and often with distinctive textures relative to their background. The results suggest that vibro-acoustography maybe a clinically useful imaging modality for detection of mass lesions.",
A comparative study of two Boolean formulations of FPGA detailed routing constraints,"We present empirical analyses of two Boolean satisfiability (SAT) formulations of FPGA (field programmable gate array) detailed routing constraints. Boolean SAT-based routing transforms a routing problem into a Boolean SAT instance by rendering geometric routing constraints as an atomic Boolean function. The generated Boolean function is satisfiable if and only if the corresponding routing is possible. Two different Boolean SAT-based routing models are analyzed: the track-based and the route-based routing constraint model. The track-based routing model transforms a routing task into a net-to-track assignment problem, whereas the route-based routing model reduces it into a routability-checking problem with explicitly enumerated set of detailed routes for nets. In both models, routing constraints are represented as CNF Boolean satisfiability clauses. Through comparative experiments, we demonstrate that the route-based formulation yields an easier-to-evaluate and more scalable routability Boolean function than the track-based method. This is empirical evidence that a smart/efficient Boolean formulation can achieve significant performance improvement in real-world applications.","Field programmable gate arrays,
Logic programming,
Boolean functions"
Mobility-based d-hop clustering algorithm for mobile ad hoc networks,"This paper presents a mobility-based d-hop clustering algorithm (MobDHop), which forms variable-diameter clusters based on node mobility pattern in MANETs. We introduce a new metric to measure the variation of distance between nodes over time in order to estimate the relative mobility of two nodes. We also estimate the stability of clusters based on relative mobility of cluster members. Unlike other clustering algorithms, the diameter of clusters is not restricted to two hops. Instead, the diameter of clusters is flexible and determined by the stability of clusters. Nodes which have similar moving pattern are grouped into one cluster. The simulation results show that MobDHop has stable performance in randomly generated scenarios. It forms lesser clusters than Lowest-ID and MOBIC algorithm in the same scenario. In conclusion, MobDHop can be used to provide an underlying hierarchical routing structure to address the scalability of routing protocol in large MANETs.","Clustering algorithms,
Mobile ad hoc networks,
Network topology,
Stability,
Scalability,
Routing protocols,
Erbium,
Computer science,
Computer networks,
Time measurement"
Stochastic models for generating synthetic HTTP source traffic,New source-level models for aggregated HTTP traffic and a design for their integration with the TCP transport layer are built and validated using two large-scale collections of TCP/IP packet header traces. An implementation of the models and the design in the ns network simulator can be used to generate web traffic in network simulations,"Stochastic processes,
Traffic control,
Telecommunication traffic,
Peer to peer computing,
Statistics,
Network servers,
Hardware,
Internet,
Protocols,
Computer science"
Constructing diffeomorphic representations for the groupwise analysis of nonrigid registrations of medical images,"Groupwise nonrigid registrations of medical images define dense correspondences across a set of images, defined by a continuous deformation field that relates each target image in the group to some reference image. These registrations can be automatic, or based on the interpolation of a set of user-defined landmarks, but in both cases, quantifying the normal and abnormal structural variation across the group of imaged structures implies analysis of the set of deformation fields. We contend that the choice of representation of the deformation fields is an integral part of this analysis. This paper presents methods for constructing a general class of multi-dimensional diffeomorphic representations of deformations. We demonstrate, for the particular case of the polyharmonic clamped-plate splines, that these representations are suitable for the description of deformations of medical images in both two and three dimensions, using a set of two-dimensional annotated MRI brain slices and a set of three-dimensional segmented hippocampi with optimized correspondences. The class of diffeomorphic representations also defines a non-Euclidean metric on the space of patterns, and, for the case of compactly supported deformations, on the corresponding diffeomorphism group. In an experimental study, we show that this non-Euclidean metric is superior to the usual ad hoc Euclidean metrics in that it enables more accurate classification of legal and illegal variations.","Image analysis,
Biomedical imaging,
Biomedical engineering,
Interpolation,
Magnetic resonance imaging,
Brain,
Image segmentation,
Euclidean distance,
Law,
Legal factors"
Systematic integration of parameterized local search into evolutionary algorithms,"Application-specific, parameterized local search algorithms (PLSAs), in which optimization accuracy can be traded off with run time, arise naturally in many optimization contexts. We introduce a novel approach, called simulated heating, for systematically integrating parameterized local search into evolutionary algorithms (EAs). Using the framework of simulated heating, we investigate both static and dynamic strategies for systematically managing the tradeoff between PLSA accuracy and optimization effort. Our goal is to achieve maximum solution quality within a fixed optimization time budget. We show that the simulated heating technique better utilizes the given optimization time resources than standard hybrid methods that employ fixed parameters, and that the technique is less sensitive to these parameter settings. We apply this framework to three different optimization problems, compare our results to the standard hybrid methods, and show quantitatively that careful management of this tradeoff is necessary to achieve the full potential of an EA/PLSA combination.","Evolutionary computation,
Heating,
Constraint optimization,
Optimization methods,
Laboratories,
Space technology,
Genetics,
Process design,
Military computing,
Computer science"
Tolls for heterogeneous selfish users in multicommodity networks and generalized congestion games,"We prove the existence of tolls to induce multicommodity, heterogeneous network users that independently choose routes minimizing their own linear function of tolls versus latency to collectively form the traffic pattern of a minimum average latency flow. This generalizes both the previous known results of the existence of tolls for multicommodity, homogeneous users (Beckman et al., 1956) and for single commodity, heterogeneous users (Cole et al., 2003). Unlike previous proofs for single commodity users in general graphs, our proof is constructive - it does not rely on a fixed point theorem - and results in a simple polynomial-sized linear program to compute tolls when the number of different types of users is bounded by a polynomial. We show that our proof gives a complete characterization of flows that are enforceable by tolls. In particular, tolls exist to induce any traffic pattern that is the result of minimizing an arbitrary function from R/sup E(G)/ to the reals that is nondecreasing in each of its arguments. Thus, tolls exist to induce flows with minimum average weighted latency, minimum maximum latency, and other natural objectives. We give an exponential bound on tolls that is independent of the number of network users and the number of commodities. We use this to show that multicommodity tolls also exist when users are not from discrete classes, but instead define a general function that trades off latency versus toll preference. Finally, we show that our result extends to very general frameworks. In particular, we show that tolls exist to induce the Nash equilibrium of general nonatomic congestion games to be system optimal. In particular, tolls exist even when 1) latencies depend on user type; 2) latency functions are nonseparable functions of traffic on edges; 3) the latency of a set S is an arbitrary function of the latencies of the resources contained in S. Our exponential bound on size of tolls also holds in this case; and we give an example of a congestion game that shows this is tight; it requires tolls that are exponential in the size of the game.","Intelligent networks,
Delay,
Polynomials,
Telecommunication traffic,
Nash equilibrium,
Transportation,
Computer science,
Cost function"
Vision based topological Markov localization,"In this paper we study the problem of acquiring a topological model of indoors environment by means of visual sensing and subsequent localization given the model. The resulting model consists of a set of locations and neighborhood relationships between them. Each location in the model is represented by a collection of representative views and their associated descriptors selected from a temporally sub-sampled video stream captured by a mobile robot during exploration. We compare the recognition performance using global image histograms as well as local scale-invariant features as image descriptors, demonstrate their strengths and weaknesses and show how to model the spatial relationships between individual locations by a Hidden Markov Model. The quality of the acquired model is tested in the localization stage by means of location recognition: given a new view or a sequence of views, the most likely location where that view came from is determined.","Mobile robots,
Hidden Markov models,
Image representation,
Histograms,
Navigation,
Computer science,
Indoor environments,
Streaming media,
Image recognition,
Testing"
DP-SLAM 2.0,"Probabilistic approaches have proved very successful at addressing the basic problems of robot localization and mapping and they have shown great promise on the combined problem of simultaneous localization and mapping (SLAM). One approach to SLAM assumes relatively sparse, relatively unambiguous landmarks and builds a Kalman filter over landmark positions. Other approaches assume dense sensor data which individually are not very distinctive, such as those available from a laser range finder. In earlier work, we presented an algorithm called DP-SLAM, which provided a very accurate solution to the latter case by efficiently maintaining a joint distribution over robot maps and poses. The approach assumed an extremely accurate laser range finder and a deterministic environment. In this work we demonstrate an improved map representation and laser penetration model, an improvement in the asymptotic efficiency of the algorithm, and empirical results of loop closing on a high resolution map of a very challenging domain.","Simultaneous localization and mapping,
Robots,
Particle filters,
Laser modes,
Performance analysis,
Algorithm design and analysis,
Computer science,
Filtering,
Maintenance engineering,
Data engineering"
Para-CORDIC: parallel CORDIC rotation algorithm,"In this paper, the parallel COrdinate Rotation DIgital Computer (CORDIC) rotation algorithm in circular and hyperbolic coordinate is proposed. The most critical path of the conventional CORDIC rotation lies in the determination of rotation directions, which depends on the sign of the remaining angle after each iteration. Using the binary-to-bipolar recoding (BBR) and microrotation angle recoding techniques, the rotation directions can be predicted directly from the binary value of the initial input angle. The original sequential CORDIC rotations can be divided into two phases where the rotations in each phase can be executed in parallel. Our proposed architectures have a more regular and simpler prediction scheme compared to previous approaches. The critical path delay is reduced since the concurrently predicted rotations can be combined using multioperand carry-save addition structures.","Signal processing algorithms,
Delay,
Matrix decomposition,
Computer science,
Concurrent computing,
Computer architecture,
Iterative algorithms,
Two dimensional displays,
Vectors"
An ethnographic study of copy and paste programming practices in OOPL,"Although programmers frequently copy and paste code when they develop software, implications of common copy and paste (C&P) usage patterns have not been studied previously. We have conducted an ethnographic study in order to understand programmers' C&P programming practices and discover opportunities to assist common C&P usage patterns. We observed programmers using an instrumented Eclipse IDE and then analyzed why and how they use C&P operations. Based on our analysis, we constructed a taxonomy of C&P usage patterns. This paper presents our taxonomy of C&P usage patterns and discusses our insights with examples drawn from our observations. From our insights, we propose a set of tools that both can reduce software maintenance problems incurred by C&P and can better support the intents of commonly used C&P scenarios.",
Transatlantic Touch: A Study of Haptic Collaboration over Long Distance,"The extent to which the addition of haptic communication between human users in a shared virtual environment (SVE) contributes to the shared experience of the users has not received much attention in the literature. In this paper we describe a demonstration of and an experimental study on haptic interaction between two users over a network of significant physical distance and a number of network hops. A number of techniques to mitigate instability of the haptic interactions induced by network latency are presented. An experiment to evaluate the use of haptics in a collaborative situation mediated by a networked virtual environment is examined. The experimental subjects were to cooperate in lifting a virtual box together under one of four conditions in a between-groups design. Questionnaires were used to report the ease with which they could perform the task and the subjective levels of presence and copresence experienced. This extends earlier work by the authors to consider the possibility of haptic collaboration under real network conditions with a number of improvements. Using the technology described in this paper, transatlantic touch was successfully demonstrated between the Touch Lab at Massachusetts Institute of Technology, USA and Virtual Environments and Computer Graphics (VECG) lab at University College London (UCL), UK in 2002. It was also presented at the Internet II demonstration meeting in 2002 between University of Southern California and the Massachusetts Institute of Technology.",
Architecture and synthesis for on-chip multicycle communication,"For multigigahertz designs in nanometer technologies, data transfers on global interconnects take multiple clock cycles. In this paper, we propose a regular distributed register (RDR) microarchitecture, which offers high regularity and direct support of multicycle on-chip communication. The RDR microarchitecture divides the entire chip into an array of islands so that all local computation and communication within an island can be performed in a single clock cycle. Each island contains a cluster of computational elements, local registers, and a local controller. On top of the RDR microarchitecture, novel layout-driven architectural synthesis algorithms have been developed for multicycle communication, including scheduling-driven placement, placement-driven simultaneous scheduling with rebinding, and distributed control generation, etc. The experimentation on a number of real-life examples demonstrates promising results. For data flow intensive examples, we obtain a 44% improvement on average in terms of the clock period and a 37% improvement on average in terms of the final latency, over the traditional flow. For designs with control flow, our approach achieves a 28% clock-period reduction and a 23% latency reduction on average.","Clocks,
Design methodology,
Microarchitecture,
Delay effects,
Registers,
Communication system control,
Processor scheduling,
Wires,
Computer science,
Clustering algorithms"
Analysis of consensus partition in cluster ensemble,"In combination of multiple partitions, one is usually interested in deriving a consensus solution with a quality better than that of given partitions. Several recent studies have empirically demonstrated improved accuracy of clustering ensembles on a number of artificial and real-world data sets. Unlike certain multiple supervised classifier systems, convergence properties of unsupervised clustering ensembles remain unknown for conventional combination schemes. In this paper, we present formal arguments on the effectiveness of cluster ensemble from two perspectives. The first is based on a stochastic partition generation model related to re-labeling and consensus function with plurality voting. The second is to study the property of the ""mean"" partition of an ensemble with respect to a metric on the space of all possible partitions. In both the cases, the consensus solution can be shown to converge to a true underlying clustering solution as the number of partitions in the ensemble increases. This paper provides a rigorous justification for the use of cluster ensemble.",
Domo: a force sensing humanoid robot for manipulation research,"Humanoid robots found in research and commercial use today typically lack the ability to operate in unstructured and unknown environments. Force sensing and compliance at each robot joint can allow the robot to safely act in these environments. However, these features can be difficult to incorporate into robot designs. We present a new force sensing and compliant humanoid under development in the humanoid robotics group at MIT CSAIL. The robot, named Domo, is to be a research platform for exploring issues in general dexterous manipulation, visual perception, and learning. In this paper we describe aspects of the design, detail proposed research directions for the robot, and illustrate how the design of humanoid robots can be informed by the desired research goals.","Humanoid robots,
Robot sensing systems,
Force control,
Force sensors,
Visual perception,
Tactile sensors,
Neck,
Firewire,
Computer science,
Intelligent robots"
Estimating topology preserving and smooth displacement fields,"We propose a method for enforcing topology preservation and smoothness onto a given displacement field. We first analyze the conditions for topology preservation on two- and three-dimensional displacement fields over a discrete rectangular grid. We then pose the problem of finding the closest topology preserving displacement field in terms of its complete set of gradients, which we later solve using a cyclic projections framework. Adaptive smoothing of a displacement field is then formulated as an extension of topology preservation, via constraints imposed on the Jacobian of the displacement field. The simulation results indicate that this technique is a fast and reliable method to estimate a topology preserving displacement field from a noisy observation that does not necessarily preserve topology. They also show that the proposed smoothing method can render morphometric analysis methods that are based on displacement field of shape transformations more robust to noise without removing important morphologic characteristics.","Topology,
Jacobian matrices,
Biomedical imaging,
Smoothing methods,
Noise shaping,
Image analysis,
Radiology,
Monitoring,
Deformable models,
Rendering (computer graphics)"
iPass: an incentive compatible auction scheme to enable packet forwarding service in MANET,"In a public mobile ad hoc network (MANET), users may be selfish and refuse to forward packets for other users. Therefore, an incentive mechanism must be in place. We adopt the ""pay for service"" model of cooperation, and propose an auction-based incentive scheme (called iPass) to enable cooperative packet forwarding behavior in MANET. Each flow pays the market price of packet forwarding service to the intermediate routers. The resource allocation mechanism in our scheme is based on the generalized Vickrey auction with reserve pricing. We prove that in our scheme, user's truthful bidding of utility remains a dominant strategy, users and routers have incentive to participate in the scheme, and packet forwarding always leads to higher social welfare for the whole network. We design a signaling protocol to implement the scheme, and show that it can serve as an explicit rate-based flow control mechanism for the network. Therefore, iPass is a joint solution of incentive engineering and flow control in a noncooperative MANET. Simulation results show that iPass is able to determine the auction outcome quickly, and at the same time achieve the goals of flow control.",
Extraction of left ventricular contours from left ventriculograms by means of a neural edge detector,"We propose a method for extracting the left ventricular (LV) contours from left ventriculograms by means of a neural edge detector (NED) in order to extract the contours which accord with those traced by a cardiologist. The NED is a supervised edge detector based on a modified multilayer neural network, and is trained by use of a modified back-propagation algorithm. The NED can acquire the function of a desired edge detector through training with a set of input images and the desired edges obtained from the contours traced by a cardiologist. The proposed contour-extraction method consists of 1) detection of ""subjective edges"" by use of the NED; 2) extraction of rough contours by use of low-pass filtering and edge enhancement; and 3) a contour-tracing method based on the contour candidates synthesized from the edges detected by the NED and the rough contours. Through experiments, it was shown that the proposed method was able to extract the contours in agreement with those traced by an experienced cardiologist, i.e., we achieved an average contour error of 6.2% for left ventriculograms at end-diastole and an average difference between the ejection fractions obtained from the manually traced contours and those obtained from the computer-extracted contours of 4.1%.","Image edge detection,
Detectors,
Cardiology,
Computed tomography,
Multi-layer neural network,
Neural networks,
X-ray imaging,
Low pass filters,
Filtering,
Computer errors"
Efficient and robust multicast routing in mobile ad hoc networks,"We present the protocol for unified multicasting through announcements (PUMA) in ad-hoc networks, which establishes and maintains a shared mesh for each multicast group, without requiring a unicast routing protocol or the preassignment of cores to groups. PUMA achieves a high data delivery ratio with very limited control overhead, which is almost constant for a wide range of network conditions. Using simulations in Qualnet 3.5, we compare PUMA with ODMRP and MAODV, which are representatives of mesh-based and tree-based multicast routing in ad hoc networks. The results from a wide range of scenarios of varying mobility, group members, number of senders, traffic load, and number of multicast groups show that PUMA attains higher packet delivery ratios than ODMRP and MAODV, while incurring far less control overhead.",
Shadow detection and removal for traffic images,"Shadow detection and removal is an important task when dealing with outdoor images. Shadows cast by objects together with the objects form distorted figures. Furthermore, separate objects can be connected through shadows. Both can confuse object recognition systems. In this paper, an effective method is presented for detecting and removing shadows from foreground figures. We assume that foreground figures have been extracted from the input image by some background subtraction method. A figure may contain an object with or without shadow or multiple objects connected by shadows. To begin, we decide whether there are shadows in a given figure. A method based on illumination assessment is developed for this purpose. Once shadows have been confirmed existing in the given figure, their locations and orientations are estimated. A number of points are-then sampled from the shadow candidates, from which attributes of shadow are computed. We do not remove shadows simply based on the computed attributes. The reason is twofold. First, the distribution of intensity within a shadow is not uniform. Second, shadows can be divided into cast and self shadows; only cast shadows are to be removed. To deal with the first issue, we recover object shapes progressively instead of directly removing shadows. The second issue is resolved based on the observation that self shadows possess denser distributions of texture than cast shadows in our application. A number of experiments have been performed. The results revealed the applicability of the proposed technique.",
An evaluation of clone detection techniques for crosscutting concerns,"Code implementing a crosscutting concern is often spread over many different parts of an application. Identifying such code automatically greatly improves both the maintainability and the evolvability of the application. First of all, it allows a developer to more easily find the places in the code that must be changed when the concern changes, and thus makes such changes less time consuming and less prone to errors. Second, it allows a developer to refactor the code, so that it uses modern and more advanced abstraction mechanisms, thereby restoring its modularity. We evaluate the suitability of clone detection as a technique for the identification of crosscutting concerns. To that end, we manually identify four specific concerns in an industrial C application, and analyze to what extent clone detection is capable of finding these concerns. We consider our results as a stepping stone toward an automated ""concern miner"" based on clone detection.",
Scene text extraction in natural scene images using hierarchical feature combining and verification,"We propose a method that extracts text regions in natural scene images using low-level image features and that verifies the extracted regions through a high-level text stroke feature. Then the two level features are combined hierarchically. The low-level features are color continuity, gray-level variation and color variance. The color continuity is used since most of the characters in a text region have the same color, and the gray-level variation is used since the text strokes are distinctive to the background in their gray-level values. Also, the color variance is used since the text strokes are distinctive in their colors to the background, and this value is more sensitive than the gray-level variations. As a high level feature, text stroke is examined using multi-resolution wavelet transforms on local image areas and the feature vector is input to a SVM (support vector machine) for verification. We tested the proposed method with various kinds of the natural scene images and confirmed that extraction rates are high even in complex images.","Layout,
Data mining,
Color,
Text recognition,
Computer science,
Wavelet transforms,
Testing,
Graphics,
Image recognition,
Roads"
Solar-aware clustering in wireless sensor networks,"Energy conservation plays a crucial in wireless sensor networks since such networks are designed to be placed in hostile and nonaccessible areas. While battery-driven sensors will run out of battery sooner or later, the use of renewable energy sources such as solar power or gravitation may extend the lifetime of a sensor network. We propose to utilize solar power in wireless sensor networks and extend LEACH a well-known cluster-based protocol for sensor networks to become solar-aware. The presented simulation results show that making LEACH solar-aware significantly extends the lifetime of sensor networks.","Intelligent networks,
Wireless sensor networks,
Solar energy,
Silicon carbide,
Batteries,
Base stations,
Wireless application protocol,
Routing protocols,
Hardware,
Computer science"
Teaching data structures using competitive games,"A motivated student is more likely to be a successful learner. Interesting assignments encourage student learning by actively engaging them in the material. Active student learning is especially important in an introductory data structures course where students learn the fundamentals of programming. In this paper, the author describes a project for a data structures course based on the idea of competitive programming. Competitive programming motivates student learning by allowing students to evaluate and improve their programs throughout an assignment by competing their code against instructor-defined code and the code of other students in a tournament environment. Pedagogical results indicate that the combination of game development and friendly student competition is a significant motivator for increased student performance.",
Video repairing: inference of foreground and background under severe occlusion,"We propose a new method, video repairing, to robustly infer missing static background and moving foreground due to severe damage or occlusion from a video. To recover background pixels, we extend the image repairing method, where layer segmentation and homography blending are used to preserve temporal coherence and avoid flickering. By exploiting the constraint imposed by periodic motion and a subclass of camera and object motions, we adopt a two-phase approach to repair moving foreground pixels: In the sampling phase, motion data are sampled and regularized by 3D tensor voting to maintain temporal coherence and motion periodicity. In the alignment phase, missing moving foreground pixels are inferred by spatial and temporal alignment of the sampled motion data at multiple scales. We experimented our system with some difficult examples, where the camera can be stationary or in motion.",
Determining correspondence in 3-D MR brain images using attribute vectors as morphological signatures of voxels,"Finding point correspondence in anatomical images is a key step in shape analysis and deformable registration. This paper proposes an automatic correspondence detection algorithm for intramodality MR brain images of different subjects using wavelet-based attribute vectors (WAVs) defined on every image voxel. The attribute vector (AV) is extracted from the wavelet subimages and reflects the image structure in a large neighborhood around the respective voxel in a multiscale fashion. It plays the role of a morphological signature for each voxel, and our goal is, therefore, to make it distinctive of the respective voxel. Correspondence is then determined from similarities of AVs. By incorporating the prior knowledge of the spatial relationship among voxels, the ability of the proposed algorithm to find anatomical correspondence is further improved. Experiments with MR images of human brains show that the algorithm performs similarly to experts, even for complex cortical structures.",
Differential serialization for optimized SOAP performance,"The SOAP protocol has emerged as a Web Service communication standard, providing simplicity, robustness, and extensibility. SOAP's relatively poor performance threatens to limit its usefulness, especially for high-performance scientific applications. The serialization of outgoing messages, which includes conversion of in-memory data types to XML-based string format and the packing of this data into message buffers, is a primary SOAP performance bottleneck. We describe the design and implementation of differential serialization, a SOAP optimization technique that can help bypass the serialization step for messages similar to those previously sent by a SOAP client or previously returned by a SOAP-based Web Service. The approach requires no changes to the SOAP protocol. Our implementation and performance study demonstrate the technique *s potential, showing a substantial performance improvement over widely used SOAP toolkits that do not employ the optimization. We identify several factors that determine the usefulness and applicability of differential serialization, present a set of techniques for increasing the situations in which it can be used, and explore the design space of the approach.","Simple object access protocol,
Web services,
Robustness,
Space exploration,
Computer science,
Design optimization,
Service oriented architecture,
Engineering profession,
US Department of Energy,
Peer to peer computing"
Exploration with active loop-closing for FastSLAM,Acquiring models of the environment belongs to the fundamental tasks of mobile robots. In the last few years several researchers have focused on the problem of simultaneous localization and mapping (SLAM). Classic SLAM approaches are passive in the sense that they only process the perceived sensor data and do not influence the motion of the mobile robot. In this paper we present a novel and integrated approach that combines autonomous exploration with simultaneous localization and mapping. Our method uses a grid-based version of the FastSLAM algorithm and at each point in time considers actions to actively close loops during exploration. By re-entering already visited areas the robot reduces its localization error and this way learns more accurate maps. Experimental results presented in this paper illustrate the advantage of our method over pervious approaches lacking the ability to actively close loops.,"Simultaneous localization and mapping,
Uncertainty,
Mobile robots,
Robot sensing systems,
Computer science,
Robot control,
Vehicles,
Current measurement,
Orbital robotics,
Space exploration"
An architectural framework for providing reliability and security support,"This paper explores hardware-implemented error-detection and security mechanisms embedded as modules in a hardware-level framework called the reliability and security engine (RSE), which is implemented as an integral part of a modern microprocessor. The RSE interacts with the processor through an input/output interface. The CHECK instruction, a special extension of the instruction set architecture of the processor, is the interface of the application with the RSE. The detection mechanisms described here in detail are: (I) the memory layout randomization (MLR) module, which randomizes the memory layout of a process in order to foil attackers who assume a fixed system layout, (2) the data dependency tracking (DDT) module, which tracks the dependencies among threads of a process and maintains checkpoints of shared memory pages in order to rollback the threads when an offending (potentially malicious) thread is terminated, and (3) the instruction checker module (ICM), which checks an instruction for its validity or the control-flow of the program just as the instruction enters the pipeline for execution. Performance simulations for the studied modules indicate low overhead of the proposed solutions.",
Camera network calibration from dynamic silhouettes,"In this paper we present an automatic method for calibrating a network of cameras from only silhouettes. This is particularly useful for shape-from-silhouette or visual-hull systems, as no additional data is needed for calibration. The key novel contribution of this work is an algorithm to robustly compute the epipolar geometry from dynamic silhouettes. We use the fundamental matrices computed by this method to determine the projective reconstruction of the complete camera configuration. This is refined into a metric reconstruction using self-calibration. We validate our approach by calibrating a four camera visual-hull system from archive data where the dynamic object is a moving person. Once the calibration parameters have been computed, we use a visual-hull algorithm to reconstruct the dynamic object from its silhouettes.","Cameras,
Calibration,
Shape,
Image reconstruction,
Video sequences,
Robustness,
Computational geometry,
Computer science,
Layout,
Humans"
Reconstruction of sculpture from its profiles with unknown camera positions,"Profiles of a sculpture provide rich information about its geometry, and can be used for shape recovery under known camera motion. By exploiting correspondences induced by epipolar tangents on the profiles, a successful solution to motion estimation from profiles has been developed in the special case of circular motion. The main drawbacks of using circular motion alone, namely the difficulty in adding new views and part of the object always being invisible, can be overcome by incorporating arbitrary general views of the object and registering its new profiles with the set of profiles resulting from the circular motion. We describe a complete and practical system for producing a three-dimensional (3D) model from uncalibrated images of an arbitrary object using its profiles alone. Experimental results on various objects are presented, demonstrating the quality of the reconstructions using the estimated motion.","Cameras,
Motion estimation,
Image reconstruction,
Shape,
Information geometry,
Data mining,
Image sequences,
Councils,
Computer science,
Information systems"
Area-efficient instruction set synthesis for reconfigurable system-on-chip designs,,"System-on-a-chip,
Field programmable gate arrays,
Resource management,
Hardware,
Computer science,
Integer linear programming,
Program processors,
Silicon compiler,
Acceleration,
VLIW"
Maximizing data extraction in energy-limited sensor networks,"We examine the problem of maximizing data collection from an energy-limited store-and-extract wireless sensor network, which is analogous to the maximum lifetime problem of interest in continuous data-gathering sensor networks. One significant difference is that this problem requires attention to ""data-awareness"" in addition to ""energy-awareness."" We formulate the maximum data extraction problem as a linear program and present a 1+omega iterative approximation algorithm for it. As a practical distributed implementation we develop a faster greedy heuristic for this problem that uses an exponential metric based on the approximation algorithm. We then show through simulation results that the greedy heuristic incorporating this exponential metric performs near-optimally (within 1 to 20% of optimal, with low overhead) and significantly better than other shortest-path routing approaches, particularly when nodes are heterogeneous in their energy and data availability","Data mining,
Intelligent networks,
Routing,
Approximation algorithms,
Iterative algorithms,
Wireless sensor networks,
Batteries,
Ad hoc networks,
Computer science,
Computer networks"
Survey of fraud detection techniques,"Due to the dramatic increase of fraud which results in loss of billions of dollars worldwide each year, several modern techniques in detecting fraud are continually developed and applied to many business fields. Fraud detection involves monitoring the behavior of populations of users in order to estimate, detect, or avoid undesirable behavior. Undesirable behavior is a broad term including delinquency, fraud, intrusion, and account defaulting. This paper presents a survey of current techniques used in credit card fraud detection, telecommunication fraud detection, and computer intrusion detection. The goal of this paper is to provide a comprehensive review of different techniques to detect frauds.","Credit cards,
Intrusion detection,
Computer crime,
Telecommunication computing,
Subscriptions,
Computer science,
Data mining,
Computerized monitoring,
Computer networks,
Neural networks"
Consequences and limits of nonlocal strategies,"This paper investigates various aspects of the nonlocal effects that can arise when entangled quantum information is shared between two parties. A natural framework for studying nonlocality is that of cooperative games with incomplete information, where two cooperating players may share entanglement. Here, nonlocality can be quantified in terms of the values of such games. We review some examples of non-locality and show that it can profoundly affect the soundness of two-prover interactive proof systems. We then establish limits on nonlocal behavior by upper-bounding the values of several of these games. These upper bounds can be regarded as generalizations of the so-called Tsirelson inequality. We also investigate the amount of entanglement required by optimal and nearly optimal quantum strategies.","Upper bound,
Quantum entanglement,
Physics,
Quantum computing,
Information science,
Computer science,
Game theory,
Computational complexity"
Using students as subjects in requirements prioritization,"When conducting research in software engineering, the ultimate goal is usually to come up with results applicable in industry. However, it is not always possible to get industrial professionals to act as subjects in research studies. Instead, students are commonly used as representatives for professionals since they are more convenient to use. This paper presents an experiment on requirements prioritization that was performed with classroom students as subjects. The result of the experiment is compared to the results of similar prioritizations made in student projects, other classroom studies, literature and in an industrial case study. The objective of this comparison was to evaluate in which cases students successfully could be used as subjects in experimentation. The result indicates that students in a classroom environment are less suitable than students in projects as representatives for professionals in studies of this kind. Experience is often mentioned as a factor to determine whether students are suitable or not as subjects. However, commitment seems to be a more important factor in this study. It is concluded that it is important that further research is performed in order to evaluate under what circumstances students are suitable, and what factors that influence the suitability.","Software engineering,
Sampling methods,
Computer industry,
Performance evaluation,
Costs,
Programming profession,
Environmental factors"
Cooperative and progressive design experience for embedded systems,"This paper describes a cooperative experiential learning activity to develop embedded systems design skills. Student teams design, build, and troubleshoot a microcontroller-based project composed of common embedded systems peripherals, including input/output and electromechanical devices, industry standard communication networks, and complex digital integrated circuits. The design experience is progressive, requiring each successive subsystem to be incorporated without disturbing previously completed subsystems. Furthermore, the design experience is based on a problem-based learning approach that motivates student learning and develops skills required by the student in a future professional capacity. These skills include designing to specification, use of third-party intellectual property, teamwork, communication, and lifelong learning skills. The design experience was offered to a cohort in conjunction with lectures using active learning techniques. Course evaluations were obtained from students and external reviewers, and the results show that the course was well received and achieved its educational objectives.",
Statistical exploratory analysis of genetic algorithms,"Genetic algorithms have been extensively used and studied in computer science, yet there is no generally accepted methodology for exploring which parameters significantly affect performance, whether there is any interaction between parameters, and how performance varies with respect to changes in parameters. This paper presents a rigorous yet practical statistical methodology for the exploratory study of genetic and other adaptive algorithms. This methodology addresses the issues of experimental design, blocking, power calculations, and response curve analysis. It details how statistical analysis may assist the investigator along the exploratory pathway. As a demonstration of our methodology, we describe case studies using four well-known test functions. We find that the effect upon performance of crossover is pre-dominantly linear, while the effect of mutation is predominantly quadratic. Higher order effects are noted but contribute less to overall behavior. In the case of crossover, both positive and negative gradients are found suggesting the use of a maximum crossover rate for some problems and its exclusion for others. For mutation, optimal rates appear higher compared with earlier recommendations in the literature, while supporting more recent work. The significance of interaction and the best values for crossover and mutation are problem specific.","Algorithm design and analysis,
Genetic algorithms,
Statistical analysis,
Genetic mutations,
Design for experiments,
Computer science,
Adaptive algorithm,
Testing,
Iterative algorithms,
Australia"
Solution-adaptive magnetohydrodynamics for space plasmas: Sun-to-Earth simulations,"Space-environment simulations, particularly those involving space plasma, present significant computational challenges. Global computational models based on magnetohydrodynamics equations are essential to understanding the solar system's plasma phenomena, including the large-scale solar corona, the solar wind's interaction with planetary magnetospheres, comets, and interstellar medium, and the initiation, structure, and evolution of solar eruptive events.","Plasma simulation,
Magnetohydrodynamics,
Computational modeling,
Maxwell equations,
Magnetosphere,
Extraterrestrial measurements,
Large-scale systems,
Plasma measurements,
Physics,
Corona"
Characterising the use of a campus wireless network,"We present the results of an analysis of the usage of our new campus-wide wireless network. A week-long traffic trace was collected in January 2003, recording address and protocol information for every packet sent and received on the wireless network. A centralised authentication log was used to match packets with wireless access points. The trace was analysed to answer questions about where, when, how much, and for what our wireless network is being used. Such information is important in evaluating design principles and planning for future network expansion.","Wireless networks,
Computer science,
Wireless application protocol,
Access protocols,
Authentication,
Area measurement,
Wireless LAN,
Buildings,
Rivers,
Geography"
Using HMM based recognizers for writer identification and verification,"In this paper, we use HMM based recognizers for the identification and verification of persons based on their handwriting. For each writer, we build an individual recognizer and train it on text lines of that writer. This gives us recognizers that are experts on the handwriting of exactly one writer. In the identification or verification phase, a text line of unknown origin is presented to each of these recognizers and each one returns a transcription that includes the log-likelihood score for the considered input. These scores are sorted and the resulting ranking is used for both identification and verification. In an identification experiment in 96.56% of all cases the writer out of a set of 100 writers is correctly identified. Second, in a verification experiment using over 8,600 text lines from 120 writers an equal error rate (EER) of about 2.5% is achieved.","Hidden Markov models,
Handwriting recognition,
Text recognition,
Forgery,
Computer science,
Error analysis,
Writing,
Optimized production technology"
An ultra high throughput and power efficient TCAM-based IP lookup engine,"Ternary content-addressable memory (TCAM) is widely used in high-speed route lookup engines. However, restricted by the memory access speed, the route lookup engines for next-generation terabit routers demand exploiting parallelism among multiple TCAMs. Traditional parallel methods always incur excessive redundancy and high power consumption. We propose in this paper an original TCAM-based IP lookup scheme that achieves an ultra high lookup throughput and a high utilization of the memory while being power efficient. In our multichip scheme, we devise a load-balanced TCAM table construction algorithm together with an adaptive load balancing mechanism. The power efficiency is well controlled by decreasing the number of TCAM entries triggered in each lookup operation. Using 133 MHz TCAM chips and given 25% more TCAM entries than the original route table, the proposed scheme achieves a lookup throughput of up to 533 Mpps and is simple for ASIC implementation","Throughput,
Engines,
Energy consumption,
Costs,
Partitioning algorithms,
Computer science,
Parallel processing,
Load management,
Application specific integrated circuits,
Random access memory"
Dynamic overlay of scratchpad memory for energy minimization,"The memory subsystem accounts for a significant portion of the aggregate energy budget of contemporary embedded systems. Moreover, there exists a large potential for optimizing the energy consumption of the memory subsystem. Consequently, novel memories as well as novel algorithms for their efficient utilization are being designed. Scratchpads are known to perform better than caches in terms of power, performance, area and predictability. However, unlike caches they depend upon software allocation techniques for their utilization. We present an allocation technique which analyzes the application and inserts instructions to dynamically copy both code segments and variables onto the scratchpad at runtime. We demonstrate that the problem of dynamically overlaying scratchpad is an extension of the global register allocation problem. The overlay problem is solved optimally using ILP formulation techniques. Our approach improves upon the only previously known allocation technique for statically allocating both variables and code segments onto the scratchpad. Experiments report an average reduction of 34% and 18% in the energy consumption and the runtime of the applications, respectively. A minimal increase in code size is also reported.","Scanning probe microscopy,
Embedded system,
Runtime,
Permission,
Computer science,
Algorithm design and analysis,
Application software,
Memory management,
Power system management,
Energy dissipation"
Robust range-only beacon localization,Most autonomous underwater vehicle (AUV) systems rely on prior knowledge of beacon locations for localization. We present a system capable of navigating without prior beacon locations. Noise and outliers are major issues; we present a powerful outlier rejection method that imposes geometric constraints on measurements. We have successfully applied our algorithm to real-world data and have demonstrated navigation performance comparable to that of systems that assume known beacon locations.,
Learning weighted naive Bayes with accurate ranking,"Naive Bayes is one of most effective classification algorithms. In many applications, however, a ranking of examples are more desirable than just classification. How to extend naive Bayes to improve its ranking performance is an interesting and useful question in practice. Weighted naive Bayes is an extension of naive Bayes, in which attributes have different weights. This paper investigates how to learn a weighted naive Bayes with accurate ranking from data, or more precisely, how to learn the weights of a weighted naive Bayes to produce accurate ranking. We explore various methods: the gain ratio method, the hill climbing method, and the Markov chain Monte Carlo method, the hill climbing method combined with the gain ratio method, and the Markov chain Monte Carlo method combined with the gain ratio method. Our experiments show that a weighted naive Bayes trained to produce accurate ranking outperforms naive Bayes.",
Large vocabulary sign language recognition based on fuzzy decision trees,"The major difficulty for large vocabulary sign recognition lies in the huge search space due to a variety of recognized classes. How to reduce the recognition time without loss of accuracy is a challenging issue. In this paper, a fuzzy decision tree with heterogeneous classifiers is proposed for large vocabulary sign language recognition. As each sign feature has the different discrimination to gestures, the corresponding classifiers are presented for the hierarchical decision to sign language attributes. A one- or two- handed classifier and a hand-shaped classifier with little computational cost are first used to progressively eliminate many impossible candidates, and then, a self-organizing feature maps/hidden Markov model (SOFM/HMM) classifier in which SOFM being as an implicit different signers' feature extractor for continuous HMM, is proposed as a special component of a fuzzy decision tree to get the final results at the last nonleaf nodes that only include a few candidates. Experimental results on a large vocabulary of 5113-signs show that the proposed method dramatically reduces the recognition time by 11 times and also improves the recognition rate about 0.95% over single SOFM/HMM.",
Topology control in heterogeneous wireless networks: problems and solutions,"Previous work on topology control usually assumes homogeneous wireless nodes with uniform transmission ranges. In this paper, we propose two localized topology control algorithms for heterogeneous wireless multihop networks with nonuniform transmission ranges: directed relative neighborhood graph (DRNG) and directed local minimum spanning tree (DLMST). In both algorithms, each node selects a set of neighbors based on the locally collected information. We prove that (1) the topologies derived under DRNG and DLMST preserve the network connectivity; (2) the out degree of any node in the resulting topology by DLMST is bounded; while the out degree of nodes in the topology by DRNG is not bounded; and (3) the topologies generated by DRNG and DLMST preserve the network bi-directionality.",
"""Where am I?"" Acquiring situation awareness using a remote robot platform","Human-robot interaction with urban search and rescue (USAR) robots needs to provide operators with a means of maintaining situation awareness (SA), especially since the USAR operators usually cannot see the robots that they are directing. We used a technique from human-computer interaction known as usability testing, plus implicit and explicit SA measurement techniques, to investigate USAR operators' levels of SA and strategies for maintaining SA. We found that operators developed different SA strategies, spent an average of 30% of their time solely in SA activities, had less SA of the space behind the robot than in front or on the sides, did not use automatically-generated maps to gain SA, and had difficulty maintaining SA when in the autonomous mode.","Human robot interaction,
Orbital robotics,
Q measurement,
Usability,
Robotics and automation,
Human computer interaction,
Computer science,
Automatic testing,
Measurement techniques,
Computer errors"
Automated segmentation of lumbar vertebrae in digital videofluoroscopic images,"Low back pain is a significant problem in the industrialized world. Diagnosis of the underlying causes can be extremely difficult. Since mechanical factors often play an important role, it can be helpful to study the motion of the spine. Digital videofluoroscopy has been developed for this study and it can provide image sequences with many frames, but which often suffer due to noise, exacerbated by the very low radiation dosage. Thus, determining vertebra position within the image sequence presents a considerable challenge. There have been many studies on vertebral image extraction, but problems of repeatability, occlusion and out-of-plane motion persist. In this paper, we show how the Hough transform (HT) can be used to solve these problems. Here, Fourier descriptors were used to describe the vertebral body shape. This description was incorporated within our HT algorithm from which we can obtain affine transform parameters, i.e., scale, rotation and center position. The method has been applied to images of a calibration model and to images from two sequences of moving human lumbar spines. The results show promise and potential for object extraction from poor quality images and that models of spinal movement can indeed be derived for clinical application.",
BigSim: a parallel simulator for performance prediction of extremely large parallel machines,"Summary form only given. We present a parallel simulator - BigSim - for predicting performance of machines with a very large number of processors. The simulator provides the ability to make performance predictions for machines such as BlueGene/L, based on actual execution of real applications. We present this capability using case-studies of some application benchmarks. Such a simulator is useful to evaluate the performance of specific applications on such machines even before they are built. A sequential simulator may be too slow or infeasible. However, a parallel simulator faces problems of causality violations. We describe our scheme based on ideas from parallel discrete event simulation and utilize inherent determinacy of many parallel applications. We also explore techniques for optimizing such parallel simulations of machines with large number of processors on existing machines with fewer number of processors.","Predictive models,
Parallel machines,
Computational modeling,
Discrete event simulation,
Yarn,
Computer science,
Computer simulation,
Design optimization,
Storms,
Process design"
Corner block list representation and its application to floorplan optimization,"We propose to use a corner block list (CBL) representation for mosaic floorplans. In a mosaic floorplan, each room has only one block assigned to it. Thus, there is a unique corner room on the top right corner of the chip. Corner block deletion and corner block insertion keep the floorplan mosaic. Through a recursive deletion process, a mosaic floorplan can be converted to a representation that is named as CBL. Given a CBL, it takes only linear time to construct the floorplan. The CBL is used for the application to very large-scale integration floorplan and building block placement. We adopt a simulated annealing process for the optimization. Soft blocks and the aspect ratio of the chip are taken into account in the optimization process. The experimental results demonstrate that the algorithm is quite promising.","Simulated annealing,
Large scale integration,
Circuit simulation,
Very large scale integration,
Computer science,
Data structures,
Intellectual property,
Heuristic algorithms,
Genetic algorithms,
Stochastic processes"
A trust brokering system and its application to resource management in public-resource grids,"Summary form only given. We present a trust brokering system that operates in a peer-to-peer manner. The network of trust brokers operate by providing peer reviews in the form of recommendations regarding potential resource targets. One of the distinguishing features of our work is that it separately models the accuracy and honesty concepts. By separately modeling these concepts, our model is able to significantly improve the performance. We apply the trust brokering system to a resource manager to illustrate its utility in a public-resource grid environment. The simulations performed to evaluate the trust-aware resource management strategies indicate that high levels of ""robustness"" can be attained by considering trust while allocating the resources.",
The Grid2003 production grid: principles and practice,"The Grid2003 Project has deployed a multivirtual organization, application-driven grid laboratory (""Grid3"") that has sustained for several months the production-level services required by physics experiments of the Large Hadron Collider at CERN (ATLAS and CMS), the Sloan Digital Sky Survey project, the gravitational wave search experiment LIGO, the BTeV experiment at Fermilab, as well as applications in molecular structure analysis and genome analysis, and computer science research projects in such areas as job and data scheduling. The deployed infrastructure has been operating since November 2003 with 27 sites, a peak of 2800 processors, work loads from 10 different applications exceeding 1300 simultaneous jobs, and data transfers among sites of greater than 2 TB/day. We describe the principles that have guided the development of this unique infrastructure and the practical experiences that have resulted from its creation and use. We discuss application requirements for grid services deployment and configuration, monitoring infrastructure, application performance, metrics, and operational experiences. We also summarize lessons learned.","Production,
Application software,
Laboratories,
Physics,
Large Hadron Collider,
Collision mitigation,
Genomics,
Bioinformatics,
Computer science,
Scheduling"
Quantifying 3-D vascular structures in MRA images using hybrid PDE and geometric deformable models,"The aim of this paper is to present a hybrid approach to accurate quantification of vascular structures from magnetic resonance angiography (MRA) images using level set methods and deformable geometric models constructed with 3-D Delaunay triangulation. Multiple scale filtering based on the analysis of local intensity structure using the Hessian matrix is used to effectively enhance vessel structures with various diameters. The level set method is then applied to automatically segment vessels enhanced by the filtering with a speed function derived from enhanced MRA images. Since the goal of this paper is to obtain highly accurate vessel borders, suitable for use in fluid flow simulations, in a subsequent step, the vessel surface determined by the level set method is triangulated using 3-D Delaunay triangulation and the resulting surface is used as a parametric deformable model. Energy minimization is then performed within a variational setting with a first-order internal energy; the external energy is derived from 3-D image gradients. Using the proposed method, vessels are accurately segmented from MRA data.",
"Reputation = f(user ranking, compliance, verity)","The selection of Web services is typically based on both functional and nonfunctional attributes of the service, such as the quality of service (QoS) levels. Reputation, a widely acknowledged nonfunctional QoS attribute is currently expressed as the average of user ratings given to the service. However, this expression confines reputation to the subjective perception of the end user and is limited by the lack of an objective representation of performance history. In this paper, we address the need for a reputation mechanism that couples the subjective perception of the end user with the objective view of performance history. To represent performance history, we propose a novel QoS metric termed verity. Verity measures the degree of consistency exhibited by the service provider in delivering the quality levels laid out in the service contract, over a range of previous transactions. We express reputation as a composition of user rating, the compliance levels exhibited by the provider and the verity value. We contend that this reputation expression is a more viable attribute of quality than user rating alone.",
Toward intelligent driver-assistance and safety warning system,"A major problem associated with the rapid growth in automotive production is an increase in traffic congestion and accidents, especially in big cities of China. To solve the problem, the government has been increasing funds for improving the traffic infrastructure, enforcing traffic laws, and educating drivers about traffic regulations. In addition, research institutes have launched R&D projects in driver assistance and safety warning systems. In particular, in 1999, the Chinese Academy of Sciences' Intelligent Control and Systems Engineering Center started the Intelligent Vehicle Platforms project. The project aims to promote the use of intelligent technology for safe, efficient, and smart vehicles and to prototype vehicular electronic and sensory products and systems for the Chinese automotive industry. One of the project's key objectives is to develop a vehicular application-specific operating system (vASOS). The National Science Foundation of China and the CAS Knowledge Innovation Program support the project. Supported by the Vehicular Embedded Computing Platform project, the Xi'an Jiaotong University (XJTU-""Jiao Tong"" means transportation in Chinese) Institute of Artificial Intelligence and Robotics and the CAS have collaborated to develop intelligent driver-assistance and safety warning systems for passenger vehicles, particularly GPS-and vision-based systems.",
Improving soft-error tolerance of FPGA configuration bits,"Soft errors that change configuration bits of an SRAM based FPGA modify the functionality of the design. The proliferation of FPGA devices in various critical applications makes it important to increase their immunity to soft errors. In this work, we propose the use of an asymmetric SRAM (ASRAM) structure that is optimized for soft error immunity and leakage when storing a preferred value. The key to our approach is the observation that the configuration bitstream is composed of 87% of zeros across different designs. Consequently, the use of ASRAM cell optimized for storing a zero (ASRAM-0) reduces the failure in time by 25% as compared to the original design. We also present an optimization that increases the number of zeros in the bitstream while preserving the functionality.","Field programmable gate arrays,
Random access memory,
Routing,
Computer errors,
Capacitance,
Error analysis,
Table lookup,
Multiplexing,
Computer science,
Design engineering"
Decentralized access point selection architecture for wireless LANs,"Wireless LANs have been widely deployed, and multiple access points (APs) are likely to become much more available for wireless stations (STAs) which can roam from one AP to another by some rule. We focus on an efficient and fair way to use the wireless access resources provided by multiple APs. In particular, our major concern is to develop a decentralized way to enable each STA to select an appropriate available AP independently. A straightforward way is to select the AP with the strongest signal; however, this is shown to need further improvement. Hence, we propose decentralized AP selection strategies to achieve an efficient and fair share of wireless access resources, and evaluate them by simulations. The results show that the proposed strategies can attain an excellent throughput performance by use of the number of active STAs sharing each AP, even if STAs employ the strategies only when entering the wireless LAN. In addition, if STAs roam from one AP to another, the total throughput and fairness can be further improved very much.","Local area networks,
Wireless LAN,
Telecommunication traffic,
Throughput,
Computer architecture,
Information science,
Computer science,
Read only memory,
Airports,
Bandwidth"
Dynamic Drawing of Clustered Graphs,This paper presents an algorithm for drawing a sequence of graphs that contain an inherent grouping of their vertex set into clusters. It differs from previous work on dynamic graph drawing in the emphasis that is put on maintaining the clustered structure of the graph during incremental layout. The algorithm works online and allows arbitrary modifications to the graph. It is generic and can be implemented using a wide range of static force-directed graph layout tools. The paper introduces several metrics for measuring layout quality of dynamic clustered graphs. The performance of our algorithm is analyzed using these metrics. The algorithm has been successfully applied to visualizing mobile object software,"Clustering algorithms,
Application software,
Engineering drawings,
Layout,
Data visualization,
Algorithm design and analysis,
Animation,
Computer science,
Performance analysis,
Software algorithms"
Navigation for human-robot interaction tasks,"One major design goal in human-robot interaction is that the robots behave in an intelligent manner, preferably in a similar way as humans. This constraint must also be taken into consideration when the navigation system for the platform is developed. However, research in human-robot interaction is often restricted to other components of the system including gestures, manipulation, and speech. On the other hand, research for mobile robot navigation focuses primarily on the task of reaching a certain goal point in an environment. We believe that these two problems can not be treated separately for a personal robot that coexists with humans in the same surrounding. Persons move constantly while they are interacting with each other. Hence, also a robot should do that, which poses constraints on the navigation system. This type of navigation is the focus of this paper. Methods have been developed for a robot to join a group of people engaged in a conversation. Preliminary results show that the platform's moving patterns are very similar to the ones of the persons. Moreover, this dynamic interaction has been judged naturally by the test subjects, which greatly increases the perceived intelligence of the robot.","Navigation,
Intelligent robots,
Human robot interaction,
Mobile robots,
Indoor environments,
Numerical analysis,
Computer science,
Cities and towns,
Testing"
Finding axes of symmetry from potential fields,"This paper addresses the problem of detecting axes of bilateral symmetry in images. In order to achieve robustness to variation in illumination, only edge-gradient information is used. To overcome the problem of edge breaks, a potential field is developed from the edge map which spreads the information in the image plane. Pairs of points in the image plane are made to vote for their axes of symmetry with some confidence values. To make the method robust to overlapping objects, only local features in the form of Taylor coefficients are used for quantifying symmetry. We define an axis of symmetry histogram, which is used to accumulate the weighted votes for all possible axes of symmetry. To reduce the computational complexity of voting, a hashing scheme is proposed, wherein pairs of points, whose potential fields are too asymmetric, are pruned by not being counted for the vote. Experimental results indicate that the proposed method is fairly robust to edge breaks and is able to detect symmetries even when only 0.05% of the possible pairs are used for voting.","Robustness,
Voting,
Image edge detection,
Histograms,
Computer science,
Lighting,
Computational complexity,
Shape,
Mirrors,
Reflection"
High-resolution determination of soft tissue deformations using MRI and first-order texture correlation,"Mechanical factors such as deformation and strain are thought to play important roles in the maintenance, repair, and degeneration of soft tissues. Determination of soft tissue static deformation has traditionally only been possible at a tissue's surface, utilizing external markers or instrumentation. Texture correlation is a displacement field measurement technique which relies on unique image patterns within a pair of digital images to track displacement. The technique has recently been applied to MR images, indicating the possibility of high-resolution displacement and strain field determination within the mid-substance of soft tissues. However, the utility of MR texture correlation analysis may vary amongst tissue types depending on their underlying structure, composition, and contrast mechanism, which give rise to variations in texture with MRI. In this study, we investigate the utility of a texture correlation algorithm with first-order displacement mapping terms for use with MR images, and suggest a novel index of image ""roughness"" as a way to decrease errors associated with the use of texture correlation for intra-tissue strain measurement with MRI. We find that a first-order algorithm can significantly reduce strain measurement error, and that an image ""roughness"" index correlates with displacement measurement error for a variety of imaging conditions and tissue types.",
Mission-critical development with open source software: lessons learned,"Using open source software components in a mission-critical project not only can keep the project within budget but can also result in a more robust and flexible tool. When considering an open source component, prospective users should evaluate the project for several characteristics: maturity, longevity, and flexibility. For greatest benefit, the users should also build and maintain a strong working relationship with the component's developers. We compiled our experiences developing SAP into a developer's guide for those considering using open source in their mission-critical application. In addition to discussing how to evaluate open source components suitability for inclusion in a mission-critical application, the guide suggests strategies for working with open source development teams.","Mission critical systems,
Open source software,
Instruments,
Mars,
Propulsion,
Laboratories,
Data analysis,
Downlink,
Data visualization,
Planets"
Conjoined-Core Chip Multiprocessing,"Chip Multiprocessors (CMP) and Simultaneous Multi-threading (SMT) are two approaches that have been proposed to increase processor efficiency. We believe these two approaches are two extremes of a viable spectrum. Between these two extremes, there exists a range of possible architectures, sharing varying degrees of hardware between processors or threads. This paper proposes conjoined-core chip multiprocessing - topologically feasible resource sharing between adjacent cores of a chip multiprocessor to reduce die area with minimal impact on performance and hence improving the overall computational efficiency. It investigates the possible sharing of floating-point units, crossbar ports, instruction caches, and data caches and details the area savings that each kind of sharing entails. It also shows that the negative impact on performance due to sharing is significantly less than the benefits of reduced area. Several novel techniques for intelligent sharing of the hardware resources to minimize performance degradation are presented.","Surface-mount technology,
Yarn,
Hardware,
Multithreading,
Costs,
Process design,
Computer science,
Milling machines,
Computer architecture,
Resource management"
Email worm modeling and defense,"Email worms constitute one of the major Internet security problems. In this paper, we present an email worm model that accounts for the behaviors of email users by considering email checking time and the probability of opening email attachments. Email worms spread over a logical network defined by email address relationship, which plays an important role in determining the spreading dynamics of an email worm. Our observations suggest that the node degrees of an email network are heavy-tailed distributed. We compare email worm propagation on three topologies: power law, small world and random graph topologies; and then study how the topology affects immunization defense on email worms. The impact of the power law topology on the spread of email worms is mixed: email worms spread more quickly on a power law topology than on a small world topology or a random graph topology, but immunization defense is more effective on a power law topology than on the other two","Computer worms,
Network topology,
Immune system,
Computer science,
Internet,
Computer security,
Power engineering computing,
Earthquake engineering,
Books"
Three-dimensional myocardial strain reconstruction from tagged MRI using a cylindrical B-spline model,"In this paper, we present a new method for reconstructing three-dimensional (3-D) left ventricular myocardial strain from tagged magnetic resonance (MR) image data with a 3-D B-spline deformation model. The B-spline model is based on a cylindrical coordinate system that more closely fits the morphology of the myocardium than previously proposed Cartesian B-spline models and does not require explicit regularization. Our reconstruction method first fits a spatial coordinate B-spline displacement field to the tag line data. This displacement field maps each tag line point in the deformed myocardium back to its reference position (end-diastole). The spatial coordinate displacement field is then converted to material coordinates with another B-spline fit. Finally, strain is computed by analytically differentiating the material coordinate B-spline displacement field with respect to space. We tested our method with strains reconstructed from an analytically defined mathematical left ventricular deformation model and ten human imaging studies. Our results demonstrate that a quadratic cylindrical B-spline with a fixed number of control points can accurately fit a physiologically realistic range of deformations. The average 3-D reconstruction computation time is 20 seconds per time frame on a 450 MHz Sun Ultra80 workstation.","Myocardium,
Capacitive sensors,
Magnetic resonance imaging,
Spline,
Image reconstruction,
Magnetic field induced strain,
Deformable models,
Magnetic resonance,
Morphology,
Reconstruction algorithms"
Incremental verification and synthesis of discrete-event systems guided by counter examples,This article presents new approaches to system verification and synthesis based on subsystem verification and the novel combined use of counterexamples and heuristics to identify suitable subsystems incrementally. The scope of safety properties considered is limited to behavioral inclusion and controllability. The verification examples considered provide a comparison of the approaches presented with straightforward state exploration and an understanding of their applicability in an industrial context.,"Discrete event systems,
Counting circuits,
Controllability,
Control system synthesis,
Safety,
Control systems,
Electrical equipment industry,
Formal languages,
Search methods,
Computer science"
FTC-Charm++: an in-memory checkpoint-based fault tolerant runtime for Charm++ and MPI,"As high performance clusters continue to grow in size, the mean time between failures shrinks. Thus, the issues of fault tolerance and reliability are becoming one of the challenging factors for application scalability. The traditional disk-based method of dealing with faults is to checkpoint the state of the entire application periodically to reliable storage and restart from the recent checkpoint. The recovery of the application from faults involves (often manually) restarting applications on all processors and having it read the data from disks on all processors. The restart can therefore take minutes after it has been initiated. Such a strategy requires that the failed processor can be replaced so that the number of processors at checkpoint-time and recovery-time are the same. We present FTC-Charms ++, a fault-tolerant runtime based on a scheme for fast and scalable in-memory checkpoint and restart. At restart, when there is no extra processor, the program can continue to run on the remaining processors while minimizing the performance penalty due to losing processors. The method is useful for applications whose memory footprint is small at the checkpoint state, while a variation of this scheme - in-disk checkpoint/restart can be applied to applications with large memory footprint. The scheme does not require any individual component to be fault-free. We have implemented this scheme for Charms++ and AMPI (an adaptive version of MPl). This work describes the scheme and shows performance data on a cluster using 128 processors.","Fault tolerance,
Runtime,
Computer crashes,
Application software,
Checkpointing,
Mission critical systems,
Computer science,
Computational modeling,
Data engineering,
Programming profession"
Towards 3D mapping in large urban environments,"This paper describes work-in-progress aimed at generating dense 3D maps of urban environments using laser range data acquired from a moving platform. These maps display both fine-scale detail (resolving features only a few centimeters across) and large-scale consistency (typical maps are approximately 0.5 km on a side). In this paper, we sketch a basic 3D mapping algorithm (paying particular attention to practical engineering details) and present preliminary results acquired on the USC University Park campus using a Segway RMP vehicle.",
"Trust negotiations: concepts, systems, and languages","Trust negotiation is a promising approach for establishing trust in open systems like the Internet, where sensitive interactions may often occur among entities with no prior knowledge of each other. In this article, the authors present a model for trust negotiation systems, and delineate the desiderata that ideal trust negotiation systems should satisfy. In defining trust negotiation requirements, they consider two different issues, policy language requirements and system requirements. They then survey the most interesting proposals that have been presented so far and evaluate them with respect to the identified requirements. Finally, they outline future research directions and identify the open issues that still have to be explored.","Protection,
Access control,
Open systems,
Web and internet services,
Companies,
Information security,
Data security,
Databases,
Operating systems,
Proposals"
Slowing down Internet worms,"An Internet worm automatically replicates itself to vulnerable systems and may infect hundreds of thousands of servers across the Internet. It is conceivable that the cyber-terrorists may use a wide-spread worm to cause major disruption to our Internet economy. While much recent research concentrates on propagation models, the defense against worms is largely an open problem. We propose a distributed antiworm architecture (DAW) that automatically slows down or even halts the worm propagation. New defense techniques are developed based on behavioral difference between normal hosts and worm-infected hosts. Particularly, a worm-infected host has a much higher connection-failure rate when it scans the Internet with randomly selected addresses. This property allows DAW to set the worms apart from the normal hosts. We propose a temporal rate-limit algorithm and a spatial rate-limit algorithm, which makes the speed of worm propagation configurable by the parameters of the defense system. DAW is designed for an Internet service provider to provide the anti-worm service to its customers. The effectiveness of the new techniques is evaluated analytically and by simulations.",
Adding high availability and autonomic behavior to Web services,"Rapid acceptance of the Web Services architecture promises to make it the most widely supported and popular object-oriented architecture to date. One consequence is that a wave of mission-critical Web Services applications will certainly be deployed in coming years. Yet the reliability options available within Web Services are limited in important ways. To use a term proposed by IBM, Web Services systems need to become far more autonomic, configuring themselves, diagnosing faults, and managing themselves. High availability applications need more attention. Moreover, the scenarios in which such issues arise often entail very large deployments, raising questions of scalability. In this paper we propose a path by which the architecture could be extended in these respects.","Availability,
Web services,
Service oriented architecture,
Mission critical systems,
Scalability,
Computer architecture,
Computer science,
Security,
Delay,
Computer applications"
Ensemble clustering in medical diagnostics,"Ensemble techniques have been successfully applied in the context of supervised learning to increase the accuracy and stability of classification. Recently, analogous techniques for cluster analysis have been suggested. Research has demonstrated that, by combining a collection of dissimilar clusterings, an improved solution can be obtained. In this paper, we examine the potential of applying ensemble clustering techniques with a focus on the area of medical diagnostics. We present several ensemble generation and integration strategies, and evaluate each approach on a number of synthetic and real-world datasets. In addition, we show that diversity among ensemble members is necessary, but not sufficient to yield an improved solution without the selection of an appropriate integration method.","Medical diagnosis,
Clustering algorithms,
Partitioning algorithms,
Supervised learning,
Medical diagnostic imaging,
Stability,
Diversity reception,
Data mining,
Computer science,
Educational institutions"
Subpath protection for scalability and fast recovery in optical WDM mesh networks,"This paper investigates survivable lightpath provisioning and fast protection switching for generic mesh-based optical networks employing wavelength-division multiplexing (WDM). We propose subpath protection, which is a generalization of shared-path protection. The main ideas of subpath protection are: 1) to partition a large optical network into smaller domains and 2) to apply shared-path protection to the optical network such that an intradomain lightpath does not use resources of other domains and the primary/backup paths of an interdomain lightpath exit a domain (and enter another domain) through a common domain-border node. We mathematically formulate the routing and wavelength-assignment (RWA) problem under subpath protection for a given set of lightpath requests, prove that the problem is NP-complete, and develop a heuristic to find efficient solutions. Comparisons between subpath protection and shared-path protection on a nationwide network with dozens of wavelengths per fiber show that, for a modest sacrifice in resource utilization, subpath protection achieves improved survivability, much higher scalability, and significantly reduced fault-recovery time.","Protection,
Scalability,
Intelligent networks,
Optical fiber networks,
Wavelength division multiplexing,
WDM networks,
Mesh networks,
Resource management,
Laboratories,
Computer science"
Highly secure and efficient routing,"In this paper, we consider the problem of routing in an adversarial environment, where a sophisticated adversary has penetrated arbitrary parts of the routing infrastructure and attempts to disrupt routing. We present protocols that are able to route packets as long as at least one nonfaulty path exists between the source and the destination. These protocols have low communication overhead, low processing requirements, low incremental cost, and fast fault detection. We also present extensions to the protocols that penalize adversarial routers by blocking their traffic.","Fault detection,
Application software,
Computer science,
Costs,
Internet,
Robustness,
Routing protocols,
Hardware,
Authentication,
Uncertainty"
Outlier detection using k-nearest neighbour graph,We present an outlier detection using indegree number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance-based method are also proposed. We compare the methods with real and synthetic datasets. The results show that the proposed method achieves reasonable results with synthetic data and outperforms compared methods with real data sets with small number of observations.,"Breast cancer,
Computer science,
Probability density function,
Data mining,
Cancer detection,
Computer security,
Intrusion detection,
Statistical distributions,
Gaussian distribution,
Pattern recognition"
Group nearest neighbor queries,"Given two sets of points P and Q, a group nearest neighbor (GNN) query retrieves the point(s) of P with the smallest sum of distances to all points in Q. Consider, for instance, three users at locations q/sub 1/ q/sub 2/ and q/sub 3/ that want to find a meeting point (e.g., a restaurant); the corresponding query returns the data point p that minimizes the sum of Euclidean distances |pq/sub i/| for 1/spl les/i/spl les/3. Assuming that Q fits in memory and P is indexed by an R-tree, we propose several algorithms for finding the group nearest neighbors efficiently. As a second step, we extend our techniques for situations where Q cannot fit in memory, covering both indexed and nonindexed query points. An experimental evaluation identifies the best alternative based on the data and query properties.","Nearest neighbor searches,
Neural networks,
Computer science,
Information retrieval,
Spatial databases,
Euclidean distance,
Application software,
Content based retrieval,
Costs,
Indexing"
Predictive cardiac motion modeling and correction with partial least squares regression,"Respiratory-induced cardiac deformation is a major problem for high-resolution cardiac imaging. This paper presents a new technique for predictive cardiac motion modeling and correction, which uses partial least squares regression to extract intrinsic relationships between three-dimensional (3-D) cardiac deformation due to respiration and multiple one-dimensional real-time measurable surface intensity traces at chest or abdomen. Despite the fact that these surface intensity traces can be strongly coupled with each other but poorly correlated with respiratory-induced cardiac deformation, we demonstrate how they can be used to accurately predict cardiac motion through the extraction of latent variables of both the input and output of the model. The proposed method allows cross-modality reconstruction of patient specific models for dense motion field prediction, which after initial modeling can be used for real-time prospective motion tracking or correction. Detailed numerical issues related to the technique are discussed and the effectiveness of the motion and deformation modeling is validated with 3-D magnetic resonance data sets acquired from ten asymptomatic subjects covering the entire respiratory range.","Predictive models,
Least squares methods,
Deformable models,
High-resolution imaging,
Motion measurement,
Abdomen,
Surface reconstruction,
Image reconstruction,
Tracking,
Magnetic resonance"
Curse of mis-alignment in face recognition: problem and a novel mis-alignment learning solution,"In this paper, we present the rarely concerned curse of mis-alignment problem in face recognition, and propose a novel mis-alignment learning solution. Mis-alignment problem is firstly empirically investigated through systematically evaluating Fisherface's sensitivity to mis-alignment on the FERET face database by perturbing the eye coordinates, which reveals that the imprecise localization of the facial landmarks abruptly degenerates the Fisherface system. We explicitly define this problem as curse of mis-alignment to highlight its graveness. We then analyze the sources of curse of mis-alignment and group the possible solutions into three categories: invariant features, mis-alignment modeling, and alignment retuning. And then we propose a set of measurement combining the recognition rate with the alignment error distribution to evaluate the overall performance of specific face recognition approach with its robustness against the mis-alignment considered. Finally, a novel mis-alignment learning method, named E-Fisherface, is proposed to reinforce the recognizer to model the mis-alignment variations. Experimental results have impressively indicated the effectiveness of the proposed E-Fisherface in tackling the curse of mis-alignment problem.","Face recognition,
Robustness,
Face detection,
Computer vision,
Image recognition,
Facial features,
Pixel,
Computer science,
Databases,
Learning systems"
Efficient content location in wireless ad hoc networks,"The advances in wireless networking have enabled new paradigms in computing. An abundance of information and services provided by remote servers is expected to become available to wireless users. A fundamental issue in this environment is efficiently locating needed content. Such content may be in the form of files, services, or any other kind of data. In this paper, we describe an algorithm for efficient content location in location-aware ad hoc networks. The Geography-based Content Location Protocol (GCLP) makes use of physical location information to lower proactive traffic while reducing query cost. The results of our analysis show that GCLP performs favorably in terms of overhead and scalability.",
An Evaluation of Microarray Visualization Tools for Biological Insight,"High-throughput experiments such as gene expression microarrays in the life sciences result in large datasets. In response, a wide variety of visualization tools have been created to facilitate data analysis. Biologists often face a dilemma in choosing the best tool for their situation. The tool that works best for one biologist may not work well for another due to differences in the type of insight they seek from their data. A primary purpose of a visualization tool is to provide domain-relevant insight into the data. Ideally, any user wants maximum information in the least possible time. In this paper we identify several distinct characteristics of insight that enable us to recognize and quantify it. Based on this, we empirically evaluate five popular microarray visualization tools. Our conclusions can guide biologists in selecting the best tool for their data, and computer scientists in developing and evaluating visualizations",
PROP: a scalable and reliable P2P assisted proxy streaming system,"The demand of delivering streaming media content in the Internet has become increasingly high for scientific, educational, and commercial applications. Three representative technologies have been developed for this purpose, each of which has its merits and serious limitations. Infrastructure-based CDNs with dedicated network bandwidths and powerful media replicas can provide high quality streaming services but at a high cost. Server-based proxies are cost-effective but not scalable due to the limited proxy capacity and its centralized control. Client-based P2P networks are scalable but do not guarantee high quality streaming service due to the transient nature of peers. To address these limitations, we present a novel and efficient design of a scalable and reliable media proxy system supported by P2P networks. This system is called PROP abbreviated from our technical theme of ""collaborating and coordinating PROxy and its P2P clients"". Our objective is to address both scalability and reliability issues of streaming media delivery in a cost-effective way. In the PROP system, the clients' machines in an intranet are self-organized into a structured P2P system to provide a large media storage and to actively participate in the streaming media delivery, where the proxy is also embedded as an important member to ensure quality of streaming service. The coordination and collaboration in the system are efficiently conducted by our P2P management structure and replacement policies. We have comparatively evaluated our system by trace-driven simulations with synthetic workloads and with a real-life workload trace extracted from the media server logs in an enterprise network. The results show that our design significantly improves the quality of media streaming and the system scalability.","Streaming media,
Bandwidth,
Internet,
Scalability,
Network servers,
Web server,
Costs,
Cache storage,
Quality of service,
Computer science"
A template-based model for license plate recognition,"Using video camera to manage the cars is gradually adopted in many lands of applications, such as electric payment in the tailgate and car parking management. An automatic recognition model of automobile's license plate number is proposed in this paper. The designed system is expected to have high recognition accuracy and reliability such that the goal of automatic recognition can be achieved. We present a system to recognize the license number in the acquired image captured from a video camera. The recognition process of our system contains four major steps. First, the system tries to locate the probable position of the license plate within the acquired image by using gradient analysis and image processing. Second, our model estimates the image parameters needed to normalize the license plate and uses the cross-correlation to detect the skew of the license plate and rectify the tilt. Third, we use a template technique to recognize the characters in the license plate. Finally, we use the information gained from the previous step to analyze the probable license numbers. We illustrate the designing processes and give the experimental results from the proposed model. Based on the experimental results, the proposed system can effectively recognize the license number. The time needed to recognize a license plate takes only 1.5 seconds.","Licenses,
Cameras,
Monitoring,
Image recognition,
Automobiles,
Humans,
Image processing,
Vehicles,
Computer science,
Automotive engineering"
Tutorial on the biology of nanotopography,The aims of this short tutorial are fourfold: 1) to introduce readers unfamiliar with the field to major concepts in the field; 2) to inform the reader of major unresolved questions; 3) to inform readers of a few major sources of relevant literature; and 4) to place the subject in relation to its relevance to other areas of science and practical application.,"Tutorial,
Nanotopography,
Nanobioscience,
Cells (biology),
Adhesives,
Chemistry,
Surfaces,
Biological systems,
Nanostructured materials,
Biological cells"
Reconstruction and quantification of the carotid artery bifurcation from 3-D ultrasound images,"Three-dimensional (3-D) ultrasound is a relatively new technique, which is well suited to imaging superficial blood vessels, and potentially provides a useful, noninvasive method for generating anatomically realistic 3-D models of the peripheral vasculature. Such models are essential for accurate simulation of blood flow using computational fluid dynamics (CFD), but may also be used to quantify atherosclerotic plaque more comprehensively than routine clinical methods. In this paper, we present a spline-based method for reconstructing the normal and diseased carotid artery bifurcation from images acquired using a freehand 3-D ultrasound system. The vessel wall (intima-media interface) and lumen surfaces are represented by a geometric model defined using smoothing splines. Using this coupled wall-lumen model, we demonstrate how plaque may be analyzed automatically to provide a comprehensive set of quantitative measures of size and shape, including established clinical measures, such as degree of (diameter) stenosis. The geometric accuracy of 3-D ultrasound reconstruction is assessed using pulsatile phantoms of the carotid bifurcation, and we conclude by demonstrating the in vivo application of the algorithms outlined to 3-D ultrasound scans from a series of patient carotid arteries.","Image reconstruction,
Carotid arteries,
Bifurcation,
Ultrasonic imaging,
Shape measurement,
Computational fluid dynamics,
Size measurement,
Ultrasonic variables measurement,
Blood vessels,
Biomedical imaging"
"Linked faults in random access memories: concept, fault models, test algorithms, and industrial results","The analysis of linked faults (LFs), which are faults that influence the behavior of each other, such that masking can occur, has proven to be a source for new memory tests, characterized by an increased fault coverage. However, many newly reported fault models have not been investigated from the point-of-view of LFs. This paper presents a complete analysis of LFs, based on the concept of fault primitives, such that the whole space of LFs is investigated and accounted for and validated. Some simulated defective circuits, showing linked-fault behavior, will be also presented. The paper establishes detection conditions along with new tests to detect each fault class. The tests are merged into a single test March SL detecting all considered LFs. Preliminary test results, based on Intel advanced caches, show that its fault coverage is high as compared with all other traditional tests and that it detects some unique faults; this makes March SL very attractive industrially.",
Two supervised learning approaches for name disambiguation in author citations,"Due to name abbreviations, identical names, name misspellings, and pseudonyms in publications or bibliographies (citations), an author may have multiple names and multiple authors may share the same name. Such name ambiguity affects the performance of document retrieval, Web search, database integration, and may cause improper attribution to authors. We investigate two supervised learning approaches to disambiguate authors in the citations. One approach uses the naive Bayes probability model, a generative model; the other uses support vector machines (SVMs) [V. Vapnik (1995)] and the vector space representation of citations, a discriminative model. Both approaches utilize three types of citation attributes: coauthor names, the title of the paper, and the title of the journal or proceeding. We illustrate these two approaches on two types of data, one collected from the Web, mainly publication lists from homepages, the other collected from the DBLP citation databases.","Supervised learning,
Computer science,
Bibliographies,
Information retrieval,
Statistics,
Web search,
Databases,
Permission,
Software libraries,
Public healthcare"
Density connected clustering with local subspace preferences,"Many clustering algorithms tend to break down in high-dimensional feature spaces, because the clusters often exist only in specific subspaces (attribute subsets) of the original feature space. Therefore, the task of projected clustering (or subspace clustering) has been defined recently. As a solution to tackle this problem, we propose the concept of local subspace preferences, which captures the main directions of high point density. Using this concept, we adopt density-based clustering to cope with high-dimensional data. In particular, we achieve the following advantages over existing approaches: Our proposed method has a determinate result, does not depend on the order of processing, is robust against noise, performs only one single scan over the database, and is linear in the number of dimensions. A broad experimental evaluation shows that our approach yields results of significantly better quality than recent work on clustering high-dimensional data.","Clustering algorithms,
Partitioning algorithms,
Data mining,
Clustering methods,
Principal component analysis,
Computer science,
Noise robustness,
Databases,
Nearest neighbor searches,
Visualization"
Why evaluate ontology technologies? Because it works!,"We deal with two types of ontology evaluation, content evaluation and ontology technology evaluation. Evaluating content is a must for preventing applications from using inconsistent, incorrect, or redundant ontologies. It's unwise to publish an ontology that one or more software applications will use without first evaluating it. A well-evaluated ontology won't guarantee the absence of problems, but it makes its use safer. Similarly, evaluating ontology technology eases its integration with other software environments, ensuring a correct technology transfer from the academic to the industrial world. We also discuss ontology libraries, ontology tool, and formal evaluation of ontology quality.","Ontologies,
Semantic Web,
Libraries,
Taxonomy,
Engines,
Crawlers,
Computer science,
Knowledge engineering,
Knowledge representation,
Industrial relations"
Walking GPS: a practical solution for localization in manually deployed wireless sensor networks,"We present the design, implementation and evaluation of a simple, practical and cost effective localization solution, called walking GPS, that can be used in real, manual deployments of wireless sensor networks. We evaluate our localization solution exclusively in real deployments of MICA2 and XSM motes. Our experiments show that 100% of the deployed motes localize (i.e,. have a location position) and that the average localization errors are within 1 to 2 meters, due mainly to the limitations of the existing commercial GPS devices.","Legged locomotion,
Global Positioning System,
Intelligent networks,
Wireless sensor networks,
Costs,
Broadcasting,
Helium,
Computer science,
Bandwidth,
Defense industry"
Clone detection in source code by frequent itemset techniques,"In this paper we describe a new approach for the detection of clones in source code, which is inspired by the concept of frequent itemsets from data mining. The source code is represented as an abstract syntax tree in XML. Currently, such XML representations exist for instance for Java, C++, or PROLOG. Our approach is very flexible; it can be configured easily to work with multiple programming languages",
Triangulation and embedding using small sets of beacons,"Concurrent with recent theoretical interest in the problem of metric embedding, a growing body of research in the networking community has studied the distance matrix defined by node-to-node latencies in the Internet, resulting in a number of recent approaches that approximately embed this distance matrix into low-dimensional Euclidean space. Here we give algorithms with provable performance guarantees for beacon-based triangulation and embedding. We show that in addition to multiplicative error in the distances, performance guarantees for beacon-based algorithms typically must include a notion of slack - a certain fraction of all distances may be arbitrarily distorted. For metrics of bounded doubling dimension (which have been proposed as a reasonable abstraction of Internet latencies), we show that triangulation-based reconstruction with a constant number of beacons can achieve multiplicative error 1 + /spl delta/ on a 1 - /spl epsiv/ fraction of distances, for arbitrarily small constants /spl delta/ and /spl epsiv/. For this same class of metrics, we give a beacon-based embedding algorithm that achieves constant distortion on a 1 - /spl epsiv/ fraction of distances; this provides some theoretical justification for the success of the recent global network positioning algorithm of Ng and Zhang, and it forms an interesting contrast with lower bounds showing that it is not possible to embed all distances in a doubling metric with constant distortion. We also give results for other classes of metrics, as well as distributed algorithms that require only a sparse set of distances but do not place too much measurement load on any one node.",
Scientific Computations on Modern Parallel Vector Systems,"Computational scientists have seen a frustrating trend of stagnating application performance despite dramatic increases in the claimed peak capability of high performance computing systems. This trend has been widely attributed to the use of superscalar-based commodity components who’s architectural designs offer a balance between memory performance, network capability, and execution rate that is poorly matched to the requirements of large-scale numerical computations. Recently, two innovative parallel-vector architectures have become operational: the Japanese Earth Simulator (ES) and the Cray X1. In order to quantify what these modern vector capabilities entail for the scientists that rely on modeling and simulation, it is critical to evaluate this architectural paradigm in the context of demanding computational algorithms. Our evaluation study examines four diverse scientific applications with the potential to run at ultrascale, from the areas of plasma physics, material science, astrophysics, and magnetic fusion. We compare performance between the vector-based ES and X1, with leading superscalar-based platforms: the IBM Power3/4 and the SGI Altix. Our research team was the first international group to conduct a performance evaluation study at the Earth Simulator Center; remote ES access in not available. Results demonstrate that the vector systems achieve excellent performance on our application suite - the highest of any architecture tested to date. However, vectorization of a particle-in-cell code highlights the potential difficulty of expressing irregularly structured algorithms as data-parallel programs.",
Modeling multimodal human-computer interaction,"Incorporating the well-known Unified Modeling Language into a generic modeling framework makes research on multimodal human-computer interaction accessible to a wide range off software engineers. Multimodal interaction is part of everyday human discourse: We speak, move, gesture, and shift our gaze in an effective flow of communication. Recent initiatives such as perceptual and attentive user interfaces put these natural human behaviors in the center of the human-computer interaction (HCI). We've designed a generic modeling framework for specifying multimodal HCI using the Object Management Group's Unified Modeling Language. Because it's a well-known and widely supported standard - computer science departments typically cover it in undergraduate courses, and many books, training courses, and tools support it - UML makes it easier for software engineers unfamiliar with multimodal research to apply HCI knowledge, resulting in broader and more practical effects. Standardization provides a significant driving force for further progress because it codifies best practices, enables and encourages reuse, and facilitates interworking between complementary tools.","Unified modeling language,
Human computer interaction,
User interfaces,
Software standards,
Computer science,
Books,
Management training,
Software tools,
Knowledge engineering,
Standardization"
Inheritable genetic algorithm for biobjective 0/1 combinatorial optimization problems and its applications,"In this paper, we formulate a special type of multiobjective optimization problems, named biobjective 0/1 combinatorial optimization problem BOCOP, and propose an inheritable genetic algorithm IGA with orthogonal array crossover (OAX) to efficiently find a complete set of nondominated solutions to BOCOP. BOCOP with n binary variables has two incommensurable and often competing objectives: minimizing the sum r of values of all binary variables and optimizing the system performance. BOCOP is NP-hard having a finite number C(n,r) of feasible solutions for a limited number r. The merits of IGA are threefold as follows: 1) OAX with the systematic reasoning ability based on orthogonal experimental design can efficiently explore the search space of C(n,r); 2) IGA can efficiently search the space of C(n,r/spl plusmn/1) by inheriting a good solution in the space of C(n,r); and 3) The single-objective IGA can economically obtain a complete set of high-quality nondominated solutions in a single run. Two applications of BOCOP are used to illustrate the effectiveness of the proposed algorithm: polygonal approximation problem (PAP) and the problem of editing a minimum reference set for nearest neighbor classification (MRSP). It is shown empirically that IGA is efficient in finding complete sets of nondominated solutions to PAP and MRSP, compared with some existing methods.","Genetic algorithms,
Space exploration,
Nearest neighbor searches,
Approximation algorithms,
System performance,
Design for experiments,
Pareto optimization,
Shape,
Cost function,
Computer science"
Efficient tracking of moving objects with precision guarantees,"Sustained advances in wireless communications, geo-positioning, and consumer electronics pave the way to a kind of location-based service that relies on the tracking of the continuously changing positions of an entire population of service users. This type of service is characterized by large volumes of updates, giving prominence to techniques for location representation and update. This paper presents several representations, along with associated update techniques, that predict the present and future positions of moving objects. An update occurs when the deviation between the predicted and the actual position of an object exceeds a given threshold. For the case where the road network, in which an object is moving, is known, we propose a so-called segment-based policy that predicts an object's movement according to the road's shape. Map matching is used for determining the road on which an object is moving. Empirical performance studies based on a real road network and GPS logs from cars are reported.",
An efficient hidden Markov model training scheme for anomaly intrusion detection of server applications based on system calls,"Recently hidden Markov model (HMM) has been proved to be a good tool to model normal behaviours of privileged processes for anomaly intrusion detection based on system calls. However, one major problem with this approach is that it demands excessive computing resources in the HMM training process, which makes it inefficient for practical intrusion detection systems. In this paper a simple and efficient HMM training scheme is proposed by the innovative integration of multiple-observations training and incremental HMM training. The proposed scheme first divides the long observation sequence into multiple subsets of sequences. Next each subset of data is used to infer one sub-model, and then this sub-model is incrementally merged into the final HMM model. Our experimental results show that our HMM training scheme can reduce the training time by about 60% compared to that of the conventional batch training. The results also show that our HMM-based detection model is able to detect all denial-of-service attacks embedded in testing traces.","Hidden Markov models,
Intrusion detection,
Databases,
Application software,
Data processing,
Data mining,
Frequency,
Computer science,
Information technology,
Australia"
Analysis of lesion detectability in Bayesian emission reconstruction with nonstationary object variability,"Bayesian methods based on the maximum a posteriori principle (also called penalized maximum-likelihood methods) have been developed to improve image quality in emission tomography. To explore the full potential of Bayesian reconstruction for lesion detection, we derive simplified theoretical expressions that allow fast evaluation of the detectability of a lesion in Bayesian reconstruction. This work is built on the recent progress on the theoretical analysis of image properties of statistical reconstructions and the development of numerical observers. We explicitly model the nonstationary variation of the lesion and background without assuming that they are locally stationary. The results can be used to choose the optimum prior parameters for the maximum lesion detectability. The theoretical results are validated using Monte Carlo simulations. The comparisons show good agreement between the theoretical predictions and the Monte Carlo results. We also demonstrate that the lesion detectability can be reliably estimated using one noisy data set.","Lesions,
Object detection,
Bayesian methods,
Image reconstruction,
Maximum likelihood detection,
Maximum likelihood estimation,
Image quality,
Tomography,
Image analysis,
Monte Carlo methods"
Toward realistic haptic rendering of surface textures,"New sophisticated haptic-rendering algorithms let users experience virtual objects through touch. We systematically investigate the unrealistic behavior of virtual haptic textures. The emerging science of haptic rendering consists of delivering properties of physical objects through the sense of touch. Owing to the recent development of sophisticated haptic-rendering algorithms, users can now experience virtual objects through touch in many exciting applications, including surgical simulations, virtual prototyping, and data perceptualization. Haptics holds great promise to enrich the sensory attributes of virtual objects that these systems can produce. One area that has received increasing attention in the haptics community is haptic texture rendering, the goal of which is to introduce micro-geometry-scale features on object surfaces. Haptic objects rendered without textures usually feel smooth, and sometimes slippery. Appropriate haptic textures superimposed on haptic objects enhance an object's realism.","Haptic interfaces,
Surface texture,
Rendering (computer graphics),
Humans,
Psychology,
Virtual environment,
Databases,
Computational modeling,
Physics,
Resonance"
On optimal layering and bandwidth allocation for multisession video broadcasting,"For video broadcasting applications in a wireless environment, layered transmission is an effective approach to support heterogeneous receivers with varying bandwidth requirements. There are several important issues that need to be addressed for such layered video broadcasting systems. At the session level, it is not clear how to allocate bandwidth resources among competing video sessions. For a session with a given bandwidth, questions such as how to set up the video layering structure (i.e., number of layers) and how much bandwidth should be allocated to each layer remain to be answered. The solutions to these questions are further complicated by practical issues such as uneven popularity among video sessions and video layering overhead. This paper presents a systematic study to address these issues for a layered video broadcasting system in a wireless environment. The approach is to employ a generic utility function for each receiver under each video session. They cast the joint problem of layering and bandwidth allocation (among sessions and layers) into an optimization problem of total system utility among all the receivers. By using a simple two-step decomposition of intersession and intrasession optimization, they derive efficient algorithms to solve the optimal layering and bandwidth allocation problem. Practical issues for deploying the optimal algorithm in typical wireless networks are also discussed. Simulation results show that the optimal layering and bandwidth allocation improves the total system utility under various settings.","Channel allocation,
Multimedia communication,
Broadcasting,
Bandwidth,
Video compression,
Contracts,
Streaming media,
Resource management,
Councils,
Computer science"
Comparison of the dispersion properties of higher order FDTD schemes and equivalent-sized MRTD schemes,"The dispersion errors of higher order finite-difference time-domain (HO-FDTD) algorithms are compared to those of multiresolution time-domain (MRTD) algorithms that have equivalent spatial stencil sizes. Both scaling-function-based MRTD (S-MRTD) and wavelet-function-based MRTD (W-MRTD) schemes are considered. In particular, the MRTD schemes considered include the Coifman scaling functions and the Cohen-Daubechies-Feauveau (CDF) biorthogonal scaling and wavelet functions. In general, the HO-FDTD schemes are more accurate than their MRTD counterparts.","Finite difference methods,
Time domain analysis,
Dispersion,
Spatial resolution,
Taylor series,
Maxwell equations,
Computer science,
Magnetic fields,
Two dimensional displays"
JDBC checker: a static analysis tool for SQL/JDBC applications,"In data-intensive applications, it is quite common for the implementation code to dynamically construct database query strings and execute them. For example, a typical Java servlet Web service constructs SQL query strings and dispatches them over a JDBC connector to an SQL-compliant database. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. For example, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this paper, we describe JDBC Checker, a sound static analysis tool to verify the correctness of dynamically generated query strings. We have successfully applied the tool to find known and unknown defects in realistic programs using JDBC. We give a short description of our tool in this paper.",
New grid scheduling and rescheduling methods in the GrADS project,"Summary form only given. The goal of the Grid Application Development Software (GrADS) project is to provide programming tools and an execution environment to ease program development for the grid. We present recent extensions to the GrADS software framework: 1. A new approach to scheduling workflow computations, applied to a 3D image reconstruction application; 2. A simple stop/migrate/restart approach to rescheduling grid applications, applied to a QR 3. A process-swapping approach to rescheduling, applied to an N-body simulation. Experiments validating these methods were carried out on both the GrADS MacroGrid (a small but functional grid) and the MicroGrid (a controlled emulation of the grid) and the results were demonstrated at the SC2003 conference.","Grid computing,
Application software,
Computer science,
Distributed computing,
Contracts,
Processor scheduling,
Software tools,
Instruments,
Usability,
Runtime"
Performance comparison of security mechanisms for grid services,"Security is one of the most important features for grid services. There are several specifications used to add security to grid services, and some of them have been implemented and are in use. However, since most of the security mechanisms involve slow XML manipulations, adding security to grid services introduces a big performance penalty. In this paper, we introduce various security mechanisms and compare their features and performance. Our evaluation shows that transport level security (SSL) is faster than message level security, and should be used if there is no special requirement to use message level security. For message level security, WS-SecureConversation is generally fast, but has a scalability problem.","XML,
Simple object access protocol,
Sockets,
Java,
Web services,
Cryptography,
Computer science,
Computer security,
Scalability,
Buildings"
E-Science: the grid and the Semantic Web,"Over the past few years, researchers have been treated to two visions of the Internet's future. One is the Semantic Web, the next generation of World Wide Web technology. The second is grid computing, the next generation of internetworked processing. The Semantic Web is described as ""an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation"". Grid computing is defined as ""flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources"". We discuss their differences and, more importantly, their similarities, and explore the work needed to bring the two together. We focus particularly on scientists' needs, an area in which the high-power computing made possible by grid computing and the large-scale, distributed information management enabled by Semantic Web technologies will need to be integrated. In particular, this enables new approaches to interdisciplinary scientific endeavors made possible by these new technologies.","Semantic Web,
Grid computing,
Application software,
Ontologies,
Joining processes,
Service oriented architecture,
Web services,
Distributed computing,
Intelligent systems,
Spine"
The state-of-art and future trends in testing embedded memories,"According to the International Technology Roadmap for Semiconductors (ITRS 2001), embedded memories will continue to dominate the increasing system on chips (SoCs) content in the next years, approaching 94% in about 10 years. Therefore the memory yield will have a dramatical impact on the overall defect-per-million (DPM) level, hence on the overall SoC yield. Meeting a high memory yield requires understanding memory designs, modelling their faulty behaviors in the presence of defects, designing adequate tests and diagnosis strategies as well as efficient repair schemes. This paper presents the state of art in memory testing including fault modeling, test design, built-in-self-test (BIST) and built-in-self-repair (BISR). Further research challenges and opportunities are discussed in enabling testing (embedded) memories, which use deep submicron technologies.","Built-in self-test,
Fault diagnosis,
Costs,
System-on-a-chip,
Art,
Logic testing,
Algorithm design and analysis,
Mathematics,
Computer science,
Embedded computing"
Sensor localization with Ring Overlapping based on Comparison of Received Signal Strength Indicator,"Sensor localization has become an essential requirement for realistic applications over wireless sensor networks (WSN). Radio propagation irregularity and the stringent constraint on hardware cost, however, make localization in WSN very challenging. Range-free localizations are more appealing than range-based ones, since they do not depend on received signal strength to estimate distance and thus need simple and cheap hardware only. In this paper, we propose a ring-overlapping, range-free approach using the Ring Overlapping based on Comparison of Received Signal Strength Indicator (ROCRSSI). Simulation results have verified the high estimation accuracy achieved with ROCRSSI.","Wireless sensor networks,
Computer science,
Hardware,
Monitoring,
Helium,
Application software,
Target tracking,
Lighting control,
Costs,
Robustness"
The Diver project: interactive digital video repurposing,"The digital interactive video exploration and reflection (Diver) system lets users create virtual pathways through existing video content using a virtual camera and an annotation window for commentary. Users can post their Dives to the WebDiver server system to generate active collaboration, further repurposing, and discussion. Although our current work focuses on video records in learning research and educational practices, Diver can aid collaborative analysis of a broad array of visual data records, including simulations, 2D and 3D animations, and static works of art, photography, and text. In addition to the social and behavioral sciences, substantive application areas include medical visualization, astronomic data or cosmological models, military satellite intelligence, and ethnology and animal behavior. Diver-style user-centered video repurposing might also prove compelling for popular media with commercial application involving sports events, movies, television shows, and video gaming. Future technical development includes possible enhancements to the interface to support simultaneous display of multiple Dives on the same source content, a more fluid two-way relation between desktop Diver and WebDiver, and solutions to the current limitations on displaying and authoring time/space cropped videos in a browser context. These developments support the tool's fundamentally collaborative, communication-oriented nature.","Collaborative work,
Digital cameras,
Analytical models,
Medical simulation,
Animation,
Art,
Photography,
Data visualization,
Military satellites,
Animal behavior"
Universally composable protocols with relaxed set-up assumptions,"A desirable goal for cryptographic protocols is to guarantee security when the protocol is composed with other protocol instances. Universally composable (UC) protocols provide this guarantee in a strong sense: A protocol remains secure even when composed concurrently with an unbounded number of instances of arbitrary protocols. However, UC protocols for carrying out general tasks are known to exist only if a majority of the participants are honest, or in the common reference string (CRS) model where all parties are assumed to have access to a common string that is drawn from some pre-defined distribution. Furthermore, carrying out many interesting tasks in a UC manner and without honest majority or set-up assumptions is impossible, even if ideally authenticated communication is provided. A natural question is thus whether there exist more relaxed set-up assumptions than the CRS model that still allow for UC protocols. We answer this question in the affirmative: we propose alternative and relaxed set-up assumptions and show that they suffice for reproducing the general feasibility results for UC protocols in the CRS model. These alternative assumptions have the flavor of a ""public-key infrastructure"": parties have registered public keys, no single registration authority needs to be fully trusted, and no single piece of information has to be globally trusted and available. In addition, unlike known protocols in the CRS model, the proposed protocols guarantee some basic level of security even if the set-up assumption is violated.","Cryptographic protocols,
Access protocols,
Information security,
Radio access networks,
Public key,
Isolation technology,
Cryptography,
Computer science"
Generating efficient test sets with a model checker,"It is well-known that counterexamples produced by model checkers can provide a basis for automated generation of test cases. However when this approach is used to meet a coverage criterion, it generally results in very inefficient test sets having many tests and much redundancy. We describe an improved approach that uses model checkers to generate efficient test sets. Furthermore, the generation is itself efficient, and is able to reach deep regions of the statespace. We have prototyped the approach using the model checkers of our SAL system and have applied it to model-based designs developed in Stateflow. In one example, our method achieves complete state and transition coverage in a Stateflow model for the shift scheduler of a 4-speed automatic transmission with a single test case.","Automatic testing,
System testing,
Costs,
Character generation,
Hardware,
Software testing,
Time measurement,
Computer science,
Laboratories,
Prototypes"
Accurate template-based correction of brain MRI intensity distortion with application to dementia and aging,"This paper examines an alternative approach to separating magnetic resonance imaging (MRI) intensity inhomogeneity from underlying tissue-intensity structure using a direct template-based paradigm. This permits the explicit spatial modeling of subtle intensity variations present in normal anatomy which may confound common retrospective correction techniques using criteria derived from a global intensity model. A fine-scale entropy driven spatial normalisation procedure is employed to map intensity distorted MR images to a tissue reference template. This allows a direct estimation of the relative bias field between template and subject MR images, from the ratio of their low-pass filtered intensity values. A tissue template for an aging individual is constructed and used to correct distortion in a set of data acquired as part of a study on dementia. A careful validation based on manual segmentation and correction of nine datasets with a range of anatomies and distortion levels is carried out. This reveals a consistent improvement in the removal of global intensity variation in terms of the agreement with a global manual bias estimate, and in the reduction in the coefficient of intensity variation in manually delineated regions of white matter.","Magnetic resonance imaging,
Dementia,
Aging,
Anatomy,
Radiology,
Image segmentation,
Robustness,
Phased arrays,
Entropy,
Low pass filters"
WS-Negotiation: an overview of research issues,"A Web service is defined as an autonomous unit of application logic that provides either some business functionality or information to other applications through an Internet connection. Web services are based on a set of XML standards such as Simple Object Access Protocol (SOAP), Universal Description, Discovery and Integration (UDDI) and Web Services Description Language (WSDL). In particular, Web services discovery is the process of finding most appropriate Web services providers needed by a Web services requestor. One of the important issues in the discovery process is for Web services providers and Web services requestors to negotiate and find a solution that is acceptable to both sides. Thus, a more sophisticated business model with negotiation feature is required for this challenging research area. As there are increasing demands for negotiation technologies in the context of Web services, this paper proposes an independent declarative XML language called WS-Negotiation for Web services providers and requestors. In general, WS-Negotiation contains three parts: negotiation message, which describes the format for messages exchanged among negotiation parties, negotiation protocol, which describes the mechanism and rules that negotiation parties should follow, and negotiation decision making, which is an internal and private decision process based on a cost-benefit model or other strategies. This paper also presents a service level agreement (SLA) template model with different domain specific vocabularies for supporting different types of business negotiations in WS-Negotiation.",
A multiple LID routing scheme for fat-tree-based InfiniBand networks,"Summary form only given. In a cluster system, performance of the interconnection network greatly affects the computation power generated together from all interconnected processing nodes. The network architecture, the interconnection topology, and the routing scheme are three key elements dominating the performance of an interconnection network. InfiniBand architecture (IBA) is a new industry standard architecture. It defines a high-bandwidth, high-speed, and low-latency message switching network that is good for constructing high-speed interconnection networks for cluster systems. Fat-trees are well-adopted as the topologies of interconnection networks because of many nice properties they have. We proposed an m-port n-tree approach to construct fat-tree-based InfiniBand networks. Based on the constructed fat-tree-based InfiniBand networks, we proposed an efficient multiple LID (MLID) routing scheme. The proposed routing scheme is composed of processing node addressing scheme, path selection scheme, and forwarding table assignment scheme. To evaluate the performance of the proposed routing scheme, we have developed a software simulator for InfiniBand networks. The simulation results show that the proposed routing scheme runs well on the constructed fat-tree-based InfiniBand networks and is able to efficiently utilize the bandwidth and the multiple paths that fat-tree topology offers under InfiniBand architecture.","Routing,
Network topology,
Multiprocessor interconnection networks,
Switches,
Computer architecture,
Bandwidth,
Packet switching,
Computer networks,
Power system interconnection,
Computer science"
Semi-autonomous human-UAV interfaces for fixed-wing mini-UAVs,"We present several human-UAV interfaces that support real-time control of a small semi-autonomous UAV. These interfaces are designed for searching tasks and other missions that typically do not have a precise predetermined flight plan. We present a detailed analysis of a PDA-based interface and describe how our other interfaces relate to this analysis. We then offer quantative and qualitative performance comparisons of the interfaces, as well as an analysis of their possible real-world applications.",
WS-Net: a Petri-net based specification model for Web services,"The emerging paradigm of Web services opens a new way of Web application design and development to quickly develop and deploy Web applications by integrating independently published Web services components to conduct new business transactions. As research aiming at facilitating Web services integration and verification, WS-Net is an executable architectural description language incorporating the semantics of colored Petri-net with the style and understandability of object-oriented concepts. WS-Net describes each Web services component in three layers: interface net declares the services that the component provides to other components; interconnection net specifies the services that the component acquires to accomplish its mission; and interoperation net describes the internal operational behaviors of the component. As an architectural model that formalizes the architectural topology and behaviors of each Web services component as well as the entire system, WS-Net facilitates the verification and monitoring of Web services integration.","Web services,
Object oriented modeling,
Architecture description languages,
Monitoring,
Software architecture,
Computer science,
Application software,
Software systems,
Computer architecture,
Service oriented architecture"
Energy-aware demand paging on NAND flash-based embedded storages,"The ever-increasing requirement for high-performance and huge-capacity memories of emerging embedded applications has led to the widespread adoption of SDRAM and NAND flash memory as main and secondary memories, respectively. In particular, the use of energy consuming memory, SDRAM, has become burdensome in battery-powered embedded systems. Intuitively, though demand paging can be used to mitigate the increasing requirement of main memory size, its applicability should be deliberately elaborated since NAND flash memory has asymmetric operation characteristics in terms of performance and energy consumption. In this paper, we present an energy-aware demand paging technique to lower the energy consumption of embedded systems considering the characteristics of interactive embedded applications with large memory footprints. We also propose a flash memory-aware page replacement policy that can reduce the number of write and erase operations in NAND flash memory. With real-life workloads, we show the system-wide energy-delay product can be reduced by 15/spl sim/30% compared to the traditional shadowing architecture.","Energy storage,
SDRAM,
Memory management,
Embedded system,
Energy consumption,
Shadow mapping,
Algorithm design and analysis,
Operating systems,
Permission,
Computer science"
Comments on: A methodology for evaluation of boundary detection algorithms on medical images,"In this paper we analyze a result previously published about a comparison between two statistical tests used for evaluation of boundary detection algorithms on medical images. We conclude that the statement made by Chalana and Kim (1997) about the performance of the percentage test has a weak theoretical foundation, and according to our results, is not correct. In addition, we propose a one-sided hypothesis test for which the acceptance region can be determined in advance, as opposed to the two-sided confidence intervals proposed in the original paper, which change according to the estimated quantity.","Detection algorithms,
Biomedical imaging,
Testing,
Telecommunication standards,
Clouds,
Statistical analysis,
Optical computing,
Image analysis,
Algorithm design and analysis,
Medical tests"
Modality independent elastography (MIE): a new approach to elasticity imaging,"The correlation between tissue stiffness and health is an accepted form of organ disease assessment. As a result, there has been a significant amount of interest in developing methods to image elasticity parameters (i.e., elastography). The modality independent elastography (MIE) method combines a nonlinear optimization framework, computer models of soft-tissue deformation, and standard measures of image similarity to reconstruct elastic property distributions within soft tissue. In this paper, simulation results demonstrate successful elasticity image reconstructions in breast cross-sectional images acquired from magnetic resonance (MR) imaging. Results from phantom experiments illustrate its modality independence by reconstructing elasticity images of the same phantom in both MR and computed tomographic imaging units. Additional results regarding the performance of a new multigrid strategy to MIE and the implementation of a parallel architecture are also presented.","Elasticity,
Image reconstruction,
Magnetic resonance imaging,
Imaging phantoms,
Diseases,
Optimization methods,
Distributed computing,
Deformable models,
Measurement standards,
Biological tissues"
Optimizing tree reconfiguration for mobile target tracking in sensor networks,"Sensor nodes have limited sensing range and are not very reliable. To obtain accurate sensing data, many sensor nodes should he deployed and then the collaboration among them becomes an important issue. In W. Zhang and G. Cao, a tree-based approach has been proposed to facilitate sensor nodes collaborating in detecting and tracking a mobile target. As the target moves, many nodes in the tree may become faraway from the root of the tree, and hence a large amount of energy may be wasted for them to send their sensing data to the root. We address the tree reconfiguration problem. We formalize it as finding a min-cost convoy tree sequence, and solve it by proposing an optimized complete reconfiguration scheme and an optimized interception-based reconfiguration scheme. Analysis and simulation are conducted to compare the proposed schemes with each other and with other reconfiguration schemes. The results show that the proposed schemes are more energy efficient than others.","Target tracking,
Intelligent networks,
Sensor phenomena and characterization,
Collaboration,
Wireless sensor networks,
Energy efficiency,
Condition monitoring,
Aggregates,
Computer science,
Reliability engineering"
Smooth Renyi entropy and applications,"We introduce a new entropy measure, called smooth Renyi entropy. The measure characterizes fundamental properties of a random variable Z, such as the amount of uniform randomness that can be extracted from Z or the minimum length of an encoding of Z.","Entropy,
Random variables,
Probability distribution,
Zinc,
Length measurement,
Application software,
Computer science,
Additives"
EpiChord: parallelizing the chord lookup algorithm with reactive routing state management,"EpiChord is a DHT lookup algorithm that demonstrates that we can remove the O(logn)-state-per-node restriction on existing DHT topologies to achieve significantly better lookup performance and resilience using a novel reactive routing state maintenance strategy that amortizes network maintenance costs into existing lookups and by issuing parallel queries. Our technique allows us to design a new class of unlimited-state-per-node DHTs that is able to adapt naturally to a wide range of lookup workloads. EpiChord is able to achieve O(1)-hop lookup performance under lookup-intensive workloads and at least O(logn)-hop lookup performance under churn-intensive workloads even in the worst case (though it is expected to perform better on average). Our simulations show that our approach can reduce both lookup latencies and path lengths by a factor of 3 by issuing only 3 queries asynchronously in parallel per lookup. Furthermore, we show that we are able to achieve this result with minimal additional communication overhead and the number of messages generated per lookup is in general no more than that for the corresponding sequential chord lookup algorithm.","Routing,
Peer to peer computing,
Costs,
Telecommunication traffic,
Probes,
Delay,
Computer network management,
Computer science,
Artificial intelligence,
Laboratories"
The sampling-based neighborhood graph: an approach to computing and executing feedback motion strategies,"This paper presents a sampling-based approach to computing and executing feedback-motion strategies by defining a global navigation function over a collection of neighborhoods in configuration space. The collection of neighborhoods and their underlying connectivity structure are captured by a sampling-based neighborhood graph (SNG), on which navigation functions are built. The SNG construction algorithm incrementally places new neighborhoods in the configuration space, using distance information provided by existing collision-detection algorithms. A termination condition indicates the probability that a specified fraction of the space is covered. Our implementation illustrates the approach for rigid and articulated bodies with up to six-dimensional configuration spaces. Even over such spaces, rapid online responses to unpredictable configuration changes can be made in a few microseconds on standard PC hardware. Furthermore, if the goal is changed, an updated navigation function can be quickly computed without performing additional collision checking.","Feedback,
Navigation,
Robots,
Path planning,
Robotics and automation,
Computer science,
Hardware,
Motion control,
Motion planning,
Error correction"
A hierarchy of dynamic software views: from object-interactions to feature-interactions,"This work presents a hierarchy of dynamic views that is constructed using tools that analyze program execution traces. At the highest-level of abstraction are the feature-interaction and implementation views, which track the inter-feature dependencies as well as the classes that implement these features. At the middle-level is the class-interaction view, which is an abstract view of the object-interactions. The object-interaction view is the base view for all the views, and captures the low-level runtime interactions between objects. Two case studies are used to demonstrate the effectiveness of our work.","Software tools,
Runtime,
Software maintenance,
Computer science,
Educational institutions,
Software performance,
Computer bugs,
Performance analysis,
Documentation,
Text processing"
Optimal global conformal surface parameterization,"All orientable metric surfaces are Riemann surfaces and admit global conformal parameterizations. Riemann surface structure is a fundamental structure and governs many natural physical phenomena, such as heat diffusion and electro-magnetic fields on the surface. A good parameterization is crucial for simulation and visualization. This paper provides an explicit method for finding optimal global conformal parameterizations of arbitrary surfaces. It relies on certain holomorphic differential forms and conformal mappings from differential geometry and Riemann surface theories. Algorithms are developed to modify topology, locate zero points, and determine cohomology types of differential forms. The implementation is based on a finite dimensional optimization method. The optimal parameterization is intrinsic to the geometry, preserves angular structure, and can play an important role in various applications including texture mapping, remeshing, morphing and simulation. The method is demonstrated by visualizing the Riemann surface structure of real surfaces represented as triangle meshes.","Surface texture,
Visualization,
Optimization methods,
Surface structures,
Solid modeling,
Computer science,
Topology,
Computational geometry,
Conformal mapping,
Computational modeling"
A lumped parameter model of plasma focus,"In this paper, a fast running computer model of a plasma focus device is presented. The model is based on the snowplow model constructed with effective parameters validated against experimental results. A pinch model is included to calculate the temporal evolution of the focal variables. The resulting neutron yield predictions are compared against available data at different pressures, electrode length, and capacitor voltages from experimental measurements, finding good agreements. The model ultimately calculates the neutron production given the geometric parameters and the filling deuterium pressure.","Plasma devices,
Neutrons,
Electrodes,
Capacitors,
Voltage,
Length measurement,
Solid modeling,
Production,
Filling,
Deuterium"
Engineering in the biological substrate: information processing in genetic circuits,"We review the rapidly evolving efforts to analyze, model, simulate, and engineer genetic and biochemical information processing systems within living cells. We begin by showing that the fundamental elements of information processing in electronic and genetic systems are strikingly similar, and follow this theme through a review of efforts to create synthetic genetic circuits. In particular, we describe and review the ""silicon mimetic"" approach, where genetic circuits are engineered to mimic the functionality of semiconductor devices such as logic gates, latched circuits, and oscillators. This is followed with a review of the analysis, modeling, and simulation of natural and synthetic genetic circuits, which often proceed in a manner similar to that used for electronic systems. We conclude by presenting examples of naturally occurring genetic and biochemical systems that recently have been conceptualized in terms familiar to systems engineers. Our review of these newly forming fields of research demonstrates that the expertise and skills contained within electrical and computer engineering disciplines apply not only to design within biological systems, but also to the development of a deeper understanding of biological functionality. This review of these efforts points to the emergence of both engineering and basic science disciplines following parallel paths.","Genetic engineering,
Information processing,
Biological system modeling,
Analytical models,
Circuit simulation,
Substrates,
Biochemical analysis,
Information analysis,
Silicon,
Semiconductor devices"
Fast fully 3-D image reconstruction in PET using planograms,"We present a method of performing fast and accurate three-dimensional (3-D) backprojection using only Fourier transform operations for line-integral data acquired by planar detector arrays in positron emission tomography. This approach is a 3-D extension of the two-dimensional (2-D) linogram technique of Edholm. By using a special choice of parameters to index a line of response (LOR) for a pair of planar detectors, rather than the conventional parameters used to index a LOR for a circular tomograph, all the LORs passing through a point in the field of view (FOV) lie on a 2-D plane in the four-dimensional (4-D) data space. Thus, backprojection of all the LORs passing through a point in the FOV corresponds to integration of a 2-D plane through the 4-D ""planogram."" The key step is that the integration along a set of parallel 2-D planes through the planogram, that is, backprojection of a plane of points, can be replaced by a 2-D section through the origin of the 4-D Fourier transform of the data. Backprojection can be performed as a sequence of Fourier transform operations, for faster implementation. In addition, we derive the central-section theorem for planogram format data, and also derive a reconstruction filter for both backprojection-filtering and filtered-backprojection reconstruction algorithms. With software-based Fourier transform calculations we provide preliminary comparisons of planogram backprojection to standard 3-D backprojection and demonstrate a reduction in computation time by a factor of approximately 15.","Image reconstruction,
Positron emission tomography,
Detectors,
Radiology,
Fourier transforms,
Biomedical imaging,
Sensor arrays,
Two dimensional displays,
Filters,
Reconstruction algorithms"
Routing improvement using directional antennas in mobile ad hoc networks,"In this paper, we present the initial design and evaluation of two techniques for routing improvement using directional antennas in mobile ad hoc networks. First, we use directional antennas to bridge permanent network partitions by adaptively transmitting selected packets over a longer distance, still transmitting most packets a shorter distance. Second, in a network without permanent partitions, we use directional antennas to repair routes in use, when an intermediate node moves out of wireless transmission range along the route; by using the capability of a directional antenna to transmit packets over a longer distance, we bridge the route breakage caused by the intermediate node's movement, thus reducing packet delivery latency. Through simulations, we demonstrate the effectiveness of our design in the context of the dynamic source routing protocol (DSR).","Directional antennas,
Intelligent networks,
Mobile ad hoc networks,
Bridges,
Routing protocols,
Directive antennas,
Transmitting antennas,
Computer science,
Delay,
Wireless networks"
Active learning to recognize multiple types of plankton,"Active learning has been applied with support vector machines to reduce the data labeling effort in pattern recognition domains. However, most of those applications only deal with two class problems. In this paper, we extend the active learning approach to multiple class support vector machines. The experimental results from a plankton recognition system indicate that our approach often requires significantly less labeled images to maintain the same accuracy level as random sampling.","Marine vegetation,
Support vector machines,
Support vector machine classification,
Machine learning,
Pattern recognition,
Sampling methods,
Image recognition,
Image sampling,
Text categorization,
Computer science"
Distributed QoS guarantees for realtime traffic in ad hoc networks,"In this paper, we propose a new cross-layer framework, named QPART (QoS protocol for ad hoc realtime traffic), which provides QoS guarantees to real-time multimedia applications for wireless ad hoc networks. By adapting the contention window sizes at the MAC layer, QPART schedules packets of flows according to their unique QoS requirements. QPART implements priority-based admission control and conflict resolution to ensure that the requirements of admitted real time flows is smaller than the network capacity. The novelty of QPART is that it is robust to mobility and variances in channel capacity and imposes no control message overhead on the network.","Telecommunication traffic,
Intelligent networks,
Ad hoc networks,
Quality of service,
Resource management,
Protocols,
Communication system traffic control,
Application software,
Channel capacity,
Computer science"
Synthesis of full-adder circuit using reversible logic,"A reversible gate has the equal number of inputs and outputs and one-to-one mappings between input vectors and output vectors; so that, the input vector states can be always uniquely reconstructed from the output vector states. This correspondence introduces a reversible full-adder circuit that requires only three reversible gates and produces least number of ""garbage outputs "", that is two. After that, a theorem has been proposed that proves the optimality of the propounded circuit in terms of number of garbage outputs. An efficient algorithm is also introduced in this paper that leads to construct a reversible circuit.","Circuit synthesis,
Logic circuits,
DH-HEMTs,
Computer science,
Logic gates,
Very large scale integration,
Terminology,
Energy loss,
Physics"
Matrix Zoom: A Visual Interface to Semi-External Graphs,"In Web data, telecommunications traffic and in epidemiological studies, dense subgraphs correspond to subsets of subjects (i.e. users, patients) that share a collection of attributes values (i.e. accessed Web pages, email-calling patterns or disease diagnostic profiles). Visual and computational identification of these ""clusters"" becomes useful when domain experts desire to determine those factors of major influence in the formation of access and communication clusters or in the detection and contention of disease spread. With the current increases in graphic hardware capabilities and RAM sizes, it is more useful to relate graph sizes to the available screen real estate S and the amount of available RAM M, instead of the number of edges or nodes in the graph. We offer a visual interface that is parameterized by M and S and is particularly suited for navigation tasks that require the identification of subgraphs whose edge density is above certain threshold. This is achieved by providing a zoomable matrix view of the underlying data. This view is strongly coupled to a hierarchical view of the essential information elements present in the data domain. We illustrate the applicability of this work to the visual navigation of cancer incidence data and to an aggregated sample of phone call traffic","Navigation,
Cancer,
Web pages,
Diseases,
Read-write memory,
Visual databases,
Mathematics,
Computer science,
Telecommunication traffic,
Telecommunication computing"
Trust in virtual teams: towards an integrative model of trust formation,"Traditional models of trust have seen trust as being created as a result of a long history of interaction, but recent studies of trust in virtual teams have shown the existence of high initial trust among team members. This paper proposes an integrated model of trust that encompasses both the traditional view of trust and the swift trust found in virtual teams. Based on the dual process theories of cognition, we argue that individuals form trust attitudes via three distinct routes at different stages of a relationship: the peripheral route, the central route, and the habitual route, irrespective. In the initial stages of a relationship when individuals lack information about each other, they rely on peripheral cues (e.g., third party information, social categories, roles, and rules) to form trust. Once individuals have shared history and knowledge of the other party, they use the central route, which involves the assessment of the other party's ability, integrity, and benevolence. Finally, after long periods of shared history in which the individuals develop a habitual pattern of trust, along with possible emotional bonds, they are no longer motivated to deliberately assess trust, and instead simply enact prior trust attitudes via the habitual route. The mediated communication environment predominantly used by virtual teams slows down the progression among the three routes, and increases perceived risk.","Virtual groups,
History,
Computer mediated communication,
Cognition,
Collaborative work,
Communication system control,
Communications technology,
Electronic mail,
Collaborative software,
Assembly"
Learning-enforced time domain routing to mobile sinks in wireless sensor fields,"We propose a learning-based approach to efficiently and reliably route data to a mobile sink in a wireless sensor field. Specifically, we consider a mobile sink that does not know when to query or does not need to query. Furthermore, the sink moves in a certain pattern within the sensor field. Such a sink passively listens for incoming data that distant source sensors unilaterally push towards it. Unlike traditional routing mechanisms, our technique takes the time-domain explicitly into account, with each node involved making the decision ""at this time what is the best way to forward the packet to the sink?"". In the presented scheme, motes (nodes in the vicinity of the sink) learn its movement pattern over time and statistically characterize it as a probability distribution function. Having obtained this information at the motes, our scheme uses reinforcement learning to locate the sink efficiently at any point of time.","Routing,
Wireless sensor networks,
Sensor phenomena and characterization,
Mobile computing,
Costs,
Computer science,
Data engineering,
Reliability engineering,
Time domain analysis,
Probability distribution"
Benchmark probes for grid assessment,"Summary form only given. Like all computing platforms, grids are in need of a suite of benchmarks by which they can be evaluated, compared and characterized. As a first step towards this goal, we have developed a set of probes that exercise basic grid operations with the goal of measuring the performance and the performance variability of basic grid operations, as well as the failure rates of these operations. We present measurement data obtained by running our probes on a grid testbed that spans 5 clusters in 3 institutions. These measurements quantify compute times, network transfer times, and Globus middleware overhead. Our results help provide insight into the stability, robustness, and performance of our testbed, and lead us to make some recommendations for future grid development.","Probes,
Grid computing,
Testing,
Middleware,
Robust stability,
Application software,
Computer science,
Supercomputers,
Computer networks,
Design engineering"
Online simultaneous localization and mapping in dynamic environments,"We propose an on-line algorithm for simultaneous localization and mapping of dynamic environments. Our algorithm is capable of differentiating static and dynamic parts of the environment and representing them appropriately on the map. Our approach is based on maintaining two occupancy grids. One grid models the static parts of the environment, and the other models the dynamic parts of the environment. The union of the two provides a complete description of the environment over time. We also maintain a third map containing information about static landmarks detected in the environment. These landmarks provide the robot with localization. Results in simulation and with physical robots show the efficiency of our approach and show how the differentiation of dynamic and static entities in the environment and SLAM can be mutually beneficial.","Simultaneous localization and mapping,
Embedded system,
Mobile robots,
Object detection,
Computer science,
Robot localization,
Testing,
Performance evaluation,
Bayesian methods"
An incentive mechanism for P2P networks,"The current peer-to-peer (P2P) information sharing paradigm does not provide incentive and service differentiation for users. Since there is no motivation to share information or resources, this leads to the ""free-riding"" and the ""tragedy of the commons"" problems. We address how one can incorporate incentive into the P2P information sharing paradigm so as to encourage users to share information and resources. Our mechanism (or protocol) provides service differentiation to users with different contribution values and connection types. The mechanism also has some desirable properties: (1) conservation of cumulative contribution and social utility in the P2P community, (2) maximization of social utility if all requesting clients have the same contribution value, and (3) incentive-based resource distribution. The resource distribution algorithm and the contribution update algorithm are computationally efficient and can be easily implemented. Experimental results illustrate the efficiency and fairness of our algorithms.",
"Interaction of retransmission, blacklisting, and routing metrics for reliability in sensor network routing","Unpredictable and heterogeneous links in a wireless sensor network require techniques to avoid low delivery rate and high delivery cost. Three commonly used techniques to help discover high quality paths include (1) link-layer retransmission, (2) blacklisting bad links, and (3) end-to-end routing metrics. Using simulation and testbed experiments, we present the first systematic exploration of the tradeoffs of combinations of these approaches, quantifying the effects of each of these three techniques. We identify several key results: one is that per-hop retransmissions (ARQ) is a necessary addition to any other mechanism if reliable data delivery is a goal. Additional interactions between the services are more subtle. First, in a multihop network, either blacklisting or reliability metrics like ETX can provide consistent high-reliability paths when added to ARQ. Second, at higher deployment densities, blacklisting has a lower routing overhead than CTX. But at lower densities, blacklisting becomes less stable as the network partitions. These results are consistent across both simulation and testbed experiments. We conclude that ETX with retransmissions is the best choice in general, but that blacklisting may be worth considering at higher densities, either with or without ETX.",
Object tracking in a 2D UWB sensor network,"We consider object tracking by a UWB sensor network using multipath measurements in different scenarios: single Tx with single Rx, multiple Tx with single Rx and multiple Tx with multiple Rx. For each scenario, we examine the Cramer-Rao lower bound (CRLB) for the high-SNR case where multipath measurements are corrupted by iid Gaussians. We focus on the dense network asymptotics and show how the CRLB is inversely proportional to the number of measurements available to the network as a whole, even when the individual measurements are taken from different locations. An order-optimal semilinear algorithm is given.","Intelligent networks,
Radar tracking,
Transmitters,
Wireless sensor networks,
Computer science,
Electric variables measurement,
Reflection,
Gaussian processes,
Physical layer,
Wireless communication"
Organizing the last line of defense before hitting the memory wall for CMPs,"The last line of defense in the cache hierarchy before going to off-chip memory is very critical in chip multiprocessors (CMPs) from both the performance and power perspectives. We investigate different organizations for this last line of defense (assumed to be L2 in this article) towards reducing off-chip memory accesses. We evaluate the trade-offs between private L2 and address-interleaved shared L2 designs, noting their individual benefits and drawbacks. The possible imbalance between the L2 demands across the CPUs favors a shared L2 organization, while the interference between these demands can favor a private L2 organization. We propose a new architecture, called Shared Processor-Based Split L2, that captures the benefits of these two organizations, while avoiding many of their drawbacks. Using several applications from the SPEC OMP suite and a commercial benchmark, Specjbb, on a complete system simulator, we demonstrate the benefits of this shared processor-based L2 organization. Our results show as much as 42.50% improvement in IPC over the private organization (with 11.52% on the average), and as much as 42.22% improvement over the shared interleaved organization (with 9.76% on the average).","Organizing,
Costs,
Parallel processing,
Computer science,
Interference,
System-on-a-chip,
Buildings,
Yarn,
Program processors,
Runtime"
Accurate estimation of the fisher information matrix for the PET image reconstruction problem,"The Fisher information matrix (FIM) plays a key role in the analysis and applications of statistical image reconstruction methods based on Poisson data models. The elements of the FIM are a function of the reciprocal of the mean values of sinogram elements. Conventional plug-in FIM estimation methods do not work well at low counts, where the FIM estimate is highly sensitive to the reciprocal mean estimates at individual detector pairs. A generalized error look-up table (GELT) method is developed to estimate the reciprocal of the mean of the sinogram data. This approach is also extended to randoms precorrected data. Based on these techniques, an accurate FIM estimate is obtained for both Poisson and randoms precorrected data. As an application, the new GELT method is used to improve resolution uniformity and achieve near-uniform image resolution in low count situations.","Positron emission tomography,
Image reconstruction,
Spatial resolution,
Image resolution,
Maximum likelihood estimation,
Signal processing,
Image processing,
Signal resolution,
Lesions,
Image analysis"
Nonuniform noise propagation by using the ramp filter in fan-beam computed tomography,"It is observed that when the homogeneity property of the ramp filter is used to derive a filtered backprojection algorithm in fan-beam tomography, the reconstructed images have nonstationary frequency components and nonstationary noise. When a short focal-length is used, higher frequency components are amplified more at the edge of the image than at the center of the image, resulting in higher noise at the edge of the image.","Computed tomography,
Image reconstruction,
Geometry,
Detectors,
Filtering,
Frequency domain analysis,
Convolution,
Digital filters,
Image edge detection,
Image analysis"
Robots as assistive technology - does appearance matter?,"This work studies the effect of a robot's design (appearance) in facilitating and encouraging interaction of children with autism with a small humanoid robot. The paper compares the children's level of interaction with the response to the robot in two different scenarios: one where the robot was dressed like a human (with a 'pretty-girl' appearance) with an uncovered face, and the other, when it appeared with plain clothing and with a featureless, masked face. The results of these, trials clearly indicate the children's preference in their initial response for interaction with a plain, featureless robot over interaction with a human like robot.","Humanoid robots,
Human robot interaction,
Autism,
Educational robots,
Computer science education,
Medical treatment,
Tellurium,
Adaptive systems,
Clothing,
Virtual environment"
Spatial transformation of motion and deformation fields using nonrigid registration,"In this paper, we present a technique that can be used to transform the motion or deformation fields defined in the coordinate system of one subject into the coordinate system of another subject. Such a transformation accounts for the differences in the coordinate systems of the two subjects due to misalignment and size/shape variation, enabling the motion or deformation of each of the subjects to be directly quantitatively and qualitatively compared. The field transformation is performed by using a nonrigid registration algorithm to determine the intersubject coordinate system mapping from the first subject to the second subject. This fixes the relationship between the coordinate systems of the two subjects, and allows us to recover the deformation/motion vectors of the second subject for each corresponding point in the first subject. Since these vectors are still aligned with the coordinate system of the second subject, the inverse of the intersubject coordinate mapping is required to transform these vectors into the coordinate system of the first subject, and we approximate this inverse using a numerical line integral method. The accuracy of our numerical inversion technique is demonstrated using a synthetic example, after which we present applications of our method to sequences of cardiac and brain images.","Magnetic resonance imaging,
Anatomy,
Shape,
Information processing,
Hospitals,
Optical imaging,
Image motion analysis,
Image reconstruction,
Brain,
Anatomical structure"
Maximizing quadratic programs: extending Grothendieck's inequality,"This paper considers the following type of quadratic programming problem. Given an arbitrary matrix A, whose diagonal elements are zero, find x /spl isin/ {-1, 1}/sup n/ such that x/sup T/Ax is maximized. Our approximation algorithm for this problem uses the canonical semidefinite relaxation and returns a solution whose ratio to the optimum is in /spl Omega/(1/ logn). This quadratic programming problem can be seen as an extension to that of maximizing x/sup T/Ay (where y's components are also /spl plusmn/1). Grothendieck's inequality states that the ratio of the optimum value of the latter problem to the optimum of its canonical semidefinite relaxation is bounded below by a constant. The study of this type of quadratic program arose from a desire to approximate the maximum correlation in correlation clustering. Nothing substantive was known about this problem; we present an /spl Omega/ (1/logn) approximation, based on our quadratic programming algorithm. We can also guarantee that our quadratic programming algorithm returns a solution to the MAXCUT problem that has a significant advantage over a random assignment.","Approximation algorithms,
Quadratic programming,
Clustering algorithms,
Engineering profession,
Linear matrix inequalities,
US Department of Energy,
Computer science"
A scalable approach to user-session based testing of Web applications through concept analysis,"The continuous use of the Web for daily operations by businesses, consumers, and government has created a great demand for reliable Web applications. One promising approach to testing the functionality of Web applications leverages user-session data collected by Web servers. This approach automatically generates test cases based on real user profiles. The key contribution of This work is the application of concept analysis for clustering user sessions for test suite reduction. Existing incremental concept analysis algorithms can be exploited to avoid collecting large user-session data sets and thus provide scalability. We have completely automated the process from user session collection and reduction through replay. Our incremental test suite update algorithm coupled with our experimental study indicate that concept analysis provides a promising means for incrementally updating reduced test suites in response to newly captured user sessions with some loss in fault detection capability and practically no coverage loss.","Automatic testing,
Application software,
Software testing,
Web server,
Computational Intelligence Society,
Government,
Algorithm design and analysis,
Clustering algorithms,
Fault detection,
Computer science"
Markov types and minimax redundancy for Markov sources,"Redundancy of universal codes for a class of sources determines by how much the actual code length exceeds the optimal code length. In the minimax scenario, one designs the best code for the worst source within the class. Such minimax redundancy comes in two flavors: average minimax or worst case minimax. We study the worst case minimax redundancy of universal block codes for Markovian sources of any order. We prove that the maximal minimax redundancy for Markov sources of order r is asymptotically equal to 1/2m/sup r/(m-1)log/sub 2/n+log/sub 2/A/sub m//sup r/-(lnlnm/sup 1/(m-1)/)/lnm+o(1), where n is the length of a source sequence, m is the size of the alphabet, and A/sub m//sup r/ is an explicit constant (e.g., we find that for a binary alphabet m=2 and Markov of order r=1 the constant A/sub 2//sup 1/=16/spl middot/G/spl ap/14.655449504 where G is the Catalan number). Unlike previous attempts, we view the redundancy problem as an asymptotic evaluation of certain sums over a set of matrices representing Markov types. The enumeration of Markov types is accomplished by reducing it to counting Eulerian paths in a multigraph. In particular, we propose exact and asymptotic formulas for the number of strings of a given Markov type. All of these findings are obtained by analytic and combinatorial tools of analysis of algorithms.","Minimax techniques,
Algorithm design and analysis,
Computer science,
Block codes,
Information theory,
Multidimensional systems,
Jacobian matrices,
Information analysis,
Source coding,
Mathematics"
Design and evaluation of a distributed scalable content discovery system,"A content discovery system (CDS) allows nodes in the system to discover contents published by some other nodes in the system. Existing CDS systems have difficulties in achieving both scalability and rich functionality. We present the design and evaluation of a distributed and scalable CDS. Our system uses rendezvous points (RPs) for content registration and query resolution, and can accommodate frequent updates from dynamic contents. Contents stored in our system can be searched via subset matching. We propose a novel mechanism that uses load balancing matrices (LBMs) to balance dynamically both registration and query load across nodes in the system to maintain high system throughput even under skewed load. Our system utilizes existing distributed hash table (DHT) mechanisms for CDS overlay network management and routing. We validate our system's scalability and load balancing properties using extensive simulation.","Peer to peer computing,
Scalability,
Road transportation,
Load management,
Cameras,
Throughput,
Telecommunication traffic,
Condition monitoring,
Computer science,
Routing"
"Employing user feedback for fast, accurate, low-maintenance geolocationing","One way to improve inferences on sensor data is to tune the algorithms through a time-consuming offline procedure. A less expensive and potentially more accurate method is to use an online procedure based on feedback from users, who often know best what the data means to them. We present a method for user-assisted location inference based on 802.11b wireless signal strengths. A user 'corrects' system geolocations by clicking on a map, recording a 'virtual access point' (VAP) at the selected point for future inferences. A best VAP is selected using simple criteria, including the VAP's creator. This permit using other's VAPs while getting their own if one exists, capturing user-specific behavior. The system is also self-maintaining with respect to changing access point deployments. Indoor experiments show very good accuracy for this simple method.","Feedback,
Sensor phenomena and characterization,
Error correction,
Computer science,
Data engineering,
Inference algorithms,
Working environment noise,
Mathematical model,
Wireless sensor networks,
Information technology"
Energy efficient data collection in distributed sensor environments,"Sensors are typically deployed to gather data about the physical world and its artifacts for a variety of purposes that range from environment monitoring, control, to data analysis. Since sensors are resource constrained, often sensor data is collected into a sensor database that resides at (more powerful) servers. A natural tradeoff exists between the sensor resources (bandwidth, energy) consumed and the quality of data collected at the server. Blindly transmitting sensor updates at a fixed periodicity to the server results in a suboptimal solution due to the differences in stability of sensor values and due to the varying application needs that impose different quality requirements across sensors. We propose adaptive data collection mechanisms for sensor environments that adjusts to these variations while at the same time optimizing the energy consumption of sensors. Our experimental results show significant energy savings compared to the naive approach to data collection.","Energy efficiency,
Sensor systems,
Power system modeling,
Sensor phenomena and characterization,
Data analysis,
Energy consumption,
Distributed computing,
Distributed databases,
Network servers,
Computer science"
Multi-robot mapping using manifold representations,"This paper introduces a new method for representing two-dimensional maps, and shows how this representation may be applied to concurrent localization and mapping problems involving multiple robots. We introduce the notion of a manifold map; this representation takes maps out of the plane and onto a two-dimensional surface embedded in a higher-dimensional space. Compared with standard planar maps, the key advantage of the manifold representation is self-consistency: manifold maps do not suffer from the 'cross over' problem that planar maps commonly exhibit in environments containing loops. This self-consistency facilitates a number of important autonomous capabilities, including robust retro-traverse, lazy loop closure, active loop closure using robot rendezvous, and, ultimately, autonomous exploration. By way of validation, this paper also includes experimental results obtained using teams of two to four robots in environments ranging in size from 400 m/sup 2/ to 900 m/sup 2/.","Robots,
Orbital robotics,
Spirals,
Path planning,
Laboratories,
Computer science,
Standards development,
Computer hacking,
Simultaneous localization and mapping,
Delay"
Rough set approximations in formal concept analysis,"An important topic of rough set theory is the approximation of undefinable sets or concepts through definable sets. It involves the construction of a system of definable sets and the definition of approximation operators. In this paper, the notion of rough set approximations is introduced into formal concept analysis. Approximation operators are defined based on both lattice-theoretic and set-theoretic operators. The results provide a better understanding of data analysis using rough set theory and formal concept analysis.","Set theory,
Lattices,
Data analysis,
Computer science"
Visualization in grid computing environments,"Grid computing provides a challenge for visualization system designers. In this research, we evolve the dataflow concept to allow parts of the visualization process to be executed remotely in a secure and seamless manner. We see dataflow at three levels: an abstract specification of the intent of the visualization; a binding of these abstract modules to a specific software system; and then a binding of software to processing and other resources. We develop an XML application capable of describing visualization at the three levels. To complement this, we have implemented an extension to a popular visualization system, IRIS Explorer, which allows modules in a dataflow pipeline to run on a set of grid resources. For computational steering applications, we have developed a library that allows a visualization system front-end to connect to a simulation running remotely on a grid resource. We demonstrate the work in two applications: the dispersion of a pollutant under different wind conditions; and the solution of a challenging numerical problem in elastohydrodynamic lubrication.","Visualization,
Grid computing,
Software systems,
XML,
Application software,
Iris,
Pipelines,
Computer applications,
Libraries,
Computational modeling"
Pre-Rake performance for pulse based UWB system in a standardized UWB short-range channel,"The pre-Rake scheme for pulse based ultrawideband (UWB) communications system is proposed in a standardized UWB channel model from IEEE 802.15 SG3a, which is based on an extensive set of indoor short-range channel measurements in this paper. The pre-Rake scheme is shown to contribute to the low-power, cost-effective UWB system designing as well as Rake combining gain. Instead of building a Rake receiver at the receiving side (e.g. portable unit), the transmitter (e.g. access point) can pre-diversity the UWB signal before transmission in the forward link estimating the channel impulse response from the reverse link. While the pre-Raked signal is convolved with the estimated channel impulse response, the function of Rake combining at the receiver is automatically performed. Monte-Carlo simulations are carried out to compare the pre-Rake with Rake results and show that pre-Rake scheme is as good as Rake combining. Then the mobile or portable unit with a conventional receiver can still achieve the diversity gain of Rake combining.","Fading,
Multipath channels,
RAKE receivers,
Computer science,
Pulse measurements,
Ultra wideband technology,
Diversity reception,
Signal resolution,
Ultra wideband communication,
Network topology"
Avida: A Software Platform for Research in Computational Evolutionary Biology,"Avida is a software platform for experiments with self-replicating and evolving computer programs. It provides detailed control over experimental settings and protocols, a large array of measurement tools, and sophisticated methods to analyze and post-process experimental data. We explain the general principles on which Avida is built, as well as its main components and their interactions. We also explain how experiments are set up, carried out, and analyzed.",
An ontology-based framework for XML semantic integration,"XML is becoming the standard for data interchange on the Web. However, XML and its schema languages do not express semantics but rather structure, such as nesting information. Therefore, semantically equivalent documents often present different document structures. We provide an ontology-based framework that aims to make two XML documents intemperate at the semantic level while retaining their nesting structure. In our global-as-view approach, we generate an RDF ontology for each of the participating XML documents, which preserves the nesting structure of the document. An RDF global ontology is the result of merging the individual ontologies. The global ontology unifies the query access and establishes semantic connections among the underlying individual databases. We consider two types of queries: those that are posed on the global ontology and those that are posed on any of the XML documents, in a P2P fashion. The former type is processed using query translation from an RDF query to an XML query. The latter type entails bidirectional query processing: the translation from an XML query to an RDF query followed by the translation from an RDF query to an XML query. To ensure the correctness of the answer to the query in the latter case, we introduce the concept of reversibility of the query translation.","Ontologies,
XML,
Books,
Resource description framework,
Databases,
Query processing,
Computer science,
Merging,
Semantic Web,
Information retrieval"
Path planning for mobile robot using the particle swarm optimization with mutation operator,"Path planning is one of the most important technologies in the navigation of the mobile robot, which should meet the optimization and real-time requests. This paper presents a novel approach of path planning. First the MAKLINK graph is built to describe the working space of the mobile robot; then the Dijkstra algorithm is used to obtain the shortest path from the start point to the goal point in the graph, finally the particle swarm optimization algorithm is adopted to get the optimal path. Aiming at the shortcoming of the PSO algorithm, that is, easily plunging into the local minimum, this paper puts forward an advanced PSO algorithm with the mutation operator. By adding a mutation operator to the algorithm, it can not only escape the attraction of the local minimum in the later convergence phase, but also maintain the characteristic of fast speed in the early phase. The results of the simulation demonstrate the effectiveness of the proposed method, which can meet the real-time requests of the mobile robot's navigation.",
KASER: knowledge amplification by structured expert randomization,"In this paper and attached video, we present a third-generation expert system named Knowledge Amplification by Structured Expert Randomization (KASER) for which a patent has been filed by the U.S. Navy's SPAWAR Systems Center, San Diego, CA (SSC SD). KASER is a creative expert system. It is capable of deductive, inductive, and mixed derivations. Its qualitative creativity is realized by using a tree-search mechanism. The system achieves creative reasoning by using a declarative representation of knowledge consisting of object trees and inheritance. KASER computes with words and phrases. It possesses a capability for metaphor-based explanations. This capability is useful in explaining its creative suggestions and serves to augment the capabilities provided by the explanation subsystems of conventional expert systems. KASER also exhibits an accelerated capability to learn. However, this capability depends on the particulars of the selected application domain. For example, application domains such as the game of chess exhibit a high degree of geometric symmetry. Conversely, application domains such as the game of craps played with two dice exhibit no predictable pattern, unless the dice are loaded. More generally, we say that domains whose informative content can be compressed to a significant degree without loss (or with relatively little loss) are symmetric. Incompressible domains are said to be asymmetric or random. The measure of symmetry plus the measure of randomness must always sum to unity.","Expert systems,
Acceleration,
Machine learning,
Radar,
Computer science,
Electronic mail,
Information retrieval,
Production,
Computer errors"
Fault-aware job scheduling for BlueGene/L systems,"Summary form only given. Large-scale systems like BlueGene/L are susceptible to a number of software and hardware failures that can affect system performance. We evaluate the effectiveness of a previously developed job scheduling algorithm for BlueGene/L in the presence of faults. We have developed two new job-scheduling algorithms considering failures while scheduling the jobs. We have also evaluated the impact of these algorithms on average bounded slowdown, average response time and system utilization, considering different levels of proactive failure prediction and prevention techniques reported in the literature. Our simulation studies show that the use of these new algorithms with even trivial fault prediction confidence or accuracy levels (as low as 10%) can significantly improve the performance of the BlueGene/L system.","Large-scale systems,
Processor scheduling,
Delay,
Parallel machines,
Scheduling algorithm,
Concurrent computing,
Switches,
Time sharing computer systems,
Computer science,
Laboratories"
The virtual resource manager: an architecture for SLA-aware resource management,"The next generation Grid will demand the Grid middleware to provide flexibility, transparency, and reliability. This implies the appliance of service level agreements to guarantee a negotiated level of quality of service. These requirements also affect the local resource management systems providing resources for the Grid. At this a gap between these demands and the features of today's resource management systems becomes apparent. In this paper we present an approach which closes this gap. Introducing the architecture of the virtual resource manager we highlight its main features of runtime responsibility, resource virtualization, information hiding, autonomy provision, and smooth integration of existing resource management system installations.","Resource management,
Computer architecture,
Middleware,
Grid computing,
Resource virtualization,
Quality of service,
Computer network management,
Computer science,
Parallel processing,
Home appliances"
A probabilistic extension to ontology language OWL,"To support uncertain ontology representation and ontology reasoning and mapping, we propose to incorporate Bayesian networks (BN), a widely used graphic model for knowledge representation under uncertainty and OWL, the de facto industry standard ontology language recommended by W3C. First, OWL is augmented to allow additional probabilistic markups, so probabilities can be attached with individual concepts and properties in an OWL ontology. Secondly, a set of translation rules is defined to convert this probabilistically annotated OWL ontology into the directed acyclic graph (DAG) of a BN. Finally, the BN is completed by constructing conditional probability tables (CPT) for each node in the DAG. Our probabilistic extension to OWL is consistent with OWL semantics, and the translated BN is associated with a joint probability distribution over the application domain. General Bayesian network inference procedures (e.g., belief propagation or junction tree) can be used to compute P(C/spl bsol/e): the degree of the overlap or inclusion between a concept C and a concept represented by a description e. We also provide a similarity measure that can be used to find the most similar concept that a given description belongs to.","Ontologies,
OWL,
Bayesian methods,
Uncertainty,
Microstrip,
Logic,
Knowledge representation,
Semantic Web,
Computer science,
Computer graphics"
Fast-searching algorithm for vector quantization using projection and triangular inequality,"In this paper, a new and fast-searching algorithm for vector quantization is presented. Two inequalities, one used for terminating the searching process and the other used to delete impossible codewords, are presented to reduce the distortion computations. Our algorithm makes use of a vector's features (mean value, edge strength, and texture strength) to reject many unlikely codewords that cannot be rejected by other available approaches. Experimental results show that our algorithm is superior to other algorithms in terms of computing time and the number of distortion calculations. Compared with available approaches, our method can reduce the computing time and the number of distortion computations significantly. Compared with the best method of reducing distortion computation, our algorithm can further reduce the number of distortion calculations by 29% to 58.4%. Compared with the best encoding algorithm for vector quantization, our approach also further reduces the computing time by 8% to 47.7%.","Vector quantization,
Encoding,
Data compression,
Decoding,
Computer science,
Nearest neighbor searches"
A CP-nets-based design and verification framework for Web services composition,"Web services aim to support efficient integration of applications over Web. Most Web services are stateful, such as services for business processes, and they converse with each other via properly ordered interactions, instead of individual unrelated invocations. In order to address efficient integration of conversational Web services, we create a unified specification model for both conversation protocol and composition; we propose methods to integrate a partner service with complex conversation protocol into a composition of Web services; assure the correctness of composition by formal verification. The mapping between our model and BPEL4WS is also discussed.","Web services,
Protocols,
Credit cards,
Formal verification,
Computer science,
Application software,
Automata,
Concurrent computing,
Specification languages,
Logic"
Placement constraints in floorplan design,"In floorplan design, it is common that a designer will want to control the positions of some modules in the final packing for various purposes like datapath alignment and I/O connection. There are several previous works focusing on some particular kinds of placement constraints. In this paper, we will present a unified method to handle all of them simultaneously, including preplace constraint, range constraint, boundary constraint, alignment, abutment, and clustering, etc., in general, nonslicing floorplans. We have used incremental updates and an interesting idea of reduced graph to improve the runtime of the method. We tested our method using some benchmark data with about 1/8 of the modules having placement constraints and the results are very promising. Good packings with all the constraints satisfied can be obtained efficiently.",
Authentic facial expression analysis,"It is argued that for the computer to be able to interact with humans, it needs to havve the communication skills o humans. One of these skills is the ability to understand the emotional state of the person. The most expressive way humans display emotions is through facial expressions. In most facial expression systems and databases, the emotion data was collected by asking the subjects to perform a series of facial expressions. However, these directed or deliberate facial action tasks typically differ in appearance and timing from the authentic facial expressions induced through events in the normal environment of the subject. In this paper, we present our effort in creating an authentic facial expression database based on spontaneous emotions derived from the environment. Furthermore, we test and compare a wide range of classifiers from the machine learning literature that can be used for facial expression classification.",
"Comments and corrections to ""Dominating sets and neighbor elimination-based broadcasting algorithms in wireless networks""","The paper by I. Stojmenovic et al. (2002) generated a lot of interest among researchers in ad hoc networks. A number of researchers questioned, through their articles, or directly to the first author, the correctness of the described procedure, and the correctness of the claim that the procedure does not need any communication exchange between nodes, in addition to ""hello"" messages needed to learn information about neighboring nodes. This correspondence completes the article by providing the actual dominating set definitions used in the procedure (from which zero communication overhead follows easily), the correct procedure (the published one has few misprints at key places), and the proof that the new definitions and procedure indeed define connected dominating sets.","Broadcasting,
Intelligent networks,
Wireless networks,
Ad hoc networks,
Computer Society,
Computer science,
Protocols,
Spine,
Topology"
Optimal inapproximability results for MAX-CUT and other 2-variable CSPs?,"In this paper, we give evidence suggesting that MAX-CUT is NP-hard to approximate to within a factor of /spl alpha//sub cw/+ /spl epsi/, for all /spl epsi/ > 0, where /spl alpha//sub cw/ denotes the approximation ratio achieved by the Goemans-Williamson algorithm (1995). /spl alpha//sub cw/ /spl ap/ .878567. This result is conditional, relying on two conjectures: a) the unique games conjecture of Khot; and, b) a very believable conjecture we call the majority is stablest conjecture. These results indicate that the geometric nature of the Goemans-Williamson algorithm might be intrinsic to the MAX-CUT problem. The same two conjectures also imply that it is NP-hard to (/spl beta/ + /spl epsi/)-approximate MAX-2SAT, where /spl beta/ /spl ap/ .943943 is the minimum of (2 + (2//spl pi/) /spl theta/)/(3 - cos(/spl theta/)) on (/spl pi//2, /spl pi/). Motivated by our proof techniques, we show that if the MAX-2CSP and MAX-2SAT problems are slightly restricted - in a way that seems to retain all their hardness -then they have (/spl alpha//sub GW/-/spl epsi/)- and (/spl beta/ - /spl epsi/)-approximation algorithms, respectively. Though we are unable to prove the majority is stablest conjecture, we give some partial results and indicate possible directions of attack. Our partial results are enough to imply that MAX-CUT is hard to (3/4 + 1/(2/spl pi/) + /spl epsi/)-approximate (/spl ap/ .909155), assuming only the unique games conjecture. We also discuss MAX-2CSP problems over non-Boolean domains and state some related results and conjectures. We show, for example, that the unique games conjecture implies that it is hard to approximate MAX-2LIN(q) to within any constant factor.","Mathematics,
Statistics,
Approximation algorithms,
Computer science,
Stability,
Additive noise,
Bipartite graph,
Labeling,
Boolean functions"
Determining intra-flow contention along multihop paths in wireless networks,"Admission control of flows is essential for providing quality of service in multihop wireless networks. In order to make an admission decision for a new flow, the expected bandwidth consumption of the flow must be correctly determined. Due to the shared nature of the wireless medium, nodes along a multihop path contend among themselves for access to the medium. This leads to intra-flow contention; contention between packets of the same flow being forwarded at different hops along a multihop path causing the actual bandwidth consumption of the flow to become a multiple of its single hop bandwidth requirement. Determining the amount of intra-flow contention is non-trivial since interfering nodes may not be able to communicate directly if they are outside each other's transmission range. In this paper, we propose two methods to determine the extent of intra-flow contention along multihop paths. The highlight of the proposed solutions is that carrier-sensing data is used to deduce information about carrier-sensing neighbors, and no high power transmissions are necessary. Analytical and simulation results show that our methods estimate intra-flow contention with low error, while significantly reducing overhead, energy consumption and latency as compared to previous approaches.","Spread spectrum communication,
Intelligent networks,
Wireless networks,
Bandwidth,
Admission control,
Power transmission,
Delay,
Computer science,
Computer networks,
Quality of service"
Automatic methods for predicting machine availability in desktop Grid and peer-to-peer systems,"In this paper we examine the problem of predicting machine availability in desktop and enterprise computing environments. Predicting the duration that a machine will run until it restarts (availability duration) is critically useful to application scheduling and resource characterization in federated systems. We describe one parametric model fitting technique and two nonparametric prediction techniques, comparing their accuracy in predicting the quantiles of empirically observed machine availability distributions. We describe each method analytically and evaluate its precision using a synthetic trace of machine availability constructed from a known distribution. To detail their practical efficacy, we apply them to machine availability traces from three separate desktop and enterprise computing environments, and evaluate each method in terms of the accuracy with which it predicts availability in a trace driven simulation. Our results indicate that availability duration can be predicted with quantifiable confidence bounds and that these bounds can he used as conservative bounds on lifetime predictions. Moreover a nonparametric method based on a binomial approach generates the most accurate estimates.","Peer to peer computing,
Availability,
Grid computing,
Computer science,
Processor scheduling,
Microcomputers,
Time measurement,
Educational institutions,
Mathematics,
Predictive models"
An evaluation of the close-to-files processor and data co-allocation policy in multiclusters,"In multicluster systems, and more generally, in grids, jobs may require coallocation, i.e., the simultaneous allocation of resources such as processors and input files in multiple clusters. While such jobs may have reduced runtimes because they have access to more resources, waiting for processors in multiple clusters and for the input files to become available in the right locations may introduce inefficiencies. In previous work, we have studied through simulations only processor coallocation. Here, we extend this work with an analysis of the performance in a real testbed of our prototype processor and data coallocator with the close-to-files (CF) job-placement algorithm. CF tries to place job components on clusters with enough idle processors which are close to the sites where the input files reside. We present a comparison of the performance of CF and the worst-fit job-placement algorithm, with and without file replication, achieved with our prototype. Our most important findings are that CF with replication works best, and that the utilization in our testbed can be driven to about 80%.","Resource management,
Testing,
Processor scheduling,
Mathematics,
Computer science,
Runtime,
Clustering algorithms,
Prototypes,
Load management,
Availability"
Group and swarm mobility models for ad hoc network scenarios using virtual tracks,"The mobility model is one of the most important factors in the performance evaluation of a mobile ad hoc network (MANET). Traditionally, the random waypoint mobility model has been used to model the node mobility, where the movement of one node is modeled as independent from all others. However, in reality, especially in large scale military scenarios, mobility coherence among nodes is quite common. One typical mobility behavior is group mobility. Thus, to investigate military MANET scenarios, an underlying realistic mobility model is highly desired. In this paper, we propose a ""virtual track"" based group mobility model (VT model) which closely approximates the mobility patterns in military MANET scenarios. It models various types of node mobility such as group moving nodes, individually moving nodes as well as static nodes. Moreover, the VT model not only models the group mobility, it also models the dynamics of group mobility such as group merge and split. Simulation experiments show that the choice of mobility model has significant impact on network performance.","Ad hoc networks,
Mobile ad hoc networks,
Routing protocols,
Computer science,
Large-scale systems,
Coherence,
Unmanned aerial vehicles"
Multicast authentication in fully adversarial networks,"We study a general version of the multicast authentication problem where the underlying network, controlled by an adversary, may drop chosen packets, rearrange the order of the packets in an arbitrary way, and inject new packets into the transmitted stream. Prior work on the problem has focused on less general models, where random, rather than adversarially-selected packets may be dropped and altered, or no additional packets may be injected into the stream. We describe an efficient and scalable authentication scheme that is based on a novel combination of error-correcting codes with standard cryptographic primitives. We prove the security of our scheme and analyze its performance in terms of the computational effort at the sender and receiver and the communication overhead. We also discuss specific design and implementation choices and compare our scheme with previously proposed approaches.","Authentication,
Intelligent networks,
Cryptography,
Code standards,
Performance analysis,
Error correction codes,
Computer science,
Data security,
Internet,
Multicast protocols"
Success factors of virtual communities from the perspective of members and operators: an empirical study,"Virtual communities have been the focus of research for some time. However, while many studies provide recommendations on how to build, extend and manage virtual communities, few verify the success factors they consider essential for virtual communities. Conclusions made regarding basic preferences and distinct priorities of different stakeholders in virtual communities have not been empirically substantiated. This study uses an online survey of members and operators of virtual communities to evaluate success factors discussed in the literature. Incongruences between members and operators are identified and analysed. This research gains first empirically validated insights into success factors for establishing and managing virtual communities. The study results are summarised in ten hypotheses.","Management information systems,
Breast cancer,
Multidimensional systems,
Psychology,
Computer science,
Demography"
Resource optimization in QoS multicast routing of real-time multimedia,"We consider a network design problem, where applications require various levels of Quality-of-Service (QoS) while connections have limited performance. Suppose that a source needs to send a message to a heterogeneous set of receivers. The objective is to design a low-cost multicast tree from the source that would provide the QoS levels (e.g., bandwidth) requested by the receivers. We assume that the QoS level required on a link is the maximum among the QoS levels of the receivers that are connected to the source through the link. In accordance, we define the cost of a link to be a function of the QoS level that it provides. This definition of cost makes this optimization problem more general than the classical Steiner tree problem. We consider several variants of this problem all of which are proved to be NP-Hard. For the variant where QoS levels of a link can vary arbitrarily and the cost function is linear in its QoS level, we give a heuristic that achieves a multicast tree with cost at most a constant times the cost of an optimal multicast tree. The constant depends on the best constant approximation ratio of the classical Steiner tree problem. For the more general variant, where each link has a given QoS level and cost we present a heuristic that generates a multicast tree with cost O(min{logr,k}) times the cost of an optimal tree, where r denotes the number of receivers, and k denotes the number of different levels of QoS required. We generalize this result to hold for the case of many multicast groups.","Routing,
Cost function,
Bandwidth,
Quality of service,
Computer networks,
Computer science,
Internet,
Streaming media,
Intelligent networks"
"Quality assessment, verification, and validation of modeling and simulation applications","Many different types of modeling and simulation (M&S) applications are used in dozens of disciplines under diverse objectives including acquisition, analysis, education, entertainment, research, and training. M&S application verification and validation (V&V) are conducted to assess mainly the accuracy, which is one of many indicators affecting the M&S application quality. Much higher confidence can be achieved in accuracy if a quality-centered approach is used. This paper presents a quality model for assessing the quality of large-scale complex M&S applications as integrated with V&V. The guidelines provided herein should be useful for assessing the overall quality of an M&S application.","Quality assessment,
Application software,
Computational modeling,
Computer simulation,
Computer science,
Collaborative software,
Collaborative work,
Software systems,
Software quality"
Preemption-aware dynamic voltage scaling in hard real-time systems,"Dynamic voltage scaling (DVS) is a well-known low-power design technique for embedded real-time systems. Because of its effectiveness on energy reduction, several variable voltage processors have been developed and many DVS algorithms targeting these processors have been proposed. However, most existing DVS algorithms focus on reducing the energy consumption of CPU only, ignoring their negative impacts on task scheduling and system wide energy consumption. In this paper, we address one of such side effects, an increase in task preemptions due to DVS. We present two preemption control techniques which can reduce the number of task preemptions of DVS algorithms. Experimental results show that the delayed-preemption technique is effective in reducing the number of preemptions incurred by DVS algorithms while achieving a high energy efficiency.","Dynamic voltage scaling,
Real time systems,
Voltage control,
Energy consumption,
Clocks,
Permission,
Microprocessors,
Computer science,
Design engineering,
Power engineering and energy"
Private codes or succinct random codes that are (almost) perfect,"Coding theory addresses the design and analysis of codes that enable communication over noisy channels. Two types of channels that have been extensively considered are the binary symmetric channel and the adversarial channel. In a binary symmetric channel each bit of the sent message is flipped independently with some probability p, implying that the noise imposed by the channel is random in nature where the amount of noise is determined by p. In an adversarial channel the message is treated as a whole, and the noise may be an arbitrary (and malicious) function of the message being sent, as long as it does not effect more that a certain fraction (say p) of the bits transmitted. Roughly speaking, any code designed for an adversarial channel can be used on a corresponding binary symmetric channel successfully, whereas the contrary is not necessarily true. In this work we present a construction that transforms the best codes for binary symmetric channels into ""codes"" for corresponding adversarial channels. The ""codes"" we present assume that the sender and the receiver of the message have a joint secret random string (which is not known to the channel). These codes are referred to as private codes. Intuitively, this private randomness allows a reduction between the random and adversarial channels. Such a reduction is simple once the size of the joint random string is /spl Theta/(n log n) (here the codes are a subset of {0,1 }/sup n/). In this work we present private codes in which the size of the joint random string is O(log n). Moreover, we show that our result is tight. Namely, to design private codes that allow communication over adversarial channels that meet the bounds achievable when communicating over binary symmetric channels, an amount of /spl Omega/(log n) shared random bits are required. To the best of our knowledge, no prior results of this nature have been presented in the past. As part of our proof we establish a connection between list decodable codes and private codes which complements a recent result of Guruswami (CCC '03) on list decoding with side information.",Computer science
Initial uplink synchronization and power control (ranging process) for OFDMA systems,"We address the initial ranging process in OFDMA systems such as IEEE 802.16a. The proposed ranging method includes three main tasks - timing estimation, multi-user ranging code detection, and power estimation. All tasks are performed based on a bank of correlators corresponding to ranging codes. The timing estimation scheme is based on the peak of correlator outputs. The multi-user ranging code detection is based on the correlator outputs and an adaptive threshold. A novel adaptive threshold setting is proposed. A simple user-power estimator is derived based on the correlator outputs. A scheme of initial power adjustment at the random access users is also proposed which brings in a significant improvement in ranging code detection performance. The simulation results show that the proposed method works well even in the presence of several simultaneous random-access users and is robust to other data-users' interference. The simplicity of the proposed method is also quite appealing.","Power control,
Frequency synchronization,
Timing,
Neodymium,
Correlators,
Downlink,
Broadcasting,
Power engineering and energy,
Computer science,
Robustness"
Computational challenges of systems biology,"Progress in the study of biological systems such as the heart, brain, and liver will require computer scientists to work closely with life scientists and mathematicians. Computer science will play a key role in shaping the new discipline of systems biology and addressing the significant computational challenges it poses.","Biology computing,
Systems biology,
Proteins,
Biological system modeling,
Biological systems,
Bioinformatics,
Heart,
Physiology,
DNA,
Genomics"
From canonical to complex flows: Recent progress on monotonically integrated LES,"Large-eddy simulation (LES) based on subgrid-scale modeling implicitly provided by the discretization algorithm has been the subject of considerable recent interest. In the monotonically integrated LES approach, flux-limiting schemes emulate the flow features in the high-wavenumber end of the inertial range region of turbulence.","Tensile stress,
Energy resolution,
Laboratories,
Filtering,
Low pass filters,
Computational complexity,
Radio access networks,
Collaboration,
Nonlinear equations,
Proposals"
Masquerade detection augmented with error analysis,"A masquerade attack, in which one user impersonates another, may be one of the most serious forms of computer abuse. Automatic discovery of masqueraders is sometimes undertaken by detecting significant departures from normal user behavior, as represented by a user profile formed from system audit data. A major obstacle for this type of research is the difficulty in obtaining such system audit data, largely due to privacy concerns. An immense contribution in this regard has been made by Schonlau et al., who have made available UNIX command-line data from 50+ users collected over a number of months. Most of the research in this area has made use of this dataset, so this paper takes as its point of departure the Schonlau et al. dataset and a recent series of experiments with this data framed by the same researchers . In extending that work with a new classification algorithm, a 56% improvement in masquerade detection was achieved at a corresponding false-alarm rate of 1.3%. In addition, encouraging results were obtained at a more realistic sequence length of 10 commands (as opposed to sequences of 100 commands used by Schonlau et al.). A detailed error analysis, based on an alternative configuration of the same data, reveals a serious flaw in this type of data which hinders masquerade detection and indicates some steps that need to be taken to improve future results. The error analysis also demonstrates the insights that can be gained by inspecting decision errors, instead of concentrating only on decision successes.","Error analysis,
Data privacy,
Classification algorithms,
Keyboards,
Contracts,
Laboratories,
Computer science,
Monitoring,
Central Processing Unit,
Operating systems"
Architectural mechanisms for dynamic changes of behavior selection strategies in behavior-based systems,"Behavior selection is typically a ""built-in"" feature of behavior-based architectures and hence, not amenable to change. There are, however, circumstances where changing behavior selection strategies is useful and can lead to better performance. In this paper, we demonstrate that such dynamic changes of behavior selection mechanisms are beneficial in several circumstances. We first categorize existing behavior selection mechanisms along three dimensions and then discuss seven possible circumstances where dynamically switching among them can be beneficial. Using the agent architecture framework activation, priority, observer, and component (APOC), we show how instances of all (nonempty) categories can be captured and how additional architectural mechanisms can be added to allow for dynamic switching among them. In particular, we propose a generic architecture for dynamic behavior selection, which can integrate existing behavior selection mechanisms in a unified way. Based on this generic architecture, we then verify that dynamic behavior selection is beneficial in the seven cases by defining architectures for simulated and robotic agents and performing experiments with them. The quantitative and qualitative analyzes of the results obtained from extensive simulation studies and experimental runs with robots verify the utility of the proposed mechanisms.","Switches,
Control systems,
Analytical models,
Artificial intelligence,
Intelligent robots,
Computer science,
Animals"
Defending against low-rate TCP attacks: dynamic detection and protection,"We consider a distributed approach to detect and to defend against the low-rate TCP attack (A. Kuzmanovic et al., August 2003). The low-rate TCP attack is essentially a periodic short burst which exploits the homogeneity of the minimum retransmission timeout (RTO) of TCP flows and forces all affected TCP flows to back off and enter the retransmission timeout state. This sort of attack is difficult to identify due to a large family of attack patterns. We propose a distributed detection mechanism which uses the dynamic time warping method to robustly and accurately identify the existence of this sort of attack. Once the attack is detected, a fair resource allocation mechanism is used so that (1) the number of affected TCP flows is minimized, and (2) we provide sufficient resource protection for the affected TCP flows. We report experimental results to quantify the robustness and accuracy of the proposed detection mechanism and the efficiency of the defense method.","Protection,
Traffic control,
Robustness,
Computer science,
Throughput,
Sun,
Resource management,
Access protocols,
Engineering profession,
Telecommunication traffic"
Toward characterizing the performance of SOAP toolkits,"The SOAP protocol underpins Web services as the standard mechanism for exchanging information in a distributed environment. The XML-based protocol offers advantages including extensibility, interoperability, and robustness. The merger of Web services and grid computing promotes SOAP into a standard protocol for the large-scale scientific applications that computational grids promise to support, further elevating the protocol's importance and requiring high-performance implementations. Various SOAP implementations differ in their implementation language, invocation model and API, and supported performance optimizations. In this paper we compare and contrast the performance of widely used SOAP toolkits and draw conclusions about their current performance characteristics. We also provide insights into various design features that can lead to optimized SOAP implementations. The SOAP implementations included in our study are gSOAP 2.4, AxisC++ CVS May 28, AxisJava 1.2, .NET 1.1.4322 and XS0AP4/XSUL 1.1.",
Feedback control of data aggregation in sensor networks,"Sensor networks have recently emerged as a new paradigm for distributed sensing and actuation. This paper describes fundamental performance trade-offs in sensor networks and the utility of simple feedback control mechanisms for distributed performance optimization. A data communication and aggregation framework is presented that manipulates the degree of data aggregation to maintain specified acceptable latency bounds on data delivery while attempting to minimize energy consumption. An analytic model is constructed to describe the relationships between timeliness, energy, and the degree of aggregation, as well as to quantify constraints that stem from real-time requirements. Feedback control is used to adapt the degree of data aggregation dynamically in response to network load conditions while meeting application deadlines. The results illustrate the usefulness of feedback control in the sensor network domain.","Feedback control,
Intelligent networks,
Wireless sensor networks,
Batteries,
Energy consumption,
Sensor phenomena and characterization,
Remote monitoring,
Constraint optimization,
Helium,
Computer science"
Topological volume skeletonization using adaptive tetrahedralization,"Topological volume skeletons represent level-set graphs of 3D scalar fields, and have recently become crucial to visualizing the global isosurface transitions in the volume. However, it is still a time-consuming task to extract them, especially when input volumes are large-scale data and/or prone to small-amplitude noise. The paper presents an efficient method for accelerating the computation of such skeletons using adaptive tetrahedralization. The tetrahedralization is a top-down approach to linear interpolation of the scalar fields in that it selects tetrahedra to be subdivided adaptively using several criteria. As the criteria, the method employs a topological criterion as well as a geometric one in order to pursue all the topological isosurface transitions that may contribute to the global skeleton of the volume. The tetrahedralization also allows us to avoid unnecessary tracking of minor degenerate features that hide the global skeleton. Experimental results are included to demonstrate that the present method smoothes out the original scalar fields effectively without missing any significant topological features.","Skeleton,
Isosurfaces,
Interpolation,
Art,
Computer science,
Data visualization,
Data mining,
Large-scale systems,
Acceleration,
Rendering (computer graphics)"
Design patterns for reconfigurable computing,"It is valuable to identify and catalog design patterns for reconfigurable computing. These design patterns are canonical solutions to common and recurring design challenges which arise in reconfigurable systems and applications. The catalog can form the basis for creating designs, for educating new designers, for understanding the needs of tools and languages, and for discussing reconfigurable design. Tying application and implementation lessons to the expansion and refinement of this catalog make those lessons more relevant to the design community. In this paper, we articulate this role for design patterns in reconfigurable computing, provide a few example patterns, offer a starting point for the contents of the catalog, and discuss the potential benefits of this effort.","Application software,
Programming profession,
Costs,
Field programmable gate arrays,
Crystallization,
Process design,
Computer science,
Buildings,
Reconfigurable architectures,
Delay"
Performance of optical flow techniques for indoor navigation with a mobile robot,"We present a comparison of four optical flow methods and three spatio-temporal filters for mobile robot navigation in corridor-like environments. Previous comparisons of optical flow methods have evaluated performance only in terms of accuracy and/or efficiency, and typically in isolation. These comparisons are inadequate for addressing applicability to continuous, real-time operation as part of a robot control loop. We emphasise the need for comparisons that consider the context of a system, and that are confirmed by in-system results. To this end, we give results for on and off-board trials of two biologically inspired behaviours: corridor centring and visual odometry. Our results show the best in-system performances are achieved using Lucas and Kanade's gradient-based method in combination with a recursive temporal filter. Results for traditionally used Gaussian filters indicate that long latencies significantly impede performance for real-time tasks in the control loop.","Image motion analysis,
Navigation,
Mobile robots,
Optical filters,
Biomedical optical imaging,
Optical sensors,
Image sequences,
Computer science,
Software engineering,
Australia"
A new approach for on-line placement on reconfigurable devices,"Summary form only given. By increasing the amount of resources on reconfigurable platforms with the ability of partial reconfigurability, the issues of the management of these resources and their sharing among different tasks will become more of a concern. Online placement is one of these management issues that are investigated. We present a new approach for online placement of modules on reconfigurable devices, by managing the occupied space rather the free space on the device. Also an optimization of communication between running modules themselves and outside of the chip is proposed. The experimental results show a considerable decrease in communication and routing costs.","Runtime,
Computer science,
Resource management,
Routing,
Costs,
Manufacturing,
Distributed processing,
Dynamic scheduling"
Improving object classification in far-field video,"Object classification in far-field video sequences is a challenging problem because of low-resolution imagery and projective image distortion. Most existing far-field classification systems are trained to work well in a constrained set of scenes, but can fail dramatically when applied to new scenes, or even different views of the same scene. We identify discriminative object features for classifying vehicles and pedestrians and develop a scene-invariant classification system that is trained on a small number of labeled examples from a few scenes, but transfers well to a wide range of new scenes. Simultaneously, we demonstrate that use of scene-specific context features (such as image position and direction of motion of objects) can greatly improve classification in any given scene. To combine these ideas, we propose a new algorithm for adapting a scene-invariant classifier to scene-specific features by retraining with the help of unlabelled data in a novel scene. Experimental results demonstrate the effectiveness of our context features and scene-transfer/adaptation algorithm for multiple urban and highway scenes.",
Analysis of rotational robustness of hand detection with a Viola-Jones detector,The research described in this paper analyzes the in-plane rotational robustness of the Viola-Jones object detection method when used for hand appearance detection. We determine the rotational bounds for training and detection for achieving undiminished performance without an increase in classifier complexity. The result - up to 15/spl deg/ total - differs from the method's performance on faces (30/spl deg/ total). We found that randomly rotating the training data within these bounds allows for detection rates about one order of magnitude better than those trained on strictly aligned data. The implications of the results effect both savings in training costs as well as increased naturalness and comfort of vision-based hand gesture interfaces.,
A laboratory testbed for embedded computer control,"There has been a tremendous growth in the use of modern embedded computers in control and other applications in the past few years. While courses offered in the electrical and computer engineering disciplines cover such topics as microprocessors, digital and analog hardware, control theory, and programming languages, there exist few courses that focus on integrating these subjects for designing embedded systems. On the other hand, there is a growing need in industry for engineers who can perform software design and system integration for various applications in embedded control. Toward filling this gap, this paper describes a laboratory testbed developed for a new course on Embedded Computer Control offered at the University of Western Ontario, London, ON, Canada. An outline of the course structure, laboratory setup, and the design aspects for implementing a modern embedded control application are presented. The embedded controller performs command, control, and user interface tasks required to operate a low-cost prototype of a thermal system. Furthermore, its network connectivity allows users to tune system parameters, start and stop running the system, and observe the status of the plant via the Internet.","User interfaces,
Digital control,
Control engineering education,
Computer science education,
Internet"
The case for a multi-hop wireless local area network,"We propose a multi-hop wireless LAN architecture and demonstrate its benefits to wireless clients. For this architecture, we define implementation paths that allow interoperation with existing wireless LANs which can lead to an incremental deployment of this system. We quantify the performance benefits of the proposed schemes through measurements in realistic wireless LAN environments. We also examine the performance of such multi-hop wireless LANs through detailed simulation studies. Our results show that these multi-hop extensions can significantly improve the wireless access experience (in terms of data throughput, latency, etc.) for clients who enable such mechanisms. More interestingly, when multi-hop extensions are enabled by some of the clients, it also positively impacts the performance at other clients that are completely unaware of these extensions.","Computer aided software engineering,
Spread spectrum communication,
Wireless LAN,
USA Councils,
Internet,
Computer science,
Educational institutions,
Throughput,
Electronic switching systems,
Computer architecture"
Supporting cooperative caching in ad hoc networks,"Most researches in ad hoc networks focus on routing, and not much work has been done on data access. A common technique used to improve the performance of data access is caching. Cooperative caching, which allows the sharing and coordination of cached data among multiple nodes, can further explore the potential of the caching techniques. Due to mobility and resource constraints of ad hoc networks, cooperative caching techniques designed for wired network may not be applicable to ad hoc networks. In this paper, we design and evaluate cooperative caching techniques to efficiently support data access in ad hoc networks. We first propose two schemes: cachedata which caches the data, and cachepath which caches the data path. After analyzing the performance of those two schemes, we propose a hybrid approach (hybridcache) which can further improve the performance by taking advantage of cachedata and cachepath while avoiding their weaknesses. Simulation results show that the proposed schemes can significantly reduce the query delay and message complexity when compared to other caching schemes.","Cooperative caching,
Intelligent networks,
Ad hoc networks,
Bandwidth,
Delay,
Computer science,
Data engineering,
Electronic mail,
Performance analysis,
Mobile ad hoc networks"
Error propagation in software architectures,"The study of software architectures is emerging as an important discipline in software engineering, due to its emphasis on large scale composition of software products, and its support for emerging software engineering paradigms such as product line engineering, component based software engineering, and software evolution. Architectural attributes differ from code-level software attributes in that they focus on the level of components and connectors, and that they are meaningful for an architecture. In this paper, we focus on a specific architectural attribute, which is the error propagation probability throughout the architecture, i.e. the probability that an error that arises in one component propagates to other components. We introduce, analyze, and validate formulas for estimating these probabilities using architectural level information.","Computer architecture,
Software engineering,
Software metrics,
Computer errors,
NASA,
Software architecture,
Information analysis,
Programming,
Computer science,
Software safety"
Double-covered broadcast (DCB): a simple reliable broadcast algorithm in MANETs,"Mobile ad hoc networks (MANETs) suffer from high transmission error rate because of the nature of radio communications. The broadcast operation, as a fundamental service in MANETs, is prone to the broadcast storm problem if forward nodes are not carefully designated. The objective of reducing the broadcast redundancy while still providing high delivery ratio for each broadcast packet is a major challenge in a dynamic environment. In this paper, we propose a simple, reliable broadcast algorithm, called double-covered broadcast (DCB), that takes advantage of broadcast redundancy to improve the delivery ratio in the environment that has rather high transmission error rate. Among 1-hop neighbors of the sender, only selected forward nodes retransmit the broadcast message. Forward nodes are selected in such a way that (1) the sender's 2-hop neighbors are covered and (2) the sender's 1-hop neighbors are either a forward node, or a nonforward node but covered by at least two forwarding neighbors. The retransmissions of the forward nodes are received by the sender as confirmation of their receiving the packet. The nonforward 1-hop neighbors of the sender do not acknowledge the reception of the broadcast. If the sender does not detect all its forward nodes' retransmissions, it will resend the packet until the maximum times of retry is reached. Simulation results show that the algorithm provides good performance for a broadcast operation under high transmission error rate environment","Mobile ad hoc networks,
Radio broadcasting,
Error analysis,
Computer network reliability,
Telecommunication network reliability,
Storms,
Redundancy,
Intelligent networks,
Computer science,
Reliability engineering"
A trust model based routing protocol for secure ad hoc networks,"Security issues have been emphasized when mobile ad hoc networks (MANETs) are employed into military and aerospace fields. We design a novel secure routing protocol for MANETs. This protocol TAODV (Trusted AODV) extends the widely used AODV (ad hoc on-demand distance vector) routing protocol and employs the idea of a trust model to protect routing behaviors in the network layer of MANETs. In the TAODV, trust among nodes is represented by opinion, which is an item derived from subjective logic. The opinions are dynamic and updated frequently as our protocol specification: if one node performs normal communications, its opinion from other nodes' points of view can be increased; otherwise, if one node performs some malicious behaviors, it is ultimately denied by the whole network. A trust recommendation mechanism is also designed to exchange trust information among nodes. The salient feature of TAODV is that, using trust relationships among nodes, there is no need for a node to request and verify certificates all the time. This greatly reduces the computation overheads. Meanwhile, with neighbors' trust recommendations, a node can make objective judgement about another node's trustworthiness to maintain the whole system at a certain security level.","Routing protocols,
Peer to peer computing,
Logic,
Mobile ad hoc networks,
Aerodynamics,
Computer science,
Ad hoc networks,
Military computing,
Aerospace engineering,
Protection"
Enforcing robust declassification,"Noninterference requires that there is no information flow from sensitive to public data in a given system. However, many systems perform intentional release of sensitive information as part of their correct functioning and therefore violate noninterference. To control information flow while permitting intentional information release, some systems have a downgrading or declassification mechanism. A major danger of such a mechanism is that it may cause unintentional information release. This paper shows that a robustness property can be used to characterize programs in which declassification mechanisms cannot be exploited by attackers to release more information than intended. It describes a simple way to provably enforce this robustness property through a type-based compile-time program analysis. The paper also presents a generalization of robustness that supports upgrading (endorsing) data integrity.","Robustness,
Information security,
Mechanical factors,
Data security,
Computer science,
Control systems,
Computer languages,
Computer security,
Data flow computing,
Information science"
Vibrotactile letter reading using a low-resolution tactor array,"Vibrotactile displays have been studied for several decades in the context of sensory substitution. Recently, a number of vibrotactile displays have been developed to extend sensory modalities in virtual reality. Some of these target the whole body as the stimulation region, but existing systems are only designed for discrete stimulation points at specific parts of the body. However, since human tactile sensation has more resolution, a higher density might be required in factor alignment in order to realize general-purpose vibrotactile displays. One problem with this approach is that it might result in an impractically high number of required tactors. Our current focus is to explore ways of simplifying the system while maintaining an acceptable level of expressive ability. As a first step, we chose a well-studied task: tactile letter reading. We examined the possibility of distinguishing alphanumeric letters by using only a 3-by-3 array of vibrating motors on the back of a chair. The tactors are driven sequentially in the same sequence as if someone were tracing the letter on the chair's back. The results showed 87% successful letter recognition in some cases, which was close to the results in previous research with much larger arrays.","Computer displays,
Virtual reality,
Humans,
Virtual environment,
Haptic interfaces,
Information science,
Laboratories,
Computer science,
Knowledge engineering,
Elbow"
On using reputations in ad hoc networks to counter malicious nodes,"Nodes in mobile ad hoc networks have a limited transmission range. Hence the nodes expect their neighbors to relay packets meant for far off destinations. These networks are based on the fundamental assumption that if a node promises to relay a packet, it relays it and does not cheat. This assumption becomes invalid when the nodes in the network have tangential or contradictory goals. The reputations of the nodes, based on their past history of relaying packets, can be used by their neighbors to ensure that the packet is relayed by the node. This paper introduces a reputation scheme for ad hoc networks. Instead of choosing the shortest path to the destination, the source node chooses a path whose next hop node has the highest reputation. This policy, when used recursively, in the presence of 40% malicious nodes, improves the throughput of the system to 65%, from 22 % throughput provided by AODV. This improvement is obtained at the cost of a higher number of route discoveries with a minimal increase in the average hop length according S. Bansal and M. Baker (2003).","Intelligent networks,
Ad hoc networks,
Counting circuits,
Relays,
Throughput,
Mobile ad hoc networks,
Routing protocols,
Computer science,
History,
Costs"
A computer-aided diagnosis for locating abnormalities in bone scintigraphy by a fuzzy system with a three-step minimization approach,"Bone scintigraphy is an effective method to diagnose bone diseases such as bone tumors. In the scintigraphic images, bone abnormalities are widely scattered on the whole body. Conventionally, radiologists visually check the whole-body images and find the distributed abnormalities based on their expertise. This manual process is time-consuming and it is not unusual to miss some abnormalities. In this paper, a computer-aided diagnosis (CAD) system is proposed to assist radiologists in the diagnosis of bone scintigraphy. The system will provide warning marks and abnormal scores on some locations of the images to direct radiologists' attention toward these locations. A fuzzy system called characteristic-point-based fuzzy inference system (CPFIS) is employed to implement the diagnosis system and three minimizations are used to systematically train the CPFIS. Asymmetry and brightness are chosen as the two inputs to the CPFIS according to radiologists' knowledge. The resulting CAD system is of a small-sized rule base such that the resulting fuzzy rules can be not only easily understood by radiologists, but also matched to and compared with their expert knowledge. The prototype CAD system was tested on 82 abnormal images and 27 normal images. We employed free-response receiver operating characteristics method with the mean number of false positives (FPs) and the sensitivity as performance indexes to evaluate the proposed system. The sensitivity is 91.5% (227 of 248) and the mean number of FPs is 37.3 per image. The high sensitivity and moderate numbers of FP marks per image shows that the proposed method can provide an effective second-reader information to radiologists in the diagnosis of bone scintigraphy.","Computer aided diagnosis,
Fuzzy systems,
Bone diseases,
Minimization methods,
Neoplasms,
Biomedical imaging,
Medical diagnostic imaging,
Pixel,
Scattering,
Brightness"
An adaptive scheme for vertical handoff in wireless overlay networks,"Vertical handoff is the switching process between heterogeneous wireless networks. Discovering the reachable wireless networks is the first step for vertical handoff. After discovering the reachable candidate networks, the mobile terminal decides whether to perform handoff or not. We present an adaptive scheme for vertical handoff in wireless overlay networks. Our system discovery method effectively discovers the candidate networks for the mobile terminal. Moreover, we propose two adaptive evaluation methods for the mobile terminal to determine the handoff time that relies on the candidates' resources and the running applications. The simulation results show that the proposed system discovery method can balance the power consumption and the system discovery time. Furthermore, the proposed handoff decision method can decide the appropriate time to perform handoff.","Intelligent networks,
Wireless networks,
Bandwidth,
Stability,
Mobile communication,
Base stations,
Computer science education,
Councils,
Computer science,
Energy consumption"
An end-to-end multipath smooth handoff scheme for stream media,"Supporting transmission of stream media over wireless mobile networks is often difficult because packets may be lost due to the rerouting of packets during handoff, and also because bursts of packet loss may occur during handoff due to the disparity in the amount of available bandwidth among different cells. In this paper, we propose an end-to-end multipath handoff scheme that provides smooth handoff for stream media in wireless networks with different amounts of available bandwidth from cell to cell. In the proposed scheme, multiple paths are established during handoff to reach a mobile destination node. The stream media sources are equipped with an adaptive multilayer encoder, and important layers in the encoded video stream are duplicated and transmitted over multiple paths during handoff. The effectiveness of the proposed multipath handoff scheme is verified and compared with existing schemes through extensive simulations. The simulation results show that the proposed scheme provides higher throughput and better quality for stream media.","Streaming media,
Propagation losses,
Bandwidth,
Nonhomogeneous media,
Degradation,
Computer science,
Wireless networks,
Wireless LAN,
Multiaccess communication,
Throughput"
Automated gesture segmentation from dance sequences,"Complex human motion (e.g. dance) sequences are typically analyzed by segmenting them into shorter motion sequences, called gestures. However, this segmentation process is subjective, and varies considerably from one choreographer to another. Dance sequences also exhibit a large vocabulary of gestures. In this paper, we propose an algorithm called hierarchical activity segmentation. This algorithm employs a dynamic hierarchical layered structure to represent human anatomy, and uses low-level motion parameters to characterize motion in the various layers of this hierarchy, which correspond to different segments of the human body. This characterization is used with a naive Bayesian classifier to derive choreographer profiles from empirical data that are used to predict how particular choreographers segment gestures in other motion sequences. When the predictions were tested with a library of 45 3D motion capture sequences (with 185 distinct gestures) created by 5 different choreographers, they were found to be 93.3% accurate.","Motion analysis,
Vocabulary,
Libraries,
Hidden Markov models,
Ubiquitous computing,
Computer science,
Heuristic algorithms,
Human anatomy,
Bayesian methods,
Testing"
Marking estimation of Petri nets with silent transitions,"In this paper we deal with the problem of estimating the marking of a labeled Petri net system based on the observation of transitions labels. In particular, we assume that a certain number of transitions are labeled with the empty string /spl epsi/, while a different label taken from a given alphabet is assigned to all the other transitions. Transitions labeled with the empty string are called silent because their firing cannot be observed. Under some technical assumptions on the structure of the T/sub /spl epsiv// -induced subnet, where T/sub /spl epsiv// denotes the set of silent transitions, we formally prove that the set of markings consistent with the observed word can be represented by a linear system with a fixed structure that does not depend on the length of the observed word.",
A flash compression layer for SmartMedia card systems,"Flash memory based SmartMedia card is becoming increasingly popular as data storage for mobile consumer electronics. Since flash memory is an order of magnitude more expensive than magnetic disks, data compression can be effectively used in managing flash memory based storage systems. However, compressed data management in flash memory is challenging because it only supports page-based I/Os. For example, when the size of compressed data is smaller than the page size, internal fragmentation occurs and this degrades the effectiveness of compression seriously. In this paper, we developed a flash compression layer (FCL) for the SmartMedia card systems. FCL stores several small compressed pages into one physical page by using a write buffer. Based on prototype implementation and simulation studies, we show that the proposed system offers the storage of flash memory more than 140% of its original size and expands the write bandwidth significantly.",
Registration of real-time 3-D ultrasound images of the heart for novel 3-D stress echocardiography,"Stress echocardiography is a routinely used clinical procedure to diagnose cardiac dysfunction by comparing wall motion information in prestress and poststress ultrasound images. Incomplete data, complicated imaging protocols and misaligned prestress and poststress views, however, are known limitations of conventional stress echocardiography. We discuss how the first two limitations are overcome via the use of real-time three-dimensional (3-D) ultrasound imaging, an emerging modality, and have called the new procedure ""3-D stress echocardiography:"" We also show that the problem of misaligned views can be solved by registration of prestress and poststress 3-D image sequences. Such images are misaligned because of variations in placing the ultrasound transducer and stress-induced anatomical changes. We have developed a technique to temporally align 3-D images of the two sequences first and then to spatially register them to rectify probe placement error while preserving the stress-induced changes. The 3-D spatial registration is mutual information-based. Image registration used in conjunction with 3-D stress echocardiography can potentially improve the diagnostic accuracy of stress testing.","Ultrasonic imaging,
Heart,
Stress,
Echocardiography,
Protocols,
Image sequences,
Ultrasonic transducers,
Probes,
Image registration,
Testing"
An adaptive high-order neural tree for pattern recognition,"A new neural tree model, called adaptive high-order neural tree (AHNT), is proposed for classifying large sets of multidimensional patterns. The AHNT is built by recursively dividing the training set into subsets and by assigning each subset to a different child node. Each node is composed of a high-order perceptron (HOP) whose order is automatically tuned taking into account the complexity of the pattern set reaching that node. First-order nodes divide the input space with hyperplanes, while HOPs divide the input space arbitrarily, but at the expense of increased complexity. Experimental results demonstrate that the AHNT generalizes better than trees with homogeneous nodes, produces small trees and avoids the use of complex comparative statistical tests and/or a priori selection of large parameter sets.","Pattern recognition,
Classification tree analysis,
Testing,
Neural networks,
Mathematics,
Computer science,
Multidimensional systems,
Feature extraction,
Decision trees"
H/sub /spl infin// control of nonperiodic two-dimensional channel flow,"This paper deals with finite-dimensional boundary control of the two-dimensional (2-D) flow between two infinite parallel planes. Surface transpiration along a few regularly spaced sections of the bottom wall is used to control the flow. Measurements from several discrete, suitably placed shear-stress sensors provide the feedback. Unlike other studies in this area, the flow is not assumed to be periodic, and spatially growing flows are considered. Using spatial discretization in the streamwise direction, frequency responses for a relevant part of the channel are obtained. A low-order model is fitted to these data and the modeling uncertainty is estimated. An H/sub /spl infin// controller is designed to guarantee stability for the model set and to reduce the wall-shear stress at the channel wall. A nonlinear Navier-Stokes PDE solver was used to test the designs in the loop. The only assumption made in these simulations is that the flow is two dimensional. The results showed that, although the problem was linearized when designing the controller, the controller could significantly reduce fundamental 2-D disturbances in practice.","Fluid flow control,
Sensor arrays,
Fluid dynamics,
Computer science,
Actuators,
Two dimensional displays,
Feedback,
Frequency,
Uncertainty,
Stress control"
A short guide to predator-prey lattice models,"In broad terms, a predator-prey model describes the dynamics of two kinds of entities: specifically, one kind could be destroyed on contact with the other. The review contains no detailed formulas, model descriptions, or techniques; rather, it presents the authors' assumptions, main results, and conclusions.","Lattices,
Biological system modeling,
Evolution (biology),
Animals,
Biology computing,
Computer simulation,
Differential equations,
Biological systems,
Personal communication networks"
Measuring and understanding user comfort with resource borrowing,"Resource borrowing is a common underlying approach in grid computing and thin-client computing. In both cases, external processes borrow resources that would otherwise be delivered to the interactive processes of end-users, creating contention that slows these processes and decreases the comfort of the end-users. How resource borrowing and user comfort are related is not well understood and thus resource borrowing tends to be extremely conservative. To address this lack of understanding, we have developed a sophisticated distributed application for directly measuring user comfort with the borrowing of CPU time, memory space, and disk bandwidth. Using this tool, we have conducted a controlled user study with qualitative and quantitative results that are of direct interest to the designers of grid and thin-client systems. We have found that resource borrowing can be quite aggressive without creating user discomfort, particularly in the case of memory and disk. We also describe an on-going Internet-wide study using our tool.","Peer to peer computing,
Computer science,
Grid computing,
Bandwidth,
Control systems,
Distributed computing,
Scientific computing,
Proteins,
Proposals,
Power generation economics"
Solving distributed constraint optimization problems using cooperative mediation,"Distributed Constraint Optimization Problems (DCDP) have, for a long time, been considered an important research area for multi-agent systems because a vast number of real-world situations can be modeled by them. The goal of many of the researchers interested in DCOP has been to find ways to solve them efficiently using fully distributed algorithms which are often based on existing centralized techniques. In this paper, we present an optimal, distributed algorithm called optimal asynchronous partial overlay (OptAPO) for solving DCOPs that is based on a partial centralization technique called cooperative mediation. The key ideas used by this algorithm are that agents, when acting as a mediator, centralize relevant portions of the DCDP, that these centralized subproblems overlap, and that agents increase the size of their subproblems as the problem solving unfolds. We present empirical evidence that shows that OptAPO performs better than other known, optimal DCOP techniques.","Constraint optimization,
Mediation,
Problem-solving,
Government,
Distributed algorithms,
Iterative algorithms,
Computer science,
Multiagent systems,
Permission,
Yarn"
Exploring extreme programming in context: an industrial case study,"A longitudinal case study evaluating the effects of adopting the extreme programming (XP) methodology was performed at Sabre Airline Solutions/spl trade/. The Sabre team was a characteristically agile team in that they had no need to scale or re-scope XP for their project parameters and organizational environment. The case study compares two releases of the same product. One release was completed just prior to the team's adoption of the XP methodology, and the other was completed after approximately two years of XP use. Comparisons of the new release project results to the old release project results show a 50% increase in productivity, a 65% improvement in pre-release quality, and a 35% improvement in post-release quality. These findings suggest that, over time, adopting the XP process can result in increased productivity and quality.","Computer aided software engineering,
Productivity,
Programming profession,
Computer science,
Educational institutions,
Performance evaluation,
Graphical user interfaces,
Data analysis,
Lead,
Hydrogen"
Keeping it too simple: how the reductive tendency affects cognitive engineering,"Certain features of tasks make especially difficult for humans. These constitute leverage points for applying intelligent technologies, but there's a flip side. Designing complex cognitive systems is itself a tough task. Cognitive engineers face the same challenges in designing systems that users confront in working the tasks that the systems are intended to aid. We discuss about these issues. We assume that the cognitive engineers will invoke one or more knowledge shields when they are confronted with evidence that their understanding and planning involves a reductive understanding. The knowledge shield phenomenon suggests that it will take effort to change the reductive mindset that people might bring to design a CCS.","Cognition,
Humans,
Intelligent systems,
Machine intelligence,
Guidelines,
Organic materials,
Cognitive science,
Resists,
Protection,
Blood"
Minimizing energy consumption in sensor networks using a wakeup radio,"For increasing the life of sensor networks, each node must conserve energy as much as possible. In this paper, we propose a protocol in which energy is conserved by amortizing the energy cost of communication over multiple packets. In addition, we allow sensors to control the amount of buffered packets since storage space is limited. To achieve this, a two-radio architecture is used which allows a sensor to ""wakeup"" a neighbor with a busy tone and send its packets for that destination. However, this process is expensive because all neighbors must awake and listen to the primary channel to determine who is the intended destination. Therefore, triggered wakeups on the primary channel are proposed to avoid using the more costly wakeup procedure. We present a protocol for efficiently determining how large the period for these wakeups should be such that energy consumption is minimized.",
Iterative construction of reversible variable-length codes and variable-length error-correcting codes,"We propose a generic algorithm for the construction of efficient reversible variable-length codes (RVLCs) and variable-length error-correcting (VLEC) codes, which optimizes the codeword length distribution. The algorithm may be applied to any existing codeword selection mechanism, and it is capable of generating codes of higher efficiency in comparison to the algorithms disseminated in the literature.","Error correction codes,
Decoding,
Iterative algorithms,
Visual effects,
Telephony,
Standards development,
MPEG 4 Standard,
Redundancy,
Computer science,
Computer errors"
Relating /spl pi/-calculus to Object-Z,"Software systems have become increasingly distributed, dynamic and mobile. The complex state and dynamic interfaces of software components and their concurrent interactions provide challenging research issues in system specification and design. An effective combination of structured state-based formalism and dynamic action-based calculus may be a good solution for modeling complex distributed mobile systems. In this paper, we investigate the semantic links between Object-Z and /spl pi/ calculus and consequently introduce a powerful specification technique PiOZ that brings the strengths of the two together. The operational semantics of PiOZ integrates state transition semantics of Object-Z and /spl pi/-calculus reduction rules. The typing rules of PiOZ are developed and reasoning of a system property is presented.","Power system modeling,
Calculus,
Computer science,
Software systems,
Mobile communication,
Sections,
Mobile computing,
Animation,
Systems engineering and theory,
Navigation"
Assessment of perfusion by dynamic contrast-enhanced imaging using a deconvolution approach based on regression and singular value decomposition,"The assessment of tissue perfusion by dynamic contrast-enhanced (DCE) imaging involves a deconvolution process. For analysis of DCE imaging data, we implemented a regression approach to select appropriate regularization parameters for deconvolution using the standard and generalized singular value decomposition methods. Monte Carlo simulation experiments were carried out to study the performance and to compare with other existing methods used for deconvolution analysis of DCE imaging data. The present approach is found to be robust and reliable at the levels of noise commonly encountered in DCE imaging, and for different models of the underlying tissue vasculature. The advantages of the present method, as compared with previous methods, include its efficiency of computation, ability to achieve adequate regularization to reproduce less noisy solutions, and that it does not require prior knowledge of the noise condition. The proposed method is applied on actual patient study cases with brain tumors and ischemic stroke, to illustrate its applicability as a clinical tool for diagnosis and assessment of treatment response.","Deconvolution,
Singular value decomposition,
Magnetic resonance imaging,
Image analysis,
Noise robustness,
Neoplasms,
Medical treatment,
In vivo,
Blood flow,
Robust stability"
Automating experiments using semantic data in a bioinformatics grid,"The transition from laboratory science to in silico e-science has facilitated a paradigmatic shift in the way we conduct modern science. We can use computationally based analytical models to simulate and investigate scientific questions such as those posed by high-energy physics and bioinformatics, yielding high-quality results and discoveries at an unprecedented rate. However, while experimental media have changed, the scientific methodologies and processes we choose for conducting experiments are still relevant. As in the lab environment, experimental methodology requires samples to undergo several processing stages. The staging of operations is what constitutes the in silico experimental process. The use of workflows formalizes earlier ad hoc approaches for representing experimental methodology. We can represent the stages of in silico experiments formally as a set of services to invoke.","Bioinformatics,
Laboratories,
Analytical models,
Physics computing,
Computational modeling,
Web services,
Scalability,
Human factors,
Intellectual property,
Genetics"
Representation and Recognition of Events in Surveillance Video Using Petri Nets,"Detection of events is an essential task in surveillance applications. This task requires finding a general event representation method and developing efficient recognition algorithms dealing with this representation. In this paper, we describe an interactive system for querying surveillance video about events. The queries may not be known in advance and have to be composed from primitive events and previously defined queries. We propose using Petri nets as both representation and recognition methods. The Petri net representation for users' queries is derived automatically from simpler event nets. Recognition is then performed by tokens moving through the Petri nets.","Surveillance,
Petri nets,
Event detection,
Interactive systems,
Stochastic processes,
Computer vision,
Ontologies,
Mathematical model,
Computer science,
Educational institutions"
A bus encoding technique for power and cross-talk minimization,"Considerable research has been done in the area of bus-encoding techniques, for either power minimization or cross-talk elimination in system-level buses, but not both together. We propose No Adjacent Transition (NAT) coding scheme, a bus encoding technique that simultaneously reduces power consumption and eliminates cross-talk. NAT-encoding and decoding algorithms are proposed and an analytical study of power dissipation is presented.","Encoding,
Power dissipation,
Network address translation,
Decoding,
Minimization,
Energy consumption,
Signal design,
Wires,
Computer science,
Power engineering and energy"
Handling interaction in fuzzy production rule reasoning,"When fuzzy production rules are used to approximate reasoning, interaction exists among rules that have the same consequent. Due to this interaction, the weighted average model frequently used in approximate reasoning does not work well in many real-world problems. In order to model and handle this interaction, this paper proposes to use a nonadditive nonnegative set function to replace the weights assigned to rules having the same consequent, and to draw the reasoning conclusion based on an integral with respect to the nonadditive nonnegative set function, rather than on the weighted average model. Handling interaction in fuzzy production rule reasoning in this way can lead to a good understanding of the rules base and an improvement of reasoning accuracy. This paper also investigates how to determine from data the nonadditive set function that cannot be specified by a domain expert.","Fuzzy reasoning,
Fuzzy sets,
Production systems,
Fuzzy systems,
Uncertainty,
Expert systems,
Mathematics,
Computer science,
Petri nets"
SCHISM: a new approach for interesting subspace mining,"High-dimensional data pose challenges to traditional clustering algorithms due to their inherent sparsity and data tend to cluster in different and possibly overlapping subspaces of the entire feature space. Finding such subspaces is called subspace mining. We present SCHISM, a new algorithm for mining interesting subspaces, using the notions of support and Chernoff-Hoeffding bounds. We use a vertical representation of the dataset, and use a depth-first search with backtracking to find maximal interesting subspaces. We test our algorithm on a number of high-dimensional synthetic and real datasets to test its effectiveness.","Clustering algorithms,
Testing,
Partitioning algorithms,
Karhunen-Loeve transforms,
Engineering profession,
Computer science,
Unsupervised learning,
Multidimensional systems,
Singular value decomposition,
US Department of Energy"
Integration of mobile ad hoc networks and the Internet using mobile gateways,"Summary form only given. Mobile ad hoc networks (MANET) and the Internet exhibit differences in their network architecture. These differences concern the various sorts of assumptions imposed not only on the structure and topology of the underlying networks, but also on communication patterns of mobile nodes in both networks. Integrating MANET and the Internet into a hybrid network is a challenging problem due to these differences. We propose a three-layer approach that uses both mobile IP and dynamic destination-sequenced distance vector (DSDV) to integrate these two types of networks into a hybrid environment, in order to provide MANET nodes with Internet connectivity and access to the Internet resources. Our approach is based on the use of mobile gateways as an interface between MANET and the Internet. These mobile gateways can use mobile IP when they communicate with the Internet and DSDV when they interact with MANET. We also show the results of several simulation experiments that were conducted to study the integrated environment.","Mobile ad hoc networks,
IP networks,
Internet,
Mobile computing,
Computer networks,
Mobile communication,
Routing protocols,
Concurrent computing,
Parallel processing,
Computer science"
A high-throughput whole-body PET scanner using flat panel PS-PMTs,"A new positron emission tomography (PET) scanner for whole-body studies has been developed. The scanner has 720 block detectors, each of which consists of a flat panel position-sensitive photomultiplier and a 16/spl times/8 BGO crystal array. The detector system is composed of 12 layers of block detector rings stacked axially, and each ring consists of a circular array of 60 block detectors. Since each block detector ring contains eight crystal rings, the total number of detector rings is 96 and the axial field of view (FOV) is 685 mm, which is sufficient to measure a whole-body with two steps of scan. The detector ring diameter is 1 020 mm and the transaxial FOV is 600 mm in diameter. Coarse slice septa are placed between the block detector rings, which are effective to reduce scattered coincidence events while keeping a high detection sensitivity. Parallel 16 personal computers are used for the data acquisition and processing to deal with a large amount of event data. The acquired data in 3-D manner are converted to 2-D data set with Fourier rebinning algorithm and reconstructed with a fast iterative algorithm ""DRAMA"". The reconstructed images for the whole body are obtained within 5 min after the scan. By using the coarse septa, the scatter fraction measured with NEMA NU2-2001 standard was 31.4% whose value was significantly lower than that of 3-D PET without septa. The system sensitivity was 9.72 cps/kBq, and the peak counts of the noise equivalent count rate used with direct random-subtraction was 113.6 kcps at 10.5 kBq/ml.","Whole-body PET,
Sensor arrays,
Event detection,
Scattering,
Iterative algorithms,
Image reconstruction,
Positron emission tomography,
Position sensitive particle detectors,
Photomultipliers,
Microcomputers"
A hierarchical N-Queen decimation lattice and hardware architecture for motion estimation,"A subsampling structure, an N-Queen lattice, for spatially decimating a block of pixels is presented. Despite its use for many applications, we demonstrate that the N-Queen lattice can be used to speed up motion estimation with nominal loss of coding efficiency. With a simple construction, the N-Queen lattice characterizes the spatial features in the vertical, horizontal, and diagonal directions for both texture and edge areas. Especially in the 4-Queen case, every skipped pixel has the minimal and equal distance of unity to the selected pixel. It can be hierarchically organized for variable nonsquare block-size motion estimation. Despite the randomized lattice, we design compact data storage architecture for efficient memory access and simple hardware implementation. Our simulations show that the N-Queen lattice is superior to several existing sampling techniques with improvement in speed by about N times and small loss in peak SNR (PSNR). The loss in PSNR is negligible for slow-motion video sequences and is less than 0.45 dB at worst for high-motion estimation sequences.","Lattices,
Hardware,
Motion estimation,
Sampling methods,
Computer science,
PSNR,
Video coding,
Memory architecture,
Video sequences,
Distortion measurement"
An object-based approach for detecting small brain lesions: application to Virchow-Robin spaces,"This paper is concerned with the detection of multiple small brain lesions from magnetic resonance imaging (MRI) data. A model based on the marked point process framework is designed to detect Virchow-Robin spaces (VRSs). These tubular shaped spaces are due to retraction of the brain parenchyma from its supplying arteries. VRS are described by simple geometrical objects that are introduced as small tubular structures. Their radiometric properties are embedded in a data term. A prior model includes interactions describing the clustering property of VRS. A Reversible Jump Markov Chain Monte Carlo algorithm (RJMCMC) optimizes the proposed model, obtained by multiplying the prior and the data model. Example results are shown on T/sub 1/-weighted MRI datasets of elderly subjects.",
Touchless monitoring of breathing function,"We have developed a novel method for noncontact measurement of breathing function. The method is based on statistical modeling of dynamic thermal data captured through an infrared imaging system. The expired air has higher temperature than the typical background of indoor environments (e.g., walls). Therefore, the particles of the expired air emit at a higher power than the background, a phenomenon which is captured as a distinct thermal signature in the infrared imagery. There is significant technical difficulty in computing this signature, however, because the phenomenon is of very low intensity and transient nature. We use an advanced statistical algorithm based on the method of moments and the Jeffrey's divergence measure to address the problem. So far, we were able to compute correctly the breathing waveforms for ten (10) subjects at distances ranging from 6-8 feet. The results were checked against concomitant ground-truth data collected with a traditional contact sensor. The technology is expected to find applications in the next generation of touchless polygraphy and in preventive health care.","Infrared imaging,
Abdomen,
Temperature,
Electrocardiography,
Computer science,
Patient monitoring,
Electrodes,
Heart,
Band pass filters,
Strain measurement"
An adaptive protocol for efficient support of range queries in DHT-based systems,"In recent years, distributed hash tables (DHTs) have been proposed as a fundamental building block for large scale distributed applications. Important functionalities such as searching have been added to the DHTs basic lookup capability. However, supporting range queries efficiently remains a difficult problem. We describe an adaptive mechanism that relies on a logical tree data structure, the range search tree (RST), to support range queries efficiently. Nodes in the RST automatically group registrations based on their values. Queries are decomposed into a small number of sub-queries for efficient resolution. The system dynamically optimizes itself to minimize the registration and query cost based on observed load. The system is fully distributed and avoids bottleneck problems encountered in traditional tree-based systems. Extensive simulation results validate the effectiveness of the system.","Protocols,
Cameras,
Large-scale systems,
Tree data structures,
Road transportation,
Computer science,
Application software,
Cost function,
Robustness,
Information retrieval"
Fuzzy semantic distance measures between ontological concepts,"An emphasis has been placed on the use of ontologies for representing application domain knowledge. Determining a degree or measure of semantic similarity, semantic distance, or semantic relatedness between concepts from different systems or domains, is becoming an increasingly important task. This paper presents a brief overview of such measures between concepts within ontological representations and provides several examples of such measures found in the research literature. These measures are then examined within the framework of fuzzy set similarity measures. The use of a semantic similarity measure between elements that are part of a domain for which an ontological structure exists is explored in order to extend standard fuzzy set compatibility measures.","Ontologies,
Fuzzy sets,
Information retrieval,
Vocabulary,
Logic,
Computer science,
Application software,
Measurement standards,
Software systems,
Business"
Incrementally reducing dispersion by increasing Voronoi bias in RRTs,"We discuss theoretical and practical issues related to using Rapidly-Exploring Random Trees (RRTs) to incrementally reduce dispersion in the configuration space. The original RRT planners use randomization to create Voronoi bias, which causes the search trees to rapidly explore the state space. We introduce RRT-like planners based on exact Voronoi diagram computation, as well as sampling-based algorithms which approximate their behavior. We give experimental results illustrating how the new algorithms explore the configuration space and how they compare with existing RRT algorithms. Initial results show that our algorithms are advantageous compared to existing RRTs, especially with respect to the number of collision checks and nodes in the search tree.","Space exploration,
Sampling methods,
Dispersion,
Nearest neighbor searches,
Computer science,
Extraterrestrial measurements"
A game theory based approach for security in wireless sensor networks,"Based on cooperative game theory, we propose a new technique for handling security issues in mobile wireless sensor networks. We define a game between sensor nodes and concentrate on three fundamental factors: cooperation, reputation and quality of security. Stronger cooperation between two nodes implies more reliable data communication between them. And the more a node cooperates the better is its reputation, which decreases when misbehavior is detected. When security of the network is compromised, the percentage of exposed traffic measures the quality of security of sensor nodes. By incorporating these three factors, we cluster the sensor nodes such that within a cluster, the payoff function of all sensor nodes are close to each other, where payoff is the largest possible individual gain for each sensor according to a defined utility metric. We define one strategy set for each node, which guarantees reaching to an equilibrium point for payoff function.","Game theory,
Intelligent networks,
Wireless sensor networks,
Communication system security,
Data security,
Computer network management,
Public key,
Network servers,
Computer security,
Computer science"
Estimation of the hemodynamic response in event-related functional MRI: Bayesian networks as a framework for efficient Bayesian modeling and inference,"A convenient way to analyze blood-oxygen-level-dependent functional magnetic resonance imaging data consists of modeling the whole brain as a stationary, linear system characterized by its transfer function: the hemodynamic response function (HRF). HRF estimation, though of the greatest interest, is still under investigation, for the problem is ill-conditioned. In this paper, we recall the most general Bayesian model for HRF estimation and show how it can beneficially be translated in terms of Bayesian graphical models, leading to 1) a clear and efficient representation of all structural and functional relationships entailed by the model, and 2) a straightforward numerical scheme to approximate the joint posterior distribution, allowing for estimation of the HRF, as well as all other model parameters. We finally apply this novel technique on both simulations and real data.","Bayesian methods,
Hemodynamics,
Intelligent networks,
Magnetic resonance imaging,
Brain modeling,
Data analysis,
Image analysis,
Magnetic analysis,
Linear systems,
Transfer functions"
Virtual fences for controlling cows,"We describe a moving virtual fence algorithm for herding cows. Each animal in the herd is given a smart collar consisting of a GPS, PDA, wireless networking and a sound amplifier. Using the GPS, the animal's location can be verified relative to the fence boundary. When approaching the perimeter, the animal is presented with a sound stimulus whose effect is to move away. We have developed the virtual fence control algorithm for moving a herd. We present simulation results and data from experiments with 8 cows equipped with smart collars.","Cows,
Animals,
Global Positioning System,
Computer science,
Personal digital assistants,
Automatic control,
Automation,
Rivers,
Educational institutions,
Computer aided manufacturing"
Network topology exploration of mesh-based coarse-grain reconfigurable architectures,"Several coarse-grain reconfigurable architectures proposed recently consist of a large number of processing elements (PEs) connected in a mesh-like network topology. We study the effects of three aspects of network topology exploration on the performance of applications on these architectures: (a) changing the interconnection between PEs; (b) changing the way the network topology is traversed while mapping operations to the PEs; and (c) changing the communication delays on the interconnects between PEs. We propose network topology traversal strategies that first schedule PEs that are spatially close and that have more interconnections among them. We use an interconnect aware list scheduling heuristic as a vehicle to perform the network topology exploration experiments on a set of designs derived from DSP applications. Our experimental results show that a spiral traversal strategy, coupled with a two neighbor interconnect topology leads to good performance for the DSP benchmarks considered. Our prototype framework thus provides an exploration environment for system architects to explore and tune coarse-grain reconfigurable architectures for particular application domains.","Network topology,
Reconfigurable architectures,
Scheduling,
Digital signal processing,
Delay effects,
Microprocessors,
Computer science,
Field programmable gate arrays,
Parallel processing,
Routing"
Psychology in human-robot communication: an attempt through investigation of negative attitudes and anxiety toward robots,"In order to study short-term and long-term influence of communication robots in daily-life applications, it is necessary to develop psychological scales measuring mental states of users of robots and social trends on them. This paper focuses on negative attitudes and anxiety toward robots, and shows results obtained through development of scales measuring them.","Psychology,
Intelligent robots,
Application software,
Humans,
Computer science education,
Informatics,
Laboratories,
Sociology,
Performance evaluation,
Psychiatry"
Introduction of local memory elements in instruction set extensions,,"Hardware,
Coprocessors,
Cryptography,
Permission,
Computer aided instruction,
Embedded computing,
Computer science,
Manuals,
Memory management,
Registers"
TCP startup performance in large bandwidth networks,"Next generation networks with large bandwidth and long delay pose a major challenge to TCP performance, especially during the startup period. We evaluate the performance of TCP Reno/Newreno, Vegas and Hoe's modification in large bandwidth delay networks. We propose a modified slow-start mechanism, called adaptive start (Astart), to improve the startup performance in such networks. When a connection initially begins or re-starts after a coarse timeout, Astart adaptively and repeatedly resets the slow-start threshold (ssthresh) based on an eligible sending rate estimation mechanism proposed in TCP Westwood. By adapting to network conditions during the startup phase, a sender is able to grow the congestion window (cwnd) fast without incurring risk of buffer overflow and multiple losses. Simulation experiments show that Astart can significantly improve the link utilization under various bandwidth, buffer size and round-trip propagation times. The method avoids both under-utilization due to premature slow-start termination, as well as multiple losses due to initially setting ssthresh too high, or increasing cwnd too fast. Experiments also show that Astart achieves good fairness and friendliness toward TCP New Reno. Lab measurements using a Free BSD Astart implementation are also reported in this paper, providing further evidence of the gains achievable via Astart.","Intelligent networks,
Bandwidth,
Internet,
Switches,
Packet switching,
Mice,
Steady-state,
Computer science,
Delay,
Tin"
Vascular segmentation of phase contrast magnetic resonance angiograms based on statistical mixture modeling and local phase coherence,"In this paper, we present an approach to segmenting the brain vasculature in phase contrast magnetic resonance angiography (PC-MRA). According to our prior work, we can describe the overall probability density function of a PC-MRA speed image as either a Maxwell-uniform (MU) or Maxwell-Gaussian-uniform (MGU) mixture model. An automatic mechanism based on Kullback-Leibler divergence is proposed for selecting between the MGU and MU models given a speed image volume. A coherence measure, namely local phase coherence (LPC), which incorporates information about the spatial relationships between neighboring flow vectors, is defined and shown to be more robust to noise than previously described coherence measures. A statistical measure from the speed images and the LPC measure from the phase images are combined in a probabilistic framework, based on the maximum a posteriori method and Markov random fields, to estimate the posterior probabilities of vessel and background for classification. It is shown that segmentation based on both measures gives a more accurate segmentation than using either speed or flow coherence information alone. The proposed method is tested on synthetic, flow phantom and clinical datasets. The results show that the method can segment normal vessels and vascular regions with relatively low flow rate and low signal-to-noise ratio, e.g., aneurysms and veins.","Magnetic resonance,
Coherence,
Phase measurement,
Magnetic field measurement,
Image segmentation,
Fluid flow measurement,
Velocity measurement,
Noise measurement,
Linear predictive coding,
Angiography"
ULC: a file block placement and replacement protocol to effectively exploit hierarchical locality in multi-level buffer caches,"In a large client/server cluster system, file blocks are cached in a multilevel storage hierarchy. Existing file block placement and replacement are either conducted on each level of the hierarchy independently, or by applying an LRU policy on more than one levels. One major limitation of these schemes is that hierarchical locality of file blocks with nonuniform strengths is ignored, resulting in many unnecessary block misses, or additional communication overhead. To address this issue, we propose a client-directed, coordinated file block placement and replacement protocol, where the nonuniform strengths of locality are dynamically identified on the client level to direct servers on placing or replacing file blocks accordingly on different levels of the buffer caches. In other words, the caching layout of the blocks in the hierarchy dynamically matches the locality of block accesses. The effectiveness of our proposed protocol comes from achieving the following three goals: (1) The multilevel cache retains the same hit rate as that of a single level cache whose size equals to the aggregate size of multilevel caches. (2) The nonuniform locality strengths of blocks are fully exploited and ranked to fit into the physical multilevel caches. (3) The communication overheads between caches are also reduced.","File servers,
Aggregates,
Buffer storage,
Access protocols,
Degradation,
Computer science,
Educational institutions,
Cache storage,
Hard disks,
Personal communication networks"
Knowledge awareness map for computer-supported ubiquitous language-learning,"This paper describes a computer supported collaborative learning (CSCL) in a ubiquitous computing environment. In the environment called CLUE, the learners provide and share individual experience and interaction corpus and discuss about them. This paper focuses on the design, implementation, and evaluation of knowledge awareness map. The map visualizes the relationship between the shared knowledge and the current and past interactions of learners. The map plays a very important role for finding peer helpers, and inducing collaboration.","Pervasive computing,
Collaborative work,
Ubiquitous computing,
Natural languages,
Visualization,
Collaboration,
Personal digital assistants,
Information science,
Intelligent systems,
Mediation"
Analysis on multiresolution mosaic images,"Image mosaicing is the act of combining two or more images and is used in many applications in computer vision, image processing, and computer graphics. It aims to combine images such that no obstructive boundaries exist around overlapped regions and to create a mosaic image that exhibits as little distortion as possible from the original images. In the proposed technique, the to-be-combined images are first projected into wavelet subspaces. The images projected into the same wavelet space are then blended. Our blending function is derived from an energy minimization model which balances the smoothness around the overlapped region and the fidelity of the blended image to the original images. Experiment results and subjective comparison with other methods are given.","Image analysis,
Image resolution,
Information science,
Application software,
Computer graphics,
Computer vision,
Image processing,
Head,
Pixel,
Energy resolution"
Improved BGP convergence via ghost flushing,"Labovitz et al. (2001) and Labovitz et al. (2000) noticed that sometimes it takes border gateway protocol (BGP) a substantial amount of time and messages to converge and stabilize following the failure of some node in the Internet. In this paper, we suggest a minor modification to BGP that eliminates the problem pointed out and substantially reduces the convergence time and communication complexity of BGP. Roughly speaking, our modification ensures that bad news (the failure of a node/edge) propagate fast, while good news (the establishment of a new path to a destination) propagate somewhat slower. This is achieved in BGP by allowing withdrawal messages to propagate with no delay as fast as the network forward them, while announcements propagate as they do in BGP with a delay at each node of one minRouteAdver (except for the first wave of announcements). As a by product of this work, a new stateless mechanism to overcome the counting to infinity problem is provided, which compares favorably with other known stateless mechanisms (in RIP and IGRP).","Convergence,
IEEE news,
Routing protocols,
Internet,
Propagation delay,
Computer science,
Complexity theory,
H infinity control,
Failure analysis,
Information analysis"
Gridding and compression of microarray images,"With the recent explosion of interest in microarray technology, massive amounts of microarray images are currently being produced. The storage and the transmission of this type of data are becoming increasingly challenging. Here we propose lossless and lossy compression algorithms for microarray images originally digitized at 16 bpp (bits per pixels) that achieve an average of 9.5-11.5 bpp (lossless) and 4.6-6.7 bpp (lossy, with a PSNR of 63 dB). The lossy compression is applied only on the background of the image, thereby preserving the regions of interest. The methods are based on a completely automatic gridding procedure of the image.","Image coding,
Pixel,
Image databases,
Digital images,
Image storage,
Shape,
Computer science,
Explosions,
Compression algorithms,
PSNR"
Autonomous smart routing for network QoS,"We present an autonomous adaptive quality of service (QoS) driven network system called a ""cognitive packet network"" (CPN), which adaptively selects paths so as to offer best effort QoS to the end users based on user defined QoS. CPN uses neural network based reinforcement learning to make routing decisions separately at each node. Measurements on an experimental test-bed are provided to show how the system responds to the choice of QoS goals. We also discuss and evaluate an extension of CPN that uses a genetic algorithm to generate and maintain paths from previously discovered information by matching their ""fitness"" with respect to the desired QoS.","Routing,
Quality of service,
Learning,
Genetic algorithms,
Jitter,
Delay,
Payloads,
Educational institutions,
Computer science,
Neural networks"
Tracer kinetic modeling of /sup 11/C-acetate applied in the liver with positron emission tomography,"It is well known that 40%-50% of hepatocellular carcinoma (HCC) do not show increased /sup 18/F-fluorodeoxyglucose (FDG) uptake. Recent research studies have demonstrated that /sup 11/C-acetate may be a complementary tracer to FDG in positron emission tomography (PET) imaging of HCC in the liver. Quantitative dynamic modeling is, therefore, conducted to evaluate the kinetic characteristics of this tracer in HCC and nontumor liver tissue. A three-compartment model consisting of four parameters with dual inputs is proposed and compared with that of five parameters. Twelve regions of dynamic datasets of the liver extracted from six patients are used to test the models. Estimation of the adequacy of these models is based on Akaike Information Criteria (AIC) and Schwarz Criteria (SC) by statistical study. The forward clearance K=K/sub 1/*k/sub 3//(k/sub 2/+k/sub 3/) is estimated and defined as a new parameter called the local hepatic metabolic rate-constant of acetate (LHMRAct) using both the weighted nonlinear least squares (NLS) and the linear Patlak methods. Preliminary results show that the LHMRAct of the HCC is significantly higher than that of the nontumor liver tissue. These model parameters provide quantitative evidence and understanding on the kinetic basis of /sup 11/C-acetate for its potential role in the imaging of HCC using PET.","Kinetic theory,
Liver,
Positron emission tomography,
Biochemistry,
Biomedical signal processing,
Electronic mail,
Humans,
Nuclear electronics,
Biomedical engineering,
Information technology"
Dependency networks for relational data,"Instance independence is a critical assumption of traditional machine learning methods contradicted by many relational datasets. For example, in scientific literature datasets, there are dependencies among the references of a paper. Recent work on graphical models for relational data has demonstrated significant performance gains for models that exploit the dependencies among instances. In this paper, we present relational dependency networks (RDNs), a new form of graphical model capable of reasoning with such dependencies in a relational setting. We describe the details of RDN models and outline their strengths, most notably the ability to learn and reason with cyclic relational dependencies. We present RDN models learned on a number of real-world datasets, and evaluate the models in a classification context, showing significant performance improvements. In addition, we use synthetic data to evaluate the quality of model learning and inference procedures.","Graphical models,
Bayesian methods,
Autocorrelation,
Context modeling,
Markov random fields,
Proteins,
Computer science,
Learning systems,
Performance gain,
Web sites"
Fuzzy discrete particle swarm optimization for solving traveling salesman problem,"Particle swarm optimization, as an evolutionary computing technique, has succeeded in many continuous problems, but research on discrete problems especially combinatorial optimization problem has been done little according to Kennedy and Eberhart (1997) and Mohan and Al-kazemi (2001). In this paper, a modified particle swarm optimization (PSO) algorithm was proposed to solve a typical combinatorial optimization problem: traveling salesman problem (TSP), which is a well-known NP-hard problem. Fuzzy matrices were used to represent the position and velocity of the particles in PSO and the operators in the original PSO formulas were redefined. Then the algorithm was tested with concrete examples in TSPLIB, experiment shows that the algorithm can achieve good results.","Particle swarm optimization,
Traveling salesman problems,
Cities and towns,
Fuzzy sets,
Concrete,
Educational institutions,
Computer science,
NP-hard problem,
Testing,
Random number generation"
Distributed caching and adaptive search in multilayer P2P networks,"To improve the scalability of Gnutella-like unstructured peer-to-peer (P2P) networks, a uniform index caching (UIC) mechanism was suggested in some earlier work. In UIC, query results are cached in all peers along the inverse query path such that the same query of other peers can be replied from their nearby-cached results. However, our experiments show that the UIC method causes a large amount of duplicated and unnecessary caching of items among neighboring peers. Aiming at improving the search efficiency, we propose a distributed caching mechanism, which distributes the cache results among neighboring peers. Furthermore, based on the distributed caching mechanism, an adaptive search approach is built which selectively forwards the query to the peers with a high probability of providing the desired cache results. All the enhancements above are defined in a protocol called distributed caching and adaptive search (DiCAS). In the DiCAS enhanced Gnutella network, all the peers are logically divided into multiple layers, with the character that all the peers in the same layer have the same group ID. The query flooding is restricted in one layer with the matched group ID. Our simulation study shows that, with the help of the index caching and search space division, the DiCAS protocol can significantly reduce the network search traffic in unstructured P2P systems without degrading query success rate.","Intelligent networks,
Nonhomogeneous media,
Telecommunication traffic,
Peer to peer computing,
Floods,
Protocols,
Internet,
Network topology,
Computer science,
Scalability"
A complexity-effective approach to ALU bandwidth enhancement for instruction-level temporal redundancy,"Previous proposals for implementing instruction-level temporal redundancy in out-of-order cores have reported a performance degradation of up to 45% in certain applications compared to an execution which does not have any temporal redundancy. An important contributor to this problem is the insufficient number of ALUs for handling the amplified load injected into the core. At the same time, increasing the number of ALUs can increase the complexity of the issue logic, which has been pointed out to be one of the most timing critical components of the processor. This paper proposes a novel extension of a prior idea on instruction reuse to ease ALU bandwidth requirements in a complexity-effective way by exploiting certain interesting properties of a dual (temporally redundant) instruction stream. We present microarchitectural extensions necessary for implementing an instruction reuse buffer (IRB) and integrating this with the issue logic of a dual instruction stream superscalar core, and conduct extensive evaluations to demonstrate how well it can alleviate the ALU bandwidth problem. We show that on the average we can gain back nearly 50% of the IPC loss that occurred due to ALU bandwidth limitations for an instruction-level temporally redundant superscalar execution, and 23% of the overall IPC loss.","Bandwidth,
Redundancy,
Hardware,
Out of order,
Proposals,
Logic,
Computer science,
Degradation,
Application software,
Timing"
Quantum walk algorithm for element distinctness,"We use quantum walks to construct a new quantum algorithm for element distinctness and its generalization. For element distinctness (the problem of finding two equal items among N given items), we get an O(N/sup 2/3/) query quantum algorithm. This improves the previous O(N/sup 3/4/) quantum algorithm of Buhrman et al. and matches the lower bound by Shi. We also give an O(N/sup k/(k+1)/) query quantum algorithm for the generalization of element distinctness in which we have to find k equal items among N items.","Quantum computing,
Sorting,
Costs,
Mathematics,
Laboratories,
Algorithm design and analysis"
Theoretical and numerical aspects of transmit SENSE,"The ideas of parallel imaging techniques, designed to shorten the acquisition time by the simultaneous use of multiple receive coils, can be adapted for parallel transmission of a spatially selective multidimensional RF pulse. In analogy to data acquisition, a multidimensional RF pulse follows a certain trajectory in k-space. Shortening this trajectory shortens the pulse duration. The use of multiple transmit coils, each with its own time-dependent waveform and spatial sensitivity, compensates for the missing parts of k-space. This results in a maintained spatial definition of the pulse profile while its duration is reduced. This paper describes the basic equations of parallel transmission with arbitrarily shaped transmit coils (""Transmit SENSE"") focusing on two-dimensional RF pulses. Results of numerical studies are presented demonstrating the theoretical feasibility of the approach.","Radio frequency,
Multidimensional systems,
Magnetic resonance imaging,
Coils,
Magnetization,
Data acquisition,
Equations,
Pulse shaping methods,
Signal generators,
Navigation"
T-UPPAAL: online model-based testing of real-time systems,"The goal of testing is to gain confidence in a physical computer based system by means of executing it. More than one third of typical project resources are spent on testing embedded and real-time systems, but still it remains ad-hoc, based on heuristics, and error-prone. Therefore systematic, theoretically well-founded and effective automated real-time testing techniques are of great practical value. Testing conceptually consists of three activities: test case generation, test case execution and verdict assignment. We present T-UPPAAL-a new tool for model based testing of embedded real-time systems that automatically generates and executes tests ""online"" from a state machine model of the implementation under test (IUT) and its assumed environment which combined specify the required and allowed observable (realtime) behavior of the IUT. T-UPPAAL implements a sound and complete randomized testing algorithm, and uses a formally defined notion of correctness (relativized timed input/output conformance) to assign verdicts. Using online testing, events are generated and simultaneously executed.","System testing,
Real time systems,
Automatic testing,
Acoustic testing,
Automata,
Embedded system,
Clocks,
Physics computing,
Computer science,
Computer errors"
Object recognition using composed receptive field histograms of higher dimensionality,"Effective methods for recognising objects or spatio-temporal events can be constructed based on receptive field responses summarised into histograms or other histogram-like image descriptors. This work presents a set of composed histogram features of higher dimensionality, which give significantly better recognition performance compared to the histogram descriptors of lower dimensionality that were used in the original papers by Swain & Bollard (1991) or Schiele & Crowley (2000). The use of histograms of higher dimensionality is made possible by a sparse representation for efficient computation and handling of higher-dimensional histograms. Results of extensive experiments are reported, showing how the performance of histogram-based recognition schemes depend upon different combinations of cues, in terms of Gaussian derivatives or differential invariants applied to either intensity information, chromatic information or both. It is shown that there exist composed higher-dimensional histogram descriptors with much better performance for recognising known objects than previously used histogram features. Experiments are also reported of classifying unknown objects into visual categories.","Object recognition,
Histograms,
Image recognition,
Statistics,
Wavelet coefficients,
Computer vision,
Laboratories,
Numerical analysis,
Computer science,
Councils"
Web mining: research and practice,"Web mining techniques seek to extract knowledge from Web data. This article provides an overview of past and current work in the three main areas of Web mining research - content, structure, and usage - as well as emerging work in semantic Web mining.","Web mining,
Databases,
Data mining,
HTML,
XML,
Semantic Web,
Iterative algorithms,
Web sites,
Information retrieval,
Artificial intelligence"
Regularized image reconstruction algorithms for positron emission tomography,"We develop algorithms for obtaining regularized estimates of emission means in positron emission tomography. The first algorithm iteratively minimizes a penalized maximum-likelihood (PML) objective function. It is based on standard de-coupled surrogate functions for the ML objective function and de-coupled surrogate functions for a certain class of penalty functions. As desired, the PML algorithm guarantees nonnegative estimates and monotonically decreases the PML objective function with increasing iterations. The second algorithm is based on an iteration dependent, de-coupled penalty function that introduces smoothing while preserving edges. For the purpose of making comparisons, the MLEM algorithm and a penalized weighted least-squares algorithm were implemented. In experiments using synthetic data and real phantom data, it was found that, for a fixed level of background noise, the contrast in the images produced by the proposed algorithms was the most accurate.","Image reconstruction,
Positron emission tomography,
Iterative algorithms,
Maximum likelihood estimation,
Convergence,
Smoothing methods,
Imaging phantoms,
Background noise,
Bayesian methods,
Maximum a posteriori estimation"
A defense-centric taxonomy based on attack manifestations,"Many classifications of attacks have been tendered, often in taxonomic form, A common basis of these taxonomies is that they have been framed from the perspective of an attacker - they organize attacks with respect to the attacker's goals, such as privilege elevation from user to root (from the well known Lincoln taxonomy). Taxonomies based on attacker goals are attack-centric; those based on defender goals are defense-centric. Defenders need a way of determining whether or not their detectors will detect a given attack. It is suggested that a defense-centric taxonomy would suit this role more effectively than an attack-centric taxonomy. This paper presents a new, defense-centric attack taxonomy, based on the way that attacks manifest as anomalies in monitored sensor data. Unique manifestations, drawn from 25 attacks, were used to organize the taxonomy, which was validated through exposure to an intrusion-detection system, confirming attack detect ability. The taxonomy's predictive utility was compared against that of a well-known extant attack-centric taxonomy. The defense-centric taxonomy is shown to be a more effective predictor of a detector's ability to detect specific attacks, hence informing a defender that a given detector is competent against an entire class of attacks.","Taxonomy,
Detectors,
Laboratories,
Monitoring,
Computer science,
Sensor systems,
Operating systems"
Tracking of vector field singularities in unstructured 3D time-dependent datasets,"We present an approach for monitoring the positions of vector field singularities and related structural changes in time-dependent datasets. The concept of singularity index is discussed and extended from the well-understood planar case to the more intricate three-dimensional setting. Assuming a tetrahedral grid with linear interpolation in space and time, vector field singularities obey rules imposed by fundamental invariants (Poincare index), which we use as a basis for an efficient tracking algorithm. We apply the presented algorithm to CFD datasets to illustrate its purpose. We examine structures that exhibit topological variations with time and describe some of the insight gained with our method. Examples are given that show a correlation in the evolution of physical quantities that play a role in vortex breakdown.","Electric breakdown,
Aircraft,
Computer simulation,
Computer science,
Vectors,
Computational modeling,
Analytical models,
Aerospace engineering,
Design engineering,
Virtual prototyping"
Music2Share - copyright-compliant music sharing in P2P systems,"Peer-to-peer (P2P) networks are generally considered to be free havens for pirated content, in particular with respect to music. We describe a solution for the problem of copyright infringement in P2P networks for music sharing. In particular, we propose a P2P protocol that integrates the functions of identification, tracking, and sharing of music with those of licensing, monitoring, and payment. This highly decentralized music-aware P2P protocol will allow access to large amounts of music of guaranteed quality; it merges in a natural way the policing functions for copyright protection and an efficient music-management infrastructure for the benefit of the user.","Multiple signal classification,
Internet,
Computer science,
Peer to peer computing,
Access protocols,
Law,
Legal factors,
Mathematics,
Intelligent networks,
Licenses"
A classifier design for detecting image manipulations,"In this paper we present a framework for digital image forensics. Based on the assumptions that some processing operations must be done on the image before it is doctored and an expected measurable distortion after processing an image, we design classifiers that discriminates between original and processed images. We propose a novel way of measuring the distortion between two images, one being the original and the other processed. The measurements are used as features in classifier design. Using these classifiers we test whether a suspicious part of a given image has been processed with a particular method or not. Experimental results show that with a high accuracy we are able to tell if some part of an image has undergone a particular or a combination of processing methods.","Watermarking,
Digital images,
Design engineering,
Information science,
Computer science,
Testing,
Law,
Legal factors,
Brightness,
Distortion measurement"
Run-time mapping of applications to a heterogeneous reconfigurable tiled system on chip architecture,"This work evaluates an algorithm that maps a number of communicating processes to a heterogeneous tiled system on chip (SoC) architecture at run-time. The mapping algorithm minimizes the total amount of energy consumption, while still providing an adequate quality of service (QoS). A realistic example is mapped using this algorithm.","Runtime,
System-on-a-chip,
Tiles,
Computer architecture,
Energy efficiency,
Energy consumption,
Network-on-a-chip,
Application software,
Mathematics,
Computer science"
Analysis of brain white matter via fiber tract modeling,"White matter fiber bundles of the human brain form a spatial pattern defined by the anatomical and functional architecture. Tractography applied to the tensor field in diffusion tensor imaging (DTI) results in sets of streamlines which can be associated with major fiber tracts. Comparison of fiber tract properties across subjects needs comparison at corresponding anatomical locations. Moreover, clinical analysis studying fiber tract disruption and integrity requires analysis along tracts and within cross-sections, which is hard to accomplish by conventional region of interest and voxel-based analysis. We propose a new framework for MR DTI analysis that includes tractography, fiber clustering, alignment via local shape parametrization and diffusion analysis across and along tracts. Feasibility is shown with the uncinate fasciculus and the cortico-spinal tracts. The extended set of features including fiber tract geometry and diffusion properties might lead to an improved understanding of diffusion properties and its association to normal/abnormal brain development.","Brain modeling,
Diffusion tensor imaging,
Tensile stress,
Clinical diagnosis,
Anisotropic magnetoresistance,
Pattern analysis,
Computer science,
Psychiatry,
Humans,
Computer architecture"
Model-based discovery of Web services,"Web services are software components that can be discovered and employed at runtime using the Internet. Conflicting requirements towards the nature of these services can be identified. From a business perspective, Web services promise to enable the formation of ad-hoc cooperations on a global scale. From a technical perspective, a high degree of standardization and rigorous specifications are required to enable the automated integration of Web services. A suitable technology for Web services has to mediate these needs for flexibility and stability. In this paper a new approach to the description of Web service semantics is introduced. It is a visual approach based on the use of software models and graph transformations and allows for the description of innovative services while providing a precise matching concept. An implementation using current standards and tools is available.",
Visual Analytics,,
A proactive approach to reconstructing overlay multicast trees,"Overlay multicast constructs a multicast delivery tree among end hosts. Unlike traditional IP multicast, the non-leaf nodes in the tree are normal end hosts, which are potentially more susceptible to failures than routers and may leave the multicast group voluntarily. In these cases, all downstream nodes are affected. Thus an important problem in overlay multicast is how to recover from node departures in order to minimize the disruption of service to those affected nodes. In this paper, we propose a proactive approach to restore overlay multicast trees. Rather than letting downstream nodes try to find a new parent after a node departure, each non-leaf node pre-calculates a parent-to-be for each of its children. When this non-leaf node is gone, all its children can find their respective new parents immediately. The salient feature of the approach is that each non-leaf node can compute a rescue plan for its children independently, and in most cases, rescue plans from multiple non-leaf nodes can work together for their children when they fail or leave at the same time. We develop a protocol for nodes to communicate with new parents so that the delivery tree can be quickly restored. Extensive simulations demonstrate that our proactive approach can recover from node departures 5 times faster than reactive methods in some cases, and 2 times faster on average.",
GKMPAN: an efficient group rekeying scheme for secure multicast in ad-hoc networks,"We present GKMPAN, an efficient and scalable group rekeying protocol for secure multicast in ad hoc networks. Our protocol exploits the property of ad hoc networks that each member of a group is both a host and a router, and distributes the group key to member nodes via a secure hop-by-hop propagation scheme. A probabilistic scheme based on predeployed symmetric keys is used for implementing secure channels between members for group key distribution. GKMPAN also includes a novel distributed scheme for efficiently updating the predeployed keys. GKMPAN has three attractive properties. First, it is significantly more efficient than group rekeying schemes that were adapted from those proposed for wired networks. Second, GKMPAN has the property of partial statelessness; that is, a node can decode the current group key even if it has missed a certain number of previous group rekeying operations. This makes it very attractive for ad hoc networks where nodes may lose packets due to transmission link errors or temporary network partitions. Third, in GKMPAN the key server does not need any information about the topology of the ad hoc network or the geographic location of the members of the group. We study the security and performance of GKMPAN through detailed analysis and simulation.","Intelligent networks,
Ad hoc networks,
Multicast protocols,
Network servers,
Cryptography,
Public key,
Information systems,
Computer science,
Electronic mail,
Decoding"
A DAML-based repository for QoS-aware semantic Web service selection,"The Web is moving toward a collection of interoperating Web services. Achieving this interoperability requires dynamic discovery of Web services on the basis of their capabilities. The capability of a service can be properly determined by using not only its functional description (or service interface), but also its quality attributes as judged by previous users of the service. We develop a service repository that extends UDDI registries. This repository combines an ontology of attributes with evaluation data. We base our repository on a new query and manipulation language based on DAML. Our language includes support for a rich set of operations, which are needed to maintain an attribute ontology, publish services, rate services, and select services based on their functional attributes as well as evaluations by others. We have implemented our approach and evaluated its practical completeness via a number of key query and manipulation templates.",
Circularscan: a scan architecture for test cost reduction,"Scan-based designs are widely used to decrease the complexity of the test generation process; nonetheless, they increase test time and volume. A new scan architecture is proposed to reduce test time and volume while retaining the original scan input count. The proposed architecture allows the use of the captured response as a template for the next pattern with only the necessary bits of the captured response being updated while observing the full captured response. The theoretical and experimental analysis promises a substantial reduction in test cost for large circuits.",
A Rank-by-Feature Framework for Unsupervised Multidimensional Data Exploration Using Low Dimensional Projections,"Exploratory analysis of multidimensional data sets is challenging because of the difficulty in comprehending more than three dimensions. Two fundamental statistical principles for the exploratory analysis are (1) to examine each dimension first and then find relationships among dimensions, and (2) to try graphical displays first and then find numerical summaries (D.S. Moore, (1999). We implement these principles in a novel conceptual framework called the rank-by-feature framework. In the framework, users can choose a ranking criterion interesting to them and sort 1D or 2D axis-parallel projections according to the criterion. We introduce the rank-by-feature prism that is a color-coded lower-triangular matrix that guides users to desired features. Statistical graphs (histogram, boxplot, and scatterplot) and information visualization techniques (overview, coordination, and dynamic query) are combined to help users effectively traverse 1D and 2D axis-parallel projections, and finally to help them interactively find interesting features","Multidimensional systems,
Data analysis,
Data visualization,
Data mining,
Computer vision,
Principal component analysis,
Computer science,
Laboratories,
Educational institutions,
Displays"
Deterrents to women taking computer science courses,"The United States faces a shortage of computer scientists. Despite the current economic downturn, the most recent estimate indicates a labor force shortage of IT professionals. The shortage of IT professionals, and especially of computer scientists, provides impetus for increasing the representation of women in computer science (CS). We examine why so few students, and particularly few women, choose to enter the beginning phase of the CS pipeline by choosing to try out CS courses. Women are seriously under-represented at this early juncture of the CS pipeline. If we are serious about making CS a more inclusive field, we need to make an impact at this early juncture. This is not to belittle the laudable efforts to reduce attrition among women CS majors. However, for maximum effect, interventions to increase the flow of women into the CS pipeline also need to occur before women declare their major.","Computer science,
Pipelines,
Engineering profession,
Springs,
Image analysis,
Educational institutions,
Programming profession"
The impact of topology on overlay routing service,"A moderate amount of recent work has been dedicated to using overlay network to support value-added network service, such as overlay multicast, OverQoS, etc. Overlay service network is a generic service framework which is designed to provide a variety of services to overlay service customers. To design an overlay service network, the first step is to choose an overlay topology connecting all the overlay service nodes. For example, RON [Anderson, DG et al., 2001] has used full mesh topology connecting all the nodes. In this paper, we did a study on the impact of topology on the overlay routing service. We found that the overlay topology has significant impact on the overlay routing in terms of routing performance and routing overhead. For example, the full mesh topology does not always give us the best performance. Moreover, the physical topology information can benefit us a lot to construct an efficient overlay topology. In addition, some alternative topologies can provide us with better performance when considering both the routing performance and overhead.","Network topology,
Peer to peer computing,
Web and internet services,
Joining processes,
Computer science,
IP networks,
Quality of service,
Costs,
Routing protocols"
Efficient face orientation discrimination,"The paper presents efficient methods to address the problem of discriminating between live facial orientations. We present the most efficient methods for this task to date, which can accurately discriminate between five facial orientations with approximately 92% accuracy using fewer than 30 pixel comparisons and greater than 99% accuracy using 150 pixel comparisons. We achieve these rates by using a boosting method to select from a large set of extremely simple features. Comparisons to other methods are given.",
Covering Pareto-optimal fronts by subswarms in multi-objective particle swarm optimization,"Covering the whole set of Pareto-optimal solutions is a desired task of multiobjective optimization methods. Because in general it is not possible to determine this set, a restricted amount of solutions are typically delivered in the output to decision makers. We propose a method using multiobjective particle swarm optimization to cover the Pareto-optimal front. The method works in two phases. In phase 1 the goal is to obtain a good approximation of the Pareto-front. In a second run subswarms are generated to cover the Pareto-front. The method is evaluated using different test functions and compared with an existing covering method using a real world example in antenna design.",
Testing the efficiency of JADE agent platform,"Agent oriented programming is often described as the next breakthrough in development and implementation of large-scale complex software system. At the same time it is rather difficult to find successful applications of agent technology, in particular precisely when large-scale systems are considered. The aim of this paper is to investigate if one of the possible limits may be the scalability of existing agent technology. We have picked JADE agent platform as technology of choice and investigated its efficiency in a number of test cases. Results of our experiments are presented and discussed.",
Thickness correction of mammographic images by means of a global parameter model of the compressed breast,"Peripheral enhancement and tilt correction of unprocessed digital mammograms was achieved with a new reversible algorithm. This method has two major advantages for image visualization. First, the display dynamic range can be relatively small, and second, adjustment of the overall luminance to inspect details is not required in most cases. The correction is useful for preprocessing in computer-aided detection/diagnosis algorithms. The method is based on knowledge of the three-dimensional compressed breast shape to equalize thickness by adding virtual tissue, which results in intensity equalization for the mammographic image. Previously described methods implicitly estimate the contribution of thickness variations to image intensity, usually by nonparametric methods. The proposed method employs a global parametric breast shape model, which is advantageous for visualization and CAD.","Image coding,
Displays,
Mammography,
Visualization,
Shape,
Dynamic range,
Image processing,
Breast cancer,
Brightness,
Radiology"
Rostra: a framework for detecting redundant object-oriented unit tests,"Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the state of the receiver object and method arguments at the beginning of the invocation. Existing tools for automatic generation of object-oriented test suites, such as Jtest and J Crasher for Java, typically ignore this state and thus generate redundant tests that exercise the same method behavior, which increases the testing time without increasing the ability to detect faults. This work proposes Rostra, a framework for detecting redundant unit tests, and presents five fully automatic techniques within this framework. We use Rostra to assess and minimize test suites generated by test-generation tools. We also present how Rostra can be added to these tools to avoid generation of redundant tests. We have implemented the five Rostra techniques and evaluated them on 11 subjects taken from a variety of sources. The experimental results show that Jtest and JCrasher generate a high percentage of redundant tests and that Rostra can remove these redundant tests without decreasing the quality of test suites.",
An overview of content-based image retrieval techniques,"Content-based image retrieval is currently a very important area of research in the area of multimedia databases. Plenty of research work has been undertaken to design efficient image retrieval techniques from image or multimedia databases. Although a large number of indexing and retrieval techniques have been developed, there are still no universally accepted feature extraction, indexing and retrieval techniques available. In this paper, we present an up-to-date review of various content-based image retrieval systems. Since the volume of literature available in the field is enormous, only selected works are mentioned.","Image retrieval,
Content based retrieval,
Information retrieval,
Multimedia databases,
Indexing,
Image segmentation,
Shape,
Image databases,
Australia,
Computer science"
TrafficView: a driver assistant device for traffic monitoring based on car-to-car communication,TrafficView is a device that can be embedded in the next generation of vehicles to provide drivers with a real-time view of the road traffic far beyond what they can physically see. Vehicles equipped with TrafficView devices disseminate traffic information using short-range wireless communication. The main benefits of disseminating traffic information in a vehicle-to-vehicle fashion are scalability and ease of deployment. The paper describes the TrafficView prototype and presents preliminary experimental results for this prototype.,"Monitoring,
Telecommunication traffic,
Prototypes,
Global Positioning System,
Intelligent transportation systems,
Road vehicles,
Computer science,
Vehicle driving,
Wireless communication,
Costs"
"Querying about the past, the present, and the future in spatio-temporal databases","Moving objects (e.g., vehicles in road networks) continuously generate large amounts of spatio-temporal information in the form of data streams. Efficient management of such streams is a challenging goal due to the highly dynamic nature of the data and the need for fast, online computations. We present a novel approach for approximate query processing about the present, past, or the future in spatio-temporal databases. In particular, we first propose an incrementally updateable, multidimensional histogram for present-time queries. Second, we develop a general architecture for maintaining and querying historical data. Third, we implement a stochastic approach for predicting the results of queries that refer to the future. Finally, we experimentally prove the effectiveness and efficiency of our techniques using a realistic simulation.",
Magnetic levitation hardware-in-the-loop and MATLAB-based experiments for reinforcement of neural network control concepts,"This paper discusses the use of a real-time digital control environment with a hardware-in-the-loop (HIL) magnetic levitation (Maglev) device for modeling and controls education, with emphasis on neural network (NN) feedforward control. Many educational advantages are realized for the students if a single environment is used for simulation, hardware implementation, and verification as compared with multienvironment settings. This real-time environment requires two personal computers (host and target) employed to control an HIL system. It requires software tools by MathWorks, Inc., a C++ compiler, an off-the-shelf data acquisition card, and the HIL (a nonlinear, open-loop, unstable, and time-varying, custom-built Maglev device) to be controlled. This environment provides for experimentation, such as data collection for system identification using NNs and their implementation as static nonlinear feedforward controllers. In addition, this environment was used to implement and demonstrate NNs with real-time dynamic weight tuning and controller performance comparison under various inputs or changes in the HIL device dynamics, as shown in the presented examples. The educational features of this environment were verified in a classroom setting in a graduate-level NN class. The presented environment is applicable to senior or graduate level (introductory intelligent and digital control) or a general introductory course on NNs with applications.","Magnetic levitation,
Control engineering education,
Digital control,
Feedforward neural networks,
Neurocontrollers,
Intelligent control,
Nonlinear systems,
Software tools,
Courseware"
Model checking rational agents,"Agent-oriented programming techniques seem appropriate for developing systems that operate in complex, dynamic, and unpredictable environments. We aim to address this requirement by developing model-checking techniques for the (automatic or semiautomatic) verification of rational-agent systems written in a logic-based agent-oriented programming language. Typically, developers apply model-checking techniques to abstract models of a system rather than the system implementation. Although this is important for detecting design errors at an early stage, developers might still introduce errors during coding. In contrast, developers can directly apply our model-checking techniques to systems implemented in an agent-oriented programming language, automatically verifying agent systems without the usual gap between design and implementation. We developed our techniques for AgentSpeak, a rational-agent programming language based on the AgentSpeak (L) abstract agent-oriented programming language. AgentSpeak shares many features of the agent-oriented programming paradigm. Similarly, we've developed techniques for automatically translating AgentSpeak programs into the model specification language of existing model-checking systems. In this way, we reduce the problem of verifying that an AgentSpeak system has certain BDI logic properties to a conventional LTL model-checking problem.","Specification languages,
Multiagent systems,
Automatic control,
Control systems,
Computer languages,
Logic programming,
Java,
Dynamic programming,
Automatic logic units,
Computer science"
Aiding comprehension of cloning through categorization,"Management of duplicated code in software systems is important in ensuring its graceful evolution. Commonly clone detection tools return large numbers of detected clones with little or no information about them, making clone management impractical and unscalable. We have used taxonomy of clones to augment current clone detection tools in order to increase the user comprehension of duplication of code within software systems and filter false positives from the clone set. We support our arguments by means of 2 case studies, where we found that as much as 53% of clones can be grouped to form function clones or partial function clones and we were able to filter out as many as 65% of clones as false positives from the reported clone pairs.",
Ontologies for modeling and simulation: issues and approaches,"Ontologies represent the next important phase of the World Wide Web, creating a semantic Web which links together disparate pieces of information and knowledge. Creating ontologies within computer simulation can be seen as a logical next phase of the Web-based modeling and simulation thrust, where the emphasis is on knowledge and its representation rather than on run-time network characteristics. We introduce the concept of an ontology and then survey two groups performing research in this area at the Universities of Florida and Georgia, respectively.",
Deterministic sampling methods for spheres and SO(3),"This paper addresses the problem of generating uniform deterministic samples over the spheres and the three-dimensional rotation group, SO(3). The target applications include motion planning, optimization, and verification problems in robotics and in related areas, such as graphics, control theory and computational biology. We introduce an infinite sequence of samples that is shown to achieve: 1) low-dispersion, which aids in the development of resolution complete algorithms, 2) lattice structure, which allows easy neighbor identification that is comparable to what is obtained for a grid in /spl Ropf//sup d/, and 3) incremental quality, which is similar to that obtained by random sampling. The sequence is demonstrated in a sampling-based motion planning algorithm.",
A peer-to-peer framework for Web service discovery with ranking,"Current Web service discovery methods are based on centralized approaches where Web services are identified based on service functionality. Examples of service functionality include car rental, hotel booking and book selling. Since higher level Web services are increasingly composed in terms of lower level Web services, it is important that service discovery not only be based on service functionality but also be based on process behavior, i.e., how a service functionality is served. Furthermore, centralized approaches to service discovery suffer from problems such as high operational and maintenance cost, single point of failure, and scalability. Another issue that has not been considered in current Web service discovery paradigms is the issue of trust and quality of service of the service provider. We, therefore, propose a structured peer-to-peer framework for Web service discovery in which Web services are located based on both service functionality and process behavior. In addition, we integrate a scalable reputation model in this distributed peer-to-peer framework to rank Web services based on both trust and service quality.","Peer to peer computing,
Web services,
Costs,
Scalability,
Proposals,
Computer science,
Books,
Quality of service,
Web and internet services,
Simple object access protocol"
A synthesis method for MVL reversible logic [multiple value logic],"An r-valued m-variable reversible logic function maps each of the r/sup m/ input patterns to a unique output pattern. The synthesis problem is to realize a reversible function by a cascade of primitive reversible gates. In this paper, we present a simple heuristic algorithm that exploits the bidirectional synthesis possibility inherent in the reversibility of the specification. The primitive reversible gates considered here are one possible extension of the well-known binary Toffoli gates. We present exhaustive results for the 9! 2-variable 3-valued reversible functions, comparing the results of our algorithm to optimal results found by breadth-first search. The approach can be applied to general m-variable, r-valued reversible specifications. Further, we show how the presented technique can be applied to irreversible specifications. The synthesis of a 3-input, 3-valued adder is given as a specific case.",
A fragile watermarking scheme for image authentication with localization and recovery,"A fragile, block-wise, and content-based watermarking for image authentication and recovery is presented. In this scheme, the watermark of each block is an encrypted form of its signature, which includes the block location, a content-feature of another block, and a CRC checksum. While the CRC checksum is to authenticating the signature, the mixture of the location indices of one block with the feature of a randomly selected block complicates the VQ attack. The encryption further strengthens the security. That all security parameters are user dependent and can be computed at both ends individually based on Diffie-Hellman key exchange method makes the scheme not only robust against collage attack but also truly oblivious. The experiments demonstrate that our scheme can detect and localize any tampering of size 8/spl times/8 pixels and above and can recover a 40% damaged image to an intelligible one with 24 dB. As for incidentally manipulated images, our scheme can invalidate all the blocks but does not further degrade the images. Comparing with the scheme of Celik et al. (2002), ours has better tamper localization accuracy while trading off 2-3 dB of the PSNR of watermarked images for recovery.","Watermarking,
Authentication,
Cyclic redundancy check,
Cryptography,
Computer science,
Security,
Robustness,
Pixel,
Information management,
Degradation"
A crosstalk aware interconnect with variable cycle transmission,"Crosstalk between wires, caused by increased capacitive coupling, is considered one of the major factors that affect the performance of interconnects such as buses. The data-dependent nature of crosstalk-induced delays necessitates bus cycle time to be designed for the worst case crosstalk. However, this pessimism incurs a significant performance penalty. Consequently, we propose a crosstalk aware interconnect that uses a faster clock and dynamically controls the number of cycles required for transmission based on the estimated delay of the data pattern to be transmitted. In order to accomplish this, we designed a crosstalk analyzer circuit that is incorporated into the sender side of the bus and support a variable cycle transmission mechanism. We evaluate the effectiveness of the proposed scheme focusing on the on-chip buses of a microprocessor and by using the SPEC2000 benchmarks. The experimental results show that the proposed approach improves performance by 31.5% as compared to the original pessimistic approach. Furthermore, we employ a coding optimization to enhance the effectiveness of the proposed approach. We also show that the proposed scheme is an area-efficient approach to improving performance as compared to other crosstalk reduction schemes.",
Parallel network RAM: effectively utilizing global cluster memory for large data-intensive parallel programs,"Large scientific parallel applications demand large amounts of memory space. Current parallel computing platforms schedule jobs without fully knowing their memory requirements. This leads to uneven memory allocation in which some nodes are overloaded. This, in turn, leads to disk paging, which is extremely expensive in the context of scientific parallel computing. To solve this problem, we propose a new peer-to-peer solution called parallel network RAM. This approach avoids the use of disk and better utilizes available RAM resources. This approach will allow larger problems to be solved while reducing the computational, communication and synchronization overhead typically involved in parallel applications.","Read-write memory,
Random access memory,
Peer to peer computing,
Parallel processing,
Concurrent computing,
Computer science,
Data engineering,
Application software,
Processor scheduling,
Context"
Detecting distributed denial-of-service attacks by analyzing TCP SYN packets statistically,"Distributed denial-of-service attacks on public servers have recently become more serious. More are SYN flood attacks, since the malicious attackers can easily exploit the TCP specification to generate traffic making public servers unavailable. To assure that network services will not be interrupted, we need faster and more accurate defense mechanisms against malicious traffic, especially SYN floods. One of the problems in detecting SYN flood traffic is that server nodes or firewalls cannot distinguish the SYN packets of normal TCP connections from those of SYN flood attack. Moreover, since the rate of normal network traffic may vary, we cannot use an explicit threshold of SYN arrival rates to detect SYN flood traffic. In this paper we introduce a mechanism for detecting SYN flood traffic more accurately by taking into consideration the the time variation of arrival traffic. We first investigate the statistics of the arrival rates of both normal TCP SYN packets and SYN flood attack packets. We then describe our new detection mechanism based on the statistics of SYN arrival rates. Our analytical results show that the arrival rate of normal TCP SYN packets can be modeled by a normal distribution and that our proposed mechanism can detect SYN flood traffic quickly and accurately regardless of time variance of the traffic.","Computer crime,
Floods,
Network servers,
Telecommunication traffic,
Traffic control,
Information analysis,
Information science,
Statistical distributions,
Gaussian distribution,
Web and internet services"
Performance analysis of content matching intrusion detection systems,"Although network intrusion detection systems (nIDS) are widely used, there is limited understanding of how these systems perform in different settings and how they should be evaluated. This paper examines how nIDS performance is affected by traffic characteristics, rulesets, string matching algorithms and processor architecture. The analysis presented in this paper shows that nIDS performance is very sensitive to these factors. Evaluating a nIDS therefore requires careful consideration of a fairly extensive set of scenarios. Our results also highlight potential dangers with the use of workloads based on combining widely-available packet header traces with synthetic packet content as well as with the use of synthetic rulesets.",
Hardware and Binary Modification Support for Code Pointer Protection From Buffer Overflow,"Buffer overflow vulnerabilities are currently the most prevalent security vulnerability; they are responsible for over half of the CERT advisories issued in the last three years. Since many attacks exploit buffer overflow vulnerabilities, techniques that prevent buffer overflow attacks would greatly increase the difficulty of writing a new worm. This paper examines both software and hardware solutions for protecting code pointers from buffer overflow attacks. We first evaluate the performance overhead of the existing Point-Guard software solution for protecting code pointers, and show that it can be applied using binary modification to protect return pointers on the stack. These software techniques guard against write attacks, but not read attacks, where an attacker is attempting to gain information about the pointer protection mechanism in order to later mount a write buffer attack. To address this, we examine encryption hardware to provide security for code pointers from read and write attacks. In addition, we show that pure software solutions can degrade program performance, and the light-weight encryption hardware techniques we examine can be used to provide protection with little performance overhead.","Hardware,
Protection,
Buffer overflow,
Software performance,
Cryptography,
Costs,
Computer science,
Computer security,
Writing,
Computer worms"
Diagnosability of t-connected networks and product networks under the comparison diagnosis model,"Diagnosability is an important factor in measuring the reliability of an interconnection network, while the (node) connectivity is used to measure the fault tolerance of an interconnection network. We observe that there is a close relationship between the connectivity and the diagnosability. According to our results, a t-regular and t-connected network with at least 2t + 3 nodes is t-diagnosable. Furthermore, the diagnosability of the product networks is also investigated in this work. The product networks, including hypercube, mesh, and tori, comprise very important classes of interconnection networks. Herein, different combinations of t-diagnosable and t-connected are employed to study the diagnosability of the product networks.",
On disjoint path pairs with wavelength continuity constraint in WDM networks,"In a WDM optical network, each fiber link can carry a certain set of wavelengths /spl Lambda/= {/spl lambda//sub 1/,/spl lambda//sub 2/,...,/spl lambda//sub W/}. One scheme for tolerating a single link failure (or node failure) in the network is the path protection scheme, which establishes an active path and a link-disjoint (or node-disjoint) backup path, so that in the event of a link failure (node failure) on the active path, data can be quickly re-routed through the backup path. We consider a dynamic scenario, where requests to establish active-backup paths between a specified source-destination node pair arrive sequentially. If a link-disjoint (node-disjoint) active-backup path pair is found at the time of the request, the paths are established; otherwise, the request is blocked. In this scenario, at the time a request arrives, not every fiber link will have all W wavelengths available for new call establishment, as some of the wavelengths may already have been allocated to earlier requests and communication through these paths may still be in progress. We assume that the network nodes do not have any wavelength converters. This paper studies the existence of a pair of link-disjoint (node-disjoint) active-backup paths satisfying the wavelength continuity constraint between a specified source-destination node pair. First we prove that both the link-disjoint and node-disjoint versions of the problem are NP-complete. Then we focus on the link-disjoint version and present an approximation algorithm and an exact algorithm for the problem. Finally, through our experimental evaluations, we demonstrate that our approximation algorithm produces near-optimal solutions in almost all of the instances of the problem in a fraction of the time required by the exact algorithm.","Intelligent networks,
WDM networks,
Protection,
Optical fiber networks,
Computer science,
Wavelength division multiplexing,
Optical fiber communication,
Optical wavelength conversion,
Approximation algorithms,
Bandwidth"
Emotional image and musical information retrieval with interactive genetic algorithm,"Several techniques in artificial intelligence have shown a great potential to develop useful human-computer interfaces, but it is still quite far from realizing a system of matching the human performance, especially in terms of emotion, intuition and inspiration. To overcome this shortcoming, we present a promising technique called interactive genetic algorithm (IGA), which performs optimization with human evaluation, and with which the user can obtain what he has in mind through repeated interaction. To project the usefulness of the IGA to develop emotional human-computer interfaces, we have applied it to the problems of image and music information retrieval. Several experiments show that our approach allows us to design and search digital media not only explicitly expressed, but also abstract images such as ""cheerful impression,"" and ""gloomy impression."" It is expected that the same approach can be applied to many other problems in musical information retrieval and manipulation based on intuition and inspiration.","Image retrieval,
Music information retrieval,
Genetic algorithms,
Humans,
Biological cells,
Performance evaluation,
Testing,
Artificial intelligence,
Data mining,
Computer science"
Building PDE codes to be verifiable and validatable,"For codes that solve nonlinear partial differential equations (PDEs), powerful methodologies already exist for verification of codes, verification of calculations, and validation (V2V). If computational scientists and engineers are serious about these issues, they will take the responsibility and the relatively little extra effort to design (or modify) their codes so that independent users can confirm V2V.",
An automated learning system for Java programming,"The increase in the numbers enrolling in university computer courses makes huge demands on resources, and maintaining standards of teaching and tutorial support is almost impossible without a massive increase in staff. To overcome this situation, the authors have developed RoboProf, an automated learning environment which, as well as generating and assessing programming exercises, provides ongoing assistance and feedback to students without extra demands on lecturer and tutors' time. This system also contains a technique for detecting plagiarism, an increasing problem in computing courses worldwide. For this research, RoboProf was used to teach Java programming to a class containing nearly 300 students in the first year of a computing degree. Use of the system by students was monitored and recorded on log files in order to investigate the extent to which usage patterns influence achieved programming skill. An analysis shows that students who complete the set of RoboProf exercises perform significantly better than those who do not. The timeliness in which these exercises are completed relative to other students is significant: early solvers get higher marks, and students solving the problems with fewer attempts get higher marks. Not surprisingly, plagiarists achieve a lower score than those who do their own work. Other factors that were found to influence programming performance included entry standards and gender. Entry qualifications impacted positively on performance, and males performed significantly better than females. There was a significant positive correlation between the score achieved in the RoboProf course and the performance in a subsequent computing course administered in the traditional manner.",
An empirical study on using stereotypes to improve understanding of UML models,"Stereotypes were introduced into the Unified Modeling Language (UML) to provide means of customizing this visual, general purpose, object-oriented modeling language, for its usage in specific application domains. The primary purpose of stereotypes is to brand an existing model element with a specific semantics. In addition, stereotypes can also be used as notational shorthand. The paper elaborates on this role of stereotypes from the perspective of UML, clarifies the role and describes a controlled experiment aimed at evaluation of the role - in the context of model understanding. The results of the experiment support the claim that stereotypes with graphical icons for their representation play a significant role in comprehension of models and show the size of the improvement.","Unified modeling language,
Object oriented modeling,
Programming,
Sections,
Software engineering,
Computer science,
Application software,
Context modeling,
Software standards,
Standards development"
Simulation validation using direct execution of wireless ad-hoc routing protocols,"Computer simulation is the most common approach to studying wireless ad-hoc routing algorithms. The results, however, are only as good as the models the simulation uses. One should not underestimate the importance of validation, as inaccurate models can lead to wrong conclusions. In this paper, we use direct-execution simulation to validate radio models used by ad-hoc routing protocols, against real-world experiments. This paper documents a common testbed that supports direct execution of a set of ad-hoc routing protocol implementations in a wireless network simulator. The testbed reads traces generated from real experiments, and uses them to drive direct-execution implementations of the routing protocols. Doing so we reproduce the same network conditions as in real experiments. By comparing routing behavior measured in real experiments with behavior computed by the simulation, we are able to validate the models of radio behavior upon which protocol behavior depends. We conclude that it is possible to have fairly accurate results using a simple radio model, but the routing behavior is quite sensitive to one of this model's parameters. The implication is that one should: i) use a more complex radio model that explicitly models point-to-point path loss; or ii) use measurements from an environment typical of the one of interest; or iii) study behavior over a range of environments to identify sensitivities.","Routing protocols,
Computational modeling,
Wireless networks,
Wireless application protocol,
Contracts,
Computer science,
Testing,
Government,
Discrete event simulation,
Educational institutions"
Teaching science with mobile computer supported collaborative learning (MCSCL),"Effectively incorporating technology into the classroom is a great challenge faced by schools today. In this article, we propose a mobile computer supported collaborative learning (MCSCL) system to support high school teachers with wirelessly networked handheld computers. This system promotes student collaboration and constructivism, without losing face-to-face contact. The MCSCL system was tested during a five week experience in a high school physics class. We observed both its qualitative and quantitative impact. Students and teachers responded very favorably to the system, and the experience also had a strong social impact outside the classroom. The MCSCL system provided a highly motivating learning environment that changed classroom dynamics and promoted collaboration between students. We obtained statistically significant results showing that the environment created by combining the teacher's instruction with the MSCSL system enabled the students to construct new knowledge based upon the previous knowledge provided by the teacher.","Education,
Mobile computing,
Collaborative work,
Educational institutions,
Collaboration,
Handheld computers,
Educational programs,
Laboratories,
Computer networks,
System testing"
On the impact of GSM encryption and man-in-the-middle attacks on the security of interoperating GSM/UMTS networks,"GSM suffers from various security weaknesses: Just recently, Barkan, Biham and Keller presented a ciphertext-only attack on the GSM encryption algorithm A5/2 which recovers the encryption key from a few dozen milliseconds of encrypted traffic within less than a second. Furthermore, it is well-known that it is possible to mount a man-in-the-middle attack in GSM during authentication which allows an attacker to make a victim mobile station authenticate itself to a fake base station which in turn forwards the authentication traffic to the real network, thus impersonating the victim mobile station to a real network and vice versa. We discuss the impact of GSM encryption attacks, that recover the encryption key, and the man-in-the-middle attack on the security of networks, which employ UMTS and GSM base stations simultaneously. We suggest to protect UMTS connections from GSM attacks by integrating an additional authentication and key agreement on intersystem handovers between GSM and UMTS.",
Towards secure design choices for implementing graphical passwords,"We study the impact of selected parameters on the size of the password space for ""Draw-A-Secret"" (DAS) graphical passwords. We examine the role of and relationships between the number of composite strokes, grid dimensions, and password length in the DAS password space. We show that a very significant proportion of the DAS password space depends on the assumption that users will choose long passwords with many composite strokes. If users choose passwords having 4 or fewer strokes, with passwords of length 12 or less on a 5 /spl times/ 5 grid, instead of up to the maximum 12 possible strokes, the size of the DAS password space is reduced from 58 to 40 bits. Additionally, we found a similar reduction when users choose no strokes of length 1. To strengthen security, we propose a technique and describe a representative system that may gain up to 16 more bits of security with an expected negligible increase in input time. Our results can be directly applied to determine secure design choices, graphical password parameter guidelines, and in deciding which parameters deserve focus in graphical password user studies.",
Design and implementation of a large-scale context fusion network,"We motivate a context fusion network (CFN), an infrastructure model that allows context-aware applications to select distributed data sources and compose them with customized data-fusion operators into a directed acyclic information fusion graph. Such a graph represents how an application computes high-level understandings of its execution context from low-level sensory data. Multiple graphs by different applications interconnect with each other to form a global graph. A key advantage of a CFN is reusability, both at code-level and instance-level, facilitated by operator composition. We designed and implemented a distributed CFN system, Solar, which maps the logical operator graph representation onto a set of overlay hosts. In particular, Solar meets the challenges inherent to heterogeneous and volatile ubicomp environments. By abstracting most complexities into the infrastructure, Solar facilitates both the development and deployment of context-aware applications. We present the operator composition model, basic services of the Solar overlay network, and programming support for the developers. We also discuss some applications built with Solar and the lessons we learned from our experience.","Large-scale systems,
Pervasive computing,
Context-aware services,
Computer applications,
Computer science,
Educational institutions,
Context modeling,
Application software,
Humans,
Environmental management"
AccMon: Automatically Detecting Memory-Related Bugs via Program Counter-Based Invariants,"This paper makes two contributions to architectural support for software debugging. First, it proposes a novel statistics-based, on-the-fly bug detection method called PC-based invariant detection. The idea is based on the observation that, in most programs, a given memory location is typically accessed by only a few instructions. Therefore, by capturing the invariant of the set of PCs that normally access a given variable, we can detect accesses by outlier instructions, which are often caused by memory corruption, buffer overflow, stack smashing or other memory-related bugs. Since this method is statistics-based, it can detect bugs that do not violate any programming rules and that, therefore, are likely to be missed by many existing tools. The second contribution is a novel architectural extension called the Check Look-aside Buffer (CLB). The CLB uses a Bloom filter to reduce monitoring overheads in the recently-proposed iWatcher architectural framework for software debugging. The CLB significantly reduces the overhead of PC-based invariant debugging. We demonstrate a PC-based invariant detection tool called AccMon that leverages architectural, run-time system and compiler support. Our experimental results with seven buggy applications and a total of ten bugs, show that AccMon can detect all ten bugs with few false alarms (0 for five applications and 2-8 for two applications) and with low overhead (0.24-2.88 times). Several existing tools evaluated, including Purify, CCured and value-based invariant detection tools, fail to detect some of the bugs. In addition, Purify's overhead is one order of magnitude higher than AccMon's. Finally, we show that the CLB is very effective at reducing overhead.","Computer bugs,
Software debugging,
Monitoring,
Robustness,
Computer science,
Personal communication networks,
Buffer overflow,
Filters,
Application software,
Reliability"
GREW - a scalable frequent subgraph discovery algorithm,"Existing algorithms that mine graph datasets to discover patterns corresponding to frequently occurring subgraphs can operate efficiently on graphs that are sparse, contain a large number of relatively small connected components, have vertices with low and bounded degrees, and contain well-labeled vertices and edges. However, for graphs that do not share these characteristics, these algorithms become highly unscalable. In this paper we present a heuristic algorithm called GREW to overcome the limitations of existing complete or heuristic frequent subgraph discovery algorithms. GREW is designed to operate on a large graph and to find patterns corresponding to connected subgraphs that have a large number of vertex-disjoint embeddings. Our experimental evaluation shows that GREW is efficient, can scale to very large graphs, and find non-trivial patterns.","Heuristic algorithms,
Algorithm design and analysis,
Performance analysis,
Computer science,
Data engineering,
High performance computing,
Laboratories,
Government,
Military computing,
Runtime"
Reputation-based semantic service discovery,"Semantic grids need to support dynamic service discovery - to enable users to look for services based on their properties. Such properties generally include the interface provided by a service (such as message types supported) - but may also include other nonfunctional properties - such as service performance and cost. This requires the provision and recording of metadata about a service that is not supported by current registry services such as UDDI. A framework to facilitate reputation-based service selection in semantic grids is presented. The proposed framework has two key features that distinguish it from other work in this area. First, an adaptive reputation-aware service discovery algorithm is provided. Second, a service-oriented distributed reputation assessment algorithm is presented. The main components of the framework are described.",
Proving ownership over categorical data,"This paper introduces a novel method of rights protection for categorical data through watermarking. We discover new watermark embedding channels for relational data with categorical types. We design novel watermark encoding algorithms and analyze important theoretical bounds including mark vulnerability. While fully preserving data quality requirements, our solution survives important attacks, such as subset selection and random alterations. Mark detection is fully ""blind"" in that it doesn't require the original data, an important characteristic especially in the case of massive data. We propose various improvements and alternative encoding methods. We perform validation experiments by watermarking the outsourced Wal-Mart sales data available at our institute. We prove (experimentally and by analysis) our solution to be extremely resilient to both alteration and data loss attacks, for example tolerating up to 80% data loss with a watermark alteration of only 25%.",
Dex: a semantic-graph differencing tool for studying changes in large code bases,"This paper describes an automated tool called Dex (difference extractor) for analyzing syntactic and semantic changes in large C-language code bases. It is applied to patches obtained from a source code repository, each of which comprises the code changes made to accomplish a particular task. Dex produces summary statistics characterizing these changes for all of the patches that are analyzed. Dex applies a graph differencing algorithm to abstract semantic graphs (ASGs) representing each version. The differences are then analyzed to identify higher-level program changes. We describe the design of Dex, its potential applications, and the results of applying it to analyze bug fixes from the Apache and GCC projects. The results include detailed information about the nature and frequency of missing condition defects in these projects.","Statistical analysis,
Application software,
Frequency,
History,
Software engineering,
Control systems,
Computer science,
Software systems,
Software testing,
Software debugging"
Automatic method completion,"Modern software development environments include tools to help programmers write code efficiently and accurately. For example many integrated development environments include variable name completion, method name completion and recently refactoring tools have been added to some environments. This work extends the idea of automatic completion to include completion of the body of a method by employing machine learning algorithms on the near duplicate code segments that frequently exist in large software projects.","Cloning,
Java,
Programming profession,
Software systems,
Detectors,
Computer science,
Educational institutions,
Machine learning algorithms,
Heart,
Software maintenance"
Region-based progressive stereo matching,"A novel region-based progressive stereo matching algorithm is presented. It combines the strengths of previous region-based and progressive approaches. The progressive framework avoids the time consuming global optimization, while the inherent problem, the sensitivity to early wrong decisions, is significantly alleviated via the region-based representation. A growing-like process matches the regions progressively using a global best-first strategy based on a cost function integrating disparity smoothness and visibility constraint. The performance on standard evaluation platform using various real images shows that the algorithm is among the state-of-the-art both in accuracy and efficiency.","Cost function,
Computer science,
Computational complexity,
Robustness,
Computer Society,
Computer vision,
Pattern recognition,
Computer errors"
Parallel tracking of all soccer players by integrating detected positions in multiple view images,"Soccer, one of the popular sports around the world, is often broadcasted on TV, and various researches have done on soccer scene images such as strategy analysis, scene recovery, automatic indexing of soccer scenes, and automatic intelligent sports casting. As robust player tracking is fundamental to those researches, there is a demand for an automatic player tracking system using soccer imaging data. We propose a method of tracking soccer players using multiple views. Tracking is done by integrating the tracking data from all cameras, using the geometrical relationship between cameras called homography. Integrating information from all cameras enables stable tracking on the scene, where the tracking by a single camera often fails in the case of occlusion.","Cameras,
Layout,
Image analysis,
TV broadcasting,
Robustness,
Target tracking,
Robot vision systems,
Foot,
Computer science,
Information analysis"
Year,,
SRDA: secure reference-based data aggregation protocol for wireless sensor networks,"Data aggregation in wireless sensor networks eliminates data redundancy, thereby improving bandwidth usage and energy utilization. The paper presents a secure data aggregation protocol, called SRDA (secure reference-based data aggregation), for wireless sensor networks. In order to reduce the number of bits transmitted, sensor nodes compare their raw sensed data value with their reference data value and then transfer only the difference data. In addition to reducing the number of transmitted bits, SRDA also establishes secure connectivity among sensor nodes without any online key distribution. The security level of the communication links is gradually increased as packets are transmitted at higher level cluster-heads, since intercepting a packet at higher levels of the clustering hierarchy provides a summary of a large number of transmissions at lower levels. Simulation results show that the proposed protocol yields significant savings in energy consumption while preserving data security.","Wireless application protocol,
Wireless sensor networks,
Data security,
Sensor phenomena and characterization,
Communication system security,
Base stations,
Computer science,
Data engineering,
Power engineering and energy,
Bandwidth"
A unified interference-collision analysis for power-aware adhoc networks,"In this paper we address the issue of controlling transmission power in power-aware ad hoc networks. Previous work that minimizes the transmission power does not consider both the energy consumed in collision resolution and the energy disbursed to overcome the interference resulting from neighboring nodes. We investigate the basic transmission power control for the 802.11 MAC protocols in which the control frames and the data frames can be transmitted at different power levels. A collision model together with an interference model of a uniformly distributed network is constructed. Based on these models, the end-to-end network throughput and the total energy consumption of the network are examined. For a network with a given node density, our results show the optimal transmission power for control messages and for data messages that will yield maximum throughput and minimum energy consumption per message.",
Landscape of clustering algorithms,"Numerous clustering algorithms, their taxonomies and evaluation studies are available in the literature. Despite the diversity of different clustering algorithms, solutions delivered by these algorithms exhibit many commonalities. An analysis of the similarity and properties of clustering objective functions is necessary from the operational/user perspective. We revisit conventional categorization of clustering algorithms and attempt to relate them according to the partitions they produce. We empirically study the similarity of clustering solutions obtained by many traditional as well as relatively recent clustering algorithms on a number of real-world data sets. Sammon's mapping and a complete-link clustering of the inter-clustering dissimilarity values are performed to detect a meaningful grouping of the objective functions. We find that only a small number of clustering algorithms are sufficient to represent a large spectrum of clustering criteria. For example, interesting groups of clustering algorithms are centered around the graph partitioning, linkage-based and Gaussian mixture model based algorithms.","Clustering algorithms,
Partitioning algorithms,
Taxonomy,
Algorithm design and analysis,
Computer science,
Data analysis,
Data structures,
Guidelines,
Cost function,
Maximum likelihood detection"
Design of an electron gun using computer optimization,"This paper considers an optimization technique in which the objective is attained via alterations to the physical geometry of the system. This optimization framework, to be considered in the context of electron guns, is known as optimal shape design. Optimal shape design has been used in a number of applications including wing design, magnetic tape design, and nozzle design, among others. In this investigation, we use the methods of shape optimization to design the cathode of an electron gun. The dynamical equations modeling the electron particle path as well as the generalized shape optimization problem will be presented. Illustrative examples of the technique on gun designs that were previously limited to spherical cathodes will be given.",
Monotonicity-based fast algorithms for MAP estimation of Markov sequences over noisy channels,"In this correspondence, we study algorithmic approach to solving the problem of maximum a posteriori (MAP) estimation of Markov sequences transmitted over noisy channels, which is also known as the MAP decoding problem. For the class of memoryless binary channels that produce independent substitution and erasure errors, the MAP sequence estimation problem can be formulated and solved as one of the longest path in a weighted directed acyclic graph. But for algorithm efficiency, we transform the graph problem to one of matrix search. If the underlying matrix is totally monotone, then the complexity of MAP sequence estimation can be greatly reduced. We give a sufficient condition for the matrix induced by MAP sequence estimation to be totally monotone, which is indeed the case if the input sequence is Gaussian Markov. Under this condition, the complexity of MAP decoding can be reduced from O(N/sup 2/M) to O(NM), where N is the size of source alphabet and M is the length of input sequence. Furthermore, for Markov sequences of fixed-length code we propose a block parsing strategy to reduce the complexity of MAP sequence estimation to O(M+N/sup 2/M/logM) or to O(M+NM/logM), depending on if the total monotonicity holds. Another significance of this correspondence lies in the applicability of the presented algorithmic approach, which has been thoroughly studied in computer science literature, to many other discrete optimization problems encountered in both source and channel coding, ranging from optimal multiresolution and multiple-description quantizer design, to context quantization for minimum conditional entropy, and to optimal packetization with uneven error protection.",
A History Mechanism for Visual Data Mining,"A major challenge of current visualization and visual data mining (VDM) frameworks is to support users in the orientation in complex visual mining scenarios. An important aspect to increase user support and user orientation is to use a history mechanism that, first of all, provides un- and redoing functionality. In this paper, we present a new approach to include such history functionality into a VDM framework. Therefore, we introduce the theoretical background, outline design and implementation aspects of a history management unit, and conclude with a discussion showing the usefulness of our history management in a VDM framework",
Optimal power-down strategies,"We consider the problem of selecting threshold times to transition a device to low-power sleep states during an idle period. The two-state case in which there is a single active and a single sleep state is a continuous version of the ski-rental problem. We consider a generalized version in which there is more than one sleep state, each with its own power consumption rate and transition costs. We give an algorithm that, given a system, produces a deterministic strategy whose competitive ratio is arbitrarily close to optimal. We also give an algorithm to produce the optimal online strategy given a system and a probability distribution that generates the length of the idle period. We also give a simple algorithm that achieves a competitive ratio of 3 + 2/spl radic/2 /spl ap/ 5.828 for any system.","Costs,
Computer science,
Sleep,
Energy consumption,
Probability distribution,
Application software,
Computer applications,
Yarn,
IP networks,
Clocks"
User mobility for opportunistic ad-hoc networking,"As mobile devices become increasingly pervasive and commonly equipped with short-range radio capabilities, we observe that it might be possible to build a network based only on pair-wise contact of users. By using user mobility as a network transport mechanism, devices can intelligently route latency-insensitive packets using power-efficient short-range radio. Such a network could provide communication capability where no network infrastructure exists, or extend the reach of established infrastructure. To collect user mobility data, we ran two user studies by giving instrumented PDA devices to groups of students to carry for several weeks. We evaluate our work by providing empirical data that suggests that it is possible to make intelligent routing decisions based on only pair-wise contact, without previous knowledge of the mobility model or location information.",
A variational approach to scene reconstruction and image segmentation from motion-blur cues,"In this paper we are interested in the joint reconstruction of geometry and photometry of scenes with multiple moving objects from a collection of motion-blurred images. We make simplifying assumptions on the photometry of the scene (we neglect complex illumination effects) and infer the motion field of the scene, its depth map, and its radiance. In particular, we choose to partition the image into regions where motion is well approximated by a simple planar translation. We model motion-blurred images as the solution of an anisotropic diffusion equation, whose initial conditions depend on the radiance and whose diffusion tensor encodes the depth map of the scene and the motion field. We propose an algorithm to infer the unknowns of the model by minimizing the discrepancy between the measured images and the ones synthesized via diffusion. Since the problem is ill-posed, we also introduce additional Tikhonov regularization terms.","Layout,
Image reconstruction,
Image segmentation,
Image resolution,
Photometry,
Cameras,
Computer science,
Computational geometry,
Lighting,
Anisotropic magnetoresistance"
Alert-driven e-service management,"Process management technology has recently been employed not only within businesses but also in provision of e-services over the Internet. Urgent requests and critical messages in these systems (referred to as alerts) should be delivered and handled timely. In particular, humans are often involved in critical conditions. Presently, most e-service systems cannot address these requirements, and alerts are often handled in an ad-hoc manner. In this paper, we propose a sophisticated alert management system for effective e-service integration under urgent constraints. We develop a model for specifying alerts, in which alerts are associated with service requests and a set of parameters are captured for their routing. The alert monitor matches the service provider to receive an alert, based on the alert specification. We then propose a routing mechanism that is initiated when the alert message is not acknowledged or serviced within the deadline, so that the alert can be forwarded to other suitable services if necessary. Monitoring is especially essential to ensure timeliness and availability of services, otherwise suitable exceptions should be raised and handled. We outline our implementation framework of an alert management system (AMS) which includes Web services for B2B interactions, together with multiple-platform support for human users. We demonstrate our approach with an example medical house-call AMS.","Routing,
Humans,
Monitoring,
Computer science,
Web services,
Web and internet services,
Business communication,
Councils,
Technology management,
Availability"
Serpentine locomotion on surfaces with uniform friction,"A common view in snake robot research is that serpentine locomotion is only possible when there is nonuniform friction. This paper demonstrates that this view is incorrect, through a simple and easily reproducible experiment. We also present a theoretical kinematical analysis, which explains the experiment.","Friction,
Robots,
Terminology,
Computer science,
Silicon carbide,
Energy loss,
Kinematics,
Shape,
Motion analysis"
TCP implementations and false time out detection in OBS networks,"This paper compares Reno, new-Reno and selective acknowledgements (SACK), the three most common TCP implementations today in (future) optical burst switched (OBS) networks. In general, SACK, which considers multiple triple duplicated ACKed (TD) losses in one round, is found to perform best in OBS networks, while new-Reno, which improves Reno in packet switched networks by fast retransmission in responding to partial ACKs, may however perform worse than Reno. All three TCP implementations react to a time out (TO) loss in the same way (i.e., using slow start). In OBS networks, where a burst may contain all packets from one round, and a burst loss occurs mainly due to contention instead of buffer overflow, such a TO event may no longer imply heavy congestion, or in other words, it may he a false TO or FTO. Such FTOs, which may he common in OBS networks especially for fast TCP flows, can significantly degrade the performance of all existing TCP implementations. Accordingly, we also propose a new TCP implementation called burst TCP (BTCP) which can detect FTOs and react properly, and as a result, improve over the existing TCP implementations significantly.","Intelligent networks,
TCPIP,
Optical packet switching,
Computer science,
Optical fiber networks,
Degradation,
Wavelength division multiplexing,
Optical burst switching,
Internet,
Spine"
Signing a digital signature without using one-way hash functions and message redundancy schemes,"In 2000, Shieh et al. proposed some multisignature schemes based on a new digital signature scheme to satisfy the special requirements of the mobile system. In these schemes, one-way hash functions and message redundancy schemes are not used. Later, Hwang and Li indicated that Shieh et al.'s digital signature scheme suffers from the forgery attacks. They also claimed that message redundancy schemes should still be used to resist some attacks. In this letter, we show another attack on Shieh et al.'s signature scheme and propose a secure digital signature scheme, where neither one-way hash functions nor message redundancy schemes are employed.",
A general compiler framework for speculative multithreaded processors,"Speculative multithreading (SpMT) promises to be an effective mechanism for parallelizing nonnumeric programs, which tend to have irregular and pointer-intensive data structures and complex flows of control. Proper thread formation is crucial for obtaining good speedup in an SpMT system. This paper presents a compiler framework for partitioning a sequential program into multiple threads for parallel execution in an SpMT system. This framework is very general and supports speculative threads, nonspeculative threads, loop-centric threads, and out-of-order thread spawning. It is therefore useful for compiling for a wide variety of SpMT architectures. For effective partitioning of programs, the compiler uses profiling, interprocedural pointer analysis, data dependence information, and control dependence information. The compiler is implemented on the SUIF-MachSUIF platform. A simulation-based evaluation of the generated threads shows that the use of nonspeculative threads and nonloop speculative threads provides a significant increase in speedup for nonnumeric programs.","Program processors,
Parallel processing,
Data structures,
Out of order,
Multithreading,
Data analysis,
Computer science,
Information analysis,
Computational modeling"
"Enhancing prototype reduction schemes with recursion: a method applicable for ""large"" data sets","Most of the prototype reduction schemes (PRS), which have been reported in the literature, process the data in its entirety to yield a subset of prototypes that are useful in nearest-neighbor-like classification. Foremost among these are the prototypes for nearest neighbor classifiers, the vector quantization technique, and the support vector machines. These methods suffer from a major disadvantage, namely, that of the excessive computational burden encountered by processing all the data. In this paper, we suggest a recursive and computationally superior mechanism referred to as adaptive recursive partitioning (ARP)/spl I.bar/PRS. Rather than process all the data using a PRS, we propose that the data be recursively subdivided into smaller subsets. This recursive subdivision can be arbitrary, and need not utilize any underlying clustering philosophy. The advantage of ARP/spl I.bar/PRS is that the PRS processes subsets of data points that effectively sample the entire space to yield smaller subsets of prototypes. These prototypes are then, in turn, gathered and processed by the PRS to yield more refined prototypes. In this manner, prototypes which are in the interior of the Voronoi spaces, and thus ineffective in the classification, are eliminated at the subsequent invocations of the PRS. We are unaware of any PRS that employs such a recursive philosophy. Although we marginally forfeit accuracy in return for computational efficiency, our experimental results demonstrate that the proposed recursive mechanism yields classification comparable to the best reported prototype condensation schemes reported to-date. Indeed, this is true for both artificial data sets and for samples involving real-life data sets. The results especially demonstrate that a fair computational advantage can be obtained by using such a recursive strategy for "" large"" data sets, such as those involved in data mining and text categorization applications.",
Off-line handwriting identification using HMM based recognizers,"An off-line, text independent system for writer identification using hidden Markov model (HMM) based recognizers is described. For each writer we build an individual recognizer and train it on text lines written by that writer. A text line of unknown origin is presented to each of these recognizers. As a result we get, from each recognizer, a transcription including the log-likelihood score for the considered input. We rank all scores, and based on the assumption that the recognizer with the highest log-likelihood is the one that has been trained using text lines of this writer, we assign the text line to the writer whose score ranks first. We tested our system using over 2,200 text lines from 50 writers and have in 94.47% of all cases correctly identified the writer. Using a simple confidence measure to define a rejection mechanism, we achieved an error rate of 0% by rejecting 15% of the results.","Hidden Markov models,
Handwriting recognition,
Text recognition,
Computer science,
System testing,
Error analysis,
Writing,
Noise shaping,
Shape,
Character recognition"
Phase unwrapping for 2-D blind deconvolution of ultrasound images,"In most approaches to the problem of two-dimensional homomorphic deconvolution of ultrasound images, the estimation of a corresponding point-spread function (PSF) is necessarily the first stage in the process of image restoration. This estimation is usually performed in the Fourier domain by either successive or simultaneous estimation of the amplitude and phase of the Fourier transform (FT) of the PSF. This paper addresses the problem of recovering the FT-phase of the PSF, which is an important reconstruction problem by itself. The purpose of this paper is twofold. First, it provides a theoretical framework, establishing that the FT-phase of the PSF can be effectively estimated by a proper smoothing of the FT-phase of the appropriate radio-frequency (RF) image. Second, it presents a novel approach to the estimation of the FT-phase of the PSF, by solving a continuous Poisson equation over a predefined smooth subspace, in contrast to the discrete Poisson equation solver used for the classical least mean squares phase unwrapping algorithms, followed by a smoothing procedure. The proposed approach is possible due to the distinct properties of the FT-phases, among which the most important property is the availability of precise values of their partial derivatives. This property overcomes the main disadvantage of the discrete schemes, which routinely use wrapped (principal ) values of the phase in order to approximate its partial derivatives. Since such an approximation is feasible subject to the restriction that the partial phase differences do not exceed /spl pi/ in absolute value, the discrete schemes perform satisfactory only for few practical situations. The proposed approach is shown to be independent of this restriction and, thus, it performs for a wider class of the phases with significantly lower errors. The main advantages of the novel method over the algorithms based on discrete schemes are demonstrated in a series of computer simulations and for in vivo measurements.","Deconvolution,
Ultrasonic imaging,
Phase estimation,
Amplitude estimation,
Smoothing methods,
Radio frequency,
Poisson equations,
Least squares approximation,
Image restoration,
Fourier transforms"
Probabilistic data association methods in visual tracking of groups,"Data association is a fundamental problem when tracking large numbers of moving targets. Most commonly employed methods of data association such as the JPDA estimator are combinatorial and therefore do not scale well to large numbers of targets. However, in many cases large numbers of targets form natural groups which can be efficiently tracked. We describe a method for defining groups based on the position and velocity of targets. This definition introduces a natural set of merging and splitting rules that are embedded into a Kalman filtering framework for tracking multiple groups. In cases where groups of different velocities cross, a general methodology for matching measurements to groups is introduced. This algorithm is based on a modified version of the PDA estimator. It is well suited to handle a high number of measurements and extends naturally to additional grouping constraints such as color or shape.","Target tracking,
Merging,
Velocity measurement,
Personal digital assistants,
Shape measurement,
Computer vision,
Data engineering,
Computer science,
Kalman filters,
Filtering"
A unified theory of timing budget management,"This work presents a theoretical framework that optimally solves many open problems in time budgeting. Our approach unifies a large class of existing time-management paradigms. Examples include time budgeting for maximizing total weighted delay relaxation, minimizing the maximum relaxation and min-skew time budget distribution. We show that many of the time management problems can be transformed into a min-cost flow instance that can be optimally and efficiently solved through well-known combinatorial techniques. Experiments include mapping of several designs, which are implemented using parameterized CoreGen IP cores, on Xilinx FPGA devices. Different time budgeting policies have been applied during the mapping stage. Our time management techniques always improved the area requirement of the implemented testbenches compared to a widely-used path-based method. We also compared the maximum budgeting and fairness in delay budget assignments. Our experimental results show that an average improvement of 19% in area can be achieved when fairness and maximum budgeting policies are combined, compared to pure maximum budgeting.",
Simultaneous escape routing and layer assignment for dense PCBs,"As die sizes are shrinking, and circuit complexities are increasing, the PCB routing problem becomes more and more challenging. Traditional routing algorithms can not handle these challenges effectively, and many high-end designs in the industry require manual routing efforts. In this paper, we propose a problem decomposition that distinguishes routing within dense components from routing in the intermediate area. In particular, we propose an effective methodology to find the escape routing solution for multiple components simultaneously such that the number of crossings in the intermediate area is minimized. For this, we model the problem as a longest path with forbidden pairs (LPFP) problem, and propose two algorithms for it. The first is an exact polynomial-time algorithm that is guaranteed to find the maximal planar routing solution on one layer. The second is a randomized algorithm that has good scalability characteristics for large circuits. Then we use these algorithms to assign the maximal subset of planar nets to each layer, and then distribute the remaining nets at the end. We demonstrate the effectiveness of these algorithms through experiments on industrial circuits.","Routing,
Circuits,
Algorithm design and analysis,
Pins,
Computer science,
Complexity theory,
Polynomials,
Scalability,
Packaging,
Nonhomogeneous media"
Merging globally rigid formations of mobile autonomous agents,,
Video understanding for metro surveillance,"We propose in this paper an approach for recognising either isolated individual, group of people or crowd behaviours in the context of visual surveillance of metro scenes using multiple cameras. In this context, a behaviour recognition module relies on a vision module composed of three tasks: (a) motion detection and frame to frame tracking, (b) multiple cameras combination and (c) long term tracking of individuals, groups of people and crowd evolving in the scene. For each tracked actor, the behaviour recognition module performs three levels of reasoning: states, events and scenarios. We have also defined a general framework to easily combine and tune various recognition methods (e.g. automaton, Bayesian network or AND/OR tree) dedicated to the analysis of specific situations (e.g. mono/multi actors activities, numerical/symbolic actions or temporal scenarios). Validation results on different methods used to recognise specific behaviours are described.","Surveillance,
Layout,
Cameras,
Object detection,
Motion detection,
Tracking,
Streaming media,
Detectors,
HDTV,
Automata"
Performance analysis of evolutionary optimization with cumulative step length adaptation,"Iterative algorithms for continuous numerical optimization typically need to adapt their step lengths in the course of the search. While some strategies employ fixed schedules, others attempt to adapt dynamically in response to the outcome of trial steps or the history of the search process. Evolutionary algorithms are of the latter kind. A control strategy that is commonly used in evolution strategies is the cumulative step length adaptation approach. This paper presents a theoretical analysis of that adaptation strategy. The analysis includes the practically relevant case of noise interfering in the optimization process. Recommendations are made with respect to choosing appropriate population sizes.","Performance analysis,
Computer science,
Job shop scheduling,
Evolutionary computation,
Filtering,
Dynamic scheduling,
History,
Stochastic resonance,
Simulated annealing,
Stochastic processes"
The inverse kinematics solutions of industrial robot manipulators,"The inverse kinematics problem of robot manipulators is solved analytically in order to have complete and simple solutions to the problem. This approach is also called as a closed form solution of robot inverse kinematics problem. In this paper, the inverse kinematics of sixteen industrial robot manipulators classified by Huang and Milenkovic were solved in closed form. Each robot manipulator has an Euler wrist whose three axes intersect at a common point. Basically, five trigonometric equations were used to solve the inverse kinematics problems. Robot manipulators can be mainly divided into four different group based on the joint structure. In this work, the inverse kinematics solutions of SN (cylindrical robot with dome), CS (cylindrical robot), NR (articulated robot) and CC (selectively compliant assembly robot arm-SCARA, Type 2) robot manipulator belonging to each group mentioned above are given as an example. The number of the inverse kinematics solutions for the other robot manipulator was also summarized in a table.","Kinematics,
Service robots,
Manipulators,
Robotic assembly,
Orbital robotics,
Closed-form solution,
Wrist,
Nonlinear equations,
Computer science education,
Mechatronics"
Ensembles of partitions via data resampling,"The combination of multiple clusterings is a difficult problem in the practice of distributed data mining. Both the cluster generation mechanism and the partition integration process influence the quality of the combinations. We propose a data resampling approach for building cluster ensembles that are both robust and stable. In particular, we investigate the effectiveness of a bootstrapping technique in conjunction with several combination algorithms. The empirical study shows that a meaningful consensus partition for an entire set of objects emerges from multiple clusterings of bootstrap samples, given optimal combination algorithm parameters. Experimental results for ensembles with varying numbers of partitions and clusters are reported for simulated and real data sets. Experimental results show improved stability and accuracy for consensus partitions obtained via a bootstrapping technique.","Clustering algorithms,
Partitioning algorithms,
Robustness,
Diversity reception,
Feature extraction,
Computational complexity,
Mutual information,
Computer science,
Data mining,
Stability"
Nanocapsules: coating for living cells,"One of the most promising tools for future applications in science and medicine is the use of nanotechnologies. Especially self-assembly systems, e.g., polyelectrolyte (PE) capsules prepared by means of the layer-by-layer technique with tailored properties, fulfill the requirements for nano-organized systems in a satisfactory manner. The nano-organized shells are suitable as coating for living cells or artificial tissue to prevent immune response. With these shells, material can be delivered to predefined organs. In this paper, some preliminary results are presented, giving a broad overview over the possibilities to use nano-organized capsules. Based on the observations that the cells while duplicating break the capsule a mutant yeast strain (Saccharomyces cerevisiae), which express GFP-tubulin under galactose promotion, was investigated by means of confocal laser scanning microscopy. The measurements reveal an increased surface charge in the region of buds developed prior encapsulation. In order to test the used PE pair for cytotoxicity, germinating conidia of the fungi Neurospora crassa were coated. The investigation with fluorescence microscopy shows a variation in the surface charge for the growing region and the conidium poles. The capsules exhibit interesting properties as valuable tool in science and a promising candidate for application in the field of medicine.","Coatings,
Fungi,
Microscopy,
Self-assembly,
Immune system,
Biological materials,
Capacitive sensors,
Current measurement,
Charge measurement,
Encapsulation"
An asymmetric watermarking system with many embedding watermarks corresponding to one detection watermark,"In this letter, we propose a new asymmetric watermarking system which can accommodate many embedding watermarks but needs only one reference watermark for detection. Such a system is useful in averting attacks that seek to estimate the embedding watermark. In the proposed system, the phase of the reference watermark is shifted randomly (clockwise or counterclockwise) in the discrete Fourier transform domain to make embedding watermarks. They are correlated with one another and have the same correlation with the reference one. We also address how to select the design parameters.","Watermarking,
Discrete Fourier transforms,
Detectors,
Object detection,
Clocks,
Discrete transforms,
Security,
Computer science,
System testing,
Signal generators"
An efficient scheduling algorithm for combined input-crosspoint-queued (CICQ) switches,"With today's ASIC technology, a large amount of memory can be easily implemented in a single chip. This makes the combined input-crosspoint-queued (CICQ) crossbar switch a more attractive solution than the traditional input-queued (IQ) crossbar switch because of the simplicity of the CICQ switch scheduling. We propose a shortest crosspoint buffer first (SCBF) scheme, and prove that it achieves 100% throughput for any admissible traffic. To facilitate hardware implementation, a maximal SCBF solution is also proposed. Our simulations show that the maximal SCBF performs almost identically to the maximum solution, and better than existing IQ and CICQ schemes. The time complexity of the maximal SCBF is O(log N), feasible for fast hardware implementation.","Scheduling algorithm,
Switches,
Iterative algorithms,
Throughput,
Hardware,
Round robin,
Traffic control,
Application specific integrated circuits,
Computer science,
Impedance matching"
Mining deterministic biclusters in gene expression data,"A bicluster of a gene expression dataset captures the coherence of a subset of genes and a subset of conditions. Biclustering algorithms are used to discover biclusters whose subset of genes are co-regulated under subset of conditions. In this paper, we present a novel approach, called DBF (deterministic biclustering with frequent pattern mining) to finding biclusters. Our scheme comprises two phases. In the first phase, we generate a set of good quality biclusters based on frequent pattern mining. In the second phase, the biclusters are further iteratively refined (enlarged) by adding more genes and/or conditions. We evaluated our scheme against FLOC and our results show that DBF can generate larger and better biclusters.",
Oil and water? High performance garbage collection in Java with MMTk,"Increasingly popular languages such as Java and C# require efficient garbage collection. This paper presents the design, implementation, and evaluation of MMTk, a Memory Management Toolkit for and in Java. MMTk is an efficient, composable, extensible, and portable framework for building garbage collectors. MMTk uses design patterns and compiler cooperation to combine modularity and efficiency. The resulting system is more robust, easier to maintain, and has fewer defects than monolithic collectors. Experimental comparisons with monolithic Java and C implementations reveal MMTk has significant performance advantages as well. Performance critical system software typically uses monolithic C at the expense of flexibility. Our results refute common wisdom that only this approach attains efficiency, and suggest that performance critical software can embrace modular design and high-level languages.","Petroleum,
Java,
Memory management,
Software engineering,
Robustness,
Hybrid power systems,
Virtual machining,
Computer science,
Buildings,
System software"
Flat-gain fiber Raman amplifiers using equally spaced pumps,"This paper analyzes the gain flatness of multiwavelength pumped fiber Raman amplifiers using equally spaced pumps with both a fixed and an optimized central pump wavelength. The signal gain ripple using equally spaced pumps is compared with the case in which the pump wavelengths are allowed to vary for two, four, and eight pumps with 20-, 40-, and 80-nm signal bandwidths. The paper shows that using an optimized central pump wavelength with equal pump spacing simplifies system design, while the gain ripple is no more than 0.4 dB larger than the ripple obtained when the pump wavelengths are optimized for the cases considered.",
General reconstruction theory for multislice X-ray computed tomography with a gantry tilt,"This paper discusses image reconstruction with a tilted gantry in multislice computed tomography (CT) with helical (spiral) data acquisition. The reconstruction problem with gantry tilt is shown to be transformable into the problem of reconstructing a virtual object from multislice CT data with no gantry tilt, for which various algorithms exist in the literature. The virtual object is related to the real object by a simple affine transformation that transforms the tilted helical trajectory of the X-ray source into a nontilted helix, and the real object can be computed from the virtual object using one-dimensional interpolation. However, the interpolation may be skipped since the reconstruction of the virtual object on a Cartesian grid provides directly nondistorted images of the real object on slices parallel to the tilted plane of the gantry. The theory is first presented without any specification of the detector geometry, then applied to the curved detector geometry of third-generation CT scanners with the use of Katsevich's formula for example. Results from computer-simulated data of the FORBILD thorax phantom are given in support of the theory.",
Auto-calibration of multi-projector display walls,"By treating projectors as pin-hole cameras, we show it is possible to calibrate the projectors of a casually-aligned, multi-projector display wall using the principles of planar auto-calibration. We also use a pose estimation technique for planar scenes to reconstruct the relative pose of a calibration camera, the projectors and the plane they project on. Together with assumptions about the pose of the camera, we use the reconstruction to automatically compute the projector-display homographies needed to render properly scaled and oriented imagery on the display wall. The main contribution of this paper is thus to provide a fully automated approach to calibrate a multi-projector display wall without the need for fiducials or interaction.",
Seamless channel transition for the staircase video broadcasting scheme,"In the literature, many broadcasting-based schemes have been proposed to efficiently support near-VOD services. However, none of these schemes allows the server to dynamically and seamlessly change the number of channels allocated to a video. Naively allocating a new set of channels for the transition could increase server's load, waste communication bandwidth, and even drain the channels of the system. In Tseng et al. (2000), it is shown how to enhance the Fast Broadcasting (FB) scheme for seamless channel transition. The problem remains open whether other broadcasting-based schemes can sustain seamless channel transition. In this paper, we show how to enhance the Staircase Broadcasting (SB) scheme so that a server can seamlessly increase or decrease the channels allocated to a video. The SB scheme has been proved to require significantly less buffering space than FB, while sustaining the same startup latency as FB. Detailed performance comparisons are presented to demonstrate the advantages of the proposed scheme.","Multimedia communication,
Broadcasting,
Bandwidth,
Educational institutions,
Computer science,
Delay,
Channel allocation,
Broadband communication,
Libraries"
Exploring large document repositories with RDF technology: the DOPE project,"This thesaurus-based search system uses automatic indexing, RDF-based querying, and concept-based visualization of results to support exploration of large online document repositories. Innovative research institutes rely on the availability of complete and accurate information about new research and development. Information providers such as Elsevier make it their business to provide the required information in a cost-effective way. The semantic Web will likely contribute significantly to this effort because it facilitates access to an unprecedented quantity of data. The DOPE project (Drug Ontology Project for Elsevier) explores ways to provide access to multiple life-science information sources through a single interface.",
Investigation of millisecond-long analog single-event transients in the LM6144 op amp,"A new category of analog single-event transients (SETs) with millisecond-long durations have been experimentally observed in the LM6144 operational amplifier. It is the first time that events with such extreme widths are under investigation in a linear integrated circuit. Relying on heavy-ion broadbeam tests, picosecond pulsed lasers diagnostics, and computer-assisted circuit modeling, we uncover the mechanisms and causes of these anomalous voltage transients. The identification of the problematic area of the IC reveals that the bias/startup circuitry is sensitive to energetic ionizing particles and can be responsible for corrupted circuit operations when subjected to a heavy-ion strike. A circuit hardening solution with minimal impact on the layout and the electrical performances of the op amp are proposed to mitigate this effect.","Operational amplifiers,
Pulse amplifiers,
Space vector pulse width modulation,
Circuit testing,
Optical pulses,
NASA,
Pulse circuits,
Optical pulse generation,
Voltage,
Radiation hardening"
Contextual processing of structured data by recursive cascade correlation,"This paper propose a first approach to deal with contextual information in structured domains by recursive neural networks. The proposed model, i.e., contextual recursive cascade correlation (CRCC), a generalization of the recursive cascade correlation (RCC) model, is able to partially remove the causality assumption by exploiting contextual information stored in frozen units. We formally characterize the properties of CRCC showing that it is able to compute contextual transductions and also some causal supersource transductions that RCC cannot compute. Experimental results on controlled sequences and on a real-world task involving chemical structures confirm the computational limitations of RCC, while assessing the efficiency and efficacy of CRCC in dealing both with pure causal and contextual prediction tasks. Moreover, results obtained for the real-world task show the superiority of the proposed approach versus RCC when exploring a task for which it is not known whether the structural causality assumption holds.",
Using a sensor network for distributed multi-robot task allocation,"We present a multi field distributed in-network task allocation (DINTA-MF) algorithm for online multi-robot task allocation (OMRTA) where tasks are allocated explicitly to robots by a pre-deployed, static sensor network. The idea of DINTA-MF is to compute several assignment fields in the sensor network and then distributively assign fields to different robots. Experimental results with a simulated alarm scenario show that our approach is able to compute solutions to the OMRTA problem in a distributed fashion and arguably in an optimal way. We compared DINTA-MF with a simpler implementation (DINTA), which uses one assignment field. The data show that DINTA-MF outperforms DINTA as the number of robots increases.","Robot sensing systems,
Navigation,
Computer networks,
Mobile robots,
Embedded system,
Distributed computing,
Wireless sensor networks,
Laboratories,
Computer science,
Sensor systems"
Supervised clustering - algorithms and benefits,"This work centers on a novel data mining technique we term supervised clustering. Unlike traditional clustering, supervised clustering assumes that the examples are classified and has the goal of identifying class-uniform clusters that have high probability densities. Four representative-based algorithms for supervised clustering are introduced: a greedy algorithm with random restart, named SRIDHCR, that seeks for solutions by inserting and removing single objects from the current solution, SPAM (a variation of the clustering algorithm PAM), an evolutionary computing algorithm named SCEC, and a fast medoid-based top-down splitting algorithm, named TDS. The four algorithms were evaluated using a benchmark consisting of four UCI machine learning data sets. In general, it seems that ""greedy"" algorithms, such as SPAM, SRIDHCR, and TDS, do not perform particularly well for supervised clustering and seem to terminate prematurely too often. We also briefly describe the applications of supervised clustering.","Clustering algorithms,
Machine learning algorithms,
Partitioning algorithms,
Unsolicited electronic mail,
Impurities,
Computer science,
Data mining,
Greedy algorithms,
Machine learning,
Unsupervised learning"
Combining Adaboost learning and evolutionary search to select features for real-time object detection,"Recently, P. Viola and M.J. Jones (2001) presented a method for real-time object detection in images using a boosted cascade of simple features. In This work we show how an evolutionary algorithm can be used within the Adaboost framework to find new features providing better classifiers. The evolutionary algorithm replaces the exhaustive search over all features so that even very large feature sets can be searched in reasonable time. Experiments on two different sets of images prove that by the use of evolutionary search we are able to find object detectors that are faster and have higher detection rates.","Object detection,
Detectors,
Genetic programming,
Evolutionary computation,
Face detection,
Robustness,
Computer vision,
Decision trees,
Convolution,
Computer science"
Pull vs push: a quantitative comparison for data broadcast,"Advances in wireless technology have resulted in an emerging broadcast-capable infrastructure. A major debate for such infrastructures is whether we should use push or pull to support large client populations. To date, push was suggested to provide a scalability that is not possible with pull. In this paper we conclude otherwise, based on simulation and experimental prototype results. A major contribution of this paper is to show that pull-based algorithms outperform push-based ones. Contrary to the conventional wisdom, this is true even for environments where request processing and scheduling overheads are a significant factor.",
Topological lines in 3D tensor fields,"Visualization of 3D tensor fields continues to be a major challenge in terms of providing intuitive and uncluttered images that allow the users to better understand their data. The primary focus of this paper is on finding a formulation that lends itself to a stable numerical algorithm for extracting stable and persistent topological features from 2nd order real symmetric 3D tensors. While features in 2D tensors can be identified as either wedge or trisector points, in 3D, the corresponding stable features are lines, not just points. These topological feature lines provide a compact representation of the 3D tensor field and are essential in helping scientists and engineers understand their complex nature. Existing techniques work by finding degenerate points and are not numerically stable, and worse, produce both false positive and false negative feature points. This work seeks to address this problem with a robust algorithm that can extract these features in a numerically stable, accurate, and complete manner.","Tensile stress,
Topology,
Data visualization,
Data mining,
Feature extraction,
Computer science,
Focusing,
Robustness,
Chromium,
Computer graphics"
"Window-based, discontinuity preserving stereo","Traditionally, the problem of stereo matching has been addressed either by a local window-based approach or a dense pixel-based approach using global optimization. In this paper we present an algorithm which combines window-based local matching into a global optimization framework. Our local matching algorithm assumes that local windows can have at most two disparities. Under this assumption, the local matching can be performed very efficiently using graph cuts. The global matching is formulated as minimization of an energy term that takes into account the matching constraints induced by the local stereo algorithm. Fast, approximate minimization of this energy is achieved through graph cuts. The key feature of our algorithm is that it preserves discontinuities both during the local as well as global matching phase.",
A multi-layered approach to security in high assurance systems,"Past efforts at designing and implementing ultra high assurance systems for government security and safety have centered on the concept of a monolithic security kernel responsible for a system-wide security policy. This approach leads to inflexible, overly complex operating systems that are too large to evaluate at the highest assurance levels (e.g., Common Criteria EAL 5 and above). We describe a new multi-layered approach to the design and verification of embedded trustworthy systems that is currently being used in the implementation of real time, embedded applications. The framework supports multiple levels of safety and multiple levels of security, based on the principle of creating separate layers of responsibility and control, with each layer responsible for enforcing its own security policy.",
Using emerging patterns and decision trees in rare-class classification,"The problem of classifying rarely occurring cases is faced in many real life applications. The scarcity of the rare cases makes it difficult to classify them correctly using traditional classifiers. In this paper, we propose an approach to use emerging patterns (EPs) (G. Dong and J. Li, 1999) and decision trees (DTs) in rare-class classification (EPDT). EPs are those itemsets whose supports in one class are significantly higher than their supports in the other classes. EPDT employs the power of EPs to improve the quality of rare-case classification. To achieve this aim, we first introduce the idea of generating nonexisting rare-class instances, and then we over-sample the most important rare-class instances. Our experiments show that EPDT outperforms many classification methods.","Decision trees,
Classification tree analysis,
Itemsets,
Computer science,
Software engineering,
Application software,
Data mining,
Law,
Legal factors,
Encoding"
Adaptive multi-resource prediction in distributed resource sharing environment,"Resource prediction can greatly assist resource selection and scheduling in a distributed resource sharing environment such as a computational Grid. Existing resource prediction models are either based on the auto-correlation of a single resource or based on the cross correlation between two resources. In this paper, we propose a multi-resource prediction model (MModel) that uses both kinds of correlations to achieve higher prediction accuracy. We also present two adaptation techniques that enable the MModel to adapt to the time-varying characteristics of the underlying resources. Experimental results with CPU load prediction in both workstation and Grid environment show that on average, the adaptive MModel (called MModel-a) can achieve from 6% to more than 96% reduction in prediction errors compared with the autoregressive (AR) model, which has previously been shown to work well for CPU load predictions.","Resource management,
Predictive models,
Grid computing,
Accuracy,
Autocorrelation,
Workstations,
History,
Computer science,
Processor scheduling,
Distributed computing"
Detecting and debugging insecure information flows,"A new approach to dynamic information flow analysis is presented that can be used to detect and debug insecure flows in programs. It can be applied offline to validate and debug a program against an information flow policy, or, when fast response is not critical, it can be applied online to prevent illegal flows in deployed programs. Since dynamic analysis alone is inherently unable to detect implicit information flows, our approach incorporates a static preprocessing phase that permits detection of most implicit flows at runtime, in addition to explicit ones. To support interactive debugging of insecure flows, it also incorporates a new forward computing algorithm for dynamic slicing, which is more precise than previous forward computing algorithms and is not restricted to programs with structured control flow. A prototype tool implementing the proposed approach has been developed for Java byte code programs. Case studies in which this tool was applied to several subject programs are described.","Debugging,
Information analysis,
Runtime,
Phase detection,
Application software,
Computer science,
Heuristic algorithms,
Prototypes,
Java,
Data security"
Sign detection in natural images with conditional random fields,"Traditional generative Markov random fields for segmenting images model the image data and corresponding labels jointly, which requires extensive independence assumptions for tractability. We present the conditional random field for an application in sign detection, using typical scale and orientation selective texture filters and a nonlinear texture operator based on the grating cell. The resulting model captures dependencies between neighboring image region labels in a data-dependent way that escapes the difficult problem of modeling image formation, instead focusing effort and computation on the labeling task. We compare the results of training the model with pseudo-likelihood against an approximation of the full likelihood with the iterative tree reparameterization algorithm and demonstrate improvement over previous methods","Computer vision,
Markov random fields,
Labeling,
Probability distribution,
Computer science,
Application software,
Filters,
Gratings,
Focusing,
Iterative methods"
Cooperative location-sensing for wireless networks,"We present the cooperative location-sensing system (CLS), an adaptive location-sensing system that enables devices to estimate their position in a self-organizing manner without the need for an extensive infrastructure or training. Hosts cooperate and share positioning information. CLS uses a grid representation that allows an easy incorporation of external information to improve the accuracy of the position estimation. We evaluated the performance of CLS via simulation and investigated the impact of the density of landmarks, degree of connectivity, range error, and grid resolution on the accuracy. We found that the average error is less than 2% of the transmission range, when used in a terrain with 20% of the hosts to be landmarks, average network connectivity above 7, and distance estimation error equal to 5% of the transmission range.","Wireless networks,
Costs,
Computer science,
Global Positioning System,
Time measurement,
Ultrasonic variables measurement,
Pulse measurements,
Adaptive systems,
Computer errors,
Estimation error"
A traffic model for networked devices in the building automation,"Traffic models play a decisive part in the performance evaluation and design of communications systems. However, the growing diversification of devices in large networks asks for automated generated traffic models. In this paper, we present a generic device model which is suited for automated generation in the domain of building automation. The traffic is deduced successive for common devices from measurements and simulations using classification of the devices by their traffic behaviour and automated parameterization of the device models","Telecommunication traffic,
Traffic control,
Intelligent networks,
Automation,
Predictive models,
Calculus,
Buildings,
Computer science,
Communication system traffic,
Communication systems"
Fast-handoff schemes for application layer mobility management,"In order to ensure proper quality of service for real-time communication in a mobile wireless Internet environment it is essential to minimize the transient packet loss when the mobile is moving between different cells (subnets) within a domain. Network layer mobility management schemes have been proposed to provide optimized fast-handoff for multimedia streams during a client's frequent movement within a domain. This paper introduces application layer techniques to achieve fast-handoff for real-time RTP/UDP based multimedia traffic in a SIP signaling environment. These techniques are based on standard SIP components such as user agent and proxy which usually participate to set up and tear down the multimedia sessions between the mobiles. Unlike network layer techniques, application layer techniques do not have to depend upon any additional components such as home agent and foreign agent. It thus provides a network access independent solution suitable for application service providers.","Mobile radio mobility management,
Delay,
Streaming media,
Multimedia systems,
Mobile computing,
Telecommunication traffic,
Application software,
Computer science,
Quality of service,
Web and internet services"
The education of a software engineer,"A successful software engineer must possess a wide range of skills and talents. Project managers know how difficult it is to find, motivate, and retain such people. Educators face a complementary, and perhaps more challenging, problem: how to prepare such engineers. The challenge of what to teach software engineers evolves over time as technologies, applications, and requirements change. As software technology has rapidly spread through every aspect of modern societies, the challenge of educating software engineers has taken on new form and become more complex and urgent. The author presents the broad outline of an educational program for a complete software engineer. A new curriculum for computer science has been developed based on these ideas and it started in October 2004 at the University of Lugano in Switzerland.","Software engineering,
Application software,
Computer science,
Education,
Project management,
Educational programs,
Knowledge engineering,
Software testing,
Unified modeling language,
Software architecture"
Assessing the robustness of self-managing computer systems under highly variable workloads,"Computer systems are becoming extremely complex due to the large number and heterogeneity of their hardware and software components, the multilayered architecture used in their design, and the unpredictable nature of their workloads. Thus, performance management becomes difficult and expensive when carried out by human beings. An approach, called self-managing computer systems, is to build into the systems the mechanisms required to self-adjust configuration parameters so that the quality of service requirements of the system are constantly met. In this paper, we evaluate the robustness of such methods when the workload exhibits high variability in terms of the interarrival time and service times of requests. Another contribution of this paper is the assessment of the use of workload forecasting techniques in the design of QoS controllers.","Robustness,
Control systems,
Quality of service,
Computer architecture,
Hardware,
Humans,
Computer displays,
Computer science,
Software systems,
Robust control"
Hardness of buy-at-bulk network design,"We consider the buy-at-bulk network design problem in which we wish to design a network for carrying multicommodity demands from a set of source nodes to a set of destination nodes. The key feature of the problem is that the cost of capacity on each edge is concave and hence exhibits economies of scale. If the cost of capacity per unit length can be different on different edges then, we say that the problem is non-uniform. The problem is uniform otherwise. We show that for any constant /spl gamma/, if NP /spl nsube/ ZPTIME(n/sup polylog n/), then there is no O(log/sup 1/2 - /spl gamma//N)-approximation algorithm for non-uniform buy-at-bulk network design and there is no O(log/sup 1/4 - /spl gamma//N)-approximation algorithm for the uniform problem.",
Analytical analysis of access-schemes of the CSMA type,"This paper introduces an analytical approach for the analysis of medium access protocols of the CSMA type, which are used in many control networks. In contrast to known approaches the proposed technique considers systems with different customer classes and class specific arrival rates. Analysis is based on a decomposition approach, which determines class specific measures from the repeated analysis of isolated queues, which are coupled by a joint computation of service rates depending on the utilization of all queues. The results presented in this paper can be used as building blocks in product form algorithms for the analytical analysis of large systems consisting of several network segments connected via routers or bridges.",
QoS topology control in ad hoc wireless networks,"This work discusses the energy efficient QoS topology control problem in ad hoc wireless networks. Given a set of nodes in a plane, end-to-end traffic demands and delay bounds between node pairs, the problem is to find a network topology that can meet the QoS requirements and the maximum transmitting power of nodes is minimized. We consider two cases of the problem: 1) the traffic demands are not splittable, and 2) the traffic demands are splittable. For the former case, the problem is formulated as an integer linear programming problem. For the latter case, the problem is formulated as a mixed integer programming problem, and an optimal algorithm has been proposed to solve the problem.",
Holographic algorithms,"We introduce a new notion of efficient reduction among computational problems. Classical reductions involve gadgets that map local solutions of one problem to local solutions of another in one-to-one, or possibly many-to-one or one-to-many, fashion. Our proposed reductions allow for gadgets with many-to-many correspondences. Their objective is to preserve the sum of the local solutions. Such reductions provide a method of translating a combinatorial problem to a family of finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if some system in the family has a solution over the complex numbers. We can derive polynomial time algorithms in this way for ten problems for which only exponential time algorithms were known before. General questions about complexity classes are also formulated. If the method is applied to a #P-complete problem then we obtain families of polynomial systems such that the solvability of any one member would imply P/sup #P/ = NC2.",
Compiler optimization of memory-resident value communication between speculative threads,"Efficient inter-thread value communication is essential for improving performance in thread-level speculation (TLS). Although several mechanisms for improving value communication using hardware support have been proposed, there is relatively little work on exploiting the potential of compiler optimization. Building on recent research on compiler optimization of scalar value communication between speculative threads, we propose compiler techniques for the optimization of memory-resident values. In TLS, data dependences through memory-resident values are tracked by the underlying hardware and preserved by re-executing any speculative thread that violates a dependence; however, re-execution incurs a large performance penalty and should be used only to resolve data dependences that are infrequent. In contrast, value communication for frequently-occurring data dependences must be very efficient. We propose using the compiler to first identify frequently-occurring memory-resident data dependences, then insert synchronization for communicating values to preserve these dependences. We find that by synchronizing frequently-occurring data dependences we can significantly improve the efficiency of parallel execution. A comparison between compiler-inserted and hardware-inserted memory synchronization reveals that the two techniques are complementary, with each technique benefitting different benchmarks.","Optimizing compilers,
Yarn,
Hardware,
Program processors,
Computer science,
Runtime,
Data structures,
Costs,
Turning"
"Parameter-free, adaptive clonal selection","The 'clonal selection' approach to optimization - an abstraction of immune system adaptation, popularized mainly by de Castro and Von Zuben - may have reached something of an impasse. The method requires several parameters that must be tuned, and it normally uses a binary representation that can limit the accuracy of the results. This paper examines the uses and drawbacks of clonal selection, and suggests some ways forward, based on an analysis of the operators for choosing the amount of mutation and the number of clones. As a result, an effective, real-valued, parameter-free clonal selection algorithm is introduced, called adaptive clonal selection (ACS).","Immune system,
Cloning,
Response surface methodology,
Genetic mutations,
Computational biology,
Computer science,
Bioinformatics,
Psychology,
Educational institutions,
Pathogens"
Performance models for evaluation and automatic tuning of symmetric sparse matrix-vector multiply,"We present optimizations for sparse matrix-vector multiply SpMV and its generalization to multiple vectors, SpMM, when the matrix is symmetric: (1) symmetric storage, (2) register blocking, and (3) vector blocking. Combined with register blocking, symmetry saves more than 50% in matrix storage. We also show performance speedups of 2.1/spl times/ for SpMV and 2.6/spl times/ for SpMM, when compared to the best nonsymmetric register blocked implementation. We present an approach for the selection of tuning parameters, based on empirical modeling and search that consists of three steps: (1) Off-line benchmark, (2) Runtime search, and (3) Heuristic performance model. This approach generally selects parameters to achieve performance with 85% of that achieved with exhaustive search. We evaluate our implementations with respect to upper bounds on performance. Our model bounds performance by considering only the cost of memory operations and using lower bounds on the number of cache misses. Our optimized codes are within 68% of the upper bounds.","Sparse matrices,
Symmetric matrices,
Registers,
Upper bound,
Costs,
Runtime,
Kernel,
Bandwidth,
Computer science,
Performance analysis"
Performance Evaluation of Parallel Large-Scale Lattice Boltzmann Applications on Three Supercomputing Architectures,"Computationally intensive programs with moderate communication requirements such as CFD codes suffer from the standard slow interconnects of commodity ""off the shelf"" (COTS) hardware. We will introduce different large-scale applications of the Lattice Boltzmann Method (LBM) in fluid dynamics, material science, and chemical engineering and present results of the parallel performance on different architectures. It will be shown that a high speed communication network in combination with an efficient CPU is mandatory in order to achieve the required performance. An estimation of the necessary CPU count to meet the performance of 1 TFlop/s will be given as well as a prediction as to which architecture is the most suitable for LBM. Finally, ratios of costs to application performance for tailored HPC systems and COTS architectures will be presented.","Large-scale systems,
Lattice Boltzmann methods,
Computer architecture,
Computational fluid dynamics,
Code standards,
Communication standards,
Hardware,
Fluid dynamics,
Materials science and technology,
Chemical engineering"
Non-redundant data clustering,"Data clustering is a popular approach for automatically finding classes, concepts, or groups of patterns. In practice, this discovery process should avoid redundancies with existing knowledge about class structures or groupings, and reveal novel, previously unknown aspects of the data. In order to deal with this problem, we present an extension of the information bottleneck framework, called coordinated conditional information bottleneck, which takes negative relevance information into account by maximizing a conditional mutual information score subject to constraints. Algorithmically, one can apply an alternating optimization scheme that can be used in conjunction with different types of numeric and non-numeric attributes. We present experimental results for applications in text mining and computer vision.","Data mining,
Mutual information,
Computer science,
Application software,
Text mining,
Computer vision,
Face detection,
Cities and towns,
Geography,
Demography"
Comprehensive structured context profiles (CSCP): design and experiences,"In dynamic heterogeneous environments, such as pervasive computing, context-aware adaptation is a key concept to meet the varying requirements of different clients. To enable such functionality, context information must be gathered and eventually presented to the application performing the adaptation. Therefore, a common representation format for the context information is required. We introduce a novel representation language for context information: comprehensive structured context profiles (CSCP). CSCP is based on the resource description framework (RDF) and is designed to be comprehensive and thoroughly structured to describe the entire context of mobile sessions. Besides the design of CSCP, we describe our experiences with CSCP in a running system, a mobility portal for context-aware adaptive access to email and Web information services.",
Processing range-monitoring queries on heterogeneous mobile objects,"We consider in this paper how to leverage heterogeneous mobile computing capability for efficient processing of real-time range-monitoring queries. In our environment, each mobile object is associated with a resident domain and when an object moves, it monitors its spatial relationship with its resident domain and the monitoring areas inside it. An object reports its location to server whenever its movement affects any query results (i.e., crossing any query boundaries) or it moves out of its resident domain. In the first case, the server updates the affected query results accordingly while in the second case, the server determines a new resident domain for the object. This distributive approach is able to provide accurate query results and real-time monitoring updates with minimal location update and server processing costs. In addition, the new scheme allows a mobile object to negotiate a resident domain based on its computing capability. Thus, a more capable object can have a larger resident domain reducing its chance of having to request a new resident domain because of moving out of it. This feature makes the new approach highly adaptive to the heterogeneity of mobile objects. In our performance study, we compare it with an existing approach using simulation. The study shows that the new technique is many times better in reducing mobile communication and server processing costs.",
Simultaneous communication and processor voltage scaling for dynamic and leakage energy reduction in time-constrained systems,"We propose a new technique for the combined voltage scaling of processors and communication links, taking into account dynamic as well as leakage power consumption. The voltage scaling technique achieves energy efficiency by simultaneously scaling the supply and body bias voltages in the case of processors and buses with repeaters, while energy efficiency on fat wires is achieved through dynamic voltage swing scaling. We also introduce a set of accurate communication models for the energy estimation of voltage scalable embedded systems. In particular, we demonstrate that voltage scaling of bus repeaters and dynamic adaption of the voltage swing on fat wires can significantly influence the system's energy consumption. Experimental results, conducted on numerous generated benchmarks and a real-life example, demonstrate that substantial energy savings can be achieved with the proposed techniques.","Dynamic voltage scaling,
Energy consumption,
Wires,
Voltage control,
Computer science,
Repeaters,
Integrated circuit interconnections,
Frequency,
CMOS technology,
Information science"
New family of p-ary sequences with optimal correlation property and large linear span,"For an odd prime p and integers n, m, and k such that n=(2m+1)k, a new family of p-ary sequences of period p/sup n/-1 with optimal correlation property is constructed using the p-ary Helleseth-Gong sequences with ideal autocorrelation, where the size of the sequence family is p/sup n/. That is, the maximum nontrivial correlation value R/sub max/ of all pairs of distinct sequences in the family does not exceed p/sup n/2/+1, which means the family has optimal correlation in terms of Welch's lower bound. The symbol distribution of the sequences in the family is enumerated. It is also shown that the linear span of the sequences in the family is (m+2)n except for the m-sequence in the family.","Autocorrelation,
Multiaccess communication,
Wireless communication,
Councils,
Computer science,
Informatics"
Roaming honeypots for mitigating service-level denial-of-service attacks,"Honeypots have been proposed to act as traps for malicious attackers. However, because of their deployment at fixed (thus detectable) locations and on machines other than the ones they are supposed to protect, honeypots can be avoided by sophisticated attacks. We propose roaming honeypots, a mechanism that allows the locations of honeypots to be unpredictable, continuously changing, and disguised within a server pool. A (continuously changing) subset of the servers is active and providing service, while the rest of the server pool is idle and acting as honeypots. We utilize our roaming honeypots scheme to mitigate the effects of service-level DoS attacks, in which many attack machines acquire service from a victim server at a high rate, against back-end servers of private services. The roaming honeypots scheme detects and filters attack traffic from outside a firewall (external attacks), and also mitigates attacks from behind a firewall (internal attacks) by dropping all connections when a server switches from acting as a honeypot into being active. Through ns-2 simulations, we show the effectiveness of our roaming honeypots scheme. In particular, against external attacks, our roaming honeypots scheme provides service response time that is independent of attack load for a fixed number of attack machines.","Computer crime,
Network servers,
Protection,
Web server,
Telecommunication traffic,
Intrusion detection,
Computer science,
Information science,
Filters,
Traffic control"
The coverage problem in three-dimensional wireless sensor networks,"One of the fundamental issues in sensor networks is the coverage problem, which reflects how well a sensor network is monitored or tracked by sensors. In this paper, we formulate this problem as a decision problem, whose goal is to determine whether every point in the service area of the sensor network is covered by at least /spl alpha/ sensors, where /spl alpha/ is a given parameter and the sensing regions of the sensors are modeled by balls (not necessarily of the same radius). This problem in a 2D space is solved in (C. F. Huang et al, ACM Int'l W'kshop on Wireless Sensor Networks and App., p.115-12 1, 2003) with an efficient polynomial-time algorithm (in terms of the number of sensors). In this paper, we show that tackling this problem in a 3D space is still feasible within polynomial time. The proposed solution can be easily translated into an efficient polynomial-time distributed protocol.",
Selection of variable block sizes in H.264,The paper aims at selecting an efficient variable block size mode in H.264 video coding standard for better compression performance. This standard allows video frames to be partitioned into variable block sizes such that blocks containing highly detailed motion are represented using small block sizes and the rest using large block sizes. New techniques for intelligent selection of the variable block sizes have been developed to reduce the computational complexity without sacrificing the quality of the coder. The proposed schemes are based on the motion vector cost and previous frame information. An improvement in the encoding time with negligible impact on the subjective and the quantitative performance has been achieved. A comparison of the proposed techniques for various test sequences is also provided.,
Localized low-weight graph and its applications in wireless ad hoc networks,"We propose a new localized structure, namely, Incident MST and RNG Graph (IMRG), for topology control and broadcasting in wireless ad hoc networks. In the construction algorithm, each node first builds a modified relative neighborhood graph (RNG'), and then informs its one-hop neighbors its incident edges in RNG'. Each node then collects all its one-hop neighbors and the two-hop neighbors who have RNG edges to some of its one-hop neighbors, and builds an Euclidean minimum spanning tree of these nodes. Each node u keeps an edge uv only if uv is in the constructed minimum spanning tree. We analytically prove that the node degree of the IMRG is at most 6, it is connected and planar, and more importantly, the total edge length of the IMRG is within a constant factor of that of the minimum spanning tree. To the best of our knowledge, this is the first algorithm that can construct a structure with all these properties using small communication messages (at most 13n total messages, each with O(logn) bits) and small computation cost, where n is the number of wireless nodes. Test results are corroborated in the simulation study.","Intelligent networks,
Ad hoc networks,
Mobile ad hoc networks,
Network topology,
Relays,
Wireless networks,
Costs,
Application software,
Computer science,
Tree graphs"
Omnicon:a mobile ip-based vertical handoff system for wireless LAN and GPRS links,"Wi-Fi based hotspots offer mobile users broadband wireless Internet connectivity in public work spaces and corporate/ university campuses. Despite aggressive deployment of these hotspots in recent years, high-speed wireless Internet access remains restricted to a small number of geographical areas because of limited physical coverage of wireless LANs. On the other hand, despite their lower throughput, cellular networks have a significantly wider coverage and are thus much more available. Recognizing that 2.5G or 3G cellular networks can effectively complement wireless LANs, we set out to develop a vertical handoff system that allows mobile users to seamlessly fall back to such cellular networks as GPRS or 3G whenever wireless LAN connectivity is not available. The resulting handoff mechanism allows a network connection on a mobile node to operate over multiple wireless access networks in a way that is completely transparent to end user applications. In this paper, we present the design, implementation, and evaluation of a fully operational vertical handoff system, called OmniCon, which enables mobile nodes to automatically switch between wireless LAN and GPRS, based on wireless LAN availability, by introducing a simple extension to existing Mobile IP implementation. We discuss the design issues in the proposed vertical handoff system for heterogeneous networks, including connection setup problems due to network address translation, and the disparity in link characteristics between wireless LANs and GPRS. A detailed performance evaluation study of the OmniCon prototype demonstrates its ability to migrate active network connections between these two wireless technologies with low handoff latency and close to zero packet loss.",
"Random landmarking in mobile, topology-aware peer-to-peer networks","DHT can locate objects in a peer-to-peer network within an efficient amount of overlay hops. Since an overlay hop is likely to consist of multiple physical hops, the ratio between the number of physical hops induced by the overlay routing process and the number of physical hops on a direct physical path is often significantly lopsided. Recently, some approaches have been suggested to optimize that ratio by building topology-aware peer-to-peer overlays. However, none of them were explicitly designed to handle node mobility. We present an approach that optimizes the overlay versus direct physical path ratio and maintains it even in the presence of node mobility. Thus, it is well suited for highly dynamic networks, such as ad-hoc networks.","Intelligent networks,
Peer to peer computing,
Routing,
Distributed computing,
Computer science,
Buildings,
Ad hoc networks,
Scalability,
Design optimization,
Conferences"
"Time-domain oversampled lapped transforms: theory, structure, and application in image coding","This paper generalizes time-domain lapped transforms (TDLTs) proposed by Tran et al. to oversampled systems, thus leading to time-domain oversampled lapped transforms (TDOLTs). These new transforms correspond to a subclass of oversampled linear-phase perfect reconstruction filterbanks (OLPPRFBs), which can be implemented by adding a prefilter before the discrete cosine transform (DCT) and a post-filter after the inverse discrete cosine transform (IDCT). Structures of the pre- and post-filters are developed, and the frame-theoretic properties of TDOLTs are analyzed. A new parameterization of lattice matrices through the Givens-QR factorization is proposed for unconstrained optimization. Comparisons with other parameterization methods are also included. Several design examples, along with some image coding results, are presented to demonstrate the validity of the theory and the potential of TDOLTs in image coding, especially in error-resilient coding.","Time domain analysis,
Image coding,
Discrete cosine transforms,
Discrete transforms,
Image reconstruction,
Gallium nitride,
Lattices,
Time frequency analysis,
Error analysis,
Computer science"
Value and Relation Display for Interactive Exploration of High Dimensional Datasets,"Traditional multidimensional visualization techniques, such as glyphs, parallel coordinates and scatterplot matrices, suffer from clutter at the display level and difficult user navigation among dimensions when visualizing high dimensional datasets. In this paper, we propose a new multidimensional visualization technique named a value and relation (VaR) display, together with a rich set of navigation and selection tools, for interactive exploration of datasets with up to hundreds of dimensions. By explicitly conveying the relationships among the dimensions of a high dimensional dataset, the VaR display helps users grasp the associations among dimensions. By using pixel-oriented techniques to present values of the data items in a condensed manner, the VaR display reveals data patterns in the dataset using as little screen space as possible. The navigation and selection tools enable users to interactively reduce clutter, navigate within the dimension space, and examine data value details within context effectively and efficiently. The VaR display scales well to datasets with large numbers of data items by employing sampling and texture mapping. A case study on a real dataset, as well as the VaR displays of multiple real datasets throughout the paper, reveals how our proposed approach helps users interactively explore high dimensional datasets with large numbers of data items","Reactive power,
Navigation,
Scattering,
Large screen displays,
Data visualization,
Computer displays,
Visual databases,
Data analysis,
Principal component analysis,
Computer science"
Dominance measures for multi-objective simulated annealing,"Simulated annealing (SA) is a provably convergent optimiser for single-objective (SO) problems. Previously proposed MO extensions have mostly taken the form of an SO SA optimising a composite function of the objectives. We propose an MO SA utilising the relative dominance of a solution as the system energy for optimisation, eliminating problems associated with composite objective functions. We also propose a method for choosing perturbation scalings promoting search both towards and across the Pareto front. We illustrate the SA's performance on standard test problems. The new SA is shown to promote rapid convergence to the true Pareto front with a good coverage of points across it.",
Symbolic model checking the knowledge of the dining cryptographers,"This paper describes the application of symbolic techniques (in particular, OBDDs) to model checking specifications in the logic of knowledge for an agent operating with synchronous perfect recall in an environment of which it has incomplete knowledge. It discusses the application of these techniques to the verification of a security protocol: Chaum's Dining Cryptographers protocol, which provides a mechanism for anonymous broadcast.","Cryptography,
Cryptographic protocols,
Information security,
Logic,
Australia,
Computer science,
Application software,
Broadcasting,
Access protocols,
Information analysis"
Shear modulus estimation using parallelized partial volumetric reconstruction,"Magnetic resonance elastography can be limited by the computationally intensive nonlinear inversion schemes that are sometimes employed to estimate shear modulus from externally induced internal tissue displacements. Consequently, we have developed a parallelized partial volume reconstruction approach to overcome this limitation. In this paper, we report results from experiments conducted on breast phantoms and human volunteers to validate the proposed technique. More specifically, we demonstrate that computational cost is linearly related to the number of subzones used during image recovery and that both subzone parallelization and partial volume domain reduction decrease execution time accordingly. Importantly, elastograms computed based on the parallelized partial volume technique are not degraded in terms of either image quality or accuracy relative to their full volume counterparts provided that the estimation domain is sufficiently large to negate the effects of boundary conditions. The clinical results presented in this paper are clearly preliminary; however, the parallelized partial volume reconstruction approach performs sufficiently well to warrant more in-depth clinical evaluation.","Image reconstruction,
Magnetic resonance,
Breast,
Imaging phantoms,
Humans,
Computational efficiency,
Concurrent computing,
Degradation,
Image quality,
Boundary conditions"
Managing trace data volume through a heuristical clustering process based on event execution frequency,"To regain architectural insight into a program using dynamic analysis, one of the major stumbling blocks remains the large amount of trace data collected. Therefore, this paper proposes a heuristic which divides the trace data into recurring event clusters. To compose such clusters the Euclidian distance is used as a dissimilarity measure on the frequencies of the events. Manual inspection of these event sequences revealed that the heuristic provides interesting starting points for further examination.","Programming profession,
Software systems,
Reverse engineering,
Protocols,
Mathematics,
Computer science,
Frequency measurement,
Inspection,
Bridges,
Writing"
A Gramian-based controller for linear periodic systems,This note proposes a new design method for the control of linear time-periodic systems. The method is based on the reachability Gramian and a specific form for the feedback gain matrix to build a novel control law for the closed-loop system. The new controller allows assignment of all the invariants of the system. Calculating the feedback requires solving a matrix integral equation for the periodic Floquet factor of the state-transition matrix of the closed-loop system.,"Control systems,
Design methodology,
State feedback,
Computer science,
Time varying systems,
Differential equations"
Automated optimization of JPEG 2000 encoder options based on model observer performance for detecting variable signals in X-ray coronary angiograms,"Image compression is indispensable in medical applications where inherently large volumes of digitized images are presented. JPEG 2000 has recently been proposed as a new image compression standard. The present recommendations on the choice of JPEG 2000 encoder options were based on nontask-based metrics of image quality applied to nonmedical images. We used the performance of a model observer [nonprewhitening matched filter with an eye filter (NPWE)] in a visual detection task of varying signals [signal known exactly but variable (SKEV)] in X-ray coronary angiograms to optimize JPEG 2000 encoder options through a genetic algorithm procedure. We also obtained the performance of other model observers (Hotelling, Laguerre-Gauss Hotelling, channelized-Hotelling) and human observers to evaluate the validity of the NPWE optimized JPEG 2000 encoder settings. Compared to the default JPEG 2000 encoder settings, the NPWE-optimized encoder settings improved the detection performance of humans and the other three model observers for an SKEV task. In addition, the performance also was improved for a more clinically realistic task where the signal varied from image to image but was not known a priori to observers [signal known statistically (SKS)]. The highest performance improvement for humans was at a high compression ratio (e.g., 30:1) which resulted in approximately a 75% improvement for both the SKEV and SKS tasks.","X-ray detection,
X-ray detectors,
Signal detection,
Transform coding,
Image coding,
Humans,
X-ray imaging,
Matched filters,
Medical signal detection,
Medical services"
Cooperative and group testing in verification of dynamic composite Web services,"Verifying Web services (WS) in a dynamic Service Oriented Architecture (SOA) is challenging because new services can be composed at runtime using existing WS. Furthermore, in a composite service, any component can be dynamically replaced during execution if the component fails. Another challenge is that the testing is time critical because verification must be conducted at runtime and in real time. We compare and contrast traditional software testing and WS testing techniques and propose a WS group testing technique to test composite services. The group testing technique also has the ability to evaluate the test scripts, automatically establish the oracle of the each test script, and identify faulty WS in a failed composite WS.","Web services,
Software testing,
Service oriented architecture,
Performance evaluation,
Runtime,
Automatic testing,
Collaboration,
Programming,
Computer science,
Fault diagnosis"
Using XML technology for the ontology-based semantic integration of life science databases,"Several hundred internet accessible life science databases with constantly growing contents and varying areas of specialization are publicly available via the internet. Database integration, consequently, is a fundamental prerequisite to be able to answer complex biological questions. Due to the presence of syntactic, schematic, and semantic heterogeneities, large scale database integration at present takes considerable efforts. As there is a growing apprehension of extensible markup language (XML) as a means for data exchange in the life sciences, this article focuses on the impact of XML technology on database integration in this area. In detail, a general architecture for ontology-driven data integration based on XML technology is introduced, which overcomes some of the traditional problems in this area. As a proof of concept, a prototypical implementation of this architecture based on a native XML database and an expert system shell is described for the realization of a real world integration scenario.","XML,
Ontologies,
Database languages,
Internet,
Jacobian matrices,
Markup languages,
Navigation,
Data warehouses,
Mirrors,
Data structures"
Cut sets and information flow in networks of two-way channels,"A network of two way channels (TWCs) is specified by a graph having an edge between vertex u and vertex v if there is a TWC between these vertices. A new cut-set bound is determined for such networks when network coding is permitted, and some implications of this bound are discussed.","Intelligent networks,
Network coding,
Electronic mail,
Mathematics,
Computer science,
Transportation,
Communication networks,
Routing"
Document image analysis for World War II personal records,"Complete collections of invaluable documents of unique historical and political significance are decaying and at the same time they are virtually inaccessible, necessitating the invention of robust and efficient methods for their conversion into a searchable electronic form. We present the issues encountered and problems addressed in the MEMORIAL project, whose goal is the establishment of a digital document workbench enabling the creation of distributed virtual archives based on documents existing in libraries, archives, museums, memorials, and public record offices. Successful approaches are described in the context of the chosen data class: a variety of typewritten documents containing personal information relating to the presence of individuals in World War II Nazi concentration camps.",
"Estimating effort by use case points: method, tool and case study","Use case point (UCP) method has been proposed to estimate software development effort in early phase of software project and used in a lot of software organizations. Intuitively, UCP is measured by counting the number of actors and transactions included in use case models. Several tools to support calculating UCP have been developed. However, they only extract actors and use cases and the complexity classification of them are conducted manually. We have been introducing UCP method to software projects in Hitachi Systems & Services, Ltd. To effective introduction of UCP method, we have developed an automatic use case measurement tool, called U-EST. This paper describes the idea to automatically classify the complexity of actors and use cases from use case model. We have also applied the U-EST to actual use case models and examined the difference between the value by the tool and one by the specialist. As the results, UCPs measured by the U-EST are similar to ones by the specialist.",
Probabilistic packet marking with non-preemptive compensation,"A new scheme in probabilistic packet marking (PPM) for IP traceback against denial-of-service attack is presented. Non-preemptive PPM is performed while a marked packet is coming, but compensates the reduction of marking probability in marked-free packets. The nonpreemptive compensation makes the probability of each marked packet arrived at the victim is equal to its original marking probability. This scheme efficiently improves the convergent amount of marked packets required for reconstructing the complete attack path.",
Web searching and information retrieval,"The first Web information services were based on traditional information retrieval (IR) algorithms and techniques. However, IR algorithms were developed for smaller and more coherent collections than the Web is. Thus Web searching requires new techniques - exploiting linkage among Web pages or extensions of the old ones, for example. This article offers an overview of today's search engine architectures and techniques in the context of IR. The authors introduce three such architectures and describe their basic components. Then they discuss the most important feature of each Web search process: page importance and its use in retrieval. Some issues and challenges in Web search engines are also summarized as well as considerations on the future of Web searching in terms of the so-called semantic Web.",
Interface metrics for reusability analysis of components,"Component-based software development relies on reusable components in order to improve quality and flexibility of products as well as increasing development productivity. This paradigm promotes deployment of reusable components as black-box units that can only work and communicate with one another through their well defined interfaces. Here, understandability of component interfaces is considered as a major quality affecting reusability of software components. A set of metrics for measuring properties believed to be relevant to understandability and reusability of software components are presented. Then, their usefulness and relevance are analyzed based upon empirical data gathered from the measurement of a variety of component interfaces. We conclude with some ideas for further research in this area.",
Two- and three-dimensional asteroid impact simulations,"Performing a series of simulations of asteroid impacts using the SAGE code, the authors attempt to estimate the effect of tsunamis and other important environmental events. The code they developed treats multiple fluids with different equations of state and different constitutive models for material strength. The adaptive mesh can refine based on a number of criteria including gradients in physical conditions or material properties. The code uses MPI for portability among many parallel-processing platforms, but hides much of this interface in a library that gives the programmer maximum flexibility. The authors' ocean impact simulations have as a goal the estimation of impact-generated tsunami events as a function of the size and energy of the projectile, partly to aid further studies of potential threats from modest-sized Earth-crossing asteroids. The authors also present a preliminary report on a simulation of the impact that created the Chicxulub crater in Mexico's Yucatan peninsula, widely believed to be responsible for the mass extinctions at the end of the Cretaceous period. They also report on progress in developing better constitutive models for the geological materials involved in this impact and in cratering processes in general.","Discrete event simulation,
Tsunami,
Differential equations,
Refining,
Material properties,
Libraries,
Programming profession,
Oceans,
Projectiles,
Geology"
Modular scheduling of guarded atomic actions,"A modular synthesis flow is essential for a scalable and hierarchical design methodology. This paper considers a particular modular flow where each module has interface methods and the internal behavior of the module is described in terms of a set of guarded atomic actions on the state elements of the module. A module can also read and update the state of other modules but only by invoking the interface methods of those modules. This paper extends the past work on hardware synthesis of a set of guarded atomic actions by Hoe and Arvind to modules of such actions.: It presents an algorithm that, given the scheduling constraints on the interface methods of the called modules, derives the ""glue logic"" and the scheduling constraints for the interface methods of the calling module such that the atomicity of the guarded actions is preserved across module boundaries. Such modules provide reusable 1P which facilitates ""correctness by construction"" design methodology. It also reduces compile-times dramati$ally in comparison to the compilation that flattens all the modules first.","Hardware design languages,
Processor scheduling,
Design methodology,
Permission,
Computer science,
Artificial intelligence,
Laboratories,
Scheduling algorithm,
Logic,
Modular construction"
Small world overlay P2P networks,"This paper considers the problem of how to construct and maintain an overlay structured P2P network based on the small world paradigm. Two main attractive properties of a small world network are (1) low average hop distance between any two randomly chosen nodes, and (2) high clustering coefficient of nodes. Having a low average hop distance implies a low latency for object lookup, while having a high clustering coefficient implies the underlying network can effectively provide object lookup even under heavy demands (for example, in a flash crowd scenario). We present a small world overlay protocol (SWOP) for constructing a small world overlay P2P network. We compare the performance of our system with that of other structured P2P networks such as Chord. We show that the SWOP protocol can achieve improved object lookup performance over the existing protocols. We also exploit the high clustering coefficient of a SWOP network to design an object replication algorithm that can effectively handle heavy object lookup traffic. As a result, a SWOP network can quickly and efficiently deliver popular and dynamic objects to a large number of requesting nodes. To the best of our knowledge, ours is the first piece of work that addresses how to handle dynamic flash crowds in a structured P2P network environment.","Peer to peer computing,
Protocols,
Telecommunication traffic,
Computer science,
Network servers,
Web server,
Routing,
Maintenance engineering,
Intelligent networks,
Delay"
Asymptotic improvement of the Gilbert-Varshamov bound on the size of binary codes,"Given positive integers n and d, let A/sub 2/(n,d) denote the maximum size of a binary code of length n and minimum distance d. The well-known Gilbert-Varshamov bound asserts that A/sub 2/(n,d)/spl ges/2/sup n//V(n,d-l), where V(n,d) = /spl sigma//sub i=0//sup d/(/sub i//sup n/) is the volume of a Hamming sphere of radius d. We show that, in fact, there exists a positive constant c such that A/sub 2/(n, d)/spl ges/c2/sup n//V(n,d-1)log/sub 2/V(n, d-1) whenever d/n/spl les/0.499. The result follows by recasting the Gilbert-Varshamov bound into a graph-theoretic framework and using the fact that the corresponding graph is locally sparse. Generalizations and extensions of this result are briefly discussed.","Binary codes,
Mathematics,
Communication system control,
Statistics,
Computer science,
Terminology,
Context"
A comparative study of single-section polarization-mode dispersion compensators,This paper shows how to use multiple importance sampling to study the performance of polarization-mode dispersion (PMD) compensators with a single differential group delay (DGD) element. We compute the eye opening penalty margin for compensated and uncompensated systems with outage probabilities of 10/sup -5/ or less with a fraction of the computational cost required by standard Monte Carlo methods. This paper shows that the performance of an optimized compensator with a fixed DGD element is comparable to that of a compensator with a variable DGD element. It also shows that the optimal value of the DGD compensator is two to three times larger than the mean DGD of the transmission line averaged over fiber realizations. This technique can be applied to the optimization of any PMD compensator whose dominant sources of residual penalty are both the DGD and the length of the frequency derivative of the polarization-dispersion vector.,
Introducing HIV/AIDS education into the electrical engineering curriculum at the University of Pretoria,"This paper describes how HIV/AIDS education is being introduced into the curriculum of the Department of Electrical, Electronic, and Computer Engineering at the University of Pretoria, Pretoria, South Africa. Third- and fourth-year students were provided with an HIV/AlDS Educational CD developed at the university. Their knowledge of the subject was tested via two quizzes-one written before they were exposed to the material on the CD and one after. In addition, a mathematical HIV/AIDS model is being incorporated into a third-year control systems course. This model is used to illustrate standard control systems engineering concepts, such as linearization, system stability, feedback, and dynamic compensation. This paper is an example of how topical nonengineering material can effectively be made part of a high-level undergraduate engineering course. Students benefit not only from the topical nature of the subject, but also from an improved understanding of control engineering concepts which can be applied to many different fields.","Demography,
Control engineering education,
Electrical engineering education,
Courseware"
Analysis-ready multiwavelets (armlets) for processing scalar-valued signals,"The notion of armlets is introduced in this letter as a precise formulation of orthonormal multiwavelets that guarantee wavelet decomposition with highpass output not being effected by polynomial perturbation of the input. A mathematical scheme for constructing armlets is given, and it is shown that the notions of armlets and balanced multiwavelets are different. In particular, while balanced wavelets are armlets, the converse is false in general. One advantage of armlets is that the weaker assumption provides flexibility to facilitate wavelet and filter construction.","Signal analysis,
Signal processing,
Polynomials,
Matrix decomposition,
Mathematics,
Wavelet analysis,
Computer science,
Statistics,
Digital filters"
An analytic placer for mixed-size placement and timing-driven placement,"We extend the APlace wirelength-driven standard-cell analytic placement framework of A.A. Kennings and I.L. Markov (2002) to address timing-driven and mixed-size (""boulders and dust"") placement. Compared with timing-driven industry tools, evaluated by commercial detailed routing and STA, we achieve an average of 8.4% reduction in cycle time and 7.5% reduction in wirelength for a set of six industry testcases. For mixed-size placement, we achieve an average of 4% wirelength reduction on ISPD02 mixed-size placement benchmarks compared to results of the leading-edge solver, Feng Shui (v2.4) (Khatkhate et al., 2004). We are currently evaluating our placer on industry testcases that combine the challenges of timing constraints, large instance sizes, and embedded blocks (both fixed and unfixed).",
Theoretical study on attribute reduction of rough set theory: comparison of algebra and information views,"Attribute reduction is an important issue of rough set theory and has already been separately studied in algebra view and information view. However, conceptions of attribute reduction based on the two views are not necessarily equivalent, they are the same only in consistent decision systems. In this paper, we theoretically study the quantitative relation between some basic notions of rough set theory like attribute reduction, attribute significance and attribute core defined in the two views. The results show that the relation between those corresponding conceptions in algebra view and information view is typically inclusion rather than equivalence, and its reason is that information view restricts attributes and systems more specifically than algebra view. The results are necessary and significant for the development and application of attribute reduction methods.","Set theory,
Algebra,
Computer science,
Information systems,
Costs,
Information theory,
Entropy,
Cognitive informatics"
Jumping-genes in evolutionary computing,"A new jumping genes paradigm is proposed for evolutionary computing. It is speedy and yet capable of generating some extreme solutions within the Pareto front. This algorithm emulates the genetic phenomenon of horizontal transmission in which the genes can be jumped from one position to another position either within its own or the other chromosome. This paper describes the concept, methodology and implementation of the algorithm that can be aptly used for evolutionary computing. The performance in simulation based on some standard test functions has demonstrated its effectiveness in multiobjective optimization and its results also have been compared with other algorithms currently in use.","Biological cells,
Computational modeling,
Genetic mutations,
Character generation,
DNA,
Genomics,
Bioinformatics,
Computer science,
Testing,
Sequences"
Gender: An Important Factor in End-User Programming Environments?,"A human-centric issue that has not been considered in the design of end-user programming environments is whether gender differences exist that are important to the design of these environments. Ignoring this issue would miss the opportunity of enhancing the effectiveness of end-user programmers by incorporating appropriate mechanisms to support gender-associated differences in decision making, learning, and problem solving. This paper takes a first step toward building a foundation for investigating this issue by surveying gender difference literature from five domains with an eye toward possible implications for end-user programming. We present a taxonomy of this literature, and derive a number of specific issues for each element of the taxonomy (stated as hypotheses). This foundation provides a starting point for organized investigations into issues that may be important for making breakthroughs in the effectiveness of end-user programmers","Programming environments,
Programming profession,
Taxonomy,
Human computer interaction,
Computer science education,
Computer science,
Software design,
Decision making,
Problem-solving,
Buildings"
"Modeling, analysis, and verification of optimal fixturing design","Fixture design is an important manufacturing activity which affects the quality of parts produced. In order to develop a viable computer-aided fixturing tool, the fixture-workpiece system has to be accurately modeled and analyzed. This paper describes the modeling, analysis, and verification of optimal fixturing configurations by the methods of force closure, optimization, and finite-element modeling (FEM). Force closure has been employed to find optimal clamping positions and sequencing, while optimization is used for determining the minimum clamping forces required to balance the cutting forces. The developed FEM is able to determine in detail what are the reaction forces, workpiece displacement, deformation in the workpiece and fixtures. In order to produce a more accurate model for predicting the behavior of the fixture-workpiece system, the developed FEM includes fixture stiffness, which past models have assumed as rigid bodies. The reaction forces on the locators are experimentally verified. A sensor-embedded experimental fixturing setup was developed to verify the modeling and the data was used to compare with the FEM. Note to Practitioners-In practice, the machinist applies excessive force to hold the part on machine table, so that it does not move when machining is carried out. However, while machining flexible parts or clamping on finished surfaces, care must be taken so that only necessary and optimal clamping forces are applied, to avoid deforming the part or damaging the surface. Therefore, in this study, we have developed a mathematical model to characterize the mechanics and conditions for a fixture to hold a given part. The developed mathematical model computes the optimal clamping forces required to hold the part in position, given the cutting conditions. The result obtained was verified by conducting the machining test using the developed sensor-embedded experimental fixture setup. The FEM was also developed to predict the behavior of fixture-workpiece system and the results obtained were encouraging.",
Text alignment with handwritten documents,"Today's digital libraries increasingly include not only printed text but also scanned handwritten pages and other multimedia material. There are, however, few tools available for manipulating handwritten pages. Here, we propose an algorithm based on dynamic time warping (DTW) for a word by word alignment of handwritten documents with their (ASCII) transcripts. We see at least three uses for such alignment algorithms. First, alignment algorithms allow us to produce displays (for example on the Web) that allow a person to easily find their place in the manuscript when reading a transcript. Second, such alignment algorithms allow us to produce large quantities of ground truth data for evaluating handwriting recognition algorithms. Third, such algorithms allow us to produce indices in a straightforward manner for handwriting material. We provide experimental results of our algorithm on a set of 70 pages of historical handwritten material & specifically the writings of George Washington. Our method achieves 74.5% accuracy on line by line alignment and 60.5% accuracy when aligning whole pages at time.","Software libraries,
Handwriting recognition,
Information retrieval,
Image segmentation,
Heuristic algorithms,
Displays,
Computer science,
Writing,
Mice,
Vocabulary"
Impact of haptic warning signal reliability in a time-and-safety-critical task,"The bulk of current haptics human-factors research focuses on mapping basic human perceptual limits. However, many realistic applications demand a better understanding of how to construct more life-like but often less controllable experiment scenarios. In this paper, we study this problem in the context of advanced automobile interfaces. We employ a throttle pedal with programmable force feedback to indicate potentially undesirable situations in the external environment and to gently but steadily guide the driver away from them. We have found evidence that within this scenario, errors in such a warning signal can have a negative effect on the behavior of the driver within the conditions studied. These experiments required a complex protocol and necessarily permitted a variety of participant tactics. Post-experiment analysis revealed that very subtle variations in participant instruction produced large differences in tactics and consequent experiment outcome.","Haptic interfaces,
Force feedback,
Automobiles,
Driver circuits,
Control systems,
Computer science,
Humans,
Application software,
Protocols,
Signal design"
Multiresolution indexing of XML for frequent queries,"XML and other types of semistructured data are typically represented by a labeled directed graph. To speed up path expression queries over the graph, a variety of structural indexes have been proposed. They usually work by partitioning nodes in the data graph into equivalence classes and storing equivalence classes as index nodes. A(k)-index introduces the concept of local bisimilarity for partitioning, allowing the trade-off between index size and query answering power. However, all index nodes in A(k)-index have the same local similarity k, which cannot take advantage of the fact that a workload may contain path expressions of different lengths, or that different parts of the data graph may have different local similarity requirements. To overcome these limitations, we propose M(k)- and M*(k)-indexes. The basic M(k)-index is workload-aware: Like the previously proposed D(k)-index, it allows different index nodes to have different local similarity requirements, providing finer partitioning only for parts of the data graph targeted by longer path expressions. Unlike D(k)-index, M(k)-index is never over-refined for irrelevant index or data nodes. However, the workload-aware feature still incurs overrefinement due to over-qualified parent index nodes. Moreover, fine partitions penalize the performance of short path expressions. To solve these problems, we further propose the M*(k)-index. An M*(k)-index consists of a collection of indexes whose nodes are organized in a partition hierarchy, allowing successively coarser partitioning information to co-exist with the finest partitioning information required. Experiments show that our indexes are superior to previously proposed indexes in terms of index size and query performance.",
Balanced multiwavelets with short filters,"In a recent work, Selesnick constructed two GHM-like scaling function vectors of dimension two that allow balanced multiwavelet decomposition. In this letter, we construct such multiwavelets and give the corresponding decomposition/reconstruction filters. Symmetry and/or antisymmetry are achieved and filterbank shifts are investigated. The fundamental tools for the study of this problem area are presented in the second and third sections for completeness.","Finite impulse response filter,
Polynomials,
Mathematics,
Vectors,
Filter bank,
Equations,
Computer science,
Statistics,
Digital filters,
Nonlinear filters"
Quantum computing with superconductors,"Superconductive technology is one of the most promising approaches to quantum computing because it offers devices with little dissipation, ultrasensitive magnetometers, and electrometers for state readout, large-scale-integration, and a family of classical electronics that could be used for quantum bit (qubit) control. The challenges this technology faces, however, are substantial: for example, control of the qubit to a part in /spl sim/10/sup 4/ must be accomplished with analog control pulses. But even after this is done, the accuracy is limited by the unavoidable decay of quantum information in the system. Recent experiments suggest the time over which this decay occurs is <1 /spl mu/s, though it is expected to lengthen as experimental methods improve. A 1-/spl mu/s decay time would mandate a very difficult to achieve maximum time of /spl sim/100 ps per analog operation. Thus, quantum computing is, simultaneously a promising technology for solving certain very hard problems in computer science and a daunting challenge for those working to develop that technology.","Quantum computing,
Superconductivity,
Pervasive computing,
Nuclear magnetic resonance,
Integrated circuit technology,
Concurrent computing,
Application software,
Computer aided manufacturing,
Paper technology,
Magnetometers"
"Ruling out PTAS for graph min-bisection, densest subgraph and bipartite clique","Assuming that NP /spl nsube//spl cap//sub /spl epsi/> 0/ BPTIME(2/sup n/spl epsi//), we show that graph min-bisection, densest subgraph and bipartite clique have no PTAS. We give a reduction from the minimum distance of code problem (MDC). Starting with an instance of MDC, we build a quasi-random PCP that suffices to prove the desired inapproximability results. In a quasi-random PCP, the query pattern of the verifier looks random in some precise sense. Among the several new techniques introduced, we give a way of certifying that a given polynomial belongs to a given subspace of polynomials. As is important for our purpose, the certificate itself happens to be another polynomial and it can be checked by reading a constant number of its values.","Polynomials,
Artificial intelligence,
Linear code,
Bipartite graph,
Vectors,
Approximation algorithms,
Computer science,
Binary codes"
Characterizing selfishly constructed overlay routing networks,"We analyze the characteristics of overlay routing networks generated by selfish nodes playing competitive network construction games. We explore several networking scenarios - some simplistic, others more realistic - and analyze the resulting Nash equilibrium graphs with respect to topology, performance, and resilience. We find a fundamental tradeoff between performance and resilience, and show that limiting the degree of nodes is of great importance in controlling this balance. Further, by varying the cost function, the game produces widely different topologies; one parameter in particular - the relative cost between maintaining an overlay link and increasing the path length to other nodes - can generate topologies with node-degree distributions whose tails vary from exponential to power-law. We conclude that competitive games can create overlay routing networks satisfying very diverse goals.","Routing,
Network topology,
Resilience,
Cost function,
Joining processes,
Nash equilibrium,
Computer science,
Performance analysis,
Power generation,
Tail"
Smart Phone: an embedded system for universal interactions,"In this paper we present a system architecture that allows users to interact with embedded systems located in their proximity using Smart Phones. We have identified four models of interaction between a Smart Phone and the surrounding environment: universal remote control, dual connectivity, gateway connectivity, and peer-to-peer. Although each of these models has different characteristics, our architecture provides a unique framework for all of the models. Central to our architecture are the hybrid communication capabilities incorporated in the Smart Phones. These phones have the unique feature of incorporating short-range wireless connectivity (e.g., Bluetooth) and Internet connectivity (e.g., GPRS) in the same personal mobile device. This feature together with significant processing power and memory can turn a Smart Phone into the only mobile device that people will carry wherever they go.","Smart phones,
Embedded system,
Computer architecture,
Peer to peer computing,
Bluetooth,
Computer vision,
Pervasive computing,
Ubiquitous computing,
Mobile handsets,
Computer science"
Time delay for arrival of MR contrast agent in collateral-dependent myocardium,"An analysis of the kinetics of myocardial contrast enhancement is an important component of myocardial perfusion studies. The contrast enhancement can be modeled by a linear time-invariant system, and the myocardial impulse response, calculated by deconvolution of the measured tissue response with an arterial input, gives a direct estimate of myocardial blood flow. In this paper, we analyze the effects of delays in the contrast enhancement, that occur in collateral-dependent myocardium, where the tracer reaches the tissue region only through branches from other coronary arteries that form natural bypass vessels. We investigate how the delayed arrival of tracer alters the myocardial impulse response. Model-independent deconvolution is applied to determine the lag between arterial input and tissue enhancement. Experimental data in a porcine model of collateral development indicate that the delayed arrival of an injected tracer, measured at rest, is a useful marker to identify collateral-dependent myocardium, and predict its flow capacitance.","Delay effects,
Myocardium,
Blood flow,
Arteries,
Magnetic resonance imaging,
Radiology,
Deconvolution,
Fluid flow measurement,
Capacitance,
Heart"
Serving radio network controller relocation for UMTS all-IP network,"To support real-time multimedia services in UMTS all-IP network, Third-Generation Partnership Project TR 25.936 proposed two approaches to support real-time serving radio network controller (SRNC) switching, which require packet duplication during SRNC relocation. These approaches significantly consume extra system resources. This paper proposes the fast SRNC relocation (FSR) approach that does not duplicate packets. In FSR, a packet buffering mechanism is implemented to avoid packet loss at the target RNC. We propose an analytic model to investigate the performance of FSR. The numerical results show that packet loss at the source RNC can be ignored. Furthermore, the expected number of packets buffered at the target RNC is small, which does not prolong packet delay.","Radio network,
Radio control,
3G mobile communication,
Delay,
Educational institutions,
Ground penetrating radar,
Multimedia systems,
Random variables,
Computer science,
Electronic mail"
Cultural algorithms: modeling of how cultures learn to solve problems,"Previous work on real-valued function optimization problems had shown that cultural learning emerged as the result of meta-level interaction or swarming of knowledge sources, ""knowledge swarms"" in the belief space. These meta-level swarms induced the swarming of individuals in the population space, ""cultural swarms"". The interaction of these knowledge source produced emergent phases of problem solving that reflected a branch and bound algorithmic process. We apply the approach to a real-world problem in engineering design. We observe the emergence of these same features in a completely different problem environment. We conclude by suggesting the emergent features are what give cultural systems their power to learn and adapt.","Cultural differences,
Problem-solving,
Particle swarm optimization,
Space exploration,
Computer science,
Automotive engineering,
Design engineering,
Power engineering and energy,
Vehicles,
Power system modeling"
Could olfactory displays improve data visualization?,"Smell (olfaction) can be critical to our daily living - for example, smelling smoke from a fire in time to leave a burning building - but it is seldom used in data visualizations or virtual reality (VR) systems. Could olfaction displays (devices that output scented air) augment data visualization, that is, communicate information relevant to many fields? Perhaps incorporating underutilized modalities such as haptics (touch), olfaction, and gustation (taste) as data visualization aids is the next logical step to optimizing human information processing. It seems reasonable that adding the sense of smell to a virtual environment (VE) would enhance the environment's presence or ""realness"". Attempts have been made to include olfactory displays in VEs (for example, John Cater's Deep Immersion Virtual Environment Laboratory at the Southwest Research Institute), but most have been unsuccessful. One reason is the lack of a standard to represent and playback smells. Olfactory effects could play a crucial role in certain training environments, such as those for fire fighters and medical personnel. Current VEs include advanced visual and audio outputs, but smell is either very limited or absent. Very few studies focus on this subject, with most discussing ambient (whole-room and long-duration) rather than specific (localized and short-duration) odors, which are relevant to data visualization and VR.","Olfactory,
Displays,
Data visualization,
Virtual reality,
Fires,
Virtual environment,
Haptic interfaces,
Humans,
Information processing,
Personnel"
"""Population"" approach improves parameter estimation of kinetic models from dynamic PET data","Kinetic modeling is used to indirectly measure physiological parameters from dynamic positron emission tomography (PET) data. Usually, the unknown parameters of the model are estimated, in any given region of interest (ROI), by least squares (LS). However, when the signal-to-noise ratio (SNR) of PET data is too low, LS does not allow reliable parameter estimation. To overcome this problem, we study in this paper the applicability of approaches originally developed in the pharmacokinetic/pharmacodynamic literature and referred to as ""population approaches"". In particular, we consider the iterative two stage (ITS) method, which, given a set of M ROIs drawn on PET images of a given individual, estimates the unknown model parameters of each ROI by exploiting the information contained in all the M ROIs. After having revised the theory behind ITS, we assess its performance versus LS by using Monte Carlo simulations which allow us to evaluate the bias of the two methods in a variety of situations. Then, we compare the performance of LS and ITS in two case studies on [/sup 18/F]FDG kinetics in human skeletal muscle. Both simulated and real case studies results show that a population approach is of potential in modeling PET images since it allows to reliably estimate model parameters also in those ROIs where either a bad SNR or a poor sampling (e.g., infrequent scanning and/or short experiment duration) make the use of LS unsuccessful.","Parameter estimation,
Kinetic theory,
Positron emission tomography,
Muscles,
Signal to noise ratio,
Humans,
Sugar,
Biochemistry,
Image analysis,
Least squares approximation"
Propagating trust in ad-hoc networks for reliable routing,"Ad-hoc networks emerge when a number of mobile wireless nodes agree to mutually cooperate with each other in order to establish communication over a wide region. All nodes in the network execute a pre-agreed routing protocol to pass packets for other nodes. The dynamic source routing (DSR) protocol is one of the unique routing protocols for ad-hoc wireless networks, where each transmitted data packet contains the complete route that it has to traverse. This is due to the fact that during route discovery, all intermediate nodes contribute faithfully to the route generation process. Such an altruistic behaviour is though difficult to realize in realistic environments and so a number of malicious nodes may also participate in the route discovery process only to sabotage the network. In this paper we present a unique mechanism for establishing trusted routes in a DSR based ad-hoc network. The proposed mechanism is specifically designed for improvised networks where the establishment of a fixed or mobile trust infrastructure is considered impractical.",
Several properties of short LDPC codes,"In this paper, we present several properties on minimum distance(d/sub min/) and girth(G/sub min/) in Tanner graphs for low-density parity-check (LDPC) codes with small left degrees. We show that the distance growth of (2, 4) LDPC codes is too slow to achieve the desired performance. We further give a tight upper bound on the maximum possible girth. The numerical results show that codes with large G/sub min/ could outperform the average performance of regular ensembles of the LDPC codes over binary symmetric channels. The same codes perform about 1.5 dB away from the sphere-packing bound on additive white Gaussian noise channels.","Parity check codes,
Iterative decoding,
Turbo codes,
Convolutional codes,
Upper bound,
Additive white noise,
NASA,
Block codes,
Communications Society,
Computer science"
The performance impact of I/O optimizations and disk improvements,"In this paper, we use real server and personal computer workloads to systematically analyze the true performance impact of various I/O optimization techniques, including read caching, sequential prefetching, opportunistic prefetching, write buffering, request scheduling, striping, and short-stroking. We also break down disk technology improvement into four basic effects—faster seeks, higher RPM, linear density improvement, and increase in track density—and analyze each separately to determine its actual benefit. In addition, we examine the historical rates of improvement and use the trends to project the effect of disk technology scaling. As part of this study, we develop a methodology for replaying real workloads that more accurately models I/O arrivals and that allows the I/O rate to be more realistically scaled than previously. We find that optimization techniques that reduce the number of physical I/Os are generally more effective than those that improve the efficiency in performing the I/Os. Sequential prefetching and write buffering are particularly effective, reducing the average read and write response time by about 50% and 90%, respectively. Our results suggest that a reliable method for improving performance is to use larger caches up to and even beyond 1% of the storage used. For a given workload, our analysis shows that disk technology improvement at the historical rate increases performance by about 8% per year if the disk occupancy rate is kept constant, and by about 15% per year if the same number of disks are used. We discover that the actual average seek time and rotational latency are, respectively, only about 35% and 60% of the specified values. We also observe that the disk head positioning time far dominates the data transfer time, suggesting that to effectively utilize the available disk bandwidth, data should be reorganized such that accesses become more sequential.",
Research on particle swarm optimization: a review,"Particle swarm optimization (PSO) explores global optimal solution through exploiting the particle's memory and the swarm's memory. Its properties of low constraint on the continuity of objective function and joint of search space, and ability of adapting to dynamic environment make PSO become one of the most important swarm intelligence methods and evolutionary computation algorithms. The fundamental and standard algorithm is introduced firstly. Then the work on the algorithm improvement during the past years is surveyed, as well as the applications on the multi-objective optimization, neural networks and electronics, etc. Finally, the problems remaining unresolved and some directions of PSO research are discussed.",
Affective computing in tele-home health,"This study exemplifies the integration of IS behavioral science in the area of technology adoption and diffusion into the design science process. We first identify the computer-mediated paradox, as it exists in the tele-home health care setting. Specifically, we address the challenges of providing quality patient inclusive of affective assessment. From the design science perspective, we then introduce an intelligent interface (MOUE) aimed at discerning emotional state from processing sensory modalities (or modes) input via various media and building (or encoding) a model of the user's emotions. We contextualize MOUE within the tele-home health care setting to provide the health care provider with an easy-to-use and useful assessment of the patient's emotional state in order to facilitate patient care. We then use an IS adoption model developed and tested in the general telemedicine context in a qualitative exploratory manner as a means to inform design science regarding adapting affective state output in consideration of tele-home health adoption and diffusion issues. Based upon this integrative exploration, we propose to expand the application of ""Wizard of Oz"" type studies by Dahlback, N., et al., (1998) to computer-mediated communication (CMC) environments to investigate how emotional state assessments influence responses from health care professionals and how MOUE can be accepted into the health care environment.","Medical services,
Behavioral science,
Process design,
Intelligent sensors,
Intelligent structures,
Buildings,
Encoding,
Context modeling,
Testing,
Telemedicine"
Emergence: a paradigm for robust and scalable distributed applications,"Natural distributed systems are adaptive, scalable and fault-tolerant. Emergence science describes how higher-level self-regulatory behaviour arises in natural systems from many participants following simple rule-sets. Emergence advocates simple communication models, autonomy and independence, enhancing robustness and self-stabilization. High-quality distributed applications such as autonomic systems must satisfy the appropriate nonfunctional requirements which include scalability, efficiency, robustness, low-latency and stability. However the traditional design of distributed applications, especially in terms of the communication strategies employed, can introduce compromises between these characteristics. This paper discusses ways in which emergence science can be applied to distributed computing, avoiding some of the compromises associated with traditionally-designed applications. To demonstrate the effectiveness of this paradigm, an emergent election algorithm is described and its performance evaluated. The design incorporates nondeterministic behaviour. The resulting algorithm has very low communication complexity, and is simultaneously very stable, scalable and robust.","Robustness,
Distributed computing,
Robust stability,
Application software,
Scalability,
Nominations and elections,
Fault tolerant systems,
Algorithm design and analysis,
Computer science,
Adaptive systems"
Fetal heart rate extraction from composite maternal ECG using complex continuous wavelet transform,"Fetal heart rate extraction from the abdominal ECG is of great importance due to the information that carries in assessing appropriately the fetus well-being during pregnancy. In this work a novel automated method is presented for the detection of the QRS complexes of the fetus cardiac activity using multichannel maternal ECG recordings. No accessory preprocessing step for noise filtering is required. The method is based on the complex continuous wavelet transform and modulus maxima theory. The proposed method was validated using real signals, recorded at different weeks of gestation, covering most of the pregnancy period. The system performs well, since almost all fetal beats are detected (accuracy: 99.5%).",
A succinct physical storage scheme for efficient evaluation of path queries in XML,"Path expressions are ubiquitous in XML processing languages. Existing approaches evaluate a path expression by selecting nodes that satisfies the tag-name and value constraints and then joining them according to the structural constraints. We propose a novel approach, next-of-kin (NoK) pattern matching, to speed up the node-selection step, and to reduce the join size significantly in the second step. To efficiently perform NoK pattern matching, we also propose a succinct XML physical storage scheme that is adaptive to updates and streaming XML as well. Our performance results demonstrate that the proposed storage scheme and path evaluation algorithm is highly efficient and outperforms the other tested systems in most cases.","XML,
Navigation,
Computer science,
Pattern matching,
System testing,
Tree data structures,
Labeling,
Relational databases,
Encoding,
Data engineering"
P2P streaming using multiple description coded video,"Today's peer-to-peer applications benefit from the fact that many users offer their resources (mostly inform of files). Those resources are mainly connected via relatively low-bandwidth, asymmetric access networks (such as ADSL or cable modems), which make it hard to realize the streaming of video data. Thus, audio visual content is usually downloaded and not streamed in today's peer-to-peer (P2P) systems. In order to provide streaming support it is necessary to take into account the asymmetric character of the up-load and download links. We show that by making use of multiple description coded (MDC) video and the fact that single descriptions can be sent from different peers, streaming in peer-to-peer applications is feasible. The paper discusses the different issues related to this topic. It explains MDC and compares it to hierarchically layered encoded video (HLEV). Further, the conditions under which MDC can be used for P2P streaming are discussed and it is shown how it can be deployed in a P2P environment.",
Efficient graphical models for processing images,"Graphical models are powerful tools for processing images. However, the large dimensionality of even local image data poses a difficulty. Representing the range of possible graphical model node variables with discrete states leads to an overwhelmingly large number of states for the model, often making both exact and approximate inference computationally intractable. We propose a representation that allows a small number of discrete states to represent the large number of possible image values at each pixel or local image patch. Each node in the graph represents the best regression function, chosen from a set of candidate functions, for estimating the unobserved image pixels from the observed samples. This permits a small number of discrete states to summarize the range of possible image values at each point in the image. Belief propagation is then used to find the best regressor to use at each point. To demonstrate the usefulness of this technique, we apply it to two problems: super-resolution and color demosaicing. In both cases, we find our method compares well against other techniques for these problems.","Graphical models,
Pixel,
Belief propagation,
Statistics,
Computer science,
Artificial intelligence,
Laboratories,
Layout,
Gray-scale,
Inference algorithms"
Distinguishing text from graphics in on-line handwritten ink,"We present a system that separates text from graphics strokes in handwritten digital ink. It utilizes not just the characteristics of the strokes, but also the information provided by the gaps between the strokes, as well as the temporal characteristics of the stroke sequence. It is built using machine learning techniques that infer the internal parameters of the system from real digital ink, collected using a tablet PC.",
A comprehensive study on backup reprovisioning to remedy the effect of multiple-link failures in WDM mesh networks,"As networks grow in size and complexity, both the probability and the impact of failures increase. The preallocated backup bandwidth cannot provide 100% protection guarantee when multiple failures occur in a network. In this study, we consider multiple concurrent failures where concurrent means that a failure occurs before the previous failure is physically repaired, and we present a comprehensive study on backup reprovisioning to combat the effect of multiple-link failures. The basic idea is to reprovision new backups for connections that become unprotected or vulnerable for the next possible failure, due to losing the primary or the backup in the first failure or due to backup resource sharing. The pros and cons of the backup-reprovisioning approach are extensively discussed. A generalized network model which can maximally explore the backup-sharing potential is assumed in this study. We then discuss the complexity of backup reprovisioning under such a network model. A reprovisioning algorithm is proposed which can significantly reduce the connection vulnerability without the knowledge of the location of next failure. The effectiveness of our reprovisioning algorithm is demonstrated through numerical examples.",
The effect of trust assumptions on the elaboration of security requirements,"Assumptions are frequently made during requirements analysis of a system-to-be about the trustworthiness of its various components (including human components). These trust assumptions can affect the scope of the analysis, derivation of security requirements, and in some cases, how functionality is realized. This work presents trust assumptions in the context of analysis of security requirements. A running example shows how trust assumptions can be used by a requirements engineer to help define and limit the scope of analysis and to document the decisions made during the process. The paper concludes with a case study examining the impact of trust assumptions on software that uses the secure electronic transaction (SET) specification.",
High-resolution functional vascular assessment with ultrasound,"In order to improve the resolution of contrast-assisted imaging systems, we have created a high-frequency destruction/contrast replenishment imaging system with a spatial resolution of 160 /spl mu/m /spl times/ 160 /spl mu/m. The system utilizes a 1-MHz cylindrically focused transducer for destruction and a 25-MHz spherically focused transducer for pulse/echo imaging. Speckle tracking and a clutter filter are applied across frames to remove the challenging physiologic motion artifacts that are obtained when imaging with a mechanically scanned transducer. Using a new estimation technique, flow constants proportional to absolute flow rate were estimated from B-mode time-intensity curves (TICs). The in vitro results indicate a correlation between the actual flow velocity and the estimated rate constant. In vivo images are presented showing blood perfusion in the ciliary processes and iris of the rabbit eye. The regions of interest (ROIs) from the ciliary processes yielded slower perfusion compared with the iris, as expected from vascular casts of the microcirculation in this region. Potential applications of this system include high-resolution perfusion assessment in small animals.",
Lazy database replication with ordering guarantees,"Lazy replication is a popular technique for improving the performance and availability of database systems. Although there are concurrency control techniques, which guarantee serializability in lazy replication systems, these techniques result in undesirable transaction orderings. Since transactions may see stale data, they may be serialized in an order different from the one in which they were submitted. Strong serializability avoids such problems, but it is very costly to implement. We propose a generalized form of strong serializability that is suitable for use with lazy replication. In addition to having many of the advantages of strong serializability, it can be implemented more efficiently. We show how generalized strong serializability can be implemented in a lazy replication system, and we present the results of a simulation study that quantifies the strengths and limitations of the approach.","Database systems,
Concurrency control,
Availability,
Transaction databases,
Computer science,
Computational modeling,
Distributed computing,
Books,
Web services,
Data engineering"
Information retrieval techniques for peer-to-peer networks,"Peer-to-peer (P2P) systems are application-layer networks that let networked hosts share resources in a distributed environment. An important attribute of P2P networks is the ability to efficiently search the contents of other peers. In this article, the authors survey existing search techniques for information retrieval (IR) in P2P networks, including recent techniques that they propose. They also present a realistic experimental evaluation and comparison of these techniques, using a distributed middleware infrastructure they designed and implemented.",
Efficient event routing in content-based publish-subscribe service networks,"Efficient event delivery in a content-based publish/subscribe system has been a challenging problem. Existing group communication solutions, such as IP multicast or application-level multicast techniques, are not readily applicable due to the highly heterogeneous communication pattern in such systems. We first explore the design space of event routing strategies for content-based publish/subscribe systems. Two major existing approaches are studied: filter-hosed approach, which performs content-based filtering on intermediate routing servers to dynamically guide routing decisions, and multicast-based approach, which delivers events through a few high-quality multicast groups that are pre-constructed to approximately match user interests. These approaches have different trade-offs in the routing quality achieved and the implementation cost and system load generated. We then present a new routing scheme called Kyra that carefully balance these trade-offs. Kyra combines the advantages of content-based filtering and event-space partitioning in the existing approaches to achieve better overall routing efficiency. We use detailed simulations to evaluate Kyra and compare it with existing approaches. The results demonstrate the effectiveness of Kyra in achieving high network efficiency, reducing implementation cost and balancing system load across the publish-subscribe service network.","Routing,
Intelligent networks,
Publish-subscribe,
Subscriptions,
Filtering,
Computer science,
Space exploration,
Costs,
Network servers,
Matched filters"
Direct verbal communication as a catalyst of agile knowledge sharing,This paper discusses the role of conversation and social interactions as the key element of effective knowledge sharing in an agile process. It also presents the observations made during a repeated experiment on knowledge sharing conducted in various groups of professionals and students. The study suggests that the focus on the pure codified approach is the critical reason of Tayloristic team failure to effectively share knowledge among all stakeholders of a software project. Drawing on the knowledge-as-relationship perspective of knowledge sharing we theorize that verbal face-to-face interaction facilitates achieving higher velocity by software development teams.,"Documentation,
Programming,
Knowledge transfer,
IEC standards,
ISO standards,
Computer science,
Knowledge management,
Production facilities,
Sociology,
Software engineering"
Concurrent constant modulus algorithm and soft decision directed scheme for fractionally-spaced blind equalization,"The paper proposes a concurrent constant modulus algorithm (CMA) and soft decision-directed (SDD) scheme for low-complexity blind equalization of high-order quadrature amplitude modulation channels. Simulation using a fractionally spaced equalization setting is used to compare the proposed scheme with the recently introduced state-of-art concurrent CMA and decision-directed (DD) scheme. The proposed CMA+SDD blind equalizer is shown to have simpler computational complexity per weight update, faster convergence speed, and slightly improved steady-state equalization performance, compared with the CMA+DD blind equalizer.",
Generic distributed assembly and repair algorithms for self-reconfiguring robots,"In this paper we present generic distributed algorithms for assembling and repairing shapes using modular self-reconfiguring robots. The algorithms work in the sliding cube model. Each module independently evaluates a set of local rules using different evaluation models. Two methods are used to determine the correctness of the algorithms - a graph analysis technique which can prove the rule set is correct for specific instances of the algorithm, and a statistical technique which can produce arbitrary bounds on the likelihood that the rule set functions correctly. An extension of the assembly algorithm can be used to produce arbitrary non-cantilevered convex shapes without holes. The algorithms have been implemented and evaluated in simulation.","Robotic assembly,
Algorithm design and analysis,
Hardware,
Intelligent robots,
Laboratories,
Distributed control,
Shape control,
Computer science,
Artificial intelligence,
Educational institutions"
Inferring and visualizing social networks on Internet relay chat,"Internet relay chat is a system that allows groups of people to collaborate and chat from anywhere in the world. Clearly defined by several RFC documents, it is arguably the most standard real-time chat system currently in use. This work describes a method of inferring the social network of a group of IRC users in a channel. An IRC bot is used to monitor a channel and perform a heuristic analysis of events to create a mathematical approximation of the social network. From this, the bot can produce a visualization of the inferred social network on demand. These visualizations reveal the structure of the social network, highlighting connectivity, clustering and strengths of relationships between users. Animated output allows viewers to see the evolution of the social network over time. Some novel ideas for future work are discussed, showing other useful applications of this system.","Internet,
Visualization,
Technology social factors,
Animation"
Scheduling multicast traffic in internally buffered crossbar switches,"Scheduling multicast traffic has been an active research topic due to the tremendous growth of multicast traffic (audio, video, teleconferencing, etc.) on the Internet. Considerable research work has been done on input queued (IQ) switches to handle the multicast traffic. Unfortunately, all the proposed solutions were of no practical value because they either lack performance or were simply not practical. Internally buffered crossbar (IBC) switches, on the other hand, have been considered as a robust alternative to buffer-less crossbar switches to improve the switching performance. However, no work has ever been done on multicasting in IBC switches. In this paper, we fill this gap and study, for the first time, the multicasting problem in IBC switches. In particular, we propose a simple scheduling scheme named multicast cross-points round robin (MXRR) for the IBC switch architecture. Our scheme was shown to handle multicast traffic more efficiently and far better than all previous schemes. Yet, MXRR is both practical and achieves high performance.","Switches,
Traffic control,
Internet,
Fabrics,
Costs,
Unicast,
Computer science,
Electronic mail,
Throughput,
Multicast algorithms"
Trust-based routing for ad-hoc wireless networks,"An ad-hoc wireless network is formed by a number of mobile nodes having limited communication range. The dynamic source routing (DSR) protocol is frequently used to extend the effective range of these nodes through mutual cooperation. The accurate execution of the DSR protocol demands sustained benevolent behaviour by all participating nodes. However, as ad-hoc networks are created in improvised environments, the realization of such altruistic behaviour is virtually impossible to achieve. In fixed networks, trust infrastructures like certification authorities and key distribution centres are generally used to provide default trust relationships. However, the creation of such an entity in an ad-hoc network is considered neither feasible nor pragmatic. In this paper, we propose a novel mechanism for establishing trust based routing in ad-hoc networks without necessitating a trust infrastructure. We accentuate, that the proposed mechanism is most suitable for ad-hoc networks that can be created on the fly without making any suppositions or imposing pre-configuration requirements.",
Assume-guarantee verification of source code with design-level assumptions,"Model checking is an automated technique that can be used to determine whether a system satisfies certain required properties. To address the ""state explosion"" problem associated with this technique, we propose to integrate assume-guarantee verification at different phases of system development. During design, developers build abstract behavioral models of the system components and use them to establish key properties of the system. To increase the scalability of model checking at this level, we have previously developed techniques that automatically decompose the verification task by generating component assumptions for the properties to hold. The design artifacts are subsequently used to guide the implementation of the system, but also to enable more efficient reasoning of the source code. In particular, we propose to use assumptions generated for the design to similarly decompose the verification of the actual system implementation. We demonstrate our approach on a significant NASA application, where design models were used to identify and correct a safety property violation, and the generated assumptions allowed us to check successfully that the property was preserved by the implementation.",
Debugging sequential circuits using Boolean satisfiability,"Logic debugging of today's complex sequential circuits is an important problem. In this paper, a logic debugging methodology for multiple errors in sequential circuits with no state equivalence is developed. The proposed approach reduces the problem of debugging to an instance of Boolean satisfiability. This formulation takes advantage of modern Boolean satisfiability solvers that handle large circuits in a computationally efficient manner. An extensive suite of experiments with large sequential circuits confirm the robustness and efficiency of the proposed approach. The results further suggest that Boolean satisfiability provides an effective platform for sequential logic debugging.","Debugging,
Sequential circuits,
Very large scale integration,
Fault diagnosis,
Robustness,
Logic design,
Computer science,
Boolean functions,
Computer bugs,
Design automation"
WISE: energy-efficient interface selection on vertical handoff between 3G networks and WLANs,"The integration of 3G networks and WLAN as complementary has begun to attract much attention in industry as well as academia. This topic is becoming a burning issue, and one of the key questions which it raises is how to support a seamless vertical handoff. This paper introduces a new interface selection algorithm for energy-efficient vertical handoff in tightly coupled systems capable of supporting seamless handoff. Our proposed scheme, Wise Interface Selection (WISE) switches the active network interface, after taking into consideration the characteristics of the network interface cards and the current level of data traffic, with the cooperation of the mobile terminals and network. Interface switching operates independently on both the downlink and the uplink for the purpose of energy conservation. We show through simulation that less energy is consumed with WISE than when only a 3G network or WLAN interface is used, resulting in a longer lifetime for the mobile terminals. In the case of TCP connections, additional throughput gain can also be obtained.","Energy efficiency,
Wireless LAN,
Telecommunication traffic,
Network interfaces,
Web and internet services,
Bandwidth,
Computer science,
Power engineering and energy,
Computer industry,
Switches"
Bayesian fusion of camera metadata cues in semantic scene classification,"Semantic scene classification based only on low-level vision cues has had limited success on unconstrained image sets. On the other hand, camera metadata related to capture conditions provides cues independent of the captured scene content that can be used to improve classification performance. We consider two problems: indoor-outdoor classification and sunset detection. Analysis of camera metadata statistics for images of each class revealed that metadata fields, such as exposure time, flash fired, and subject distance, is most discriminative for both indoor-outdoor and sunset classification. A Bayesian network is employed to fuse content-based and metadata cues in the probability domain and degrades gracefully, even when specific metadata inputs are missing (a practical concern). Finally, we provide extensive experimental results on the two problems, using content-based and metadata cues to demonstrate the efficacy of the proposed integrated scene classification scheme.",
The existence of finite abstractions for branching time model checking,"Abstraction is often essential to verify a program with model checking. Typically, a concrete source program with an infinite (or finite, but large) state space is reduced to a small, finite state, abstract program on which a correctness property can be checked. The fundamental question we investigate in this paper is whether such a reduction to finite state programs is always possible, for arbitrary branching time temporal properties. We begin by showing that existing abstraction frameworks are inherently incomplete for verifying purely existential or mixed universal-existential properties. We then propose a new, complete abstraction framework which is based on a class of focused transition systems (FTS's). The key new feature in FTS's is a way of ""focusing"" an abstract state to a set of more precise abstract states. While focus operators have been defined for specific contexts, this result shows their fundamental usefulness for proving non-universal properties. The constructive completeness proof provides linear size maximal models for properties expressed in logics such as CTL and the mu-calculus. This substantially improves upon known (worst-case) exponential size constructions for their universal fragments.",
On the streaming model augmented with a sorting primitive,"The need to deal with massive data sets in many practical applications has led to a growing interest in computational models appropriate for large inputs. The most important quality of a realistic model is that it can be efficiently implemented across a wide range of platforms and operating systems. In this paper, we study the computational model that results if the streaming model is augmented with a sorting primitive. We argue that this model is highly practical, and that a wide range of important problems can be efficiently solved in this (relatively weak) model. Examples are undirected connectivity, minimum spanning trees, and red-blue line segment intersection, among others. This suggests that using more powerful, harder to implement models may not always be justified. Our main technical contribution is to show a hardness result for the ""streaming and sorting"" model, which demonstrates that the main limitation of this model is that it can only access one data stream at a time. Since our model is strong enough to solve ""pointer chasing"" problems, the communication complexity based techniques commonly used in showing lower bounds for the streaming model cannot be adapted to our model. We therefore have to employ techniques to obtain these results. Finally, we compare our model to a popular restriction of external memory algorithms that access their data mostly sequentially.",
An approach to facilitate reliability testing of Web services components,"The paradigm of Web services that transforms the Internet from a repository of data into a repository of services has been gathering significant momentum in both academia and industry in recent years. However, as more and more Web services are published on the Internet, how to choose the most appropriate Web service components from the sea of ever-changing, unpredictable, and largely uncontrollable Web services poses a big challenge. we propose a mobile agent-based approach that selects reliable Web service components in a cost-effective manner.","Testing,
Web services,
Web and internet services,
Software maintenance,
NIST,
Application software,
Collaborative software,
Computer science,
Access protocols,
Software standards"
A self-tuning cache architecture for embedded systems,"Memory access can account for about half of a microprocessor system's power consumption. Customizing a microprocessor cache's total size, line size and associativity to a particular program is well known to have tremendous benefits for performance and power. Customizing caches has until recently been restricted to core-based flows, in which a new chip will be fabricated. However, several configurable cache architectures have been proposed recently for use in pre-fabricated microprocessor platforms. Tuning those caches to a program is still however a cumbersome task left for designers, assisted in part by recent computer-aided design (CAD) tuning aids. We propose to move that CAD on-chip, which can greatly increase the acceptance of configurable caches. We introduce on-chip hardware implementing an efficient cache tuning heuristic that can automatically, transparently, and dynamically tune the cache to an executing program. We carefully designed the heuristic to avoid any cache flushing, since flushing is power and performance costly. By simulating numerous Powerstone and MediaBench benchmarks, we show that such a dynamic self-tuning cache can reduce memory-access energy by 45% to 55% on average, and as much as 97%, compared with a four-way set-associative base cache, completely transparently to the programmer.",
"Applying evolutionary algorithms to problems with noisy, time-consuming fitness functions","For many ""real world"" applications of evolutionary computation, the fitness function is obscured by random noise. This interferes with the evaluation and selection process and adversely affects the performance of the algorithm. We present a study of noise compensation techniques designed to better counteract the negative effects of noise. We introduce algorithms that vary the number of samples used per candidate based on the amount of noise present at that point in the search space. Results show that these algorithms are significantly better than the traditional technique used by the optimisation community and that noise compensation is indeed a difficult task that warrants further investigation.",
A new method in locating and segmenting palmprint into region-of-interest,"Various techniques in analyzing palmprint have been proposed but to the best of our knowledge, none has been studied on the selection and division of the region-of-interest (ROI). Previous methods were always applied only to a fixed size square region chosen as the central part of the palm, which were then divided into square blocks for extraction of local features. We proposed a new method in locating and segmenting the ROI for palmprint analysis, where the selected region varies with the size of the palm. Instead of square blocks, the region is divided into sectors of elliptical half-rings, which are less affected by misalignment due to rotational error. More importantly, our arrangement of the feature vectors ensures that only features extracted from the same spatial region of two aligned palms will be compared with each other. Encouraging results obtained favor the use of this method in the future development of palmprint analysis techniques.",
Incentive mechanisms for large collaborative resource sharing,"We study the nature of sharing resources in distributed collaborations such as Grids and peer-to-peer systems. By applying the theoretical framework of the multi-person prisoner's dilemma to this resource sharing problem, we show that in the absence of incentive schemes, individual users are apt to hold back resources, leading to decreased system utility. Using both the theoretical framework as well as simulations, we compare and contrast three different incentive schemes aimed at encouraging users to contribute resources. Our results show that soft-incentive schemes are effective in incentivizing autonomous entities to collaborate, leading to increased gains for all participants in the system.",
Automated cluster-based Web service performance tuning,"Active harmony provides a way to automate performance tuning. We apply the Active Harmony system to improve the performance of a cluster-based web service system. The performance improvement cannot easily be achieved by tuning individual components for such a system. The experimental results show that there is no single configuration for the system that performs well for all kinds of workloads. By tuning the parameters, Active Harmony helps the system adapt to different workloads and improve the performance up to 16%. For scalability, we demonstrate how to reduce the time when tuning a large system with many tunable parameters. Finally an algorithm is proposed to automatically adjust the structure of cluster-based web systems, and the system throughput is improved up to 70% using this technique.","Web services,
Web server,
Transaction databases,
Middleware,
Computer science,
Educational institutions,
Scalability,
Clustering algorithms,
Throughput,
Business"
Towards a service-oriented ad hoc grid,"In its current state, service-oriented grid computing focuses on the unification of resources through virtualization, to enable on demand distributed computing within a preconfigured environment. Organizations or inter-organizational communities willing to share their computational resources typically create a centrally planned grid, where dedicated grid administrators manage the nodes and the offered grid services. In this paper, we present the idea of a spontaneously formed, service-oriented ad hoc grid to harness the unused resources of idle networked workstations and high-performance computing nodes on demand. We discuss the requirements of such an ad hoc grid, show how the service-oriented computing paradigm can be used to realize it and present a proof-of-concept implementation based on the Globus Toolkit 3.0. The features of this system are peer-to-peer based node discovery, automatic node property assessment, hot deployment of services into a running system and added inter-service security.",
Extractor codes,"We study error-correcting codes for highly noisy channels. For example, every received signal in the channel may originate from some half of the symbols in the alphabet. Our main conceptual contribution is an equivalence between error-correcting codes for such channels and extractors. Our main technical contribution is a new explicit error-correcting code based on Trevisan's extractor that can handle such channels, and even noisier ones. Our new code has polynomial-time encoding and polynomial-time soft-decision decoding. We note that Reed-Solomon codes cannot handle such channels, and our study exposes some limitations on list decoding of Reed-Solomon codes. Another advantage of our equivalence is that when the Johnson bound is restated in terms of extractors, it becomes the well-known Leftover Hash Lemma. This yields a new proof of the Johnson bound which applies to large alphabets and soft decoding. Our explicit codes are useful in several applications. First, they yield algorithms to extract many hardcore bits using few auxiliary random bits. Second, they are the key tool in a recent scheme to compactly store a set of elements in a way that membership in the set can be determined by looking at only one bit of the representation. Finally, they are the basis for the recent construction of high-noise, almost-optimal rate list-decodable codes over large alphabets.","Decoding,
Error correction codes,
Computer science,
Computer errors,
History"
Architecture-aware adaptive clustering of OO systems,"The recovery of software architecture is a first important step towards re-engineering a software system. Architecture recovery usually involves clustering. The problem with current clustering techniques is that they decide exclusively based on syntactic dependencies instead of looking at higher-level semantic information. As a result, the recovered architecture is not always meaningful to a human software engineer. We propose an approach that combines clustering with pattern-matching techniques to recover meaningful decompositions. Pattern-matching is used to identify architectural clues-small structural patterns that provide semantic information to allow for a rating of the dependencies found between a system's entities. These clues are used to compute an adaptive interclass similarity measure which is then used by a clustering algorithm to produce the final system decomposition.",
Games as a motivation for freshman students learn programming,"Programming is a difficult skill to acquire. It is best learned by practice and, if students are to learn effectively, at least some of this practice have to be self-directed. Instructor's key role is to persuade our students to do this and thus to motivate them. In the past, our students identified programming as a vital skill in demand by industry. Consequently they were motivated to acquire a useful skill that would be relevant in some future job or lucrative career. Nowadays, our WEB age students have no idea why they have to study programming. Programming courses are seen simply as mandatory parts of the degree course to be negotiated. Their world of computing is multithreaded computer programs with impressive human interfaces for games and WEB. They cannot relate them with the classical programming exercises that ask for single-threaded programs performing a sequence of calculations. Given this, we decided to introduce our students in problem solving using what they view as real-world problems such as games and WEB-programming.","Programming profession,
Educational institutions,
Engineering profession,
Computer interfaces,
Employee welfare,
Education,
Software engineering,
Humans,
Problem-solving,
Physics"
Analyze the worm-based attack in large scale P2P networks,"Peer-to-peer (P2P) computing has become an interesting research topic during recent years. In this paper, we address issue related to analyzing the worm-based attack in P2P systems. Particularly, our technologies include: 1) generic mathematical models for attacker/defender and different P2P systems; 2) practical and effective attack prevention schemes. We find that our proposed defense strategy can efficiently improve the performance of worm detection and system recovery.","Intelligent networks,
Large-scale systems,
Computer worms,
Mathematical model,
Biological system modeling,
Peer to peer computing,
Internet,
Computer science,
System recovery,
Degradation"
Language identification using parallel syllable-like unit recognition,"Automatic spoken language identification (LID) is the task of identifying the language from a short utterance of the speech signal. The most successful approach to LID uses phone recognizers of several languages in parallel. The basic requirement to build a parallel phone recognition (PPR) system is annotated corpora. A novel approach is proposed for the LID task which uses parallel syllable-like unit recognizers, in a framework similar to the PPR approach in the literature. The difference is that unsupervised syllable models are built from the training data. The data is first segmented into syllable-like units. The syllable segments are then clustered using an incremental approach. This results in a set of syllable models for each language. Our initial results on the OGI MLTS corpora show that the performance is 69.5%. We further show that if only a subset of syllable models that are unique (in some sense), are considered, the performance improves to 75.9%.","Natural languages,
Signal processing,
Automatic speech recognition,
Computer science,
Training data,
Humans,
Information resources,
Frequency,
Testing,
Performance analysis"
Proxy-based sensor deployment for mobile sensor networks,"To provide satisfactory coverage is very important in many sensor network applications such as military surveillance. In order to obtain the required coverage in harsh environments, mobile sensors are helpful since they can move to cover the area not reachable by static sensors. Previous work on mobile sensor deployment is based on a round by round process, where sensors move iteratively until the maximum coverage is reached. Although these solutions can deploy mobile sensors in a distributed way, the mobile sensors may move in a zig-zag way and waste a lot of energy compared to moving directly to the final location. To address this problem, we propose a proxy-based sensor deployment protocol. Instead of moving iteratively, sensors calculate their target locations based on a distributed iterative algorithm, move logically, and exchange new logical locations with their new logical neighbors. Actual movement only occurs when sensors determine their final locations. Simulation results show that the proposed protocol can significantly reduce the energy consumption compared to previous work, while maintaining similar coverage.","Intelligent sensors,
Sensor phenomena and characterization,
Protocols,
Costs,
Computer science,
Application software,
Military computing,
Iterative algorithms,
Energy consumption,
Wireless communication"
An architecture for key management in hierarchical mobile ad-hoc networks,"In recent years, mobile ad-hoc networks have received a great deal of attention in both academia and industry to provide anytime-anywhere networking services. As wireless networks are rapidly deployed, the security of wireless environment will be mandatory. In this paper, we describe a group key management architecture and key agreement protocols for secure communication in mobile ad-hoc wireless networks (MANETs) overseen by unmanned aerial vehicles (UAVs). We use implicitly certified public keys method, which alleviates the certificate overhead and improves computational efficiency. The architecture uses a two-layered key management approach where the group of nodes is divided into: 1) Cell groups consisting of ground nodes and 2) control groups consisting of cell group managers. The chief benefit of this approach is that the effects of a membership change are restricted to the single cell group.",
Preliminary results on using static analysis tools for software inspection,"Software inspection has been shown to be an effective defect removal practice, leading to higher quality software with lower field failures. Automated software inspection tools are emerging for identifying a subset of defects in a less labor-intensive manner than manual inspection. This paper investigates the use of automated inspection for a large-scale industrial software system at Nortel Networks. We propose and utilize a defect classification scheme for enumerating the types of defects that can be identified by automated inspections. Additionally, we demonstrate that automated code inspection faults can be used as efficient predictors of field failures and are effective for identifying fault-prone modules.",
A rough sets based approach to feature selection,"Feature selection techniques aim at reducing the number of unnecessary features in classification rules. The features are measured by their necessity in heuristic feature selection techniques. Rough set theory has been used to define the necessity of features in literature. We propose a new rough set based feature selection approach called Parameterized Average Support Heuristic (PASH). The PASH considers the overall quality of the potential set of rules. It selects features causing high average support of rules over all decision classes. In addition, the PASH arms with parameters that are used to adjust the level of approximation.","Rough sets,
Set theory,
Computer science,
Arm,
Machine learning,
Supervised learning,
Degradation,
Accuracy,
Intelligent systems"
Silhouette Lookup for Automatic Pose Tracking,"Computers should be able to detect and track the articulated 3-D pose of a human being moving through a video sequence. Current tracking methods often prove slow and unreliable, and many must be initialized by a human operator before they can track a sequence. This paper introduces a simple yet effective algorithm for tracking articulated pose, based upon looking up observed silhouettes in a collection of known poses. The new algorithm runs quickly, can initialize itself without human intervention, and can automatically recover from critical tracking errors made while tracking previous frames in a video sequence.","Humans,
Cameras,
Video sequences,
Application software,
Particle tracking,
Smoothing methods,
Data mining,
Infrared heating,
Computer vision,
Computer science"
Investigating autistic children's attitudes towards strangers with the theatrical robot - a new experimental paradigm in human-robot interaction studies,"In this paper, as part of a larger study into the possible use of robots in therapy or education of children with autism, we studied the effects of two different robot appearances on autistic children's behaviour towards the robot. We used a novel experimental paradigm, the theatrical robot, which is discussed in the context of other evaluation methods used in the field of human-robot interaction (HRI). The two appearances used were a plain/robotic and an 'ordinary human' appearance. The response of children with autism towards the plain/robotic robot was notably more social and pro-active. The ordinary-human appearance resulted in the avoidance behaviour or 'aloofness', a typical behaviour that autistic children show towards strangers. Implications of these results for our work on robots and autism, as well as other HRI research are discussed.",
Open source software development: a case study of FreeBSD,"A common claim is that open source software development produces higher quality software at lower cost than traditional commercial development To validate such claims, researchers have conducted case studies of ""successful"" open source development projects. This case study of the FreeBSD project provides further understanding of open source development. The FreeBSD development process is fairly well-defined with proscribed methods for determining developer responsibilities, dealing with enhancements and defects, and for managing releases. Compared to the Apache project, FreeBSD uses a smaller set of core developers that implement a smaller portion of the system, and uses a more well-defined testing process. FreeBSD and Apache have a similar ratio of core developers to (1) people involved in adapting and debugging the system, and (2) people who report problems. Both systems have similar defect densities, and the developers are also users in both systems.","Open source software,
Computer aided software engineering,
Linux,
Software quality,
Costs,
Kernel,
Laboratories,
Computer science,
System testing,
Debugging"
Qualitative analysis of sketched route maps: translating a sketch into linguistic descriptions,"In this correspondence, we introduce our work on sketch understanding, focusing here on the analysis of a sketched route map. A route map is drawn to help someone navigate along a path for the purpose of reaching a goal. A hand-sketched route map does not generally contain complete map information and is not necessarily drawn to scale, but yet it contains the correct qualitative information for route navigation. Here we propose a methodology for extracting a qualitative model of a sketched route map, based on human navigation strategies, using spatial relationships. Linguistic descriptions are generated from the sketch, both in the form of detailed descriptions at discrete path steps and also as a high-level route description. To describe the path linguistically, one must first be able to understand the path in a qualitative sense. We assert that the translation of a sketch into linguistic descriptions illustrates that the essential qualitative path knowledge has been extracted. The methodology is demonstrated using example sketches drawn on a handheld PDA.",
High-current heavy ion beams in the electrostatic plasma lens,"We describe applications of the electrostatic plasma lens for manipulating and focusing moderate-energy, high-current, broad, heavy ion beams. Use of a plasma lens in this way has been successfully demonstrated in a series of experiments carried out collaboratively between the Institute of Physics, National Academy of Sciences, Kiev, Ukraine, and the Lawrence Berkeley National Laboratory, Berkeley, CA, in recent years. Here, we briefly review the plasma lens fundamentals, peculiarities of focusing heavy ion beams, and summarize some recent developments (experiments, computer simulations, theory). We show that there is a very narrow range of low magnetic field for which the optical properties of the lens improve markedly. This opens up some attractive possibilities for the development of a new-generation compact lens based on permanent magnets. Preliminary experimental results obtained at Kiev and Berkeley on the operation of a permanent magnet plasma lens for manipulating wide aperture high-current heavy ion beams are presented and summarized.","Ion beams,
Electrostatics,
Lenses,
Plasma applications,
Plasma properties,
Permanent magnets,
Nuclear and plasma sciences,
Collaboration,
Physics,
Laboratories"
Depth-map-based scene analysis for active navigation in virtual angioscopy,"This work presents an approach dealing with virtual exploratory navigation inside vascular structures. It is based on the notion of active vision in which only visual perception drives the motion of the virtual angioscope. The proposed fly-through approach does not require a premodeling of the volume dataset or an interactive control of the virtual sensor during the fly-through. Active navigation combines the on-line computation of the scene view and its analysis, to automatically define the three-dimensional sensor path. The navigation environment and the camera-like model are first sketched. The basic stages of the active navigation framework are then described: the virtual image computation (based on ray casting), the scene analysis process (using depth map), the navigation strategy, and the virtual path estimation. Experimental results obtained from phantom model and patient computed tomography data are finally reported.","Image analysis,
Navigation,
Computed tomography,
Endoscopes,
Magnetic resonance imaging,
Automatic control,
Path planning,
Biomedical imaging,
Surgery,
Anatomical structure"
Motion-based robotic self-recognition,"We present a method for allowing a humanoid robot to recognize its own motion in its visual field, thus enabling it to distinguish itself from other agents in the vicinity. Our approach consists of learning a characteristic time window between the initiation of motor movement and the perception of arm motions. The method has been implemented and evaluated on an infant humanoid platform. Our results demonstrate the effectiveness of using the delayed temporal contingency in the action-perception loop as a basis for simple self-other discrimination. We conclude by suggesting potential applications in social robotics and in generating forward models of motion.","Mirrors,
Reflection,
Automatic testing,
Robot sensing systems,
Computer science,
Humanoid robots,
Psychology,
Humans,
Robot vision systems,
Cameras"
Test-cost sensitive naive Bayes classification,"Inductive learning techniques such as the naive Bayes and decision tree algorithms have been extended in the past to handle different types of costs mainly by distinguishing different costs of classification errors. However, it is an equally important issue to consider how to handle the test costs associated with querying the missing values in a test case. When the value of an attribute is missing in a test case, it may or may not be worthwhile to take the effort to obtain its missing value, depending on how much the value results in a potential gain in the classification accuracy. In this paper, we show how to obtain a test-cost sensitive naive Bayes classifier (csNB) by including a test strategy which determines how unknown attributes are selected to perform test on in order to minimize the sum of the mis-classification costs and test costs. We propose and evaluate several potential test strategies including one that allows several tests to be done at once. We empirically evaluate the csNB method, and show that it compares favorably with its decision tree counterpart.",
An integrative and interactive framework for improving biomedical pattern discovery and visualization,"Recent progress in medical sciences has led to an explosive growth of data. Due to its inherent complexity and diversity, mining such volumes of data to extract relevant knowledge represents an enormous challenge and opportunity. Interactive pattern discovery and visualization systems for biomedical data mining have received relatively little attention. Emphasis has been traditionally placed on automation and supervised classification problems. Based on self-adaptive neural networks and pattern-validation statistical tools, this paper presents a user-friendly platform to support biomedical pattern discovery and visualization. It has been tested on several types of biomedical data, such as dermatology and cardiology data sets. The results indicate that in comparison to traditional techniques, such as Kohonen Maps, this platform may significantly improve the effectiveness and efficiency of pattern discovery and classification tasks, including problems described by several classes. Furthermore, this study shows how the combination of graphical and statistical tools may make these patterns more meaningful.",
Supporting flexible collaborative distance learning in the CURE platform,"At the German Distance Learning University, five collaborative distance learning scenarios have been recently identified by students and teachers as an important future form of collaborative learning in the university's virtual learning space. An analysis of these scenarios showed a great variance between them and a need to support runtime tailoring of the respective learning environments. Since no existing learning platform addresses all the needs, the CURE collaborative learning platform was developed to support the implementation of such a variety of tailorable learning environments. CURE is based on a room concept and supports the implementation and runtime tailoring of collaborative distance learning environments for our scenarios. Initial experience indicates its applicability.",
Noise suppression in UWB transmitted reference systems,"Transmitted reference (TR) systems have recently been proposed for ultra wideband (UWB) communications. They considerably simplify synchronization and channel estimation, which are known to be difficult problems in UWB communications. We extend existing receivers for TR-UWB systems by replacing the correlation operation by a linear combination of specific parts of the correlation and weighting the parts that have a small noise contribution more than parts that have a large noise contribution. This turns out to improve the performance considerably.","Channel estimation,
Ultra wideband technology,
Delay estimation,
Positron emission tomography,
Pulse modulation,
Mathematics,
Computer science,
Ultra wideband communication,
Data models"
People detection and tracking in high resolution panoramic video mosaic,"We have designed a physical awareness system called CAMEO, the camera assisted meeting event observer, which consists of a multi-camera omnidirectional vision system designed to be used in meeting environments. CAMEO is designed to monitor the activities of people in meetings so that it can generate a semantically-indexed summary of what occurred in the meeting. In this paper, we describe CAMEO's fast people detection and tracking module. This module makes use of a combination of frame differencing, face detection, and adaptive color blob tracking based on mean shift analysis to detect and track people in the panoramic image. We describe this algorithm and present experimental results from captured meeting logs.",
Geometric and shading correction for images of printed materials: a unified approach using boundary,"We present a novel approach that uses boundary interpolation to correct (1) geometric distortion and (2) shading artifacts present in images of printed materials. Unlike existing approaches, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can estimate the intrinsic illumination component of the distorted image to correct shading artifacts.","Shape,
High-resolution imaging,
Interpolation,
Books,
Geometry,
Optical character recognition software,
Optical distortion,
Surface reconstruction,
Computer science,
Lighting"
Practical automated process and product metric collection and analysis in a classroom setting: lessons learned from Hackystat-UH,"Measurement definition, collection, and analysis is an essential component of high quality software engineering practice, and is thus an essential component of the software engineering curriculum. However, providing students with practical experience with measurement in a classroom setting can be so time-consuming and intrusive that it's counter-productive-teaching students that software measurement is ""impractical"" for many software development contexts. In this research, we designed and evaluated a very low-overhead approach to measurement collection and analysis using the Hackystat system with special features for classroom use. We deployed this system in two software engineering classes at the University of Hawaii during Fall, 2003, and collected quantitative and qualitative data to evaluate the effectiveness of the approach. Results indicate that the approach represents substantial progress toward practical, automated metrics collection and analysis, though issues relating to the complexity of installation and privacy of user data remain.",
'Visual-fidelity' dataglove calibration,"This paper presents a novel calibration method for data-gloves with many degrees of freedom. The goal of our method is to establish a mapping from the sensor values of the glove to the joint angles of an articulated hand that is of 'high visual' fidelity. This is in contrast to previous methods that aim at determining the absolute values of the real joint angles with high accuracy. The advantage of our method is that it can be simply carried through without the need for auxiliary calibration hardware (such as cameras), while still producing visually correct mappings. To achieve this, we developed a method that explicitly models the cross-couplings of the abduction sensors with the neighboring flex sensors. The results show that our method performs superior to linear calibration in most cases","Data gloves,
Calibration,
Hardware,
Virtual reality,
Virtual environment,
Fingers,
Sensor phenomena and characterization,
Computer science,
Cameras,
Instruments"
Asteroid exploration with autonomic systems,"NASA is studying advanced technologies for a future robotic exploration mission to the asteroid belt. The prospective ANTS (Autonomous Nano Technology Swarm) mission comprises autonomous agents including ""worker"" agents (small spacecraft) designed to cooperate in asteroid exploration under the overall authority of at least one ""ruler"" agent (a larger spacecraft) whose goal is to cause science data to be returned to Earth. The ANTS team (ruler plus workers and messenger agents), but not necessarily any individual on the team, exhibit behaviors that qualify it as an autonomic system, where an autonomic system is defined as a system that self-reconfigures, self-optimizes, self-heals, and self-protects. Autonomic system concepts lead naturally to realistic, scalable architectures rich in capabilities and behaviors. In-depth consideration of a major mission like ANTS in terms of autonomic systems brings new insights into alternative definitions of autonomic behavior. This paper gives an overview of the ANTS mission and discusses the autonomic properties of the mission.",
Artifact analysis and reconstruction improvement in helical cardiac cone beam CT,"With the introduction of cone beam (CB) scanners, cardiac volumetric computed tomography (CT) imaging has the potential to become a noninvasive imaging tool in clinical routine for the diagnosis of various heart diseases. Heart rate adaptive reconstruction schemes enable the reconstruction of high-resolution volumetric data sets of the heart. Artifacts, caused by strong heart rate variations, high heart rates and obesity, decrease the image quality and the diagnostic value of the images. The image quality suffers from streak artifacts if suboptimal scan and reconstruction parameters are chosen, demanding improved gating techniques. In this paper, an artifact analysis is carried out which addresses the artifacts due to the gating when using a three-dimensional CB cardiac reconstruction technique. An automatic and patient specific cardiac weighting technique is presented in order to improve the image quality. Based on the properties of the reconstruction algorithm, several assessment techniques are introduced which enable the quantitative determination of the cycle-to-cycle transition smoothness and phase homogeneity of the image reconstruction. Projection data of four patients were acquired using a 16-slice CBCT system in low pitch helical mode with parallel electrocardiogram recording. For each patient, image results are presented and discussed in combination with the assessment criteria.","Image reconstruction,
Computed tomography,
Detectors,
High-resolution imaging,
Heart rate,
Image quality,
Gravity,
Image analysis,
Cardiac disease,
Hospitals"
Supporting persistent social groups in ubiquitous computing environments using context-aware ephemeral group service,"We analyze the role of the social group in a ubiquitous computing (Ubicomp) environment as a source of contextual information. A model is presented to address the social group member's perceptions of how devices in a ubicomp environment should aid them in collaboration. Based on the model a distributed context-aware group membership management scheme is developed. We then present a prototype implementation for a context-aware ephemeral group membership management scheme, along with a sample application, and experimental results that demonstrate the feasibility of our system.",
Fast analysis of transient scattering from lossy inhomogeneous dielectric bodies,"A time domain integral equation (TDIE)-based approach for analyzing transient wave scattering from linear lossy media is proposed. The pertinent TDIEs are cast in terms of a “conduction current corrected flux density” and are solved using a marching-on-in-time (MOT) scheme that incorporates a differential equation update algorithm for the aforementioned flux. The scheme is accelerated by the PWTD algorithm, and it is shown that the computational complexity and memory requirements of the resulting solver scales as

(NtNs) and

(Ns), where Nt and Ns denote the number of temporal and spatial degrees of freedom in the flux expansion, respectively. Numerical results that validate the accuracy and efficacy of the proposed method are presented.",
Embedded software optimization for MP3 decoder implemented on RISC core,"This paper proposes general software optimization techniques for embedded systems based on processors, which mainly include general optimization methods in high language and software and hardware co-optimization in assembly language. Then these techniques are applied to optimize our MP3 decoder, which is based on RISC32, a RISC core compatible with MIPSI instruction set. The last optimization decoder requires 48 MIPS and 49 Kbytes memory space to decode 128 Kbps, 44.1 KHz joint stereo MP3 in real time with CPI 1.15, and we have achieved performance increase of 46.7% and memory space decrease of 38.8% over the original decoding software.",
Cue integration through discriminative accumulation,"Object recognition systems aiming to work in real world settings should use multiple cues in order to achieve robustness. We present a new cue integration scheme, which extends the idea of cue accumulation to discriminative classifiers. We derive and test the scheme for support vector machines (SVMs), but we also show that it is easily extendible to any large margin classifier. In the case of one-class SVMs the scheme can be interpreted as a new class of Mercer kernels for multiple cues. Experimental comparison with a probabilistic accumulation scheme is favorable to our method. Comparison with voting scheme shows that our method may suffer as the number of object classes increases. Based on these results, we propose a recognition algorithm consisting of a decision tree where decisions at each node are taken using our accumulation scheme. Results obtained using this new algorithm compare very favorably to accumulation (both probabilistic and discriminative) and voting scheme.","Voting,
Object recognition,
Robustness,
Computer vision,
Histograms,
Humans,
Laboratories,
Numerical analysis,
Computer science,
Testing"
Stochastic deconvolution over groups,"In this paper, we address a class of inverse problems that are formulated as group convolutions. This is a rich area of research with applications to Radon transform inversion for tomography, wide-band and narrow-band signal processing, inverse rendering in computer graphics, and channel estimation in communications, as well as robotics and polymer science. We present a group-theoretic framework for signal modeling and analysis for such problems and propose a minimum mean-square error (MMSE) deconvolution method in a probabilistic setting. Key components of our approach are group representation theory and the concept of group stationarity. The proposed deconvolution method incorporates a priori information and noise statistics into the inversion process, which leads to a natural regularized solution. We present recovery of self-similar processes that are ""blurred"" and embedded in noise as a demonstration example. The method is applicable to a wide range of inverse problems involving both commutative and noncommutative groups including finite, compact, and majority of well-behaved locally compact groups.","Stochastic processes,
Deconvolution,
Inverse problems,
Convolution,
Application software,
Tomography,
Wideband,
Narrowband,
Signal processing,
Rendering (computer graphics)"
Experimental evaluation of TCP performance in multi-hop wireless ad hoc networks,"The paper presents experimental measurements of TCP bulk data transfer performance in a multi-hop wireless ad hoc network environment. The paper first studies how TCP throughput is affected by AODV routing, user mobility, and the number of hops traversed in the network. The paper then studies the effectiveness of rate-based pacing (RBP) of TCP packets in improving TCP throughput. Contrary to prior simulation results in the networking literature, our measurement results show no performance advantages for RBP TCP in our experimental scenarios.",
Analyses of CCD images of nucleon-silicon interaction events,"Sets of image frames were captured from a CCD device exposed to continuous beams of high energy neutrons. The tracks of short-range fragments from nuclear spallation interactions in the silicon of the device's pixels have been analyzed in respect of their frequency, intensity, directionality and other pertinent parameters. A comparison between these results and the predictions of computer code models of nucleon interactions in silicon is presented. Comparisons are also made with equivalent images of neutron events in an APS camera.",
Efficient learning of hierarchical latent class models,"Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are hidden. In earlier work, we have demonstrated in principle the possibility of reconstructing HLC models from data. We address the scalability issue and develop a search-based algorithm that can efficiently learn high-quality HLC models for realistic domains. There are three technical contributions: (1) the identification of a set of search operators; (2) the use of improvement in BIC score per unit of increase in model complexity, rather than BIC score itself, for model selection; and (3) the adaptation of structural EM for situations where candidate models contain different variables than the current model. The algorithm was tested on the COIL Challenge 2000 data set and an interesting model was found.","Bayesian methods,
Testing,
Computer science,
Laboratories,
Intelligent systems,
Intelligent networks,
Scalability,
Phylogeny,
Terminology,
Clustering algorithms"
The MUSART Testbed for Query-by-Humming Evaluation,,
Automatic composition of Web services with contingency plans,"The semantic Web technology and the Web services description language extensibility may be combined to describe services in an unambiguous and machine interpretable way, automating Web services discovery, selection and invocation. In this paper, we present an algorithm and a prototype for the automatic composition of Web services that implement workflows described in a high level language. Our approach has many advantages comparing to the manual creation of a simple program composition, such as smaller implementation time and cost, reliability with the generation of contingency plans, greater capacity to evolve with the dynamic service discovery, and faster execution time with the use of heuristics. We use the OWLS ontology to semantically describe Web services metadata and indexes to help selecting them. The proposed algorithm considers that equivalent services may have different interfaces and also respects preferences of the users.","Web services,
Semantic Web,
Ontologies,
Programming profession,
Prototypes,
Robustness,
Systems engineering and theory,
Computer science,
High level languages,
Costs"
Structures for in-network moving object tracking in wireless sensor networks,"One important application of wireless sensor networks is the tracking of moving objects. The recent progress has made it possible for tiny sensors to have more computing power and storage space. Therefore, a sensor network can be considered as a distributed database, on which one can conduct in-network data processing. This paper considers in-network moving object tracking in a sensor network. This typically consists of two operations: location update and query. We propose a message-pruning tree structure that is an extension of the earlier work (H.T. Kung and D. Vlah, March 2003), which assumes the existence of a logical structure to connect sensors in the network. We formulate this problem as an optimization problem. The formulation allows us to take into account the physical structure of the sensor network, thus leading to more efficient solutions than in the previous paper of H.T. Kung and D. Vlah (March 2003) in terms of communication costs. We evaluate updating and querying costs through simulations.","Intelligent networks,
Wireless sensor networks,
Costs,
Distributed databases,
Data processing,
Tree data structures,
Handheld computers,
Communication system software,
Embedded software,
Computer science"
Blind Clustering of Popular Music Recordings Based on Singer Voice Characteristics,,
Load balanced short path routing in wireless networks,"We study wireless network routing algorithms that use only short paths, for minimizing latency, and achieve good load balance, for balancing the energy use. We consider the special case when all the nodes are located in a narrow strip with width at most /spl radic/3/2 /spl ap/ 0.86 times the communication radius. We present algorithms that achieve good performance in terms of both measures simultaneously. In addition, our algorithms only use local information and can deal with dynamic change and mobility efficiently.","Routing,
Intelligent networks,
Wireless networks,
Delay,
Strips,
Wireless sensor networks,
Relays,
Computer science,
Spread spectrum communication,
Centralized control"
Clustered components analysis for functional MRI,A common method of increasing hemodynamic response (SNR) in functional magnetic resonance imaging (fMRI) is to average signal timecourses across voxels. This technique is potentially problematic because the hemodynamic response may vary across the brain. Such averaging may destroy significant features in the temporal evolution of the fMRI response that stem from either differences in vascular coupling to neural tissue or actual differences in the neural response between two averaged voxels. Two novel techniques are presented in this paper in order to aid in an improved SNR estimate of the hemodynamic response while preserving statistically significant voxel-wise differences. The first technique is signal subspace estimation for periodic stimulus paradigms that involves a simple thresholding method. This increases SNR via dimensionality reduction. The second technique that we call clustered components analysis is a novel amplitude-independent clustering method based upon an explicit statistical data model. It includes an unsupervised method for estimating the number of clusters. Our methods are applied to simulated data for verification and comparison to other techniques. A human experiment was also designed to stimulate different functional cortices. Our methods separated hemodynamic response signals into clusters that tended to be classified according to tissue characteristics.,
Software fault tree analysis for product lines,"The current development of high-integrity product lines threatens to outstrip existing tools for product-line verification. Software Fault Tree Analysis (SFTA) is a technique that has been used successfully to investigate contributing causes to potential hazards in safety-critical applications. This paper adapts SFTA to product lines of systems. The contribution is to define: (1) the technique to construct a product-line SFTA; and (2) the pruning technique required to reuse the SFTA for the analysis of a new system in the product line. The paper describes how product-line SFTA integrates with forward-analysis techniques such as Software Failure Modes, Effects, and Criticality Analysis (SFMECA), supports requirements evolution, and helps identify previously unforeseen constraints on the systems to be built. Applications to two small examples are used to illustrate the technique.","Fault trees,
Product safety,
Software safety,
Application software,
Computer science,
Hazards,
Software reusability,
Propulsion,
Laboratories,
Failure analysis"
An adaptive level set segmentation on a triangulated mesh,"Level set methods offer highly robust and accurate methods for detecting interfaces of complex structures. Efficient techniques are required to transform an interface to a globally defined level set function. In this paper, a novel level set method based on an adaptive triangular mesh is proposed for segmentation of medical images. Special attention is paid to an adaptive mesh refinement and redistancing technique for level set propagation, in order to achieve higher resolution at the interface with minimum expense. First, a narrow band around the interface is built in an upwind fashion. An active square technique is used to determine the shortest distance correspondence (SDC) for each grid vertex. Simultaneously, we also give an efficient approach for signing the distance field. Then, an adaptive improvement algorithm is proposed, which essentially combines two basic techniques: a long-edge-based vertex insertion strategy, and a local improvement. These guarantee that the refined triangulation is related to features along the front and has elements with appropriate size and shape, which fit the front well. We propose a short-edge elimination scheme to coarsen the refined triangular mesh, in order to reduce the extra storage. Finally, we reformulate the general evolution equation by updating: 1) the velocities and 2) the gradient of level sets on the triangulated mesh. We give an approach for tracing contours from the level set on the triangulated mesh. Given a two-dimensional image with N grids along a side, the proposed algorithms run in O(kN) time at each iteration. Quantitative analysis shows that our algorithm is of first order accuracy; and when the interface-fitted property is involved in the mesh refinement, both the convergence speed and numerical accuracy are greatly improved. We also analyze the effect of redistancing frequency upon convergence speed and accuracy. Numerical examples include the extraction of inner and outer surfaces of the cerebral cortex from magnetic resonance imaging brain images.",
Using area-based presentations and metrics for localization systems in wireless LANs,"We show the utility of WLAN localization using areas and volumes as the fundamental localization unit. We demonstrate that area-based algorithms have a critical advantage over point-based approaches because they are better able to describe localization uncertainty, which is a common theme across WLAN based localization systems. Next, we present two novel area-based algorithms. To evaluate area-based approaches, we introduce several new localization performance metrics. We then evaluate the two algorithms using our metrics with data collected from our local WLAN. Finally, we compare our area-based algorithms against traditional point-based approaches. We find that using areas improves the ability of the localization system to give users meaningful alternatives in the face of position uncertainty.","Wireless LAN,
Local area networks,
Uncertainty,
Computer science,
Measurement,
Investments,
Costs,
Ultrasonic imaging,
Computer errors,
Computer networks"
An evolutionary approach for gene expression patterns,"This study presents an evolutionary algorithm, called a heterogeneous selection genetic algorithm (HeSGA), for analyzing the patterns of gene expression on microarray data. Microarray technologies have provided the means to monitor the expression levels of a large number of genes simultaneously. Gene clustering and gene ordering are important in analyzing a large body of microarray expression data. The proposed method simultaneously solves gene clustering and gene-ordering problems by integrating global and local search mechanisms. Clustering and ordering information is used to identify functionally related genes and to infer genetic networks from immense microarray expression data. HeSGA was tested on eight test microarray datasets, ranging in size from 147 to 6221 genes. The experimental clustering and visual results indicate that HeSGA not only ordered genes smoothly but also grouped genes with similar gene expressions. Visualized results and a new scoring function that references predefined functional categories were employed to confirm the biological interpretations of results yielded using HeSGA and other methods. These results indicate that HeSGA has potential in analyzing gene expression patterns.","Gene expression,
Genetic algorithms,
Pattern analysis,
Clustering methods,
Displays,
Monitoring,
Testing,
Computer science,
Bioinformatics,
Evolutionary computation"
Concept formation and learning: a cognitive informatics perspective,"Concepts are the basic units of thought that underlie human intelligence and communication. From the perspective of cognitive informatics, a layered framework is suggested for concept formation and learning. It combines cognitive science and machine learning approaches. The philosophical issues and various views of concepts are reviewed. Concept learning methods are presented based on the classical view of concepts.","Cognitive informatics,
Humans,
Cognitive science,
Computer architecture,
Learning systems,
Computational intelligence,
Artificial intelligence,
Computer science,
Information processing,
Machine intelligence"
The pulse protocol: energy efficient infrastructure access,"We present the pulse protocol which is designed for multi-hop wireless infrastructure access. While similar to the more traditional access point model, it is extended to operate across multiple hops. This is particularly useful for conference, airport, or large corporate deployments. In these types of environments where users are highly mobile, energy efficiency becomes of great importance. The pulse protocol utilizes a periodic flood initiated at the network gateways which provides both routing and synchronization to the network. This synchronization is used to allow idle nodes to power off their radios for a large percentage of the time when they are not needed for packet forwarding. This results in substantial energy savings. Through simulation we validate the performance of the routing protocol with respect to both packet delivery and energy savings.","Access protocols,
Energy efficiency,
Peer to peer computing,
Routing protocols,
Ad hoc networks,
Wireless application protocol,
Spread spectrum communication,
Computer science,
Airports,
IP networks"
Instrument recognition in accompanied sonatas and concertos,"A system for musical instrument recognition is introduced. In contrast to most existing systems, it can identify a solo instrument even in the presence of an accompanying keyboard instrument or orchestra. To enable recognition in the presence of a highly polyphonic background, we use features based solely on the partials of the target tone. The approach is based on the assumption that it is possible to extract the most prominent fundamental frequency and the corresponding harmonic overtone series, and that these most often belong to the solo instrument. Classification is carried out using a Gaussian classifier trained on examples of monophonic music. Testing our system on accompanied sonatas and concertos we achieved a recognition rate of 86% for 5 different instruments, an accuracy comparable to that of systems limited to monophonic music only.","Instruments,
Frequency estimation,
Power system harmonics,
Pattern matching,
Computer science,
Convolution,
Acoustic noise,
Friction"
GATES: a grid-based middleware for processing distributed data streams,"Increasingly, a number of applications rely on, or can potentially benefit from, analysis and monitoring of data streams. Moreover, many of these applications involve high volume data streams and require distributed processing of data arising from a distributed set of sources. Thus, we believe that a grid environment is well suited for flexible and adaptive analysis of these streams. This paper reports the design and initial evaluation of a middleware for processing distributed data streams. Our system is referred to as GATES (grid-based adaptive execution on streams). This system is designed to use the existing grid standards and tools to the extent possible. It flexibly achieves the best accuracy that is possible while maintaining the real-time constraint on the analysis. We have developed a self-adaptation algorithm for this purpose. Results from a detailed evaluation of this system demonstrate the benefits of distributed processing, and the effectiveness of our self-adaptation algorithm.",
Generating coalition structures with finite bound from the optimal guarantees,,
"CS AKTive Space, or how we learned to stop worrying and love the semantic Web",We discuss about the CS AKTive Space which addresses many core semantic Web challenges to offer an integrated information overview of who's doing what and where in UK computer science research.,"Semantic Web,
Content addressable storage,
Computer science,
Space technology,
Databases,
Knowledge management,
Web sites,
Organizing,
Ontologies,
Resource description framework"
Palmprint identification using palmcodes,"This paper investigates a new approach for the palmprint identification using real Gabor function (RGF) filtering. Inkless composite hand images have been used to automatically to extract the palmprints from peg-free imaging setup. These palmprints, after normalization, are subjected to selective feature sampling by a bank of RGF. Each of these filtered images has been used to extract significant features (PalmCode) from each of 6 concentric circular bands. Our preliminary experimental results using 400 low-resolution palmprint images achieve the recognition rate of 97.50% and also illustrate the shortcomings of results presented in earlier work. The results show the uniqueness of palmprint texture, even in the two hands of an individual and its possible use in biometrics based personal recognition.",
Sphere-filled organ model for virtual surgery system,"We have been developing a virtual surgery system that is capable of simulating surgical maneuvers on elastic organs. In order to perform such maneuvers, we have created a deformable organ model using a sphere-filled method instead of the finite element method. This model is suited for real-time simulation and quantitative deformation. Furthermore, we have equipped this model with a sense of touch and a sense of force by connecting it to a force feedback device. However, in the initial stage the model became problematic when faced with complicated incisions. Therefore, we modified this model by developing an algorithm for organ deformation that performs various, complicated incisions while taking into account the effect of gravity. As a result, the sphere-filled model allowed our system to respond to various incisions that deform the organ. Thus, various physical manipulations that involve pressing, pinching, or incising an organ's surface can be performed. Furthermore, the deformation of the internal organ structures and changes in organ vasculature can be observed via the internal spheres' behavior.",
Modeling adaptive and evolvable software product lines using the variation point model,"The product line approach provides a systematic approach for software reuse. A challenge with modeling adaptive and evolvable software product lines is how to model variability. This paper describes four different approaches to modeling variability, modeling variability using parameterization, modeling variability using information hiding, modeling variability using inheritance, and modeling variability using variation points. The variation point model (VPM) is used to demonstrate the fourth approach. VPM allows a reuser or application engineer to extend components at pre-specified variation points. For this to be possible, a variation point must be modeled such that the reuser has enough knowledge to build a variant.",
Coordinated multi-robot exploration through unsupervised clustering of unknown space,"This paper proposes a new coordination algorithm for efficiently exploring an unknown environment with a team of mobile robots. The proposed technique subsequently applies a well-known unsupervised clustering algorithm (k-means) in order to fairly divide the remaining unknown space into as many disjoint regions as available robots. Each robot is primarily responsible for exploring its assigned region and can help other robots on its way through. Unknown space is dynamically repartitioned as new areas are discovered by the team, balancing thus the overall workload among team members and naturally leading to greater dispersion over the environment and thus faster broad coverage than with previous greedy-like approaches, which guide robots based on maximum profit strategies that simply trade off between distance to the closest frontiers and amount of unknown cells likely to be discovered from them.",
Subspace selection for clustering high-dimensional data,"In high-dimensional feature spaces traditional clustering algorithms tend to break down in terms of efficiency and quality. Nevertheless, the data sets often contain clusters which are hidden in various subspaces of the original feature space. In this paper, we present a feature selection technique called SURFING (subspaces relevant for clustering) that finds all subspaces interesting for clustering and sorts them by relevance. The sorting is based on a quality criterion for the interestingness of a subspace using the k-nearest neighbor distances of the objects. As our method is more or less parameterless, it addresses the unsupervised notion of the data mining task ""clustering"" in a best possible way. A broad evaluation based on synthetic and real-world data sets demonstrates that SURFING is suitable to find all relevant sub-spaces in high dimensional, sparse data sets and produces better results than comparative methods.","Clustering algorithms,
Data mining,
Space technology,
Biomedical informatics,
Computer science,
Sorting,
Principal component analysis,
Clustering methods,
Navigation,
Density measurement"
Automatic feature-based global motion estimation in video sequences,"In this paper, automatic and efficient global motion estimation for video sequences is proposed. In this estimation, we find and pick up pairs of feature points in the edge lines within the successive frames using the match algorithm. We then choose the best set of three pair points to measure the parameters of the Affine Model which describes the global motion. The result shows that we can obtain the Affine parameters of global motion efficiently in video sequences.","Motion estimation,
Video sequences,
Cameras,
Object segmentation,
Layout,
Image edge detection,
Motion detection,
Computer science,
Motion measurement,
MPEG 4 Standard"
A low-cost PC-based virtual oscilloscope,"The integration of personal computers (PCs) with the present-day measurement and instrumentation world has opened up the door for ""virtual instrumentation."" This paper describes the development of a low-cost, PC-based, virtual digital oscilloscope. A data acquisition device using the computer parallel port has been fabricated with full analog interface and 8-b analog-to-digital converter with associated control-logic and timing circuitries. A software package aimed at managing the acquisition process and displaying the acquired signal has also been developed. This device has been conceived as a simple plug-in board to be incorporated to any IBM PC parallel port-thus, in many cases, eliminating the requirements of multiple, expensive, digital oscilloscopes. The project, initially designed as an undergraduate thesis, is currently being used for laboratory instructional purposes. The features of this device make it suitable for implementing as an educational resource for undergraduate students from Electrical, Electronics, Instrumentation, and Computer Science faculties.",
Binary self-dual codes with automorphisms of composite order,"In this paper, we present some results concerning a decomposition of binary self-dual codes having an automorphism of an order which is the product of two odd prime numbers. These results are applied to construct self-dual [72,36,12] codes with an automorphism of order 15. Furthermore, it is proved that the automorphism group of a putative binary extremal self-dual [72,36,16] code contains at most 28 nontrivial types of automorphisms of odd order. A complete list of these possible types of automorphisms is presented.","Vectors,
Binary codes,
Computer science,
Linear code,
Hamming weight,
Mathematics"
Boundary coverage criteria for test generation from formal models,"This paper proposes a new family of model-based coverage criteria, based on formalizing boundary-value testing heuristics. The new criteria form a hierarchy of data-oriented coverage criteria, and can be applied to any formal notation that uses variables and values. They can be used either to measure the coverage of an existing test set, or to generate tests from a formal model. We give algorithms that can be used to generate tests that satisfy the criteria. These algorithms and criteria have been incorporated into the BZ-TESTING-TOOLS (BZ-TT) tool-set for automated test case generation from B, Z and UML/OCL specifications, and have been used and validated on several industrial applications in the domain of critical software, particularly smart cards and transport systems.",
Multiple priorities in a two-lane buffered crossbar,"A significant advantage of buffered crossbar switches is that they can directly operate on variable-size packets. However, in order to support multiple priority levels, separate queues per priority are needed at each crosspoint, in order to prevent HOL blocking and buffer hogging; these queues are expensive because they each need a size of at least one maximum-size packet. In this paper, we propose a scheme that uses only two queues per crosspoint to effectively support multiple priorities. We adaptively adjust the priority levels of the two queues so that most traffic goes through the ""lower"" queue, while the ""upper"" queue remains usually available for higher priority packets to overtake the former. Through simulation, and assuming 8 priority levels, we compare our scheme to an ideal system that uses 8 queues per crosspoint. For realistic traffic, the two systems perform almost identically, although ours uses 4 times less memory in the crossbar. Even under a highly irregular traffic pattern, Bursts60, our system does not increase the average delay of any priority level by more than 75% compared to the ideal system.","Switches,
Costs,
Traffic control,
Packet switching,
CMOS technology,
Quality of service,
Computer science,
Delay,
Throughput,
Ethernet networks"
On the robustness of soft state protocols,"Soft state has been a mantra of Internet protocol design for the past decade. System designers build protocols that implement soft state mechanisms based on intuition or on qualitative arguments that the design is ""better"", yet there has never been a formal performance evaluation study that draws the same conclusion. In fact, previous attempts [P. Ji et al., 2003 and S. Raman et al., 1999] to build such a quantitative argument have found that pure soft state protocols significantly under-perform their hard state counterparts, and that only soft-hard hybrids can match hard state protocol performance. In this paper, we argue otherwise. We develop models that provide a performance-oriented explanation and justification of the Internet designer's intuition. The novel observation is that, if network conditions are known, a hard state protocol can always be configured to outperform its soft state counterpart. However, in reality, network conditions are unpredictable, and that soft state protocols are much more resilient to unanticipated fluctuations in these conditions.","Robustness,
Protocols,
Internet,
Computer science,
Communication channels,
Communication system control,
Fluctuations,
Design methodology,
Stochastic processes,
Information analysis"
Long bone panoramas from fluoroscopic X-ray images,"This paper presents a new method for creating a single panoramic image of a long bone from several individual fluoroscopic X-ray images. Panoramic images are useful preoperatively for diagnosis, and intraoperatively for long bone fragment alignment, for making anatomical measurements, and for documenting surgical outcomes. Our method composes individual overlapping images into an undistorted panoramic view that is the equivalent of a single X-ray image with a wide field of view. The correlations between the images are established from the graduations of a radiolucent ruler imaged alongside the long bone. Unlike existing methods, ours uses readily available hardware, requires a simple image acquisition protocol with minimal user input, and works with existing fluoroscopic C-arm units without modifications. It is robust and accurate, producing panoramas whose quality and spatial resolution is comparable to that of the individual images. The method has been successfully tested on in vitro and clinical cases.","Bones,
X-ray imaging,
Implants,
Orthopedic surgery,
Hardware,
Protocols,
Joints,
Distortion measurement,
Computer science,
Robustness"
Low dimensional adaptive texture feature vectors from class distance and class difference matrices,"In many popular texture analysis methods, second or higher order statistics on the relation between pixel gray level values are stored in matrices. A high dimensional vector of predefined, nonadaptive features is then extracted from these matrices. Identifying a few consistently valuable features is important, as it improves classification reliability and enhances our understanding of the phenomena that we are modeling. Whatever sophisticated selection algorithm we use, there is a risk of selecting purely coincidental ""good"" feature sets, especially if we have a large number of features to choose from and the available data set is limited. In a unified approach to statistical texture feature extraction, we have used class distance and class difference matrices to obtain low dimensional adaptive feature vectors for texture classification. We have applied this approach to four relevant texture analysis methods. The new adaptive features outperformed the classical features when applied to the most difficult set of 45 Brodatz texture pairs. Class distance and difference matrices also clearly illustrated the difference in texture between cell nucleus images from two different prognostic classes of early ovarian cancer. For each of the texture analysis methods, one adaptive feature contained most of the discriminatory power of the method.",
Using Web service technologies to create an information broker: an experience report,"This paper reports on our experiences with using the emerging Web service technologies and tools to create a demonstration information broker system as part of our research into information management in a distributed environment. To provide a realistic context, we chose to study the use of information in the healthcare domain, and this context sets some challenging parameters and constraints for our research and for the demonstration system. In this paper, we both report on the extent to which existing Web service technologies have proved to be mature enough to meet these requirements, and also assess their current limitations.",
A user-guided cognitive agent for network service selection in pervasive computing environments,"Connectivity is central to pervasive computing environments. We seek to catalyze a world of rich and diverse connectivity through technologies that drastically simplify the task of providing, choosing, and using wireless network services; creating a new and more competitive environment for these capabilities. A critical requirement is that users actually benefit from this rich environment, rather than simply being overloaded with choices. We address this with an intelligent software agent that transparently and continually chooses from among available network services based on its user's individual needs and preferences, while requiring only minimal guidance and user interaction. We present an overview and model of the network service selection problem. We then describe an adaptive user agent that learns its user's network service preferences from a very minimal, intuitive set of inputs, and autonomously and continually selects the service that best meets the user's needs. Results from preliminary user experiments are presented that demonstrate the effectiveness of our agent.","Intelligent networks,
Pervasive computing,
Environmental economics,
Internet,
Base stations,
Wireless LAN,
Large-scale systems,
Computer science,
Artificial intelligence,
Wireless networks"
SELAR: scalable energy-efficient location aided routing protocol for wireless sensor networks,"Wireless sensor networks (WSN) consist of thousands of tiny and low cost nodes with very limited energy, computing power and communication capabilities. We consider these issues in the design of a simple, scalable, energy-efficient location aided routing (SELAR) protocol for WSN. In SELAR, location and energy information of neighboring nodes together with the location information of the sink node are used to perform the routing function. Through simulations, we show that SELAR performs considerably better than flooding in terms of network lifetime, energy distribution, and amount of data delivered.","Energy efficiency,
Routing protocols,
Wireless sensor networks,
Floods,
Clocks,
Random number generation,
Computer networks,
Computer simulation,
Wireless application protocol,
Computer science"
Fast and exact out-of-core k-means clustering,"Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset. In this paper, we present a new algorithm which typically requires only one or a small number of passes on the entire dataset, and provably produces the same cluster centers as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centers, and then takes one or more passes over the entire dataset to adjust these cluster centers. We provide theoretical analysis to show that the cluster centers thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared to k-means.","Clustering algorithms,
Data mining,
Computer science,
Data engineering,
Sampling methods,
Algorithm design and analysis,
Statistics,
Pattern recognition,
Databases,
Convergence"
Video-object segmentation using multi-sprite background subtraction,"The background subtraction algorithm is a frequently-used object segmentation technique because of its algorithmic simplicity. However, we show that, for general rotational camera-motion, it is impractical, or even impossible, to use a single background image. As a solution, we propose to use multi-sprite backgrounds which enables processing of arbitrary rotational camera-motion. The paper describes a complete video-object segmentation system employing multi-sprites. The system generates object masks and background sprites that are compatible with MPEG-4 object-oriented video-coding tools. Good segmentation results are also obtained for sequences which cannot be processed with ordinary background images.",
Photo classification by integrating image content and camera metadata,"Despite years of research, semantic classification of unconstrained photos is still an open problem. Existing systems have only used features derived from the image content. However, Exif metadata recorded by the camera provides cues independent of the scene content that can be exploited to improve classification accuracy. Using the problem of indoor-outdoor classification as an example, analysis of metadata statistics for each class revealed that exposure time, flash use, and subject distance are salient cues. We use a Bayesian network to integrate heterogeneous (content-based and metadata) cues in a robust fashion. Based on extensive experimental results, we make two observations: (1) adding metadata to content-based cues gives highest accuracies; and (2) metadata cues alone can outperform content-based cues alone for certain applications, leading to a system with high performance, yet requiring very little computational overhead. The benefit of incorporating metadata cues can be expected to generalize to other scene classification problems.","Layout,
Apertures,
Image classification,
Digital cameras,
Brightness,
Computer science,
Laboratories,
Statistical analysis,
Bayesian methods,
Robustness"
/spl nu/-Quaternion splines for the smooth interpolation of orientations,We present a new method for smoothly interpolating orientation matrices. It is based upon quaternions and a particular construction of /spl nu/-spline curves. The new method has tension parameters and variable knot (time) spacing which both prove to be effective in designing and controlling key frame animations.,
QoS of timeout-based self-tuned failure detectors: the effects of the communication delay predictor and the safety margin,"Unreliable failure detectors have been an important abstraction to build dependable distributed applications over asynchronous distributed systems subject to faults. Their implementations are commonly based on timeouts to ensure algorithm termination. However, for systems built on the Internet, it is hard to estimate this time value due to traffic variations. Thus, different types of predictors have been used to model this behavior and make predictions of delays. In order to increase the quality of service (QoS), self-tuned failure detectors dynamically adapt their timeouts to the communication delay behavior added of a safety margin. In this paper, we evaluate the QoS of a failure detector for different combinations of communication delay predictors and safety margins. As the results show, to improve the QoS, one must consider the relation between the pair predictor/margin, instead of each one separately. Furthermore, performance and accuracy requirements should be considered for a suitable relationship.",
From goals to aspects: discovering aspects from requirements goal models,"Aspect-oriented programming (AOP) has been attracting much attention in the software engineering community by advocating that programs should be structured according to programmer concerns, such as ""efficient use of memory"". However, like other programming paradigms in their early days, AOP hasn't addressed yet earlier phases of software development. In particular, it is still an open question how one identifies aspects early on in the software development process. This work proposes an answer to this question. Specifically, we show that aspects can be discovered during goal-oriented requirements analysis. Our proposal includes a systematic process for discovering aspects from relationships between functional and nonfunctional goals. We illustrate the proposed process with a case study adapted from the literature.","Software systems,
Proposals,
Computer science,
Software engineering,
Programming profession,
Software maintenance"
Rec-I-DCM3: a fast algorithmic technique for reconstructing phylogenetic trees,"Phylogenetic trees are commonly reconstructed based on hard optimization problems such as maximum parsimony (MP) and maximum likelihood (ML). Conventional MP heuristics for producing phylogenetic trees produce good solutions within reasonable time on small datasets (up to a few thousand sequences), while ML heuristics are limited to smaller datasets (up to a few hundred sequences). However, since MP (and presumably ML) is NP-hard, such approaches do not scale when applied to large datasets. In this paper, we present a new technique called Recursive-Iterative-DCM3 (Rec-I-DCM3), which belongs to our family of disk-covering methods (DCMs). We tested this new technique on ten large biological datasets ranging from 1,322 to 13,921 sequences and obtained dramatic speedups as well as significant improvements in accuracy (better than 99.99%) in comparison to existing approaches. Thus, high-quality reconstructions can be obtained for datasets at least ten times larger than was previously possible.","Phylogeny,
Computer science,
Sequences,
Algorithm design and analysis,
Maximum likelihood estimation,
Testing,
Evolution (biology),
History,
Organisms,
Planets"
Explicit proactive handoff with motion prediction for mobile IP,"Mobile IP has been widely accepted, but lacks a fast handoff mechanism. In this paper, we introduce an explicit proactive handoff scheme with motion prediction. Since each user has patterns of movement, a mobile node predicts its future motion and explicitly notifies its old foreign agent which subnet it is likely to handoff to. During a handoff, the old foreign agent duplicates and forwards packets to the predicted subnets. With our scheme, network-layer handoff latency can be reduced to the level of link-layer handoff latency, and the number of packets lost during handoffs is also minimized. With a real network activity trace, we demonstrate that this scheme is able to predict motion accurately, with only a small overhead in bandwidth consumption and computation.","Delay,
Mobile computing,
Bandwidth,
Switches,
Birth disorders,
Computer science,
Computer networks,
Wireless LAN,
Radio link,
Spine"
A robust protocol for proving ownership of multimedia content,"We explore the problem of proving ownership or origin of multimedia content like image/video or audio signals through watermarking. The need for watermarking arises out of the insufficiency of present copyright laws for claiming ownership of digital content. Watermarking schemes however, are threatened by counterfeit attacks, which primarily use the freedom available in choice of signature or choice of the watermarking method. A restrictive protocol for watermarking could go a long way in rendering counterfeit attacks extremely difficult. We suggest a comprehensive protocol as an extension of the one suggested by Craver et al. , that makes it possible for the true owner to claim ownership unambiguously, while making it extremely difficult for a pirate to do so.","Robustness,
Protocols,
Watermarking,
Protection,
Counterfeiting,
Art,
Data encapsulation,
Books,
Digital images,
Computer science"
A centralized key management scheme for hierarchical access control,"Key management schemes are used to provide access control to data streams for legitimate users. The users often have certain partially ordered relations, while data streams also form some partially ordered relations. Previous key management schemes have failed to take into consideration either the user relations or data stream relations. We propose a centralized key management scheme for hierarchical access control that considers both partially ordered users and partially ordered data streams. Our scheme improves the efficiency of key management by encrypting multiple equivalent data streams with a single data encryption key, instead of encrypting each data stream with a unique data encryption key in the multi-group key management scheme (Sun, Y. and Ray Liu, K.J., IEEE INFOCOM, 2004). We develop a simulation model to evaluate the performance of our proposed scheme. Simulation results show that our scheme reduces at least 20% of storage overhead at every user and rekey overhead compared to the multi-group key management scheme.","Access control,
Cryptography,
Permission,
Silver,
Gold,
Finance,
Broadcasting,
Computer science,
Web and internet services,
Teleconferencing"
Test cost reduction through a reconfigurable scan architecture,"Scan-based designs are widely used to keep test generation complexity within practical limits; nevertheless, scan-based design substantially increases test application time and test data volume. A novel scan-based design is proposed to reduce the test cost. The new scan-design exploits the low specified bit density of the test sets. The circular structure of the proposed architecture enables the use of the captured response of the previously applied pattern as a template for the subsequent pattern while allowing the full observation of the captured response. The functionality provided by the new architecture is utilized to update the template quickly to obtain the next pattern. The experimental results show a substantial reduction in test cost, reaching 90% levels.","Costs,
Circuit testing,
Sequential analysis,
Test pattern generators,
Computer architecture,
Controllability,
Observability,
Computer science,
Data engineering,
Design engineering"
Mobile polymorphic applications in ubiquitous computing environments,"Ubiquitous computing envisions an environment where physical and digital devices are seamlessly integrated. Users can access their applications and data anywhere in the environment. Applications are not bound to any single device and can migrate with the user to different environments. Therefore, application mobility is an important aspect of ubiquitous computing. We consider the problem of migrating applications across different ubiquitous computing environments (i.e. across different rooms, buildings or even cities). Migration is a tough problem because different environments have different resources (devices or services) available. The context of the environments may be different as well. Hence, mobile applications must adapt to changing contexts and resource availabilities as they migrate from one environment to the next. We introduce the notion of polymorphic applications, where applications can change their structure in order to adapt to different environments. While the structure of polymorphic applications can change during migration, the functionality and the state of the application are preserved as far as possible. This enables users to perform the same tasks as they move from one environment to the next, seamlessly. We make use of ontologies to ensure that the initial and final structures of a migrating application are semantically similar in terms of functionality and behavior. This paper describes our framework for enabling mobile polymorphic applications.","Mobile computing,
Ubiquitous computing,
Application software,
Availability,
Handheld computers,
Computer science,
Cities and towns,
Ontologies,
Digital cameras,
Pervasive computing"
Assured reconfiguration of embedded real-time software,"It is often the case that safety-critical systems have to be reconfigured during operation because of issues such as changes in the systems operating environment or the failure of software or hardware components. Operational systems exist that are capable of reconfiguration, but previous research and the techniques employed in operational systems for the most part either have not addressed the issue of assurance or have been developed in an ad hoc manner. In this paper we present a comprehensive approach to assured reconfiguration, providing a framework for formal verification that allows the developer of a reconfigurable system to use a set of application-level properties to show general reconfiguration properties. The properties and design are illustrated through an example from NASA's runway incursion prevention system.","Embedded software,
Software systems,
Safety,
Control systems,
Application software,
Computer science,
Programming,
Degradation,
Computer architecture,
Space missions"
Continuous space estimation for WLAN location determination systems,"WLAN location determination systems add to the value of a wireless network by providing the user location without using any extra hardware. Current systems return the estimated user location from a set of discrete locations in the area of interest, which limits the accuracy of such systems to how far apart the selected points are. In this paper, we present two techniques to estimate the user location in the continuous physical space, namely the center of mass technique and time averaging technique. We test the performance of the two techniques in the context of the Horus WLAN location determination system under two different testbeds. Using the center of mass technique, the performance of the Horus system is enhanced by more than 13% for the first testbed and more than 6% for the second testbed. The time-averaging technique enhances the performance of the Horus system by more than 24% for the first testbed and more than 15% for the second testbed. The techniques are general and can be applied to any of the current WLAN location determination systems to enhance their accuracy. Moreover, the two techniques are independent and can be applied together to further enhance the accuracy of the current WLAN location determination systems","Wireless LAN,
System testing,
Wireless networks,
Computer science,
Educational institutions,
Hardware,
Local area networks,
Phase estimation,
History"
Robot navigation using qualitative landmark states from sketched route maps,"The goal of this work is to illustrate and evaluate a novel method for communicating with a mobile robot, namely, by drawing a sketch. The user draws a sketched route map to direct a mobile robot along a specified path. In this paper we focus on the navigation of the sketched path in the real environment. Challenges include sketch inaccuracies such as distortion or abstraction and low sensory resolution of the robot. Our method is based on utilizing spatial relations to extract a sequence of qualitative landmark states from the sketched map, which in turn the robot follows in a real environment to replicate the sketched route. Several examples are included.","Navigation,
Robot kinematics,
Mobile robots,
Robot sensing systems,
Layout,
Robustness,
Computer science,
Spatial resolution,
Personal digital assistants,
Virtual environment"
Hard real-time communication in bus-based networks,"Route selection is an important aspect of the design of real-time systems in which messages might have to travel over multiple hops to reach their destination and multiple paths exist between a source and a destination. The length of a route affects the ability to meet deadlines and greedy routing might leave certain messages with no feasible route. We consider bus-based networks on which periodic message transmissions need to be scheduled and present a technique for synthesizing routes such that all messages meet their deadlines. Our offline technique enables system designers to configure routes in a large-scale embedded system. In our solution, we allow message fragmentation and utilize multiple paths to satisfy the requirements of each message. The routing problem is NP-complete and our approximation algorithm is based on a linear programming formulation. In our methodology, we deal with both earliest deadline first and rate monotonic scheduling at each bus in the system. Apart from point-to-point messages, we discuss scheduling multicast messages to facilitate the publisher/subscriber model. Finally, we also mention some heuristics for online routing which might be of value in soft real-time systems.","Intelligent networks,
Timing,
Routing,
Aerospace electronics,
Real time systems,
Communication system control,
Ethernet networks,
Computer science,
Network synthesis,
Large-scale systems"
Kinetic parameter estimation from renal measurements with a three-headed SPECT system: a Simulation study,"We present here a direct least-squares estimation (DLSE) method for the determination of renal kinetic parameters from sequences of very fast acquisitions performed with a three-headed single photon emission computed tomography (SPECT) system. A simple linear model for the behavior of the radiopharmaceutical, as well as a spatial model for its spatial distribution are defined. The model enables one to estimate the kinetic parameters directly from the projections, once the plasma concentration function is known. A new technique for the accurate reconstruction of time-radioactivity curves based on the direct reconstruction of the region-of-interest contents from a series of data from three-projections is presented. The technique is used to determine the plasma concentration function with a sub-second time resolution. The spatially-variant geometrical response is also included in the model to compensate for the spatial resolution of the SPECT system. Results obtained from simulations are presented. Basic spatial and time features of the simulations are derived from a patient study. Noise and segmentation errors are also simulated. The DLSE method is compared with the conventional one of deriving kinetic parameters from the time series of reconstructed images. The standard deviation of results given by DLSE is less than 2%, whereas with the conventional method it is between 5% and 6%. Within the limit of statistical fluctuations, DLSE results are unbiased whereas those of the conventional method are overestimated by 24%.","Kinetic theory,
Parameter estimation,
Image reconstruction,
Spatial resolution,
Computational modeling,
Single photon emission computed tomography,
Plasma simulation,
Solid modeling,
Image segmentation,
Fluctuations"
Learning polyline maps from range scan data acquired with mobile robots,"Geometric representations of the environment play an important role in mobile robotics as they support various tasks such as motion control and accurate localization. Popular approaches to represent the geometric features of an environment are occupancy grids or line models. Whereas occupancy grids require a huge amount of memory and therefore do not scale well with the size of the environment, line models are unable to correctly represent corners or connections between objects. In this paper we present an algorithm that learns sets of polylines from laser range scans. Starting with an initial set of polylines generated from the range scans it iteratively optimizes these polylines using the Bayesian information criterion. During the optimization process our algorithm utilizes information about the angles between line segments extracted from the original range scans. We present experiments illustrating that our algorithm is able to learn accurate and highly compact polyline maps from laser range data obtained with mobile robots.","Mobile robots,
Iterative algorithms,
Laser modes,
Data mining,
Bayesian methods,
Robot sensing systems,
Computer science,
Mobile computing,
Solid modeling,
Path planning"
Understanding the effects of hotspots in wireless cellular networks,"In this work, we study and quantify the effects of hotspots in wireless cellular networks. Hotspots are caused when the bandwidth resources available at some location in the network are not enough to sustain the needs of the users, which are then blocked or dropped. A deeper understanding of hotspots can help in conducting more realistic simulations and enable improved network design. We identify some causes for the formation of hotspots and based on them, categorize hotspots into three different types: a) capacity based, b) delay based, and c) preferential mobility based. We show how these types have different effects on network performance. We also consider the effects of hotspots from various perspectives such as the number of hotspots, the placement of hotspots, etc. We also develop a fluid flow model and an analytical model to study hotspots. The fluid flow model is surprisingly simple yet effective in helping us understand hotspots and their properties. We also describe an analytical model in which we consider a cell as an M/M/B/B queue. We use these models to substantiate some of the observations from the simulations.","Intelligent networks,
Land mobile radio cellular systems,
Analytical models,
Queueing analysis,
Fluid flow,
Computer science,
Delay,
Bandwidth,
Load management,
Telecommunication traffic"
Characteristics of weighted feature vector in content-based image retrieval applications,"Color and texture feature vectors of an image are always considered to be an important attribute in content-based image retrieval system. Both of these feature vectors of an image can be combined for the performance enhancement of the content-based image retrieval system. One of the standard ways of extracting color feature from an image is to generate a color histogram. Using Haar wavelet or Daubechies' wavelet the texture feature of an image can be extracted. These two feature vectors and the feature vectors in the database are normalized so that the value of a bin is always between [0,1]. During retrieval, both color and texture feature vectors of query image is combined, weighted and compared with the color and texture feature vectors of each of the database images using Manhattan distance metric. The retrieved result is dependent on the weight given to each of the feature vector. We have done a detailed study of the performance of different combination of weights to color (w/sub c/) and texture (w/sub t/) features on a large database of images. Different combination weights are used in for evaluation and the results shows that texture feature vector weight (W/sub t/) in the range of W/sub c/ /spl plusmn/0.1 to w/sub c/ /spl plusmn/0.2 perform better than the other combinations.","Image retrieval,
Content based retrieval,
Image databases,
Information retrieval,
Spatial databases,
Image storage,
Application software,
Computer science,
Feature extraction,
Histograms"
Symbolic model checking of UML statechart diagrams with an integrated approach,"This paper puts forward a new approach for the specification and verification of finite state systems. The design of a system is first specified in UML statechart diagrams, then formalized in the /spl pi/calculus and finally verified automatically by NuSMV. We demonstrate an application of the proposed approach using the SET/A protocol as an example.","Unified modeling language,
Software tools,
Protocols,
Computer science,
Application software,
Yarn,
Software standards,
Computer industry,
Concurrent computing,
Mathematical model"
Interactive teaching of elementary digital logic design with WinLogiLab,"This paper presents an interactive computerized teaching suite developed for the design of combinatorial and sequential logic circuits. This suite fills a perceived gap in the currently available computer-based teaching software, with the purpose of providing alternative-mode subject delivery. The authors were, therefore, prompted to develop a Microsoft-Windows tutorial suite, WinLogiLab, comprising a set of interactive tutorials that show the link between Boolean algebra and digital combinatorial and sequential circuits. The combinatorial tutorials follow the initial design steps: from Boolean algebra, to truth tables, to minimization techniques, to production of the combinatorial circuit in a seamless way. Similarly, the sequential tutorials can design simple finite-state counters and can model more complex finite-state automata.","Computer aided instruction,
Design automation,
Interactive systems,
Finite state machines,
Sequential logic circuits,
Combinational logic circuits,
Boolean algebra,
Electronics engineering education,
Computer science education"
Making the pyramid technique robust to query types and workloads,"The effectiveness of many existing high-dimensional indexing structures is limited to specific types of queries and workloads. For example, while the Pyramid technique and the iMinMax are efficient for window queries, the iDistance is superior for kNN queries. We present a new structure, called the P/sup +/-tree, that supports both window queries and kNN queries under different workloads efficiently. In the P/sup +/-tree, a B/sup +/-tree is employed to index the data points as follows. The data space is partitioned into subspaces based on clustering, and points in each subspace are mapped onto a single dimensional space using the Pyramid technique, and stored in the B/sup +/ -tree. The crux of the scheme lies in the transformation of the data which has two crucial properties. First, it maps each subspace into a hypercube so that the Pyramid technique can be applied. Second, it shifts the cluster center to the top of the pyramid, which is the case that the Pyramid technique works very efficiently. We present window and kNN query processing algorithms for the P/sup +/-tree. Through an extensive performance study, we show that the P/sup +/-tree has considerable speedup over the Pyramid technique and the iMinMax for window queries and outperforms the iDistance for kNN queries.","Robustness,
Hypercubes,
Indexing,
Computer science,
Query processing,
Clustering algorithms,
Multidimensional systems,
Biomedical imaging,
Geographic Information Systems,
Data analysis"
Multi-agent patrolling with reinforcement learning,,"Learning,
Surveillance,
Robot kinematics,
Computer networks,
Permission,
Computer science,
Computational modeling,
Computer simulation,
Distributed computing,
Legged locomotion"
Distributed Ant: a system to support application deployment in the grid,"e-Science has much to benefit from the emerging field of grid computing. However, construction of e-science grids is a complex and inefficient undertaking. In particular, deployment of user applications can present a major challenge due to the scale and heterogeneity of the grid. In spite of this, deployment is not supported by current grid computing middleware or configuration management systems, which focus on a super-user approach to application management. Hence, individual users with limited resource control deploy applications manually, which is not a grid scalable solution. This paper presents our motivation, design and implementation of a grid scalable, user-oriented, secure application deployment system, Distributed Ant (DistAnt). DistAnt extends the Ant build file environment to provide a flexible procedural deployment description and implements a set of deployment services.","Grid computing,
Application software,
Resource management,
Middleware,
Quantum computing,
Computer science,
Software engineering,
Pervasive computing,
Ubiquitous computing,
Computer architecture"
A variable rate execution model,"We present a task model for adaptive real-time tasks in which a task's execution rate requirements are allowed to change at any time. The model, variable rate execution (VRE), is an extension of the rate-based execution (RBE) model. We relax the constant execution rate assumption of canonical real-time task models by allowing both the worst case execution time (WCET) and the period to be variable. The VRE model also supports tasks joining and leaving the system at any time. A schedulability condition for the VRE task model is presented that can be used as an online admission control test for the acceptance of new tasks or rate changes. Finally, a VRE scheduler was implemented in Linux as a loadable module, and several experiments demonstrate its correctness and analyze the overhead.",
A general architecture to support mobility in learning,"A rather new tendency in distance learning is the usage of mobile and wireless technologies to support learners and educators. In this paper we present an architecture, where the functionalities of e-learning platform are presented as Web services and on top of it a mobile learning management system is taking the responsibilities of adapting those services for the mobile users and for providing additional mobile specific services. Such a system should have three main functionalities - ""context discovery"", ""mobile content management and adaptation"" and ""packaging and synchronization"".","Electronic learning,
Computer aided instruction,
Content management,
Engines,
Informatics,
Service oriented architecture,
Web services,
Computer science education,
Management training,
Authorization"
Signaling methods for multimedia steganography,"Conventional communication methods employ a wide variety of signaling techniques that essentially map a bit sequence to a real-valued sequence (which is a representation of a point in the signal constellation). The real-valued sequence is in turn transmitted over a communications channel. However, communication techniques for the purpose of multimedia steganography or data hiding have to transmit the real-valued sequence corresponding to a point in the signal constellation superimposed on the original content (without affecting the fidelity of the original content noticeably). In this paper, we explore practical solutions for signaling methods for multimedia steganography. Data hiding is seen as a sophisticated signaling technique using a periodic signal constellation. We propose such a signaling method and present both theoretical and simulated evaluations of its performance in an additive noise scenario. The problem of optimal choice of the parameters for the proposed technique is also explored, and solutions are presented.","Steganography,
Data encapsulation,
Constellation diagram,
Detectors,
Data mining,
Communication channels,
Additive noise,
Art,
Distortion,
Computer science"
Evaluation of a human-robot interface: development of a situational awareness methodology,This paper outlines a methodology to evaluate supervisory user interfaces for robotic vehicles based on an assessment of situational awareness. The results of an initial experiment are discussed. The evaluation method will be validated in a future experiment that will also result in a benchmarked user interface.,
Semi-fragile watermarking scheme for authentication of JPEG images,"With the increasing popularity of JPEG images, a need arises to devise effective watermarking techniques which consider JPEG compression as an acceptable manipulation. In this paper, we present a semi-fragile watermarking scheme which embeds a watermark in the quantized DCT domain. It is tolerant to JPEG compression to a pre-determined lowest quality factor, but is sensitive to all other malicious attacks, either in spatial or transform domains. Feature codes are extracted based on the relative sign and magnitudes of coefficients, and these are invariant due to an important property of JPEG compression. The employment of a nine-neighborhood mechanism ensures that non-deterministic block-wise dependence is achieved. Analysis and experimental results are provided to support the effectiveness of the scheme.","Watermarking,
Authentication,
Transform coding,
Image coding,
Robustness,
Q factor,
Image storage,
Computer science,
Discrete cosine transforms,
Feature extraction"
Segmentation of the breast region in mammograms using snakes,"The largest single feature on a mammogram is the skin-air interface, or breast contour. Extraction of the breast contour is useful for a number of reasons. Foremost it allows the search for abnormalities to be limited to the region of the breast without undue influence from the background of the mammogram. Segmentation of the breast-region from the background is made difficult by the tapering nature of the breast, such that the breast contour lies in between the soft-tissue and the non-breast region. This paper explores the application of snakes to the problem of automatically extracting the breast region in mammograms.","Breast cancer,
Mammography,
Image analysis,
Computer interfaces,
Information science,
Data mining,
Diseases,
Cancer detection,
Computer aided diagnosis,
Image processing"
Path planning for minimal energy curves of constant length,"In this paper we present a new path planning technique for a flexible wire. We first introduce a new parametrization designed to represent low-energy configurations. Based on this parametrization we can find curves that satisfy endpoint constraints. Next, we present three different techniques for minimizing energy within the self-motion manifold of the curve. We introduce a local planner to find smooth minimal energy deformations for these curves that can be used by a general path planning algorithm. Using a simplified model for obstacles, we can find minimal energy curves of fixed length that pass through specified tangents at given control points. Finally, we show that the parametrization introduced in this paper is a good approximation of true minimal energy curves. Our work has applications in surgical suturing and snake-like robots.","Path planning,
Shape,
Capacitive sensors,
Surgery,
Computer science,
Wire,
Orbital robotics,
Cables,
Robot sensing systems"
Coverage issue in sensor networks with adjustable ranges,"In this paper, we study the problem of maintaining sensing coverage by keeping a small number of active sensor nodes and a small amount of energy consumption in wireless sensor networks. This paper extends a result from [21] where only uniform sensing range among all sensors is used. We adopt an approach that allows non-uniform sensing ranges for different sensors. As opposed to the uniform sensing range node scheduling model in [21], two new energy-efficient models of different sensing ranges are proposed. Our objective is to minimize the overlapped sensing area of sensor nodes, thus to reduce the overall energy consumption by sensing to prolong the whole network¿s life time, and at the same time to achieve the high ratio of coverage. Extensive simulation is conducted to verify the effectiveness of our node scheduling models.",
IDR: an intrusion detection router for defending against distributed denial-of-service (DDoS) attacks,"Distributed denial-of-service (DDoS) attack has turned into one of the major security threats in recent years. Usually the only solution is to stop the services or shut down the victim and then discard the attack traffic only after the DDoS attack characteristics (such as the destination ports of the attack packets) are known. In this paper, we introduce a generic DDoS attack detection mechanism as well as the design and setup of a testbed for performing experiments and analysis. Our results showed that the mechanism can detect DDoS attack. This enables us to proceed to the next steps of packet classification and traffic control.",
Gateway placement for latency and energy efficient data aggregation [wireless sensor networks],We propose the use of multiple gateways to significantly reduce latency and energy consumption in multi-hop wireless sensor networks during data aggregation. We have derived efficient integer linear programming formulations as well as a novel negative selection statistically-tuned heuristics. The heuristics are based on newly developed relaxation based lower bounds that are also used to quantify the effectiveness of the proposed heuristics. Our simulation study indicates that the use of gateways can often reduce latency and energy consumption by several times.,
Cell nuclei segmentation using fuzzy logic engine,The task of segmenting cell nuclei in microscope images is a classical image analysis problem. The accurate nuclei segmentation may contribute to development of successful system which automate the analysis of microscope images for pathology detection. In this article we describe a method for semi-supervised training of fuzzy logic engine. The fuzzy logic engine is applied to connect a set of parameters proven to be important for nucleus segmentation. In addition each parameter for itself is detected using a set of fuzzy logic rules. We present results of nuclei segmentation using fuzzy logic set of rules.,
Exponentially many steps for finding a Nash equilibrium in a bimatrix game,"The Lemke-Howson algorithm is the classical algorithm for the problem NASH of finding one Nash equilibrium of a bimatrix game. It provides a constructive and elementary proof of existence of an equilibrium, by a typical ""directed parity argument"", which puts NASH into the complexity class PPAD. This paper presents a class of bimatrix games for which the Lemke-Howson algorithm takes, even in the best case, exponential time in the dimension d of the game, requiring /spl Omega/((/spl theta//sup 3/4/)/sup d/) many steps, where /spl theta/ is the golden ratio. The ""parity argument"" for NASH is thus explicitly shown to be inefficient. The games are constructed using pairs of dual cyclic polytopes with 2d suitably labeled facets in d-space.","Nash equilibrium,
Game theory,
Computer science,
Algorithm design and analysis,
Linear programming,
Mathematics,
IP networks,
Electronic commerce,
Routing,
Complexity theory"
A general framework for learning rules from data,"With the aim of getting understandable symbolic rules to explain a given phenomenon, we split the task of learning these rules from sensory data in two phases: a multilayer perceptron maps features into propositional variables and a set of subsequent layers operated by a PAC-like algorithm learns Boolean expressions on these variables. The special features of this procedure are that: i) the neural network is trained to produce a Boolean output having the principal task of discriminating between classes of inputs; ii) the symbolic part is directed to compute rules within a family that is not known a priori; iii) the welding point between the two learning systems is represented by a feedback based on a suitability evaluation of the computed rules. The procedure we propose is based on a computational learning paradigm set up recently in some papers in the fields of theoretical computer science, artificial intelligence and cognitive systems. The present article focuses on information management aspects of the procedure. We deal with the lack of prior information about the rules through learning strategies that affect both the meaning of the variables and the description length of the rules into which they combine. The paper uses the task of learning to formally discriminate among several emotional states as both a working example and a test bench for a comparison with previous symbolic and subsymbolic methods in the field.","Computer networks,
Multilayer perceptrons,
Neural networks,
Welding,
Learning systems,
Output feedback,
Neurofeedback,
Computer science,
Artificial intelligence,
Information management"
The base-line DataFlow system of the ATLAS trigger and DAQ,"The base-line design and implementation of the ATLAS DAQ DataFlow system is described. The main components of the DataFlow system, their interactions, bandwidths, and rates are discussed and performance measurements on a 10% scale prototype for the final ATLAS TDAQ DataFlow system are presented. This prototype is a combination of custom design components and of multithreaded software applications implemented in C++ and running in a Linux environment on commercially available PCs interconnected by a fully switched gigabit Ethernet network.","Data acquisition,
Large Hadron Collider,
Detectors,
Delay,
Software prototyping,
Field programmable gate arrays,
Optical arrays,
Protons,
Physics,
Event detection"
A 19.2 GOPS mixed-signal filter with floating-gate adaptation,"We have built a 48-tap, mixed-signal adaptive FIR filter with 8-bit digital input and an analog output with 10 bits of resolution. The filter stores its tap weights in nonvolatile analog memory cells using synapse transistors, and adapts using the least mean square (LMS) algorithm. We run the input through a digital tapped delay line, multiply the digital words with the analog tap weights using mixed-signal multipliers, and adapt the tap coefficients using pulse-based feedback. The accuracy of the weight updates exceeds 13 bits. The total die area is 2.6 mm/sup 2/ in a 0.35-/spl mu/m CMOS process. The filter delivers a performance of 19.2 GOPS at 200 MHz, and consumes 20 mW providing a 6-mA differential output current.","Finite impulse response filter,
Least squares approximation,
Nonvolatile memory,
Adaptive filters,
Adaptive systems,
Computer science,
Bridge circuits,
MOSFET circuits,
Application software,
Circuit noise"
Genetic algorithm based synthesis of multi-output ternary functions using quantum cascade of generalized ternary gates,"Ternary quantum circuits have recently been introduced to help reduce the size of multi-valued logic for multi-level quantum computing systems. However, synthesizing these quantum circuits is not easy. We describe a new genetic algorithm based synthesizer for ternary quantum circuits. Our results show some of the synthesized circuits use fewer gates than previously published methods.","Genetic algorithms,
Quantum computing,
Circuit synthesis,
Multivalued logic,
Polynomials,
Algorithm design and analysis,
Galois fields,
Computer science,
DH-HEMTs,
Synthesizers"
Bayesian Computation in Recurrent Neural Circuits,"A large number of human psychophysical results have been successfully explained in recent years using Bayesian models. However, the neural implementation of such models remains largely unclear. In this article, we show that a network architecture commonly used to model the cerebral cortex can implement Bayesian inference for an arbitrary hidden Markov model. We illustrate the approach using an orientation discrimination task and a visual motion detection task. In the case of orientation discrimination, we show that the model network can infer the posterior distribution over orientations and correctly estimate stimulus orientation in the presence of significant noise. In the case of motion detection, we show that the resulting model network exhibits direction selectivity and correctly computes the posterior probabilities over motion direction and position. When used to solve the well-known random dots motion discrimination task, the model generates responses that mimic the activities of evidence-accumulating neurons in cortical areas LIP and FEF. The framework we introduce posits a new interpretation of cortical activities in terms of log posterior probabilities of stimuli occurring in the natural world.",
A taxonomy of broadcast indexing schemes for multi channel data dissemination in mobile databases,"Data broadcasting strategy is known as a scalable way to disseminate information to mobile users. However, with a very large set of broadcast items, the query access time of mobile clients raise accordingly, due to high waiting time for mobile clients to find their data of interest. One possible solution is to split the database information into several broadcast channels. In this paper, we introduce taxonomy of index dissemination for multibroadcast channel based on B* tree structure. We consider three indexing schemes namely: (i) nonreplicated indexing scheme (NRI), (ii) partially-replicated indexing scheme (PRI), and (iii) fully-replicated indexing scheme (FRI). Simulation model is developed to find out the access time performance of each scheme.","Taxonomy,
Broadcasting,
Indexing,
Databases,
Mobile computing,
Portable computers,
Batteries,
Energy consumption,
Switches,
Computer science"
Characterization of a 16-bit threshold logic single-electron technology adder,Single electron technology (SET) is one of the future technologies distinguished by its small and low-power devices. SET also provides simple and elegant solutions for threshold-logic gates (TLGs). This paper presents the design of an optimal TLG adder implemented in SET. This 16-bit Kogge-Stone style adder was fully designed and simulated using a Monte Carlo simulator. The simulation results give a quantitative estimate of both the delay and the power dissipation of the adder. The characteristics of our adder are compared with recent results estimating the energy-delay characteristics of advanced CMOS adders.,"Adders,
CMOS technology,
Power dissipation,
Logic devices,
Delay estimation,
Computer science,
Electrons,
Monte Carlo methods,
Costs,
Circuits"
Experience with teaching black-box testing in a computer science/software engineering curriculum,"Software testing is a popular and important technique for improving software quality. There is a strong need for universities to teach testing rigorously to students studying computer science or software engineering. This paper reports the experience of teaching the classification-tree method as a black-box testing technique at the University of Melbourne, Melbourne, Australia, and Swinburne University of Technology, Melbourne, Australia. It aims to foster discussion of appropriate teaching methods of software testing.","Computer science education,
Software testing,
Software engineering,
Courseware"
On new fuzzy morphological associative memories,"In this paper, the new fuzzy morphological associative memories (FMAMs) based on fuzzy operations (/spl and/,/spl middot/) and (/spl or/,/spl middot/) are presented. FMAM with (/spl or/,/spl middot/) is extremely robust for dilative noise and FMAM with (/spl and/,/spl middot/) is extremely robust for erosive noise. Autoassociative FMAM has the unlimited storage capability and can converge in one step. The convex autoassociative FMAM can be used to achieve a reasonable tradeoff for the mixed noise. Finally, comparisons between autoassociative FMAM and the famous FAM are discussed. FMAM, as another new encoding way of fuzzy rules, still has a multitude of open problems worthy to explore in the future.","Fuzzy neural networks,
Associative memory,
Noise robustness,
Fuzzy control,
Image coding,
Convergence,
Neural networks,
Computer science,
Fuzzy logic,
Subspace constraints"
ACT: an adaptive CORBA template to support unanticipated adaptation,"We propose an Adaptive CORBA Template (ACT), which enables run-time improvements to CORBA applications in response to unanticipated changes in either their functional requirements or their execution environments. ACT enhances CORBA applications by transparently weaving adaptive code into their object request brokers (ORBs) at run time. The woven code intercepts and adapts the requests, replies, and exceptions that pass through the ORBs. Specifically, ACT can be used to develop an object-oriented framework in any language that supports dynamic loading of code and can be applied to any CORBA ORB that supports portable interceptors. Moreover, ACT can be used to support interoperation among otherwise incompatible adaptive CORBA frameworks. To evaluate the performance and functionality of ACT, we implemented a prototype in Java. Our experimental results show that the overhead introduced by the ACT infrastructure is negligible, while the adaptations offered are highly flexible.","Adaptive coding,
Java,
Quality of service,
Fault tolerance,
Application software,
Middleware,
Distributed computing,
Software engineering,
Computer science,
Runtime"
A non-parametric blur measure based on edge analysis for image processing applications,"A nonparametric image blur measure is presented. The measure is based on edge analysis and is suitable for various image processing applications. The proposed measure for any edge point is obtained by combining the standard deviation of the edge gradient magnitude profile and the value of the edge gradient magnitude using a weighted average. The standard deviation describes the width of the edge, and its edge gradient magnitude is also included to make the blur measure more reliable. Moreover, the value of the weight is related to image contrast and can be calculated directly from the image. Experiments on natural scenes indicate that the proposed technique can effectively describe the blurriness of images in image processing applications.",
A GRASP algorithm for the multi-objective knapsack problem,"In this article, we propose a greedy randomized adaptive search procedure (GRASP) to generate a good approximation of the efficient or Pareto optimal set of a multi-objective combinatorial optimization problem. The algorithm is based on the optimization of all weighted linear utility functions. In each iteration, a preference vector is defined and a solution is built considering the preferences of each objective. The found solution is submitted to a local search trying to improve the value of the utility function. In order to find a variety of efficient solutions, we use different preference vectors, which are distributed uniformly on the Pareto frontier. The proposed algorithm is applied for the 0/1 knapsack problem with r = 2, 3, 4 objectives and n = 250, 500, 750 items. The quality of the approximated solutions is evaluated comparing with the solutions given by three genetic algorithms from the literature.","Genetic algorithms,
Simulated annealing,
Pareto optimization,
Vectors,
Hardware,
Software systems,
Costs,
Large-scale systems,
Bibliographies,
Testing"
THROWS: an architecture for highly available distributed execution of Web services compositions,"Web services emergence has triggered extensive research efforts. Currently, there is a trend towards deploying business processes as an orchestration of Web services compositions. Given that Web services are inherently loosely-coupled and are primarily built independently, they are most likely to have characteristics (e.g., transaction support, failure recovery, access policies) that might not be compliant with each other. It follows that guaranteeing the reliability and availability of the obtained Web services compositions is a challenging issue. Aligned with this tendency, we focus on the availability and reliability of Web services compositions. Specifically, in this paper, we propose THROWS, an architecture for highly available distributed execution of Web services compositions. In THROWS architecture, the execution control is hierarchically delegated among dynamically discovered engines. The progress of the compositions execution by several distributed engines is continuously captured. Moreover, the Web services compositions executed through the architecture we propose are previously specified as an hierarchy of arbitrary-nested transactions. These transactions execution is provided with retrial and compensation mechanisms which allow the highly available Web services compositions execution.",
Distributed hybrid earthquake engineering experiments: experiences with a ground-shaking grid application,"Earthquake engineers have traditionally investigated the behavior of structures with either computational simulations or physical experiments. Recently, a new hybrid approach has been proposed that allows tests to be decomposed into independent substructures that can be located at different test facilities, tested separately, and integrated via a computational simulation. We describe a grid-based architecture for performing such novel distributed hybrid computational/physical experiments. We discuss the requirements that underlie this extremely challenging application of grid technologies, describe our architecture and implementation, and discuss our experiences with the application of this architecture within an unprecedented earthquake engineering test that coupled large-scale physical experiments in Illinois and Colorado with a computational simulation. Our results point to the remarkable impacts that grid technologies can have on the practice of engineering, and also contribute to our understanding of how to build and deploy effective grid applications.",
Empirical study of session-based workload and reliability for Web servers,"The growing availability of Internet access has led to significant increase in the use of World Wide Web. If we are to design dependable Web-based systems that deal effectively with the increasing number of clients and highly variable workload, it is important to be able to describe the Web workload and errors accurately. In this paper we focus on the detailed empirical analysis of the session-based workload and reliability based on the data extracted from actual Web logs often Web servers. First, we address the data collection process and describe the methods for extraction of workload and error data from Web log files. Then, we introduce and analyze several intra-session and inter-session metrics that collectively describe Web workload in terms of user sessions. Furthermore, we analyze Web error characteristics and estimate the request-based and session-based reliability of Web servers. Finally, we identify the invariants of the Web workload and reliability that apply through all data sets considered. The results presented in this paper show that session-based workload and reliability are better indicators of the users perception of the Web quality than the request-based metrics and provide more useful measures for tuning and maintaining of the Web servers.","Web server,
Web sites,
Data mining,
Availability,
Client-server systems,
Web and internet services,
Network servers,
Computer science,
Error analysis,
Maintenance"
Modeling the social & technical processes of interorganizational information integration,"Government leaders and IT executives increasingly recognize that interorganizational information integration (III) is a critical and complex process. Due to the need for integrated information at all levels of government, interorganizational information integration can no longer be pursued through ad hoc approaches that primarily rely on intuitive understandings of the way government operates by Boar (1999). This paper presents an effort currently underway to model the social and technical processes of interorganizational information integration to improve our understanding of information system development and of interorganizational collaboration. This research seeks to enhance both the conceptual and practical models of III by building new understanding of the interaction among the social and technical processes in interorganizational information integration.","Government,
Business,
Companies,
Information systems,
Information technology,
Online Communities/Technical Collaboration,
Buildings,
Logic,
Computer architecture,
Mediation"
Peer-to-peer information sharing in a mobile ad hoc environment,"With the rapidly increasing adoption of more and more powerful wireless-enabled personal mobile devices, users are facing new opportunities and challenges in making connections with other users in order to share relevant information. This is especially true, given the dynamic nature of potential interactions between typical mobile users. Peer-to-peer applications have established themselves as a popular and effective method for information sharing in static environments, and our research is examining the issues involved in peer-to-peer application deployments for dynamic mobile ad hoc environments. The natural context of direct local proximity between peer devices coupled with user preferences as a context for information filtering provides a powerful mechanism for dynamic and opportunistic information exchange. In this paper, we demonstrate how peer-to-peer protocols can be successfully implemented in a mobile ad hoc environment, in order to enable information sharing applications. We discuss the issues involved in the design and implementation of mobile deployments of peer-to-peer applications, and we illustrate these approaches in an information sharing application for Bluetooth-enabled mobile users.","Peer to peer computing,
Bluetooth,
Protocols,
Internet,
Mobile computing,
Computer science,
Educational institutions,
Software systems,
Information systems,
Information filtering"
Diagnosis of scan-chains by use of a configurable signature register and error-correcting codes,"In this paper a new diagnosis method for scan designs with many scan-paths based on error correcting linear block codes with N information bits and K control bits is proposed, where N is the number of scan-paths. The new approach can be implemented on a modified STUMPS-architecture. In diagnosis mode the test has K times to be repeated. In the K repetitions of the test the outputs of the scan-paths are connected to a configurable signature register (with disconnected feedback logic) according to the coefficients of the K syndrome equations of the code. By monitoring the one-dimensional output sequence of the configurable signature register the failing scan-cells in the different scan-paths can be identified with the resolution of the selected error correcting code. Since for the relevant codes, e.g. (shortened) Hamming codes, T-error correcting BCH-code, the ratio K/N decreases very fast with an increasing number N the method is useful for a large number of scan-paths.","Error correction codes,
Circuit testing,
Logic testing,
Output feedback,
Circuit faults,
Fault diagnosis,
Automatic testing,
Registers,
Computer science,
Design methodology"
Analysis of grid service composition with BPEL4WS,"The open grid services infrastructure (OGSI) defines a distributed system framework by integrating grid and Web services technologies to facilitate resource sharing. In OGSI, Web services are supplemented with additional features in order to meet the requirements of grid computing. However, the issue of grid service composition is not well addressed in the OGSI framework. We apply BPEL4WS (business process execution language for Web services) as a business workflow description language for the composition of grid services. We provide an in depth analysis of BPEL4WS and OGSI in terms of their similarities and differences in areas such as life cycle management, Web service instantiation and instance group management. Based on our analysis we propose a high-level architecture to compliment OGSI with BPEL4WS for defining process workflow among grid services. We describe a prototype system which shows how the proposed architecture can be used in modelling or orchestrating grid services with BPEL4WS.","Web services,
Resource management,
Simple object access protocol,
Computer science,
Technology management,
Prototypes,
Web and internet services,
XML,
Standards development,
Chaos"
WS-FIT: a tool for dependability analysis of Web services,This work provides an overview of fault injection techniques and their applicability to testing SOAP RPC based Web service systems. We also give a detailed example of the WS-FIT package and use it to detect a problem in a Web service based system.,"Web services,
System testing,
Measurement techniques,
Computer science,
Application software,
Software testing,
Fault tolerant systems,
Performance evaluation,
Hardware,
Simple object access protocol"
ILP formulations and optimal solutions for the RWA problem,"We present a review of the various integer linear programming (ILP) formulations that have been proposed for the routing and wavelength assignment problem in WDM optical networks with a unified and simplified notation. We consider both symmetrical and asymmetrical traffic matrices. We propose a new formulation for symmetrical traffic. We show that all formulations proposed under asymmetrical traffic assumptions are equivalent (i.e. same optimal value for their continuous relaxations) although their number of variables and constraints differ. We propose an experimental comparison of various lower and upper bounds with the objective of minimizing the blocking rate, and show that several benchmark problems proposed by Krishnaswamy and Sivarajan (2001) can be solved exactly or with a fairly high precision.","Telecommunication traffic,
Symmetric matrices,
Optical fiber networks,
Upper bound,
Wavelength routing,
Wavelength division multiplexing,
Traffic control,
Wavelength assignment,
WDM networks,
Computer science"
Roles for agent assistants in field science: understanding personal projects and collaboration,"A human-centered approach to computer systems design involves reframing analysis in terms of the people interacting with each other. The primary concern is not how people can interact with computers, but how work systems (facilities, tools, roles, and procedures) can be designed to help people pursue their personal projects, as they work independently and collaboratively. Two case studies provide empirical requirements. First, an analysis of astronaut interactions with CapCom on Earth during one traverse of Apollo 17 shows what kind of information was conveyed and what might be automated today. A variety of agent and robotic technologies are proposed that deal with recurrent problems in communication and coordination during the analyzed traverse. Second, an analysis of biologists and a geologist working at Haughton Crater in the High Canadian Arctic reveals how work interactions between people involve independent personal projects, sensitively coordinated for mutual benefit. In both cases, an agent or robotic system's role would be to assist people, rather than collaborating, because today's computer systems lack the identity and purpose that consciousness provides.","Collaboration,
Collaborative work,
Robot sensing systems,
Robot kinematics,
System analysis and design,
Collaborative tools,
Information analysis,
Earth,
Robotics and automation,
Space technology"
Optimality and scalability study of existing placement algorithms,"Placement is an important step in the overall IC design process in deep submicron technologies, as it defines the on-chip interconnects which have become the bottleneck in determining circuit performance. The rapidly increasing design complexity, combined with the demand for the capability of handling nearly flattened designs for physical hierarchy generation, poses significant challenges to existing placement algorithms. There are very few studies dedicated to understanding the optimality (i.e., the comparison of the solution of an algorithm to the optimal solution) and scalability (i.e., the analysis of the degradation of the performance of an algorithm as the input size of the problem increases) of placement algorithms, due to the limited sizes of existing benchmarks and limited knowledge of optimal solutions. The contribution of this work includes three parts. 1) We implemented an algorithm [Placement Examples with Known Optimal (PEKO) algorithm] for generating synthetic benchmarks that have known optimal wirelengths and can match any given net degree distribution profile. 2) Using benchmarks of 10 k to 2 M placeable modules with known optimal solutions, we studied the optimality and scalability of four state-of-the-art placers, Dragon (Wang et al., 2000), Capo (Caldwell et al., 2000), mPL (Chan et al., 2000), and mPG (Chang et al., 2002) from academia, and a leading edge industrial placer, QPlace (Cadence 1999) from Cadence. For the first time our study reveals the gap between the results produced by these tools versus true optimal solutions. The wirelengths produced by these tools are 1.59 to 2.40 times the optimal in the worst cases, and are 1.43 to 2.12 times the optimal on average. As for scalability, the average solution quality of each tool deteriorates by an additional 9% to 17% when the problem size increases by a factor of ten. These results indicate significant room for improvement in existing placement algorithms. 3) We studied the impact of nonlocal nets on the quality of the placers by extending the PEKO algorithm (PEKU algorithm) to generate synthetic placement benchmarks with a known upper bound of the optimal wirelength. For these benchmarks, the wirelengths produced by these tools are 1.75 to 2.18 times the wirelength upper bound in the worst case, and are 1.62 to 2.07 times the wirelength upper bound on average. Moreover, in our study we found that the effectiveness of the algorithms varies for circuits with different characteristics.","Scalability,
Algorithm design and analysis,
Upper bound,
Integrated circuit interconnections,
Circuit optimization,
Computer science,
Process design,
Performance analysis,
Degradation,
Design optimization"
Optimising expanding ring search for multi-hop wireless networks,"Expanding ring search (ERS) is a widely used technique to reduce broadcast overhead in multi-hop wireless networks (e.g., ad-hoc and sensor networks). ERS works by searching successively larger areas in the network centred around the source of broadcast. Network-wide broadcast is initiated only if L successive searches fail. This paper explores if there exists an optimal L that would minimise the broadcast cost of ERS. A theoretical model is developed to analyse the expected broadcast cost as a function of L. Using this model, we show that an optimal L exists for any random network topology. The analytical results are validated through extensive numerical experiments that consider a large number of random network topologies of varying sizes and hop lengths. By tuning the parameter L to the optimum value, broadcast cost can be reduced up to 52% depending on the topology.",
Large-scale fluid-structure interaction simulations,"Combining computational-science disciplines, such as in fluid-structure interaction simulations, introduces a number of problems. The authors offer a convenient and cost-effective approach for coupling computational fluid dynamics (CFD) and computational structural dynamics (CSD) codes without rewriting them. With the advancement of numerical techniques and the advent, first, of affordable 3D graphics workstations and scalable compute servers, and, more recently, PCs with sufficiently large memory and 3D graphics cards, public-domain and commercial software for each of the computational core disciplines has matured rapidly and received wide acceptance in the design and analysis process. Most of these packages are now at the threshold mesh generation pre-processor. This has prompted the development of the next logical step: multidisciplinary links of codes, a trend that is clearly documented by the growing number of publications and software releases in this area. In this paper, we concentrate on fluid-structure and fluid-structure-thermal interaction, in which changes of geometry due to fluid pressure, shear, and heat loads considerably affect the flowfield, changing die loads in turn. Problems in this category include: steady-state aerodynamics of wings under cruise conditions; aeroelasticity of vibrating - that is, elastic - structures such as flutter and buzz (aeroplanes and turbines), galloping (cables and bridges), and maneuvering and control (missiles and drones); weak and nonlinear structures, such as wetted membranes (parachutes and tents) and biological tissues (hearts and blood vessels); and strong and nonlinear structures, such as shock-structure interaction (command and control centers, military vehicles) and hypersonic flight vehicles.","Large-scale systems,
Computational fluid dynamics,
Computational modeling,
Aerodynamics,
Graphics,
Vehicle dynamics,
Workstations,
Personal communication networks,
Process design,
Packaging"
Class decomposition for GA-based classifier agents - a Pitt approach,"This paper proposes a class decomposition approach to improve the performance of GA-based classifier agents. This approach partitions a classification problem into several class modules in the output domain, and each module is responsible for solving a fraction of the original problem. These modules are trained in parallel and independently, and results obtained from them are integrated to form the final solution by resolving conflicts. Benchmark classification data sets are used to evaluate the proposed approaches. The experiment results show that class decomposition can help achieve higher classification rate with training time reduced.",
A framework for the coordination of legged robot gaits,"This paper introduces a framework for representing, generating, and then tuning gaits of legged robots. We introduce a convenient parametrization of gait generators as dynamical systems possessing designer specified stable limit cycles over an appropriate torus. This parametrization affords a continuous selection of operation within a coordination design plane, inspired by biology, spanned by axes that determine the mix of ""feedforward/feedback"" and ""centralized/decentralized"" control. Tuning the gait generator parameters through repeated physical experiments with our robot hexapod, RHex, determines the appropriate operating point - the mix of feedback and degree of control decentralization - to achieve significantly increased performance relative to the centralized feedforward operating point that has governed its previous behavior. The present preliminary experiments with these new gaits suggest that they may permit for the first time locomotion over extremely rough terrain that is almost as reliable, rapid, and energy efficient as the very fastest or most efficient outcomes centralized feedforward gaits can achieve on level ground.","Legged locomotion,
Robot kinematics,
Leg,
Hip,
Clocks,
Computer science,
Feeds,
Feedback,
Mobile robots,
Open loop systems"
Systematic authentication codes from highly nonlinear functions,"Recently, highly nonlinear functions have been successfully employed to construct authentication codes with and without secrecy. In this paper, we construct four classes of systematic authentication codes from perfect nonlinear functions and almost-perfect nonlinear functions. The systematic authentication codes presented in this paper are either better than existing codes or as good as the best codes known.","Authentication,
Transmitters,
Cryptography,
State-space methods,
Communication channels,
Councils,
Laboratories,
Computer science,
Mathematics"
A comparison of application-level and router-assisted hierarchical schemes for reliable multicast,"One approach to achieving scalability in reliable multicast is to use a hierarchy. A hierarchy can be established at the application level, or by using router-assist. With router-assist we have more fine-grain control over the placement of error-recovery functionality, therefore, a hierarchy produced by assistance from the routers is expected to have better performance. In this paper, we test this hypothesis by comparing two schemes, one that uses an application-level hierarchy (ALH) and another that uses router-assisted hierarchy (RAH). Contrary to our expectations, we find that the qualitative performance of ALH is comparable to RAH. We do not model the overhead of creating the hierarchy nor the cost of adding router-assist to the network. Therefore, our conclusions inform rather than close the debate of which approach is better.",
Learning continuous time Markov chains from sample executions,"Continuous-time Markov Chains (CTMCs) are an important class of stochastic models that have been used to model and analyze a variety of practical systems. In this paper we present an algorithm to learn and synthesize a CTMC model from sample executions of a system. Apart from its theoretical interest, we expect our algorithm to be useful in verifying black-box probabilistic systems and in compositionally verifying stochastic components interacting with unknown environments. We have implemented the algorithm and found it to be effective in learning CTMCs underlying practical systems from sample runs.","Stochastic systems,
Hidden Markov models,
System testing,
Software systems,
Performance analysis,
Computer bugs,
Machine learning algorithms,
Computer science,
Stochastic processes,
Algebra"
A parallel Fuzzy C-Mean algorithm for image segmentation,"This paper proposes a parallel Fuzzy C-Mean (FCM) algorithm for image segmentation. The sequential FCM algorithm is computationally intensive and has significant memory requirements. For many applications such as medical image segmentation and geographical image analysis that deal with large size images, sequential FCM is very slow. In our parallel FCM algorithm, dividing the computations among the processors and minimizing the need for accessing secondary storage, enhance the performance and efficiency of image segmentation task as compared to the sequential algorithm.","Image segmentation,
Clustering algorithms,
Partitioning algorithms,
Fuzzy sets,
Image storage,
Image recognition,
Image processing,
Prototypes,
Computer science,
Application software"
The small world of software reverse engineering,Research in maintenance and reengineering has flourished and evolved into a central part of software engineering research worldwide. We have a look at this research community through the publications of its members in several international conferences. We analyze our results using various graph and text mining techniques. We contrast our findings to other research communities.,"Reverse engineering,
Collaboration,
Software engineering,
Software maintenance,
Software architecture,
Computer science,
Text mining,
Social network services,
Computer industry,
History"
HARMONICA: enhanced QoS support with admission control for IEEE 802.11 contention-based access,"Currently deployed IEEE 802.11 WLANs work mostly with distributed coordination function (DCF) mode at the MAC layer, which does not provide QoS support. The upcoming IEEE standard 802.11e achieves service differentiation by assigning different channel access parameters (CAPs) to different traffic classes at the MAC layer. However, such relative differentiation does not yield QoS guarantee. In practice, appropriately selecting CAPs a priori is difficult. Time-varying traffic loads also make the use of fixed CAPs inefficient for both QoS support and channel utilization. We propose a novel architecture called HARMONICA, in which the access point dynamically selects the best CAPs for each traffic class to optimally match their QoS requirements. We present and discuss a simple admission control mechanism used by HARMONICA to avoid congestion. Our simulation results demonstrate that under an interference-free environment, HARMONICA can guarantee the QoS for all traffic classes while simultaneously achieving quasi-optimal channel utilization.","Admission control,
Quality of service,
Traffic control,
Interference,
Computer science,
USA Councils,
Telecommunication traffic,
Internet,
Collision avoidance,
Multiaccess communication"
Information measures in fuzzy decision trees,"Decision trees are a popular form of classification models. It is well known that classical trees lack the ability of modelling vagueness. By connecting fuzzy systems and classical decision trees, we try to achieve classifiers that can model vagueness and are comprehensible. We discuss the core problem of how to compute the information measure used in the induction of fuzzy trees and propose some improvements. In addition, we consider fuzzy rule bases derived from fuzzy decision trees and present some heuristic strategies to prune them. We report the results of experiments in which we compare our approach to other well-known classification methods.","Decision trees,
Fuzzy sets,
Testing,
Classification tree analysis,
Partitioning algorithms,
Computer science,
Data analysis,
Electronic mail,
Joining processes,
Neural networks"
Support for evolving software architectures in the ArchWare ADL,"Software that cannot evolve is condemned to atrophy: it cannot accommodate the constant revision and re-negotiation of its business goals nor intercept the potential of new technology. To accommodate change in software systems, we have defined an active software architecture to be: dynamic in that the structure and cardinality of the components and interactions are changeable during execution; updatable in that components can be replaced; decomposable in that an executing system may be (partially) stopped and split up into its components and interactions; and reflective in that the specification of components and interactions may be evolved during execution. Here we describe the facilities of the ArchWare architecture description language (ADL) for specifying active architectures. The contribution of the work is the unique combination of concepts including: a /spl pi/-calculus based communication and expression language for specifying executable architectures; hyper-code as an underlying representation of system execution that can be used for introspection; a decomposition operator to incrementally break up executing systems; and structural reflection for creating new components and binding them into running systems.","Computer architecture,
Business,
Software architecture,
File servers,
Computer science,
Software systems,
Architecture description languages,
Atrophy,
Reflection,
Costs"
Learning user models of mobility-related activities through instrumented walking aids,"We present a robotic walking aid capable of learning models of users' walking-related activities. Our walker is instrumented to provide guidance to elderly people when navigating their environments; however, such guidance is difficult to provide without knowing what activity a person is engaged in (e.g., where a person wants to go). The main contribution of this paper is an algorithm for learning models of users of the walker. These models are defined at multiple levels of abstractions, and learned from actual usage data using statistical techniques. We demonstrate that our approach succeeds in determining the specific activity in which a user engages when using the walker. One of our proto-type walkers was tested in an assisted living facility near Pittsburgh, PA; a more recent model was extensively evaluated in a university environment.","Instruments,
Acquired immune deficiency syndrome,
Legged locomotion,
Navigation,
Senior citizens,
Testing,
Computer science,
Monitoring,
Motion control,
Mathematical model"
Advanced block size selection algorithm for inter frame coding in H.264/MPEG-4 AVC,"A fast inter-mode selection algorithm is proposed to improve the encoder efficiency of the H.264/MPEG-4 AVC standard, but with insignificant degradation in picture quality. The modified fast inter-mode selection (MFInterms) algorithm extends previous work to provide a. more efficient prediction of mode decision. The strategy incorporates temporal similarity detection and the detection of different moving features within a macroblock. Simulation results demonstrate a speed up in encoding time of up to 73% compared with the H.264 benchmark.","MPEG 4 Standard,
Automatic voltage control,
Cost function,
Lagrangian functions,
Rate-distortion,
Partitioning algorithms,
Computer science,
Degradation,
Encoding,
Motion estimation"
Modeling Web-based dialog flows for automatic dialog control,"In Web-based applications, the dialog control logic is often hidden in or entwined with the presentation and/or application logic, even if the latter tiers are well-separated. This makes it difficult to control complex dialog structures like nested dialogs, and to reconcile the device-independent business logic with the device-specific interaction patterns required by different clients' I/O capabilities. To avoid continuous re-implementation of the dialog control logic, we present a dialog control framework that is separate from the presentation and business tiers, and manages arbitrarily nested dialog flows on different presentation channels. The framework relies on dialog specifications developed using the dialog flow notation, which are translated into an object-oriented dialog flow model for efficient run-time lookups. This way, the framework automates the dialog control aspect of Web-based application development and leaves only the tasks of implementing the business logic, designing the hypertext pages, and specifying the dialog flow to the developer","Automatic control,
Logic devices,
Application software,
Object oriented modeling,
Books,
Telematics,
Computer science,
Runtime,
Logic design,
Marketing and sales"
TreeCast: a stateless addressing and routing architecture for sensor networks,"Summary form only given. Recent advances in technology have made low-cost, low-power wireless sensors a reality. A network of such nodes can coordinate among themselves for distributed sensing and processing of certain phenomena. We propose an architecture to provide a stateless solution in sensor networks for efficient addressing and routing. We name our architecture TreeCast. We propose a unique method of address allocation, building up multiple disjoint trees which are geographically inter-twined and rooted at the data sink. Using these trees, routing messages to and from the sink node without maintaining any routing state in the sensor nodes is possible. Next, we use this address allocation method for scoped addressing, through which sensor nodes of a particular type or in a particular region can be targeted. Evaluation of our protocol using ns-2 simulations shows how well our addressing and routing schemes perform.","Sensor phenomena and characterization,
Wireless sensor networks,
Computer architecture,
Routing protocols,
Humans,
Robustness,
NASA,
Computer science,
Performance evaluation,
Sensor systems"
PRIX: indexing and querying XML using prufer sequences,"We propose a new way of indexing XML documents and processing twig patterns in an XML database. Every XML document in the database can be transformed into a sequence of labels by Prufer's method that constructs a one-to-one correspondence between trees and sequences. During query processing, a twig pattern is also transformed into its Prufer sequence. By performing subsequence matching on the set of sequences in the database, and performing a series of refinement phases that we have developed, we can find all the occurrences of a twig pattern in the database. Our approach allows holistic processing of a twig pattern without breaking the twig into root-to-leaf paths and processing these paths individually. Furthermore, we show that all correct answers are found without any false dismissals or false alarms. Experimental results demonstrate the performance benefits of our proposed techniques.","Indexing,
XML,
Databases,
Query processing,
Merging,
Moon,
Computer science,
Pattern matching,
Information representation,
Internet"
Plasma reflow bumping of Sn-3.5 Ag solder for flux-free flip chip package application,"A new flux-free reflow process using Ar+10%H/sub 2/ plasma was investigated for application to solder bump flip chip packaging. The 100-/spl mu/m diameter Sn-3.5wt%Ag solder balls were bonded to 250-/spl mu/m pitch Cu/Ni under bump metallurgy (UBM) pattern by laser solder ball bonding method. Then, the Sn-Ag solder balls were reflowed in Ar+H/sub 2/ plasma. Without flux, the wetting between solder and UBM occurred in Ar+H/sub 2/ plasma. During plasma reflow, the solder bump reshaped and the crater on the top of bump disappeared. The bump shear strength increased as the Ni/sub 3/Sn/sub 4/ intermetallic compounds formed in the initial reflow stage but began to decrease as coarse (Cu,Ni)/sub 6/Sn/sub 5/ grew at the solder/UBM interface. As the plasma reflow time increased, the fracture mode changed from ductile fracture within the solder to brittle fracture at the solder/UBM interface. The off-centered bumps self-aligned to patterned UBM pad during plasma reflow. The micro-solder ball defects occurred at high power prolonged plasma reflow.",
Link-Rank: a graphical tool for capturing BGP routing dynamics,"Failures at the BGP level can have significant impact on the overall Internet. Understanding the behavior of BGP is thus both an important practical challenge and an interesting research problem. To understand the true dynamics, and help interpret the multiple gigabytes of BGP log data, we have developed the ""Link-Rank"" graphical toolset. Link-Rank weighs the links between autonomous systems by the number of routing prefixes going through each link. Tracing these graphs over time results in a directed graph that shows the weight changes of the logical inter-AS links. From this graph one can easily visualise the complex BGP path changes and also combine views from multiple vantage points, to get a better picture of global routing dynamics. We illustrate the usefulness of Link-Rank by using it to examine BGP routing dynamics in three example cases. These examples show Link-Rank is able to help BGP analysts estimate the scope of routing changes and to reveal important routing dynamics in the presence of superfluous BGP update messages.","Routing,
Internet,
Condition monitoring,
Computer science,
Visualization,
Access protocols,
Failure analysis,
Gunshot detection systems,
Glass,
Large Hadron Collider"
The use of encrypted functions for mobile agent security,"Mobile agent technology is a new paradigm of distributed computing that can replace the conventional client-server model. However, it has not become popular due to some problems such as security. The fact that computers have complete control over all the programs makes it very hard to protect mobile agents from untrusted hosts. In this paper we propose a security approach for mobile agents, which protect mobile agents from malicious hosts. Our new approach prevents privacy attacks and integrity attacks to mobile agents from malicious hosts. This approach is an extension of mobile cryptography, as proposed by Sander and Tschudin, and it removes many problems found in the original idea of mobile cryptography while preserving most of the benefits. Although the original idea of mobile cryptography allowed direct computations without decryptions on encrypted mobile agents, it did not provide any practical ways of implementation due to the fact that no homomorphic encryption schemes are found for their approach. Our approach provides a practical idea for implementing mobile cryptography by suggesting a hybrid method that mixes a function composition technique and a homomorphic encryption scheme that we have found. Like the original mobile cryptography, our approach will encrypt both code and data including state information in a way that enables direct computation on encrypted data without decryption.",
Mobility management and its applications in efficient broadcasting in mobile ad hoc networks,"We study an efficient broadcast scheme in mobile ad hoc networks (MANETs). The objective is to determine a small set of forward nodes to ensure full coverage. We first study several methods to select a small forward node set assuming that the neighborhood information can be updated in a timely manner. Then we consider a general case, where each node updates its neighborhood information based asynchronously on a predefined frequency and node move even during the broadcast process. The virtual network constructed from local views of nodes may not be connected, its links may not exist in the physical network, and the global view constructed from collection of local views may not be consistent. In this paper, we first give a sufficient condition for connectivity at the physical network to ensure the connectivity at the virtual network. We then propose a solution using two transmission ranges to address the link availability issue. The neighborhood information as well as the forward node set are determined based on a short transmission range while the broadcast process is done on a long transmission range. The difference between these two ranges is based on the update frequency and the speed of node movement. Finally, we propose a mechanism called aggregated local view to ensure consistency of the global view. By these, we extend Wu and Dai's coverage condition for broadcasting in a network with mobile nodes. The simulation study is conducted to evaluate the coverage of the proposed scheme.","Intelligent networks,
Mobile radio mobility management,
Broadcasting,
Mobile ad hoc networks,
Frequency,
Application software,
Computer science,
Radio access networks,
Availability,
Lungs"
Innovative methodology to improve the quality of electronic engineering formation through teaching industrial computer engineering,"An innovative educational methodology adapted to the requirements of a new era with new societal and industrial challenges for electronic engineers is proposed in this paper. This active methodology, known as the Educational Innovation Project (EIP), is being studied in the Electronic Engineering (EE) degree of the Higher Technical School of Design Engineering at the Polytechnic University of Valencia, Valencia, Spain. The main objective of the EIP methodology is to improve the process of teaching and learning in order to increase student success. To accomplish this objective, the EIP method addresses various issues. From an organizational viewpoint, different structural aspects of the EE degree have been adapted, such as balancing and integrating lectures and laboratory sessions, advancing into interdisciplinary studies coordinated among all the subjects of the course, and strengthening the work in teams to tackle real engineer problems. The industrial computer engineering (ICE) subject is taken as a reference to show how these aspects have been applied. Regarding the faculty, lecturers participate in an open and permanent process of further training; attitudes toward cooperation and exchanges of experience among them are promoted; and research and reflection on new methodologies is encouraged. One of the challenges of the implementation of the EIP project is the development of multidisciplinary projects by team workers. The knowledge acquired from all the subjects is put into practice through the development of a common project to undertake real engineering problems.","Electronics engineering education,
Computer science education"
Detecting anomalous human interactions using laser range-finders,"We present a laser range-finder-based system for tracking people in an outdoor environment and detecting interactions between them. The system does not use identities of people for tracking. Observed tracks are automatically segmented into individual activities using an entropy-based measure (Jensen-Shannon divergence (Lin, J, 1991)). Two people situated close to each other throughout the duration of an activity represents an interaction. The observed activities are combined using a hierarchical clustering algorithm to generate a representative set. The frequency of occurrence of these activities is modeled by a Poisson distribution. During the monitoring phase, this model is used to compute the probability of observing the detected activities and interactions; an anomaly is flagged if this probability falls below a threshold. Experimental results from an outdoor courtyard environment are described where the system indicates anomalies when there is a sudden increase in the number of people in the environment or in the number of interactions. This detection occurs without giving the system any a priori concepts of space occupancy.","Humans,
Laser modes,
Laser theory,
Computer science,
Frequency,
Monitoring,
Clustering algorithms,
Phase detection,
Surveillance,
Machine vision"
Defense against low-rate TCP-targeted denial-of-service attacks,"Low-rate TCP-targeted denial-of-service (DoS) attacks aim at the fact that most operating systems in use today have a common base TCP retransmission timeout (RTO) of 1 sec. An attacker injects periodic bursts of packets to fill the bottleneck queue and forces TCP connections to timeout with near-zero throughput. This work proposes randomization on TCP RTO as defense against such attacks. With RTO randomization, an attacker cannot predict the next TCP timeout and consequently cannot inject the burst at the exact instant. An analytic performance model on the throughput of randomized TCP is developed and validated. Simulation results show that randomization can effectively mitigate the impact of such DoS attacks while maintaining fairness and friendliness to other connections.","Computer crime,
Neck,
Throughput,
Performance analysis,
Computer science,
Operating systems,
Clocks,
Buffer overflow,
Frequency,
Collaborative work"
"A coordinated data collection approach: design, evaluation, and comparison","We consider the problem of collecting a large amount of data from several different hosts to a single destination in a wide-area network. This problem is important since improvements in data collection times in many applications such as wide-area upload applications, high-performance computing applications, and data mining applications are crucial to performance of those applications. Often, due to congestion conditions, the paths chosen by the network may have poor throughput. By choosing an alternate route at the application level, we may be able to obtain substantially faster completion time. This data collection problem is a nontrivial one because the issue is not only to avoid congested link(s), but to devise a coordinated transfer schedule which would afford maximum possible utilization of available network resources. Our approach for computing coordinated data collection schedules makes no assumptions about knowledge of the topology of the network or the capacity available on individual links of the network. This approach provides significant performance improvements under various degrees and types of network congestions. To show this, we give a comprehensive comparison study of the various approaches to the data collection problem which considers performance, robustness, and adaptation characteristics of the different data collection methods. The adaptation to network conditions characteristics are important as the above applications are long lasting, i.e., it is likely changes in network conditions will occur during the data transfer process. In general, our approach can be used for solving arbitrary data movement problems over the Internet. We use the Bistro platform to illustrate one application of our techniques.","Internet,
Computer science,
High performance computing,
Computer applications,
Data mining,
Processor scheduling,
Government,
Large-scale systems,
Throughput,
Computer networks"
A multi-robot approach to stealthy navigation in the presence of an observer,"We propose a simple, reactive method for multiple robots carrying out sequential low-visibility navigation in the presence of an observer. Initially, the robots have no map of the environment but know the locations of the observer and goal. They generate an occupancy grid representation of the environment which is modeled using potential fields with embedded task information. These fields are combined and navigation waypoints extracted. Each robot carries out its traverse independently and shares its experience with its successor. The experience information consists of the occupancy grid and a filtered version of the traveled path used to assist the subsequent robot to traverse a lower visibility path. This produces a robust and reactive solution for stealthy navigation since there is no global path planning and the robots are not committed to any particular path. Experiments in simulation and real outdoor environments substantiate the approach and demonstrate the benefits of sharing information in reducing cumulative visibility. The experiments also demonstrate the algorithm's versatility in taking advantage of an environment that changes between robot traverses.","Navigation,
Robot sensing systems,
Mesh generation,
Data mining,
Information filtering,
Information filters,
Robustness,
Computer science,
Path planning,
Interference"
Evaluation of JPEG 2000 encoder options: human and model observer detection of variable signals in X-ray coronary angiograms,"Previous studies have evaluated the effect of the new still image compression standard JPEG 2000 using nontask based image quality metrics, i.e., peak-signal-to-noise-ratio (PSNR) for nonmedical images. In this paper, the effect of JPEG 2000 encoder options was investigated using the performance of human and model observers (nonprewhitening matched filter with an eye filter, square-window Hotelling, Laguerre-Gauss Hotelling and channelized Hotelling model observer) for clinically relevant visual tasks. Two tasks were investigated: the signal known exactly but variable task (SKEV) and the signal known statistically task (SKS). Test images consisted of real X-ray coronary angiograms with simulated filling defects (signals) inserted in one of the four simulated arteries. The signals varied in size and shape. Experimental results indicated that the dependence of task performance on the JPEG 2000 encoder options was similar for all model and human observers. Model observer performance in the more tractable and computationally economic SKEV task can be used to reliably estimate performance in the complex but clinically more realistic SKS task. JPEG 2000 encoder settings different from the default ones resulted in greatly improved model and human observer performance in the studied clinically relevant visual tasks using real angiography backgrounds.","Humans,
X-ray detection,
X-ray detectors,
Signal detection,
Transform coding,
X-ray imaging,
Matched filters,
Computational modeling,
Image coding,
Image quality"
PrudentExposure: a private and user-centric service discovery protocol,"Service discovery as an essential element in pervasive computing environments is widely accepted. Much active research on service discovery has been conducted, but privacy has been ignored and may be sacrificed. While it is essential that legitimate users should be able to discover services of which they have credentials, it is also necessary that services be hidden from illegitimate users. Since service information, service provider's information, service requests, and credentials to access services via service discovery protocols may be sensitive, we may want to keep them private. Existing service discovery protocols do not solve these problems. We present a user-centric model, called Prudentexposure, as the first approach designed for exposing minimal information privately, securely, and automatically for both service providers and users of service discovery protocols. We analyze the mathematical properties of our model and formally verify our security protocol.","Digital audio players,
Privacy,
Bluetooth,
Pervasive computing,
Information security,
Personal digital assistants,
Headphones,
Computer science,
Access protocols,
Mathematical model"
On the characteristics of Internet traffic variability: spikes and elephants,"Analysing and modeling of traffic play a vital role in designing and controlling of networks effectively. To construct a practical traffic model that can be used for various networks, it is necessary to characterize aggregated traffic and user traffic. This paper investigates these characteristics and their relationship. Our analyses are based on a huge number of packet traces from five different networks on the Internet. We found that: (1) marginal distributions of aggregated traffic fluctuations follow positively skewed (non-Gaussian) distributions, which leads to the existence of ""spikes"", where spikes correspond to an extremely large value of momentary throughput; (2) the amount of user traffic in a unit of time has a wide range of variability; and (3) flows within spikes are more likely to be ""elephant flows"", where an elephant flow is an IP flow with a high volume of traffic. These findings are useful in constructing a practical and realistic Internet traffic model.",
Decentralized trust management and accountability in federated systems,"In this paper, we describe three key problems for trust management in federated systems and present a layered architecture for addressing them. The three problems we address include how to express and verify trust in a flexible and scalable manner, how to monitor the use of trust relationships over time, and how to manage and reevaluate trust relationships based on historical traces of past behavior. While previous work provides the basis for expressing and verifying trust, it does not address the concurrent problems of how to continuously monitor and manage trust relationships over time. These problems close the loop on trust management and are especially relevant in the context of federated systems where remote resources can be acquired across multiple administrative domains and used in potentially undesirable ways (e.g., to launch denial-of-service attacks).","Computer networks,
Remote monitoring,
Grid computing,
Face detection,
Resource management,
System testing,
Internet,
Computerized monitoring,
Authentication,
Computer network management"
Cryptanalysis of security enhancement for the timestamp-based password authentication scheme using smart cards,"Recently, Yang and Shieh proposed two password authentication schemes. In 2002, Chan and Cheng pointed out that Yang and Shieh's timestamp-based authentication scheme was vulnerable to the forgery attack. In 2003, Sun and Yeh pointed out that Yang and Shieh's scheme was vulnerable to the forgery attack and explained Chan and Cheng's attack was unreasonable. Later, Shen et al. modified Yang and Shieh's scheme to resist Chan and Cheng's attack. However, in this paper, we shall point out that Shen et al.'s improvement is still vulnerable to the forgery attack.","Authentication,
Smart cards,
Forgery,
Sun,
Resists,
Chaos,
Councils,
Contracts,
Computer science,
Information management"
VRing: a case for building application-layer multicast rings (rather than trees),"The paper presents the design, analysis and performance evaluation of VRing, a novel application-layer multicast (ALM) protocol that establishes a virtual ring as an overlay network among the multicast group members in a self-organizing and distributed manner. In order to reduce the routing delay of the ring overlay network, we propose to form a spare ring overlay structure that improves connectivity among group members. The design of the spare ring is justified by proposing, and analytically studying, the performance of a data delivery and duplicate suppression mechanism that makes use of both the original ring and the spare ring for forwarding data packets. We conduct simulations of both VRing and a hierarchical ALM protocol, NICE, using the J-Sim network simulator Simulation results show that, although VRing has a higher path stretch and a higher link stress than NICE, it provides less control overhead, consumes less bandwidth, and provides lower average node degree than NICE. Furthermore, VRing achieves a higher average data delivery ratio in the presence of failures than NICE. The performance improvement is especially pronounced for larger multicast groups.","Computer aided software engineering,
Multicast protocols,
Bandwidth,
Performance analysis,
Routing protocols,
Scalability,
Unicast,
Computer science,
Application software,
Stress"
A marking scheme using Huffman codes for IP traceback,"In (distributed) denial of service attack ((D)DoS), attackers send a huge number of packets with spoofed source addresses to disguise themselves toward a target host or network Various IP traceback techniques such as link testing, marking, and logging to find out the real source of attacking packets have been proposed. We present a marking scheme (with marking and traceback algorithms) in which a router marks a packet with a link that the packet came through. Links of a router are represented by Huffman codes according to the traffic distribution among the links. If the packet runs out of space allotted for the marking field in the packet header, then the router stores the marking field in the router's local memory along with a message digest of the packet. We analyze the memory requirement of routers to store marking fields, compare the scheme with other existing techniques, and address practical issues to deploy the scheme in the Internet. The scheme marks every packet, therefore IP traceback can be accomplished with only a packet unlike in probabilistic markings; also it requires far less amount of memory compared to logging methods and is robust in case of DDoS.","Computer crime,
Testing,
Protocols,
Encoding,
Computer science,
Robustness,
Web and internet services,
Network interfaces,
Information filtering,
Information filters"
Augmented reality working planes: a foundation for action and construction at a distance,"This paper introduces the concept of augmented reality working planes to action and construction at a distance for mobile outdoor augmented reality systems. We have based our new AR working planes technique on CAD working planes, but by using coordinate systems relative to the body they can be specified and used much more intuitively than on a desktop system. We demonstrate in this paper how AR working planes can be used for the display of information, the manipulation of existing 3D objects, and the creation of new geometry. AR working planes are particularly well suited to supporting 3D modelling in mobile outdoor AR systems. This modelling task is a particularly difficult problem, because the environment is typically at a scale much larger than the user, and direct manipulation techniques can not be used.","Augmented reality,
Humans,
Wearable computers,
Virtual reality,
Geometry,
Laboratories,
Information science,
Lakes,
Australia,
Mobile computing"
Managing don't cares in Boolean satisfiability,Advances in Boolean satisfiability solvers have popularized their use in many of today's CAD VLSI challenges. Existing satisfiability solvers operate on a circuit representation that does not capture all of the structural circuit characteristics and properties. This work proposes algorithms that take into account the circuit don't care conditions thus enhancing the performance of these tools. Don't care sets are addressed in this work both statically and dynamically to reduce the search space and guide the decision making process. Experiments demonstrate performance gains.,"Circuits,
Very large scale integration,
Decision making,
Logic testing,
Computer science,
Terminology,
Performance gain,
Engines,
Robustness,
Data structures"
Construction of fuzzy signature from data: an example of SARS pre-clinical diagnosis system,"There are many areas where objects with very complex and sometimes interdependent features are to be classified; similarities and dissimilarities are to be evaluated. This makes a complex decision model difficult to construct effectively. Fuzzy signatures are introduced to handle complex structured data and interdependent feature problems. Fuzzy signatures can also be used in cases where data is missing. This work presents the concept of a fuzzy signature and how its flexibility can be used to quickly construct a medical pre-clinical diagnosis system. A severe acute respiratory syndrome (SARS) pre-clinical diagnosis system using fuzzy signatures is constructed as an example to show many advantages of the fuzzy signature. With the use of this fuzzy signature structure, complex decision models in the medical field should be able to be constructed more effectively.","Fuzzy systems,
Fuzzy sets,
Data mining,
Computer science,
Medical diagnostic imaging,
Fuzzy set theory,
Humans,
Medical control systems,
Input variables,
Computational efficiency"
Testing low-degree polynomials over prime fields,"We present an efficient randomized algorithm to test if a given function f : F/sub p/ /sup n/ /spl rarr/ F/sub p/ (where p is a prime) is a low-degree polynomial. This gives a local test for generalized Reed-Muller codes over prime fields. For a given integer t and a given real /spl epsiv/ > 0, the algorithm queries f at 1//spl epsiv/ + t/spl middot/p/sup 2r/p-1+O(1)/ points to determine whether f can be described by a polynomial of degree at most t. If f is indeed a polynomial of degree at most t, our algorithm always accepts, and if f has a relative distance at least e from every degree t polynomial, then our algorithm rejects f with probability at least 1/2. Our result is almost optimal since any such algorithm must query f on at least /spl Omega/(1//spl epsiv/ + p/sup r+1/p-1/) points.","Polynomials,
Computer science,
Automatic testing,
Codes"
Finding good candidate cycles for efficient p-cycle network design,"An important problem in p-cycle network design is to find a set of p-cycles to protect a given working capacity distribution so that the total spare capacity used by the p-cycles is minimized. Existing approaches for solving the problem include ILP and heuristic algorithm. Both require a set of candidate p-cycles to be precomputed. In this paper, we propose an algorithm to compute a small set of candidate p-cycles that can lead to good performance when used by ILP or the heuristic algorithm. The key idea of the algorithm is to generate a combination of high efficiency cycles and short cycles so that both densely distributed and sparsely distributed working capacities can be efficiently protected by the candidate cycles. The algorithm is also flexible in that the number of cycles generated is controlled by an input parameter. Simulation study showed that the cycles generated by our algorithm can lead to near optimal solutions when used by either ILP or the heuristic algorithm","Protection,
Heuristic algorithms,
Costs,
Routing,
Computer science,
WDM networks,
Integer linear programming"
Estimation of the radiometric response functions of a color camera from differently illuminated images,"The mapping that relates the image irradiance to the image brightness (intensity) is known as the Radiometric Response Function or Camera Response Function. This usually unknown mapping is nonlinear and varies from one color channel to another. In this paper, we present a method to estimate the radiometric response functions (of R, G and B channels) of a color camera directly from the images of an arbitrary scene taken under different illumination conditions (The illumination conditions are not assumed to be known). The response function of a channel is modeled as a gamma curve and is recovered by using a constrained nonlinear minimization approach by exploiting the fact that the material properties of the scene remain constant in all the images. The performance of the proposed method is demonstrated experimentally.","Radiometry,
Cameras,
Lighting,
Layout,
Brightness,
Shape measurement,
Photometry,
Computer science,
Influenza,
Optical imaging"
Manipulation gaits: sequences of grasp control tasks,"In dexterous manipulation, an object must be reconfigured while maintaining a stable grasp. This may require that the object be re-grasped in order to avoid finger workspace limits. We present a set of closed-loop controllers designed to achieve force-related objectives such as wrench closure, and show how they may be concurrently combined. Furthermore, we show that dexterous manipulation behavior may be generated by sequencing concurrent combinations of these controllers. We show that dexterous manipulation can be viewed as a task that is accomplished in the context of a wrench closure constraint. We hypothesize this approach can generalize to any task that must be accomplished while maintaining a set of constraints.","Force control,
Robot kinematics,
Control systems,
Fingers,
Legged locomotion,
Orbital robotics,
Motion control,
Laboratories,
Computer science,
Mobile robots"
Global localization and relative pose estimation based on scale-invariant features,"The capability of maintaining the pose of the mobile robot is central for basic navigation and map building tasks. In This work we describe a vision-based hybrid localization scheme based on scale-invariant keypoints. In the first stage the topological localization is accomplished by matching the keypoints detected in the current view with the database of model views. Once the best match has been found, the relative pose between the model view and the current image is recovered. We demonstrate the efficiency of the location recognition approach and present a closed form solution to the relative pose recovery for the case of planar motion and unknown focal length of the camera. The approach is demonstrated on several examples of indoor environments.","Histograms,
Mobile robots,
Navigation,
Buildings,
Principal component analysis,
Object recognition,
Image edge detection,
Computer science,
Image databases,
Closed-form solution"
Using Information from Prior Runs to Improve Automated Tuning Systems,"Active Harmony is an automated runtime performance tuning system. In this paper we describe a parameter prioritizing tool to help focus on those parameters that are performance critical. Historical data is also utilized to further speed up the tuning process. We first verify our proposed approaches with synthetic data and finally we verify all the improvements on a real cluster-based web service system. Taken together, these changes allow the Active Harmony system to reduce the time spent tuning from 35% up to 50% and at the same time, reduce the variation in performance while tuning.",
Understanding tradeoffs among different architectural modeling approaches,"Over the past decade, a number of architecture description languages (ADLs) have been proposed to facilitate modeling and analysis of software architecture. While each claims to have various benefits, to date, there have been few studies to assess the relative merits of these approaches. In this paper, we describe our experience using two ADLs to model a system initially described in UML, and compare their effectiveness in identifying system design flaws. We also describe the techniques we used for extracting architectural models from a UML system description.","Documentation,
Unified modeling language,
Software architecture,
Computer science,
Architecture description languages,
Space missions,
Collaborative software,
System analysis and design,
Availability,
Proposals"
A polynomial time algorithm for computing an Arrow-Debreu market equilibrium for linear utilities,"We provide the first polynomial time exact algorithm for computing an Arrow-Debreu market equilibrium for the case of linear utilities. Our algorithm is based on solving a convex program using the ellipsoid algorithm and simultaneous diophantine approximation. As a side result, we prove that the set of assignments at equilibria is convex and the equilibria prices themselves are log-convex. Our convex program is explicit and intuitive, which allows maximizing a concave function over the set of equilibria. On the practical side, Ye developed an interior point algorithm (Ye, 2004) to find an equilibrium based on our convex program. We also derive separate combinatorial characterizations of equilibrium for Arrow-Debreu and Fisher cases. Our convex program can be extended for many non-linear utilities (Codenotti and Varadarajan, 2004; Jain and Ye) and production models (Jain). Our paper also makes a powerful theorem even more powerful in the area of geometric algorithms and combinatorial optimization. The main idea in this generalization is to allow ellipsoids not to contain the whole convex region but a part of it. This theorem is of independent interest.","Polynomials,
Approximation algorithms,
Ellipsoids,
Production,
Computer science,
Feedback"
VoIP packet loss concealment based on two-side pitch waveform replication technique using steganography,"Speech quality of VoIP (voice over Internet protocol) may potentially be degraded by transmission errors such as packet loss and delay, which is basically inevitable in best-effort communications. This study investigates an error concealment technique for such degradation by using a receiver-based technique called pitch waveform replication. For enhancing the conventional technique, this study proposes a waveform reconstruction technique that also takes account of the pitch variation between the backward and forward frames of gap frames. From experimental results of objective evaluation, it is indicated that the proposed technique may potentially be useful for improving the speech quality, compared with the conventional technique.","Internet telephony,
Steganography,
Degradation,
Computer errors,
Information science,
Propagation losses,
Propagation delay,
Speech analysis,
Linear approximation"
Light Collages: lighting design for effective visualization,"We introduce Light Collages - a lighting design system for effective visualization based on principles of human perception. Artists and illustrators enhance perception of features with lighting that is locally consistent and globally inconsistent. Inspired by these techniques, we design the placement of light sources to convey a greater sense of realism and better perception of shape with globally inconsistent lighting. Our algorithm segments the objects into local surface patches and uses a number of perceptual heuristics, such as highlights, shadows, and silhouettes, to enhance the perception of shape. We show our results on scientific and sculptured datasets.","Visualization,
Humans,
Light sources,
Art,
Geometry,
Educational institutions,
Shape,
Inverse problems,
Painting,
Computer science"
Exploring software evolution using spectrographs,"Software systems become progressively more complex and difficult to maintain. To facilitate maintenance tasks, project managers and developers often turn to the evolution history of the system to recover various kinds of useful information, such as anomalous phenomena and lost design decisions. An informative visualization of the evolution history can help cope with this complexity by highlighting conspicuous evolution events using strong visual cues. We present a scalable visualization technique called evolution spectrographs (ESG). An evolution spectrograph portrays the evolution of a spectrum of components based on a particular property measurement. We describe several special-purpose spectrographs and discuss their use in understanding and supporting software evolution through the case studies of three large software systems (OpenSSH, KOffice and FreeBSD).","Software systems,
History,
Software maintenance,
Frequency,
Project management,
Costs,
Data visualization,
Time measurement,
Computer science,
Particle measurements"
Energy - responsiveness tradeoffs for real-time systems with mixed workload,"We explore the performance tradeoffs for real-time systems with dynamic voltage scaling (DVS) capability, when the workload includes aperiodic jobs as well as periodic tasks. As opposed to the assumptions of early works on real-tune DVS or nonpower-aware scheduling of hybrid task sets, the settings require the consideration of two often-conflicting objectives: Improving the responsiveness of aperiodic jobs and reducing the energy consumption. We propose the composite metric, energy * average response time, as a performance measure in energy-aware scheduling of hybrid task sets. Then we develop our framework that integrates dynamic reclaiming algorithm (DRA) and total bandwidth server (TBS) mechanism in variable-speed settings. In addition to the static algorithm, we propose basic reclaiming scheme (BRS) and mutual reclaiming scheme (MRS) that enable the reuse of the system slack arising from early task completions. We also present our bandwidth sharing scheme (BSS) that aggressively exploits the bandwidth reserved for TBS to further slow down the periodic tasks. We provide an experimental evaluation of our algorithms under different workloads and speed settings, and show that BSS can provide significant performance improvements when the actual variability in the workload is high.","Delay,
Real time systems,
Energy consumption,
Voltage control,
Bandwidth,
Dynamic voltage scaling,
Scheduling algorithm,
Processor scheduling,
Timing,
Computer science"
Middleware specialization for memory-constrained networked embedded systems,"General purpose middleware has been shown to be effective off-the-shelf, in meeting diverse functional requirements for a wide range of distributed systems. However, middleware customization is necessary for many networked embedded systems because of the resource constraints in the networked nodes. We demonstrate that reduced middleware footprint can be achieved while maintaining real-time properties of applications running on such systems. We also give evidence that empirical measurement using a representative application is crucial to guide (1) selection of feature subsets from general purpose middleware and (2) trade-offs among different dimensions of design metrics including real-time, footprint, and portability.","Middleware,
Embedded system,
Real time systems,
Actuators,
Contracts,
Wireless sensor networks,
Vibration measurement,
Computer science,
Large-scale systems,
Costs"
Design of an anthropomorphic robot head for studying autonomous development and learning,"We describe the design of an anthropomorphic robot head intended as a research platform for studying autonomously learning active vision systems. The robot head closely mimics the major degrees of freedom of the human neck/eye apparatus and allows a number of facial expressions. We show that our robot head can shift its direction of gaze at speeds which come close to that of human saccades. Since our design only makes use of low cost consumer grade components, it paves the way for widespread use of anthropomorphic robot heads in science, education, health-care, and entertainment.","Anthropomorphism,
Humans,
Robot vision systems,
Cognitive robotics,
Machine vision,
Biological systems,
Cognitive science,
Intelligent robots,
Biology,
Computer vision"
Organization of the information flow in the perception-action loop of evolved agents,"Sensor evolution in nature aims at improving the acquisition of information from the environment and is intimately related with selection pressure towards adaptivity and robustness. Recent work in the area aims at studying the perception-action loop in a formalized information-theoretic manner. This paves the way towards a principled and general understanding of the mechanisms guiding the evolution of sensors in nature and provides insights into the design of mechanisms of artificial sensor evolution. In our paper we study the perception-action loop of agents. We evolve finite-state automata as agent controllers to solve an information acquisition task in a simple virtual world and study how the information flow is organized by evolution. Our analysis of the evolved automata and the information flow provides insight into how evolution organizes sensoric information acquisition, memory, processing and action selection. In addition, the results are compared to ideal information extraction schemes following from the Information Bottleneck principle.","Actuators,
Information theory,
Sensor phenomena and characterization,
Control systems,
Adaptive systems,
Automata,
Computer science,
Educational institutions,
Sensor systems,
Robustness"
Physics of the interaction Process in a typical coaxial virtual cathode oscillator based on computer modeling using MAGIC,"Physical basis of the interaction process in a typical coaxial vircator has been studied using MAGIC, a three-dimensional PIC computer code. Detailed investigations of the complex dynamics of electrons in and near the interaction space have revealed that it takes /spl sim/6 ns for the virtual cathode to be formed and oscillations to begin. Once the oscillations have started three groups of electrons can be distinguished: those that pass through the semipermeable anode and form a virtual cathode, those that travel parallel to the cylindrical anode, or even around its inner edge in small circles, and finally those called ""stray"" electrons which are ejected from the cylindrical anode and land on the walls of the output waveguide. We have then shown that by reducing the influence of stray electrons the output power of the vircator can be increased by as much as 52%. As a test of the accuracy of the simulation model the input impedance of the device and its frequency of oscillations have been reproduced, respectively, to within 5% and 2.5% of their measured values.","Coaxial components,
Cathodes,
Oscillators,
Physics computing,
Electrons,
Anodes,
Space exploration,
Power generation,
Testing,
Impedance"
New control system aspects for physical experiments,"New control system aspects are introduced for the design of slow control systems for physical experiments. Mainly, they are based on a comprehensive usage of XML technologies. A second paradigm for future control systems is the consequent usage of the model-view-controller (MVC) pattern. Main aspect of all (hardware and software) system components is the use throughout of standards for interfaces, protocols and architecture. In particular, the software is based on the unifying XML specifications XSchema, XPath, XQuery, XLink, and OPC XML. A first application of these technologies is the KArlsruhe TRItium Neutrino (KATRIN) Slow Control System (KSC) of a neutrino experiment at the Forschungszentrum Karlsruhe. Main characteristic of KSC is its homogeneous structure with its code being spread over several subsystems. Implications of this distributed method are system stability, independence of subsystems and fast and comfortable maintenance.","Control systems,
XML,
Neutrino sources,
Control system synthesis,
Hardware,
Software systems,
Software standards,
Protocols,
Computer architecture,
Application software"
Dynamic backoff for wireless personal networks,"Based on IEEE 802.15.4 low-rate wireless personal area networks (LR-WPANs), this paper proposes a memorized backoff scheme (MBS) with the exponential weighted moving average (EWMA) approach to dynamically adjust the size of the contention window. The proposed scheme can be implemented in the standard IEEE 802.15.4 medium access control (MAC) protocol without adding any new message type and without modifying the communicating procedure. An analytic model and a simulation model are developed to evaluate the performance of IEEE 802.15.4, MBS and MBS+EWMA. The numerical results indicate that in terms of goodput, completion rate, average MAC delay, average queuing delay and average number of collisions for each data frame, our proposed scheme significantly outperforms the standard IEEE 802.15.4 backoff scheme.","Network topology,
Peer to peer computing,
Media Access Protocol,
Computer science,
Access protocols,
Delay,
Routing,
Wireless personal area networks,
Communication standards,
Performance analysis"
Mining interesting contrast rules for a web-based educational system,"Web-based educational technologies allow educators to study how students learn (descriptive studies) and which learning strategies are most effective (causal/predictive studies). Since web-based educational systems collect vast amounts of student profile data, data mining and knowledge discovery techniques can be applied to find interesting relationships between attributes of students, assessments, and the solution strategies adopted by students. This paper focuses on the discovery of interesting contrast rules, which are sets of conjunctive rules describing interesting characteristics of different segments of a population. In the context of webbased educational sy stems, contrast rules help to identifY attributes characterizing patterns of performance disparity between various groups of students. We propose a general formulation of contrast rules as well as a framework for finding such patterns. We apply this technique to an online educational sy stem developed at Michigan State University called LON-CAP A.","Data mining,
Computer science,
Educational technology,
Data analysis,
Computer science education,
Computer networks,
Web pages,
Computational modeling,
Databases,
Demography"
ASAAM: aspectual software architecture analysis method,"Software architecture analysis methods aim to predict the quality of a system before it has been developed. In general, the quality of the architecture is validated by analyzing the impact of predefined scenarios on architectural components. Hereby, it is implicitly assumed that an appropriate refactoring of the architecture design can help in coping with critical scenarios and mending the architecture. This paper shows that there are also concerns at the architecture design level which inherently crosscut multiple architectural components, which cannot be localized in one architectural component and which, as such, can not be easily managed by using conventional abstraction mechanisms. We propose the aspectual software architecture analysis method (ASAAM) to explicitly identify and specify these architectural aspects and make them transparent early in the software development life cycle. ASAAM introduces a set of heuristic rules that help to derive architectural aspects and the corresponding tangled architectural components from scenarios. The approach is illustrated for architectural aspect identification in the architecture design of a window management system.","Software architecture,
Computer architecture,
Software design,
Programming,
Q factor,
Computer science,
Software systems,
Design methodology,
Risk analysis,
Costs"
Issues in evolving GP based classifiers for a pattern recognition task,"This paper discusses issues when evolving genetic programming (GP) classifiers for a pattern recognition task such as handwritten digit recognition. Developing elegant solutions for handwritten digit classification is a challenging task. Similarly, design and training of classifiers using genetic programming is a relatively new approach in pattern recognition as compared to other traditional techniques. Several strategies for GP training are outlined and the empirical observations are reported. The issues we faced such as training time, a variety of fitness landscapes and accuracy of results are discussed. Care has been taken to test GP using a variety of parameters and on several handwritten digits datasets.","Pattern recognition,
Genetic programming,
Handwriting recognition,
Character recognition,
Venus,
Computer science,
Testing,
Application software,
Classification tree analysis,
Focusing"
LRED: a robust active queue management scheme based on packet loss ratio,"Active queue management (AQM) is an effective method to enhance congestion control, and to achieve tradeoff between link utilization and delay. The de facto standard, random early detection (RED), and most of its variants use queue length as a congestion indicator to trigger packet dropping. The proportional-integral (PI), use both queue length and traffic input rate as congestion indicators; effective stability model and practical design rules built on the TCP control model and abstracted AQM model reveal that such schemes enhance the stability of a system. In this paper, we propose an AQM scheme with fast response time, yet good robustness. The scheme, called loss ratio based RED (LRED), measures the latest packet loss ratio, and uses it as a complement to queue length in order to dynamically adjust packet drop probability. Employing the closed-form relationship between packet loss ratio and the number of TCP flows, this scheme is responsive even if the number of TCP flows varies significantly. We also provide the design rules for this scheme based on the well-known TCP control model. This scheme's performance is examined under various network configurations, and compared to existing AQM schemes, including PI, random exponentially marking (REM), and adaptive virtual queue (AVQ). Our simulation results show that, with comparable complexity', this scheme has short response time, better robustness, and more desirable tradeoff than PI, REM, and AQV, especially under highly dynamic network and heavy traffic load.","Robustness,
Traffic control,
Telecommunication computing,
Stability,
Computer science,
Engineering management,
Delay effects,
Pi control,
Proportional control,
Communication system traffic control"
Time-energy design space exploration for multi-layer memory architectures,"This paper presents an exploration algorithm which examines execution time and energy consumption of a given application, while considering a parameterized memory architecture. The input to our algorithm is an application given as an annotated task graph and a specification of a multi-layer memory architecture. The algorithm produces Pareto trade-off points representing different multi-objective execution options for the whole application. Different metrics are used to estimate parameters for application-level Pareto points obtained by merging all Pareto diagrams of the tasks composing the application. We estimate application execution time although the final scheduling is not yet known. The algorithm makes it possible to trade off the quality of the results and its runtime depending on the used metrics and the number of levels in the hierarchical composition of the tasks' Pareto points. We have evaluated our algorithm on a medical image processing application and randomly generated task graphs. We have shown that our algorithm can explore huge design space and obtain (near) optimal results in terms of Pareto diagram quality.","Space exploration,
Memory architecture,
Bandwidth,
Scheduling,
Memory management,
Computer science,
Energy consumption,
Application software,
Algorithm design and analysis,
Computer architecture"
A memory-efficient and high-speed sine/cosine generator based on parallel CORDIC rotations,"The sine/cosine function generator is based on parallelization of the original CORDIC algorithm by predicting all the rotation directions directly from the binary bits of the initial input angle. Unlike previous approaches that require complicated circuits or exponentially increased ROM, our proposed architecture has a relatively simple prediction scheme through an efficient angle recoding. The critical path delay is also reduced by utilizing the predicted rotation directions to design an efficient multioperand carry-save addition structure.","Signal generators,
Prediction algorithms,
Circuits,
Read only memory,
Delay,
Frequency,
Arithmetic,
Computer science,
Equations,
Error correction"
Dynamic scheduling of parallel jobs with QoS demands in multiclusters and grids,"This paper addresses the dynamic scheduling of parallel jobs with QoS demands (soft-deadlines) in multiclusters and grids. Three metrics (over-deadline, makespan and idle-time) are combined with variable weights to evaluate the scheduling performance. These three metrics are used to measure the extent of the jobs' QoS demand compliance, the resource throughput and the resource utilization. Two levels of performance optimisation are applied in the multicluster. At the multicluster level, a scheduler (which we call MUSCLE) allocates parallel jobs with high packing potential to the same cluster; it also takes the jobs' QoS requirements into account and employs a heuristic to allocate suitable workloads to each cluster to balance the overall system performance. At the single cluster level, an existing workload manager, called TITAN, utilizes a genetic algorithm to further improve the scheduling performance of the jobs previously allocated by MUSCLE. Extensive experimental studies are conducted to verify the effectiveness of the scheduling mechanism as well as the effect of the prediction accuracy on the scheduling performance. The results show that compared with traditional distributed workload allocation policies, the comprehensive scheduling performance of parallel jobs is significantly improved across the multicluster, and the presence of prediction errors does not dramatically weaken the performance advantage.","Dynamic scheduling,
Processor scheduling,
Throughput,
Resource management,
Muscles,
Grid computing,
Contracts,
Optimization,
Helium,
Computer science"
"Downloading replicated, wide-area files - a framework and empirical evaluation","The challenge of efficiently retrieving files that are broken into segments and replicated across the wide-area is of prime importance to wide-area, peer-to-peer, and grid file systems. Two differing algorithms addressing this challenge have been proposed and evaluated. While both have been successful in differing performance scenarios, there has been no unifying work that can view both algorithms under a single framework. We define such a framework, where download algorithms are defined in terms of four dimensions: the number of simultaneous downloads, the degree of work replication, the failover strategy, and the server selection algorithm. We then explore the impact of varying parameters along each of these dimensions.","File systems,
Peer to peer computing,
File servers,
Network servers,
Partitioning algorithms,
Computer science,
Grid computing,
Greedy algorithms,
Redundancy,
Scheduling algorithm"
Towards provision of quality of service guarantees in job scheduling,"Considerable research has focused on the problem of scheduling dynamically arriving independent parallel jobs on a given set of resources. There has also been some recent work in the direction of providing differentiated service to different classes of jobs using statically or dynamically calculated priorities assigned to the jobs. However, the potential and usability of a quality of service based scheme has not been much studied. In This work, we extend a previously proposed scheme (QoPS) to provide quality of service to submitted jobs; we propose extensions to the algorithm in multiple aspects: (i) studying the effect of user tolerance towards missed deadlines on the overall profit attainable by the supercomputer center, (it) providing artificial slack to some jobs to maximize the overall profit and (hi) utilizing a kill-and-restart mechanism to further improve the profit attainable.","Quality of service,
Processor scheduling,
Dynamic scheduling,
Scheduling algorithm,
Costs,
Delay,
Admission control,
Computer science,
Usability,
Supercomputers"
Temporal vs. spectral approach to feature extraction from prehensile EMG signals,There are generally two nonparametric approaches in feature extraction from temporal signals: temporal and spectral approach. Both approaches were used in classification of prehensile electromyographic (EMG) signals. The goal of this paper is to define and evaluate some successful methods in both approaches and to determine experimentally which method and approach is the most appropriate. The evaluation is based on classification of real EMGs with an ART-based classifier. The efficiency analysis is also provided. The results have shown that a less expensive temporal approach has strong advantages over the spectral methods.,"Feature extraction,
Electromyography,
Signal processing,
Computer science,
Electromagnetic compatibility,
Pattern recognition,
Image converters,
Performance analysis"
Fairness measures for best effort traffic in wireless networks,"This paper proposes that fairness in wireless networks should be measured using one of the following new measures: the deterministic unfairness bound called the wireless absolute fairness bound (WAFB) or the statistical unfairness bound called the 99-percentile wireless absolute fairness bound (WAFB/sub 99/). Compared with previous fairness definitions, the new fairness measures are better suited for measuring fairness of scheduling disciplines that exploit multiuser diversity. A new scheduling discipline called opportunistic proportional fair scheduling is defined. Numerical results show that the new scheduling discipline has slightly higher throughput and slightly better fairness than proportional fair scheduling.","Telecommunication traffic,
Intelligent networks,
Wireless networks,
Throughput,
Global Positioning System,
Traffic control,
Delay,
Processor scheduling,
Computer science,
Scheduling algorithm"
Recurring patterns of facilitation interventions in GSS sessions,"Now that GSS (group support systems) have been commercialized and are present in an increasing number of organizational settings, sustained use of GSS within organizations is an important subject of research. One of the aspects of GSS use that clearly poses challenges for organizations concerns GSS session design. ThinkLets are chunked facilitation techniques that can be used as building blocks for GSS sessions. In order to develop a library of useful thinkLets for organizations to use, this study researched patterns of thinkLets use in a large number of GSS sessions. We identified which thinkLets were used most and which patterns of thinkLets in terms of a fixed sequence emerged as 'best practices'. By identifying such sequences, called modules, we can offer organizations powerful capsules of knowledge to wield GSS effectively. The results of our study represent a first version of a lexicon for the discipline of collaboration engineering with thinkLets and opens various avenues for research on making organizational collaboration more productive.","Collaborative work,
Productivity,
Computer interfaces,
Technology management,
User interfaces,
Educational institutions,
Information science,
Management information systems,
Information analysis,
Commercialization"
Stability of walking in a multilegged robot suffering leg loss,"This article describes: tests of fault tolerance of the eight-legged walking robot SCORMON in the event of leg loss. It evaluates different gaits, which are based on biological research on insect and arachnid walking and concludes with a discussion, what the best gait for the SCORPION system is, when 2 legs are lost It also includes a short introduction to the SCORPION robot and its biomimetic software approach and its performance in different terrain.","Legged locomotion,
Stability,
Leg,
Computer science,
Biomimetics,
Mobile robots,
Robot sensing systems,
Communication system control,
Nonlinear equations,
Biological system modeling"
Visualized online simple sequencing authoring tool for SCORM-compliant content package,"The purpose of this study is to create a visualized online simple sequencing authoring tool (VOSSAT) that provides an easy-to-use and Web-based interface to help instructors edit existing SCORM-compliant content packages. With vigorous development of the Internet, Web-based e-learning systems have become more and more popular Shamble content object reference model (SCORM) 1.3, the most popular standard for the consistency of course format among different Web-based e-learning systems, provides the simple sequencing specification (SSS). Like supply chain management, the inputs of VOSSAT are SCORM-compliant content packages; the outputs of VOSSAT are content packages to be delivered at SCORM-compliant learning management system. In terms of authoring processes of metadata file, a teacher needs to only fill some options in the annotation windows. Then, the VOSSAT automatically and dynamically generates the paragraphs needed for presenting the sequencing of learning path properly. The methodology of this study is to analyze the IMS SSS behaviors and to propose VOSSAT as a module on content repository management system (CRMS). The results of this study are shown by scenarios. Finally, the implications of this study for education are also included.","Visualization,
Packaging,
Electronic learning,
Computer science education,
Content management,
Computer interfaces,
Supply chain management,
Material storage,
Internet,
Process design"
A Viterbi algorithm for a trajectory model derived from HMM with explicit relationship between static and dynamic features,"This paper introduces a Viterbi algorithm to obtain a sub-optimal state sequence for trajectory-HMM, which is derived from HMM with explicit relationship between static and dynamic features. The trajectory-HMM can alleviate some limitations of HMM, which are (i) constant statistics within HMM state and (ii) conditional independence of observations given the state sequence, without increasing the number of model parameters. The proposed algorithm was applied to state-boundary optimization for Viterbi training and N-best rescoring. In a speaker-dependent continuous speech recognition experiment, trajectory-HMM with the proposed algorithm achieved about 14% error reduction over the standard HMM with the conventional Viterbi algorithm.","Hidden Markov models,
Viterbi algorithm,
Speech recognition,
Statistics,
Computational modeling,
Iterative decoding,
Cepstral analysis,
Computer science,
Computational complexity,
Humans"
"Computers for communication, not calculation: media as a motivation and context for learning","As the skills that constitute literacy evolve to accommodate digital media, computer science education finds itself in a sorry state. While students are more in need of computational skills than ever, computer science suffers dramatically low retention rates and a declining percentage of women and minorities. Studies of the problem point to the overemphasis in computer science classes on abstraction over application, technical details instead of usability, and the stereotypical view of programmers as loners lacking creativity. In spring 2003, Georgia Institute of Technology trialed a new course, Introduction to Media Computation, which teaches programming and computation in the context of media creation and manipulation. Students implement PhotoShop-style filters and digital video special effects, splice sounds, and search Web pages. The course is open only to noncomputer science and nonengineering majors at Georgia Tech, such as liberal arts, management and architecture students. The course is supported through the use of a Web-based collaboration environment where students actively share and discuss their digital creations. The results have been dramatic. 120 students enrolled, 2/3 female, and only three students withdrew. By the end of the semester, the combined withdrawal, failure and D-grade rate had reached 11.5% - compared to 42.9% in the traditional introductory computer science course. 60% of the students who took media computation reported that they would be interested in taking an advanced version of the course; only 6% reported that they would otherwise be interested in taking more computer science. Results of the trial indicate that media computation motivates and engages an audience that is poorly served by traditional computer science courses.",
Moment: maintaining closed frequent itemsets over a stream sliding window,"This paper considers the problem of mining closed frequent itemsets over a sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of item-sets over a sliding-window. The selected itemsets consist of a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent item-sets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than previous approaches.","Itemsets,
Data mining,
Monitoring,
Data structures,
Memory management,
Tree data structures,
Association rules,
Computer science,
Time factors,
Costs"
Leveraging single rate schemes in multiple rate multicast congestion control design,"A significant impediment to deployment of multicast services is the daunting technical complexity of developing, testing and validating congestion control protocols fit for wide-area deployment. Protocols such as pragmatic general multicast congestion control (pgmcc) and TCP-friendly multicast congestion control (TFMCC) have recently made considerable progress on the single rate case, i.e., where one dynamic reception rate is maintained for all receivers in the session. However, these protocols have limited applicability, since scaling to session sizes beyond tens of participants with heterogeneous available bandwidth necessitates the use of multiple rate protocols. Unfortunately, while existing multiple rate protocols exhibit better scalability, they are both less mature than single rate protocols and suffer from high complexity. We propose a new approach to multiple rate congestion control that leverages proven single rate congestion control methods by orchestrating an ensemble of independently controlled single rate sessions. We describe a new multiple rate congestion control algorithm for layered multicast sessions that employs a single rate multicast congestion control as the primary underlying control mechanism for each layer. Our new scheme combines the benefits of single rate congestion control with the scalability and flexibility of multiple rates to provide a sound multiple rate multicast congestion control policy.","Control design,
Multicast protocols,
Testing,
Internet,
Bandwidth,
Scalability,
Computer science,
Centralized control,
Communication system control,
Impedance"
A fault tolerant protocol for massively parallel systems,"Summary form only given. As parallel machines grow larger, the mean time between failure shrinks. With the planned machines of near future, therefore, fault tolerance will become an important issue. The traditional method of dealing with faults is to checkpoint the entire application periodically and to start from the last checkpoint. However, such a strategy wastes resources by requiring all the processors to revert to an earlier state, whereas only one processor has lost its current state. We present a scheme for fault tolerance that aims at low overhead on the forward path (i.e. when there are no failures) and a fast recovery from faults, without wasting computation done by processors that have not faulted. The scheme does not require any individual component to be fault-free. We present the basic scheme and performance data on small clusters. Since it is based on Charm++ and Adaptive MPI, where each processor houses several virtual processors, the scheme has potential to reduce fault recovery time significantly, by migrating the recovering virtual processors.","Fault tolerant systems,
Protocols,
Fault tolerance,
Computer crashes,
Parallel machines,
Concurrent computing,
Costs,
Hardware,
Computer science,
Application software"
Prediction of protein coding regions in DNA sequences using Fourier spectral characteristics,"Existing discrete Fourier transform (DFT)-based algorithms for identifying protein coding regions in DNA sequences (S. Tiwari et al., 1997, D. Anastassiou, 2001, D. Kotlar et al., 2003) exploit the empirical observation that the spectrum of protein coding regions of length N nucleotides has a peak at frequency k=N/3. In this paper, we prove the aforementioned and several other empirical observations attributed to DNA sequences. Our analytical results lead to faster and more accurate DFT-based algorithms for predicting coding regions.","DNA,
Sequences,
Splicing,
Discrete Fourier transforms,
Frequency,
Protein engineering,
Prediction algorithms,
Computer science,
Algorithm design and analysis,
Genomics"
A mulitple level network approach for clock skew minimization with process variations,"We investigate the effect of multilevel network for clock skew. We first define the simplified RC circuit model of a hybrid clock mesh/tree structure. The skew reduction effect of shunt segment contributed by the mesh is derived analytically from the simplified model. The result indicates that the skew decreases proportionally to the exponential of -R/sub 1//R, where R/sub 1/ is the driving resistance of a leaf node in the clock tree and R is the resistance of a mesh segment. Based on our analysis, we propose a hybrid multilevel mesh and tree structure for global clock distribution. A simple optimization scheme is adopted to optimize the routing resource distribution of the multilevel mesh. Experimental results show that by adding a mesh to the bottom-level leaves of an H-tree, the clock skew can be reduced from 29.2 ps to 8.7 ps, and the multilevel networks with same total routing area can further reduce the clock skew by another 30%. We also discuss the inductive effect of mesh in the appendix. When the clock frequency is less than 4 GHz, our RC model remains valid for clock meshes with grounded shielding or using differential signals.","Clocks,
Tree data structures,
Routing,
Frequency,
Delay,
Registers,
Microprocessors,
Minimization,
Computer science,
Circuits"
Improved inapproximability of lattice and coding problems with preprocessing,"We show that the closest vector problem with preprocessing (CVPP) is NP-hard to approximate to within /spl radic/3-/spl epsi/ for any /spl epsi/>0. In addition, we show that the nearest codeword problem with preprocessing (NCPP) is NP-hard to approximate to within 3-/spl epsi/. These results improve previous results of Feige and Micciancio. We also present the first inapproximability result for the relatively nearest codeword problem with preprocessing (RNCP). Finally, we describe an n-approximation algorithm to CVPP.","Lattices,
Cryptography,
Vectors,
Polynomials,
Computational complexity,
Computer science,
Approximation algorithms,
Linear code,
Application software,
Decoding"
Topology design for service overlay networks with bandwidth guarantees,"The Internet still lacks adequate support for QoS applications with real-time requirements. In great part, this is due to the fact that provisioning of end-to-end QoS to traffic that traverses multiple autonomous systems (ASs) requires a level of cooperation between ASs that is difficult to achieve in the current architecture. Recently, service overlay networks have been considered as an approach to QoS deployment that avoids these difficulties. In this study, we address the problem of the topological synthesis of a service overlay network, where endsystems and nodes of the overlay network (provider nodes) are connected through ISPs that supports bandwidth reservations. We express the topology design problem as an optimization problem. Even though the design problem is related to the (in general NP-hard) quadratic assignment problem, we are able to show that relatively simple heuristic algorithms can deliver results that are sometimes close to the optimal solution.","Network topology,
Bandwidth,
Peer to peer computing,
IP networks,
Web and internet services,
Telecommunication traffic,
Computer science,
Application software,
Network synthesis,
Design optimization"
Sensory channel grouping and structure from uninterpreted sensor data,In this paper we focus on the problem of making a model of the sensory apparatus from raw uninterpreted sensory data as defined by Pierce and Kuipers (1997). The method relies on generic properties of the agent's world such as piecewise smooth effects of movement on sensory features. We extend a previously described algorithm with an information-theoretic distance metric that can find informational structure not found by the original algorithm. We also use the method to create metric projections of the sensory and motor systems of a robot. Data from a real robot show that the metric projections for example can be used to distinguish the vision sensors from all other sensors and also to find their functional layout. Finally we present an application of the method where the real layout of the vision sensors is found from scrambled vision data.,
Trapping conversational speech: extending TRAP/tandem approaches to conversational telephone speech recognition,"Temporal patterns (TRAP) and tandem MLP/HMM approaches incorporate feature streams computed from longer time intervals than the conventional short-time analysis. These methods have been used for challenging small- and medium-vocabulary recognition tasks, such as Aurora and SPINE. Conversational telephone speech recognition is a difficult large-vocabulary task, with current systems giving incorrect output for 20-40% of the words, depending on the system complexity and test set. Training and test times for this problem also tend to be relatively long, making rapid development quite difficult. In this paper we report experiments with a reduced conversational speech task that led to the adoption of a number of engineering decisions for the design of an acoustic front end. We then describe our results with this front end on a full-vocabulary conversational telephone speech task. In both cases the front end yielded significant improvements over the baseline.","Telephony,
Speech recognition,
Hidden Markov models,
System testing,
Speech analysis,
Acoustic testing,
Computer science,
Pattern analysis,
Acoustical engineering,
Design engineering"
Studying the evolution of software systems using evolutionary code extractors,Software systems are continuously changing and adapting to meet the needs of their users. Empirical studies are needed to better understand the evolutionary process followed by software systems. These studies need tools that can analyze and report various details about the software system's history. We propose evolutionary code extractors as a type of tool to assist in empirical source code evolution research. We present the design dimensions for such an extractor and discuss several of the challenges associated with automatically recovering the evolution of source code.,"Software systems,
Computer bugs,
Predictive models,
Resource management,
Telephony,
Software architecture,
Computer science,
Software tools,
History,
Monitoring"
Evaluating architectural stability with real options theory,Architectural stability refers to the extent to which a software architecture is flexible enough to respond to changes in stakeholders' requirements and the environment. We contribute to a model that exploits options theory to evaluate architectural stability. We describe how we have derived the model: the analogy and assumptions made; its formulation and possible interpretations. We use a refactoring case study to empirically evaluate the model. The results show that the model can provide insights into architectural stability and investment decisions related to the evolution of software systems.,"Stability,
Computer architecture,
Software architecture,
Investments,
Software systems,
Environmental economics,
Uncertainty,
Computer science,
Educational institutions,
Software engineering"
Efficient exploration of on-chip bus architectures and memory allocation,"Separation between computation and communication in system design allows the system designer to explore the communication architecture independently of component selection and mapping. We present an iterative two-step exploration methodology for bus-based on-chip communication architecture and memory allocation, assuming that memory traces from the processing elements are given from the mapping stage. The proposed method uses a static performance estimation technique to reduce the large design space drastically and quickly, and applies a trace-driven simulation technique to the reduced set of design candidates for accurate performance estimation. Since local memory traffic as well as shared memory traffic are involved in bus contention, memory allocation is considered as an important axis of the design space in our technique. The viability and efficiency of the proposed methodology are validated by two real-life examples, 4-channel digital video recorder (DVR) and an equalizer for OFDM DVB-T receiver.",
Boundary Matting for View Synthesis,"In the last few years, new view synthesis has emerged as an important application of 3D stereo reconstruction. While the quality of stereo has improved, it is still imperfect, and a unique depth is typically assigned to every pixel. This is problematic at object boundaries, where the pixel colors are mixtures of foreground and background colors. Interpolating views without explicitly accounting for this effect results in objects with a ""cut-out"" appearance. To produce seamless view interpolation, we propose a method called boundary matting, which represents each occlusion boundary as a 3D curve. We show how this method exploits multiple views to perform fully automatic alpha matting and to simultaneously refine stereo depths at the boundaries. The key to our approach is the unifying 3D representation of occlusion boundaries estimated to sub-pixel accuracy. Starting from an initial estimate derived from stereo, we optimize the curve parameters and the foreground colors near the boundaries. Our objective function maximizes consistency with the input images, favors boundaries aligned with strong edges, and damps large perturbations of the curves. Experimental results suggest that this method enables high-quality view synthesis with reduced matting artifacts.","Stereo vision,
Layout,
Interpolation,
Computer vision,
Computer science,
Application software,
Stereo image processing,
Image reconstruction,
Sampling methods,
Sprites (computer)"
Hybrid HMM/SVM model for the analysis and segmentation of teleoperation tasks,"The automatic execution of a complex task requires the identification of an underlying mental model to derive a possible task control sequence. The model aims at analysing and segmenting the task in simpler sub-tasks. As an example of a complex task, in this paper we consider teleoperation where a person commands a remote robot. This paper presents a new modeling approach using hidden Markov models (HMM) and support vector machines (SVM) to analyse the force/torque signals of a teleoperation task. The task is divided into simpler sub-tasks and the model is used to segment the signals in each sub-task. The segmentation gives informations on the system behavior identifying the changes of the model states. Peg in hole force/torque data are used for testing the model. The results are consistent with the literature with respect to off-line analysis, whereas a significant increase of performance is achieved for on-line analysis.","Hidden Markov models,
Support vector machines,
Support vector machine classification,
Performance analysis,
Neural networks,
Probability distribution,
Computer science,
Cognitive science,
Automatic control,
Robotics and automation"
Global weather prediction and high-end computing at NASA,The authors demonstrate the current capabilities of NASA's finite-volume General Circulation Model in high-resolution global weather prediction and discuss its development path in the foreseeable future. This model is a prototype of a future NASA Earth-modeling system intended to unify development activities across various disciplines within NASA's Earth Science Enterprise.,"Weather forecasting,
NASA,
Atmospheric modeling,
Predictive models,
Atmosphere,
Equations,
Physics,
Hurricanes,
Oceans,
Data assimilation"
Improving network system security with function extraction technology for automated calculation of program behavior,"Malicious attacks on systems are a threat to business, government, and defense. Many attacks exploit system behavior unknown to the developers who created it. In today's state of art, software engineers have no practical means to determine how a sizable program will behave in all circumstances of use. This sobering reality lies at the heart of many problems in security and survivability. If full behavior is unknown, so too are embedded errors, vulnerabilities, and malicious code. This paper describes function-theoretic foundations for automated calculation of full program behavior. These foundations treat program control structures as mathematical functions or relations. The function, or behavior, of control structures can be abstracted in a stepwise process into procedure-free expressions that specify their net functional effects. Problems of computability and complexities of language semantics appear to have engineering solutions. Automated behavior calculation will add rigor to security and survivability engineering.","Security,
Software engineering,
Open source software,
Automatic control,
Programming profession,
Humans,
Automation,
Government,
Art,
Heart"
Non-uniform information dissemination for dynamic grid resource discovery,"Effective use of computational grids requires up-to-date information about widely-distributed resources within it - a challenging problem given the scale of the grid, and the continuously changing state of the resources. We propose nonuniform information dissemination protocols to efficiently propagate information to distributed repositories, without requiring flooding or centralized approaches. Capitalizing on the observation that grid resources are of more interest to nearby users, we disseminate resource information with a frequency and resolution inversely proportional to the distance from the resource. Results indicate a significant reduction in the overhead compared to uniform dissemination to all repositories.","Grid computing,
Protocols,
Computer applications,
Computer science,
Floods,
Frequency,
Programming profession,
Engineering profession,
US Department of Energy,
Processor scheduling"
Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks,"Depending on the connectivity, recurrent networks of simple computational units can show very different types of dynamics, ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time-varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular, we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes place. Employing a recently developed framework for analyzing real-time computations, we show that only near the critical boundary can such networks perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems that are capable of doing complex computational tasks should operate near the edge of chaos, that is, the transition from ordered to chaotic dynamics.",
Analysis of sparse-partial wavelength conversion in wavelength-routed WDM networks,"Wavelength conversion has been shown as one of the key techniques to improve blocking performance in a wavelength-routed all-optical network. Given that wavelength converters nowadays remain very expensive, how to make effective use of a limited number of wavelength converters becomes an important issue. We propose sparse-partial wavelength conversion (SPWC) network architecture with the inherent flexibility that can facilitate network carriers to migrate the optical backbone to support wavelength conversion. We demonstrate that this network architecture can significantly save the number of wavelength converters, yet achieving excellent blocking performance. Theoretical and simulation results indicate that, the performance of a wavelength-routed WDM network with only 1-5%of wavelength conversion capability is very close to that with full-complete wavelength conversion capability.","Intelligent networks,
WDM networks,
Optical wavelength conversion,
Wavelength conversion,
Wavelength routing,
Wavelength division multiplexing,
Optical switches,
Computer science,
Spine,
Costs"
Biased support vector machine for relevance feedback in image retrieval,"Recently, support vector machines (SVMs) have been engaged on relevance feedback tasks in content-based image retrieval. Typical approaches by SVMs treat the relevance feedback as a strict binary classification problem. However, these approaches do not consider an important issue of relevance feedback, i.e. the unbalanced dataset problem, in which the negative instances largely outnumber the positive instances. For solving this problem, we propose a novel technique to formulate the relevance feedback based on a modified SVM called biased support vector machine (Biased SVM or BSVM). Mathematical formulation and explanations are provided for showing the advantages. Experiments are conducted to evaluate the performance of our algorithms, in which promising results demonstrate the effectiveness of our techniques.","Support vector machines,
Image retrieval,
Content based retrieval,
Support vector machine classification,
Negative feedback,
Humans,
Machine learning,
Neurofeedback,
Bayesian methods,
Computer science"
Realistic Modeling and Svnthesis of Resources for Computational Grids,"Understanding large Grid platform configurations and generating representative synthetic configurations is critical for Grid computing research. This paper presents an analysis of existing resource configurations and proposes a Grid platform generator that synthesizes realistic configurations of both computing and communication resources. Our key contributions include the development of statistical models for currently deployed resources and using these estimates for modeling the characteristics of future systems. Through the analysis of the configurations of 114 clusters and over 10,000 processors, we identify appropriate distributions for resource configuration parameters in many typical clusters. Using well-established statistical tests, we validate our models against a second resource collection of 191 clusters and over 10,000 processors, and show that our models effectively capture the resource characteristics found in real world resource infrastructures. These models are realized in a resource generator, which can be easily recalibrated by running it on a training sample set.","Grid computing,
Computer science,
Mesh generation,
Monitoring,
Resource management,
Permission,
Computer networks,
Supercomputers,
Network synthesis,
Testing"
Flexible spatial models for grouping local image features,"A key step for the effective use of local image features (i.e., highly distinctive and robust features) for recognition or image matching is the appropriate grouping of feature matches. Spatial constraints are important in this grouping because, during a recognition process, they allow for the reduction of the number of hypotheses that must be verified and also reduce the number of false positives present in each of these hypotheses. A common choice for this grouping task is to use the Hough transform on the global spatial transformation parameters of the hypothesized matches. Here, instead, we use semi-local spatial constraints which allow for a greater range of shape deformations. A comparison with Hough transform shows that our method is more robust to both rigid and non-rigid deformations. Its functionality is demonstrated in an exemplar-based object recognition system that deals well with severe non-rigid deformations. We also show the efficacy of our flexible spatial grouping far long range motion problems.","Robustness,
Image databases,
Spatial databases,
Image recognition,
Object recognition,
Gabor filters,
Object detection,
Computer science,
Image matching,
Shape"
Usage policy-based CPU sharing in virtual organizations,"Resource sharing within grid collaborations usually implies specific sharing mechanisms at participating sites. Challenging policy issues can arise within virtual organizations (VOs) that integrate participants and resources spanning multiple physical institutions. Resource owners may wish to grant to one or more VOs the right to use certain resources subject to local policy and service level agreements, and each VO may then wish to use those resources subject to VO policy. Thus, we must address the question of what usage policies (UPs) should be considered for resource sharing in VOs. As a first step in addressing this question, we develop and evaluate different UP scenarios within a specialized context that mimics scientific grids within which the resources to be shared are computers. We also present a UP architecture and define roles and functions for scheduling resources in such grid environments while satisfying resource owner policies.",
Precisiated natural language (PNL) - toward an enlargement of the role of natural languages in scientific theories,"This work discusses the precisiated natural language, which is a subset of a natural language. This subset is equipped with constraint-centered semantics (CSNL) and is translatable into what is called the generalized constraint language. The ways does the concept of a precisiated natural language can serve to enlarge the role of the natural languages in scientific theories were also discussed.","Natural languages,
Random variables,
Probability distribution,
Fuzzy logic,
Computer science,
Laboratories,
Telephony,
Humans,
Robustness,
Fuzzy sets"
Analysis of point-to-point packet delay in an operational network,"In this paper we perform a detailed analysis of point-to-point packet delay in an operational tier-1 network. The point-to-point delay is the time between a packet entering a router in one PoP (an ingress point) and its leaving a router in another PoP (an egress point). It measures the one-way delay experienced by packets from an ingress point to an egress point across an ISP's network and provides the most basic information regarding the delay performance of the ISP's network. Using packet traces captured in the operational network, we obtain precise point-to-point packet delay measurements and analyze the various factors affecting them. Through a simple, step-by-step, systematic methodology and careful data analysis, we identify the major network factors that contribute to point-to-point packet delay and characterize their effect on the network delay performance. Our findings are: 1) delay distributions vary greatly in shape, depending on the path and link utilization; 2) after constant factors dependent only on the path and packet size are removed, the 99th percentile variable delay remains under 1 ms over several hops and under link utilization below 90% on a bottleneck; 3) a very small number of packets experience very large delay in short bursts",
Dual frame motion compensation with uneven quality assignment,"Video codecs that use motion compensation have shown PSNR gains from the use of multiple frame prediction, in which more than one past reference frame is available for motion estimation. In dual frame motion compensation, one short-term reference frame and one long-term reference frame are available for prediction. In this paper, we propose a dual frame motion compensation technique that allocates bits unevenly among frames to periodically create a high-quality frame that serves as the long-term reference frame for some time. By modifying an MPEG-4 encoder to use this technique on a set of video sequences, we show that it outperforms a normal dual frame motion compensation scheme in which the long-term reference frames are regular frames that are not allocated any extra rate.","Motion compensation,
Video codecs,
Motion estimation,
MPEG 4 Standard,
Video sequences,
Transform coding,
Bit rate,
Delay effects,
Computer science,
PSNR"
Evaluation of online assessment: the role of feedback in learner-centered e-learning,"Advancement of the information and communication technologies enables the integration of technology with daily activities and education is not an exception. E-learning, which applies the concept of open and distance learning is learning through the Internet. It had been reviewed as an efficient knowledge transfer mechanism. E-learning is seen as a future application worldwide, promoting life long learning by enabling learners to learn anytime, anywhere and at the learner's pace. This paper presents the evaluation of an online test based on a case study of an e-commerce course offered by the Computation Department, University of Manchester Institute of Science and Technology (UMIST). The main aim of the online test is to provide 'rich' feedback to students, which is one of the requirements of the learner-centered learning paradigm. The online test, in the form of multiple choice questions, provides feedback through automatic grading, providing correct answers and referring the students to the learning content which explains the correct answers. Evaluation of the online test was based on two criteria: functionality and usability. In terms of functionality, evaluation was meant to get the students' view of the feedback provided by the system, while in terms of usability, the evaluation sought to ensure that the system not only functions as expected by the users but is also usable. Results show that the online test is suitable for online-learning and provides rich feedback.","Feedback,
Electronic learning,
Testing,
Usability,
Communications technology,
Educational technology,
Computer aided instruction,
Internet,
Knowledge transfer,
Continuing professional development"
A weight-based map matching method in moving objects databases,"In location management, the trajectory represents the motion of a moving object in 3D space-time, i.e., a sequence (x, y, t). Unfortunately, location technologies, cannot guarantee error-freedom. Thus, map matching (a.k.a. snapping), matching a trajectory to the roads on the map, is necessary. We introduce a weight-based map matching method, and experimentally show that, for the offline situation, on average, our algorithm can get up to 94% correctness depending on the GPS sampling interval.","Databases,
Global Positioning System,
Trajectory,
Road vehicles,
Computer science,
Space technology,
Computer errors,
Sampling methods,
Spatiotemporal phenomena,
Error correction"
A service discovery: a service broker approach,"Mobile computing and pervasive computing introduce a more dynamic network environment. As a side-effect, location of services becomes problematic for clients accessing the services. A number of service discovery solutions aim to raise the abstraction level of the location to solve the problem; the trend is to discover services not based on network addresses but based on service usage intentions. Even though the current service discovery solutions (Bluetooth, Jini, universal Plug'N'Play, salutation, and service location protocol) succeed in doing this, they have a limited support for interoperability. Nonetheless, a need for federating service discovery domains is imminent due to the nature of mobile and pervasive computing. In this paper we design a middleware broker component, service broker, which is capable of connecting service discovery solutions. We also introduce shortly the implementation of the prototype.","Pervasive computing,
Mobile computing,
Joining processes,
Protocols,
Middleware,
Multimedia computing,
Bluetooth,
Computer networks,
Telecommunication computing,
Prototypes"
Statistical quality of service guarantee for temporal consistency of real-time data objects,"In this paper, we study the problem of temporal consistency maintenance where a certain degree of temporal inconsistency is tolerable. We propose a suite of statistical more-less (SML) approaches to tradeoff of quality of service (QoS) of temporal consistency against the number of supported transactions. We begin with a base-line algorithm, SML-BA, which provides the requested QoS of temporal consistency. We then propose SML with optimization (SML-OPT) to further improve the QoS by better utilizing the excessive CPU capacity. Finally, we enhance SML-OPT with a slack reclaiming scheme (SML-SR). The reclaimed slacks are used to process jobs whose required computation time is larger than the guaranteed computation time. Simulation experiments are conducted to compare the performance of these schemes (SML-BA, SML-OPT and SML-SR) together with the deterministic more-less and half-half schemes. Our results show that the SML schemes are effective in trading off the schedulability of transactions and the QoS guaranteed. Moreover, SML-SR performs best and offers a significant QoS improvement over SML-BA and SML-OPT.","Quality of service,
Processor scheduling,
Transaction databases,
Real time systems,
Computer science,
Drives,
Computational modeling,
Database systems,
Testing,
Sampling methods"
Using geometric primitives to calibrate traffic scenes,"In this paper, we address the problem of recovering the intrinsic and extrinsic parameters of a camera or a group of cameras in a setting overlooking a traffic scene. Unlike many other settings, conventional camera calibration techniques are not applicable in this case. We present a method that uses certain geometric primitives commonly found in traffic scenes in order to recover calibration parameters. These primitives provide needed redundancy and are weighted depending on the significance of there corresponding image features. We show experimentally that these primitives are capable of achieving accurate results suitable for most traffic monitoring applications.","Layout,
Cameras,
Calibration,
Robot vision systems,
Application software,
Computer science,
Monitoring,
Redundancy,
Stereo vision,
Robustness"
How Colorful Was Your Day? Why Questionnaires Cannot Assess Presence in Virtual Environments,"This paper argues that a scientific basis for “presence” as it's usually understood in virtual environments research, can not be established on the basis of postexperience presence questionnaires alone. To illustrate the point, an arbitrary mental attribute called “colorfulness of the experience” is conjured up, and a set of questions administered to 74 respondents with an online questionnaire. The results suggested that colorfulness of yesterday's experiences was associated with the extent to which a person accomplished their tasks, and also associated with yesterday being a “good”, “pleasant”, but not frustrating day. The meaning lessness of this analysis illustrates that the equivalent methodology used by presence researchers, may, similarly, bring into being the idea of presence in the minds of VE participants. However, it is argued that there can be no evidence on this methodological basis that presence played any role in their actual mental activity or behavior at the time of the experience. It is concluded that presence researchers must move away from heavy reliance on questionnaires in order to make any progress in this area.",
The effect of router buffer size on HighSpeed TCP performance,"We study the effect of the IP router buffer size on the throughput of HighSpeed TCP (HSTCP). We are motivated by the fact that, in high speed routers, the buffer size is important, as a large buffer size might be a constraint. We first derive an analytical model for HighSpeed TCP and we show that for a small buffer size, equal to 10% of the bandwidth-delay product, HighSpeed TCP can achieve more than 90% of the bottleneck capacity. We also show that setting the buffer size equal to 20% can increase the utilization of HighSpeed TCP up to 98%. On the contrary, setting the buffer size to less than 10% of the bandwidth-delay product can decrease HighSpeed TCP's throughput significantly. We also study the performance effects under both DropTail and RED AQM (active queue management). Analytical results obtained using a fixed-point approach are compared to those obtained by simulation.","TCPIP,
Throughput,
Analytical models,
Bandwidth,
Protocols,
Proposals,
Space technology,
Space heating,
Delay effects,
Computer science"
Parallel fuzzy cognitive maps as a tool for modeling software development projects,"Fuzzy cognitive maps (FCM) are useful tool for simulating and analyzing dynamic systems. The FCMs have a very simple structure, and thus are very easy to comprehend and use. Despite of the simplicity, they have been successfully adopted in many different areas, such as electrical engineering, medicine political science, international relations, military science, history: supervisory systems, etc. Software development is a complex process, and there are many factors that influence its progress. To effectively handle larger development processes, they are usually divided into subtasks, which are assigned to different teams of workers, and often are performed in parallel. However, some constraints that impose particular sequence of realization of these subtasks, i.e. some tasks cannot be started before completing others, usually exist. Proper division of a project into subtasks and establishing relations between them are essential to correctly manage software projects. Neglecting these constraints often leads to problems that, in consequence, cause misestimating the overall time and budget. This paper introduces a new architecture of FCM, which combines a number of simple FCM models that work simultaneously into a novel parallel FCMs model. It uses a special purpose coordinator module to synchronize simulation of each FCM model. This approach extends application of FCMs to complex systems, which contain multiple subtasks that run in parallel, and thus must be simulated with multiple FCM models. In addition, application of parallel FCMs to analyze and design software development processes is presented. FCM models are focused on simulating and analyzing factors, such as progress and communication, and their relationships, which are based on theoretical research studies and practical implementations. The parallel FCM model is used to simulate complex projects where multiple tasks exist. The paper is based on our previous work where FCM models, which describe relationships between the above factors for individual development tasks, were developed. The newly proposed architecture allows for efficient analysis of dependences between tasks performed in parallel.","Fuzzy cognitive maps,
Programming,
Analytical models,
Application software,
Medical simulation,
Electrical engineering,
International relations,
History,
Project management,
Computer architecture"
Query aggregation for providing efficient data services in sensor networks,"Providing efficient data services is one of the fundamental requirements for wireless sensor networks. The data service paradigm requires that the application submit its requests as queries and the sensor network transmits the requested data to the application. While most existing work in this area focuses on data aggregation, not much attention has been paid to query aggregation. For many applications, especially ones with high query rates, query aggregation is very important. We study a query aggregation-based approach for providing efficient data services. In particular: (1) we propose a multi-layered overlay-based framework consisting of a query manager and access points (nodes), where the former provides the query aggregation plan and the latter executes the plan; (2) we design an effective query aggregation algorithm to reduce the number of duplicate/overlapping queries and save overall energy consumption in the sensor network Our performance evaluations show that by applying our query aggregation algorithm, the overall energy consumption can be significantly reduced and the sensor network lifetime can be prolonged correspondingly.","Intelligent networks,
Acoustic sensors,
Wireless sensor networks,
Distributed databases,
Data communication,
Computer science,
Energy consumption,
Temperature sensors,
Educational institutions,
Data engineering"
Object tracking by adaptive feature extraction,"Tracking objects in the high-dimensional feature space is not only computationally expensive but also functionally inefficient. Selecting a low-dimensional discriminative feature set is a critical step to improve tracker performance. A good feature set for tracking can differ from frame to frame due to the changes in the background against the tracked object, and due to an on-line algorithm that adaptively determines a advantageous distinctive feature set. In this paper, multiple heterogeneous features are assembled, and likelihood images are constructed for various subspaces of the combined feature space. Then, the most discriminative feature is extracted by principal component analysis (PCA) based on those likelihood images. This idea is applied to the mean-shift tracking algorithm [D. Comaniciu et al., June 2000], and we demonstrate its effectiveness through various experiments.","Feature extraction,
Target tracking,
Principal component analysis,
Histograms,
Image color analysis,
Computer science,
Educational institutions,
Assembly,
Particle filters,
Particle tracking"
Perfect constant-weight codes,"In his pioneering work from 1973, Delsarte conjectured that there are no nontrivial perfect codes in the Johnson scheme. Many attempts were made, during the years which followed, to prove Delsarte's conjecture, but only partial results have been obtained. We survey all these attempts, and prove some new results having the same flavor. We also present a new method, taking a different approach, which we hope can lead to the settling of this conjecture. We show how this new method rules out sets of parameters as well as specific given parameters.","Codes,
Hamming distance,
Galois fields,
Polynomials,
Computer science,
Materials science and technology"
Institutional theory: a new perspective for research into IS/IT security in organisations,"The aim of this position paper is to argue for the suitability of an institutional perspective in IS/IT (information systems/information technology) security research. Institutional theory, including some of its central concepts, is presented, along with examples of how it has been used in information systems research. A discussion of how the theory could benefit managerial IS/IT research concludes the paper.","Information security,
Computer security,
Communication system security,
Humans,
Management information systems,
Open systems,
Cultural differences,
Paper technology,
Information technology,
Equations"
OnCall: defeating spikes with a free-market application cluster,"Even with reasonable overprovisioning, today's Internet application clusters are unable to handle major traffic spikes and flash crowds. As an alternative to fixed-size, dedicated clusters, we propose a dynamically-shared application cluster model based on virtual machines. The system is dubbed ""OnCall"" for the extra computing capacity that is always on call in case of traffic spikes. OnCall's approach to spike management relies on the use of an economically-efficient marketplace of cluster resources. OnCall works autonomically by allowing applications to trade computing capacity on a free market through the use of automated market policies; the appropriate applications are then automatically activated on the traded nodes. As demonstrated in our prototype implementation, OnCall allows applications to handle spikes while still maintaining inter-application performance isolation and providing useful resource guarantees to all applications on the cluster.",
A framework for selecting a location based service (LBS) strategy and service portfolio,Location-based services (LBS) add value by exploiting knowledge of a mobile device's location. The arrival of broadband mobile data networks has revived interest in these services which are seen as a major source of revenue growth. We propose a framework that supports the generation and assessment of LBS service concepts. The proposed framework offers a promising foundation for a design theory of LBS strategies and services. The analysis of the framework points towards avenues for future research by highlighting deficiencies in current knowledge of mobility and location awareness.,"Portfolios,
Context awareness,
Business,
Computer architecture,
Management information systems,
Knowledge management,
Mobile radio mobility management,
Heart,
Investments,
Intelligent systems"
An efficient three-stage classifier for handwritten digit recognition,"This work proposes an efficient three-stage classifier for handwritten digit recognition based on NN (neural network) and SVM (support vector machine) classifiers. The classification is performed by 2 NNs and one SVM. The first NN is designed to provide a low misclassification rate using a strong rejection criterion. It is applied on a small set of easy to extract features. Rejected patterns are forwarded to the second NN that uses additional, more complex features, and utilizes a well-balanced rejection criterion. Finally, rejected patterns from the second NN are forwarded to an optimized SVM that considers only the ""top k"" classes as ranked by the NN. This way a very fast SVM classification is obtained without sacrificing the classifier accuracy. The obtained recognition rate is among the best on the MNIST database and the classification time is much better compared to the single SVM applied on the same feature set.","Handwriting recognition,
Neural networks,
Support vector machines,
Support vector machine classification,
Feature extraction,
Spatial databases,
Information technology,
Mathematics,
Computer science,
Training data"
Grid Services and Service Discovery for HLA-Based Distributed Simulation,"Modelling and simulation permeate all areas of business, science and engineering and increasingly complex simulation systems often require huge computing resources and data sets that are geographically distributed. The widely adopted platform for building distributed simulations is the High Level Architecture (HLA). Deficiencies associated with HLA have been well discussed in the literature. The advent of Grid technology enables the use of distributed computing resources and facilitates the access of geographically distributed data. In this paper, we propose a framework for executing large-scale distributed simulations using Grid services. The framework addresses some of the deficiencies of HLA, including dynamic discovery and resource utilization. End-users can construct large-scale distributed simulations using this framework with ease.","Computational modeling,
Distributed computing,
Large-scale systems,
Web services,
Resource management,
Concurrent computing,
Data engineering,
Computer simulation,
Computer architecture,
Buildings"
"A high performance, low complexity algorithm for compile-time task scheduling in heterogeneous systems","Summary form only given. The heterogeneous computing environment is an interesting computing platform due to the fact that a single parallel architecture may not be adequate for exploiting all of a program's available parallelism. In some cases, heterogeneous systems have been shown to produce higher performance for lower cost than a single large machine. Task scheduling is the key issue when aiming at high performance in this kind of environment. A large number of scheduling heuristics have been presented in the literature, most of them target only homogeneous computing systems. We present a simple scheduling algorithm based on list-scheduling and task-duplication on a bounded number of heterogeneous machines called heterogeneous critical parents with fast duplicator (HCPFD). The analysis and experiments have shown that HCPFD outperforms on average all other higher complexity algorithms.","Scheduling algorithm,
Processor scheduling,
Concurrent computing,
Costs,
Distributed computing,
Computer science,
Parallel architectures,
Parallel processing,
Algorithm design and analysis,
Computational efficiency"
IEEE 802.11 fragmentation-aware energy-efficient ad-hoc routing protocols,"We define techniques to compute energy-efficient paths, using the IEEE 802.11 fragmentation mechanism, within the framework of on-demand routing protocols. We focus on one specific on-demand routing protocol, namely the ad-hoc on-demand vector routing protocol (AODV), and show how it should be adapted to compute energy-efficient paths. The choice of energy-efficient paths depends on link error rates on different wireless links, which in turn depend on channel noise. We show how our scheme accounts for such channel characteristics in computing such paths and how it exploits the IEEE 802.11 fragmentation mechanism to generate optimum energy-efficient paths. We perform a detailed study of the AODV protocol and our energy-efficient variants, under various noise and node mobility conditions. Our results show that our proposed variants of on-demand routing protocols can achieve orders of magnitude improvement in energy-efficiency of reliable data paths.",
On energy-constrained real-time scheduling,"In this paper, we explore the feasibility and performance optimization problems for real-time systems that must remain functional during an operation/mission with a fixed, initial energy budget. We show that the feasibility problem is NP-hard in the context of systems with dynamic voltage scaling (DVS) capability and discrete speed levels. Then, we focus on energy-constrained periodic task systems where the available energy budget is not sufficient to meet all the deadlines. We propose techniques to maximize the total number of deadlines met and the total reward (utility) while guaranteeing the completion of the mission and a minimum performance for each task. We consider separately: (i) systems with or without DVS capability, and (ii) off-line (static) and on-line (dynamic) solutions to select most valuable jobs for execution. We also discuss the tractability of the involved optimization problems. Our on-line algorithms combine job promotion, job demotion and speed reduction techniques to maximize the system performance while guaranteeing the completion of the mission. We evaluate our schemes through simulations and show that the on-line schemes can yield significant performance improvements over static solutions.",
"Segmenting, modeling, and matching video clips containing multiple moving objects","This paper presents a novel representation for dynamic scenes composed of multiple rigid objects that may undergo different motions and be observed by a moving camera. Multi-view constraints associated with groups of affine-invariant scene patches and a normalized description of their appearance are used to segment a scene into its rigid parts, construct three-dimensional protective, affine, and Euclidean models of these parts, and match instances of models recovered from different image sequences. The proposed approach has been implemented, and it is applied to the detection and recognition of moving objects in video sequences and the identification of shots that depict the same scene in a video clip (shot matching).","Layout,
Image segmentation,
Shape,
Image sequences,
Indexing,
Computer science,
Cameras,
Object detection,
Gunshot detection systems,
Video sequences"
How to have a successful free software project,"Some free software projects have been extremely successful. This rise to prominence can be attributed to the high quality and suitability of the software. This quality and suitability is achieved through an elaborate peer-review process performed by a large community of users, who act as co-developers to identify and correct software defects and add features. Although this process is crucial to the success of free software projects, there is more to the free software development than the creation of a 'bazaar'. In this paper we draw on existing free software projects to define a lifecycle model for free software. This paper then explores each phase of the lifecycle model and agrees that, while the bazaar phase attracts the most attention, it is the initial modular design that accommodates diverse interventions. Moreover, it is the period of transition from the initial group to the larger community based development that is crucial in determining whether a free software project will succeed or fail.","Open source software,
Software quality,
Programming,
Software engineering,
Linux,
Collaborative software,
Computer science,
Software performance,
Buildings,
Terminology"
Data dissemination over wireless sensor networks,"In sensor networks, it is crucial to design and employ energy-efficient communication protocols since nodes are battery-powered, and thus their lifetimes are limited. We propose a data dissemination protocol for periodic data updates in wireless sensor networks, called SAFE (sinks accessing data from environments), which attempts to save energy through data delivery path sharing among multiple sinks that have common interests. Simulation results show that the proposed protocol is energy-efficient as well as scalable to a large sink population.","Wireless sensor networks,
Access protocols,
Routing protocols,
Energy efficiency,
Wireless application protocol,
Batteries,
Computer science,
Unicast,
Radio communication,
Network topology"
Excellence in computer simulation,"Excellent computer simulations are done for a purpose. The most valid purposes are to explore uncharted territory, to resolve a well-posed scientific or technical question, or to make a good design choice. Stand-alone modeling can serve the first purpose; the other two goals need a full integration of the modeling effort into a scientific or engineering program. Some excellent work, much of it related to the US Department of Energy's laboratories, is reviewed. Some less happy stories are recounted.",
Tracetree: a scalable mechanism to discover multicast tree topologies in the Internet,"The successful deployment of multicast in the Internet requires the availability of good network management solutions. Discovering multicast tree topologies is an important component of this task. Network managers can use topology information to monitor and debug potential multicast forwarding problems. In addition, the collected topology has several other uses, for example, in reliable multicast transport protocols, in multicast congestion control protocols, and in discovering network characteristics. We present a mechanism for discovering multicast tree topologies using the forwarding state in the network. We call our approach tracetree. First, we present the basic operation of tracetree. Then, we explore various issues related to its functionality (e.g., scalability, security, etc.). Next, we provide a detailed evaluation by comparing it to the currently available alternatives. Finally, we discuss a number of deployment issues. We believe that tracetree provides an efficient and scalable mechanism for discovering multicast tree topologies and therefore fills an important void in the area of multicast network management.","Internet,
Network topology,
Multicast protocols,
Transport protocols,
Monitoring,
Routing,
IP networks,
Computer science,
Intelligent networks,
Availability"
Classification error rate for quantitative evaluation of content-based image retrieval systems,"A major problem in the field of content-based image retrieval is the lack of a common performance measure which allows the researcher to compare different image retrieval systems in a quantitative and objective manner. We analyze different proposed performance evaluation measures, select an appropriate one, and give quantitative results for four different, freely available image retrieval tasks using combinations of features. This work gives a concrete starting point for the comparison of content-based image retrieval systems. An appropriate performance measure and a set of databases are proposed and results for different retrieval methods are given.","Error analysis,
Image retrieval,
Content based retrieval,
Image databases,
Information retrieval,
Performance analysis,
Image analysis,
Measurement standards,
Computer science,
Concrete"
An effective support vector machines (SVMs) performance using hierarchical clustering,"The training time for SVMs to compute the maximal marginal hyper-plane is at least O(N/sup 2/) with the data set size N, which makes it nonfavorable for large data sets. This work presents a study for enhancing the training time of SVMs, specifically when dealing with large data sets, using hierarchical clustering analysis. We use the dynamically growing self-organizing tree (DGSOT) algorithm for clustering because it has proved to overcome the drawbacks of traditional hierarchical clustering algorithms. Clustering analysis helps find the boundary points, which are the most qualified data points to train SVMs, between two classes. We present a new approach of combination of SVMs and DGSOT, which starts with an initial training set and expands it gradually using the clustering structure produced by the DGSOT algorithm. We compare our approach with the Rocchio Bundling technique in terms of accuracy loss and training time gain using two benchmark real data sets.","Support vector machines,
Clustering algorithms,
Bagging,
Testing,
Buildings,
Partitioning algorithms,
Computer science,
Support vector machine classification,
Kernel,
Data mining"
Enforcing policies in pervasive environments,"This work presents an architecture and a proof of concept implementation of a security infrastructure for mobile devices in an infrastructure based pervasive environment. The security infrastructure primarily consists of two parts, the policy engine and the policy enforcement mechanism. Each mobile device within a pervasive environment is equipped with its own policy enforcement mechanism and is responsible for protecting its resources. A mobile device consults the nearest policy server, notifies its current state including its present user, network presence, other accessible devices and location information if available. Using this information the policy server queries the ""Rei"" engine to dynamically create a policy certificate and issues it to the requesting device. The system wide policy is described in a semantic language ""Rei"", a lightweight and extensible language which is able to express comprehensive policies using domain specific information. The ""Rei"" policy engine is able to dynamically decide what rights, prohibitions, obligations, dispensations an actor has on the domain actions. A policy certificate is created and issued to the device. The policy certificate contains a set of granted permissions and a validity period and scope within which the permissions are valid. The policy certificate can be revoked by the policy enforcer based on expiration of the validity period or a combination of timeout, loss of contact with an assigned network. X.509 based public key infrastructure is used to provide identification and authentication.",
Investigation on single-electron dynamics in coupled GaAs-AlGaAs quantum wires,"The aim of this paper is the study of the single-electron coherent propagation in a quantum-computing gate made of coupled quantum wires. The structure under investigation is based on a two-dimensional (2-D) electron gas realized in a modulation-doped GaAs-AlGaAs heterostructure. A number of surface electrodes are used to form one-dimensional channels. The profile of the conduction band at the heterojunction has been computed numerically by solving the three-dimensional Poisson equation on the whole structure at 300 mK. Finally, a single-electron wavefunction is propagated within the so-formed quantum wire geometry by means of a 2-D, time-dependent Schro/spl uml/dinger solver. Results are shown for a single-qubit rotation gate implementing a quantum-NOT transformation. This work is part of a feasibility study on a solid-state realization of a universal set of quantum gates.","Wires,
Electrons,
Solid state circuits,
Electrodes,
Heterojunctions,
Quantum computing,
Gallium arsenide,
Epitaxial layers,
Geometry,
Computer science"
WS-trustworthy: a framework for Web services centered trustworthy computing,"The emerging paradigm of Web services has been gaining significant momentum in the recent years since it offers a promising way to facilitate business-to-business (B2B) collaboration. However, it is not clear that this new model of Web services provides any measurable increase in computing trustworthiness. We propose a generic framework to control the trustworthiness of computing in the domain of Web services. A layered model is established to highlight four key elements: resources, policies, validation processes, and management. The robustness of this model exhibits its flexibility and extensibility. Examples utilizing our framework are reported.","Web services,
Web and internet services,
Simple object access protocol,
Application software,
Collaborative software,
System testing,
Computer science,
Collaboration,
Resource management,
Robustness"
A delay-based approach using fuzzy logic to improve TCP error detection in ad hoc networks,"In recent years, a great deal of effort has been devoted to make the TCP protocol more resilient to the random packet losses inherent in the wireless channels in ad hoc networks. In this paper, we investigate the use of fuzzy logic theory for assisting the TCP error detection mechanism in such networks. An elementary fuzzy logic engine is presented as an intelligent technique for discriminating packet loss due to congestion from packet loss by wireless induced errors. The architecture of the proposed fuzzy-based error detection mechanism is also introduced and discussed. The full approach, for inferring the internal state of the network, relies on round trip time (RTT) measurements only. Hence, this is an end-to-end scheme which requires only end nodes cooperation. Preliminary simulation evaluations show how viable this approach may be.","Fuzzy logic,
Intelligent networks,
Ad hoc networks,
Computer errors,
Wireless application protocol,
Time measurement,
Bit error rate,
Access protocols,
Computer science,
Mathematics"
Cluster Computing on the Fly: resource discovery in a cycle sharing peer-to-peer system,"Peer-to-peer computing, the harnessing of idle compute cycles throughout the Internet, offers exciting new research challenges in the converging domains of networking and distributed computing. Our system, Cluster Computing on the Fly, seeks to harvest cycles from ordinary users in an open access, non-institutional environment. We conduct a comprehensive study of generic searching methods in a highly dynamic peer-to-peer environment for locating idle cycles for workpile applications which are heavy consumers of cycles. We compare four scalable search methods: expanding ring, advertisement-based, random walk and rendezvous point. We model a variety of workloads, simple scheduling strategies and stabilities of hosts. Our preliminary results show that under light workloads, rendezvous point performs best, while under heavy workloads, its performance falls below the other techniques. We expected rendezvous point to consistently outperform the other search techniques because of its inherent advantage in gathering knowledge about the idle cycles. However in a peer-to-peer environment, which satisfies requests on-demand, large jobs may dominate, resulting in delays for scheduling smaller jobs.","Peer to peer computing,
Scheduling,
Internet,
Grid computing,
Computer architecture,
Computer science,
Computer networks,
IP networks,
Application software,
Convergence"
Design and analysis of a fast local clustering service for wireless sensor networks,"We present a fast local clustering service, FLOC, that partitions a multi-hop wireless network into nonoverlapping and approximately equal-sited clusters. Each cluster has a clusterhead such that all nodes within unit distance of the clusterhead belong to the cluster but no node beyond distance m from the clusterhead belongs to the cluster. By asserting m /spl ges/ 2, FLOC achieves locality: effects of cluster formation and faults/changes at any part of the network are contained within most m units. By taking unit distance to be the reliable communication radius and m to be the maximum communication radius, FLOC exploits the double-band nature of wireless radio-model and achieves clustering in constant time regardless of the network size. Through simulations and experiments with actual deployments, we analyze the tradeoffs between clustering time and the quality of clustering, and suggest suitable parameters for FLOC to achieve a fast completion time without compromising the quality of the resulting clustering.","Wireless sensor networks,
Spread spectrum communication,
Large-scale systems,
Communication system control,
Computer science,
Design engineering,
Telecommunication network reliability,
Analytical models,
Aggregates,
Base stations"
Nonrectangular shaping and sizing of soft modules for floorplan-design improvement,"Many previous works on floorplanning with nonrectangular modules assume that the modules are predesignated to have particular nonrectangular shapes, e.g., L-shaped, T-shaped, etc. However, this is not common in practice because rectangular shapes are more preferable in many designing steps. Those nonrectangular shapes are actually generated during floorplanning in order to further optimize the solution. In this paper, we study this problem of changing the shapes and dimensions of the flexible modules to fill up the unused area of a preliminary floorplan, while keeping the relative positions between the modules unchanged. This feature will also be useful in fixing small incremental changes during engineering change order modifications. We formulate the problem as a mathematical program. The formulation is such that the dimensions of all of the rectangular and nonrectangular modules can be computed by closed-form equations in O(m) time in each corresponding Lagrangian relaxation subproblem (LRS) where m is the total number of edges in the constraint graphs. As a result, the total time for the whole shaping and sizing process is O(k/spl times/m), where k is the number of iterations on the LRS. Experimental results show that the amount of area reused is 3.7% on average, while the total wirelength can be reduced by 0.43% on average because of the more compacted result packing.","Shape,
Routing,
Integrated circuit interconnections,
Lagrangian functions,
Equations,
Timing,
Computer science,
Delay,
Optimization methods"
Ternary Galois field expansions for reversible logic and Kronecker decision diagrams for ternary GFSOP minimization [Galois field sum of products],"Ternary Galois field sum of products (TGFSOP) expressions are found to be good choice for ternary reversible, and especially quantum, logic design. In this paper, we propose 16 ternary Galois field expansions (TGFE) and introduce three ternary Galois field decision diagrams (TGFDD) using the proposed TGFEs, which are useful for reversible and quantum logic design. We also propose a heuristic for creating TGFDDs and a method for flattening the TGFDDs for determining TGFSOP expressions. We provide experimental results to show the effectiveness of the developed methods.","Galois fields,
Multivalued logic,
Minimization,
Quantum computing,
Circuit synthesis,
Logic design,
Computer science,
Design engineering,
Physics computing,
Software systems"
Target transmission radius over LMST for energy-efficient broadcast protocol in ad hoc networks,"We investigate minimum energy broadcasting problem where mobile nodes have the capability to adjust their transmission range. The power consumption for two nodes at distance r is r/sup /spl alpha// + c, where /spl alpha/ /spl ges/ 2 and c is a constant that includes signal processing and minimal reception power. We show that, for c > 0 (which is realistic assumption), it is not optimal to minimize transmission range. Furthermore, we demonstrate that there exists an optimal radius, computed with a hexagonal tiling of the network area that minimizes the power consumption. For /spl alpha/ > 2 and c > 0, the optimal radius is r = /spl alpha//spl radic/(2c//spl alpha/-2), which is derived theoretically, and confirmed experimentally. We propose also a localized broadcast algorithm TR-LBOP that takes this optimal radius into account. This protocol is experimentally shown to be efficient compared to existing localized protocol LBOP and globalized BIP protocol. Most importantly, TR-LBOP is shown to have limited energy overhead with respect to BIP for all network densities, which is not the case with LBOP whose overhead explodes for higher densities.","Energy efficiency,
Broadcasting,
Intelligent networks,
Ad hoc networks,
Energy consumption,
Computer networks,
Tree graphs,
Wireless application protocol,
Computer science,
Mobile computing"
System integration of the MiCES small animal PET scanner,"A small animal positron emission tomography scanner for small animals has been developed based on the University of Washington micro-crystal element (MiCE2) detector module design. The scanner, MiCES, uses 72 MiCE2 modules, each consisting of a 22/spl times/22 array of 0.8/spl times/0.8/spl times/10 mm MLS crystals coupled to a Hamamatsu R5900-00-C12 PMT. Alternate rows of detectors (in the axial direction) are offset by the axial gap between crystal arrays and the entire detector and electronics system is continuously rotated. The system electronics is based on the use of the IEEE 1394a (FireWire) standard for transmitting data from detector nodes to the host computer. The electronics are organized around rows of four detector modules. Each module is connected to a threshold summing board that converts the 12 PS PMT signals to 4 position signals. The signals are then digitized and acquired by the host computer in a list mode format. Each set of electronics (supporting four modules) is essentially independent with its own control processor and field programmable gate arrays. Each electronics module receives basic commands from a single control processor, which in turns communicates with the host computer. In addition, during acquisition, data specific commands are sent to the electronics modules via FireWire (each module supports a control and a data logical unit). This paper provides an overview of the MiCES system and provides details of system integration and testing.","Mice,
Animals,
Positron emission tomography,
Detectors,
Firewire,
Process control,
Multilevel systems,
Crystals,
Sensor arrays,
Field programmable gate arrays"
A probabilistic approach to learning costs for graph edit distance,"Graph edit distance provides an error-tolerant way to measure distances between attributed graphs. The effectiveness of edit distance based graph classification algorithms relies on the adequate definition of edit operation costs. We propose a cost inference method that is based on a distribution estimation of edit operations. For this purpose, we employ an expectation maximization algorithm to learn mixture densities from a labeled sample of graphs and derive edit costs that are subsequently applied in the context of a graph edit distance computation framework. We evaluate the performance of the proposed distance model in comparison to another recently introduced learning model for edit costs.","Costs,
Pattern recognition,
Inference algorithms,
Computer science,
Computer errors,
Classification algorithms,
Area measurement,
Neural networks,
Machine learning,
Machine learning algorithms"
IEEE 802.11 DCF enhancements for noisy environments,"We study analytically the performance of the IEEE 802.11 DCF (distributed coordination function) for infrastructure networks in noisy environments. We show that using the standard binary exponential backoff (BEB) mechanism in noisy environments results in a poor throughput performance due to its inability to differentiate between the causes of unsuccessful packet transmissions and verify the analytical model using the ns-2 simulator. We propose an enhanced BEB mechanism that enhances IEEE 802.11 with a capability of differentiating between different types of unsuccessful transmissions. We study the proposed mechanism analytically, verify it using ns-2 and show that the new mechanism enhances the network performance by up to orders of magnitude with respect to the network error rates.","Working environment noise,
Analytical models,
Throughput,
Performance analysis,
Error analysis,
Local area networks,
Wireless LAN,
Multiaccess communication,
Computer science,
Educational institutions"
Time-domain Simulation of electronic noises,"In this paper, a procedure is proposed to computer simulate the electronic noise of ionizing-radiation spectrometers. The viewpoint of the simulator is the output of the preamplifier, with or without an anti-aliasing filter, just in front of the ADC. Examples are given for the case of segmented high purity Germanium detectors (HPGe). The method makes use of the fractional calculus basics. A software procedure provides the noisy waveform as a function of the fundamental electrical-physical parameters of the system, including: detector capacitance, detector leakage current, feedback resistor, 1/f-noise coefficient of the input transistor, temperature of the preamplifier input devices. The ADC quantization noise is also included in the simulation.","Time domain analysis,
Computational modeling,
1f noise,
Detectors,
Preamplifiers,
Leak detection,
Computer simulation,
Spectroscopy,
Filters,
Germanium"
Service differentiation mechanisms for IEEE 802.11-based wireless networks,"In this paper, we study the two medium access control (MAC) sublayer policies, namely, schedule after backoff (SAB) and schedule before backoff (SBB), which offer differentiated services in IEEE 802.11-based wireless networks. Both policies control channel access based on frame priorities. In the SAB policy, each node maintains concurrent backoff instances (one per priority class) with different parameter values to realize service differentiation. High-priority data frames use a shorter contention window, so that, in most cases, they are transmitted before the low-priority ones. The enhanced distributed coordination function (EDCF) scheme specified in the upcoming IEEE 802.11e standard is based on the SAB policy. Recent studies [A. Lindgren et al., June 2003] have shown that under high loads of high-priority traffic, EDCT starves low-priority frames. We propose SBB policy, which provides service differentiation without the need for maintaining concurrent backoff instances. In SBB, frame transmission proceeds in two sequential steps: (i) selecting the next frame to be transmitted, and (ii) channel access functionality until the selected frame is either successfully transmitted or its number of retransmissions exceeds the retry limit. These two steps are independent and can be performed using several alternatives. Depending on the way the two steps are carried out, we present four SBB-based MAC schemes. We evaluate SAB and SBB-based MAC schemes through simulation experiments. Our simulation results show that the SBB policy does not starve low-priority traffic and offers differentiated service on par with SAB. This suggests that maintenance of concurrent backoff instances contributes little towards performance improvement.",
On integrating fluid models with packet simulation,"Fluid models have been shown to he efficient and accurate in modelling large IP networks. However, unlike packet models, it is difficult to extract packet-level information from them. In this paper, we present a hybrid simulation method that maintains the performance advantage of fluid models while providing detailed packet level information for selected packet traffic flows. We propose two models to account for the interaction between background TCP traffic in a fluid network and foreground packet traffic of interest. The first assumes that the packet traffic poses a negligible load on the fluid network whereas the second accounts for the added load by transforming the packet traffic into fluid flows and solving the resulting enhanced fluid model. The first of these yields an efficient one pass solution algorithm whereas the second requires an additional pass to account for the packet traffic load. We establish the correctness of both approaches and present their implementation within ns-2. Comparisons between the hybrid models and a classical packet simulation show the two pass approach to be quite accurate and computationally efficient.",
Password-based authentication: a system perspective,"User authentication in computer systems has been a cornerstone of computer security for decades. The concept of a user id and password is a cost effective and efficient method of maintaining a shared secret between a user and a computer system. One of the key elements in the password solution for security is a reliance on human cognitive ability to remember the shared secret. In early computing days with only a few computer systems and a small select group of users, this model proved effective. With the advent of the Internet, e-commerce, and the proliferation of PCs in offices and schools, the user base has grown both in number and in demographic base. Individual users no longer have single passwords for single systems, but are presented with the challenge of remembering numerous passwords for numerous systems, from email, to web accounts, to banking and financial services. This paper presents a conceptual model depicting how users and systems work together in this function and examines the consequences of the expanding user base and the use of password memory aids. A system model of the risks associated with password-based authentication is presented from a user centric point of view including the construct of user password memory aids. When confronted with too much data to remember, users develop memory aids to assist them in the task of remembering important pieces of information. These user password memory aids form a bridge between otherwise unconnected systems and have an effect on system level security across multiple systems interconnected by the user. A preliminary analysis of the implications of this user centric interconnection of security models is presented.",
PKUAS: an architecture-based reflective component operating platform,"Reflective middleware is the major approach to improving the adaptability of middleware and its applications. Current researches and practices pay little attention on the usability of reflective middleware. There is also lacking a systematic way to adapt a runtime system via reflective middleware. This paper presents the design and implementation of PKUAS (Peking University Application Server), an architecture-based reflective component operating platform compliant with Java 2 Platform Enterprise Edition. PKUAS constructs and represents its platform and applications from the perspective of software architecture so as to provide an understandable, user-friendly and systematic way to use reflective middleware.",
An interaction system for watch computers using tactile guidance and bidirectional segmented strokes,"We introduce an input system that is based on bidirectional strokes that are segmented by tactile landmarks. By giving the user tactile feedback about the length of a stroke during input, we decrease the dependence of the GUI on the visual display. By concatenating separate strokes into multistrokes, complex commands may be entered, which may encode commands, data content, or both simultaneously. To demonstrate their power, we show how multistrokes can be used to traverse a menu hierarchy quickly. In addition, we show how inter-landmark segments of the sensor may be used for continuous and discrete parameter entry, resulting in a multifunctional interaction paradigm. We also introduce multiwidgets, which allow the direct control of multiple virtual widgets without the need to change the state of the device or use modifier buttons. This approach to input does not depend on material displayed visually to the user, and, thanks to tactile guidance, may be used by expert users as an eyes-free user interface. We believe that these benefits make this interaction system especially suitable for wearable computer systems that use a head-worn display and wrist-worn watch-style devices.","Watches,
User interfaces,
Wearable computers,
Feedback,
Graphical user interfaces,
Computer displays,
Personal digital assistants,
Computer science,
Computer interfaces,
Navigation"
On-demand location-aided QoS routing in ad hoc networks,"With the development and application of position devices, location-based routing has received growing attention. However, little study has been done on QoS routing with the aid of location information. The existing location-based routing approaches, such as flooding-based routing schemes and localized routing schemes, have their limitations. Motivated by ticket-based routing, we propose an on-demand location-aided, ticket-based QoS routing protocol (LTBR). Two special cases of LTBR, LTBR-1 and LTBR-2, are discussed in detail. LTBR-1 uses a single ticket to find a route satisfying a given QoS constraint. LTBR-2 uses multiple tickets to search valid routes in a limited area. All tickets are guided via both location and QoS information. LTBR has lower overhead compared with the original ticket-based routing, because it does not rely on an underlying routing table. On the other hand, LTBR can find routes with better QoS qualities than traditional location-based protocols. Our simulation results show that LTBR-1 can find high quality routes in relatively dense networks with high probability and very low overhead. In sparse networks, LTBR-2 can be used to enhance the probability of finding high quality routes with acceptable overhead.",
Mobile agents: ten reasons for failure,"Mobile agents have often been advocated as the solution to the problem of designing and implementing distributed applications in a dynamic environment. Mobile agents provide a very appealing, intuitive, and apparently simple abstraction. Unfortunately there are many difficult problems that have to be addressed in order to make mobile agent-based applications work reliably. This paper discusses some of the problems associated with mobile agents and argues that other forms of code mobility can provide similar advantages while raising much fewer issues.","Mobile agents,
Application software,
Testing,
Java,
Network servers,
Debugging,
Computer science,
World Wide Web,
Computer networks,
Databases"
An approach to placement-coupled logic replication,"We present a set of techniques for placement-coupled, timing driven logic replication. Two components are at the core of the approach. First is an algorithm for optimal timing driven fanin tree embedding; the algorithm is very general in that it can easily incorporate complex objective functions (e.g., placement costs) and can perform embedding on any graph-based target. Second we introduce the Replication nee which allows us to induce large fanin trees from a given circuit which can then be optimized by the embedder. We have built an optimization engine around these two ideas and report promising results for the FPGA domain including clock period reductions of up to 36% compared with a timing-driven placement from VPR [12] and almost double the average improvement of local replication from [I]. These results are azhieved with modest area and runtime overhead.","Circuits,
Computer science,
Tree graphs,
Delay,
Programmable logic arrays,
Programmable logic devices,
Permission,
Cost function,
Engines,
Field programmable gate arrays"
Volunteer availability based fault tolerant scheduling mechanism in desktop grid computing environment,"Fault tolerance is essential to the further development of desktop grid computing system in order to guarantee continuous and reliable execution of tasks in spite of failures. In a desktop grid computing environment, volunteers are often susceptible to volunteer autonomy failures such as volatility failure and interference failure in the middle of execution of tasks because a desktop grid computing maximally respects autonomy of volunteers. The failures result in an independent livelock problem (i.e. the delay and blocking of the entire execution of a job). Therefore, the failures should be considered in a scheduling mechanism. In This work, in order to tolerate volunteer autonomy failures, we propose a new fault tolerant scheduling mechanism. First, we specify a volunteer autonomy failures and an independent livelock problem. Then, we propose a volunteer availability which reflects the degree of volunteer autonomy failures. Finally, we propose a fault tolerant scheduling mechanism based on volunteer availability (which is called VAFTSM).","Fault tolerance,
Processor scheduling,
Grid computing,
Internet,
Web server,
Fault tolerant systems,
Delay,
Availability,
Computer crashes,
Computer science"
Communication efficient construction of decision trees over heterogeneously distributed data,We present an algorithm designed to efficiently construct a decision tree over heterogeneously distributed data without centralizing. We compare our algorithm against a standard centralized decision tree implementation in terms of accuracy as well as the communication complexity. Our experimental results show that by using only 20% of the communication cost necessary to centralize the data we can achieve trees with accuracy at least 80% of the trees produced by the centralized version.,"Decision trees,
Data mining,
Distributed decision making,
Message passing,
Computer science,
Algorithm design and analysis,
Communication standards,
Complexity theory,
Costs,
Communication channels"
Symmetry breaking in population-based optimization,"Argues that the performance of evolutionary algorithms working on hard optimization problems depends strongly on how the population breaks the ""symmetry"" of the search space. The splitting of the search space into widely separate regions containing local optima is a generic property of a large class of hard optimization problem. This phenomenon is discussed by reference to two well studied examples, the Ising perceptron and the satisfiability problem (K-SAT). A finite population will quickly concentrate on one region of the search space. The cost of crossover between solutions in different regions of search space can accelerate this symmetry breaking. This, in turn, can dramatically reduce the amount of exploration, leading to suboptimal solutions being found. An analysis of symmetry breaking using diffusion model techniques borrowed from classical population genetics is presented. This shows how symmetry breaking depends on parameters such as the population size and selection rate.","Evolutionary computation,
Costs,
Acceleration,
Genetics,
Computer science,
NP-hard problem,
Analytical models,
Stochastic processes,
Differential equations,
Performance gain"
Variable-branch tree-structured vector quantization,"Tree-structured vector quantizers (TSVQ) and their variants have recently been proposed. All trees used are fixed M-ary tree structured, such that the training samples in each node must be artificially divided into a fixed number of clusters. This paper proposes a variable-branch tree-structured vector quantizer (VBTSVQ) based on a genetic algorithm, which searches for the number of child nodes of each splitting node for optimal coding in VBTSVQ. Moreover, one disadvantage of TSVQ is that the searched codeword usually differs from the full searched codeword. Briefly, the searched codeword in TSVQ sometimes is not the closest codeword to the input vector. This paper proposes the multiclassification encoding method to select many classified components to represent each cluster, and the codeword encoded in the VBTSVQ is usually the same as that of the full search. VBTSVQ outperforms other TSVQs in the experiments presented here.",
Identifying the combination of genetic factors that determine susceptibility to cervical cancer,"Cervical cancer is common among women all over the world. Although infection with high-risk types of human papillomavirus (HPV) has been identified as the primary cause of cervical cancer, only some of those infected go on to develop cervical cancer. Obviously, the progression from HPV infection to cancer involves other environmental and host factors. Recent population-based twin and family studies have demonstrated the importance of the hereditary component of cervical cancer, associated with genetic susceptibility. Consequently, single-nucleotide polymorphism (SNP) markers and microsatellites should be considered genetic factors for determining what combinations of genetic factors are involved in precancerous changes to cervical cancer. This study employs a Bayesian network and four different decision tree algorithms, and compares the performance of these learning algorithms. The results of this study raise the possibility of investigations that could identify combinations of genetic factors, such as SNPs and microsatellites, that influence the risk associated with common complex multifactorial diseases, such as cervical cancer. The web site associated with this study is http://140.115.155.8/FactorAnalysis/.","Genetics,
Cervical cancer,
Humans,
Cities and towns,
Bioinformatics,
Amino acids,
Decision trees,
Testing,
Computer science,
Lesions"
Hierarchy theorems for probabilistic polynomial time,"We show a hierarchy for probabilistic time with one bit of advice, specifically we show that for all real numbers 1 /spl les/ /spl alpha/ /spl les/ /spl beta/, BPTIME(n/sup /spl alpha//)/l /spl sube/ BPTIME(n/sup /spl beta//)/l. This result builds on and improves an earlier hierarchy of Barak using O(log log n) bits of advice. We also show that for any constant d > 0, there is a language L computable on average in BPP but not on average in BPTIME (n/sup d/). We build on Barak's techniques by using a different translation argument and by a careful application of the fact that there is a PSPACE-complete problem L such that worst-case probabilistic algorithms for L take only slightly more time than average-case algorithms.","Polynomials,
Computer science,
Computational modeling,
Approximation algorithms,
Computer simulation,
Computer errors"
Maxmin overlay multicast: rate allocation and tree construction,"Although initially proposed as the deployable alternative to IP multicast, overlay multicast actually offers us great flexibilities on QoS-aware resource allocation for network applications. For example, in overlay multicast streaming, (1) the streaming rate of each client can be diversified to better accommodate network heterogeneity, through various end-to-end streaming adaptation techniques; and (2) one can freely organize the overlay session by rearranging the multicast tree, for the purpose of better resource utilization and fairness among all clients. The goal of this paper, is to find the max-min rate allocation in overlay multicast, which is pareto-optimal in terms of network resource utilization, and max-min fair. We approach this goal in two steps. First, we present a distributed algorithm, which is able to return the max-min rate allocation for any given overlay multicast tree. Second, we study the problem of finding the optimal tree, whose max-min rate allocation is optimal among all trees. After proving its NP-hardness, we propose a heuristic algorithm of overlay multicast tree construction. A variation of the heuristic is also designed to handle the dynamic client join/departure. Both of them have approximation bound of 1/2 to the optimal value. Experimental results show that they achieve high average throughput, almost saturate link utilization, and consistent min-favorability.","Resource management,
Unicast,
Switches,
Computer science,
Application software,
Distributed algorithms,
Heuristic algorithms,
Multicast algorithms,
Throughput,
Intelligent networks"
On optimizing energy consumption for mobile handsets,"To reduce energy consumption (EC), a mobile handset system can be designed in such a way that while the data-receiving unit in a mobile handset is receiving and monitoring data packets, the rest part of the handset (i.e., a processing unit and a user interface) is switched off into a sleep mode. In this paper, we study the timing when the rest part of the handset should wake up. Several schemes (i.e., Always-ON , Always-OFF, Wake-up upon Arrival, Wake-up upon Full, and the fractional threshold scheme) are studied and compared in terms of energy saving and the packet dropping probability (PDP). We formulate the total EC for all theses schemes analytically. Furthermore, we show how to choose optimal thresholds for the fractional threshold scheme for the following two-optimization problems: 1) minimizing the switch-on rate with a bound on the PDP and 2) minimizing the total EC with a bound on the PDP. Our study shows that the fractional threshold scheme is the best scheme. Simulations are carried out to validate analytic models.","Energy consumption,
Mobile handsets,
Telephone sets,
User interfaces,
Packet switching,
Batteries,
Computer science,
Design optimization,
Monitoring,
Timing"
Improved thermal analysis of buried landmines,"In this paper, we address the problem of the detection and identification of surface-laid and shallowly buried landmines from measured infrared images. A three-dimensional thermal model has been developed to study the effect of the presence of landmines in the thermal signature of the bare soil. Based on this model, a target identification procedure is proposed aiming at detecting and classifying the anomalies found on the soil thermal signature. In our approach, landmines are thought of as a thermal barrier in the natural flow of the heat inside the soil, which produces a perturbation of the expected thermal pattern on the surface. The detection of these perturbations will put into evidence the presence of potential mine targets. We propose an iterative procedure to classify the detected perturbations as mines or nonmines and to estimate their depth of burial. This paper describes the main principles of our method and illustrates classification results on a set of acquired images. Qualitative and quantitative comparisons with independent component analysis are also given.","Landmine detection,
Infrared detectors,
Soil,
Independent component analysis,
Infrared imaging,
Nondestructive testing,
Temperature,
Image processing,
Computer science,
Predictive models"
A Fast Tree Pattern Matching Algorithm for XML Query,"Finding all distinct matchings of the query tree pattern is the core operation of XML query evaluation. The existing methods for tree pattern matching are decomposition-matching-merging processes, which may produce large useless intermediate result or require repeated matching of some sub-patterns. We propose a fast tree pattern matching algorithm called TreeMatch to directly £nd all distinct matchings of a query tree pattern. The only requirement for the data source is that the matching elements of the non-leaf pattern nodes do not contain sub-elements with the same tag. The TreeMatch does not produce any intermediate results and the £nal results are compactly encoded in stacks, from which the explicit representation can be produced ef£ciently.","Pattern matching,
XML,
Database languages,
Query processing,
Internet,
Relational databases,
Computer science,
Data mining,
Merging,
Lifting equipment"
PCA-based feature transformation for classification: issues in medical diagnostics,"The goal of this paper is to propose, evaluate, and compare several data mining strategies that apply feature transformation for subsequent classification, and to consider their application to medical diagnostics. We (1) briefly consider the necessity of dimensionality reduction and discuss why feature transformation may work better than feature selection for some problems; (2) analyze experimentally whether extraction of new components and replacement of original features by them is better than storing the original features as well; (3) consider how important the use of class information is in the feature extraction process; and (4) discuss some interpretability issues regarding the extracted features.","Medical diagnosis,
Feature extraction,
Data mining,
Medical diagnostic imaging,
Computer science,
Electronic mail,
Learning systems,
Information systems,
Educational institutions,
Application software"
An efficient algorithm for mining frequent sequences by a new strategy without support counting,"Mining sequential patterns in large databases is an important research topic. The main challenge of mining sequential patterns is the high processing cost due to the large amount of data. We propose a new strategy called direct sequence comparison (abbreviated as DISC), which can find frequent sequences without having to compute the support counts of nonfrequent sequences. The main difference between the DISC strategy and the previous works is the way to prune nonfrequent sequences. The previous works are based on the antimonotone property, which prune the nonfrequent sequences according to the frequent sequences with shorter lengths. On the contrary, the DISC strategy prunes the nonfrequent sequences according to the other sequences with the same length. Moreover, we summarize three strategies used in the previous works and design an efficient algorithm called DISC-all to take advantages of all the four strategies. The experimental results show that the DISC-all algorithm outperforms the PrefixSpan algorithm on mining frequent sequences in large databases. In addition, we analyze these strategies to design the dynamic version of our algorithm, which achieves a much better performance.","Itemsets,
Algorithm design and analysis,
Costs,
Computer science,
Performance analysis,
Data mining,
Data analysis,
Working environment noise,
Transaction databases,
Unsolicited electronic mail"
Scheduling communication in real-time sensor applications,"We consider a class of wireless sensor applications - such as mobile robotics - that impose timeliness constraints. We assume that these applications are built using commodity 802.11 wireless networks and focus on the problem of providing qualitatively-better QoS during network transmission of sensor data. Our techniques are designed to explicitly avoid network collisions and minimize the completion time to transmit a set of sensor messages. We argue that this problem is NP-complete and present three heuristics, based on edge coloring, to achieve these goals. Our simulations results show that the minimum weight color heuristic is robust to increases in communication density and yields results that are close to the optimal solution.","Wireless sensor networks,
Robot sensing systems,
Temperature sensors,
Image sensors,
Thermal sensors,
Application software,
Computer science,
Mobile robots,
Monitoring,
Wireless application protocol"
Support for disconnected operation via architectural self-reconfiguration,"In distributed and mobile environments, the connections among the hosts on which a software system is running are often unstable. As a result of connectivity losses, the overall availability of the system decreases. The distribution of software components onto hardware nodes (i.e., deployment architecture) may be ill-suited for the given state of the target hardware environment and may need to be altered to improve the software system's availability. In this paper, we present a flexible, software architecture-based solution for disconnected operation that increases the availability of a system during disconnection by allowing the system to monitor and automatically redeploy itself.","Hardware,
Computer architecture,
Software systems,
Availability,
Frequency,
Computer science,
Monitoring,
Prefetching,
Buffer storage,
Linear programming"
A multibit geometrically robust image watermark based on Zernike moments,"In image watermarking, the watermark robustness to geometric transformations is still an open problem. Using invariant image features to carry the watermark is an effective approach to addressing this problem. In this paper, a multibit geometrically robust image watermarking algorithm using Zernike moments is proposed. Some Zernike moments of an image are selected, and their magnitudes are dither-modulated to embed an array of bits. The watermarked image is obtained via reconstruction from the modified moments and those left intact. In watermark extraction, the embedded bits are estimated from the invariant magnitudes of the Zernike moments using a minimum distance decoder. Simulation results show that the hidden message can be decoded at low error rates, robust against image rotation, scaling and flipping, and as well, a variety of other distortions such as lossy compression.","Watermarking,
Image coding,
Noise robustness,
Decoding,
Additive noise,
Computer science,
Image reconstruction,
Error analysis,
Protection,
Transform coding"
Reliability growth in software products,"Most of the software reliability growth models work under the assumption that reliability of software grows due to the bugs that cause failures being removed from the software. While correcting bugs will improve reliability, another phenomenon has often been observed - the failure rate of a software product, as observed by the user, improves with time irrespective of whether bugs are corrected or not. Consequently, the reliability of a product, as observed by users, varies, depending on the length of time they have been using the product. One reason for this reliability growth is that as the users gain experience with the product, they learn to use the product correctly and find work-around for failure-causing situations. Another factor that affects this growth is that following the product installation, the user discovers that other actions may be required, like installing new drivers, upgrading other software to a compatible version, etc. to properly configure the new product. In this paper we present a simple model to represent this phenomenon - we assume that the failure rate for a product decays with a factor /spl alpha/ per unit time. Applying this failure rate decay model to the data collected on reported failures and number of units of the product sold, it is possible to determine the initial failure rate, the decay factor, and the steady state failure rate of a product. The paper provides a number of examples where this model has been applied to data captured from released products.","Software reliability,
Computer bugs,
Software debugging,
Reliability engineering,
Steady-state,
Computer science,
Software systems,
System testing,
Usability,
Software measurement"
Perceptual grouping for contour extraction,"This paper describes an algorithm that efficiently groups line segments into perceptually salient contours in complex images. A measure of affinity between pairs of lines is used to guide group formation and limit the branching factor of the contour search procedure. The extracted contours are ranked, and presented as a contour hierarchy. Our algorithm is able to extract salient contours in the presence of texture, clutter, and repetitive or ambiguous image structure. We show experimental results on a complex line-set.",
Learning style and factors contributing to success in an introductory computer science course,"An introductory course in computer science (CSI) is required of virtually all engineering majors at the University of Minnesota Duluth. From 2001-present an extensive battery of visualization software was developed for this course. Students consistently ranked the visualization software as more important to their learning than any other element of the course. However, these rankings were not highly correlated with actual outcomes. This study of learning style determined that reflective and verbal learners outperformed active and visual ones. Student opinions of the value of programming projects and lectures rank highest and seem to cut across learning style preference. Background familiarity with computers and software was not a strong correlate, although involvement in computer and video gaming was found to be negatively correlated with course success.","Computer science,
Visualization,
Handheld computers,
Portable computers,
Chemical industry,
Education,
Batteries,
Programming profession,
Computational modeling,
Engineering profession"
Object dependency of resolution in reconstruction algorithms with interiteration filtering applied to PET data,"In this paper, we study the resolution properties of those algorithms where a filtering step is applied after every iteration. As concrete examples we take filtered preconditioned gradient descent algorithms for the Poisson log likelihood for PET emission data. For nonlinear estimators, resolution can be characterized in terms of the linearized local impulse response (LLIR). We provide analytic approximations for the LLIR for the class of algorithms mentioned above. Our expressions clearly show that when interiteration filtering (with linear filters) is used, the resolution properties are, in most cases, spatially varying, object dependent and asymmetric. These nonuniformities are solely due to the interaction between the filtering step and the Poisson noise model. This situation is similar to penalized likelihood reconstructions as studied previously in the literature. In contrast, nonregularized and postfiltered maximum-likelihood expectation maximization (MLEM) produce images with nearly ""perfect"" uniform resolution when convergence is reached. We use the analytic expressions for the LLIR to propose three different approaches to obtain nearly object independent and uniform resolution. Two of them are based on calculating filter coefficients on a pixel basis, whereas the third one chooses an appropriate preconditioner. These three approaches are tested on simulated data for the filtered MLEM algorithm or the filtered separable paraboloidal surrogates algorithm. The evaluation confirms that images obtained using our proposed regularization methods have nearly object independent and uniform resolution.","Reconstruction algorithms,
Positron emission tomography,
Nonlinear filters,
Spatial resolution,
Image resolution,
Filtering algorithms,
Concrete,
Algorithm design and analysis,
Image reconstruction,
Maximum likelihood estimation"
Development of a soccer-playing dynamically-balancing mobile robot,"In this paper, we make two contributions. First, we present a new domain, called Segway Soccer, for investigating the coordination of dynamically formed, mixed human-robot teams within the realm of a team task that requires real-time decision making and response. Segway Soccer is a game of soccer between two teams consisting of Segway riding humans and Segway RMP-based robots. We believe Segway Soccer is the first game involving both humans and robots in cooperative roles and with similar capabilities. In conjunction with this new domain, we present our work towards developing a soccer playing robot using the Segway RMP platform and vision as its primary sensing modality. As Segway Soccer is set in the outdoors, we have developed novel vision algorithms to adapt to changes in lighting conditions. We present the domain of Segway Soccer, its inherent challenges, and our work towards this goal.",
Aggregating information in peer-to-peer systems for improved join and leave,"We introduce the Distributed Approximative System Information Service (DASIS) as a useful scheme to aggregate approximative information on the state of a peer-to-peer system. We present how this service can be integrated into existing peer-to-peer systems, such as Kademlia and Chord. As a sample application, we show how DASIS can be employed for establishing an effective deterministic join algorithm. Through simulation, we demonstrate that the insertion of peers using DASIS information results in a well-balanced system. Moreover, our join algorithm gracefully resolves load imbalances in the system due to unfortunate biased leaves of peers.","Peer to peer computing,
Topology,
Computer science,
Aggregates,
Computational modeling,
Internet,
Multimedia systems,
Multimedia databases,
Online Communities/Technical Collaboration,
Database systems"
Fuzzy ant clustering by centroid positioning,We present a swarm intelligence based algorithm for data clustering. The algorithm uses ant colony optimization principles to find good partitions of the data. In the first stage of the algorithm ants move the cluster centers in feature space. The cluster centers found by the ants are evaluated using a reformulated fuzzy c-means criterion. In the second stage the best cluster centers found are used as the initial cluster centers for the fuzzy c-means (FCM) algorithm. Results on 8 datasets show that the partitions found by FCM using the ant initialization are better optimized than those from randomly initialized FCM. Hard c-means was also used in the second stage and the partitions from the algorithm are better optimized than those from randomly initialized hard c-means.,"Clustering algorithms,
Partitioning algorithms,
Ant colony optimization,
Iterative algorithms,
Stochastic processes,
Equations,
Computer science,
Data engineering,
Fuzzy logic,
Particle swarm optimization"
Low-complexity predictive trellis-coded quantization of speech line spectral frequencies,"Low-complexity block-constrained trellis coded quantization (BC-TCQ) structures are studied, and a predictive BC-TCQ encoding method is developed for quantization of line spectrum frequency (LSF) parameters for speech coding applications. The performance is compared with the linear predictive coding (LPC) vector quantizers used in the AMR-WB (ITU-G.722.2) and IS-641 speech coding standards, demonstrating reduction in spectral distortion (SD) and significant reduction in encoding complexity.","Quantization,
Frequency,
Speech coding,
Encoding,
Lattices,
Linear predictive coding,
Costs,
Computer science,
Convolutional codes,
Rate-distortion"
Model checking action- and state-labelled Markov chains,"In this paper we introduce the logic asCSL, an extension of continuous stochastic logic (CSL), which provides powerful means to characterise execution paths of action- and state-labelled Markov chains. In asCSL, path properties are characterised by regular expressions over actions and state-formulas. Thus, the executability of a path not only depends on the available actions but also on the validity of certain state formulas in intermediate states. Our main result is that the model checking problem for asCSL can be reduced to CSL model checking on a modified Markov chain, which is obtained through a product automaton construction. We provide a case study of a scalable cellular phone system which shows how the logic asCSL and the model checking procedure can be applied in practice.","Stochastic processes,
Probabilistic logic,
Computer science,
Algebra,
Power system modeling,
Law,
Legal factors,
Switches,
Automata,
Cellular phones"
Interactive virtual endoscopy in coronary arteries based on multimodality fusion,"A novel approach for platform-independent virtual endoscopy in human coronary arteries is presented in this paper. It incorporates previously developed and validated methodology for multimodality fusion of two X-ray angiographic images with pullback data from intravascular ultrasound (IVUS). These modalities pose inherently different challenges than those present in many tomographic modalities that provide parallel slices. The fusion process results in a three- or four-dimensional (3-D/4-D) model of a coronary artery, specifically of its lumen/plaque and media/adventitia surfaces. The model is used for comprehensive quantitative hemodynamic, morphologic, and functional analyses. The resulting quantitative indexes are then used to supplement the model. Platform-independent visualization is achieved through the use of the ISO/IEC-standardized Virtual Reality Modeling Language (VRML). The visualization includes an endoscopic fly-through animation that enables the user to interactively select vessel location and fly-through speed, as well as to display image pixel data or quantification results in 3-D. The presented VRML virtual-endoscopy system is used in research studies of coronary atherosclerosis development, quantitative assessment of coronary morphology and function, and vascular interventions.","Endoscopes,
Arteries,
Data visualization,
Humans,
X-ray imaging,
Ultrasonic imaging,
Tomography,
Surface morphology,
Hemodynamics,
Functional analysis"
Lateral error recovery for application-level multicast,"We consider the delivery of reliable and streaming services using application-level multicast (ALM) by means of UDP, where packet loss has to be recovered via retransmission in a timely manner in order to offer high level of service. Since packets may be lost due to congestion, tree-reconfiguration or node failure, the traditional ""vertical"" recovery, whereby upstream nodes retransmit the lost packet is no longer effective. We therefore propose and investigate lateral error recovery (LER). In LER, hosts are divided into a number of planes, each of which forms an independent ALM tree. Since the correlation of error among the planes is likely to be low, a node can effectively recover its error ""laterally"" from nearby nodes in other planes. We employ the technique of global network positioning (GNP) to map the hosts into a coordinate space and identify a set of close neighbors for error recovery by constructing a Voronoi diagram for each plane. We present centralized and distributed algorithm on how to construct the Voronoi diagrams. Using Internet-like topologies, we show via simulations that our system achieves low overheads in terms of relative delay penalty and physical link stress. For reliable service, lateral recovery greatly reduces the average recovery time as compared with vertical recovery schemes. For streaming applications, LER achieves much lower residual loss rate under a certain deadline constraint.","Delay,
Computer errors,
Residual stresses,
Quality of service,
Computer science,
Asia,
Economic indicators,
Distributed algorithms,
Internet,
Network topology"
A variable depth LSB data hiding technique in images,"In this paper, a data hiding technique by variable depth LSB substitution is proposed. Firstly, pixels of cover-image are grouped according to themselves luminance values. Then the occurrence of pixel-values in each group is counted and sorted in monotone increasing. Finally, a bitplane-wise data hiding method is used to hide data in cover image. The worst-square-error between the stego-image and the cover-image is formulated. Theory analysis indicates that image quality of the stego-image hidden by this method can improve from 0 db to 4 db against simple LSB (least significant bit) substitution method. Experimental results show that the stego-image is visually indistinguishable from the original cover-image.","Data encapsulation,
Image quality,
Data security,
Genetic algorithms,
Pixel,
Computer science,
Image analysis,
Radio access networks,
Information security,
Scattering"
Scheduling for efficient data broadcast over two channels,"The broadcast domain of wireless communication is very effective in distributing information to large audiences. In this work, an efficient data broadcast has been scheduled from a server to many clients using the broadcast disk model and a simple two-channel broadcast model are examined to present some interesting scheduling results for this model.","Broadcasting,
Optimal scheduling,
Processor scheduling,
Electronic mail,
Bandwidth,
Computer networks,
Error correction codes,
Computer science,
Application software,
Wireless communication"
A novel video caption detection approach using multi-frame integration,"Captions in videos often play an important role in video information indexing and retrieval. In this paper, we present a novel video caption detection approach. We first apply a new multiple frame integration (MFI) method to minimize variation of the background of the image. A time-based minimum (or maximum) pixel value search is employed and a Sobel edge map is used to determine the mode of search. Then block-based text detection is performed, i.e., a small window is used to scan the image and classify as text or non-text, using Sobel edges as features. We use a two-level pyramid to detect various text sizes. Finally, we present a new iterative text line decomposition method and accurate text bounding boxes are extracted from candidate text areas. Experimental result shows that the proposed approach achieves high precision and recall.","Information retrieval,
Image edge detection,
Indexing,
Iterative methods,
Detection algorithms,
Computer science,
Data mining,
Object detection,
Content based retrieval,
Video sequences"
Dual contouring with topology-preserving simplification using enhanced cell representation,"We present a fast, topology-preserving approach for isosurface simplification. The underlying concept behind our approach is to preserve the disconnected surface components in cells during isosurface simplification. We represent isosurface components in a novel representation, called enhanced cell, where each surface component in a cell is represented by a vertex and its connectivity information. A topology-preserving vertex clustering algorithm is applied to build a vertex octree. An enhanced dual contouring algorithm is applied to extract error-bounded multiresolution isosurfaces from the vertex octree while preserving the finest resolution isosurface topology. Cells containing multiple vertices are properly handled during contouring. Our approach demonstrates better results than existing octree-based simplification techniques.","Isosurfaces,
Topology,
Clustering algorithms,
Data mining,
Computer graphics,
Computer science,
Computer errors,
Chromium,
Data structures,
Data visualization"
Expectation-maximization for a linear combination of Gaussians,"We propose a modified expectation-maximization algorithm that approximates an empirical probability density function of scalar data with a linear combination of Gaussians (LCG). Due to both positive and negative components, the LCG approximates inter-class transitions more accurately than a conventional mixture of only positive Gaussians. Experiments in segmenting multi-modal medical images show the proposed LCG-approximation results in more adequate region borders.","Gaussian processes,
Probability density function,
Gaussian approximation,
Probability distribution,
Character generation,
Frequency,
Parameter estimation,
Image segmentation,
Pattern recognition,
Computer science"
Improve the robot calibration accuracy using a dynamic online fuzzy error mapping system,"Traditional robot calibration implements model and modeless methods. The compensation of position error in modeless method is to move the end-effector of robot to the target position in the workspace, and to find the position error of that target position by using a bilinear interpolation method based on the neighboring 4-point's errors around the target position. A camera or other measurement devices can be utilized to find or measure this position error, and compensate this error with the interpolation result. This paper provides a novel fuzzy interpolation method to improve the compensation accuracy obtained by using a bilinear interpolation method. A dynamic online fuzzy inference system is implemented to meet the needs of fast real-time control system and calibration environment. The simulated results show that the compensation accuracy can be greatly improved by using this fuzzy interpolation method compared with the bilinear interpolation method.","Robots,
Calibration,
Fuzzy systems,
Interpolation,
Position measurement,
Kinematics,
Real time systems,
Error compensation,
Computer science,
Optimal control"
Access control for semantic Web services,"In this paper, we present an approach to enable access control for semantic Web services. Our approach builds on the idea of autonomous granting of access rights, decision making based on independent trust structures and respects privacy requirements of the users. Our framework allows the specification and computation of complex access control policies in a manageable and efficient way. Therefore, our approach is useful not only in Web services based applications (typically client-server architecture) but also in peer-to-peer and agent-based applications.","Access control,
Semantic Web,
Web services,
Information security,
Algebra,
Authorization,
Informatics,
Information systems,
Computer security,
Computer science"
Finding a History for Software Engineering,"Historians and software engineers are both looking for a history for software engineering. For historians, it is a matter of finding a point of perspective from which to view an enterprise that is still in the process of defining itself. For software engineers, it is the question of finding a usable past, as they have sought to ground their vision of the enterprise on historical models taken from science, engineering, industry, and the professions. The article examines some of those models and their application to software engineering.","History,
Software engineering,
Computer industry,
Application software,
Manufacturing"
Learning From a Small Number of Training Examples by Exploiting Object Categories,"In the last few years, object detection techniques have progressed immensely. Impressive detection results have been achieved for many objects such as faces [11, 14, 9] and cars [11]. The robustness of these systems emerges from a training stage utilizing thousands of positive examples. One approach to enable learning from a small set of training examples is to find an efficient set of features that accurately represent the target object. Unfortunately, automatically selecting such a feature set is a difficult task in itself. In this paper we present a novel feature selection method that is based on the notion of object categories. We assume that when learning to recognize a new object (like an apple) we also know a category it belongs to (fruit). We further assume that features that are useful for learning other objects in the same category (e.g. pear or orange) will also be useful for learning the novel object. This leads to a simple criterion for selecting features and building classifiers. We show that our method gives significant improvement in detection performance in challenging domains.","Object detection,
Face detection,
Computer science,
Robustness,
Humans,
Computer vision,
Computer displays,
Switches,
Computer Society,
Pattern recognition"
Light types for polynomial time computation in lambda-calculus,"We propose a new type system for lambda-calculus ensuring that well-typed programs can be executed in polynomial time: dual light affine logic (DIAL). DIAL has a simple type language with a linear and an intuitionistic type arrow, and one modality. It corresponds to a fragment of light affine logic (LAL). We show that contrarily to LAL, DIAL ensures good properties on lambda-terms: subject reduction is satisfied and a well-typed term admits a polynomial bound on the reduction by any strategy. Finally we establish that as LAL, DIAL allows to represent all polytime functions.","Polynomials,
Logic design,
Set theory,
Space exploration,
Informatics,
Programming profession,
Runtime,
Computational complexity,
Computer science"
Synthesis and recognition of facial expressions in virtual 3D views,"The human face exhibits complex and rich changes that are both unpredictable and varying in time. In this paper we present a novel method for synthesising and recognition of facial expression changes at extreme 3D views, based on images at near frontal views. Given a sequence of images of facial expressions at near frontal views, we automatically generate virtual expressions at extreme 3D views with corresponding semantic labelling of the expressions. This is accomplished by two components: a shape component where modelling of the shape changes is accomplished through the use of a mixture of probabilistic PCA (MPPCA) and a texture component where modelling of the semantic changes is performed through auto-clustering of facial expression subspaces in the MPPCA feature space.",
A pheromone-based utility model for collaborative foraging,"Multi-agent research often borrows from biology, where remarkable examples of collective intelligence may be found. One interesting example is ant colonies¿ use of pheromones as a joint communication mechanism. In this paper we propose two pheromone-based algorithms for artificial agent foraging, trail-creation, and other tasks. Whereas practically all previous work in this area has focused on biologically-plausible but ad-hoc single pheromone models, we have developed a formalism which uses multiple pheromones to guide cooperative tasks. This model bears some similarity to reinforcement learning. However, our model takes advantage of symmetries common to foraging environments which enables it to achieve much faster reward propagation than reinforcement learning does. Using this approach we demonstrate cooperative behaviors well beyond the previous ant-foraging work, including the ability to create optimal foraging paths in the presence of obstacles, to cope with dynamic environments, and to follow tours with multiple waypoints.We believe that this model may be used for more complex problems still.","Collaboration,
Biological system modeling,
Learning,
Computer science,
Insects,
Optimization methods,
Joining processes,
Decision making,
Biology,
Artificial intelligence"
Online testable reversible logic circuit design using NAND blocks,A technique for an on-line testable reversible logic circuit is presented. Three new reversible logic gates have been introduced in this paper. These gates can be used to implement reversible digital circuits of various levels of complexity. The major feature of these gates is that they provide on-line testability for circuits implemented using them. The application of these gates in implementation of a subset of MCNC benchmark circuits is provided.,"Circuit testing,
Logic testing,
Logic circuits,
Logic gates,
Energy dissipation,
Computer science,
Digital circuits,
Benchmark testing,
Physics computing,
Logic devices"
Service selection in dynamic demand-driven Web services,"Recently, Web services have become a new technology trend for Enterprise Application Integration (EAI) and more and more applications based on Web services are emerging. One of the problems in using Web services in business applications such as logistics is services composition automatically and efficiently. In this paper, we present a Dynamic, Demand-Driven Web services Engine called D3D-Serv to implement composite service functionality that is used to dynamically build composite services from existing services according to different business logics and requirements. In this D3D-Serv framework, the most challenging function to implement is dynamic selection of service providers at run time. The highly dynamic and distributed nature of Web services often makes some service providers overloaded at certain times while others idle. To solve this problem, we propose an efficient services selection and execution strategy that is based on the queuing theory and can provide guarantees for the QOS (Quality of Service) under provider's limited resources. Preliminary experimental results have shown that this algorithm is effective.","Web services,
Logistics,
Logic,
Application software,
Computer science,
Quality of service,
Web and internet services,
Laboratories,
Engines,
Queueing analysis"
Load-balanced routing through virtual paths: highly adaptive and efficient routing scheme for ad hoc wireless networks,"Routing protocols for ad hoc wireless networks consider the path with the minimum number of hops as the optimal path to any given destination. However, this strategy does not balance the traffic load over the network, and may create congested areas. These congested areas greatly degrade the performance of the routing protocols. In this paper, we propose a routing scheme that balances the load over the network by selecting a path based on traffic sizes. We present a simulation study to demonstrate the effectiveness of the proposed scheme.","Wireless networks,
Routing protocols,
Telecommunication traffic,
Degradation,
Traffic control,
Mobile computing,
Mobile ad hoc networks,
Load management,
Adaptive systems,
Computer science"
Analysis of payment transaction security in mobile commerce,"Mobile payment is the process of two parties exchanging financial value using a mobile device in return for goods or services. This paper is an analysis of the security issues in mobile payment for m-commerce. We introduce m-commerce and mobile payment, discuss the public key infrastructure as a basis for secure mobile technologies, and study the features for different security technologies employed in current m-commerce market, including WAP, SIM application toolkit and J2M. In addition, we compare the effectiveness of these security technologies in supporting a secure mobile payment, and discuss research issues to enhance the security of mobile payment for large scale deployment of m-commerce.","Business,
Mobile computing,
Public key,
Wireless application protocol,
Public key cryptography,
Computer science,
Personal digital assistants,
Communication system security,
Information security,
Authentication"
"Non-Euclidean distance measures in AIRS, an artificial immune classification system","The AIRS classifier, based on principles derived from resource limited artificial immune systems, performs consistently well over a broad range of classification problems. This paper explores the effects of adding nonEuclidean distance measures to the basic AIRS algorithm using four well-known publicly available classification problems having various proportions of real, discrete, and nominal features.","Artificial immune systems,
Immune system,
Testing,
Performance loss,
Computer science,
Iris,
Diabetes,
Sonar,
Performance evaluation,
Supervised learning"
Simultaneous driver sizing and buffer insertion using a delay penalty estimation technique,"To achieve timing closure in a placed design, buffer insertion and driver sizing are two of the most effective transforms that can be applied. Since the driver-sizing solution and the buffer-insertion solution affect each other, suboptimal solutions may result if these techniques are applied sequentially instead of simultaneously. We show how to simply extend van Ginneken's buffer-insertion algorithm to simultaneously incorporate driver sizing and introduce the idea of a delay penalty to encapsulate the effect of driver sizing on the previous stage. The delay penalty can be precomputed efficiently via dynamic programming. Experimental results show that using driver sizing with a delay-penalty function obtains designs with superior timing and area characteristics.","Delay estimation,
Delay effects,
Dynamic programming,
Timing,
Wire,
Optimization,
Computer science,
Integrated circuit interconnections,
Heuristic algorithms,
Libraries"
An algorithmic approach to identifying link failures,"Due to the Internet's sheer size, complexity, and various routing policies, it is difficult if not impossible to locate the causes of large volumes of BGP update messages that occur from time to time. To provide dependable global data delivery we need diagnostic tools that can pinpoint the exact connectivity changes. We describe an algorithm, called MVSChange that can pin down the origin of routing changes due to any single link failure or link restoration. Using a simplified model of BGP, called simple path vector protocol (SPVP), and a graph model of the Internet, MVSChange takes as input the SPVP update messages collected from multiple vantage points and accurately locates the link that initiated the routing changes. We provide theoretical proof for the correctness of the design.","Internet,
Monitoring,
Topology,
Computer science,
Fault tolerance,
Intersymbol interference,
Routing protocols,
Fault diagnosis,
Graph theory,
Data analysis"
Efficient filtration of sequence similarity search through singular value decomposition,"Similarity search in textual databases and bioinformatics has received substantial attention in the past decade. Numerous filtration and indexing techniques have been proposed to reduce the curse of dimensionality. This paper proposes a novel approach to map the problem of whole- genome sequence similarity search into an approximate vector comparison in the well-established multidimensional vector space. We propose the application of the singular value decomposition (SVD) dimensionality reduction technique as a pre-processing filtration step to effectively reduce the search space and the running time of the search operation. Our empirical results on a prokaryote and a eukaryote DNA contig dataset, demonstrate effective filtration to prune non-relevant portions of the database with up to 2.3 times faster running time compared with q-gram approach. SVD filtration may easily be integrated as a pre-processing step for any of the well-known sequence search heuristics as BLAST, QUASAR and FastA. We analyze the precision of applying SVD filtration as a transformation-based dimensionality reduction technique, and finally discuss the imposed trade-offs.","Filtration,
Singular value decomposition,
Bioinformatics,
Sequences,
Databases,
Genomics,
DNA,
Computer science,
Indexing,
Multidimensional systems"
Semantic location modeling for location navigation in mobile environment,"Location-based applications require a well-formed representation of spatial knowledge. Current location models can be classified into symbolic or geometric models. The former attempts to represent logical entities and their semantics, but requires a large amount of manual effort for describing them. On the other hand, the latter represents the geometric coordinates but not the semantics. In this paper, we present a semantic location model which preserves topology and distance semantics to support location navigation but at the same time facilitates programmatic model construction and maintenance. The model is based on a sound location theory. It is mainly composed of two hierarchies: a location hierarchy and an exit hierarchy, which can be derived from spatial maps, such as floor plans, without manual intervention. Through a series of model construction algorithms and a real example, we show that our model is simple but powerful enough to capture spatial connectivity and hierarchical relationship to support location-based applications. Furthermore, the location and exit hierarchies are easy to understand by human users.","Navigation,
Solid modeling,
Topology,
Floors,
Ubiquitous computing,
Computer science,
Application software,
Humans,
Mobile computing,
Spatial databases"
Development of a medium-bore high-efficiency helical coil electromagnetic launcher,"The design, development, and testing of a medium-bore helical coil electromagnetic launcher (HCEL) that can accelerate a 500-gram mass to 100 m/s with an efficiency of 13.2% is presented and discussed. The HCEL has a bore size of 40 mm and a length of 750 mm. The measured HCEL inductance gradient of 150 mH/m is more than a 100 times larger than a conventional railgun. The HCEL is powered by a sequentially fired pulse forming network consisting of four 15-kJ electrolytic capacitor banks. Typical PFN V-I characteristics are approximately 15-kA peak current at 550 V for 8-ms pulse length. A computer model of the HCEL launcher is constructed in PSPICE and accurately predicts launcher performance within 25%. Theoretical and numerical predictions of launcher performance are compared with experimental measurements, including projectile velocity, launcher voltage, launcher current, and inductance gradient.","Coils,
Electromagnetic launching,
Inductance measurement,
Testing,
Life estimation,
Boring,
Electromagnetic measurements,
Railguns,
Capacitors,
Predictive models"
Scan power minimization through stimulus and response transformations,"Scan-based cores impose considerable test power challenges due to excessive switching activity during shift cycles. The consequent test power constraints force SOC designers to sacrifice parallelism among core tests, as exceeding power thresholds may damage the chip being tested. Reduction of test power for SOC cores can thus increase the number of cores that can be tested in parallel, improving significantly SOC test application time. In this paper, we propose a scan chain modification technique that inserts logic gates on the scan path. The consequent beneficial test data transformations are utilized to reduce the scan chain transitions during shift cycles and hence test power. We introduce a matrix band algebra that models the impact of logic gate insertion between scan cells on the test stimulus and response transformations realized. As we have successfully modeled the response transformations as well, the methodology we propose is capable of truly minimizing the overall test power. The test vectors and responses are analyzed in an intertwined manner, identifying the best possible scan chain modification, which is realized at minimal area cost. Experimental results justify the efficacy of the proposed methodology as well.","Logic testing,
Logic gates,
Power dissipation,
Parallel processing,
Matrices,
Algebra,
Inverters,
Computer science,
Power engineering and energy,
Logic functions"
Architecture paradigms and their influences and impacts on component-based software systems,"Object-oriented architecture (OOA), component-based architecture (CBA), and service-based architecture (SBA) represent three technical architecture paradigms in current software systems. object, component, and service are three key concepts in distributed software systems. From implementation point of view, a service is implemented by one or more components, which in turn are often implemented in object-oriented programming languages like C+ + and Java. Distributed component-based software systems can be structured in any of the architecture paradigms, some have more advantages than others depending on business requirements. Understanding the characteristics, features, benefits, and concerns of the architecture paradigms is crucial to the successful design, implementation and operation of a distributed system. In this paper, we describe the characteristics of the three architecture paradigms and the business drivers for their applications. The parallel evolution of architecture paradigms and software development methodologies is discussed in the context of practical business needs. Component-based software developers for distributed systems should decide on the architecture paradigms based on business requirements. The evolution of architecture paradigms and the selection of architecture paradigms have profound influences and impacts on component-based distributed systems, in the way components are designed and in the way component interactions are implemented. We discuss these influences and impacts with the goal of deriving some practical principles and strategies to best deal with them in software engineering practices.","Computer architecture,
Software systems,
Software engineering,
Java,
Software architecture,
Web server,
Application software,
Mathematics,
Imaging phantoms,
Object oriented programming"
ATLAS trigger/DAQ RobIn prototype,"A Toroidal LHC ApparatuS (ATLAS) Trigger/Data Acquisition (TDAQ) system connects via 1600 Read-Out-Links (ROL) to the ATLAS subdetectors. Each Read-Out-Buffer (RobIn) prototype attaches to 2 ROLs, buffers the incoming event data stream of 160 MB/s each, and provides samples upon request to the TDAQ system. We present the design of the PCI-based RobIn module, which is built around a XILINX XV2V1500 field-programmable-gate-array, together with initial results from rapid prototyping studies.","Data acquisition,
Prototypes,
Large Hadron Collider,
Field programmable gate arrays,
Computer architecture,
Bandwidth,
Buffer storage,
Physics,
Hardware,
Optical buffering"
A quantitative analysis of anonymous communications,"This paper quantitatively analyzes anonymous communication systems (ACS) with regard to anonymity properties. Various ACS have been designed & implemented. However, there are few formal & quantitative analyzes on how these systems perform. System developers argue the security goals which their systems can achieve. Such results are vague & not persuasive. This paper uses a probabilistic method to investigate the anonymity behavior of ACS. In particular, this paper studies the probability that the true identity of a sender can be discovered in an ACS, given that some nodes have been compromised. It is through this analysis that design guidelines can be identified for systems aimed at providing communication anonymity. For example, contrary to what one would intuitively expect, these analytic results show that the probability that the true identity of a sender can be discovered might not always decrease as the length of communication path increases.","Internet,
Protection,
Communication system security,
Electronic voting,
Performance analysis,
Guidelines,
Electronic mail,
Protocols,
Computer science education,
Educational programs"
Understanding and enhancing polarization in complex materials,"Recent advances in theoretical methods and high-performance computing allow for reliable first-principles investigations of complex materials. This article focuses on calculating and predicting the properties of piezoelectrics and ""designing"" new materials with enhanced piezoelectric responses. This paper considers two systems: boron-nitride nanotubes (BNNTs) and polymers in the polyvinylidene fluoride (PVDF) family.","Piezoelectric polarization,
Electrons,
Piezoelectric materials,
Crystalline materials,
Stress,
Ferroelectric materials,
Wave functions,
Semiconductor materials,
Capacitive sensors,
Pyroelectricity"
Kernel subspace LDA with optimized kernel parameters on face recognition,"This work addresses the problem of selection of kernel parameters in kernel fisher discriminant for face recognition. We propose a new criterion and derive a new formation in optimizing the parameters in RBF kernel based on the gradient descent algorithm. The proposed formulation is further integrated into a subspace LDA algorithm and a new face recognition algorithm is developed. FERET database is used for evaluation. Comparing with the existing kernel LDA-based methods with kernel parameter selected by experiment manually, the results are encouraging.","Kernel,
Linear discriminant analysis,
Face recognition,
Machine learning algorithms,
Mathematics,
Support vector machines,
Computer science,
Image databases,
Lighting,
Algorithm design and analysis"
On feature interactions among Web services,"Web services promise to allow businesses to adapt rapidly to changes in the business environment, and the needs of different customers. However, the rapid introduction of new services paired with the dynamicity of the business environment also leads to undesirable interactions that negatively impact service quality and user satisfaction. In this paper, we propose an approach for modeling such undesirable interactions as feature interactions. Our approach for detecting interactions is based on goal-oriented analysis and scenario modeling. It allows us to reason about feature interactions in terms of goal conflicts, and feature deployment. Two case studies illustrate the approach. The paper concludes with a discussion, and an outlook on future research.","Web services,
Security,
Computer science,
Systems engineering and theory,
Telephony,
Filters,
Software systems,
Service oriented architecture,
Privacy,
Law"
Overhead reduction techniques for software dynamic translation,"Summary form only given. Software dynamic translation (SDT) is a technology that allows programs to be modified as they are running. The overhead of monitoring and modifying a running program's instructions is often substantial in SDT systems. As a result, SDT can be impractically slow, especially in SDT systems that do not or cannot employ dynamic optimization to offset overhead. This is unfortunate since SDT has obvious advantages in modern computing environments and interesting applications of SDT continue to emerge. We investigate several overhead reduction techniques, including indirect branch translation caching, fast returns, and static trace formation that can improve SDT performances significantly.","Application virtualization,
Linux,
Switches,
Computer science,
Distributed processing,
Application software,
Instruction sets,
Buildings,
Performance evaluation,
Memory management"
Sequence designs for ultra-wideband impulse radio with optimal correlation properties,"We formulate a combinatorial model of impulse radio sequences (IRSs) to study sequence or signal design for ultra-wideband (UWB) radio with unmodulated time hopping. Using this combinatorial model for IRSs, we develop necessary and sufficient conditions for the existence of IRSs. Several novel constructions for IRSs with optimal correlation properties are given. The constructions involve the Welch construction for Costas arrays, a quadratic polynomial construction over finite fields, recursive techniques for optical orthogonal codes, and combinatorial design techniques using perfect Mendelsohn designs (PMDs).",
Specification test coverage adequacy criteria = specification test generation inadequacy criteria,"The successful analysis technique model checking can be employed as a test-case generation technique to generate tests from formal models. When using a model checker for test case generation, we leverage the witness (or counter-example) generation capability of model-checkers for constructing test cases. Test criteria are expressed as temporal properties and the witness traces generated for these properties are instantiated to create complete test sequences, satisfying the criteria. In this report we describe an experiment where we investigate the fault finding capability of test suites generated to provide three specification coverage metrics proposed in the literature (state, transition, and decision coverage). Our findings indicate that although the coverage may seem reasonable to measure the adequacy of a test suite, they are unsuitable when used to generate test suites. In short, the generated test sequences technically provide adequate coverage, but do so in a way that tests only a small portion of the formal model. We conclude that automated testing techniques must be pursued with great caution and that new coverage criteria targeting formal specifications are needed.","Automatic testing,
System testing,
Formal specifications,
Logic testing,
Computer science,
Electronic mail,
Explosions,
Performance evaluation,
Production systems,
Aerospace engineering"
Benchmarking block ciphers for wireless sensor networks,"The energy efficiency requirement of wireless sensor networks (WSN) is especially high because the sensor nodes are meant to operate without human intervention for a long period of time with little energy supply. Besides, available storage is scarce due to their small physical size. Therefore choosing the most storage- and energy-efficient block cipher for WSN is important. However to our knowledge so far no systematic work has been conducted in this area. We have identified the candidates of block ciphers suitable for WSN based on existing literature and authoritative recommendations. We have benchmarked these candidates and based on this benchmark, we have selected the suitable ciphers for WSN, namely Rijndael for high security and energy efficiency requirements; but MISTY1 for good storage and energy efficiency. In terms of operation mode, we recommend output feedback mode for static networks, but counter mode for dynamic networks.",
A Geometric Programming Framework for Optimal Multi-Level Tiling,"Determining the optimal tile size-one that minimizes the execution time-is a classical problem in compilation and performance tuning of loop kernels. Designing a model of the overall execution time of a tiled loop nest is an important subproblem. Both problems become harder when tiling is applied at multiple levels. We present a framework for determining the optimal tile sizes for a fully permutable, perfectly nested, rectangular loop with uniform dependences. Our framework supports multiple levels of tiling and uses a BSP style high level model for estimating the overall execution time of a loop program. In our framework, the problem of determining the optimal tile sizes, subject to memory capacity and bandwidth constraints, is modeled as a geometric program and transformed into a convex optimization problem, which can be solved efficiently. The model is validated through experimental results obtained by running twenty loop programs for different levels of tiling and different program and tile parameters. Our framework is very general and can also be used to solve the optimal tile size problem with many other models of execution time.","Tiles,
Cost function,
Computer science,
Bandwidth,
Kernel,
Solid modeling,
Constraint optimization,
Law,
Legal factors,
Shape"
The design and deployment of a wearable vibrotactile feedback system,"We present work on the development and deployment of a wearable system for displaying vibrotactile stimuli at multiple locations on a person. This system is targeted as a general-purpose controller, with the flexibility to support many types of output devices, such as pager motors, muffin fans, and solenoids. We describe the deployment of one configuration of our system for use in the military, and discuss design changes we made that resulted from this deployment. Our major design goals include physical robustness, light weight, low power, and wireless communication. Once these goals are attained, we will explore the size of the ""vocabulary"" of cues that can be unambiguously identified by the wearer.","Feedback,
Control systems,
Biomedical monitoring,
Deafness,
Haptic interfaces,
Cities and towns,
Torso,
Fingers,
Lifting equipment,
Computer science"
Application of the modified group delay function to speaker identification and discrimination,"In this paper, we explore new methods by which speakers can be identified and discriminated, using features derived from the Fourier transform phase. The modified group delay feature (MODGDF) which is a parameterized form of the modified group delay function is used as a front end feature in this study. A Gaussian mixture model (GMM) based speaker identification system is built with the MODGDF as the front end feature. The system is tested on both clean (TIMIT) and noisy telephone (NTIMIT) speech. The results obtained are compared with traditional Mel frequency cepstral coefficients (MFCC) which is derived from the Fourier transform magnitude. When both MFCC and MODGDF were combined, the performance improved by about 4% indicating that both phase and magnitude contain complementary information. In an earlier paper (Murthy et al. (2003)), it was shown that the MODGDF does possess phoneme specific characteristics. In this paper we show that the MODGDF has speaker specific properties. We also make an attempt to understand speaker discriminating characteristics of the MODGDF using the nonlinear mapping technique based on Sammon mapping (Sammon (1969)) and find that the MODGDF empirically demonstrates a certain level of linear separability among speakers.","Delay,
Fourier transforms,
Mel frequency cepstral coefficient,
Speech,
Application software,
Computer science,
Laboratories,
System testing,
Cepstral analysis,
Telephony"
Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining,"In this paper, we propose an automated method to decide on the number of fuzzy sets and for the autonomous mining of both fuzzy sets and fuzzy association rules. We compare the proposed multiobjective GA based approach with: 1) CURE based approach; 2) Chien et al. (2001) clustering approach. Experimental results on JOOK transactions extracted from the adult data of United States census in year 2000 show that the proposed method exhibits good performance over the other two approaches in terms of runtime, number of large itemsets and number of association rules.","Genetic algorithms,
Association rules,
Data mining,
Fuzzy sets,
Itemsets,
Clustering algorithms,
Field-flow fractionation,
Computer science,
Runtime,
Humans"
An ubiquitous architectural framework and protocol for object tracking using RFID tags,"A completely visible pervasive transaction environment where it is possible to link all related transactions of physical objects and trace their mobility through their entire life process, has been elusive. With the emergence of radio frequency identification (RFID) based tags, it is now practicable to automatically collect information pertaining to any object's place, time, transaction, etc. Based on the pervasive deployment of RFID tags, we propose A novel ubiquitous architecture followed by a protocol for tracking mobile objects in real-time. Our delay analysis and simulation results indicate that the delay incurred in the database update of current tag location is very low and is in the order of seconds, thus providing a very fine granularity in time for consistent location update requests.","Protocols,
RFID tags,
Radiofrequency identification,
Delay effects,
Supply chains,
Airports,
Security,
Computer science,
Analytical models,
Transaction databases"
Extending the information system lifecycle through enterprise application integration: a case study experience,"Enterprise application integration (EAI) technologies support a direct move away from disparate systems operating in parallel towards a more common shared architecture, where systems evolve and merge together. Such an emergence however, presents a paradigm shift in the way that information system (IS) lifecycles are viewed. The integration of IS in-line with the needs of the business is altering IS identity and extending their lifecycle. This makes evaluating the full impact of the system difficult, as it has no definitive start and/or end. The authors demonstrate, through a case study of IS applications within an e-government framework, that EAI can be used as a portfolio of technologies that improves infrastructure integration. However, in doing so, the authors create the need to re-think traditional IS-lifecycle norms.","Information systems,
Computer aided software engineering,
Electronic government,
Management information systems,
Business,
Portfolios,
Application software,
Terminology,
Risk management,
Computer networks"
Modelling the variation in human decision making,"This paper presents the results of the research on modelling the variation in human decision making. The relationship between the uncertainty introduced to the membership functions (mfs) of a fuzzy logic system (FLS) and the variation in FLS's decision making is explored using two separate methods. Initially uncertainty is introduced to a type-1 FLS by adding noise to its mfs and the effect on decision making is examined. Secondly an interval type-2 FLS is developed by representing the terms used in the FLS with interval type-2 fuzzy sets and the variation in decision making is studied using the FLS's interval outputs. The variations in ranking of umbilical acid-base assessments by six experts are compared to the simulation results from the developed FLSs. It is shown that there is a direct relationship between the variation in decision making and the uncertainty in the linguistic terms used, and the level of variation is proportional to the magnitude of uncertainty.","Humans,
Decision making,
Fuzzy sets,
Uncertainty,
Fuzzy logic,
Processor scheduling,
Computer science,
Medical expert systems,
Medical services,
Biomedical equipment"
Pulse pair beamforming and the effects of reflectivity field variations on imaging radars,"Coherent radar imaging (CRI), which is fundamentally a beamforming process, has been used to create images of microscale, reflectivity structures within the resolution volume of atmospheric Doppler radars. This powerful technique has the potential to unlock many new discoveries in atmospheric studies. The Turbulent Eddy Profiler (TEP) is a unique 915 MHz boundary layer radar consisting of a maximum of 91 independent receivers. The TEP configuration allows sophisticated CRI algorithms to be implemented providing significant improvement in angular resolution. The present work includes a thorough simulation study of some of the capabilities of the TEP system. The pulse pair processor, used for radial velocity and spectral width estimation with meteorological radars, is combined with beamforming technique, in an efficient manner, to the imaging radar case. By numerical simulation the new technique is shown to provide robust and computationally efficient estimates of the spectral moments. For this study, a recently developed atmospheric radar simulation method is employed that uses the ten thousand scattering points necessary for the high resolution imaging simulation. Previous methods were limited in the number of scatterers due to complexity issues. Radial velocity images from the beamforming radar are used to estimate the three-dimensional wind field map within the resolution volume. It is shown that a large root mean square (RMS) error in imputed three-dimensional wind fields can occur using standard Fourier imaging. This RMS error does not improve even as SNR is increased. The cause of the error is reflectivity variations within the resolution volume. The finite beamwidth of the beamformer skews the radial velocity estimate, and this results in poor wind field estimates. Adaptive Capon beamforming consistently outperforms the Fourier method in the quantitative study and has been demonstrated to enhance the performance compared to the Fourier method.","Radar imaging,
Imaging,
Array signal processing,
Reflectivity,
Image resolution,
Atmospheric modeling"
Empirical mode decomposition of voiced speech signal,"This paper describes a new technique, called the empirical mode decomposition (EMD) that has recently been pioneered by N. E. Huang and al., for adaptively representing nonstationary signals as sums of zero-mean AM-FM components [N. E. Huang, et al., 1998]. The components, called intrinsic mode functions (IMFs), allow the analysis of frequency composition of one-dimensional signals. Applied to speech signal, the EMD allows us to study the different intrinsic oscillatory modes. Besides, computing the LPC analysis of each mode provides an estimation of formants. The presented method is firstly applied on a sum of pure frequency signals. Among different modes we can detect all frequencies taking a part of a signal.","Signal analysis,
Linear predictive coding,
Speech analysis,
Iterative algorithms,
Signal processing,
Computer science,
Educational institutions,
Time frequency analysis,
Shape,
Wavelet analysis"
Pessimism in the stochastic analysis of real-time systems: concept and applications,"The exact stochastic analysis of most real-time systems is becoming unaffordable in current practice. On one side, the exact calculation of the response time distribution of the tasks is not possible except for simple periodic and independent task sets. On the other side, in practice, tasks introduce complexities like release jitter, blocking in shared resources, stochastic dependencies, etc, which can not be handled by the periodic and independent task set model. This paper introduces the concept of pessimism in the stochastic analysis of real-time systems in the following sense: the exact probability of missing any deadline is always lower than that derived from the pessimistic analysis. Therefore, if real-time constraints are expressed as probabilities of missing deadlines, the pessimistic stochastic analysis provides safe results. Some applications of the pessimism concept are presented. Firstly, the practical problems that arise in the stochastic analysis of periodic and independent task sets are addressed. Secondly, we extend to the stochastic case some well known techniques of the deterministic analysis, such as the blocking in shared resources, and the task priority assignment.",
"Hierarchical Bloom filter arrays (HBA): a novel, scalable metadata management system for large cluster-based storage","An efficient and distributed scheme for file mapping or file lookup scheme is critical in decentralizing metadata management within a group of metadata servers. This work presents a technique called HBA (hierarchical Bloom filter arrays) to map file names to the servers holding their metadata. Two levels of probabilistic arrays, i.e., Bloom filter arrays, with different accuracies are used on each metadata server. One array, with lower accuracy and representing the distribution of the entire metadata, trades accuracy for significantly reduced memory overhead, while the other array, with higher accuracy, caches partial distribution information and exploits the temporal locality of file access patterns. Extensive trace-driven simulations have shown our HBA design to be highly effective and efficient in improving performance and scalability of file systems in clusters with 1,000 to 10,000 nodes (or superclusters).","Scalability,
Engineering management,
File servers,
Bandwidth,
Image storage,
Computer science,
Information filtering,
Information filters,
High performance computing,
Computer networks"
The design of s-boxes by simulated annealing,Substitution boxes are important components in many modern day block and stream ciphers. Their study has attracted a great deal of attention over many years. The development of a variety of cryptosystem attacks has lead to the development of criteria for resilience to such attacks. Some general criteria such as high nonlinearity and low autocorrelation have been proposed (providing some protection against attacks such as linear cryptanalysis and differential cryptanalysis). There has been little application of evolutionary search to the development of s-boxes. In This work we show how a cost function that has found excellent single-output Boolean functions can be generalised to provide improved results for small s-boxes.,"Simulated annealing,
Cryptography,
Boolean functions,
Modems,
Cost function,
Linearity,
Computational modeling,
Jacobian matrices,
Computer science,
Resilience"
Control and data flow graph extraction for high-level synthesis,"The first step in high level synthesis consists of translating a behavioural specification into its corresponding register transfer language (RTL) description. Behavioural specifications are composed by writing code in a hardware description language such as VHDL. The process of translation starts by first deriving a control and data flow graph (CDFG) from the source code of the behavioural specification. The derivation of the CDFG has been mostly done manually, which makes this process time-consuming and error-prone at least in the earlier stage of synthesis. In this paper, we describe a tool that we have developed for automatic conversion of the given VHDL behavioural specification of a circuit into its corresponding CDFG. Since there is no widely accepted format for representing CDFGs, we opted to make the tool generate several representations of the derived CDFG in different formats to accommodate different implementation approaches. This design decision makes our tool quite flexible and highly useful. The proposed tool has been tested on operation and control-dominated behavioural specifications to ensure its accuracy. Experimental results on benchmark circuits show that the tool is highly accurate and can produce a CDFG in a few seconds using minimal computing resources.",
Limitations of equation-based congestion control in mobile ad hoc networks,"Equation-based congestion control has been a promising alternative to TCP for real-time multimedia streaming over the Internet. However, its behavior remains unknown in the mobile ad hoc wireless network (MANET) domain. We study the behavior of TFRC (TCP Friendly Rate Control (S. Floyd et al., 2000; M. Handley et al., 2003) over a wide range of MANET scenarios, in terms of throughput fairness and smoothness. Our result shows that while TFRC is able to maintain throughput smoothness in MANET, it obtains less throughput than the competing TCP flows (i.e., being conservative). We analyze several factors contributing to TFRC's conservative behavior in MANET, many of which are inherent to the MANET network. We also show that TFRC's conservative behavior cannot be completely corrected by tuning its loss event interval estimator. Our study shows the limitations of applying TFRC to the MANET domain, and reveals some fundamental difficulties in doing so.",
MINCE: matching instructions using combinational equivalence for extensible processor,"Designing custom-extensible instructions for extensible processors is a computationally complex task because of the large design space. The task of automatically matching candidate instructions in an application (e.g. written in a high-level language) to a pre-designed library of extensible instructions is especially challenging. Previous approaches have focused on identifying extensible instructions (e.g. through profiling), synthesizing extensible instructions, estimating expected performance gains etc. In this paper we introduce our approach of automatically matching extensible instructions as this key step is missing in automating the entire design flow of an ASIP with extensible instruction capabilities. Since matching using simulation is practically infeasible (simulation time), and traditional pattern matching approaches would not yield reliable results (ambiguity related to a functionally equivalent code that can be represented in many different ways), we adopt combinational equivalence checking. Our MINCE tool as part of our ASIP design flow consists of a translator, a filtering algorithm and a combinational equivalence checking tool. We report matching times of extensible instructions that are 7.3x faster on average (using Mediabench applications) compared to the best known approaches to the problem (partial simulations). In all our experiments MINCE matched correctly and the outcome of the matching step yielded an average speedup of the application of 2.47x. As a summary, our work represents a key step towards automating the whole design flow of an ASIP with extensible instruction capabilities.","Application specific processors,
Application software,
Libraries,
Pattern matching,
Embedded system,
Space exploration,
Computer science,
Design engineering,
Australia,
National electric code"
Design and implementation of MPICH2 over InfiniBand with RDMA support,"Summary form only given. For several years, MPI has been the de facto standard for writing parallel applications. One of the most popular MPI implementations is MPICH. Its successor, MPICH2, features a completely new design that provides more performance and flexibility. To ensure portability, it has a hierarchical structure based on which porting can be done at different levels. In this paper, we present our experiences in designing and implementing MPICH2 over InfiniBand. Because of its high performance and open standard, InfiniBand is gaining popularity in the area of high-performance computing. Our study focuses on optimizing the performance of MPl-1 functions in MPICH2. One of our objectives is to exploit remote direct memory access (RDMA) in InfiniBand to achieve high performance. We have based our design on the RDMA channel interface provided by MP1CH2, which encapsulates architecture-dependent communication functionalities into a very small set of functions. Starting with a basic design, we apply different optimizations and also propose a zero-copy-based design. We characterize the impact of our optimizations and designs using microbenchmarks. We have also performed an application-level evaluation using the NAS parallel benchmarks. Our optimized MPICH2 implementation achieves 7.6/spl mu/s latency and 857 MB/s bandwidth, which are close to the raw performance of the underlying InfiniBand layer. Our study shows that the RDMA channel interface in MPICH2 provides a simple, yet powerful, abstraction that enables implementations with high performance by exploiting RDMA operations in InfiniBand. To the best of our knowledge, this is the first high-performance design and implementation ofMPICH2 on InfiniBand using RDMA support.","Concurrent computing,
Laboratories,
Writing,
Design optimization,
Delay,
Bandwidth,
Access protocols,
Communication standards,
Information science,
Supercomputers"
Incremental satisfiability counting for real-time systems,"Testing constraints for real-time systems are usually verified through the satisfiability of propositional formulae. In this paper, we propose an alternative where the verification of timing constraints can be done by counting the number of truth assignments instead of Boolean satisfiability. This number can also tell us how ""far away"" a given specification is from satisfying its safety assertion. Furthermore, specifications and safety assertions are often modified in an incremental fashion, where problematic bugs are fixed one at a time. To support this development, we propose an incremental algorithm for counting satisfiability. Our proposed incremental algorithm is optimal as no unnecessary nodes are created during each counting. This works for the class of expressions, known as path RTL ([F. Jahanian et al. (1987), F. Wang et al. (1994)]). To illustrate this application, we show how incremental satisfiability counting can be applied to a well-known rail-road crossing example, particularly when its specification is still being refined.","Real time systems,
Safety,
Timing,
Debugging,
Arithmetic,
Logic,
Computer science,
System testing,
Computer bugs,
Costs"
Multilayer versus single-layer optical cross-connect architectures for waveband switching,"Waveband switching (WBS) in conjunction with multigranular optical cross-connect (MG-OXC) architectures can reduce the cost and complexity of switching nodes. In this paper, we study two MG-OXC architectures: the single-layer and the multilayer MG-OXCs, and compare their performances with both off-line (static) and on-line (dynamic) traffic. In the off-line case, a near-optimal integer linear programming models (called off-ILP models) for each of the MG-OXC architectures aims to reduce the size of the MG-OXC, and compares them with the balanced path routing with heavy-traffic first waveband assignment (BPHT) heuristic developed for the multilayer MG-OXCs. The two architectures are then compared in terms of the number of wavelength a fixed number of wavelengths on each link. We also propose a novel efficient heuristic algorithm, called maximum overlap ratio (MOR) to satisfy new requests and compare it with the on-ILP, first-fit, and random-fit algorithms. We compare the two architectures in terms of the blocking probability, weighted (request) acceptance ratio, which serves as an indication of hops (WH) and MG-OXC ports required to satisfy a given set of traffic demands. In the on-line case, we develop an on-line ILP model called on-ILP, which aims to minimize the number of used ports for each of the MG-OXC architectures, given the revenue generated by satisfying the requests. Our results indicate that using WBS with either single-layer or multilayer MG-OXCs can reduce the number of ports (hence the size and cost) of the switching nodes compared to using ordinary OXCs (without waveband switching). In particular, in the off-line case, using single-layer MG-OXCs provides a greater reduction in size than multilayer MG-OXCs, while in the online case, using the multilayer MG-OXC is better","Costs,
Optical switches,
Wavelength division multiplexing,
Computer architecture,
Computer science,
Traffic control,
Spine,
Optical crosstalk,
Passive optical networks,
WDM networks"
Location cache: a low-power L2 cache system,"While set-associative caches incur fewer misses than direct-mapped caches, they typically have slower hit times and higher power consumption, when multiple tag and data banks are probed in parallel. This paper presents the location cache structure which significantly reduces the power consumption for large set-associative caches. We propose to use a small cache, called location cache to store the location of future cache references. If there is a hit in the location cache, the supported cache is accessed as a direct-mapped cache. Otherwise, the supported cache is referenced as a conventional set-associative cache. The worst case access latency of the location cache system is the same as that of a conventional cache. The location cache is virtually indexed so that operations on it can be performed in parallel with the TLB address translation. These advantages make it ideal for L2 cache systems where traditional way-predication strategies perform poorly. We used the CACTI cache model to evaluate the power consumption and access latency of proposed cache architecture. Simplescalar CPU simulator was used to produce final results. It is shown that the proposed location cache architecture is power-efficient. In the simulated cache configurations, up-to 47% of cache accessing energy and 25% of average cache access latency can be reduced.","Energy consumption,
Delay,
Permission,
Data engineering,
Power engineering and energy,
Computer science,
Performance evaluation,
Power system modeling,
Cache memory,
Prefetching"
A framework for learning biped locomotion with dynamical movement primitives,"This article summarizes our framework for learning biped locomotion using dynamical movement primitives based on nonlinear oscillators. Our ultimate goal is to establish a design principle of a controller in order to achieve natural humanlike locomotion. We suggest dynamical movement primitives as a central pattern generator (CPG) of a biped robot, an approach we have previously proposed for learning and encoding complex human movements. Demonstrated trajectories are learned through movement primitives by locally weighted regression, and the frequency of the learned trajectories is adjusted automatically by a frequency adaptation algorithm based on phase resetting and entrainment of coupled oscillators. Numerical simulations and experimental implementation on a physical robot demonstrate the effectiveness of the proposed locomotion controller. Furthermore, we demonstrate that phase resetting contributes to robustness against external perturbations and environmental changes by numerical simulations and experiments.","Legged locomotion,
Oscillators,
Frequency,
Robot kinematics,
Laboratories,
Robotics and automation,
Numerical simulation,
Mechanical systems,
Computational intelligence,
Computer science"
Numerical modelling of the electric field in HV substations,A hybrid technique combining the charge simulation and boundary element method has been employed to solve the problem in question. In this model the influence of tower frameworks and of the apparatus construction is taken into account. The basic boundary integral equations are presented and the computer program is briefly described. Selected numerical results concerning a 400 kV outdoor substation are given. The numerical technique and the computer program can be a useful aid for designers of electric power objects as well as for environmental protection authorities. In the near future the program will be applied in a commercial software package for the electric and magnetic-field computation near HV installations.,"Laplace equations,
substations,
boundary-elements methods,
boundary integral equations,
electromagnetic interference"
Using transient/persistent errors to develop automated test oracles for event-driven software,"Today's software-intensive systems contain an important class of software, namely event-driven software (EDS). All EDS take events as input, change their state, and (perhaps) output an event sequence. EDS is typically implemented as a collection of event-handlers designed to respond to individual events. The nature of EDS creates new challenges for test automation. In this paper, we focus on those relevant to automated test oracles. A test oracle is a mechanism that determines whether a software executed correctly for a test case. A test case for an EDS consists of a sequence of events. The test case is executed on the EDS, one event at a time. Errors in the EDS may ""appear"" and later ''disappear"" at several points (e.g., after an event is executed) during test case execution. Because of the behavior of these transient (those that disappear) and persistent (those that don't disappear) errors, EDS require complex and expensive test oracles that compare the expected and actual output multiple times during test case execution. We leverage our previous work to study several applications and observe the occurrence of persistent/transient errors. Our studies show that in practice, a large number of errors in EDS are transient and that there are specific classes of events that lead to transient errors. We use the results of this study to develop a new test oracle that compares the expected and actual output at strategic points during test case execution. We show that the oracle is effective at detecting errors and efficient in terms of resource utilization","Automatic testing,
Software testing,
Embedded software,
Automation,
Discrete event simulation,
Application software,
Costs,
Computer errors,
Computer science,
Educational institutions"
High-level grid application environment to use legacy codes as OGSA grid services,"One of the biggest obstacles in the wide-spread industrial take-up of grid technology is the existence of a large amount of legacy code that is not accessible as grid services. The paper describes a new approach (GEMLCA: grid execution management for legacy code architecture) to deploy legacy codes as grid services without modifying the original code. Moreover, we show a workflow execution oriented grid portal technology (P-GRADE portal) by which such legacy code based grid services can be applied in complex business processes. GEMLCA has been implemented as GT-3 services but can be easily ported into the new WSRF grid standards.","Portals,
Middleware,
Mesh generation,
Application software,
Parallel processing,
Computer science,
Production,
Testing,
Java,
Graphical user interfaces"
Generalized adaptive notch filters,"The problem of identification/tracking of quasi-periodically varying systems is considered. This problem is a generalization, to the system case, of a classical signal processing task of either elimination or extraction of nonstationary sinusoidal signals buried in noise. The proposed solution is based on the exponentially weighted basis function (EWBF) approach. First, the global EWBF algorithm is derived and its decomposed, parallel-form and cascade-form variants, are described. Then the frequency-adaptive versions of both schemes are obtained using the recursive prediction error method. In the (special) signal processing case the paper offers new attractive solutions to the problem of adaptive notch filtering.",
On universality of general reversible multiple-valued logic gates,"A set of p-valued logic gates (primitives) is called universal if an arbitrary p-valued logic function can be realized by a logic circuit built up from a finite number of gates belonging to this set. In this paper, we consider the problem of determining the number of universal single-gate libraries of p-valued reversible logic gates with two inputs and two outputs, under the assumption that constant signals can be applied to an arbitrary number of inputs. We have proved some properties of such gates and established that over 97% of ternary gates are universal.","Logic gates,
Logic circuits,
Quantum computing,
Logic functions,
Computer science,
Delay,
Information technology,
DH-HEMTs,
Libraries,
Calculus"
Texture analysis by genetic programming,"This work presents the use of genetic programming (GP) to a complex domain, texture analysis. Two major tasks of texture analysis, texture classification and texture segmentation, are studied. Bitmap textures are used in this investigation. In classification tasks, the results show that GP is able to evolve accurate classifiers based on texture features. Moreover by using the presented method, GP is able to evolve accurate classifiers without extracting texture features. In texture segmentation tasks, the investigation shows that a fast and accurate segmentation method can be developed based on GP generated texture classifiers. Our further investigation show that the accuracies are not achieved by chance. There are regularities been captured by GP-generated classifiers in performing texture discrimination.","Genetic programming,
Image texture analysis,
Image segmentation,
Feature extraction,
Computer science,
Information technology,
Information analysis,
Medical diagnosis,
Object detection,
Layout"
An extended operational profile model,"Operational profiles are a quantification of usage patterns for a software application. These profiles are used to measure software reliability by testing the software in a manner that represents actual use. The current definition of an operational profile states that it is the set of operations available in the application, and the operations probabilities of occurrence in customer usage scenarios. This definition is too limited. In most industrial applications, focusing on operations alone does not offer adequate representation of the use of software. The limited definition of operational profiles can restrict their applicability and hence software reliability analysis for many software development organizations. This paper describes a formal and practical extension of the current definition of operational profiles to increase their applicability.","Software reliability,
Software testing,
Application software,
System testing,
Software measurement,
Terminology,
Computer science,
Computer industry,
Reliability engineering,
Programming"
Fault-tolerant data delivery for multicast overlay networks,"Overlay networks represent an emerging technology for rapid deployment of novel network services and applications. However, since public overlay networks are built out of loosely coupled end-hosts, individual nodes are less trustworthy than Internet routers in carrying out the data forwarding function. Here we describe a set of mechanisms designed to detect and repair errors in the data stream. Utilizing the highly redundant connectivity in overlay networks, our design splits each data stream to multiple sub-streams which are delivered over disjoint paths. Each sub-stream carries additional information that enables receivers to detect damaged or lost packets. Furthermore, each node can verify the validity of data by periodically exchanging Bloom filters, the digests of recently received packets, with other nodes in the overlay. We have evaluated our design through both simulations and experiments over a network testbed. The results show that most nodes can effectively detect corrupted data streams even in the presence of multiple tampering nodes.","Fault tolerance,
Fault detection,
Computer science,
IP networks,
Robustness,
Protocols,
Streaming media,
Application software,
Filters,
Testing"
Constructing dynamic test environments for genetic algorithms based on problem difficulty,"In recent years the study of dynamic optimization problems has attracted an increasing interest from the community of genetic algorithms and researchers have developed a variety of approaches into genetic algorithms to solve these problems. In order to compare their performance, an important issue is the construction of standardized dynamic test environments. Based on the concept of problem difficulty, This work proposes a new dynamic environment generator using a decomposable trap function. With this generator, it is possible to systematically construct dynamic environments with changing and bounding difficulty and hence, we can test different genetic algorithms under dynamic environments with changing but controllable difficulty levels.","Genetic algorithms,
System testing,
Character generation,
Crosstalk,
Computer science,
Control systems,
Robustness,
Production,
Space stations,
Genetic mutations"
Resource allocation for heterogeneous services in multiuser OFDM systems,"In this paper, resource allocation for heterogeneous services is studied in multiuser orthogonal frequency division multiplexing (OFDM) systems. We propose a resource allocation algorithm, which is designed to improve the system throughput while satisfying the quality of service (QoS) requirements of both the real-time and nonreal-time services. In the proposed algorithm, the resources, composed of subcarriers and transmit power, are adaptively allocated to the users based on their service types and channel states over two sequential steps: (1) resource allocation for the real-time users that minimizes the resource usage required to satisfy the data rate requirement, and (2) resource allocation for the nonreal-time users that maximizes the system throughput using the remaining resource. The performance of the proposed resource allocation algorithm is evaluated in a frequency-selective fading channel, and compared with that of a simple resource allocation algorithm. Numerical results show that the proposed algorithm provides a significant throughput gain over the conventional algorithm.","Resource management,
Throughput,
Real time systems,
Quality of service,
Delay,
OFDM modulation,
Mobile communication,
Transmitters,
Channel state information,
Computer science"
Registration of diffusion tensor images,This paper presents a novel affine registration algorithm for diffusion tensor images. The proposed metric derived from the standpoint of diffusion profiles not only has concrete physical underpinning but also can be extended for comparing higher-order diffusion models. The non-translational part of the affine transformation is parametrized in the spirit of the Polar Decomposition Theorem. The registration objective function and its derivatives are derived analytically by combining this parametrization scheme with finite strain tensor reorientation. The affine algorithm is embeded in a multi-resolution piecewise affine framework for non-rigid registration.,
Bayesian reinforcement learning for coalition formation under uncertainty,,
Parallel routing and wavelength assignment for optical multistage interconnection networks,"Multistage interconnection networks (MINs) are among the most efficient switching architectures in terms of the number of switching elements (SEs) used. For optical MINs (OMINs), two I/O connections with neighboring wavelengths cannot share a common SE due to crosstalk. In this paper, we focus on the wavelength dilation approach, in which the I/O connections sharing a common SE will be assigned different wavelengths with enough wavelength spacing. We first study the permutation capacity of OMINs, then propose fast parallel routing and wavelength assignment algorithms for OMINs. By applying our permutation decomposition and graph coloring techniques, the proposed algorithms can route any permutation without crosstalk in wavelength-rearrangeable space-strict-sense Banyan networks and wavelength-rearrangeable space-rearrangeable Benes networks in polylogarithmic time using a linear number of processors.","Wavelength routing,
Wavelength assignment,
Optical interconnections,
Optical fiber networks,
Multiprocessor interconnection networks,
Optical crosstalk,
Space technology,
Computer science,
Communication switching,
Optical fiber communication"
An Orthogonal Multi-objective Evolutionary Algorithm for Multi-objective Optimization Problems with Constraints,"In this paper, an orthogonal multi-objective evolutionary algorithm (OMOEA) is proposed for multi-objective optimization problems (MOPs) with constraints. Firstly, these constraints are taken into account when determining Pareto dominance. As a result, a strict partial-ordered relation is obtained, and feasibility is not considered later in the selection process. Then, the orthogonal design and the statistical optimal method are generalized to MOPs, and a new type of multi-objective evolutionary algorithm (MOEA) is constructed. In this framework, an original niche evolves first, and splits into a group of sub-niches. Then every sub-niche repeats the above process. Due to the uniformity of the search, the optimality of the statistics, and the exponential increase of the splitting frequency of the niches, OMOEA uses a deterministic search without blindness or stochasticity. It can soon yield a large set of solutions which converges to the Pareto-optimal set with high precision and uniform distribution. We take six test problems designed by Deb, Zitzler et al., and an engineering problem (W) with constraints provided by Ray et al. to test the new technique. The numerical experiments show that our algorithm is superior to other MOGAS and MOEAs, such as FFGA, NSGAII, SPEA2, and so on, in terms of the precision, quantity and distribution of solutions. Notably, for the engineering problem W, it finds the Pareto-optimal set, which was previously unknown.",
Shape correspondence through landmark sliding,"Motivated by improving statistical shape analysis, this paper presents a novel landmark-based method for accurate shape correspondence, where the general goal is to align multiple shape instances by corresponding a set of given landmark points along those shapes. Different from previous methods, we consider both global shape deformation and local geometric features in defining the shape-correspondence cost function to achieve a consistency between the landmark correspondence and the underlying shape correspondence. According to this cost function, we develop a novel landmark-sliding algorithm to achieve optimal landmark-based shape correspondence with preserved shape topology. The proposed method can be applied to correspond various 2D shapes in the forms of single closed curves, single open curves, self-crossing curves, and multiple curves. We also discuss the practical issue of landmark initialization. The proposed method has been tested on various biological shapes arising from medical image analysis and validated in constructing statistical shape models.","Shape measurement,
Cost function,
Topology,
Biomedical imaging,
Image analysis,
Application software,
Computer science,
Medical tests,
Biological system modeling,
Computer vision"
An evaluation of aspect-oriented programming for Java-based real-time systems development,"Some concerns, such as debugging or logging functionality, cannot be captured cleanly, and are often tangled and scattered throughout the code base. These concerns are called crosscutting concerns. Aspect-oriented programming (AOP) is a paradigm that enables developers to capture crosscutting concerns in separate aspect modules. The use of aspects has been shown to improve understandability and maintainability of systems. It has been shown that real-time concerns, such as memory management and thread scheduling, are crosscutting concerns [A. Corsaro et al., (2002), M.Deters et al., (2001), A. Gal et al., (2002)]. However it is unclear whether encapsulating these concerns provides benefits. We were interested in determining whether using AOP to encapsulate real-time crosscutting concerns afforded benefits in system properties such as understandability and maintainability. This paper presents research comparing the system properties of two systems: a real-time sentient traffic simulator and its aspect-oriented equivalent. An evaluation of AOP is presented indicating both benefits and drawbacks with this approach",
A symmetric modal lambda calculus for distributed computing,"We present a foundational language for spatially distributed programming, called Lambda 5, that addresses both mobility of code and locality of resources. In order to construct our system, we appeal to the powerful propositions-as-types interpretation of logic. Specifically, we take the possible worlds of the intuitionistic modal logic IS5 to be nodes on a network, and the connectives /spl square/ and /spl diams/ to reflect mobility and locality, respectively. We formulate a novel system of natural deduction for IS5, decomposing the introduction and elimination rules for /spl square/ and /spl diams/, thereby allowing the corresponding programs to be more direct. We then give an operational semantics to our calculus that is type-safe, logically faithful, and computationally realistic.","Calculus,
Distributed computing,
Logic programming,
Scientific computing,
Computer languages,
Computer networks,
Internet,
Large-scale systems,
Physics computing,
Instruments"
Adaptive digital image inpainting,"Digital image inpainting is a technique that can repair a portion of damaged or removed image by means of automatic mechanisms. Image inpainting tools can be applied to repairing damaged historical documents. In this paper, we propose an adaptive mechanism, which is based on a color interpolation mechanism. The repairing procedure checks the surrounding information of a damaged pixel and decides the range of references that can be used to compute an interpolated color. We have tested on more than 2000 images include painting, photo, and cartoon drawing. The evaluation shows that our mechanism produces a very good result.","Digital images,
Image restoration,
Interpolation,
Watermarking,
Frequency conversion,
Computer science,
Information management,
Testing,
Painting,
Image processing"
Shedding light on stereoscopic segmentation,"We propose a variational algorithm to jointly estimate the shape, albedo, and light configuration of a Lambertian scene from a collection of images taken from different vantage points. Our work can be thought of as extending classical multi-view stereo to cases where point correspondence cannot be established, or extending classical shape from shading to the case of multiple views with unknown light sources. We show that a first naive formalization of this problem yields algorithms that are numerically unstable, no matter how close the initialization is to the true geometry. We then propose a computational scheme to overcome this problem, resulting in provably stable algorithms that converge to (local) minima of the cost functional. Although we restrict our attention to Lambertian objects with uniform albedo, extensions of our framework are conceivable.","Layout,
Lighting,
Shape,
Geometry,
Reflectivity,
Stereo vision,
Computer vision,
Reflection,
Detectors,
Computer science"
Parameterization of the score threshold for a text-dependent adaptive speaker verification system,"We present a computationally efficient strategy for setting a priori thresholds in an adaptive speaker verification system. We have two motivations: to eliminate the externally preset overall system thresholds and replace them with automatically-set internal thresholds conditioned by a target FA rate and calculated at runtime; to counter the verification score shifts resulting from online adaptation. Our approach entails calculating the trajectory of the score threshold as a function of 1) length of the password, 2) target FA, 3) the number of training frames in the speaker model. The solution is successful at both achieving the target FA rates and keeping the FA rate constant during online adaptation. Furthermore, it is algorithmically simple and requires negligible computational resources. The threshold function is calibrated on a Japanese database and experimental results are presented on 12 databases in four different languages.","Adaptive systems,
Databases,
Materials testing,
Speaker recognition,
Security,
Speech,
Calibration,
Computational efficiency,
Computer science,
Runtime"
Sonel mapping: acoustic modeling utilizing an acoustic version of photon mapping,"Acoustic modeling of even small simple environments is a complex, computationally expensive task. Sound, just as light, is itself a wave phenomenon. Although there are several key differences between light and sound there are also several similarities. Given the similarities which exist between sound and light, this work investigates the application of photon mapping (suitably modified), to model environmental acoustics. The resulting acoustic sonel mapping technique can be used to model acoustic environments while accounting for diffuse and specular acoustic reflections as well as refraction and diffraction effects.","Optical reflection,
Acoustic reflection,
Acoustic diffraction,
Acoustic applications,
Computer science,
Acoustic refraction,
Computational modeling,
Computer graphics,
Acoustic propagation,
Optical propagation"
An early evaluation of WSRF and WS-Notification via WSRF.NET,"The Web Services Resource Framework (WSRF) and its companion WS-Notification were introduced in January 2004 as a new model on which to build grids. This paper contains early observations made while implementing the full suite of WSRF and WS-Notification specifications on the Microsoft .NET Platform. While the potential of WSRF and WS-Notification remains strong, initial observations are that there are many challenges that remain to be solved, most notably the implied programming model derived from the specifications, particularly the complexity of service-side and client-code and the complexity of WS-Notification.","Web services,
Service oriented architecture,
Grid computing,
Protocols,
Production facilities,
Computer science,
Middleware,
Subcontracting,
Computer architecture,
Wire"
Omni-directional face detection based on real AdaBoost,"We propose an omni-directional face detection method based on the confidence-rated AdaBoost algorithm, called real AdaBoost, proposed by R.E. Schapire and Y. Singer (see Machine Learning, vol.37, p.297-336, 1999). To use real AdaBoost, we configure the confidence-rated look-up-table (LUT) weak classifiers based on Haar-type features. A nesting-structured framework is developed to combine a series of boosted classifiers into an efficient object detector. For omni-directional face detection, our method has achieved a rather high performance and the processing speed can reach 217 ms per 320/spl times/240 image. Experiment results on the CMU+MIT frontal and the CMU profile face test sets are reported to show its effectiveness.","Face detection,
Object detection,
Detectors,
Boosting,
Real time systems,
Iterative algorithms,
Computer science,
Testing,
Bayesian methods,
Table lookup"
A hypergraph model for the yeast protein complex network,"Summary form only given. We consider a hypergraph model for the protein complex network obtained from a large-scale experimental study to characterize the proteome of the yeast. Our model views the yeast proteome as a hypergraph, with the proteins corresponding to vertices and the complexes corresponding to hyperedges. Previous work has modeled the protein complex data as a protein-protein interaction graph or as a complex intersection graph; both models lose information and require more space. Our results show that the yeast protein complex hyper-graph is a small-world and power-law hypergraph. We design an algorithm for computing the k-core of a hypergraph, and use it to identify the core proteome, the maximum core of the protein complex hypergraph. We show that the core proteome of the yeast is enriched in essential and homologous proteins. We implement greedy approximation algorithms for variant minimum weight vertex covers of a hypergraph; these algorithms can be used to improve the reliability and efficiency of the experimental method that identifies the protein complex network.",
Enhancing Bluetooth TCP throughput via link layer packet adaptation,"TCP throughput limitations over wireless links have received considerable attention in the last few years. One of the problems is that TCP congestion control interprets packet losses as an indication of congestion, whereas in wireless links, losses could be due to transient link quality degradations. In this paper, we propose and study a link layer solution and evaluate its effects on TCP in the context of Bluetooth. We enhance the Bluetooth link layer to make use of channel state information and accordingly adapt the Bluetooth packet type to enhance TCP throughput We propose a simple analytical method to determine the optimal packet type for a given channel state by adding FEC support or changing packet size. Since wireless interfaces, such as 802.11 or Bluetooth, can provide information regarding the channel state using relevant APIs, this simple enhancement can easily be added to the link layer. We implemented this functionality in the Bluetooth link layer. Our simulation experiments show that the proposed adaptive packet type solution significantly improves TCP throughput. The throughput enhancement increases with the error rate. For high error rates close to 0.1%, the link layer enhanced with the adaptive scheme is able to maintain good TCP throughput, whereas throughput is almost zero when the adaptive scheme is not used.","Bluetooth,
Throughput,
Forward error correction,
Error analysis,
Internet,
Buffer overflow,
Computer science,
Degradation,
Channel state information,
Transport protocols"
Compiling a benchmark of documented multi-threaded bugs,"Summary form only given. Testing multithreaded, concurrent, or distributed programs is acknowledged to be a very difficult task. We decided to create a benchmark of programs containing documented multithreaded bugs that can be used in the development of testing tool for the domain. In order to augment the benchmark with a sizable number of programs, we assigned students in a software testing class to write buggy multithreaded Java programs and document the bugs. This paper documents this experiment. We explain the task that was given to the students, go over the bugs that they put into the programs both intentionally and unintentionally, and show our findings. We believe this part of the benchmark shows typical programming practices, including bugs, of novice programmers. In grading the assignments, we used our technologies to look for undocumented bugs. In addition to finding many undocumented bugs, which was not surprising given that writing correct multithreaded code is difficult, we also found a number of bugs in our tools. We think this is a good indication of the expected utility of the benchmark for multithreaded testing tool creators.","Computer bugs,
Benchmark testing,
Programming profession,
Interleaved codes,
Software testing,
Java,
Computer science,
Writing,
Utility theory,
Internet"
Influence of mobility models on the performance of routing protocols in ad-hoc wireless networks,"The patterns of movement followed by the nodes in an ad-hoc network play an important role in the performance of routing protocols. These different patterns of movement of the nodes can be classified into different mobility models, each of which are characterized by their own distinctive features. We have explored the influence of three such mobility models (Pursue, Column and RPGM-RW) on the performance of three well-known ad-hoc routing protocols namely DSR, AODV and DSDV. The simulation environment used for this purpose was NS-2. The significance of this study lies in the fact that there has been very limited investigation of the effect of mobility models on routing protocol performance in ad-hoc networks.","Routing protocols,
Intelligent networks,
Wireless networks,
Ad hoc networks,
Computer science,
Throughput,
Measurement,
Physical layer,
Radio network,
USA Councils"
On the placement and granularity of FPGA configurations,"Dynamic FPGA reconfiguration represents an overhead that can be critical to the performance of a realised circuit. To address this problem, This work presents a technique that is applicable at the times of loading the configuration data on the device. The technique involves reusing the on-chip configuration fragments to implement the next configuration thereby reducing the amount of data that must be externally transferred to the configuration memory. This work provides an analysis of the effect of circuit placement and configuration granularity on configuration reuse. The problem of finding placements of each circuit in a sequence of circuits so as to maximize configuration re-use is considered in detail. A greedy solution to this NP complete problem was found to reduce configuration overheads by less than 5% for a benchmark set. The effect of configuration granularity on configuration reuse was also considered and it was found that reducing the size of the unit of configuration allowed us to reduce the size of the benchmark configurations by 41%.","Field programmable gate arrays,
Circuits,
Australia,
Embedded system,
Computer science,
Real time systems,
Operating systems,
Tail,
Petroleum,
Electronics packaging"
Use of virtualization tools in computer network laboratories,"The paper describes how virtualization tools can be used in computer network laboratories to simplify and dramatically reduce its deployment and management costs. In particular, the Virtual Network User Mode Linux (VNUML) free-software tool (developed as part of the Euro6IX 1ST research project) is introduced, showing how it can be used to easily build complex virtual network scenarios.","Intelligent networks,
Computer networks,
Laboratories,
Computer network management,
Testing,
Application software,
Costs,
Local area networks,
Computational modeling,
Computer simulation"
Combined estimation/coding of highband spectral envelopes for speech spectrum expansion,"The paper addresses the problem of expanding the bandwidth of narrowband speech signals, focusing on the estimation of highband spectral envelopes. It is well known that there is not enough mutual information between the two bands. We show that this happens because narrowband spectral envelopes have a one-to-many relationship with highband spectral envelopes. A combined estimation/coding scheme for the missing spectral envelope is proposed, which employs this relationship to produce a high quality highband reconstruction, provided that there is an appropriate excitation. Subjective tests using the TIMIT database indicate that 134 bits/sec for the highband spectral envelope are adequate for a DCR (degradation category rating) score of 4.41. This is an improvement of 22.8% over a typical estimation of highband envelopes using the usual mapping functions, in terms of DCR score.","Speech coding,
Narrowband,
Mutual information,
Bandwidth,
Testing,
Computer science,
Databases,
Wideband,
Costs,
Parameter estimation"
Capacity-approaching codes on the q-ary symmetric channel for large q,"We give a short survey of several techniques to construct codes on GF(q) that approach the capacity of the q-ary symmetric channel. The q-ary symmetric channel represents the next level of difficulty after the binary erasure channel (BEC). Since the channel is more complex than the BEC, one may hope that codes and decoding algorithms that approach the capacity of this channel for large q may be modified to yield capacity-approaching codes for smaller values of q as well.","Maximum likelihood decoding,
Channel capacity,
Cyclic redundancy check,
Polynomials,
Parity check codes,
Capacity planning,
Random variables,
Mathematics,
Computer science,
Computer errors"
Automated Algorithms for Multiscale Morphometry of Neuronal Dendrites,"We describe the synthesis of automated neuron branching morphology and spine detection algorithms to provide multiscale three-dimensional morphological analysis of neurons. The resulting software is applied to the analysis of a high-resolution (0.098 μm × 0.098 μm × 0.081 μ m) image of an entire pyramidal neuron from layer III of the superior temporal cortex in rhesus macaque monkey. The approach provides a highly automated, complete morphological analysis of the entire neuron; each dendritic branch segment is characterized by several parameters, including branch order, length, and radius as a function of distance along the branch, as well as by the locations, lengths, shape classification (e.g., mushroom, stubby, thin), and density distribution of spines on the branch. Results for this automated analysis are compared to published results obtained by other computer-assisted manual means.",
Cooperation and accounting strategy for multi-hop cellular networks,"Multi-hop cellular networks (also called hybrid networks) appears to be a promising combination of the dynamics of mobile ad hoc networks and the reliability of infrastructured wireless networks. However, several known weaknesses of mobile ad hoc networks still persist. Besides the security and routing issues, the cooperation among nodes is of great importance. We propose a highly decentralized accounting and security architecture that provides a solid foundation for a cooperation scheme based on rewards and is applicable to multi-hop cellular networks. This scheme incorporates a security architecture which is based on public key cryptography and uses digital-signatures and certificates.",
Recurrent neural network with backpropagation through time for speech recognition,"Speech recognition and understanding have been studied for many years. The neural network is well-known as a technique that is able to classify nonlinear problems. Much research has been done in applying neural networks to solving the problem of recognizing speech such as Arabic. Arabic offers a number of challenges to speech recognition. We propose a fully-connected hidden layer between the input and state nodes and the output. We also investigate and show that this hidden layer makes the learning of complex classification tasks more efficient. We also investigate the difference between LPCC (linear predictive cepstrum coefficients) and MFCC (Mel-frequency cepstral coefficients) in the feature extraction process. The aim of the study was to observe the differences in the 29 letters of the Arabic alphabet from ""alif"" to ""ya"". The purpose of this research is to upgrade the knowledge and understanding of Arabic alphabet or words using a fully-connected recurrent neural network (FCRNN) and backpropagation through time (BPTT) learning algorithm. Six speakers (a mixture of male and female) in a quiet environment are used in training.","Recurrent neural networks,
Backpropagation,
Speech recognition,
Neurofeedback,
Neural networks,
Natural languages,
Writing,
Neurons,
Software engineering,
Computer science"
Factors influencing the adoption of residential broadband connections to the Internet,"Broadband adoption has been slower than anticipated in the United States resulting in a number of proposed policy interventions ranging from nonintervention to extreme measures. Digital divide advocates are concerned that demographics of broadband users reveal the same digital divide gaps that are observed in computer ownership and home Internet access. This study analyzes the factors influencing the adoption of residential broadband services using the current population survey data from September 2001. Three statistical models on computer ownership, home Internet access, and broadband access are analyzed to illustrate the differences in demographics between the dependent variables. Using a logit regression model, the results show that the digital divide is the widest for computer ownership and the narrowest for broadband access. The implications of this study are that public policy interventions to bridge the digital divide should focus more on computer ownership, and less on broadband specific policies.","Internet,
IP networks,
Public policy,
Communication networks,
Demography,
Home computing,
US Government,
Communication cables,
DSL,
Bridges"
The reconstruction of user sessions from a server log using improved time-oriented heuristics,"Web usage mining plays an important role in the personalization of Web services, adaptation of Web sites, and the improvement of Web server performance. It applies data mining techniques to discover Web access patterns from Web usage data. In order to discover access patterns, Web usage data should be reconstructed into sessions with or without user identification. However, not all Web server logs contain complete information for constructing user sessions. One approach for solving such a problem is to use time-oriented heuristics to reconstruct user sessions. The paper describes improved statistical-based time-oriented heuristics for the reconstruction of user sessions from a server log. Comparative analyses are carried out using two similarity measures. The performance results of the proposed improved heuristics are promising and in some cases show reasonable improvements.","Hydrogen,
Web server,
Data mining,
Web services,
Computer science,
Niobium,
Electronic mail,
Web mining,
Buildings,
Software agents"
Efficiency enhancement of genetic algorithms via building-block-wise fitness estimation,"This paper studies fitness inheritance as an efficiency enhancement technique for a class of competent genetic algorithms called estimation distribution algorithms. Probabilistic models of important sub-solutions are developed to estimate the fitness of a proportion of individuals in the population, thereby avoiding computationally expensive function evaluations. The effect of fitness inheritance on the convergence time and population sizing are modeled and the speed-up obtained through inheritance is predicted. The results show that a fitness-inheritance mechanism which utilizes information on building-block fitnesses provides significant efficiency enhancement. For additively separable problems, fitness inheritance reduces the number of function evaluations to about half and yields a speed-up of about 1.75-2.25.","Genetic algorithms,
Predictive models,
Laboratories,
Convergence,
Algorithm design and analysis,
Computational modeling,
Scalability,
Computer science,
Evolutionary computation,
Reliability theory"
Real-time multi-stereo depth estimation on GPU with approximative discontinuity handling,,
Adaptive Blind Separation with an Unknown Number of Sources,"The blind source separation (BSS) problem with an unknown number of sources is an important practical issue that is usually skipped by assuming that the source number n is known and equal to the number m of sensors. This letter studies the general BSS problem satisfying m ≥ n. First, it is shown that the mutual information of outputs of the separation network is a cost function for BSS, provided that the mixing matrix is of full column rank and the m×m separating matrix is nonsingular. The mutual information reaches its local minima at the separation points, where the m outputs consist of n desired source signals and m−n redundant signals. Second, it is proved that the natural gradient algorithm proposed primarily for complete BSS (m n) can be generalized to deal with the overdetermined BSS problem (m>n), but it would diverge inevitably due to lack of a stationary point. To overcome this shortcoming, we present a modified algorithm, which can perform BSS steadily and provide the desired source signals at specified channels if some matrix is designed properly. Finally, the validity of the proposed algorithm is confirmed by computer simulations on artificially synthesized data.",
A two-phase genetic and set partitioning approach for the vehicle routing problem with time windows,"The vehicle routing problem with time windows (VRPTW) is a well-known and complex combinatorial problem, which has received considerable attention in recent years. Results from exact methods have been improved exploring parallel implementations and modern branch-and-cut techniques. However, 23 out of the 56 high order instances from Solomon's test set still remain unsolved. Additionally, in many cases a prohibitive time is needed to find the exact solution. Many efficient heuristic methods have been developed to make possible a good solution in a reasonable amount of time. Using travel distance as the main objective, this paper proposes a robust heuristic approach for the VRPTW using an efficient genetic algorithm and a set partitioning formulation. The tests were produced using both real numbers and truncated data type, making it possible to compare the results with previous heuristic and exact methods published. Furthermore, computational results show that the proposed heuristic approach outperforms all previous known heuristic methods in the literature, in terms of the minimal travel distance.","Vehicles,
Routing,
Testing,
Computer science,
Genetic algorithms,
Logistics,
Robustness,
Operations research,
Costs,
Transportation"
Support vector classifiers and network intrusion detection,"Within network security, there is the task of intrusion detection. Intrusion detection is a classification task that attempts to discern if a given request for network service is an intrusion attempt or a safe request. Since the creation of the 1999 KDD Cup network intrusion data set, several machine learning approaches to this task have been found to be successful. In this work we propose using the successful support vector machine (SVM) learning approach to classify network requests. We use computational experiments to explore two factors that influence SVM performance in this task and demonstrate two novel approaches to this task.","Intrusion detection,
Support vector machines,
Support vector machine classification,
Machine learning,
Data security,
Testing,
Milling machines,
Educational institutions,
IP networks,
Computer science"
Tooth contour extraction for matching dental radiographs,"In dental biometrics, the contours of teeth in dental radiographs are utilized for human identification. However, in many images, the contours of teeth are fuzzy and only partially visible. To extract the contours from these images, we propose a method based on the active contour models. A new dynamic energy is proposed for the directional snake to discriminate boundaries of adjacent teeth. Matching result shows the contours extracted with this method perform better than traditional methods.","Teeth,
Dentistry,
Diagnostic radiography,
Biometrics,
Humans,
Active contours,
Computer science,
Image databases,
Spatial databases,
Image edge detection"
Bayesian color estimation for adaptive vision-based robot localization,"In this article we introduce a hierarchical Bayesian model to estimate a set of colors with a mobile robot. Estimating colors is particularly important if objects in an environment can only be distinguished by their color. Since the appearance of colors can change due to variations in the lighting condition, a robot needs to adapt its color model to such changes. We propose a two level Gaussian model in which the lighting conditions are estimated at the upper level using a switching Kalman filter. A hierarchical Bayesian technique learns Gaussian priors from data collected in other environments. Furthermore, since estimation of the color model depends on knowledge of the robot's location, we employ a Rao-Blackwellised particle filter to maintain a joint posterior over robot positions and lighting conditions. We evaluate the technique in the context of the RoboCup AIBO league, where a legged AIBO robot has to localize itself in an environment similar to a soccer field. Our experiments show that the robot can localize under different lighting conditions and adapt to changes in the lighting condition, for example, due to a light being turned on or off.","Bayesian methods,
Robot localization,
Robot sensing systems,
Robot vision systems,
State estimation,
Mobile robots,
Cameras,
Computer science,
Particle filters,
Gaussian distribution"
A study of various composite kernels for kernel eigenvoice speaker adaptation,"Eigenvoice-based methods have been shown to be effective for fast speaker adaptation when the amount of adaptation data is small, say, less than 10 seconds. In traditional eigenvoice (EV) speaker adaptation, linear principal component analysis (PCA) is used to derive the eigenvoices. Recently, we proposed that eigenvoices found by nonlinear kernel PCA could be more effective, and the eigenvoices thus derived were called kernel eigenvoices (KEV). One of our novelties is the use of composite kernel that makes it possible to compute state observation likelihoods via kernel functions. We investigate two different composite kernels: direct sum kernel and tensor product kernel for KEV adaptation. In an evaluation on the TIDIGITS task, it is found that KEV speaker adaptations using either form of composite kernel are equally effective, and they outperform a speaker-independent model and the adapted models from EV, MAP, or MLLR adaptation using 2.1s and 4.1s of speech. For example, with 2.1s of adaptation data, KEV adaptation outperforms the speaker-independent model by 27.5%, whereas EV, MAP, and MLLR adaptations are not effective at all.","Kernel,
Principal component analysis,
Maximum likelihood linear regression,
Tensile stress,
Error analysis,
Speech recognition,
Computer science,
Speech analysis,
Bayesian methods,
Face recognition"
Toward construction of a mobile system with long-range RFID sensors,"A simple mobile system is described for humans and robots with a radio-frequency identification (RFID) tag sensor. In this paper, the system assumes to be used in a hospital to identify the location of patients and recognize their state. At first we use one positive RFID sensor. The range of this directional antenna is long (about 5 meters, 2 meters) and an RIFD antenna is set on a rotating pan-tilt platform we made, making the best use of the directional sensor. It is difficult to recognize a person with an ID tag because the microwave output from the antenna attenuates by human's moisture. However, we confirm the effectiveness of this RFID system against the people who have a tag by experiments. Experimental results show that this system is available for identifying a person. At last we were able to make the extended system, using three RFID sensors without the rotating platform","Radiofrequency identification,
Sensor systems,
Microwave antennas,
Hospitals,
Moisture,
Radio frequency,
Computer science,
Systems engineering and theory,
Humans,
Robot sensing systems"
Randomized directed neighborhoods with edge migration in particle swarm optimization,"A key feature of particle swarm optimization algorithms is that fitness information shared with individuals in a particle's neighborhood. The kind of neighborhood structure that is used affects the rate at which information is disseminated throughout the population. Existing work has studied global and simple local topologies, as well as more complex, but fixed neighborhood structures. This paper looks at randomly generated, directed graph structures in which information flows in one direction only, and also outgoing edges randomly migrate from one source node to another. Experimental evidence indicates that this random dynamic topology, when used with an inertia weight PSO, performs competitively with some existing methods and outperforms others.","Particle swarm optimization,
Topology,
Mathematics,
Computer science,
Algorithm design and analysis,
Design optimization,
Insects,
Convergence,
Lattices"
SVM-based salient region(s) extraction method for image retrieval,"In region-based image retrieval, not all the regions are important for retrieving similar images and rather, the user is often interested in performing a query on only salient regions. Therefore, we propose a new method for extraction of salient regions using support vector machines (SVM) and a method for importance score learning according to the user's interaction. Once an image is segmented, our algorithm permits the attention window (AW) according to the variation of an image and selects salient regions by using the pre-defined feature vector and SVM within the AW. By using SVM, we do not need to determine the heuristic feature parameters and produce more reasonable results. The distance values from SVM are used for initial importance scores of salient regions and our proposed updating algorithm using relevance feedback updates them automatically. Through performance comparison with parametric salient extraction method, our proposed method shows better performance as well as semantic query interface for object-level image retrieval.","Image retrieval,
Support vector machines,
Image segmentation,
Feedback,
Content based retrieval,
Computer science,
Machine learning,
Shape,
Humans,
Object segmentation"
Recognizing wrist pulse waveforms with improved dynamic time warping algorithm,"This paper proposes an improved DTW algorithm to recognize pulse waveform. Our approach avoids the pathologic alignment that can be caused by original DTW algorithm and DDTW algorithm. Extensive experiments on 1000 pulse waveforms demonstrate the effect of our approach. Compared with original DTW and DDTW algorithms, our algorithm reduces the FRR and FAR greatly. By calculating the similarities between the pulse sample and the five pulse templates, our approach has 92.3% agreement rate with expert.","Wrist,
Heuristic algorithms,
Pulse shaping methods,
Shape,
Dynamic programming,
Computer science,
Pathology,
Physiology,
Psychology,
Space vector pulse width modulation"
Nonlocal evolutionary adaptation in gridplants,"A simulated model of plant growth and evolution was studied. Plants start out as seeds on a 2D grid. Plant genomes are modeled as instructions telling a plant where to grow and where to place seeds. Energy is gained by occupying grid space in analogy to collection of light by leaf surface area. At the end of a generation, cells currently occupied by plants are cleared and the seeds dropped by all the plants sprout to become the new plants. Each seed produced has a probability of mutation to the genome it contains. The simulated plants evolve to play a game of competitive exclusion, in which grid space is a limited resource. This work tested the hypothesis that the evolved plants would display nonlocal adaptation, i.e. that the plants would not only adapt to their local environment, but would acquire general skill that would enable them to grow competitively against plants that were never a part of their environment. Statistical tests show that populations of plants that have evolved for a larger number of generations are able to occupy more grid space when played against populations of plants evolved for a shorter time. This occurs even if the two competing populations come from entirely different lineages. This improvement in competitive ability continues over the course of the evolution performed in this study, without appearing to reach an equilibrium after which further evolution fails to improve the plants. This suggests that the plants are continually discovering generally useful strategies, rather than adapting only to their local environment.","Organisms,
Evolution (biology),
Biological system modeling,
Genomics,
Bioinformatics,
Testing,
Computer science,
Computational biology,
Computational modeling,
Computer simulation"
Comparing the structure of power-law graphs and the Internet AS graph,"In this work we devise algorithmic techniques to compare the interconnection structure of the Internet AS graph with that of graphs produced by topology generators that match the power-law degree distribution of the AS graph. We are guided by the existing notion that nodes in the AS graph can be placed in tiers with the resulting graph having an hierarchical structure. Our techniques are based on identifying graph nodes at each tier, decomposing the graph by removing such nodes and their incident edges, and thus explicitly revealing the interconnection structure of the graph. We define quantitative metrics to analyze and compare the decomposition of synthetic power-law graphs with the Internet-AS graph. Through experiments, we observe qualitative similarities in the decomposition structure of the different families of power-law graphs and explain any quantitative differences based on their generative models. We believe our approach provides insight into the interconnection structure of the AS graph and finds continuing applications in evaluating the representativeness of synthetic topology generators.",
A modified approach to fuzzy Q learning for mobile robots,A modified approach to fuzzy Q-learning is presented in this paper. A reward sharing mechanism is added to increase the learning speed and to allow treatment of each fuzzy rule as a separate learning node. A new method of exploration is also proposed to increase the learning performance. Two basic robot behaviours which are a goal-seeking and an obstacle avoidance behaviour are simulated to show the promise of the proposed techniques. The goal-seeking behaviour is implemented on a real robot. The experimental results show that this method is practical for a real-world problem.,"Mobile robots,
Learning,
Fuzzy systems,
Orbital robotics,
Hybrid intelligent systems,
Input variables,
Fuzzy logic,
Robot sensing systems,
Error correction,
Computer science"
One-Bit-Matching Conjecture for Independent Component Analysis,"The one-bit-matching conjecture for independent component analysis (ICA) could be understood from different perspectives but is basically stated as “all the sources can be separated as long as there is a one-toone same-sign-correspondence between the kurtosis signs of all source probability density functions (pdf's) and the kurtosis signs of all model pdf's” (Xu, Cheung, & Amari, 1998a). This conjecture has been widely believed in the ICA community and implicitly supported by many ICA studies, such as the Extended Infomax (Lee, Girolami, & Sejnowski, 1999) and the soft switching algorithm (Welling & Weber, 2001). However, there is no mathematical proof to confirm the conjecture theoretically. In this article, only skewness and kurtosis are considered, and such a mathematical proof is given under the assumption that the skewness of the model densities vanishes. Moreover, empirical experiments are demonstrated on the robustness of the conjecture as the vanishing skewness assumption breaks. As a by-product, we also show that the kurtosis maximization criterion (Moreau & Macchi, 1996) is actually a special case of the minimum mutual information criterion for ICA.",
Deception detection under varying electronic media and warning conditions,"With an increasing amount of business-related tasks and decisions being supported by communication technology, it is important to understand and explore the vulnerabilities that may result from its use. One of these weaknesses is the transmission of deceptive information. Very little research investigating mediated deception and its detection exists, however. This paper reports the results of one such investigation. An experiment was conducted involving an interactive interview of deceitful applicants for a fictitious scholarship, using one of three different computer-based media. Results showed that people were extremely successful at deceiving others no matter what medium was used, and the only recipients of lies that had a realistic chance at uncovering lies were those who were warned beforehand. There were no differences in the number of false alarms issued by warned and unwarned receivers. Warned receivers also rated their electronic medium poorly. Possible implications of this study are offered.","Business communication,
Organizational aspects,
Humans,
Computer mediated communication,
Educational institutions,
Communications technology,
Scholarships,
Information technology,
Decision making,
Costs"
Interactive multimodal user interfaces for mobile devices,"Portable devices come with different limitations in user interaction like limited display size, small keyboard, and different sorts of input and output capabilities. With the advance of speech recognition and speech synthesis technologies, their complementary use becomes attractive for mobile devices in order to implement real multimodal user interaction. However, current systems and formats do not sufficiently integrate advanced multimodal interactions. We introduce an advanced generic multimodal interaction and rendering system (MIRS) dedicated for mobile devices. MIRS incorporates efficient processing of XML specification languages for limited, mobile devices and comes with the XML-based dialog and interface specification language (DISL). DISL can be considered as an UIML subset, which is enhanced by the means of state-oriented dialog specifications. The dialog specification is based on ODSN (object oriented dialog specification notation), which has been introduced to define user interface control by means of interaction states with transition rules.","User interfaces,
Speech recognition,
Streaming media,
Specification languages,
Keyboards,
Speech synthesis,
Rendering (computer graphics),
Ambient intelligence,
Humans,
Displays"
Reliable message delivery for mobile agents: push or pull?,"Two of the fundamental issues in designing protocols for message passing between mobile agents (MAs) are tracking the migration of the target agent and forwarding messages to it. Even with an ideal fault-free network-transport mechanism, messages can be dropped during MA migration. Therefore, in order to provide reliable message delivery, protocols need to overcome message loss caused by asynchronous operations of agent migration and message forwarding. In this paper, two known message forwarding approaches, namely push and pull, are explored to design adaptive and reliable message delivery protocols. Based on a commonly used MA tracking model, the pros and cons of these two approaches are evaluated, both qualitatively and quantitatively. The comparative performance evaluation is presented in terms of network traffic and delay in message processing. We also propose improvements to the pull approach to reduce network traffic and the message delay. We conclude that with different message passing and migration patterns and varying requirements of real-time message processing, specific applications can select different message delivery approaches to achieve the desired level of performance and flexibility.","Mobile agents,
Laboratories,
Protocols,
Message passing,
Mobile computing,
Electronic mail,
Computer science,
Target tracking,
Telecommunication traffic,
Traffic control"
An adaptive learning approach for noisy data streams,"Two critical challenges typically associated with mining data streams are concept drift and data contamination. To address these challenges, we seek learning techniques and models that are robust to noise and can adapt to changes in timely fashion. We approach the stream-mining problem using a statistical estimation framework, and propose a fast and robust discriminative model for learning noisy data streams. We build an ensemble of classifiers to achieve timely adaptation by weighting classifiers in a way that maximizes the likelihood of the data. We further employ robust statistical techniques to alleviate the problem of noise sensitivity. Experimental results on both synthetic and real-life data sets demonstrate the effectiveness of this model learning approach.","Data mining,
Noise robustness,
Working environment noise,
Voting,
Bagging,
Computer science,
Contamination,
Telecommunication traffic,
Traffic control,
Monitoring"
Multilevel hierarchical mobility management scheme in complicated structured networks,"Micromobility management is a key issue for achieving enhanced mobile Internet services. Many micromobility management schemes supporting fast handover, e.g., Hierarchical Mobile IPv6 (HMIPv6), have been proposed to minimize the packet loss during handover and the handover latency. We have previously proposed a multilevel hierarchical mobility management scheme that enhances HMIPv6 mobility management. Our scheme manages the micromobility of a mobile terminal (MT) using a mobility management router (the mobility anchor point, or MAP), with a management domain matched to the MT's mobility. This usage of MAP aims to efficiently decentralize the load of mobility management. In our previous work, we have confirmed the effectiveness of our scheme in a network having a simple tree-based hierarchy. The actual networks, however, have some local variation and/or some redundancy in structure. In such a complicated structured network, the MAP domains intricately overlap each other. This causes difficulty in selecting the MAP suitable for managing the MT mobility because the suitable MAP for an MT changes according to the complicated local conditions. This paper proposes a method to allow an MT to select the suitable MAP for the MT in such a network by adjusting the selection criteria individually configured for use at a particular place. Finally, the performance of our scheme is evaluated using simulation experiments. The simulation results show that our scheme works well in complicated structured networks by using the proposed additional method.","Mobile radio mobility management,
Intelligent networks,
Web and internet services,
Delay,
Streaming media,
Multimedia systems,
Information science,
Electronic mail,
Technology management,
Mobile communication"
Resource constrained and speculative scheduling of an algorithm class with run-time dependent conditionals,"We present a significant extension of the quantified equation based algorithm class of piecewise regular algorithms. The main contributions of the following paper are: the class of piecewise regular algorithms are extended by allowing run-time dependent conditionals; a mixed integer linear program is given to derive optimal schedules of the novel class we call dynamic piecewise regular algorithms; and in order to achieve highest performance, we present a speculative scheduling approach. The results are applied to an illustrative example.","Scheduling algorithm,
Runtime,
Optimal scheduling,
Processor scheduling,
Dynamic scheduling,
Parallel processing,
Communication system control,
Difference equations,
Computer science,
Heuristic algorithms"
Robustness of class-based path-vector systems,"Griffin, Jaggard, and Ramachandran [2004] introduced a framework for studying design principles for path-vector protocols, such as the border gateway protocol (BGP) used for inter-domain routing in the Internet. They outlined how their framework could describe hierarchical-BGP-like systems in which routing at a node is determined by the relationship with the next-hop node on a path (e.g., an ISP-peering relationship) and some additional scoping rules (e.g., the use of backup routes). The robustness of these class-based path-vector systems depends on the presence of a global constraint on the system, but an adequate constraint has not yet been given in general. In This work, we give the best-known sufficient constraint that guarantees robust convergence. We show how to generate this constraint from the design specification of the path-vector system. We also give centralized and distributed algorithms to enforce this constraint, discuss applications of these algorithms, and compare them to algorithms given in previous work on path-vector protocol design.","Robustness,
Routing protocols,
Internet,
Distributed algorithms,
Algorithm design and analysis,
Peer to peer computing,
Mathematics,
Computer science,
Convergence,
IP networks"
Neural network fusion strategies for identifying breast masses,"In this work, we introduce the perceptron average neural network fusion strategy and implemented a number of other fusion strategies to identify breast masses in mammograms as malignant or benign with both balanced and imbalanced input features. We numerically compare various fixed and trained fusion rules, i.e., the majority vote, simple average, weighted average, and perceptron average, when applying them to a binary statistical pattern recognition problem. To judge from the experimental results, the weighted average approach outperforms the other fusion strategies with balanced input features, while the perceptron average is superior and achieves the goals with lowest standard deviation with imbalanced ensembles. We concretely analyze the results of above fusion strategies, state the advantages of fusing the component networks, and provide our particular broad sense perspective about information fusion in neural networks.",
VLSI based analog power system emulator for fast contingency analysis,"This paper is concerned with the development of a fast, programmable, and reconfigurable power system emulator using an analog/mixed-signal VLSI microchip. The proposed microchip is capable of emulating behaviors of large power system networks under various conditions with faster than real-time computation time that is independent of the size of the power system network. The proposed model focuses on static security analysis, where the main objective is to access the existing operating state of the system. If the state is found to be secure, contingency analysis is performed to evaluate system vulnerability and time necessary to obtain such results. This time is compared to traditional computational techniques to evaluate static security of a power system. This paper discusses the design methodology of the proposed microchip and describes a smaller scale prototype of the analog emulator that is currently being developed on printed circuit boards (PCBs).","Very large scale integration,
Power system analysis computing,
Power system security,
Power system modeling,
Real time systems,
Computer networks,
Performance analysis,
Performance evaluation,
Design methodology,
Prototypes"
Stand-alone laboratory sessions in sensors and signal processing,"Real laboratory experiments can help students to gain a better understanding of theoretical problems. Great efforts are needed, however, to improve the quality of laboratory sessions, and a large number of tutors are usually required. Moreover, students may lose time because of hardware failure or an inefficient experimental setup. Several educational tools (based on virtual instruments) have been developed allowing for optimized time scheduling and remote access to laboratory sessions. Drawbacks related to hardware failure have not, however, been seriously addressed. This paper proposes an educational tool made up of a user-friendly interface controlling experimental boards. It basically consists of an array of optical sensing devices connected to suitable conditioning circuits, which are interfaced to a virtual instrument by means of a data acquisition system. To solve the previously mentioned drawbacks, a solution based on both pre-cabled hardware and PC-based measurement stations has been adopted. Moreover, the ability to configure self-educational tasks optimizes time scheduling for students during laboratory activities. The proposed system allows students to improve their knowledge in the field of optical sensing devices, virtual instrumentation, data acquisition systems, and signal processing. The paper describes an application of the tool as a simple system for surface recognition. This application is one of the laboratory tasks performed in measurement classes during this engineering course at the University of Catania, Catania, Italy.","Student experiments,
Computer aided instruction,
Optical transducers,
User modeling,
Data acquisition,
Electronics engineering education,
Computer science education,
Signal processing"
Optimization-intensive watermarking techniques for decision problems,"Recently, a number of watermarking-based intellectual property protection techniques have been proposed. Although they have been applied to different stages in the design process and have a great variety of technical and theoretical features, all of them share two common properties: 1) they are applied solely to optimization problems and 2) do not involve any optimization during the watermarking process. In this paper, we propose the first set of optimization-intensive watermarking techniques for decision problems. In particular, we demonstrate, by example of the Boolean satisfiability (SAT) problem, how one can select a subset of superimposed watermarking constraints so that the uniqueness of the signature and the likelihood of satisfying the satisfiability problem are simultaneously maximized. We have developed three SAT watermarking techniques: adding clauses, deleting literals, and push-out and pull-back. Each technique targets different types of signature-induced constraint superimposition on an instance of SAT problem. In addition to comprehensive experimental validation, we theoretically analyze the potentials and limitations of the proposed watermarking techniques. Furthermore, we analyze the three proposed optimization-intensive watermarking SAT techniques in terms of their suitability for copy detection.","Watermarking,
Protection,
Design automation,
Computer science,
Intellectual property,
Design optimization,
Logic design,
Electronic design automation and methodology,
Application specific integrated circuits,
Law"
Fuzzy logic and multiobjective evolutionary algorithms as soft computing tools for persistent query learning in text retrieval environments,"Persistent queries are a specific kind of queries used in information retrieval systems to represent a user's long-term standing information need. These queries can present many different structures, being the ""bag of words"" that most commonly used. They can be sometimes formulated by the user, although this task is usually difficult for him and the persistent query is then automatically derived from a set of sample documents he provides. In this work we aim at getting persistent queries with a more representative structure for text retrieval issues. To do so, we make use of soft computing tools: fuzzy logic is considered for representation and inference purposes by dealing with the extended Boolean query structure, and multiobjective evolutionary algorithms are applied to build the persistent fuzzy query. Experimental results show how both an expressive fuzzy logic-based query structure and a proper learning process to derive it are needed in order to get a good retrieval efficacy, when comparing our process to single-objective evolutionary methods to derive both classic Boolean and extended Boolean queries.","Fuzzy logic,
Evolutionary computation,
Information retrieval,
Database languages,
Computer science,
Inference algorithms,
Information filtering,
Information filters,
Routing,
Context-aware services"
Relocation using laser and vision,"We present a method for solving the first location problem using 2D laser and vision. Our observation is a two-dimensional laser scan together with its corresponding image. The observation is segmented into textured vertical planes; each vertical plane contains geometrical information about its location given by the laser scan, plus the gray level image obtained by the camera. The rich plane texture allows a safe plane recognition. Once two planes are recognized as correspondent, the computer vision geometry allows to compute the relative camera motion. The proposed algorithm outperforms both laser-only and vision-only algorithms. This is shown in the experimental results where a map composed of 8 observations of a 20/spl times/3 meter corridor is used to successfully locate the robot (without any other prior) in 163 out of 192 initial test robot locations.","Vehicles,
Cameras,
Simultaneous localization and mapping,
Laser theory,
Image segmentation,
Computer vision,
Photometry,
Computer science,
Geometrical optics,
Computational geometry"
A new VLSI model of neural microcircuits including spike time dependent plasticity,"This work presents a new VLSI model for biological neural systems, a unified research tool for neuro- as well as computer science. It allows construction of neural microcircuits close to the biological specimen while maintaining a speed several orders faster than real time. The synapse model includes an implementation of spike time dependent plasticity (STDP). Therefore, the VLSI system allows the investigation of key aspects of plasticity without a speed penalty. Additionally, this system is a research tool for new concepts of information processing like liquid or any-time computing. The analog, continuous-time operation of the neuron is implemented in a contemporary deep-submicron process technology. Thereby it realizes a powerful computing system that is not based on the Turing paradigm.","Very large scale integration,
Neural microtechnology,
Power system modeling,
Biological system modeling,
Neurons,
Computational modeling,
Brain modeling,
Neuroscience,
Information processing,
Testing"
Flossiping: a new routing protocol for wireless sensor networks,"In this paper we present a new routing protocol, flossiping, for wireless sensor networks. It can be regarded as an enhancement to existing flooding and gossiping approach by using a single branch gossiping with low-probability random selective relaying in order to achieve a better overall performance. By letting each sensor node decide its own activity in the routing procedure, we implement a simple zero-overhead resource-aware routing protocol. It has the advantages of flexibility in delay-power tradeoff, affordability in terms of resource consumptions, and reliability in terms of packet lost-free. Simulation results show that a range of optimal operating points considering delay and power consumption tradeoff can be found with user-customized cost functions.","Routing protocols,
Wireless sensor networks,
Delay,
Sensor phenomena and characterization,
Floods,
Energy consumption,
Relays,
Broadcasting,
Laboratories,
Computer science"
Ensuring content and intention consistency in real-time group editors,"Real-time group editors allow distributed users to work on local replicas of a shared document simultaneously to achieve high responsiveness and free interaction. Operational transformation (OT) is the standard method for consistency maintenance in state-of-the-art group editors. It is potentially able to achieve content consistency (convergence) as well as intention consistency (so that the converged content is what the users intend), while traditional concurrency control methods such as locking and serialization often cannot. However, existing OT algorithms are often not able to really guarantee consistency due to important algorithmic flaws that have been there for fourteen years. We present a novel state difference based transformation (SDT) algorithm to solve the problem. Our result also reveals that the standard priority schemes to break ties in distributed systems should be used with more caution.","Delay,
Concurrency control,
Collaborative software,
Collaborative work,
Protocols,
Computer science,
Convergence,
Collaboration,
Computer networks,
IP networks"
Workspace importance sampling for probabilistic roadmap planning,"Probabilistic roadmap (PRM) planners have been successful in path planning of robots with many degrees of freedom, but they behave poorly when a robot's configuration space contains narrow passages. This paper presents workspace importance sampling (WIS), a new sampling strategy for PRM planning. Our main idea is to use geometric information from a robot's workspace as ""importance"" values to guide sampling in the corresponding configuration space. By doing so, WIS increases the sampling density in narrow passages and decreases the sampling density in wide-open regions. We tested the new planner on rigid-body and articulated robots in 2-D and 3-D environments. Experimental results show that WIS improves the planner's performance for path planning problems with narrow passages.","Monte Carlo methods,
Sampling methods,
Orbital robotics,
Path planning,
Testing,
Robots,
Computer science,
Application software,
Design automation,
Computer graphics"
"Constructing a balanced, (log(N)/1oglog(N))-diameter super-peer topology for scalable P2P systems","Current peer-to-peer (P2P) file sharing applications are remarkably simple and robust, but their inefficiency can produce very high network loads. The use of super-peers has been proposed to improve the performance of unstructured P2P systems. These have the potential to approach the performance and scalability of structured systems, while retaining the benefits of unstructured P2P systems. There has, however, been little consensus on the best topology for connecting these super-peers, or how to construct the topology in a distributed, robust way. In this paper we propose a scalable unstructured P2P system (SUPS). The unique aspect of SUPS is a protocol for the distributed construction of a super-peer topology that has highly desirable performance characteristics. The protocol is inspired by the theory of random graphs. We describe the protocol, and demonstrate experimentally that it produces a balanced and low-diameter super-peer topology at low cost. We show that the method is very robust to super-peer failures and inconsistent information, and compare it with other approaches.","Peer to peer computing,
Robustness,
Scalability,
Network topology,
Protocols,
Internet,
Computer science,
Telecommunication traffic,
Routing,
Broadcasting"
Towards autonomous service composition in a grid environment,"Web services are becoming important in applications from electronic commerce to application interoperation. While numerous efforts have focused on service composition, service selection among similar services from multiple providers has not been addressed. Such issue is more serious when services are embraced in Grid platforms, which are usually resource-conscious. Experimental results show that our considerations are valid and our preliminary solution works well in our Globus grid network.","Web services,
Costs,
Computer science,
Application software,
Logic programming,
Electronic commerce,
Grid computing,
Simple object access protocol,
Runtime,
Resource management"
DSP for practicing engineers: a case study in Internet course delivery,"With the explosion of the Internet and the desire of many institutions to disseminate courses across the world, many students look to online education with promise. However, institutions planning to provide distance learning opportunities may wish to have a model to analyze before venturing forth. There are many factors that ultimately influence the methods of delivery, content, length, and technical support for an online course. This paper presents the design, implementation, and evaluation of an online course titled DSP for Practicing Engineers at the Georgia Institute of Technology, Atlanta, and analyzes feedback from students who have taken the course and the staff who administered it. Different aspects of course development are discussed at length, including curriculum and media type.","Computer aided instruction,
Signal processing,
Internet,
Electrical engineering education,
Computer science education"
Evolving digital circuits using multi expression programming,Multi expression programming (MEP) is a genetic programming (GP) variant that uses linear chromosomes for solution encoding. A unique MEP feature is its ability of encoding multiple solutions of a problem in a single chromosome. These solutions are handled in the same time complexity as other techniques that encode a single solution in a chromosome. In this paper MEP is used for evolving digital circuits.,"Digital circuits,
Biological cells,
Genetic programming,
Input variables,
Encoding,
Computer science,
Linear programming,
Wires,
Joining processes,
Humans"
Application-layer mobility support for streaming real-time media,"This paper presents an UDP-based socket extension called the resilient mobile socket (RMS), which provides application-layer mobility support by encapsulating other sockets into a new aggregated socket abstraction. Encapsulated sockets can then be added or removed without disturbing running applications. RMS also provides a method for soft handovers where several encapsulated sockets are used simultaneously during a handover. As a proof of concept, a working prototype has been built by integrating RMS with Marratech Pro, a commercially available e-meeting application. This prototype has been used to evaluate RMS and to investigate how GSM audio quality is affected by handovers. The result from the investigation shows that soft handovers can be executed without loosing packets or causing extra latency, while a hard handover in average took around 200 ms to complete. This indicates that proactive handovers and redundancy are important, but that more work must he done to predict disconnections.","Streaming media,
Sockets,
Prototypes,
Delay,
Computer science,
Mobile computing,
GSM,
Land mobile radio cellular systems,
Internet telephony,
Mobile communication"
Self-encoded spread spectrum and Turbo coding,"Self-encoded multiple access (SEMA) is a unique realization of random spread spectrum. As the term implies, the spreading code is obtained from the random digital information source instead of the traditional pseudo noise (PN) code generators. The time-varying random codes can provide additional security in wireless communications. Multirate transmissions or multi-level grade of services are also easily implementable in SEMA. In this paper, we analyze the performance of SEMA in additive white Gaussian noise (AWGN) channels and Rayleigh fading channels. Differential encoding eliminates the BER effect of error propagations due to receiver detection errors. The performance of SEMA approaches the random spread spectrum discussed in literature at high signal to noise ratios. For performance improvement, we employ multiuser detection and Turbo coding. We consider a downlink synchronous system such as base station to mobile communication though the analysis can be extended to uplink communications.","Bit error rate,
Receivers,
Turbo codes,
Registers,
Lead,
Generators"
Island multicast: the combination of IP multicast with application-level multicast,"The Internet nowadays consists of multicast-capable domains or ""islands"" interconnected by multicast-incapable routers. In order to achieve efficient global multicast, we propose and study island multicast (IM) where overlay connections are used between islands while IP multicast is used within an island. IM may use any existing application-level multicast protocol to build island overlay. We describe how to elect a representative (or leader) in each island for such a purpose. We also present the mechanisms for electing the bridging nodes for overlay connections. Using Internet-like topologies, we show that IM achieves much higher bandwidth efficiency as compared to using application-level multicast alone, at the cost of a small increase in end-to-end delay.","Bridges,
Internet,
Multicast protocols,
Bandwidth,
Delay,
Network topology,
Costs,
Unicast,
Councils,
Computer science"
An integrated mobility management scheme in IPv6 based wireless networks,"Although session initiation protocol (SIP) is considered to be the mobility support protocol for next generation multimedia communications, it still has several difficulties in completely substituting conventional Mobile IP. When both Mobile IPv6 and SIP are adopted for a mobile node in an IPv6 based network, there is severe signaling overhead due to a redundant registration procedure. Therefore, we propose a new mobility management scheme that integrates the signaling procedure of Mobile IPv6 and SIP. Performance comparisons are made between our integration method and the previous method in which separate registrations for the Mobile IPv6 and SIP are performed without integration. Numerical results show that the proposed method efficiently reduces the delay time related to location registration and handoff.","Mobile radio mobility management,
Intelligent networks,
Wireless networks,
TCPIP,
Wireless application protocol,
Quality of service,
Computer science,
Next generation networking,
Multimedia communication,
Mobile communication"
Extending Self-Organizing Particle Systems to Problem Solving,"Self-organizing particle systems consist of numerous autonomous, purely reflexive agents (“particles”) whose collective movements through space are determined primarily by local influences they exert upon one another. Inspired by biological phenomena (bird flocking, fish schooling, etc.), particle systems have been used not only for biological modeling, but also increasingly for applications requiring the simulation of collective movements such as computer-generated animation. In this research, we take some first steps in extending particle systems so that they not only move collectively, but also solve simple problems. This is done by giving the individual particles (agents) a rudimentary intelligence in the form of a very limited memory and a top-down, goal-directed control mechanism that, triggered by appropriate conditions, switches them between different behavioral states and thus different movement dynamics. Such enhanced particle systems are shown to be able to function effectively in performing simulated search-and-collect tasks. Further, computational experiments show that collectively moving agent teams are more effective than similar but independently moving ones in carrying out such tasks, and that agent teams of either type that split off members of the collective to protect previously acquired resources are most effective. This work shows that the reflexive agents of contemporary particle systems can readily be extended to support goal-directed problem solving while retaining their collective movement behaviors. These results may prove useful not only for future modeling of animal behavior, but also in computer animation, coordinated movement control in robotic teams, particle swarm optimization, and computer games.",
A connectionless approach to mobile ad hoc networks,"Today's communication techniques for mobile ad hoc networks take a connection-oriented approach. Mobile nodes need to discover routes and establish a connection before they can communicate. This strategy is not robust as it cannot adapt to frequent unpredictable topology changes due to high mobility. Constant reconnections incur significant overhead making these schemes unsuitable for applications such as voice and video. To address these issues, we explore a connectionless paradigm in this paper. We leverage technology such as GPS (global positioning system) to allow a source node to discover a grid path to the destination node. Data packets are relayed along this path toward the destination using different intermediate nodes at different times without having to first establish connections between them. The performance of this new solution is essentially unaffected by node mobility. As a result, it is suitable for a wide range of mobile applications.","Mobile ad hoc networks,
Mobile communication,
Global Positioning System,
Wireless networks,
Routing,
Computer science,
Robustness,
Relays,
Ad hoc networks,
Spread spectrum communication"
Medical image retrieval and registration: towards computer assisted diagnostic approach,"A large number of medical images in digital format are generated by hospitals and clinics every day. Such images constitute an important source of anatomical and functional information for diagnosis of diseases, medical research and education. These images include high-resolution 2-D or 3-D spatial and temporal data of various modalities (CT, MRI, X-ray, ultrasound etc.) require effective processing and organization techniques. Two of the most important techniques along this direction are; image retrieval and image registration. Image retrieval is the technique to find similar images from an image archive by their textual and visual contents and image registration is the establishment of correspondences between images or between image and physical space. Characteristics of medical images differ significantly from other general-purpose images and hence require special treatment. This article presents a highlight of recent research in medical image retrieval and registration, main techniques and trends, key issues and limitations. We also point out some promising research directions and propose a system architecture for computer aided diagnosis, where both technologies can be integrated and complement each other.","Biomedical imaging,
Medical diagnostic imaging,
Image retrieval,
Image registration,
Hospitals,
Diseases,
Computer science education,
Computed tomography,
Magnetic resonance imaging,
X-ray imaging"
A partial-range-aware localization algorithm for ad-hoc wireless sensor networks,"Current wireless sensor network localization algorithms are categorized either as range-free or range-aware algorithms based on whether they use the range (i.e., distance) information. Although wireless sensor systems usually have available received signal strength (RSS) readings, this useful information has not been effectively used in the existing localization algorithms. The existing range-free algorithms do not use this information, and the range-aware algorithms require sophisticated range techniques to estimate ranges. In this paper, we introduce and develop a partial-range-aware localization scheme utilizing RSS readings. The scheme can be implemented in a distributed way and can be used in any hop-based range-free algorithms to improve their localization accuracy. Through extensive simulation, the results show that the partial-range-aware algorithm can improve the localization accuracy by reducing position errors up to 50% of the previous range-free localization results.","Wireless sensor networks,
Hardware,
Quantization,
Ultrasonic variables measurement,
Computer science,
Sensor systems,
Measurement errors,
Distance measurement,
RF signals,
Energy measurement"
Verifiable concurrent programming using concurrency controllers,"We present a framework for verifiable concurrent programming in Java based on a design pattern for concurrency controllers. Using this pattern, a programmer can write concurrency controller classes defining a synchronization policy by specifying a set of guarded commands and without using any of the error-prone synchronization primitives of Java. We present a modular verification approach that exploits the modularity of the proposed pattern, i.e., decoupling of the controller behavior from the threads that use the controller. To verify the controller behavior (behavior verification) we use symbolic and infinite state model checking techniques, which enable verification of controllers with parameterized constants, unbounded variables and arbitrary number of user threads. To verify that the threads use a controller in the specified manner (interface verification) we use explicit state model checking techniques, which allow verification of arbitrary thread implementations without any restrictions. We show that the correctness of the user threads can be verified using the concurrency controller interfaces as stubs, which improves the efficiency of the interface verification significantly. We also show that the concurrency controllers can be automatically optimized using the specific notification pattern. We demonstrate the effectiveness of our approach on a Concurrent Editor implementation which consists of 2800 lines of Java code with remote procedure calls and complex synchronization constraints.","Concurrent computing,
Automatic control,
Yarn,
Java,
Programming profession,
Computer science,
Error correction,
Condition monitoring,
Concurrency control,
Safety"
Optimizing the integration of a statistical language model in HMM based offline handwritten text recognition,"Although handwritten text recognition has been studied for some years, only few authors have used statistical language models to increase the performance of their recognizers. In those few cases where a language model has been used, its integration has not been systematically optimized. We investigate the optimization of the integration of statistical language models into HMM based recognition systems for offline handwritten text. Based on experiments with the IAM database we show that the recognition performance of a general offline handwritten text recognizer can be substantially improved.","Hidden Markov models,
Text recognition,
Handwriting recognition,
Decoding,
Computer science,
Databases,
Feature extraction,
Probability,
Natural languages,
Speech recognition"
Addressing mobility in wireless sensor media access protocol,"Handling mobility in wireless sensor networks presents several new challenges. Techniques developed for other mobile networks, such as mobile phone or mobile ad hoc networks are not applicable, as in these networks energy is not a very critical resource. The paper presents a new adaptive mobility-aware sensor MAC protocol (MS-MAC) for mobile sensor applications. In MS-MAC protocol, a node detects its neighbor's mobility based on a change in received signal level from the neighbor, or a loss of connection with this neighbor after a timeout period. By propagating mobility presence information, and distance from nearest border node, each node learns its relative distance from the nearest mobile node and from nearest border node. Depending on the mobile node movement direction and the distances from mobile and border nodes, a node may trigger its neighbor search mechanism to quicken the connection setup time. Simulation results show that the new mobility-aware MAC protocol can work very energy-efficiently when the network is stationary, whereas it performs much better in terms of throughput than the existing sensor MAC (SMAC) protocol in scenarios involving mobile sensors.","Wireless sensor networks,
Media Access Protocol,
Access protocols,
Quality of service,
Batteries,
Throughput,
Biosensors,
Wearable sensors,
Intelligent networks,
Computer science"
HybridTreeMiner: an efficient algorithm for mining frequent rooted trees and free trees using canonical forms,"Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. In this paper, we present HybridTreeMiner, a computationally efficient algorithm that discovers all frequently occurring subtrees in a database of rooted unordered trees. The algorithm mines frequent subtrees by traversing an enumeration tree that systematically enumerates all subtrees. The enumeration tree is defined based on a novel canonical form for rooted unordered trees - the breadth-first canonical form (BFCF). By extending the definitions of our canonical form and enumeration tree to free trees, our algorithm can efficiently handle databases of free trees as well. We study the performance of our algorithms through extensive experiments based on both synthetic data and datasets from real applications. The experiments show that our algorithm is competitive in comparison to known rooted tree mining algorithms and is faster by one to two orders of magnitudes compared to a known algorithm for mining frequent free trees.","Tree graphs,
Databases,
XML,
Computer networks,
Application software,
Computer science,
Tree data structures,
Computational biology,
Pattern recognition,
Biology computing"
A new multi-path selection scheme for video streaming on overlay networks,"This paper presents a new multi-path selection scheme for video streaming on overlay networks. Our scheme uses an overlay network architecture that makes minimal assumptions about the knowledge of the underlying network. We first propose a new QoS metric link correlation and a path correlation model for multi-path selection problem. After discussing the tractability of minimal correlation multi-path selection problem, we present an efficient algorithm called correlation cost routing to select multi-path in overlay networks. The simulation results show that the average peak signal-to-noise ratio (PSNR) of the transmitted multiple descriptions coding (MDC) video using our multi-path selection algorithm improves by up to 3.2 dB over maximally link-disjoint multi-path selection method. Furthermore our new algorithm is more efficient than previous methods since it shares the same complexity with Dijsktra algorithm.","Streaming media,
Routing,
Network servers,
Costs,
PSNR,
Web server,
Web and internet services,
IP networks,
Computer science,
Laboratories"
Selecting better EEG channels for classification of mental tasks,"In this work a new method is proposed to reduce the number of EEG channels needed to classify mental tasks. By applying genetic algorithm to the search space consisting of 6 channel combinations of 19 EEG channels the more salient combinations of them in classification of three mental tasks are selected. This algorithm reduces the calculation time and the final results are verified by our observations. Obtained results bring forward the concept of systematic and intelligent selection criteria for choosing superior EEG channels of subjects for mental task classification. This may find applications in the field of brain computer interfaces which are based on classifications of mental tasks, by reducing the number of EEG channels.","Electroencephalography,
Genetic algorithms,
Backpropagation algorithms,
Signal processing algorithms,
Computer science,
Application software,
Brain computer interfaces,
Data mining,
Biological neural networks,
Feature extraction"
Boundary based corner detection and localization using new 'cornerity' index: a robust approach,"In this paper, a novel boundary based corner detection algorithm is proposed. The proposed algorithm is computationally fast and efficient. The proposed method computes an expected point for every point on a boundary curve. An expected point corresponding to a point on the boundary curve is defined to be the geometrical centroid of the symmetrical boundary segment of size 2k+1, for some integer k>0, within the neighborhood of the point in consideration. A new 'cornerity' index for a point on the boundary curve is defined to be the distance between the point and its corresponding expected point. The larger the cornerity index, the stronger is the evidence that the boundary point is a corner point. A set of rules is worked out to guide the process of locating true corner points. The conducted experiments establish that the proposed approach is invariant to image transformations viz., rotation, translation and scaling.",
Autonomy mode suggestions for improving human-robot interaction,"Robot systems can have autonomy levels ranging from fully autonomous to teleoperated. Some systems have more than one autonomy mode that an operator can select. In studies, we have found that operators rarely change autonomy modes, even when it would improve their performance. This paper describes a method for suggesting autonomy mode changes on a robot designed for an urban search and rescue application.","Robot sensing systems,
Cognitive robotics,
Orbital robotics,
Switches,
Navigation,
Computer science,
Cognition,
Humans,
Sonar detection,
Collaboration"
Designs of high quality streaming proxy systems,"Researchers often use segment-based proxy caching strategies to deliver streaming media by partially caching media objects. The existing strategies mainly consider increasing the byte hit ratio and/or reducing the client perceived startup latency (denoted by the metric delayed startup ratio). However, these efforts do not guarantee continuous media delivery because the to-be-viewed object segments may not be cached in the proxy when they are demanded. The potential consequence is playback jitter at the client side due to proxy delay in fetching the uncached segments, which we call proxy jitter. Thus, for the best interests of clients, a correct model for streaming proxy system design should aim to minimize proxy jitter subject to reducing the delayed startup ratio and increasing the byte hit ratio. However, we have observed two major pairs of conflicting interests inherent in this model: (1) one between improving the byte hit ratio and reducing proxy jitter, and (2) the other between improving the byte hit ratio and reducing the delayed startup ratio. In this study, we first propose an active prefetching method for in-time prefetching of uncached segments, which provides insights into the first pair of conflicting interests. Second, we further improve our lazy-segmentation scheme which effectively addresses the second pair of the conflicting interests. Finally, considering our main objective of minimizing proxy jitter and optimizing the two trade-offs, we propose a new streaming proxy system called Hyper Proxy by effectively coordinating both prefetching and segmentation techniques. Synthetic and real workloads are used to systematically evaluate our system. The performance results show that the hyper proxy system generates minimum proxy jitter with a low delayed startup ratio and a small decrease of byte hit ratio compared with existing schemes","Delay,
Streaming media,
Jitter,
Prefetching,
Computer science,
Educational institutions,
Mobile computing,
Laboratories,
Internet,
Web server"
GRIDCOLE: a Grid Collaborative Learning Environment,"Research in the computer supported collaborative learning (CSCL) field is currently tackling three important problems which are closely related: difficult integration of CSCL tools, scarce software reuse and technification. This work presents a Grid Collaborative Learning Environment (GRIDCOLE) that combines IMS learning design (IMS-LD) and Open Grid Services Architecture (OGSA) technologies in order to address these issues. More specifically, GRIDCOLE allows users with no technical skills easy integration and use of applications that effectively support collaborative learning processes. IMS-LD is employed to formally describe the design of these collaborative learning processes while applications are built using tools provided by third-party suppliers as OGSA-compliant Grid services. With this aim, the GRIDCOLE architecture and its main functionalities are introduced. A simple prototype is also presented in order to show the feasibility of the most important concepts introduced in this paper. Furthermore, a real collaborative learning scenario in which we plan to validate GRIDCOLE is introduced.","Collaborative work,
Application software,
Collaborative tools,
Computer architecture,
Cognitive science,
Telecommunication computing,
Educational technology,
Communications technology,
Education,
Computational modeling"
Program slicing in the presence of database state,"Program slicing has long been recognised as a valuable technique for supporting the software maintenance process. However, many programs operate over some kind of external state, as well as the internal program state. Arguably, the most significant form of external state is that used to store data associated with the application, for example, in a database management system. We propose an approach to supporting slicing over both program and database state, which requires the introduction of two new forms of data dependency into the standard program dependency graph. Our method expands the usefulness of program slicing techniques to the considerable number of database application programs that are being maintained within industry and science today.","Computer science,
Software maintenance,
Programming profession,
Application software,
Database systems,
Filters,
Data structures,
Information retrieval,
Bridges,
Software standards"
Maximization of time-to-first-failure for multicasting in wireless networks: optimal solution,"We consider the problem of maximizing the time-to-first-failure (TTFF), defined as the time till the first node in the network runs out of battery energy, in energy constrained broadcast wireless networks. We show that the TTFF criterion, by itself, fails to provide the ""ideally optimum"" multicast tree and propose a composite weighted objective function which maximizes the TTFF and minimizes the sum of transmitter powers. We then develop a mixed integer linear programming (MILP) model for solving the joint optimization problem optimally. We also consider the case of prioritized nodes and show how the model can be modified to deal with such priorities.","Intelligent networks,
Wireless networks,
Broadcasting,
Batteries,
Transmitters,
Mixed integer linear programming,
Unicast,
Computer science,
Propulsion,
Laboratories"
"An infrastructure for scalable, reliable semantic portals","This article extends and implements the basic conceptual framework SEAL (semantic portal) with the aim of using semantic technologies in real-world scenarios through a reliable, scalable infrastructure. We also show how multiple semantic portals can interact with one another to exchange knowledge, thus fostering reuse. The building blocks of our extension include knowledge integration, processing, representation, organization, and access. The authors harness the access facilities of semantic Web technologies to provide an infrastructure suitable for industrial applications.",
Detection and visualization of anomalous structures in molecular dynamics simulation data,"We explore techniques to detect and visualize features in data from molecular dynamics (MD) simulations. Although the techniques proposed are general, we focus on silicon (Si) atomic systems. The first set of methods use 3D location of atoms. Defects are detected and categorized using local operators and statistical modeling. Our second set of exploratory techniques employ electron density data. This data is visualized to glean the defects. We describe techniques to automatically detect the salient isovalues for isosurface extraction and designing transfer functions. We compare and contrast the results obtained from both sources of data. Essentially, we find that the methods of defect (feature) detection are at least as robust as those based on the exploration of electron density for Si systems.","Data visualization,
Data analysis,
Data engineering,
Lattices,
Computer science,
Electrons,
Silicon,
Fluid dynamics,
Computational modeling,
Physics"
A framework for constraint-based development and autonomic management of distributed applications,"We propose a framework for the deployment and subsequent autonomic management of component-based distributed applications. An initial deployment goal is specified using a declarative constraint language, expressing constraints over aspects such as component-host mappings and component interconnection topology. A constraint solver is used to find a configuration that satisfies the goal, and the configuration is deployed automatically. The deployed application is instrumented to allow subsequent autonomic management. If, during execution, the manager detects that the original goal is no longer being met, the satisfy/deploy process can be repeated automatically in order to generate a revised deployment that does meet the goal.","Probes,
Application software,
Topology,
Engines,
LAN interconnection,
Instruments,
Humans,
Computer science,
Face detection,
Automatic control"
"Integrating ontology and workflow in PROTEUS, a grid-based problem solving environment for bioinformatics","Bioinformatics is as a bridge between life science and computer science: computer algorithms are needed to face complexity of biological processes. Bioinformatics applications manage complex biological data stored into distributed and often heterogeneous databases and require large computing power. We discuss requirements of such applications and present the architecture of PROTEUS, a grid-based problem solving environment that integrates ontology and workflow approaches to enhance composition and execution of bioinformatics applications on the grid.","Ontologies,
Problem-solving,
Bioinformatics,
Biology computing,
Application software,
Bridges,
Computer science,
Biological processes,
Energy management,
Distributed databases"
GAME: a Generic Automated Marking Environment for programming assessment,"In this paper, a Generic Automated Marking Environment (GAME) is proposed for assessing student programming projects and exercises with an aim to facilitate student-centred learning. GAME has been designed to automatically assess programming assignments written in a variety of languages. The system has been implemented in Java and contains marker modules that are tailored to each specific language. A framework has been set in place to enable easy addition of new marker modules to extend the system's functionality. Currently, the system is able to mark programs written in Java and the C language. To use the system, instructors are required to provide a simple ""marking schema"" for any given assessment item, which includes pertinent information such as the location of files and the model solution. GAME has been tested on a number of student programming exercises and assignments providing encouraging results.","Automatic programming,
Information technology,
Java,
Feedback,
Gold,
Postal services,
Australia,
Electronic mail,
Testing,
Computer science"
OGSI.NET: OGSI-compliance on the .NET framework,"The Open Grid Service Infrastructure (OGSI) has been designed to facilitate the creation of multiple, interoperable Grid service hosting environments, but to date only one fully OGSI-compliant hosting environment exists, Globus Toolkit version 3 (GT3). This paper describes the design and implementation of OGSI.NET, which is the second, independent hosting environment that is fully compliant with the OGSI specification. While the Microsoft suite of Visual Studio, Internet Information Services (IIS), and the Web Services Enhancements (WSE) provide a robust foundation upon which we implemented OGSI.NET, the challenges included constructing the Grid service container, parsing OGSI WSDL (""gwsdl""), and supporting Grid service handles (GSH). By describing our approach to these challenges, and by describing the unique Grid attribute programming. model we are developing as part of OGSI.NET, we contribute an early evaluation of OGSI and the broader Open Grid Services Architecture (OGSA).","Web services,
Grid computing,
Computer architecture,
Context-aware services,
Computer science,
Web and internet services,
Robustness,
Containers,
Ear,
Service oriented architecture"
Probabilistic temporal interval networks,"A probabilistic temporal interval network is a constraint satisfaction problem where the nodes are temporal intervals and the edges are uncertain interval relations. We attach a probability to each of Allen's basic interval relations. An uncertain relation between two temporal intervals is represented as a disjunction of Allen's probabilistic basic relations. Using the operations of inversion, composition, and addition, defined for this probabilistic representation, we present a path consistency algorithm.","Constraint theory,
Heuristic algorithms,
Possibility theory,
Computer science,
Optimization methods,
Computer networks,
Writing,
Uncertainty,
Algebra,
Algorithm design and analysis"
Transparent self-optimization in existing CORBA applications,"This paper addresses the design of adaptive middleware to support autonomic computing in pervasive computing environments. The particular problem we address here is how to support self-optimization to changing network connection capabilities as a mobile user interacts with heterogeneous elements in a wireless network infrastructure. The goal is to enable self-optimization to such changes transparently with respect to the core application code. We propose a solution based on the use of the generic proxy, which is a specific CORBA object that can intercept and process any CORBA request using rules and actions that can be introduced to the knowledge base of the proxy during execution. To explore its design and operation, we have incorporated the generic proxy into ACT (Sadjadi and McKinley, 2004), an adaptive middleware framework we designed previously to support adaptation in CORBA applications. Details of the generic proxy are presented. A case study is described in which a generic proxy is used to support self-optimization in an existing image retrieval application, when executed in a heterogeneous wireless environment.","Middleware,
Communication system security,
Computer networks,
Pervasive computing,
Wireless networks,
Military computing,
Intelligent networks,
Application software,
Software engineering,
Computer science"
Consistent enterprise software system architecture for the CIO - a utility-cost based approach $,"Previously, business operations of most large companies were supported by a number of isolated software systems performing diverse specific tasks, from real-time process control to administrative functions. In order to better achieve business goals, these systems have in recent years been extended, and more importantly, integrated into a company-wide system in its own right, the enterprise software system. Due to its history, this system is composed of a considerable number of heterogeneous and poorly understood components interacting by means of equally diverse and confusing connectors. To enable informed decision-making, the Chief Information Officer (CIO), responsible for the overall evolution of the company's enterprise software system, requires management tools. This paper proposes enterprise software system architecture (ESSA) as a foundation for an approach for managing the company's software system portfolio. In order to manage the overwhelming information amounts associated with the enterprise software system, this approach is based on two concepts. Firstly, the approach explicitly relates the utility of knowledge to the cost of its acquisition. The utility of knowledge is derived from the increased value of better-informed decision-making. The cost of knowledge acquisition is primarily related to the resources spent on information searching. Secondly, the approach focuses on ensuring the consistency of the architectural model.","Software systems,
Computer architecture,
Decision making,
Costs,
Companies,
Real time systems,
Process control,
History,
Connectors,
Portfolios"
Comprehensive logfiles for autonomic systems,"Summary form only given. A proposal for a new generation of logfiles with regard to new challenges posed by autonomic computing is presented. While a variety of techniques are being developed on the way to autonomic computing, the problem of a system's logfiles remains to be critical. Several recommendations for logfile design are introduced: (a) Event type and source of events have to be distinguishably incorporated into the design, (b) Logfiles should incorporate hierarchical numbering schemes, (c) Information contained in logfiles should be categorized into classes, (d) Logfiles have to easily lend themselves to automatic analysis, (e) Information density of logfiles should be considered in the design of logging functionality. A metric to measure information entropy of logfiles is proposed. The type of information to be included in logfiles in order to support various aspects of autonomic computing as originally defined by IBM's eight elements is specified.","Information entropy,
Humans,
Computer science,
Proposals,
Information analysis,
Information resources,
Frequency measurement,
Guidelines,
Large-scale systems,
Distributed processing"
A power-aware and QoS-aware service model on wireless networks,"Many studies show that the wireless network interface (WNI) accounts for a significant part of the power consumed by mobile terminals. Thus, putting the WNI into sleep when it is idle is an effective technique to save power. To support streaming applications, existing techniques cannot put the WNI into sleep due to strict delay requirements. We present a novel power-aware and QoS-aware service model over wireless networks. In the proposed model, mobile terminals use proxies to buffer data so that the WNIs can sleep for a long time period. To achieve power-aware communication while satisfying the delay requirement of each flow, a scheduling scheme, called priority-based bulk scheduling (PBS), is designed to decide which flow should be served at which time. Through analysis, we prove that the PBS service model can provide delay assurance and achieve power efficiency. We use audio-on-demand and Web access as case studies to evaluate the performance of the PBS service model. Experimental results show that PBS achieves excellent QoS provision for each flow and significantly reduces the power consumption.","Wireless networks,
Energy consumption,
Delay,
Streaming media,
Pervasive computing,
Personal digital assistants,
Batteries,
Protocols,
Computer science,
Power engineering and energy"
A laptop servo for control education,"A novel portable laptop control experiment is presented, which does not require the space and expense typically required for undergraduate control laboratories. Interactive experiments are designed to cover all of the topics required for an introductory course in controls, as well as a haptics experiment. It is believe that the system is useful for practicing engineers who wish to update their control knowledge and skills.","Portable computers,
Servomechanisms,
Control systems,
DC motors,
Microcontrollers,
Computer science education,
Libraries,
Industrial control,
Power engineering and energy,
Systems engineering and theory"
How good is your blind spot sampling policy,"Assessing software costs money and better assessment costs exponentially more money. Given finite budgets, assessment resources are typically skewed towards areas that are believed to be mission critical. This leaves blind spots: portions of the system that may contain defects which may be missed. Therefore, in addition to rigorously assessing mission critical areas, a parallel activity should sample the blind spots. This paper assesses defect detectors based on static code measures as a blind spot sampling method. In contrast to previous results, we find that such defect detectors yield results that are stable across many applications. Further, these detectors are inexpensive to use and can be tuned to the specifics of the current business situations.","Sampling methods,
Costs,
Mission critical systems,
Detectors,
Project management,
NASA,
Aerospace engineering,
Systems engineering and theory,
Proposals,
Computer science"
Efficient TCP connection failover in Web server clusters,"Web clusters continue to be widely used by large enterprises and organizations to host online services. Providing services without interruption is critical to the revenue and perceived image of both hosts and content providers. Therefore, server node failure and recovery should be invisible to the clients. Most of the existing fault-tolerance schemes simply stop dispatching future client requests to the failed server. They do not recover those connections handled by the node at the time of failure, which makes the failure visible to some clients. Making the failure transparent requires both application-layer and transport-layer mechanisms. While atomic application-layer primary-backup failover schemes have been addressed at length in previous literature, a transport-layer scheme is necessary in order to make them invisible to the clients. We describe a transparent TCP connection failover mechanism. Besides transparency, our solution is also highly efficient, and does not need any dedicated hardware support.","Web server,
Network servers,
Dispatching,
Hardware,
Costs,
Computer crashes,
Computer science,
Fault tolerance,
Condition monitoring,
Internet"
The consistency of task-based authorization constraints in workflow,"Workflow management systems (WFMSs) have attracted a lot of interest both in academia and the business community. A workflow consists of a collection of tasks that are organized to facilitate some business process specification. To simplify the complexity of security administration, it is common to use role-based access control (RBAC) to grant authorization to roles and users. Typically, security policies are expressed as constraints on users, roles, tasks and the workflow itself. A workflow system can become very complex and involve several organizations or different units of an organization, thus the number of security policies may be very large and their interactions very complex. It is clearly important to know whether the existence of such constraints will prevent certain instances of the workflow from completing. Unfortunately, no existing constraint models have considered this problem satisfactorily. In this paper, we define a model for constrained workflow systems that includes local and global cardinality constraints, separation of duty constraints and binding of duty constraints. We define the notion of a workflow specification and of a constrained workflow authorization schema. Our main result is to establish necessary and sufficient conditions for the set of constraints that ensure a sound constrained workflow authorization schema, that is, for any user or any role who are authorized to a task, there is at least one complete workflow instance when this user or this role executes this task.","Authorization,
Workflow management software,
Access control,
Computer science,
Information security,
Business process re-engineering,
Sufficient conditions,
Automation,
History,
Permission"
Learning classifiers from imbalanced data based on biased minimax probability machine,"We consider the problem of the binary classification on imbalanced data, in which nearly all the instances are labelled as one class, while far fewer instances are labelled as the other class, usually the more important class. Traditional machine learning methods seeking an accurate performance over a full range of instances are not suitable to deal with this problem, since they tend to classify all the data into the majority, usually the less important class. Moreover, some current methods have tried to utilize some intermediate factors, e.g., the distribution of the training set, the decision thresholds or the cost matrices, to influence the bias of the classification. However, it remains uncertain whether these methods can improve the performance in a systematic way. In this paper, we propose a novel model named biased minimax probability machine. Different from previous methods, this model directly controls the worst-case real accuracy of classification of the future data to build up biased classifier;. Hence, it provides a rigorous treatment on imbalanced data. The experimental results on the novel model comparing with those of three competitive methods, i.e., the naive Bayesian classifier, the k-nearest neighbor method, and the decision tree method C4.5, demonstrate the superiority of our novel model.","Machine learning,
Minimax techniques,
Costs,
Sampling methods,
Classification tree analysis,
Educational institutions,
Computer science,
Data engineering,
Learning systems,
Bayesian methods"
Providing data transfer with QoS as agreement-based service,"Over the last decade, grids have become a successful tool for providing distributed environments for secure and coordinated execution of applications. The successful deployment of many realistic applications in such environments on a large scale has motivated their use in experimental science [L. C. Pearlman et al., (2004), K. Keahey et al. (2004)] where grid-based computations are used to assist in ongoing experiments. In such scenarios, quality of service (QoS) guarantees on execution as well as data transfer is desirable. The recently proposed WS-Agreement model [K. Czajkowski et al. K. Keahey et al. (2004)] provides an infrastructure within which such quality of service can be negotiated and obtained. We have designed and implemented a data transfer service that exposes an interface based on this model and defines agreements which guarantee that, within a certain confidence level, file transfer can be completed under a specified time. The data transfer service accepts a client's request for data transfer and makes an agreement with the client based on QoS metrics (such as the transfer time and confidence level with which the service can be provided). In our approach we use prediction as a base for formulating an agreement with the client, and we combine prediction and rate limiting to adoptively ensure that the agreement is met.","Quality of service,
Protocols,
Bandwidth,
Computer science,
Application software,
Internet,
Telecommunication traffic,
Mathematics,
Large-scale systems,
Grid computing"
Design strategies to improve performance of GIS Web services,"GIS systems are ubiquitous distributed systems, since geo-spatial information adheres to almost everything. Considering the characteristics of GIS, the following four design-decision issues are particularly crucial: transactional mode (synchronous versus asynchronous), service granularity (finegrained versus coarse-grained), delivery manner (chunk versus stream), and transmission formats (GML versus binary). We have shared our experience in making choices in these four dimensions.","Geographic Information Systems,
Web services,
Simple object access protocol,
Web and internet services,
Computer industry,
Maintenance engineering,
Computer science,
Laboratories,
Concurrent computing,
Data communication"
Capturing outlines using cubic Bezier curves,"Capturing outlines is a useful method for shape compression and digitization for computer storage and for subsequent computational efficiency. This work presents an optimal cubic Bezier curve design, which is particularly useful for capturing outlines of 2D shapes including hand-drawn shapes and fonts. The methodology, discussed in this paper differs from traditional approaches in that, it is not aimed to find characteristic points, which are interpolated to get approximated curve. The proposed method is used to find suitable location of control points for any curve such that a simple Bezier curve generated over these control points approximates the original curve.","Shape control,
Approximation error,
Computer science,
Petroleum,
Minerals,
Information technology,
Image coding,
Computational efficiency,
Image converters,
Equations"
Design of phase-controlled class E inverter with asymmetric circuit configuration,"A design of phase-controlled class E inverter with asymmetric circuit configuration is presented. By using the presented design method, it is possible to derive the design values, which let the inverters achieve zero-voltage switching continuously in the control range, are derived. By carrying out circuit experiments, it is verified that the experimental results agree with numerical predictions quantitatively, and the validity of the presented design procedure is denoted.","Inverters,
Zero voltage switching,
Design methodology,
Power generation,
Voltage control,
Frequency,
Electromagnetic interference,
Computer science,
Circuit topology,
Equations"
Biclustering in gene expression data by tendency,"The advent of DNA microarray technologies has revolutionized the experimental study of gene expression. Clustering is the most popular approach of analyzing gene expression data and has indeed proven to be successful in many applications. Our work focuses on discovering a subset of genes which exhibit similar expression patterns along a subset of conditions in the gene expression matrix. Specifically, we are looking for the order preserving clusters (OP-cluster), in each of which a subset of genes induce a similar linear ordering along a subset of conditions. The pioneering work of the OPSM model, which enforces the strict order shared by the genes in a cluster, is included in our model as a special case. Our model is more robust than OPSM because similarly expressed conditions are allowed to form order equivalent groups and no restriction is placed on the order within a group. Guided by our model, we design and implement a deterministic algorithm, namely OPC-tree, to discover OP-clusters. Experimental study on two real datasets demonstrates the effectiveness of the algorithm in the application of tissue classification and cell cycle identification. In addition, a large percentage of OP-clusters exhibit significant enrichment of one or more function categories, which implies that OP-clusters indeed carry significant biological relevance.","Gene expression,
Clustering algorithms,
Computer science,
DNA,
Data analysis,
Algorithm design and analysis,
Application software,
Robustness,
Self organizing feature maps,
Graphics"
Model-independent method for fMRI analysis,"This paper presents a fast method for delineation of activated areas of the brain from functional magnetic resonance imaging (fMRI) time series data. The steps of the work accomplished are as follows. 1) It is shown that the detection performance evaluated by the area under the receiver operating characteristic curve is directly related to the signal-to-noise ratio (SNR) of the composite image generated in the detection process. 2) Detection and segmentation of activated areas are formulated in a vector space framework. In this formulation, a linear transformation (image combination method) is shown to be desirable to maximize the SNR of the activated areas subject to the constraint of removing inactive areas. 3) An analytical solution for the problem is found. 4) Image pixel vectors and expected time series pattern (signature) for inactive pixels are used to calculate weighting vector and identify activated regions. 5) Signatures of the activated regions are used to segment different activities. 6) Segmented images by the proposed method are compared with those generated by the conventional methods (correlation, t-statistic, and z statistic). Detection performance and SNRs of the images are compared. The proposed approach outperforms the conventional methods of fMRI analysis. In addition, it is model-independent and does not require a priori knowledge of the fMRI response to the paradigm. Since the method is linear and most of the work is done analytically, numerical implementation and execution of the method are much faster than the conventional methods.",
Fitting subdivision surfaces to unorganized point data using SDM,"We study the reconstruction of smooth surfaces from point clouds. We use a new squared distance error term in optimization to fit a subdivision surface to a set of unorganized points, which defines a closed target surface of arbitrary topology. The resulting method is based on the framework of squared distance minimization (SDM) proposed by Pottmann et al. Specifically, with an initial subdivision surface having a coarse control mesh as input, we adjust the control points by optimizing an objective function through iterative minimization of a quadratic approximant of the squared distance function of the target shape. Our experiments show that the new method (SDM) converges much faster than the commonly used optimization method using the point distance error function, which is known to have only linear convergence. This observation is further supported by our recent result that SDM can be derived from the Newton method with necessary modifications to make the Hessian positive definite and the fact that the Newton method has quadratic convergence.","Surface fitting,
Newton method,
Surface reconstruction,
Optimization methods,
Minimization methods,
Shape control,
Least squares methods,
Spline,
Computer science,
Computer errors"
Classification of Hebrew calligraphic handwriting styles: preliminary results,"We present preliminary results for document classification of ancient Hebrew manuscripts. The main goal is to analyze documents of different writing styles in order to identify the locations, the dates, and the writer of the test documents. This analysis depends crucially on good binarization of the corrupted manuscripts. We propose an accurate method for binarization of the manuscripts. We further propose and test topological features for handwriting style classification based a selected subset of the Hebrew alphabet. In our preliminary experiments we have used only two characters, the character Aleph and the character Lamed. Our results so far yield 100% correct classification of a set of fourteen documents written by fourteen different writers.","Text analysis,
Image segmentation,
Image analysis,
Feature extraction,
Libraries,
Testing,
Handwriting recognition,
Graphics,
Character recognition,
Computer science"
Learning hierarchical models of activity,"This paper investigates learning hierarchical statistical activity models in indoor environments. The abstract hidden Markov model (AHMM) is used to represent behaviors in stochastic environments. We train the model using both labeled and unlabeled data and estimate the parameters using expectation maximization (EM). Results are shown on three datasets: data collected in lab, entryway, and home environments. The results show that hierarchical models outperform flat models.","Hidden Markov models,
Stochastic processes,
Indoor environments,
Parameter estimation,
Robots,
Humans,
Orbital robotics,
Floors,
Computer science,
Hospitals"
Analyzing connectivity-based multi-hop ad-hoc positioning,"We investigate the theoretical limits of positioning algorithms. In particular, we study scenarios where the nodes do not receive anchors directly (multi-hop) and where no physical distance or angle information whatsoever is available (connectivity-based). Since we envision large-scale sensor networks as an application, we are interested in fast, distributed algorithms. As such, we show that plain hop algorithms are not competitive. Instead, for one-dimensional unit disk graphs we present an optimal algorithm HS. For two or more dimensions, we propose an algorithm GHoST which improves upon the basic hop algorithm in theory and in simulations.",
A complete solution for iBGP stability,"BGP routers within an Autonomous System (AS) exchange their inter-AS routing information via the internal Border Gateway Protocol (iBGP). Within an AS, every BGP router needs to maintain an iBGP peering session with every border BGP router. This peering scheme fails to scale due to the large number of iBGP peering sessions required. Current solutions to this scalability limitation divide the AS into clusters, with a route reflector acting us a representative of the cluster. Clustering, however, introduces routing anomalies. Furthermore, BGP also uses a Multi-Exit Discriminator (MED) to differentiate multiple links connecting the same pair of AS'ms. However, clustering in combination with the use of MED values further aggravates routing anomalies. In this paper, we propose a complete solution that solves both clustering induced anomalies and MED induced anomalies in iBGP. Our solution requires multiple path disseminations between route reflectors and selective single-path dissemination from each route reflector to its client.",
Reinforcement learning of coordination in heterogeneous cooperative multi-agent systems,,"Multiagent systems,
Convergence,
Permission,
Intelligent networks,
Computer science,
Learning systems,
Control systems,
System testing,
Broadcasting"
Population-based incremental interactive concept learning for image retrieval by stochastic string segmentations,"We propose a method for concept-based medical image retrieval that is a superset of existing semantic-based image retrieval methods. We conceive of a concept as an incremental and interactive formalization of the user's conception of an object in an image. The premise is that such a concept is closely related to a user's specific preferences and subjectivity and, thus, allows to deal with the complexity and content-dependency of medical image content. We describe an object in terms of multiple continuous boundary features and represent an object concept by the stochastic characteristics of an object population. A population-based incrementally learning technique, in combination with relevance feedback, is then used for concept customization. The user determines the speed and direction of concept customization using a single parameter that defines the degree of exploration and exploitation of the search space. Images are retrieved from a database in a limited number of steps based upon the customized concept. To demonstrate our method we have performed concept-based image retrieval on a database of 292 digitized X-ray images of cervical vertebrae with a variety of abnormalities. The results show that our method produces precise and accurate results when doing a direct search. In an open-ended search our method efficiently and effectively explores the search space.","Image retrieval,
Stochastic processes,
Image segmentation,
Biomedical imaging,
Information retrieval,
Image databases,
Space exploration,
Feedback,
X-ray imaging,
Spine"
Pursuit-evasion in an unknown environment using gap navigation trees,"In this paper we present an online algorithm for pursuit-evasion in a unknown simply connected environment, for one pursuer that has minimal sensing and carries a set of stationary sentries that it can drop off and pick up during the pursuit. In our sensing model, the pursuer is only able to detect discontinuities in depth information (gaps), and it is able to find all of the evaders without any explicit localization or geometric information, by using a gap navigation tree. The strategy is based on growing an evader-free region, by reading ""exploration"" schedules from the gap navigation tree, that is constructed online. We prove that a pursuer with k + 1 sentries can clear any environment that could be cleared by k pursuers using the algorithm in L.J. Guibas et al. (1999), which required a complete map and perfect sensing.","Navigation,
Robot sensing systems,
Mobile robots,
Computer science,
Floors,
Game theory,
Automata,
Feedback control,
Data structures,
Tree graphs"
RDMA control support for fine-grain parallel computations,"We concern parallel computations with communication based on remote direct memory access (RDMA), which provides low level un-buffered access to distributed memory of computational nodes. Fine grain computations involve very frequent transmissions of small messages. For their efficient execution with RDMA communication a special memory infrastructure - rotating buffers (RB) - is introduced. Their organization is adjusted to program needs in advance - before program execution. It allows an intensive use of all communication resources available in the system based on additional synchronization between involved processes. The proposed method is illustrated by an example of a typical fine-grain problem which is the discrete fast Fourier Transform (FFT). ""The transpose algorithm"" of FFT has been implemented with the RDMA rotating buffers and its efficiency is compared with a solution based on standard message passing library MPI.","Concurrent computing,
Communication system control,
Distributed computing,
Libraries,
Workstations,
Automatic control,
Message passing,
Delay,
Information technology,
Computer science"
An introduction to computing system dependability,"It is important that computer engineers, software engineers, project managers, and users understand the major elements of current technology in the field of dependability, yet this material tends to be unfamiliar to researchers and practitioners alike. Researchers are often concerned in one way or another with some aspect of what is mistakenly called software ""reliability"". All practitioners are concerned with the ""reliability"" of the software that they produce but researchers and practitioners tend not to understand fully the broader impact of their work. A lot of research, such as that on testing, is concerned directly with software dependability. Understanding dependability more fully allows researchers to be more effective. Similarly, practitioners can direct their efforts during development more effectively if they have a better understanding of dependability.",
Interacting with sensor networks,"We develop distributed algorithms for sensor networks that respond by directing a target (robot or human) through a region. The sensor network models the event levels sensed across a geographical area, adapts to changes, and guides a moving object incrementally across the network. We describe a device we call a flashlight for interacting with the sensor field. This interaction includes collecting navigation information from the sensors in the local neighborhood, activating and deactivating specified areas of the sensor network, and detecting events in the sensor network. We report on hardware experiments using a physical sensor network consisting of Mote sensors.","Intelligent sensors,
Humans,
Navigation,
Sensor systems,
Chemical sensors,
Protocols,
Computer science,
Event detection,
Fires,
Mobile communication"
New perspectives on teaching and learning software systems development in large groups,"This paper outlines perspectives on the teaching and learning of software systems development in large groups based on the designed, final year, software systems analysis (SSA) and software systems design (SSD) subjects within the undergraduate Software Engineering Program at the Faculty of Engineering, University of Technology Sydney (UTS). Topics described by this paper include shared experiences in implementing these subjects over the last two years and discusses their successes, as well as common problems both experienced and anticipated. While the SSA and SSD subjects are practice based, its content is the synthesis of all subjects taught in the Software Engineering Program and practiced in industrial internships. The object of these two subjects is to bridge the gap between the students' individualistic, often unplanned, unbudgeted and uncoordinated efforts, that dominate the traditional approach to teaching and learning software subjects; and the demands of planned, process driven, well coordinated and budgeted - modern, large team based development of complex software systems, demanded by industry. The subjects are evolutionary in nature, but the most critical aspect and challenge is to maintain a careful balance of academic and pedagogical interests with ever changing industry demands and practices.","Education,
Software systems,
Software design,
Software engineering,
Project management,
Object oriented modeling,
Software maintenance,
Australia,
Process design,
Design engineering"
Adaptive rate-distortion optimization using perceptual hints,"The paper proposes a novel video coding approach that performs adaptive rate-distortion optimization guided by perceptual hints. The key idea is to adjust adaptively the Lagrange multipliers of the RDO coder control module based on visual attention analysis. The observation is that human vision is sensitive to movement of well-structured objects while tolerating large distortion in moving areas with random structure (texture-wise and motion-wise). The proposed algorithm analyzes permissible perceptual distortions and, accordingly, assigns larger Lagrange multipliers to regions that are perceptually less sensitive to distortion so that rate reduction is weighted more than distortion reduction in these regions. Experiments show that this scheme achieves bit-rate saving of 5-7% with virtually the same perceptual quality and is very promising for practical systems","Rate-distortion,
Distortion measurement,
Quadratic programming,
Signal analysis,
Performance analysis,
Humans,
Computer science,
Lagrangian functions,
Psychology,
Video codecs"
Extensible Markov model,"A Markov chain is a popular data modeling tool. This paper presents a variation of Markov chain, namely extensible Markov model (EMM). By providing a dynamically adjustable structure, EMM overcomes the problems caused by the static nature of the traditional Markov chain. Therefore, EMMs are particularly well suited to model spatiotemporal data such as network traffic, environmental data, weather data, and automobile traffic. Performance studies using EMMs for spatiotemporal prediction problems show the advantages of this approach.","Spatiotemporal phenomena,
Telecommunication traffic,
Traffic control,
Power system modeling,
Training data,
Computer science,
Biomedical engineering,
Weather forecasting,
Automobiles,
Cities and towns"
Inferring specifications to detect errors in code,"A new static program analysis method for checking structural properties of code is proposed. The user need only provide a property to check; no further annotations are required. An initial abstraction of the code is computed that over-approximates the effect of function calls. This abstraction is then iteratively refined in response to spurious counterexamples. The refinement involves inferring a context-dependent specification for each function call, so that only as much information about a function is used as is necessary to analyze its caller. When the algorithm terminates, the remaining counterexample is guaranteed not to be spurious, but because the program and its heap are finitized, absence of a counterexample does not constitute proof","Java,
Computer errors,
Computer science,
Artificial intelligence,
Information analysis,
Iterative algorithms,
Writing,
Detectors,
Performance analysis,
Software engineering"
"Approximating bounded, nonorientable surfaces from points","We present an approach to surface approximation from points that allows reconstructing surfaces with boundaries, including globally nonorientable surfaces. The surface is defined implicitly using directions of weighted covariances and weighted averages of the points. Specifically, a point belongs to the surface, if its direction to the weighted average has no component into the direction of smallest covariance. For bounded surfaces, we require in addition that any point on the surface is close to the weighted average of the input points. We compare this definition to alternatives and discuss the details and parameter choices. Points on the surface can be determined by intersection computations. We show that the computation is local and, therefore, no globally consistent orientation of normals is needed. Continuity of the surfaces is not affected by the particular choice of local orientation. We demonstrate our approach by rendering several bounded (and nonorientable) surfaces using ray casting.","Surface reconstruction,
Shape,
Solids,
Displays,
Computer science,
Casting,
Information geometry,
Clouds,
Robustness,
Sampling methods"
An ad hoc on-demand routing protocol with high packet delivery fraction,"A mobile ad hoc network (MANET) is a system of wireless mobile nodes dynamically self-organizing in arbitrary and temporary network topologies. Frequent topology changes caused by node mobility make the raising of the packet delivery fraction in MANET a challenging problem. In this paper, we propose a scheme to improve packet delivery fraction of the ad hoc on-demand distance vector (AODV) protocol and compare it with the performance of several routing protocols. A variety of scenarios using a variety of pause time and size of the ad hoc network were simulated.","Routing protocols,
Mobile ad hoc networks,
Network topology,
Computer science,
Mobile computing,
Ad hoc networks,
Spread spectrum communication,
Radio propagation,
Maintenance engineering,
Computational modeling"
Responding to policies at runtime in TrustBuilder,"Automated trust negotiation is the process of establishing trust between entities with no prior relationship through the iterative disclosure of digital credentials. One approach to negotiating trust is for the participants to exchange access control policies to inform each other of the requirements for establishing trust. When a policy is received at run-time, a compliance checker determines which credentials satisfy the policy so they can be disclosed. In situations where severed sets of credentials satisfy a policy and some of the credentials are sensitive, a compliance checker that generates all the sets is necessary to insure that the negotiation succeeds whenever possible. Compliance checkers designed for trust management do not usually generate all the satisfying sets. In this paper, we present two practical algorithms for generating all satisfying sets given a compliance checker that generates only one set. The ability to generate all of the combinations provides greater flexibility in how the system or user establishes trust. For example, the least sensitive credential combination could be disclosed first. These Ideas have been implemented in TrustBuilder, our prototype system for trust negotiation.","Runtime,
Access control,
Computer science,
Prototypes,
Licenses,
Credit cards,
Open systems,
Internet,
Security,
Protection"
ID-based proxy blind signature,"Blind signature is the concept to ensure anonymity of e-coin. Untracebility and unlinkability are two main properties of real coin, which require mimicking electronically. Proxy signature schemes allow a proxy signer to generate a proxy signature on behalf of an original signer. All the previous proxy signature schemes are based on ElGamal-type schemes. We propose a new proxy blind signature scheme based on an ID-based signature scheme, which uses bilinear pairings of elliptic curves or hyperelliptic curves.","Elliptic curves,
Computer science,
Information security,
Laboratories,
Public key,
Elliptic curve cryptography"
IS project selection: the role of strategic vision and IT governance,"The prioritization of information systems projects is a function of the strategic vision of the organization. We develop a model of the IT selection process that is based upon the influence of strategic vision. Strategic vision influences both the type of projects considered, the resources allocated to information systems, and the processes and mechanisms for justifying IT investments. Two of the processes that influence the final project selection are the reporting structure of the IS organization and the involvement of a steering committee for investment prioritization. We suggest that this influences the criteria used to make IT investment decisions.","Investments,
Information systems,
Application software,
Management information systems,
Costs,
Environmental economics,
Resource management,
Outsourcing,
Computer industry,
Pricing"
High-resolution 3-D shape integration of dentition and face measured by new laser scanner,"Face and dentition were measured using a high-resolution three-dimensional laser scanner to circumvent problems of radiation exposure and metal-streak artifacts associated with X-ray computed tomography. The resulting range data were integrated in order to visualize the dentition relative to the face. The acquisition interval for dentition by laser scanner was 0.18 mm, and complicated morphologies of the occlusal surface could be sufficiently reproduced. Reproduction of occlusal condition of upper and lower dentitions was conducted by matching the surface of the occlusal impression record with upper dentition data. To integrate dentition and face, a marker plate interface was devised and adopted on the lower dental cast or by the subject directly. Integration was performed by matching both sets of interface data. Reproduction of the occlusal condition and integration of the dentition and face were accomplished and visualized satisfactorily by computer graphics. The integration accuracy was examined by changing the attachment angle of the marker plate, and the marker plate attached at 45/spl deg/ showed the smallest error of 0.2 mm. The current noninvasive method is applicable to clinical examination, diagnosis and explanation to the patient when dealing with the physical relationship between face and dentition.","Shape measurement,
X-ray lasers,
Data visualization,
Surface morphology,
Computed tomography,
X-ray imaging,
Surface emitting lasers,
Dentistry,
Computer graphics,
Computer errors"
Sequential diagnosis of processor array systems,"We examine the diagnosis of processor array systems formed as two-dimensional arrays, with boundaries, and either four or eight neighbors for each interior processor. We employ a parallel test schedule. Neighboring processors test each other, and report the results. Our diagnostic objective is to find a fault-free processor or set of processors. The system may then be sequentially diagnosed by repairing those processors tested faulty according to the identified fault-free set, or a job may be run on the identified fault-free processors. We establish an upper bound on the maximum number of faults which can be sustained without invalidating the test results under worst case conditions. We give test schedules and diagnostic algorithms which meet the upper bound as far as the highest order term. We compare these near optimal diagnostic algorithms to alternative algorithms, both new and already in the literature, and against an upper bound ideal case algorithm, which is not necessarily practically realizable. For eight-way array systems with N processors, an ideal algorithm has diagnosability 3N/sup 2/3/-2N/sup 1/2/ plus lower-order terms. No algorithm exists which can exceed this. We give an algorithm which starts with tests on diagonally connected processors, and which achieves approximately this diagnosability. So the given algorithm is optimal to within the two most significant terms of the maximum diagnosability. Similarly, for four-way array systems with N processors, no algorithm can have diagnosability exceeding 3N/sup 2/3//2/sup 1/3/-2N/sup 1/2/ plus lower-order terms. And we give an algorithm which begins with tests arranged in a zigzag pattern, one consisting of pairing nodes for tests in two different directions in two consecutive test stages; this algorithm achieves diagnosability (3/2)(5/2)/sup 1/3/N/sup 2/3/-(5/4)N/sup 1/2/ plus lower-order terms, which is about 0.85 of the upper bound due to an ideal algorithm.","Sequential diagnosis,
Fault diagnosis,
Upper bound,
Processor scheduling,
System testing,
Sequential analysis,
Scheduling algorithm,
Field-flow fractionation,
Lattices,
Computer science"
Applying ant colony optimization to routing in optical multistage interconnection networks with limited crosstalk,"Summary form only given. Ant colony optimization (ACO) technique can be successfully implemented to solve many combinatorial optimization problems. In this paper we use the ACO technique to route messages through an N/spl times/N optical multistage interconnection network (OMIN) allowing up to 'C' limited crosstalk's (conflicts between messages within a switch) where 'C' is a technology driven parameter and is always less than log/sub 2/N. Messages with switch conflicts satisfying the crosstalk constraint are allowed to pass in the same group, but if there is any link conflict, then messages are routed in a different group. The focus here is to minimize the number of passes required for routing allowing up to 'C' limited crosstalks in an N/spl times/N OMIN. In this paper we show how the ACO technique can be applied to the routing problem, and its performance is compared to that of the degree-descending algorithm using simulation techniques. Finally the lower bound estimate on the minimum number of passes required is calculated and compared to the results obtained using the two algorithms discussed. The results obtained show that the ACO technique performs better than the degree-descending algorithm and is quite close to optimal algorithms to the problem.","Ant colony optimization,
Routing,
Intelligent networks,
Optical crosstalk,
Optical interconnections,
Optical fiber networks,
Multiprocessor interconnection networks,
Heuristic algorithms,
Optical switches,
Computer science"
Coupon based incentive systems and the implications of equilibrium theory,"""Coupons "" is an incentive scheme that gives users credit for forwarding information to other users over wireless, potentially ad hoc networks. Having previously performed an initial evaluation of the main characteristics, this paper first examines how this idea works in more complex, hybrid networks and then focuses on the effects of user behavior on system performance. By considering implications of game theory concepts, such as evolutionary stable strategies, particular emphasis is given on how we can reasonably constrain behavior to a range of values, all of which result in good system performance. Results show that by developing and validating effective incentive systems, we can greatly improve the ability to efficiently disseminate information to users throughout the evolving Internet.","Internet,
Ad hoc networks,
System performance,
Game theory,
Intelligent systems,
Intelligent networks,
Programmable control,
Computer science,
Incentive schemes,
Performance evaluation"
Local and global comparison of continuous functions,We introduce local and global comparison measures for a collection of k /spl les/ d real-valued smooth functions on a common d-dimensional Riemannian manifold. For k = d = 2 we relate the measures to the set of critical points of one function restricted to the level sets of the other. The definition of the measures extends to piecewise linear functions for which they are easy to compute. The computation of the measures forms the centerpiece of a software tool which we use to study scientific datasets.,"Software performance,
Combustion,
Software measurement,
Computer science,
Mathematics,
Computational modeling,
Ignition,
Fires,
Strips,
Level set"
GridSpeed: a Web-based grid portal generation server,"GridSpeed is a grid portal hosting server that automatically generates and publishes a customized Web interface to the grid for applications, with minimal effort required from the user. Users need only to specify information regarding their application using simple GridSpeed Web forms. With GridSpeed, users need not make any modifications to their applications nor write any glue code to publish the application on the Web, not requiring any knowledge of Perl, JSP (Java server pages) or Java servlets. Moreover, the portal generated by GridSpeed provides an application frontend as well as a set of fundamental portal services such as an information service, monitoring service, data management, single sign-on, and so forth. GridSpeed publishes a set of portals as grid services themselves generated by the system that is shamble, searchable, and accessible from others interested in using the application. This feature facilitates the reuse of application portals for specific application domains, as well as increases the number of available grid applications accessible on the Web. This work describes an overview and architecture of the GridSpeed system, and evaluates the system for two real-world scientific applications: BLAST and MCell.","Portals,
Mesh generation,
Application software,
Java,
Resource management,
Computer industry,
Toy industry,
Informatics,
Supercomputers,
Computer science"
Adaptive language modeling with varied sources to cover new vocabulary items,"N-gram language modeling typically requires large quantities of in-domain training data, i.e., data that matches the task in both topic and style. For conversational speech applications, particularly meeting transcription, obtaining large volumes of speech transcripts is often unrealistic; topics change frequently and collecting conversational-style training data is time-consuming and expensive. In particular, new topics introduce new vocabulary items which are not included in existing models. In this work, we use a variety of data sources (reflecting different sizes and styles), combined using mixture n-gram models. We study the impact of the different sources on vocabulary expansion and recognition accuracy, and investigate possible indicators of the usefulness of a data source. For the task of recognizing meeting speech, we obtain a 9% relative reduction in the overall word error rate and a 61% relative reduction in the word error rate for ""new"" words added to the vocabulary over a baseline language model trained from general conversational speech data.","Vocabulary,
Training data,
Speech recognition,
Natural languages,
Error analysis,
Automatic speech recognition,
Testing,
Ear,
Computer science,
Web sites"
Concurrency Control Mechanisms for Closely Coupled Collaboration in Multithreaded Peer-to-Peer Virtual Environments,"As collaboration in virtual environments becomes more object-focused and closely coupled, the frequency of conflicts in accessing shared objects can increase. In addition, two kinds of concurrency control “surprises” become more disruptive to the collaboration. Undo surprises can occur when a previously visible change is undone because of an access conflict. Intention surprises can happen when a concurrent action by a remote session changes the structure of a shared object at the same perceived time as a local access of that object, such that the local user might not get what they expect because they have not had time to visually process the change. A hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises, which allows the concurrency scheme to be tailored to the tolerance for such surprises. One mechanism is semioptimistic; the other two are pessimistic. Designed for peer-to-peer virtual environments in which several threads have access to the shared scene graph, these algorithms are straightforward and relatively simple. They can be implemented using C/C++ and Java, under Windows and Unix, on both desktop and immersive systems. In a series of usability experiments, the average performance of the most conservative concurrency control mechanism on a local LAN was found to be quite acceptable.",
A provably good algorithm for high performance bus routing,"As the clock frequencies used in industrial applications increase, the timing requirements on routing problems become tighter, and current routing tools can not successfully handle these constraints any more. We focus on the high-performance single-layer bus routing problem, where the objective is to match the lengths of all nets belonging to each bus. An effective approach to solve this problem is to allocate extra routing resources around short nets during routing; and use those resources for length extension afterwards. We first propose a provably optimal algorithm for routing nets with min-area max-length constraints. Then, we extend this algorithm to the case where minimum constraints are given as exact length bounds. We also prove that this algorithm is optimal within a constant factor. Both algorithms proposed are also shown to be scalable for large circuits, since the respective time complexities are O(A) and O(A log A), where A is the area of the intermediate region between chips.","Routing,
Clocks,
Circuits,
Frequency,
Timing,
Wires,
Computer science,
Computer industry,
Application software,
Resource management"
Training in peacekeeping operations using virtual environments,"Proper training of military personnel, at all levels, has never been more crucial. In this article, we describe the application of virtual environment technology to a novel and complex task-the military checkpoint. In the process, we also address differences between an immersive virtual environment training system and a destop version.","Virtual reality,
Virtual environment,
Computer networks,
Application software,
Head,
Layout,
Vehicle detection,
Glass,
Computer displays,
Switches"
Dynamic classifier selection for effective mining from noisy data streams,"Mining from data streams has become an important and challenging task for many real-world applications such as credit card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single ""best"" classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence.","Data mining,
Testing,
Distributed control,
Computer science,
Application software,
Credit cards,
Protection,
Heuristic algorithms,
Partitioning algorithms"
Automatic synthesis of communication-based coordinated multi-robot systems,"To enable the successful deployment of task-achieving multi-robot systems (MRS), coordination mechanisms must be utilized in order to effectively mediate the interactions between the robots and the task environment. Over the past decade, there have been a number of elegant experimentally demonstrated MRS coordination mechanisms. Most of these mechanisms have been task-specific in nature, typically providing only empirical insights into coordination design and little in the way of systematic techniques to assist in the design of coordinated MRS for new task domains. To fully realize the potentials of MRS, formally-grounded systematic techniques amenable to analysis are needed in order to facilitate the design of coordinated MRS. We address this problem by presenting a formal framework for describing and reasoning about coordination in a MRS. Using this principled foundation, we are developing a suite of general methods for automatically synthesizing the controllers of robots constituting a MRS such that the given task is performed in a coordinated fashion. This paper presents a method for the automatic synthesis of a specific type of controller, one that is stateless but capable of inter-robot communication. We also present a graph coloring-based approach for minimizing the number of necessary unique communication messages. The synthesis of such communicative controllers provides a means for assessing the uses and limitations of communication in MRS coordination. We present experimental validation of our formal approach of controller synthesis in a multi-robot construction domain through physically-realistic simulations and in real-robot demonstrations.",
Dependability of safety-critical systems,"Dependability, i.e. the capability of a system of delivering the expected service, is a fundamental requirement for safety-critical applications. It is becoming a very important matter in the automotive field. Indeed, car industries are planning to replace the traditional mechanical-hydraulic apparatus used for the execution of the driving commands with all-electric systems, the so-called drive-by-wire (DbW) systems, which are made of components that are not as reliable as the ones of the traditional apparatus. This paper, starting from the existing literature, provides at first a conceptual framework for the dependability topic. Then dependable architectures for DbW systems are presented. As a case of study, the steering maneuver is considered.","Automotive engineering,
Humans,
Computer science,
Application software,
Transducers,
Communication networks,
Signal processing,
Costs,
Manufacturing"
Improved Real-Time Stereo on Commodity Graphics Hardware,"This paper presents a detailed description of an advanced real-time correlation-based stereo algorithm running completely on the graphics processing unit (GPU). This is important since it allows to free up the main processor for other tasks including high-level interpretation of the stereo results. Compared to previous GPU-based stereo implementations our implementation includes some advanced features such as adaptive windows and cross-checking. By taking advantage of advanced features of recent GPUs the proposed algorithm is also a lot faster than previous implementations. Our implementation running on an ATI Radeon 9800 graphics card achieves over 289 million disparity evaluations per second including all the overhead to download images and read-back the disparity map, which is several times faster than commercially available CPU-based implementations.","Hardware,
Stereo vision,
Computer graphics,
Computer science,
Computer vision,
Application software,
Central Processing Unit,
Casting,
Optimization methods,
Personal communication networks"
"Development of a wheeled mobile robot ""octal wheel"" realized climbing up and down stairs","This paper proposes an eight-wheeled robot which is able to climb over the uneven terrain for rescue, de-mining other works. In order to perform these works without human assistance, robots must have the ability to move on rugged terrain. Wheeled vehicles have advantages for moving efficiency and speed but the disadvantage is that the diameter of the wheel limits which obstacles can be surmounted. This paper proposes a mechanism which eliminates the disadvantages of a wheeled system. This mechanism is applied to a self-standing type eight-wheeled robot which is able to climb up and down stairs by utilizing a command form remote controller. Experimental results demonstrate the effectiveness of this mechanism and robot.","Mobile robots,
Wheels,
Humans,
Gears,
Servomechanisms,
Servomotors,
Computer science,
Control systems,
Tires,
Educational institutions"
Robust resource allocation for sensor-actuator distributed computing systems,"This research investigates two distinct issues related to a resource allocation: its robustness and the failure rate of the heuristic used to determine the allocation. The target system consists of a number of sensors feeding a set of heterogeneous applications continuously executing on a set of heterogeneous machines connected together by high-speed heterogeneous links. There are number of quality of service (QoS) constraints that must be satisfied. A heuristic failure occurs if the heuristic cannot find an allocation that allows the system to meet its QoS constraints. The system is expected to operate in an uncertain environment where the workload, i.e., the load presented by the set of sensors, is likely to change unpredictably, possibly invalidating a resource allocation that was based on the initial workload estimate. The focus of this paper is the design of a static heuristic that: (a) determines a robust resource allocation, i.e., a resource allocation that maximizes the allowable increase in workload until a run-time reallocation of resources is required to avoid a QoS violation, and (b) has a very low failure rate. This study proposes a heuristic that performs well with respect to the failure rates and robustness to unpredictable workload increases. This heuristic is, therefore, very desirable for systems where low failure rates can be a critical requirement and where unpredictable circumstances can lead to unknown increases in the system workload.","Robustness,
Resource management,
Distributed computing,
Quality of service,
Sensor systems and applications,
Sensor systems,
Computer science,
Active appearance model,
Application software,
Runtime"
Edge-disjoint paths in planar graphs,"We study the maximum edge-disjoint paths problem (MEDP). We are given a graph G = (V, E) and a set T = {s/sub 1/t/sup 1/, s/sub 2/t/sup 2/,..., s/sub k/t/sup k/} of pairs of vertices: the objective is to find the maximum number of pairs in T that can be connected via edge-disjoint paths. Our main result is a poly-logarithmic approximation for MEDP on undirected planar graphs if a congestion of 2 is allowed, that is, we allow up to 2 paths to share an edge. Prior to our work, for any constant congestion, only a polynomial-factor approximation was known for planar graphs although much stronger results are known for some special cases such as grids and grid-like graphs. We note that the natural multi-commodity flow relaxation of the problem has an integrality gap of /spl Omega/(/spl radic/|V|) even on planar graphs when no congestion is allowed. Our starting point is the same relaxation and our result implies that the integrality gap shrinks to a poly-logarithmic factor once 2 paths are allowed per edge. Our result also extends to the unsplittable flow problem and the maximum integer multicommodity flow problem. A set X /spl sube/V is well-linked if for each S /spl sub/ V, |/spl delta/(S)| /spl ges/ min{|S /spl cap/ X |, |(V - S) /spl cap/ X|}. The heart of our approach is to show that in any undirected planar graph, given any matching M on a well-linked set X, we can route /spl Omega/(|M|) pairs in M with a congestion of 2. Moreover, all pairs in M can be routed with constant congestion for a sufficiently large constant. This results also yields a different proof of a theorem of Klein, Plotkin, and Rao that shows an O(1) maxflow-mincut gap for uniform multicommodity flow instances in planar graphs. The framework developed in this paper applies to general graphs as well. If a certain graph theoretic conjecture is true, it yields poly-logarithmic integrality gap for MEDP with constant congestion.","Polynomials,
Approximation algorithms,
Heart,
Computational Intelligence Society,
Engineering profession,
Very large scale integration,
Circuits,
Routing,
High-speed networks,
Tree graphs"
Accuracy of link capacity estimates using passive and active approaches with CapProbe,"CapProbe is an inexpensive and accurate means to estimate capacity. CapProbe combines both dispersion and end-to-end delay to estimate the capacity of the narrowest link on a path. We evaluate in this paper the accuracy of CapProbe estimation, and its dependence on end systems speed, packet sizes, narrow link speeds, and other system parameters. We test kernel and user level implementations of CapProbe and find the kernel implementation to be much more accurate. We also evaluate through experiments the effect of probing packet size on the accuracy of CapProbe estimation. Finally, we explore the idea of a ""passive CapProbe"" within the context of a TCP flow. Passive here means that the dispersion and delay observed for the TCP flow data and ACK packets, without introducing any additional probing packets. We test active and passive versions of CapProbe with TCP. The active version is found to produce more accurate capacity estimates than the passive version.","Delay estimation,
Bandwidth,
Testing,
Kernel,
Streaming media,
Routing protocols,
Multicast protocols,
Throughput,
Computer science,
Internet"
Developing the technique of measurements of magnetic field in the CMS steel yoke elements with flux-loops and Hall probes,"Compact muon solenoid (CMS) is a general-purpose detector designed to run at the highest luminosity at the CERN large hadron collider (LHC). Its distinctive features include a 4 T superconducting solenoid with 6 m diameter by 12.5 m long free bore, enclosed inside a 10000-ton return yoke made of construction steel. Accurate characterization of the magnetic field everywhere in the CMS detector, including the large ferromagnetic parts of the yoke, is required. To measure the field in and around ferromagnetic parts, a set of flux-loops and Hall probe sensors will be installed on several of the steel pieces. Fast discharges of the solenoid during system commissioning tests will be used to induce voltages in the flux-loops that can be integrated to measure the flux in the steel at full excitation of the solenoid. The Hall sensors will give supplementary information on the axial magnetic field and permit estimation of the remanent field in the steel after the fast discharge. An experimental R&D program has been undertaken, using a test flux-loop, two Hall sensors, and sample disks made from the same construction steel used for the CMS magnet yoke. A sample disc, assembled with the test flux-loop and the Hall sensors, was inserted between the pole tips of a dipole electromagnet equipped with a computer-controlled power supply to measure the excitation of the steel from full saturation to zero field. The results of the measurements are presented and discussed.","Magnetic field measurement,
Collision mitigation,
Steel,
Hall effect devices,
Magnetic sensors,
Solenoids,
Detectors,
Large Hadron Collider,
Superconducting magnets,
Testing"
Source-based QoS service routing in distributed service networks,"Based on the distributed and composable services model, the QoS service routing/composition problem has emerged as the middleware support for multimedia applications. Different from the conventional QoS data routing, QoS service routing presents additional challenges caused by the service functionality, service dependency, resource requirement heterogeneity, and loop issues that make solutions for QoS data-routing inapplicable to QoS service routing. Existing solutions for addressing this problem are either not generic enough or not integrated, so that they either become inapplicable to new environments/metrics or the computed paths are sub-optimal. This paper presents a generic and integrated approach for computing optimal service paths, and shows an aggregate performance function - F- that optimizes several QoS metrics at the same time. Simulations show that F is superior, and integrating service configuration selection with service path finding is desirable.","Routing,
Intelligent networks,
Bandwidth,
Application software,
Aggregates,
Middleware,
Computer science,
Computational modeling,
Software performance,
NASA"
Efficient image segmentation by mean shift clustering and MDL-guided region merging,"We present an efficient color and texture segmentation algorithm by combining two statistical techniques: mean shift clustering and minimum description length (MDL) principle. Mean shift clustering is proven in generating robust and accurate segmentation results for color images, but the selection of the two scale parameters remains a challenging problem for images with texture. Optimization based on MDL principle requires little parameter tuning, but the initial input has a strong impact on its efficiency and effectiveness. Our approach is to apply mean shift clustering to generate an initial over-segmentation and then merge regions based on MDL principle. Objects with texture can be extracted with reasonable accuracy by merging regions under the guidance of MDL principle, without the need of convolving the image with a bank of filters. Experimental results on a variety of natural scene images are reported and compared with the JSEG algorithm. It takes about 1 second for our algorithm to process a 320/spl times/240 color image on a conventional PC.","Image segmentation,
Merging,
Clustering algorithms,
Image color analysis,
Image edge detection,
Computer science,
Robustness,
Filter bank,
Layout,
Image texture analysis"
Learnability and automatizability,"We consider the complexity of properly learning concept classes, i.e. when the learner must output a hypothesis of the same form as the unknown concept. We present the following upper and lower bounds on well-known concept classes: 1) We show that unless NP = RP, there is no polynomial-time PAC learning algorithm for DNF formulae where the hypothesis is an OR-of-thresholds. Note that as special cases, we show that neither DNF nor OR-of-thresholds are properly learnable unless NP = RP. Previous hardness results have required strong restrictions on the size of the output DNF formula. We also prove that it is NP-hard to learn the intersection of /spl lscr/ /spl ges/ 2 halfspaces by the intersection of k halfspaces for any constant k > 0. Previous work held for the case when k = /spl lscr/; 2) Assuming that NP /spl nsube/ DTIME(2/sup n/spl epsi//) for a certain constant /spl epsiv/ < 1 we show that it is not possible to learn size s decision trees by size s/sup k/ decision trees for any k /spl ges/ 0. Previous hardness results for learning decision trees held for k /spl les/ 2; 3) We present the first nontrivial upper bounds on properly learning DNF formulae and decision trees. In particular we show how to learn size s DNF by DNF in time 2/sup O~/(/spl radic/(n log s)), and how to learn size s decision trees by decision trees in time n/sup O(log s)/. The hardness results for DNF formulae and intersections of halfspaces are obtained via specialized graph products for amplifying the hardness of approximating the chromatic number as well as applying work on the hardness of approximate hypergraph coloring. The hardness results for decision trees, as well as the upper bounds, are obtained by developing a connection between automatizability in proof complexity and learnability, which may have other applications.",
"An integrated laboratory for processor organization, compiler design, and computer networking","An integrated laboratory dealing with processor organization, compiler design, and computer networking has been developed. The goals of the laboratory are to make it possible for each student to work with modern and attractive materials and to learn about the interfaces between system modules, to provide students with opportunities to collaborate in the construction of a large system, and to give students a sense of accomplishment. The goals have been met based on the responses of students who have used it, verifying its effectiveness. This paper describes the design and development of the baseline components to be integrated, the laboratory organization and schedule, and the results and evaluation of the laboratory.","Computer science education,
Program compilers,
Collaborative work,
Computer networks,
Laboratories"
MobiVoD: a video-on-demand system design for mobile ad hoc networks,"We present a design for a system that provides video-on-demand (VOD) services to mobile ad hoc clients. Such a system allows the clients to access video information anytime anywhere. MobiVoD, the proposed solution, overcomes many difficulties currently challenging video streaming in a mobile ad hoc network. The new environment includes a three-tier architecture, in which the mobile VOD system employs a periodic broadcast protocol to achieve maximum scalability; and the clients leverage an ad hoc network caching technique to minimize the service delay. This system can sustain client failure and mobility, and provide true VOD services to most clients.","Mobile ad hoc networks,
Wireless LAN,
Bluetooth,
Watches,
Wireless networks,
Personal digital assistants,
Video on demand,
Computer science,
Mobile computing,
Streaming media"
Formal description of the cognitive process of decision making,"Decision making is one of the basic cognitive processes of human behaviors by which a preferred option or a course of actions is chosen from among a set of alternatives based on certain criteria. Decision theories are widely applied in a number of disciplines encompassing cognitive science, computer science, management science, economics, sociology, psychology, political science, and statistics. The studies on decision making can be categorized into two classes: descriptive and normative theories. A number of decision strategies have been proposed from different angles and application domains such as the maximum expected utility and Bayesian method. However, there is still a lack of a fundamental and mathematical decision model and a rigorous cognitive process for decision making. This paper presents a decision making process on the basis of the layered reference model of the brain (LRMB). The cognitive process of decision making is modeled as a sequence of Cartesian-product based selections. A rigorous description of the decision process in real-time process algebra (RTPA) is presented. Different decision making strategies are comparatively analyzed. The result shows these strategies can be well fit in the formally described decision process. The cognitive process of decision making may be applied in a wide range of decision-based systems, such as cognitive informatics, software agent systems, expert systems, and decision support systems.","Decision making,
Humans,
Decision theory,
Cognitive science,
Computer science,
Sociology,
Psychology,
Statistics,
Application software,
Utility theory"
Folk psychology for human modelling: extending the BDI paradigm,,"Psychology,
Humans,
Computer science,
Software engineering,
Computational modeling,
Computer simulation,
Knowledge representation,
Decision making,
Computer languages,
Terminology"
On maintaining interactivity in event delivery synchronization for mirrored game architectures,"Online amusement applications, as distributed multiplayer videogames and interactive story telling, are gaining increasing attention both from entertainment industry and from scientific community. Providing a pleasant experience to players requires a rapid delivery of game actions among the various nodes in the network. A high playability degree should be guaranteed independently of the user's location, utilized device (PC, PDA, cellphone), type of connection (wired, wireless), and number of contemporary players. To this aim, we have devised an innovative approach to design the event delivery service for networked multiplayer game applications. Exploiting the semantics of the game, our scheme relaxes the ordering and reliability properties, upholding the interactivity level while preserving the game state consistency. The main contribution of our work is to show the benefits in event delivery synchronization obtained by employing, in this context, RED techniques borrowed from networking queuing management.","Games,
Computer architecture,
Computer science,
Network servers,
Application software,
Toy industry,
Consumer electronics,
Internet,
Computer industry,
Personal digital assistants"
Cache Refill/Access Decoupling for Vector Machines,"Vector processors often use a cache to exploit temporal locality and reduce memory bandwidth demands, but then require expensive logic to track large numbers of outstanding cache misses to sustain peak bandwidth from memory. We present refill/access decoupling, which augments the vector processor with a Vector Refill Unit (VRU) to quickly pre-execute vector memory commands and issue any needed cache line refills ahead of regular execution. The VRU reduces costs by eliminating much of the outstanding miss state required in traditional vector architectures and by using the cache itself as a cost-effective prefetch buffer. We also introduce vector segment accesses, a new class of vector memory instructions that efficiently encode two-dimensional access patterns. Segments reduce address bandwidth demands and enable more efficient refill/access decoupling by increasing the information contained in each vector memory command. Our results show that refill/access decoupling is able to achieve better performance with less resources than more traditional decoupling methods. Even with a small cache and memory latencies as long as 800 cycles, refill/access decoupling can sustain several kilobytes of in-flight data with minimal access management state and no need for expensive reserved element buffering.","Bandwidth,
Prefetching,
Delay,
Supercomputers,
Random access memory,
Vector processors,
Costs,
Parallel processing,
Concurrent computing,
Computer architecture"
Approximating Component Selection,"Simulation composability is a difficult capability to achieve due to the challenges of creating components, selecting combinations of components, and integrating the selected components. We address the second of these challenges through analysis of Component Selection (CS), the NP-complete process of selecting a minimal set of components to satisfy a set of objectives. Due to the high order of computational complexity of CS, we examine approximating solutions that make the CS process practicable. We define two variations of CS and prove that good approximations to optimal solutions result from applying a standard Greedy selection algorithm to each. Despite our creation of approximable variations of CS, we conjecture that any proof of the inapproximability of CS will reveal theoretical limitations of its practicality. We conclude that reasonably constrained variations of CS can be solved satisfactorily, and efficiently, but more general cases appear to never be solvable in a similar manner.","Approximation algorithms,
Cost function,
Approximation methods,
Polynomials,
Computer science,
Algorithm design and analysis,
Minimization methods,
Equations"
Collaborative filtering with maximum entropy,"As users navigate through online document collections on high-volume Web servers, they depend on good recommendations. We present a novel maximum-entropy algorithm for generating accurate recommendations and a data-clustering approach for speeding up model training. Recommender systems attempt to automate the process of ""word of mouth"" recommendations within a community. Typical application environments such as online shops and search engines have many dynamic aspects.","Collaboration,
Filtering,
Entropy,
Navigation,
Computer science,
Context modeling,
Collaborative work,
Bayesian methods,
History,
Search engines"
Fixed-outline floorplanning through evolutionary search,"We address the practical problem of fixed-outline VLSI floorplanning with minimizing the objective of area. This problem was shown significantly much more difficult than the well-researched floorplan problems without fixed-outline regime (N. Saurabh, et al. (2001)). We successfully develop an algorithm with evolutionary search to efficiently handle the fixed-die floorplanning problem and achieve near 100% successful probability, on the average.","Robustness,
Very large scale integration,
Computer science,
Modems,
Wheels"
The MINOS data acquisition system,"The MINOS long-baseline neutrino experiment consists of two detectors separated by 730 km. Both are equipped with identical data acquisition (DAQ) systems, based on continuous, dead time free readout. Data are read from the untriggered front-end electronics by VME single board computers and transferred across high-speed PCI data links for consolidation by data routing processors. An array of Linux computers selects events of interest using software-based trigger algorithms. We present the design of the DAQ system and report on experience gathered during early operation of the experiment.","Data acquisition,
Neutrino sources,
Solid scintillation detectors,
Physics,
Astronomy,
Laboratories,
Optical fibers,
Acceleration,
Extraterrestrial measurements,
Protons"
Object recognition using segmentation for feature detection,"A new method is presented to learn object categories from unlabeled and unsegmented images for generic object recognition. We assume that each object can be characterized by a set of typical regions, and use a new segmentation method - ""similarity-measure segmentation"" - to split the images into regions of interest. This approach may also deliver segments, which are split into several disconnected parts, which turn out to be a powerful description of local similarities. Several textural features are calculated for each region, which are used to learn object categories with boosting. We demonstrate the flexibility and power of our method by excellent results on various datasets. In comparison, our recognition results are significantly higher than the results published in related work.","Object recognition,
Computer vision,
Object detection,
Image segmentation,
Electric variables measurement,
Detectors,
Computer science,
Signal processing,
Boosting,
Pattern recognition"
A probability-based algorithm to adjust contention window in IEEE 802.11 DCF,"The paper proposes a new probability-based algorithm to adjust the contention window in 802.11 DCF (distributed coordination function). Our main motivation is based on the observation that 802.11 DCF decreases the contention window to the initial value after each successful transmission, which essentially assumes that each successful transmission is an indication that the system is under low traffic loading. PDCF (probability-based DCF) takes a more conservative measure by halving the contention window size with a probability f after each successful transmission. This decrease behavior lowers the collision probability, especially when the competing node number is large. We compute the optimal value for f, and the numerical results from both analysis and simulation demonstrate that PDCF significantly improve the performance of 802.11 DCF, including throughput, fairness, and energy efficiency. In addition, PDCF is flexible for supporting priority access by selecting different values of f for different traffic types; it is fully compatible with the original 802.11 DCF, and simple to implement, as it does not require changes in control message structure and access procedures in DCF.","Wireless LAN,
Computer science,
USA Councils,
Size measurement,
Performance analysis,
Computational modeling,
Analytical models,
Throughput,
Energy efficiency,
Traffic control"
Choosing a starting configuration for particle swarm optimization,The performance of particle swarm optimization can be improved by strategically selecting the starting positions of the particles. The work suggests the use of generators from centroidal Voronoi tessellations as the starting points for the swarm. The performance of swarms initialized with this method is compared with the standard PSO algorithm on several standard test functions. Results suggest that CVT initialization improves PSO performance in high dimensional spaces.,"Particle swarm optimization,
Computer science,
Testing,
Stochastic processes"
Sampling-based planning for discrete spaces,"In this paper, we introduce several discrete space search algorithms based on continuous-space motion planning techniques such as rapidly exploring random trees (RRTs) and probabilistic roadmaps (PRMs). We describe methods for adapting these algorithms for discrete use by replacing distance metrics with cost-to-go heuristic estimates and substituting local planners for straight-line connectivity. Finally, we explore coverage and optimality properties of these algorithms in discrete spaces.","Space exploration,
Computer science,
Search problems,
Testing,
Motion control,
Euclidean distance"
A Web-service oriented framework for building SCORM compatible learning management systems,"Managing the large amount of learning content involved in the development of an expandable learning management system (LMS) is a critical task for most educational organizations. In designing such a system, consideration must be given not only to the requirement of instructors and students, but also to the adaptive qualities of the system itself, because of the rapidly changing software environment. Unlike most enterprises, educational organization can't afford high system maintenance. Development of a flexible, expandable and reliable learning management system is, therefore, imperative. This paper proposes a Web service oriented framework in building a sharable content object reference model (SCORM) compatible LMS. We integrated the SCORM concept with distributed computing technologies to build the components of the LMS and demonstrate how these services can be configured to create a complete system. This framework increases reusability and reduces the maintenance requirements of the LMS. It provides a way for small educational organizations, with limited resources, to build a complete SCORM compatible LMS. In such an ideal situation, the content provider not only provides the content, but also the service for the consumers of that content.","Least squares approximation,
Web services,
Programming profession,
Content management,
Computer science,
Distributed computing,
Educational technology,
Technology management,
Computer architecture,
Computer science education"
Efficient Barrier and Allreduce on Infiniband clusters using multicast and adaptive algorithms,"Popular algorithms proposed in the literature for doing Barrier and Allreduce in clusters, such as pair-wise exchange, dissemination and gather-broadcast do not give an optimal performance when there is skew among the nodes in the cluster. In pair-wise exchange and dissemination, all the nodes must arrive for the completion of each step. The gather-broadcast algorithm assumes a fixed tree topology. We propose to use hardware multicast of InfiniBand in the design of an adaptive algorithm that performs well in the presence of skew. In this approach, the topology of the tree is not fixed but adapts depending on the skew. The last arriving node becomes the root of the tree if the skew is sufficiently large. We have carried out in-depth evaluation of our scheme and use synchronization delay as the performance metric for Barrier and Allreduce in the presence of skew. Our performance evaluation shows that our design scales very well with system size. Our designs can reduce the synchronization delay by a factor of 2.28 for Barrier and by a factor of 2.18 in the case of Allreduce. We have examined different skew scenarios and showed that the adaptive design performs either better or comparably to the existing schemes.","Hardware,
Adaptive algorithm,
Clustering algorithms,
Multicast algorithms,
Topology,
Delay,
Computer science,
Algorithm design and analysis,
Measurement,
Personal communication networks"
A new pheromone updating strategy in ant colony optimization,"This work presents a new pheromone updating strategy , which is used to optimize ACO (ant colony optimization) in solving the traveling salesman problem. At first, the paper introduces the principle, the characteristics, the construction and the realization method about the ACO. Then, an improved ant colony optimization algorithm using a new pheromone updating strategy is proposed. The pheromone trail of each edge is set with a lower limit at the beginning iterations of the algorithm, and the worst ant judged by its tour length like the best ant used in ACO is allowed to perform global trail updating. At last, we demonstrate the efficiency of the algorithm by means of experimental study.","Ant colony optimization,
Cities and towns,
Machine learning algorithms,
Machine learning,
Cybernetics,
Traveling salesman problems,
Circuits,
Sun,
Computer science,
Conference management"
A layered design of discretionary access controls with decidable safety properties,"An access control design can be viewed as a three layered entity: the general access control model; the parameterization of the access control model; and the initial users and objects of the system before it goes live. The design of this three-tiered mechanism can be evaluated according to two broad measures, the expressiveness versus the complexity of the system. In particular, the question arises: What security properties can be expressed and verified? We present a general access control model which can be parameterized at the second layer to implement (express) any of the standard Discretionary Access Control (DAC) models. We show that the safety problem is decidable for any access control model implemented using our general access control model. Until now, all general access control models that were known to be sufficiently expressive to implement the full range of DAC models had an undecidable safety problem. Thus, given our model all of the standard DAC models (plus many others) can be implemented in a system in which their safety properties are decidable.","Safety,
Access control,
Protection,
Security,
Computer languages,
Computer science,
Heart,
Particle measurements,
Privacy,
Prototypes"
The preservation of favored building blocks in the struggle for fitness: the puzzle algorithm,"The shortest common superstring (SCS) problem, known to be NP-complete, seeks the shortest string that contains all strings from a given set. In this paper, we present a novel coevolutionary algorithm-the Puzzle Algorithm-where a population of building blocks coevolves alongside a population of solutions. We show experimentally that our novel algorithm outperforms a standard genetic algorithm (GA) and a benchmark greedy algorithm on instances of the SCS problem inspired by deoxyribonucleic acid (DNA) sequencing. We next compare our previously presented cooperative coevolutionary algorithm with the Co-Puzzle Algorithm-the puzzle algorithm coupled with cooperative coevolution-showing that the latter proves to be top gun. Finally, we discuss the benefits of using our puzzle approach in the general field of evolutionary algorithms.","Genetic algorithms,
DNA,
Evolutionary computation,
Greedy algorithms,
Computer science,
Sequences,
NP-complete problem,
Application software,
Data compression,
Computational biology"
Gust of me: reconnecting mother and son,"The Gust of Presence conceptual design lets parents and children who live apart reconnect in a more friendship-based relationship. With two Gustbowls, parents and children can communicate in a simple way that requires little effort and could subtly become a part of their daily routines. Gustbowl is designed to promote and support informal, unobtrusive interactions in families whose members live apart. The Gustbowl helps families keep in touch, rather than just exchange information, by letting members be a part of each other's daily routines. This lets them have the little encounters that are ordinary to members who live together yet are greatly missed by members who live apart. We describe how the Delft design team created the Gustbowl from user studies by developing the concept for and field-testing an experiential prototype.","Subspace constraints,
Testing,
Process design,
Communications technology,
Product design,
Prototypes,
Cultural differences,
Global communication,
Probes,
Packaging"
A mobility gateway for small-device networks,"Networks of small devices, such as environmental sensors, introduce a number of new challenges for traditional protocols and approaches. In particular, the extreme resource constraints characteristics of these devices force the engineering of device and application-specific optimizations in order to reduce complexity. By offloading some, if not most, of the complexity out of the small device and into the network, we simplify the design of individual devices, and may make otherwise infeasible applications possible. We look specifically at the problem of global mobility, and its associated protocol Mobile IP (MIP). We introduce the mobility gateway which performs MIP processing on behalf of registered devices. The mobility gateway provides an interface between the optimized world of small devices and the larger Internet. Through simulation, we investigate the potential savings offered by this technique in terms of bandwidth and power. In a sample scenario, we find the total number of bytes transmitted over the wireless channel reduced by as much as 70%, and the battery life of the device extended by more than 100 hours.","Pervasive computing,
Protocols,
Sensor phenomena and characterization,
Clouds,
Internet,
Intelligent sensors,
Bandwidth,
Handheld computers,
Mobile computing,
Computer science"
Coordinating multiple concurrent negotiations,,"Yarn,
Computer science,
Robustness,
Context-aware services,
Pervasive computing,
Costs,
Protocols,
Proposals,
Multidimensional systems,
Permission"
Simplified simulation models for indoor MANET evaluation are not robust,"We evaluate the robustness of simplified mobility and radio propagation models for indoor MANET simulations. A robust simplification allows researchers to extrapolate simulation results and reach reliable conclusions about the expected performance of protocols in real life. We show that common simplified mobility and radio propagation models are not robust. Experiments with DSR and DSDV, two representative MANET routing protocols, show that the simplifications affect the two protocols in very different manners. Even for a single protocol, the effects on perceived performance can vary erratically as parameters change. These results cast doubt on the soundness of evaluations of MANET routing protocols based on simplified mobility and radio propagation models, and expose the urgent need for more research on realistic MANET simulation.","Robustness,
Mobile ad hoc networks,
Radio propagation,
Routing protocols,
Peer to peer computing,
Computational modeling,
Computer simulation,
Indoor environments,
Predictive models,
Computer science"
Avoiding detection in a dynamic environment,"Remaining elusive while navigating to a goal in a dynamic environment containing an observer requires taking advantage of opportunistic cover as it occurs. A reactive navigation approach is needed that recognizes the utility of environment features in offering protective cover. We present an approach that allows stealthy traverses in unknown environments containing dynamic objects. It is a frontier-based method that allows a robot to follow in the obscuring shadow of objects despite their dynamics, and take advantage of more opportunistic cover if it becomes available. An analysis of our approach in off-line modeling and experiments conducted in simulation and outdoor environments demonstrate its effectiveness in achieving high quality solutions for stealthy navigation.","Navigation,
Vehicle dynamics,
Mobile robots,
Cost function,
Computer science,
Protection,
Analytical models,
Security,
Reconnaissance,
Surveillance"
On bulk loading TPR-Tree,"TPR-tree is a practical index structure for moving object databases. Due to the uniform distribution assumption, TPR-tree's bulk loading algorithm (TPR) is relatively inefficient in dealing with non-uniform datasets. In this paper we present a histogram-based bottom up algorithm (HBU) along with a modified top-down greedy split algorithm (TGS) for TPR-tree. HBU uses histograms to refine tree structures for different distributions. Empirical studies show that HBU outperforms both TPR and TGS for all kinds of non-uniform datasets, is relatively stable over varying degree of skewness and better for large datasets and large query windows.","Databases,
Partitioning algorithms,
Indexes,
Histograms,
Computer science,
Tree data structures,
Mobile computing,
Computer applications,
Military computing,
Traffic control"
Toolkits for open innovation - the case of mobile phone games,"User toolkits enable consumers to develop customized product applications without having dedicated technical knowledge. Applied in the field of handheld computing, toolkits are a very powerful instrument to meet the growing demand for customized mobile applications. In this paper, the theoretical principles and functionalities of user interaction toolkits are translated into a technical software concept for mobile phone games. The presented toolkit is a new Internet based application allowing users to create a customized game on their desktop computer and to transfer it to handheld devices. In contrast to well known open source software development projects, no programming expertise is required. Thus, participation of innovative users is comparatively easy. In line with the open source concept, but as an extending feature to the toolkit approach, the presented solution is embedded in an online community. Thus, contributions by innovative users can be stored in a library leading to a continuously growing information pool of available components. Games and components can be passed on easily between customers, facilitating the adoption of other users' contributions as well as collaborative development between users. The community feature of the toolkit does not only provide the common toolset of online communities allowing for user-to-user communication, e.g. chats and bulletin boards, or text based contributions, e.g. recommendations, product evaluations and voting tools, but it also enables users to exchange and jointly develop actual product prototypes. In this regard, the presented 'toolkit for open innovation' is the foundation of a 'value web' in a unique manner: a value web in between consumers and users. The paper further derives economic benefits of the developed toolkit for mobile phone game creation theoretically and integrates them into a diversified business model. The toolkit as a distinct performance feature has the potential to establish a social environment for its most enthusiastic users and thereby to strengthen user relationships with the provider and increase overall business success.",
Skeleton based performance prediction on shared networks,"The performance skeleton of an application is a short running program whose performance in any scenario reflects the performance of the application it represents. Such a skeleton can be employed to quickly estimate the performance of a large application under existing network and node sharing. This work presents and validates a framework for automatic construction of performance skeletons of parallel applications. The approach is based on capturing the compute and communication behavior of an executing application, summarizing this behavior and then generating a synthetic skeleton program based on the summarized information. We demonstrate that automatically generated performance skeletons take an order of magnitude less time to execute than the application they represent, yet predict the application execution time with reasonable accuracy. For the NAS benchmark suite, we observed that the average-error in predicting the execution time was 6%. This research is motivated by the problem of performance driven resource selection in shared network and Grid environments.",
Car license plate extraction using color and edge information,"We propose a novel method for car license plate extraction from color images. First, the edge map is obtained by applying Sobel edge detector. Second, a neural network is used as a classifier to identify pixel color. Finally, candidate license plate regions are located by information of edge and color distributions. Experimental results show that the proposed method is fairly reliable and can result in only a small number of false alarms.",
Optimising static workload allocation in multiclusters,"Summary form only given. Workload allocation and job dispatching are two fundamental components in static job scheduling for distributed systems. We address the static workload allocation techniques for two types of job stream in multicluster systems, namely, nonreal-time job streams and soft-real-time job streams, which request different qualities of service. Two workload allocation strategies (called ORT and OMR) are developed by establishing and numerically solving two optimisation equation sets. The ORT strategy achieves the optimised mean response time for the nonreal-time job stream; while the OMR strategy can gain the optimised mean miss rate for the soft-real-time job stream over multiple clusters (these strategies can also be applied in a single cluster system). The effectiveness of both strategies is demonstrated through theoretical analysis. The proposed workload allocation schemes are combined with two job dispatching strategies (weighted random and weighted round-robin) to generate new static job scheduling algorithms for multicluster environments. These algorithms are evaluated through extensive experimental studies and the results show that compared with static approaches without the optimisation techniques, the proposed workload allocation schemes can significantly improve the performance of static job scheduling in multiclusters, in terms of both the mean response time (for the nonreal-time jobs) and the mean miss rate (for soft-real-time jobs).","Delay,
Dispatching,
Ordinary magnetoresistance,
Dynamic scheduling,
Contracts,
Processor scheduling,
Equations,
Scheduling algorithm,
Helium,
Computer science"
Group shared protection (GSP): a scalable solution for spare capacity reconfiguration in mesh WDM networks,"This paper proposes a novel framework of shared protection, namely group shared protection (GSP), in mesh wavelength division multiplexing (WDM) networks with dynamically arriving connection requests. Based on the (M:N)/sup n/ control architecture, GSP has n mutually independent protection groups, each of which contains N SRLG-disjoint working paths protected by M protection paths. Due to the SRLG-disjointedness of the working paths in each protection group, GSP not only allows the spare capacity to be totally sharable among the corresponding working paths, but also reduces the number of working paths affected due to a single link failure. Based on the framework, an integer linear program (ILP) formulation that can optimally reconfigure the spare capacity for a specific protection group whenever a working-protection path-pair joins is proposed. Two heuristics namely link-shared protection (LSP) and ring-shared protection (RSP) are introduced for further compromising the performance and the computational complexity. The proposed schemes are compared with a reported one, namely successive survivable routing (SSR). The experimental results show that LSP, RSP and SSR yield similar performance in terms of resource sharing, whereas ILP outperforms all of them by (6-16%). Due to the limited number of working paths in each protection group, ILP can handle a dynamically arriving connection request in a reasonable amount of time. Also, we find that the number of affected working paths in GSP is about half of that in SSR. We conclude that GSP provides a scalable and efficient solution for dynamic spare capacity reconfiguration following the (M:N)/sup n/ control architecture.","Protection,
Intelligent networks,
WDM networks,
Wavelength division multiplexing,
Routing,
Time sharing computer systems,
Computer science,
Computer architecture,
Resource management,
IP networks"
MERTZ: a quest for a robust and scalable active vision humanoid head robot,"We present the design and construction of MERTZ, an active-vision humanoid head robot, with the immediate goal of having the robot runs continuously for many hours a day without supervision at various locations. We address how the lack of robustness and reliability lead to limitations and scalability issues in research robotic platforms. We propose to attend to these issues in parallel with the course of robot development. Drawing from lessons learned from our previous robots, we incorporated various fault prevention strategies into the electromechanical design. We have implemented a preliminary system, integrating sensorimotor, vision, and audio in order to test the full range of all degrees of freedom and enable the robot to engage in simple visual and verbal interaction with people. We conducted a series of experiment where the robot ran for $2 hours within 9 days at different public spaces. The robot interacted with a large number of passersby and collected at least 100,000 face images of at least 600 individuals within 4 days. We learned various lessons involving the robustness of current design and identified a set of failure modes. Lastly, we present the long term research direction for the robot.","Robustness,
Humanoid robots,
Robot vision systems,
Robot sensing systems,
Orbital robotics,
Head,
Intelligent robots,
Parallel robots,
Human robot interaction,
Computer science"
Statistical estimation of resistance/conductance by electrical impedance tomography measurements,"This paper is built upon the assumption that in electrical impedance tomography, vectors of voltages and currents are linearly dependent through a resistance matrix. This linear relationship was confirmed experimentally and may be derived analytically under certain assumptions regarding electrodes (Isaacson, 1991). Given measurement data consisting of voltages and currents, we treat this relationship as a linear statistical model. Thus, our goal is not to reconstruct the image but directly estimate its electromagnetic properties reflected in the resistance and/or conductance matrix using electrical impedance tomography (EIT) measurements of voltages and currents on the periphery of the body. Since no inverse problem is involved the algorithm for estimation merely reduces to one matrix inversion. We estimate the impedance resistance matrix using well established statistical inference techniques for linear regression models. We provide a comprehensive treatment for a two-dimensional homogeneous body of a circular shape, by which many concepts of electrical impedance tomography, such as width of electrodes, the difference between voltage-current and current-voltage systems are illustrated. Our theory may be applied to various tests including EIT hardware calibration and whether the medium is homogeneous. These tests are illustrated on phantom agar data.","Electric resistance,
Tomography,
Electrical resistance measurement,
Impedance measurement,
Electric variables measurement,
Immune system,
Voltage,
Electrodes,
Current measurement,
Electromagnetic measurements"
A virtual deadline scheduler for window-constrained service guarantees,"This paper presents an approach to window-constrained scheduling, that is suitable for multimedia and weakly-hard real-time systems. Our algorithm called virtual deadline scheduling (VDS) attempts to service m out of k job instances by their virtual deadlines, that may be some finite time after the corresponding real-time deadlines. By carefully deriving virtual deadlines, VDS outperforms our earlier dynamic window-constrained scheduling (DWCS) algorithm when servicing jobs with different request periods. Additionally, VDS is able to limit the extent to which a fraction of all job instances are serviced late, while maximizing resource utilization. Simulations show that VDS can provide better window-constrained service guarantees than other related algorithms, while having as good or better delay bounds for all scheduled jobs. Finally, an implementation of VDS in the Linux kernel compares favorably against DWCS for a range of scheduling loads.",
Path-oriented test data generation using symbolic execution and constraint solving techniques,"Automatic test data generation is a challenging task in software engineering research. This paper studies a path-oriented approach to the problem, which is based on the combination of symbolic execution and constraint solving. Methods for representing expressions and path conditions are discussed. An implemented toolkit is described with some examples. The toolkit transforms an input program (possibly embedded with assertions) to an extended finite state machine and then performs depth-first or breadth-first search on it. The goal is to find values for input variables such that a terminal state can be reached. If successful, input test data are found (which might reveal a bug in the program).","System testing,
Automatic testing,
Software testing,
Software engineering,
Laboratories,
Computer science,
Automata,
Input variables,
Programming,
Flow graphs"
Efficient Linkage Discovery by Limited Probing,"This paper addresses the problem of discovering the structure of a fitness function from binary strings to the reals under the assumption of bounded epistasis. Two loci (string positions) are epistatically linked if the effect of changing the allele (value) at one locus depends on the allele at the other locus. Similarly, a group of loci are epistatically linked if the effect of changing the allele at one locus depends on the alleles at all other loci of the group. Under the assumption that the size of such groups of loci are bounded, and assuming that the function is given only as a “black box function”, this paper presents and analyzes a randomized algorithm that finds the complete epistatic structure of the function in the form of the Walsh coefficients of the function.","embedded landscapes,
epistasis,
linkage,
probing,
MAXSAT,
Walsh analysis"
The new LHCb trigger and DAQ strategy: a system architecture based on gigabit-ethernet,"The LHCb software trigger has two levels: a high-speed trigger running at 1 MHz with strictly limited latency and a second level running below 40 kHz without latency limitations. The trigger strategy requires full flexibility in the distribution of the installed CPU power to the two software trigger levels because of the unknown background levels and event topology distribution at the time the LHC accelerator will start its operation. This requirement suggests using a common CPU farm for both trigger levels fed by a common data acquisition (DAQ) infrastructure. The limited latency budget of the first level of software trigger has an impact on the organization of the CPU farm performing the trigger function for optimal usage of the installed CPU power. We will present the architecture and the design of the hardware infrastructure for the entire LHCb software triggering system based on Ethernet as link technology that fulfills these requirements. The performance of the event-building of the combined traffic of both software trigger levels, as well as the expected scale of the system will be presented.","Data acquisition,
Detectors,
Delay,
Computer architecture,
Assembly systems,
Jacobian matrices,
Large Hadron Collider,
Software performance,
Hardware,
Topology"
Precise identification of side-effect-free methods in Java,"Knowing which methods do not have side effects is necessary in a variety of software tools for program understanding, restructuring, optimization, and verification. We present a general approach for identifying side-effect-free methods in Java software. Our technique is parameterized by class analysis and is designed to work on incomplete programs. We present empirical results from two instantiations of the approach, based on rapid type analysis and on points-to analysis. In our experiments with several components, on average 22% of the investigated methods were identified as free of side effects. We also present a precision evaluation which shows that the approach achieves almost perfect precision - i.e., it almost never misses methods that in reality have no side effects. These results indicate that very precise identification of side-effect-free methods is possible with simple and inexpensive analysis techniques, and therefore can be easily incorporated in software tools.","Java,
Software tools,
Information analysis,
Computer science,
Optimization methods,
Programming profession,
Unified modeling language,
Reverse engineering,
Optimizing compilers,
Debugging"
Efficient computation of canonical form for boolean matching in large libraries,"This paper presents an efficient technique for solving a Boolean matching problem in cell-library binding, where the number of cells in the library is large. As a hasis of the Boolean matching, we use the notion NP-representative (NI'R); two functions have the same NPR if one can he obtained from the other hy a permutation andlor complementation(s) of the variables. By using a table look-up and a tree-based hreadthfirst search strategy, our method quickly computes NPR for a given function. Boolean matching of the given function against the whole library is determined by checking the presence of its NPR in a hash table, which stores NPRs for all the library functions and their complements. The effectiveness of our method is demonstrated through experimental results, which shows that it is more than two orders of magnitude faster than the Hinsherger-Kolla's algorithm-the fastest Boolean matching algorithm for large libraries.","Libraries,
Circuit synthesis,
Logic circuits,
Boolean functions,
Computer science,
Costs,
Sufficient conditions"
A study of Chinese text summarization using adaptive clustering of paragraphs,"Automatic summarization is an important research issue in natural language processing. This paper presents a special summarization method to generate single-document summary with maximum topic completeness and minimum redundancy. It initially implements the semantic-class-based vector representations of various kinds of linguistic units in a document by means of HowNet (an existing ontology), which can improve the representation quality of traditional term-based vector space model in a certain degree. Then, by adopting K-means clustering algorithm as well as a clustering analysis algorithm, we can capture the number of different latent topic regions in a document adoptively. Finally, topic representative sentences are selected from each topic region to form the final summary. In order to evaluate the effectiveness of the proposed summarization method, a novel metric which is known as representation entropy is used for summarization redundancy evaluation. Preliminary experimental results show that the proposed method outperforms the conventional basic summarization method under the evaluation scheme when dealing with diverse genres of Chinese documents with free writing style and flexible topic distribution.","Clustering algorithms,
Natural language processing,
Algorithm design and analysis,
Partitioning algorithms,
Helium,
Computer science,
Ontologies,
Entropy,
Writing,
Internet"
Theory and practice behind the course designing enterprisewide IT systems,"As a consequence of the co-evolution of business and information technology (IT), the responsibilities of software engineers are expanding. They are becoming much more involved in business-related issues. When defining computing curriculums, this trend needs to be taken into consideration, for example, by proposing courses on business and IT integration. This paper presents a transdisciplinary, problem-based learning course that addresses business and IT. The target audience is computer science and software engineering students. The course has three modules: a competitive game to illustrate business thinking, role-playing to practice IT requirement analysis, and an IT integration project to present how modern off-the-shelf technologies contribute to IT system realization. Each module has several sections comprising experiential learning and traditional ex cathedra lectures. The originality of the course lies in the combination of breadth of the subject and depth on what is taught. The goals of the course and its detailed contents are presented: the emphasis is on the process-related/technical and emotional learning experience by the students and on the author's experiences gained from teaching that course.","Computer science education,
Game theory,
Information systems,
Design methodology"
Quantum and classical strong direct product theorems and optimal time-space tradeoffs,"A strong direct product theorem says that if we want to compute k independent instances of a function, using less than k times the resources needed for one instance, then our overall success probability is exponentially small in k. We establish such theorems for the classical as well as quantum query complexity of the OR function. This implies slightly weaker direct product results for all total functions. We prove a similar result for quantum communication protocols computing k instances of the disjointness function. These results imply a time-space tradeoff T/sup 2/S = /spl Omega/(N/sup 3/) for sorting N items on a quantum computer, which is optimal up to polylog factors. They also give several tight time-space and communication-space tradeoffs for the problems of Boolean matrix-vector multiplication and matrix multiplication.","Quantum mechanics,
Quantum computing,
Error probability,
Complexity theory,
Ink,
Sorting,
Distributed computing,
Computational modeling,
Probability distribution,
Computer science"
Automatic identification of bacterial types using statistical imaging methods,"The objective of the current study is to develop an automatic tool to identify microbiological data types using computer-vision and statistical modeling techniques. Bacteriophage (phage) typing methods are used to identify and extract representative profiles of bacterial types out of species such as the Staphylococcus aureus. Current systems rely on the subjective reading of profiles by a human expert. This process is time-consuming and prone to errors, especially as technology is enabling the increase in the number of phages used for typing. The statistical methodology presented in this work, provides for an automated, objective and robust analysis of visual data, along with the ability to cope with increasing data volumes.",
A robust and efficient video stabilization algorithm,"The acquisition of digital video usually suffers from undesirable camera jitter due to unstable random camera motion, which is produced by a hand-held camera or a camera in a vehicle moving on a non-smooth road or terrain. We propose a real-time robust video stabilization algorithm to remove undesirable motion jitter and produce a stabilized video. We first compute the optical flow between successive frames, followed by estimating the camera motion by fitting the computed optical flow field to a simplified affine motion model with a trimmed least squares method. Then, the computed camera motions are smoothed temporally to reduce the motion vibrations by using a regularization method. Finally, we transform all frames of the video based on the original and smoothed motions to obtain a stabilized video. Experimental results are given to demonstrate the stabilization performance and the efficiency of the proposed algorithm.","Robustness,
Optical computing,
Image motion analysis,
Optical filters,
Digital cameras,
Motion estimation,
Jitter,
Flowcharts,
Least squares approximation,
Computer science"
Least squares ellipsoid specific fitting,"In this paper, a sufficient condition for a quadric surface to be an ellipsoid has been developed and a closed-form solution for ellipsoid fitting is developed based on this constraint, which is a best fit to the given data amongst those ellipsoids whose short radii are at least half of their major radii, in the sense of algebraic distance. A simple search procedure is proposed to pursuit the 'best' ellipsoid when data cannot be well described by this type of ellipsoid. The proposed fitting algorithm is quick, stable and insensitive to small errors in the data.","Least squares methods,
Ellipsoids,
Surface fitting,
Sufficient conditions,
Scattering,
Geometry,
Nonlinear equations,
Computer science,
Pattern recognition,
Machine vision"
Multimedia and Cooperative learning in signal Processing techniques in communications,"The letter describes how multimedia technology and cooperative learning were used to teach an online course, Signal Processing techniques in Communication Systems (SP/CS). Students study online presentations enhanced by multimedia animations and perception quizzes, discuss problems using an asynchronous conferencing system, and complete cooperative laboratory simulations based on MATLAB. A locally designed course management system provides access to the online features of the course. A survey that assesses the benefits of multimedia and cooperative support materials is also included.","Signal processing,
Collision mitigation,
Animation,
Multimedia systems,
MATLAB,
Computer science education,
Quadratic programming,
Java,
IEEE news,
Laboratories"
A study of symbol segmentation method for handwritten mathematical formula recognition using mathematical structure information,"Symbol segmentation is very important in handwritten mathematical formula recognition, since it is the very first portion of the recognition process. This paper proposes a new symbol segmentation method using mathematical structure information. The base technique of symbol segmentation employed in the existing methods is dynamic programming which optimizes the overall results of individual symbol recognition. The new method we propose here improves symbol recognition performance by using correction values together with evaluation values of symbol recognition. These correction values are calculated from the relations among handwritten stroke positions and mathematical structure. There is no report which takes account of mathematical structure information for symbol segmentation in the handwritten mathematical formula recognition. Our experiments have proven that the recognition rate of symbol segmentation by existing methods is between 90.2% and 93.3%, while our proposed method gives correct recognition rate of 97.1%.","Handwriting recognition,
Optimization methods,
Pattern recognition,
Lattices,
Magnetooptic recording,
Information science,
Information technology,
Dynamic programming,
Computer interfaces,
Markup languages"
Personal servers as digital keys,"Personal servers are an attractive concept: people carry around a device that takes care of computing, storage and communication on their behalf in a pervasive computing environment. So far personal servers have mainly been considered for accessing personal information. In this paper, we consider personal servers in the context of a digital key system. Digital keys are an interesting alternative to physical keys for mail or good delivery companies whose employees access tens of private buildings every day. We present a digital key system tailored for the current incarnation of personal servers, i.e., a Bluetooth-enabled mobile phone. We describe how to use Bluetooth for this application, we present a simple authentication protocol and we provide a detailed analysis of response time and energy consumption on the mobile phone.","Mobile handsets,
Pervasive computing,
Personnel,
Postal services,
Bluetooth,
Delay,
Mobile communication,
Companies,
Computer science,
Context"
Vector addition tree automata,"We introduce a new class of automata, which we call vector addition tree automata. These automata are a natural generalization of vector addition systems with states, which are themselves equivalent to Petri-nets. Then, we prove that the decidability of provability in multiplicative exponential linear logic (which is an open problem) is equivalent to the decidability of the reachability relation for vector addition tree automata. This result generalizes the well-known connection existing between Petri nets and the !-horn fragment of multiplicative exponential linear logic.","Automata,
Logic,
Petri nets,
Vectors,
Computer science,
Natural languages"
Neural network-based reputation model in a distributed system,"Current centralized trust models are inappropriate to apply in a large distributed multi-agent system, due to various evaluation models and partial observations in local level reputation management. This paper proposes a distributed reputation management structure, and develops a global reputation model. The global reputation model is a novel application of neural network techniques in distributed reputation evaluations. The experimental results showed that the model has robust performance under various estimation accuracy requirements. More important, the model is adaptive to changes in distributed system structures and in local reputation evaluations.","Neural networks,
Intelligent networks,
Master-slave,
Computer science,
Aggregates,
Memory management,
Context modeling,
Multiagent systems,
Robustness,
Load management"
Agent-based modelling: a case study in HIV epidemic,"This research presents an agent-based, bottom-up modelling approach to develop a simulation tool for estimating and predicting the spread of the human immunodeficiency virus (HIV) in a given population. HIV is mainly a sexually transmitted disease (STD) causing a serious problem to human health. The virus is transmitted from an infected person to another who was previously healthy through different biological, social and environmental factors. The research develops the simulation tool by modelling these factors by agents. Although research has and is being conducted to estimate and predict the spread of the HIV epidemic, the proposed research seeks to investigate the spread using a different approach. The previous models used a top-down modelling approach. They are built from the general characteristics and behaviours of the population. They have not explored the potential use of agent technology. This research attempts to investigate the flexibility that the multi-agent system offers. Agent-based models are close to the situations that exist in a given real system that consists of autonomous components interacting with each other. The modelling approach has the advantage of observing the interaction made between agents, which is a difficult task in the top-down modelling approach. The research investigates the performance of the tool and presents the first results obtained.","Computer aided software engineering,
Human immunodeficiency virus,
Biological system modeling,
Predictive models,
Multiagent systems,
Diseases,
Mathematical model,
Computer science,
Africa,
Computational modeling"
Bisimulation theory for switching linear systems,"A general notion of hybrid bisimulation is proposed and related to the notions of algebraic, state-space and input-output equivalences for the class of switching linear systems. An algebraic characterization of hybrid bisimulations and a procedure converging in a finite number of steps to the maximal hybrid bisimulation are derived. Bisimulation-based reduction and simulation-based abstraction are defined and characterized. Connections with observability are investigated.","Linear systems,
Observability,
Computer science,
Control systems,
State-space methods,
Computational modeling,
Mathematics,
Switching systems,
Laser sintering"
A new analysis of the value of unlabeled data in semi-supervised learning for image retrieval,"There has been an increasing interest in using unlabeled data in semi-supervised learning for various classification problems. Previous work shows that unlabeled data can improve or degrade the classification performance depending on whether the model assumption matches the ground-truth data distribution, and also on the complexity of the classifier compared with the size of the labeled training set. In this paper, we provide a new analysis on the value of unlabeled data by considering different distributions of the labeled and unlabeled data and showing the migrating effect for semi-supervised learning. Extensive experiments have been performed in the context of image retrieval applications. Our approach evaluates the value of unlabeled data from a new aspect and is aimed to provide a guideline on how unlabeled data should be used","Image analysis,
Semisupervised learning,
Information retrieval,
Image retrieval,
Degradation,
Maximum likelihood estimation,
Computer science,
Guidelines,
Web search,
Text categorization"
Environmental and economic trade-offs in consumer electronic products recycling: a case study of cell phones and computers,"The fate of retired cell phones differs from that of retired PCs: approximately 70% of retired collected cell phones are refurbished and resold (the remaining units are recycled or discarded) whereas only a small percentage of PCs are ever used again. Considerable attention has been focused to develop policies that minimize the environmental impacts of electronic waste in general, but there is little consistency between the strategies because trade-offs between economic costs and environmental benefits are not well understood. This paper presents results of a survey conducted to better understand the economics of cell phone recycling. We find that the net cost to recyclers of collecting each cell phone (
6)farexceedstheestimatedcosttotransport,sort,dismantle,refine,anddisposeofhazardousandnon−hazardouswastes(
0.74) associated with discarded phones. These results were compared with the collection and processing cost of PCs for which recycling is typically not profitable. Our findings are informative for formulating better policies to manage the end-of-life of consumer electronic products.",
The gender factor performing visualization tasks on computer media,"There has been a recent trend to develop new, complex, and often predominantly visual, user interfaces as media front ends' to computer applications. This trend assumes that varied user groups have equivalent propensities to perceive, interpret, and understand the multidimensional spatial properties and relationships of objects visually presented through the on-screen medium. However, it is well known that there exist gender differences in certain spatial abilities, suggesting that there may be gender biases to effectively utilize certain visual cues presented through computer-mediated interfaces. We test this notion with three experiments (object matching, positioning, and resizing) using 2D and 3D computer screen media. Whereas female subjects under-perform male subjects in the object matching and positioning tasks, they outperform male subjects in the resizing task. Implications for the design of gender-effective user interface media are discussed.","Visualization,
User interfaces,
Computer applications,
Computer interfaces,
Web page design,
Human computer interaction,
Information systems,
Testing,
Hardware,
Usability"
Understanding the energy efficiency of simultaneous multithreading,"Simultaneous multithreading (SMT) has proven to be an effective method of increasing the performance of microprocessors by extracting additional instruction-level parallelism from multiple threads. In current microprocessor designs, power-efficiency is of critical importance, and we present modeling extensions to an architectural simulator to allow us to study the power-performance efficiency of SMT. After a thorough design space exploration we find that SMT can provide a performance speedup of nearly 20% for a wide range of applications with a power overhead of roughly 24%. Thus, SMT can provide a substantial benefit for energy-efficiency metrics such as ED/sup 2/. We also explore the underlying reasons for the power uplift, analyze the impact of leakage-sensitive process technologies, and discuss our model validation strategy.","Energy efficiency,
Multithreading,
Surface-mount technology,
Microprocessors,
Microarchitecture,
Computer science,
Permission,
Parallel processing,
Yarn,
Space exploration"
Configuration-sensitive process scheduling for FPGA-based computing platforms,"Reconfigurable computing has become an important part of research in software systems and computer architecture. While prior research on reconfigurable computing have addressed architectural and compilation/programming aspects to some extent, there is still not much consensus on what kind of operating system (OS) support should be provided. In this paper, we focus on OS process scheduler, and demonstrate how it can be customized considering the needs of reconfigurable hardware. Our process scheduler is configuration sensitive, that is, it reuses the current FPGA configuration as much as possible. Our extensive experimental results show that the proposed scheduler is superior to classical scheduling algorithms such first-come-first-serve (FCFS) and shortest job first (SJF).",
Enhancing real-time CORBA via real-time Java features,"End-to-end middleware predictability is essential to support quality of service (QoS) capabilities needed by distributed real-time and embedded (DRE) applications. Real-time CORBA is a middleware standard that allows DRE applications to allocate, schedule, and control the QoS of CPU, memory, and networking resources. Existing real-time CORBA solutions are implemented in C++, which is generally more complicated and error-prone to program than Java. The real-time specification for Java (RTSJ) provides extensions that enable Java to be used for developing DRE systems. Real-time CORBA does not currently leverage key RTSJ features, such as scoped memory and real-time threads. Thus, integration of real-time CORBA and RTSJ is essential to ensure the predictability required for Java-based DRE applications. We provide the following contributions to the study of middleware for DRE applications. First we analyze the architecture of ZEN, our implementation of real-time CORBA, identifying sources for the application of RTSJ features. Second, we describe how RTSJ features, such as scoped memory and real-time threads, can be associated with key ORB components to enhance the predictability of DRE applications using realtime CORBA and the RTSJ. Third, we perform preliminary qualitative and quantitative analysis of predictability enhancements arising from our application of RTSJ features. Our results show that use of RTSJ features can considerably improve the predictability of DRE applications written using Real-time CORBA and real-time Java.","Java,
Middleware,
Yarn,
Real time systems,
Quality of service,
Computer science,
Application software,
Processor scheduling,
Communication standards,
Memory management"
Reusable functional composition patterns for Web services,"Developers write Web service composition programs in terms of functionalities (e.g., ""WebSearch"") to postpone choosing which services of the same functionality to invoke (Google or Yahoo). We provide a higher level of abstraction than this for higher reuse. We express high-level ""patterns"" (e.g., ""SearchAndCollectData"") as both objects that can be ""specialized"" to particular applications (""SearchAnd-DownloadPapers"" vs. ""SearchAndAddBooksInCart"") and objects that are reusable in the construction of higher-level ones. Our approach lets developers write patterns in terms of high-level functionalities (e.g., ""CollectData "") and later decide on services to compose that have lower-level functionalities (e.g., ""DownloadPapers"" or ""addBooksIn-Carts""). We describe our prototype and show an example of nested pattern specialization. We also discuss a reuse trade-off, showing that too much abstraction makes the pattern less expressive. Rather, we suggest developers capture what must be guaranteed in every context of invocation, regardless of the service selection.","Web services,
Databases,
Writing,
Computer science,
Prototypes,
Context-aware services,
Abstracts,
Books,
Pattern matching,
Usability"
XChange: coupling parallel applications in a dynamic environment,"Modern computational science applications are becoming increasingly multidisciplinary, involving widely distributed research teams and their underlying computational platforms. A common problem for the grid applications used in these environments is the necessity to couple multiple, parallel subsystems, with examples ranging from data exchanges between cooperating, linked parallel programs, to concurrent data streaming to distributed storage engines. This work presents the XChange/sub mxn/ middleware infrastructure for coupling componentized distributed applications. XChange/sub mxn/ implements the basic functionality of well-known services like the CCA Forum's MxN project, by providing efficient data redistribution across parallel application components. Beyond such basic functionality, however, XChange/sub mxn/ also addresses two of the problems faced by wide area scientific collaborations, which are (1) the need to deal with dynamic application/component behaviors, such as dynamic arrivals and departures due to the availability of additional resources, and (2) the need to 'match' data formats across disparate application components and research teams. In response to these needs, XChange/sub mxn/ uses an anonymous publish/subscribe model for linking interacting components, and the data being exchanged is dynamically specialized and transformed to match end point requirements. The pub/sub paradigm makes it easy to deal with dynamic component arrivals and departures. Dynamic data transformation enables the 'inflight' correction of data or needs mismatches for cooperating components. This work describes the design and implementation of XChange/sub mxn/, and it evaluates its implementation compared to those of less flexible transports like MPI. It also highlights the utility ofXChange/sub mxn/'s 'inflight' data specialization, by applying it to the SmartPointer parallel data visualization environment developed at our institution. Interestingly, using XChange/sub mxn/ did not significantly affect performance but led to a reduction in the size of the code base.","Atmospheric modeling,
Distributed computing,
Computational modeling,
Data visualization,
Educational institutions,
Computer applications,
Engines,
Joining processes,
Portals,
Analytical models"
Customization of enterprise content management systems: an exploratory case study,"Enterprise content management (ECM) systems are mostly implemented in organizations by acquiring commercial software packages and customizing them to meet the organizational requirements. The customization aspect of ECM systems lacks empirical research. This paper explores the concepts of ECM customization and issues identified with ECM customization. The data are based on an in-depth case study from the oil industry and complemented with a secondary analysis of 60 vendor-reported cases of ECM implementations. The results show considerable customization challenges related to ECM, especially concerning integration, usability and functional adaptation. A resulting framework of customization concepts in ECM is suggested and discussed, along with issues for further research.","Content management,
Computer aided software engineering,
Electrochemical machining,
Enterprise resource planning,
Management information systems,
Software packages,
Knowledge management,
Educational institutions,
Petroleum industry,
Usability"
Evaluation of fingerprint orientation field registration algorithms,"The majority of modern, fingerprint registration algorithms are based on the alignment of minutiae features. However, shortcomings of this approach are becoming apparent due to the difficulty of extracting minutiae from noisy or low quality images. This papers explores a novel approach to fingerprint registration based on orientation field alignment. One main advantage of this method is that orientation fields can be computed reliably for poor quality images, providing a robust feature for registration. Three orientation field alignment algorithms are presented, and their performance is evaluated using an FVC2002 dataset.","Fingerprint recognition,
Feature extraction,
Nonlinear distortion,
Computer science,
Australia,
Robustness,
Law enforcement,
Forensics,
Biometrics,
Image matching"
Artificial intelligence with uncertainty,,"Artificial intelligence,
Uncertainty,
Entropy,
Chaos,
Fractals,
Complex networks,
Evolution (biology),
Automation,
Computer science,
Physics"
Expand-Ahead: A Space-Filling Strategy for Browsing Trees,"Many tree browsers allow subtrees under a node to be collapsed or expanded, enabling the user to control screen space usage and selectively drill-down. However, explicit expansion of nodes can be tedious. Expand-ahead is a space-filling strategy by which some nodes are automatically expanded to fill available screen space, without expanding so far that nodes are shown at a reduced size or outside the viewport. This often allows a user exploring the tree to see further down the tree without the effort required in a traditional browser. It also means the user can sometimes drill-down a path faster, by skipping over levels of the tree that are automatically expanded for them. Expand-ahead differs from many detail-in-context techniques in that there is no scaling or distortion involved. We present 1D and 2D prototype implementations of expand-ahead, and identify various design issues and possible enhancements to our designs. Our prototypes support smooth, animated transitions between different views of a tree. We also present the results of a controlled experiment which show that, under certain conditions, users are able to drill-down faster with expand-ahead than without",
A high-speed power and resolution adaptive flash analog-to-digital converter,"A high-speed and small-area power and resolution adaptive flash ADC is presented. The high-speed power and resolution adaptive ADC (HSPRA-ADC) utilizes an encoder design which significantly improves its speed and minimizes the chip area over the earlier design. Moreover, the ADC also achieves lower power consumption compared to the earlier design. The HSPRA-ADC enables exponential power reduction with linear resolution reduction. The unused parallel voltage comparators are switched to the standby mode during which they consume only the leakage power. The HSPRA-ADC was designed and simulated using 0.18 /spl mu/m and 0.07 /spl mu/m CMOS technologies. The HSPRA-ADC is desirable in wireless mobile applications.","Analog-digital conversion,
Inverters,
Signal resolution,
Read only memory,
Threshold voltage,
Energy consumption,
Computer science,
CMOS technology,
Binary codes,
Power engineering and energy"
Object Class Recognition with Many Local Features,"In this paper we present a method to recognize an object class by learning a statistical model of the class. The probabilistic model decomposes the appearance of an object class into a set of local parts and models the appearance, relative location, co-occurrence, and scale of these parts. However, in many object classification approaches that use local features, learning the parameters is exponential in the number of parts because of the problem of matching local features in the image to parts in the model. In this paper we present a learning method that overcomes this difficulty by adding new parts to the model incrementally, using the Maximum-Likelihood framework. When we add a part to the model, a set of candidate parts are selected and the part that increases the likelihood of the data the most is added to the model. Once this part is added to the model, the parameters for all parts up to this point are updated using EM. The learning and recognition in this approach are translation and scale invariant, robust to background clutter, and has less restriction on the number of parts in the model. The validity of the approach is demonstrated on a real world dataset, where the approach is competitive with others, and where the learning for a rich model is much faster than previous approaches.","Robustness,
Face recognition,
Image recognition,
Space exploration,
Computer science,
Learning systems,
Computer vision,
Motorcycles,
Shape"
Improving simulation-based verification by means of formal methods,"The design of complex systems is largely ruled by the time needed for verification. Even though formal methods can provide higher reliability, in practice often simulation based verification is used. Large testbenches are created and if the design produces the correct output for all stimuli it is said to be correct. But there is no guarantee that the testbench is complete in the sense that it contains test-cases for all ¿important¿ situations. We propose an approach to detect ¿gaps¿ in testbenches, i.e. behavior that is not tested. The approach relies on automatic generation of properties from the testbench in terms of a formal property language. By construction the properties are valid within the testbench. A model checker proves the validity of the property on the design. If this proof succeeds, the testbench covers all possible situations for given signals. In case of failure counter-examples are produced. These counter-examples represent behavior that is not tested, i.e. a gap in the testhench. The feasibility of the approach is underlined by experiments.","Circuit simulation,
Computational modeling,
Circuit testing,
Computer science,
Automatic testing,
Moore's Law,
Manufacturing,
Productivity,
Security,
Clocks"
Integrity management in component based systems,"There is a need for mechanisms for maintaining and restoring software integrity on deployed systems. Dynamic replacement, removal and addition of components in deployed systems is supported by most component models. This is needed to enable the software on a device to evolve in the period that it is owned by a consumer, but endangers the integrity of the software on these devices. For high volume consumer devices the challenge is to keep the devices operating reliable and robust in the period that it is owned, used and possibly reconfigured by a consumer. To this end, we propose mechanisms and tools, for maintaining system integrity on deployed systems.","Robustness,
Project management,
Software systems,
Software maintenance,
Home appliances,
Computer architecture,
Monitoring,
Mathematics,
Computer science,
Software architecture"
Experiments with explicit for-loops in genetic programming,"Evolving programs with explicit loops presents major difficulties, primarily due to the massive increase in the size of the search space. Fitness evaluation becomes computationally expensive and a method for dealing with infinite loops must be implemented. We have investigated ways of dealing with these problems by the evolution of for-loops of increasing semantic complexity. We have chosen two problems - a modified Santa Fe ant problem and a sorting problem - which have natural looping constructs in their solution and a solution without loops is not possible unless the tree depth is very large. We have shown that by controlling the complexity of the loop structures it is possible to evolve smaller and more understandable programs for these problems.","Genetic programming,
Iron,
Counting circuits,
Computer science,
Information technology,
Sorting,
Space technology,
Robots,
Embedded computing,
Books"
Monte Carlo simulation of complex radiotherapy treatments,"Monte Carlo simulation is an accurate way of assessing radiotherapy dose distribution in nonhomogeneous volumes, but it requires long processing times. A new distribution model simulates radiotherapy treatments and runs on a PC network, which reduces the processing time and makes for a powerful treatment-verification tool.",
Improved action point model in traffic flow based on driver's cognitive mechanism,"Car-following modelling in traffic flow theory has been becoming of increasing importance in traffic engineering and Intelligent Transport System(ITS), the point of concentration in this research field is how to analysis and measurement of driver cognitive behaviour. Based on qualitative description of driving behaviour with the new concept of driver's multi-typed information process and multi-ruled decision-making mechanism, this paper has analysed in more detail the AP (action point) model, and ameliorated AP model by eliminating its deficiency. The emphasis of this paper is placed on the deduction of the acceleration equations by considering that the following car is subjected in congested traffic flow. Furthermore, from the cybernetics perspective, this paper has carried out numeral simulation to car-following behaviour with deceleration and acceleration algorithms. The model validation and simulation results have shown that the improved action point car-following model can replicated car-following behaviour and be able to use to reveal the essence of traffic flow characteristics.","Traffic control,
Vehicle safety,
Road safety,
Automotive engineering,
Road vehicles,
Vehicle driving,
Acceleration,
Equations,
Educational programs,
Computer vision"
Agent approach for service discovery and utilization,"There is an extensive set of published and usable services on the Internet. Human based approaches to discover and utilize these services is not only time consuming, but also requires continuous user interaction. This paper describes an agent approach for service discovery and utilization (AASDU), which focuses on using light weight autonomous agents built into a multi-agent referral community, and Web service standards (namely UDDI, SOAP, WSDL and XML). The AASDU approach proposes to use agents that interact with end users by accepting their queries to discover services, and efficiently manage service invocation. AASDU uses intrinsic multi-agent properties to allow agents to communicate and cooperate with one another. Each agent conforms to a communication protocol that allows it to send and receive messages from another agent, without needing to know the address of the receiving agent. There is a need for effective and efficient communication among components in a multi-disciplinary, cross-organizational architecture. This has resulted in a proliferation of communication building blocks, or middleware, for distributed scientific computing. The most recent, and quite network-based services, referred to as Web services. This paper also discusses the interoperability that can be achieved between software components through the use of Web service standards and protocols in the context of AASDU.","Web services,
Web and internet services,
Humans,
Autonomous agents,
Simple object access protocol,
XML,
Computer architecture,
Middleware,
Scientific computing,
Software standards"
Enriched media-experience of sport events,"This paper describes a system where Internet-enabled sensor technology was integrated into a context-aware platform to give viewers of sport events an enriched media experience. The system was developed as a proof of concept and was evaluated during real-life use at the Vasaloppet crosscountry ski event. Using Bluetooth wireless ad-hoc networking and GPRS technology, sensor data was transmitted from contestants to the context-aware platform Alipes, which in turn presented the sport event viewer with a personalized, context-aware view. In this paper, we discuss the system architecture and integration of components. The system was evaluated both from technical and user perspectives, where the evaluation results confirm our approach to be technically feasible and that the system provide an enriched media-experience for the majority of viewers.","Testing,
Internet,
Wireless sensor networks,
TV broadcasting,
Computer science,
Sensor phenomena and characterization,
Sensor systems,
Bluetooth,
Ground penetrating radar,
Information retrieval"
Assigning tasks in a 24-hour software development model,"With the advent of globalization and the Internet, the concept of global software development is gaining ground. The global development model opens up the possibility of 24-hour software development by effectively utilizing the time zone differences. To harness the potential of the 24-hour software development model for reducing the overall development time, a key issue is the allocation of project tasks to the resources in the distributed team. In this paper, we examine this issue of task allocation in order to minimize the completion time of a project. We discuss a model for distributed team across time-zones and propose a task allocation algorithm for the same. We apply the approach on tasks of a few synthetic projects and two real projects and show that there is a potential to reduce the project duration as well as improve the resource utilization through 24-hour development.","Programming,
Optimal scheduling,
Globalization,
Internet,
Resource management,
Collaborative software,
Costs,
Computer science,
Communications technology,
International collaboration"
HOURS: achieving DoS resilience in an open service hierarchy,"Hierarchical systems have been widely used to provide scalable distributed services in the Internet. Unfortunately, such a service hierarchy is vulnerable to DoS attacks. This paper presents HOURS that achieves DoS resilience in an open service hierarchy. HOURS ensures high degree of service accessibility for each surviving node by: 1) augmenting the service hierarchy with hierarchical overlay networks with rich connectivity; 2) making the connectivity of each overlay highly unpredictable; and 3) recovering the overlay when its normal operations are disrupted. We analyze an HOURS-protected open service hierarchy, and demonstrate its high degree of resilience to even large-scale, topology-aware DoS attacks.","Resilience,
Computer crime,
Topology,
Web and internet services,
Large-scale systems,
Web server,
Hierarchical systems,
Computer science,
Electronic mail,
Certification"
Stiffness analysis of a class of parallel mechanisms for micro-positioning applications,Micro-positioning devices are increasingly being made of parallel manipulators due to their superior stiffness characteristics. This paper explores a variant of the classical six degrees-of-freedom Stewart-Gough platforms for use in micro-positioning applications. Stiffness values of both these mechanisms are computed and compared for relative gain in theoretical stiffness achieved. An over-sized version of the platform was built for educational purposes.,"Application software,
Computer science,
Jacobian matrices,
Costs,
Assembly,
Hydraulic actuators,
Shape,
Matrix converters,
Whales"
Anti-windup schemes for discrete time systems: an LMI-based design,"Two anti-windup schemes for discrete time systems to achieve global stability and induced l/sub 2/ performance are presented. The first scheme is a direct extension of an existing static anti-windup scheme for continuous systems. As in continuous time, a problem of implementing an algebraic loop formed arises. However, while preserving the stability and performance, an iterative algorithm to solve the algebraic loop within its prescribed time sampling may be employed. Alternatively, a closely related explicit static anti-windup scheme is proposed, in which no iterative solution at each time step is required. Simulation results are given for both schemes.","Discrete time systems,
Stability,
Closed loop systems,
Output feedback,
Control systems,
Linear systems,
State feedback,
Computer science,
Drives,
Australia"
Knowledge management in software engineering education,The importance of knowledge management (KM) is increasingly recognized in education since it deals with information and knowledge resources. The challenge for educational organizations is to develop effective strategies for managing the knowledge resources and providing appropriate access to this information. This article examines the key components of KM that may have a potential impact on success of the KM practices in educational context. The purpose of this investigation is to support the learning activity in the software engineering program within a university. Results indicated that both academic staff and students were aware of the importance of these components and the role they play in successfully managing knowledge in the program; however the actual implementation of such criteria was inadequate and needed improvement.,"Knowledge management,
Software engineering,
Educational programs,
Computer science education,
Educational technology,
Educational institutions,
Decision making,
Resource management,
Companies,
Management information systems"
A novel ant clustering algorithm based on cellular automata,"Based on the principle of cellular automata in artificial life, an artificial ant sleeping model (ASM) and an ant algorithm for cluster analysis (A4C) are presented. Inspired by the behaviors of gregarious ant colonies, we use the ant agent to represent a data object. In ASM, each ant has two states: a sleeping state and an active state. The ant's state is controlled by a function of the ant's fitness to the environment it locates and a probability for the ants becoming active. The state of an ant is determined only by its local information. By moving dynamically, the ants form different subgroups adaptively, and hence the data objects they represent are clustered. Experimental results show that the A4C algorithm on ASM is significantly better than other clustering methods in terms of both speed and quality. It is adaptive, robust and efficient, achieving high autonomy, simplicity and efficiency.","Clustering algorithms,
Algorithm design and analysis,
Particle swarm optimization,
Partitioning algorithms,
Computer science,
Ant colony optimization,
Design optimization,
Helium,
Software algorithms,
Clustering methods"
Minimum classification error training of landmark models for real-time continuous speech recognition,"Though many studies have shown the effectiveness of the minimum classification error (MCE) approach to discriminative training of HMM for speech recognition, few if any have reported MCE results for large (> 100 hours) training sets in the context of real-world, continuous speech recognition. Here we report large gains in performance for the MIT JUPITER weather information task as a result of MCE-based batch optimization of acoustic models. Investigation of word error rate versus computation time showed that small MCE models significantly outperform the maximum likelihood (ML) baseline at all points of equal computation time, resulting in up to 20% word error rate reduction for in-vocabulary utterances. The overall MCE loss function was minimized using Quickprop, a simple but effective second-order optimization method suited to parallelization over large training sets.","Speech recognition,
Lattices,
Management training,
Jupiter,
Optimization methods,
Computer errors,
Laboratories,
Error analysis,
Maximum likelihood estimation,
Computer science"
Update propagation through replica chain in decentralized and unstructured P2P systems,"We propose a novel algorithm, called update propagation through replica chain (UPTReC), to maintain file consistency in decentralized and unstructured peer-to-peer (P2P) systems. In UPTReC, each file has a logical replica chain composed of all replica peers (RPs) which are defined as peers that have replicas of the file. Each RP acquires partial knowledge of the bi-directional chain by keeping a list of information about k nearest RPs, called probe peers, in each direction. When an RP initiates an update, it pushes the update to all possible online (active) RPs through the replica chain. A reconnected RP pulls an online RP to synchronize the replica status and the information of the probe peers. An analytical model is derived to evaluate the performance of the UPTReC algorithm. The analytical results provide a better understanding of the system in choosing the system parameters for probabilistically guaranteed file consistency with minimum overheads. Simulation experiments are conducted to compare the performance with an existing update propagation algorithm based on the rumor spreading scheme. The experimental results show that the UPTReC can significantly reduce (up to 70%) overhead messages and also achieve smaller stale query ratio for files prone to frequent updates.","Costs,
Peer to peer computing,
Probes,
Topology,
Control systems,
Intelligent networks,
Computer science,
Maintenance engineering,
Bidirectional control,
Analytical models"
Rethinking the pipeline as object-oriented states with transformations,"The pipeline is a simple and intuitive structure to speed up many problems. Novice parallel programmers are usually taught this structure early on. However, expert parallel programmers typically eschew using the pipeline in coarse-grained applications because it has three serious problems that make it difficult to implement efficiently. First, processors are idle when the pipeline is not full. Second, load balancing is crucial to obtaining good speedup. Third, it is difficult to incrementally incorporate more processors into an existing pipeline. Instead, experts recast the problem as a master/slave structure which does not suffer from these problems. This paper details a transformation that allows programs written in a pipeline style to execute using the master/slave structure. Parallel programmers can benefit from both the intuitive simplicity of the pipeline and the efficient execution of a master/slave structure. This is demonstrated by performance results from two applications.","Pipelines,
Master-slave,
Programming profession,
Load management,
Concurrent computing,
Parallel programming,
Yarn,
Computer science,
Decoding,
Production facilities"
Low power design using dual threshold voltage,"We study the reduction of static power consumption by dual threshold voltage assignment. Our goal is, under given timing constraint, to select a maximum number of gates working at high-Vth such that the total power gain is maximized. We propose a maximum independent set based slack assignment algorithm to select gates for high-Vth. The results show that our assignment algorithm can achieve about 68% improvement as compared to results without using dual Vth.","Threshold voltage,
Timing,
Delay,
Circuits,
Energy consumption,
CMOS process,
Fabrication,
Computer science,
Subthreshold current,
Sleep"
C2P2: a peer-to-peer network for on-demand automobile information services,"This short work outlines challenges of delivering continuous media and traffic information to mobile car-to-car peer-to-peer (C2P2) network of devices. We analyze network connectivity of a C2P2 cloud as a function of radio range of each device. A novel concept introduced by C2P2 is on-demand delivery of continuous media, audio and video clips to moving vehicles.","Peer to peer computing,
Telecommunication traffic,
Roads,
Computer science,
Intelligent vehicles,
Automobile manufacture,
Radio navigation,
Global Positioning System,
Databases,
Mobile computing"
Approximate safety enforcement using computed viability envelopes,"A numerical method is proposed for the constraint of the state of a dynamical system such that it cannot enter a predefined failure region. The proposed approach to this viability problem involves an explicit numerical approximation of a viability envelope, coupled with a practical strategy for enforcing containment that is based upon a predictive look-ahead strategy. The approach can be applied to achieve automated ""intervention when necessary"" to enforce system safety at interactive rates. Applications are shown to several low-dimensional systems, including steering control of a vehicle constrained to a given environment geometry.","Safety,
Control systems,
Computer science,
Automatic control,
Trajectory,
Force feedback,
Geometry,
Vehicle dynamics,
Wheels,
Roads"
A calculation for the compensation voltages in dynamic voltage restorers by use of PQR power theory,"This paper discusses how to calculate the compensation voltages in dynamic voltage restorers (DVR) by use of PQR power theory. Directly sensed three-phase voltages are transformed to p-q-r coordinates without time delay, then the reference voltages in p-q-r coordinates have very simple form: dc values. The controller in p-q-r coordinates is very simple and clear, has better steady state and dynamic performance than conventional controllers. The controlled variables in p-q-r coordinates are then inversely transformed to the original a-b-c coordinates instantaneously, generating reference compensation voltages to a DVR. The control algorithm can be used for various kinds of series compensators such as dynamic voltage restorers (DVR), series active filters (SAP), synchronous static series compensators (SSSC), and bootstrap variable inductances (BVI). The control algorithm was applied to an experimental DVR system. The experimental results verified the performance of the proposed control algorithm.","Voltage control,
Control systems,
Steady-state,
Circuit faults,
Delay effects,
Power system restoration,
Power quality,
Control engineering,
Educational institutions,
Computer science"
A novel simplex homotopic fixed-point algorithm for computation of optimal PWM patterns,"A novel simplex homotopic fixed-point algorithm to solve the optimal PWM problem is introduced in this paper. The goal of optimal PWM is to select the switching instants (angles) in such a way that a waveform with a particular characteristic is obtained and a certain criterion is minimized. The algorithms, which have been given so far in the literature to address this problem, use off-line precalculations and look-up tables. The proposed algorithm transforms the nonlinear equations to a polynomial set, uses the simplex homotopic fixed-point approach to find the initial values of the roots, and then uses cubic iteration to refine the roots. The salient features of the algorithm presented in this paper are: high accuracy, fast speed (low computational cost), and robustness even when facing ill-conditioned cases. With typical contemporary DSP, this algorithm can be used to set amplitudes of several tens of harmonics within one cycle of the AC power waveform. Since the switching angles are calculated independently from each other, the algorithm can be easily implemented for parallel processing. This would enable its use in demanding waveform processing applications.","Pulse width modulation,
Polynomials,
Nonlinear equations,
Power system harmonics,
Chebyshev approximation,
Real time systems,
Voltage,
Computer science,
Costs,
Erbium"
Best-response multiagent learning in non-stationary environments,,"Learning,
Stochastic processes,
Game theory,
Permission,
Computer science,
Autonomous agents,
Algorithm design and analysis,
Process control,
Convergence"
A regular parallel RSA processor,"High performance VLSI implementation of the RSA algorithm using the systolic array is presented. High-speed applications of RSA systems require parallel implementations of modular multipliers. Besides using the systolic architecture which is popular in hardware-based RSA systems, a block-based scheme is used to further eliminate global signals, with a pipelined bus to convey data globally. The control signals and intermediate results used for sequential multiplications are transmitted by shift registers. All signals, except for the clock signal, are limited in one block or between two adjacent blocks. A carry-save-adder structure is used for calculating the iterative step of the algorithm, which contributes to speed improvement and area saving. In addition, long modular multipliers suffer from the effect of large fanout. Novel architectures are proposed to eliminate the fanout bottleneck, which reduce the achievable minimum clock period of long modular multipliers. Compared to the original modular multiplier architecture with fanout bottleneck, the proposed architectures can achieve an increase of over 7% in throughput without increase in area. The Chinese remainder theorem (CRT) technique increases the decryption data rate by a factor of four. Two redundant blocks are added to adapt to the on-line partition of the multiplier and the variation of the length of P and Q in CRT mode.","Broadcasting,
Systolic arrays,
Clocks,
Shift registers,
Iterative algorithms,
Cathode ray tubes,
Security,
Delay,
Computer science,
Very large scale integration"
Video summarization by video structure analysis and graph optimization,"We propose a novel video summarization method that combines video structure analysis and graph optimization. First, we analyze the structure of the video, find the boundaries of video scenes, then we calculate each scene's skimming length based on its structure and content entropy. Second, we define a spatial-temporal dissimilarity function between video shots and model each video scene as a graph, then find each scene's optimal skimming in the graph with dynamic programming. Finally, the whole video's skimming is obtained by concatenating the skimmings of the scenes. Experimental results show that our approach preserves the scene level structure and ensures balanced coverage of the major contents of the original video.","Layout,
Optimization methods,
Computer science,
Entropy,
Dynamic programming,
Analytical models,
Motion analysis,
Tree data structures,
Image sequences,
Cameras"
Learning Chance Probability Functions for Shape Retrieval or Classification,"Several example-based systems for shape retrieval and shape classification directly match input shapes to stored shapes, without using class membership information to perform the matching. We propose a method for improving the accuracy of this type of system. First, the system learns a set of chance probability functions (CPFs). The CPFs estimate the probabilities of obtaining a query shape with particular distances from each training example by chance. The learned CPFs are used at runtime to rapidly estimate the chance probabilities of the observed distances between the actual query shape and the database shapes. These estimated probabilities are then used as a dissimilarity measure for shape retrieval and/or nearest-neighbor classification. The CPF learning method is parameter-free. Experimental evaluation demonstrates that: (1) chance probabilities yield higher accuracy than Euclidean distances; (2) the learned CPFs support fast matching; and (3) the CPF-based system outperforms prior systems on a standard benchmark test of retrieval accuracy.","Shape measurement,
Information retrieval,
Runtime,
Databases,
Impedance matching,
Benchmark testing,
System testing,
MPEG 7 Standard,
Computer science,
Learning systems"
Activity recognition based on multiple motion trajectories,"We propose a method for activity recognition based on multiple motion trajectories. Motion trajectories generated from body parts (hand, feet, and joints) are used as features. We not only recognize each activity but also temporally locate the start and end point of its duration. Input sequences are divided into separate temporal segments based on the number of detected trajectories. Segments with same number of trajectories are temporally segmented using the HMM model for each movement (activity). The experimental results show that our approach can successfully locate each activity in continuous video sequences.",
Parallel mining of association rules from text databases on a cluster of workstations,"Summary form only given. We propose a new algorithm named Parallel Multipass with Inverted Hashing and Pruning (PMIHP) for mining association rules between words in text databases. The characteristics of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot handle text databases efficiently because of the large number of itemsets (i.e., sets of words) that need to be counted. The new PMIHP algorithm is a parallel version of our multipass with inverted hashing and pruning (MIHP) algorithm, which was shown to be quite efficient than other existing algorithms in the context of mining text databases. The PMIHP algorithm reduces the overhead of communication between miners running on different processors because they are mining local databases asynchronously and prune the global candidates by using the inverted hashing and pruning technique.","Data mining,
Association rules,
Workstations,
Itemsets,
Transaction databases,
Clustering algorithms,
Computer science,
Data engineering,
Context,
Linux"
Quantification of ohmic and intrinsic flux losses in helical flux compression Generators,"Helical magnetic flux compression generators (MFCGs) are the most promising energy sources with respect to their current amplification and compactness. They are able of producing high current pulses required in many pulsed power applications with at least one order of magnitude higher energy density than capacitive storage with similar discharge characteristics. However, the main concern with MFCGs is their intrinsic flux loss that limits severely their performance and which is not yet well understood. In general, all flux losses have a differing degree of impact, depending on the generator's volume, current and energy amplification, size of the driven load, and angular frequency of armature-helix contact point. Although several computer models have been developed in the open literature, none of them truly quantify, starting from basic physics principles, the ohmic and intrinsic flux losses in helical MFCGs. This paper describes a novel method that provides a separate calculation of intrinsic flux losses (flux that is left behind in the conductors and lost for compression) and ohmic losses, being especially easy to implement and fast to calculate. We also provide a second method that uses a simple flux quantification, making a mathematical connection between the intrinsic flux losses, quantified by the first method, and the intrinsic flux losses observed in the generators. This second method can also be used to a priori estimate the MFCG performance. Further, we will show experimental and calculated data and discuss the physical efficiency limits and scaling of generator performance at small sizes.","Magnetic flux,
Performance loss,
Pulse amplifiers,
Conductors,
Magnetic materials,
Skin,
Energy storage,
Fault location,
Frequency,
Physics computing"
Progressive sampling schemes for approximate clustering in very large data sets,"The extensible fast fuzzy c-means algorithm (eFFCM) finds clusters in very large digital images. eFFCM identifies a representative subsample of the image, which is then clustered using the fuzzy c-means (FCM) algorithm. The subsample solution is then extended to secure an approximate clustering of the remaining pixels in the image. This article discusses generalized eFFCM (geFFCM), the extension of eFFCM to general non-image data. Our extension accelerates literal fuzzy c-means (LFCM) on all (loadable) data sets. Second, geFFCM provides feasibility - a way to find (approximate) clusters - for data sets that are too large to be loaded in a single computer. Our experiments suggest that the chi-squared or divergence test for goodness of fit alone identifies good subsamples. This new subsampling method should be equally effective for acceleration and feasibility with VL data by any extensible clustering algorithm (not just FCM).","Sampling methods,
Clustering algorithms,
Acceleration,
Pixel,
Scalability,
Digital images,
Testing,
Computer science,
Fuzzy logic,
Fuzzy sets"
Extending path profiling across loop backedges and procedure boundaries,"Since their introduction, path profiles have been used to guide the application of aggressive code optimizations and performing instruction scheduling. However, for optimization and scheduling, it is often desirable to obtain frequency counts of paths that extend across loop iterations and cross procedure boundaries. These longer paths, referred to as interesting paths, account for over 75% of the flow in a subset of SPEC benchmarks. Although the frequency counts of interesting paths can be estimated from path profiles, the degree of imprecision of these estimates is very high. We extend Ball Larus (BL) paths to create slightly longer overlapping paths and develop an instrumentation algorithm to collect their frequencies. While these paths are slightly longer than BL paths, they enable very precise estimation of frequencies of potentially much longer interesting paths. Our experiments show that the average cost of collecting frequencies of overlapping paths is 86.8%, which is 4.2 times that of BL paths. However, while the average imprecision in estimated total flow of interesting paths derived from BL path frequencies ranges from -38 % to +138 %, the average imprecision inflow estimates derived from overlapping path frequencies ranges only from -4% to +8%.","Frequency estimation,
Processor scheduling,
Instruments,
Costs,
Computer science,
Application software,
Arithmetic"
Modeling and analysis of 2D service differentiation on e-commerce servers,"A scalable e-commerce server should be able to provide different levels of quality of service (QoS) to different types of requests according to clients' navigation patterns and the server capacity. In this paper, we propose a two-dimensional (2D) service differentiation (DiffServ) model for online transactions: inter-session and intra-session. The inter-session model aims to provide different levels of QoS to sessions from different customer classes, and the intra-session model aims to provide different levels of QoS to requests in different states of a session. We introduce service slowdown as a QoS metric of e-commerce servers. It is defined as the weighted sum of request slowdown in different sessions and in different session states. We formulate the problem of 2D DiffServ provisioning as an optimization of processing rate allocation with the objective of minimizing service slowdown. We derive the optimal allocations for an M/G/1 server under various server load conditions and prove that the optimal allocations guarantees requests' slowdown to be square-root proportional to their pre-specified differentiation weights in both dimensions. We evaluate the optimal allocation scheme via extensive simulations and compare it with a tailored proportional DiffServ scheme. Simulation results validate that both allocation schemes can achieve predictable, controllable, and fair 2D slowdown differentiation on e-commerce servers. The optimal allocation scheme guarantees 2D DiffServ at a minimum cost of service slowdown.",
Susceptibility matrix: a new aid to software auditing,"Testing for security is lengthy, complex, and costly, so focusing test efforts in areas that have the greatest number of security vulnerabilities is essential. This article describes a taxonomy-based approach that gives an insight into the distribution of vulnerabilities in a system.","Computer security,
Information security,
Computer science,
Taxonomy,
Computer errors,
Privacy,
Software testing,
Operating systems,
Software systems,
Bars"
MADF: Mobile-assisted data forwarding for wireless data networks,"In a cellular network, if there are too many data users in a cell, data may suffer long delay, and system's quality-of-service (QoS) will degrade. Some traditional schemes such as dynamic channel-allocation scheme (DCA) will assign more channels to hot (or overloaded) cells through a central control system (CC) and the throughput increase will be upper bounded by the number of new channels assigned to the cell. In mobile-assisted data forwarding (MADF), we add an ad-hoc overlay to the fixed cellular infrastructure and special channels-called forwarding channels-are used to connect mobile units in a hot cell and its surrounding cold cells without going through the hot cell's base station. Thus, mobile units in a hot cell can forward data to other cold cells to achieve load balancing. Most of the forwarding-channel management work in MADF is done by mobile units themselves in order to relieve the load from the CC. The traffic increase in a certain cell will not be upper bounded by the number of forwarding channels. It can be more if the users in hot cell are significantly far away from one another and these users can use the same forwarding channels to forward data to different cold neighboring cells without interference. We find that, in a system using MADF, under a certain delay requirement, the throughput in a certain cell or for the whole network can be greatly improved.","Delays,
Base stations,
Mobile communication,
Throughput,
Ad hoc networks,
Time division multiple access,
Wireless communication"
Towards a personalized e-learning scheme for teachers,"One of the most important topics in modern e-learning schemes for teachers is the treatment of information and users at a personalized level. In this framework, the automated extraction of user profiles from an e-learning system is an interesting and important problem. In this paper we present the designing and materializing of such a profile-based system for teachers, which extends on previous work on profile extraction and re-evaluation in the direction of automated extraction of user preferences. Our approach relies on suitably adapted fundamental e-learning models, such as the IEEE e-learning model, as well as on a novel mechanism that creates, updates and uses users' profiles.","Electronic learning,
Data mining,
Education,
Feedback,
Data analysis,
Web server,
Educational technology,
Computer science,
Communications technology,
Internet"
Scalable packet digesting schemes for IP traceback,"Identifying the sources of an attack is an important task in the Internet security area. An attack could consist of a large number of packet streams generated by many compromised slaves that consume resources associated with various network elements to deny normal services or a few offending packets to disable a system. Several techniques based on probabilistic samples of transit packets have been developed, to determine the sources of large packet flows. It seems that logging of packet digests is necessary for traceback of an individual packet. A clever technique based on Bloom filters has recently been proposed to generate the audit trails for each individual packet within the network. The scheme is effective. However, the storage requirement is approximately 0.5% of the link capacity, which becomes a problem as link capacity increases. In this paper, we propose packet digesting schemes for flows and sets of packets sharing the same source and destination addresses. Compared with the individual packet digesting scheme, these schemes can achieve similar goals and are much more scalable. Simulations with real Internet traffic show that the storage requirements of our proposed schemes are one to two orders of magnitude lower.","Filtering,
Electronic mail,
IP networks,
Telecommunication traffic,
Computer crime,
Protocols,
Computer science,
Internet,
Communication system security,
Computer security"
On the variable-delay reliability function of discrete memoryless channels with access to noisy feedback,"We give a scheme for variable-delay reliable communication over a noisy discrete memoryless channel with a noisy feedback DMC being available between the receiver and the transmitter. This scheme works when the capacity of the feedback channel is greater than the capacity of the forward channel and the target rate of communication on the forward channel is less than its capacity. Rather than looking at a single block in isolation, we consider the scenario where we expect this system to carry an infinite stream of packets. We show that in the limit of nearly noiseless feedback channels, we approach the Burnashev bound on the reliability of variable-delay channel codes.","Feedback,
Additive white noise,
Gaussian noise,
Memoryless systems,
Decoding,
Transmitters,
Computer science,
Communication systems,
Information theory,
Computer hacking"
Real-time video delivery using peer-to-peer bartering networks and multiple description coding,"Broadband Internet access using ADSL or cable modems provides sufficient bandwidth for real-time video streaming. If television channels could be distributed on the Internet, each channel would get a world wide audience. However, television distribution from a single sender does not scale and IP-level multicasting is complex and costly. We propose a solution based on application-level multicasting using a P2P network, called P2P-TV. Each peer receives a video stream and must also forward this stream to others. This work presents an architecture for streaming video. Our P2P-TV proposal aims to solve three problems that are currently not addressed in other P2P streaming proposals, namely 1) maximizing the usage of all available peer bandwidth, 2) taking current network conditions into account (network awareness), and 3) the freeriding problem. Multiple description coding is an integral part of our solution. By splitting the video stream into smaller streams we can utilize all the bandwidth of a peer. Multiple streams allow more efficient adaptation of the multicast tree to current network conditions. Bittorrent-like bartering of content is also enabled by using multiple streams.",
A performance evaluation tool for RAID disk arrays,"While the performance of individual disks is well understood, the performance of disk arrays configured as redundant arrays of independent disks - RAID requires further investigation. We report on a tool that can be used for the performance evaluation of k disk failure tolerant (kDFT) disk arrays. RAID0 is a 0DFT. RAID5 is a 1DFT utilizing a single check disk. RAID6, EVENODD, and RDP are 2DFTs with two check disks and a minimum level of redundancy, while RM2 is a 2DFT with a slightly higher redundancy level. The capacity overhead is less important for high capacity modern disks than the disk access overhead to update parities, especially the small write penalty. Degraded mode operation and rebuild processing incur extra overhead on surviving disks. The RAID level, configuration, operating mode (normal, degraded with 1 or 2 failures, and rebuild mode with various options), and workload characteristics have a significant effect on RAID performance, which can be evaluated using our tool. We consider here only discrete requests and derive cost functions to determine: (a) the volume of data to be transmitted; (b) the maximum attainable throughput for given disk characteristics; (c) the mean response time given the execution plans of RAID operations specified as dags (directed acyclic graphs). Analytic solutions are possible when arrivals are Poisson with the FCFS policy. Some interesting results obtained by the tool are given to illustrate its capabilities.","Redundancy,
Degradation,
Protection,
Computer science,
Cost function,
Throughput,
Delay,
Explosions,
Storage automation,
Capacity planning"
Coordinated views to assist exploration of spatio-temporal data: a case study,"In this paper we focus on providing coordinated visual strategies to assist users in performing tasks driven by the presence of temporal and spatial attributes. We introduce temporal visualization techniques targeted at such tasks, and illustrate their use with an application involving a climate classification process. The climate classification requires extensive processing of a database containing daily rain precipitation values collected along over fifty years at several spatial locations in the Sao Paulo state, Brazil. We identify user exploration tasks typically conducted as part of the data preparation required in this process, and then describe how such tasks may be assisted by the multiple visual techniques provided. Issues related to the use of the multiple techniques by an end-user are also discussed.","Computer aided software engineering,
Data visualization,
Spatial databases,
Visual databases,
Rain,
Data analysis,
Computer science,
Face,
Geographic Information Systems,
Information systems"
SOLONet: sub-optimal location-aided overlay network for MANETs,"Overlay networks have made it easy to implement multicast functionality in wireless ad hoc networks. Their flexibility to adapt to different environments has helped in their steady growth. In MANET, the position of nodes constantly changes; as a result, overlay multicast trees that are built using location information to account for node movement would certainly have a low latency. However, the performance gains of such a tree are offset by the overhead involved in maintaining precise location information. As the degree of (location) accuracy increases, the performance improves but the overhead required to store and broadcast this information also increases. In this paper, we present SOLONet, a design to build a sub-optimal location aided overlay multicast tree, where location updates of each member node are event based. Our simulation results indicate that such a sub-optimal tree does not compromise the performance gains of a location aided overlay multicast tree.","Mobile ad hoc networks,
Broadcasting,
Delay,
Multicast protocols,
Computer science,
Performance gain,
Network topology,
Computer hacking,
Unicast,
Application software"
An on-line evaluation system for optical see-through augmented reality,This work introduces a technique that allows final users to evaluate and recalibrate their AR system as frequently as needed. We developed an interactive game as a prototype for such evaluation system and explain how this technique can be implemented to be used in real life.,"Augmented reality,
Personal digital assistants,
Optical feedback,
Calibration,
Mice,
Displays,
Virtual reality,
Games,
Computer science,
Educational institutions"
An automated face reader for fatigue detection,"An automated system for facial expression recognition is always desirable. However, it is a challenging issue due to the richness and ambiguities with daily facial expressions. This paper presents an efficient approach to recognition of facial expressions of interest. By integrating dynamic Bayesian network (DBN) with the general facial expression language (FACS), a task-oriented stochastic and temporal framework is constructed to systematically represent and recognize facial expressions. Based on the DBN, analysis results from previous periods and prior knowledge of the application domain can be integrated both spatially and temporally. With the top-down inference, the system can make dynamic and active selection among multiple sensing channels so as to achieve efficient recognition. With the bottom-up inference from observed evidences, the current facial expression can be classified with a desired confident level via belief propagation. We apply this task-oriented framework to fatigue facial expression analysis. Experimental results verify the high efficiency of our approach.","Face detection,
Fatigue,
Hidden Markov models,
Face recognition,
Bayesian methods,
Displays,
Gold,
Computer science,
Stochastic systems,
Belief propagation"
Cellz: a simple dynamic game for testing evolutionary algorithms,"The game of Cellz has been designed as a test bed for evolutionary algorithms. The game has a minimal set of rules that nonetheless offer the possibility for complex behaviour to emerge. Computationally, the game is cheap to simulate, which leads to rapid runs of evolutionary algorithms. A key feature of the game is the cell division process, which can lead to evolution in situ without reference to any externally defined fitness function. This paper describes the rationale behind the development of Cellz, the rules of the game and the software interfaces for the cell controllers. The randomness in the game initialisation leads to extremely noisy fitness functions, which adds to the challenge of evolving high-performance controllers. Initial results demonstrate that an evolved perceptron-type controller can achieve mediocre performance on the single species game.","Testing,
Evolutionary computation,
Physics,
Automata,
Computer science,
Algorithm design and analysis,
Computational modeling,
Genetics,
Feedforward systems,
Neural networks"
Content repurposing,,"Space technology,
Multimedia computing,
Humans,
Food technology,
Bandwidth,
Computer science,
Wireless networks,
Computer Society,
Video recording,
Digital cameras"
Lecture videos for e-learning: current research and challenges,"E-learning is an emerging new education approach that augments learning experiences by integrating multimedia and network technologies. As an integrated part of e-learning, the lecture videos captured in classrooms contain most of the instructional content. Effective use of these videos, however, remains a challenging task. This paper reviews previous research work on capturing, analyzing, indexing, and retrieval of lecture (instructional) videos, and introduces on-going research efforts related with instructional videos. This paper compares instructional video to other video genres and addresses its special issues and difficulties. We present the current challenges in content-based indexing and retrieval of instructional videos. Improving these techniques for lecture videos has significant educational and social benefits.","Videos,
Electronic learning,
Streaming media,
Computer aided instruction,
Indexing,
Internet,
Computer science,
Educational technology,
Content based retrieval,
Multimedia systems"
A Shadow Based Method for Image to Model Registration,"This paper presents a novel method for 2D to 3D texture mapping using shadows as cues. This work is part of a larger set of methods that address the entire 3D modeling pipeline to create geometrically and photometrically accurate models using a variety of data sources. The focus is on building models of large outdoor, urban, historic and archaeological sites. We pose registration of 2D images with the 3D model as an optimization problem that uses knowledge of the Sun's position to estimate shadows in a scene, and use the shadows produced as a cue to refine the registration parameters. Results are presented for registration where ground truth is known and also for a large scale model consisting of 14 3D scans and 10 images on a large archaeological site in Sicily.","Solid modeling,
Geographic Information Systems,
Pipelines,
Data visualization,
Photometry,
Layout,
Computer vision,
Computer science,
Refining,
Large-scale systems"
Performance analysis of energy consumption in 3GPP networks,"Energy conservation is critical for third generation (3G) cellular systems. In particular, future high-speed packet data services are expected to exhaust the energy of a mobile station rapidly. In 3GPP systems, the radio resource is mainly controlled by the radio resource control (RRC). This paper quantifies the energy consumption and the reconnection ratio for 3GPP systems based on the RRC state machine. Bursty and streaming packet data services are investigated. Through analytic models and comprehensive simulation with various traffic models, the impacts of diverse timer values on the RRC state machine are illustrated. In addition to quantifying energy consumption, appropriate timer values are suggested.","Performance analysis,
Energy consumption,
Intelligent networks,
Energy conservation,
Analytical models,
Radio control,
Control systems,
Circuits,
Downlink,
Computer science"
"MVC model, struts framework and file upload issues in web applications based on J2EE platform","This work describes the web applications based on the J2EE platform indicating the model-view-controller (MVC) model, struts framework and file upload issues. The development of internet technology and the will to standardize the mechanisms used in implementation of internet applications was the basis background for the development of J2EE platform. The project of international office TUL internet service is an application based on the above mentioned environment. The specification of the project is closely connected with the activities and needs of the staff of the international office. The article presents the use of MVC model on J2EE platform on an example of open source Apache/Tomcat server and the database management system MySql.",
GPER: geographic power efficient routing in sensor networks,"This work presents a new geographical power efficient routing (GPER) protocol for sensor networks. Each sensor node makes local decisions as to how far to transmit: therefore, the protocol is power efficient, highly distributed and scalable. In GPER, given a final destination, each node first establishes a sub-destination within its maximum radio range. The node, however, may decide to relay the packet to this sub-destination through an intermediary node, if this preserves power. Furthermore, this intermediary node may act independently and alter the subdestination based on its own power range and neighborhood status. Simulation results show that the routing power consumption using GPER is close to optimal obtainable based on full knowledge of the network. GPER provides 60%-90% savings over other power-sensitive routing solutions. For sensor networks with highly varying node densities, we propose an extension, GPER-2, which captures the network topology better. Simulations show that although GPER works well, GPER-2 can improve on GPER up to 20%, especially when variations are large.","Intelligent networks,
Wireless sensor networks,
Routing protocols,
Energy consumption,
Intelligent sensors,
Relays,
Network topology,
Computer science,
Electronic mail,
Personal area networks"
Scheduling real time parallel structures on cluster computing with possible processor failures,"Efficient task scheduling is essential for achieving high performance computing applications for distributed systems. Most of existing real-time systems consider schedulability as a main goal and ignores other effects such as machines failures. In This work we develop an algorithm to efficiently schedule parallel task graphs (fork-join structures). Our scheduling algorithm considers more than one factor at the same time. These factors are scheduability, reliability of the participating processors and achieved degree of parallelism. To achieve most of these goals, we composed an objective function that combines these different factors simultaneously. The proposed objective function is adjustable to provide the user with a way to prefer one factor to the others. The simulation results indicate that our algorithm produces schedules where the applications deadlines are met, reliability is maximized and the application parallelism is exploited.","Processor scheduling,
Concurrent computing,
Application software,
Real time systems,
Scheduling algorithm,
Parallel processing,
Telecommunication network reliability,
Computer science,
High performance computing,
Clustering algorithms"
Towards adaptive intrusion detection in mobile ad hoc networks,"One of the main challenges in budding intrusion detection systems (IDSs) for mobile ad hoc networks (MANETs) is to integrate the mobility impact and adjust the behavior of IDSs dynamically. In this paper, focusing on the protection of MANET routing protocols, we first demonstrate that nodes' moving speed, a commonly used parameter in tuning IDS performance, is not an effective metric for the performance measurement of IDSs for MANETs. We then propose a new feature - the link change rate, which can not only act as a unified metric in measuring MANET IDS performance, but also be used to facilitate local MANET IDSs to select normal profiles adaptively. We utilize different mobility models to study the performance of our proposed adaptive mechanisms at different mobility levels. Simulation results show that our proposed adaptive mechanisms are effective and less dependent on mobility models. Detailed analysis of simulation results is also provided.","Intrusion detection,
Intelligent networks,
Mobile ad hoc networks,
Routing,
Peer to peer computing,
Computer science,
Velocity measurement,
Ad hoc networks,
Sun,
Protection"
Predictive scheduling in multi-carrier wireless networks with link adaptation,"Channel-aware scheduling and link adaptation methods are widely considered to be crucial for realizing high data rates in wireless networks. However, predicting future channel states, and adjusting transmission schedules and parameters accordingly, may consume valuable system resources, such as bandwidth, time, and power. The paper considers the trade-offs between prediction quality and throughput in a wireless network that uses link adaptation and channel-aware scheduling. In particular, we study the effects on the throughput of the look-ahead window, i.e., the range of future time slots on which we have channel state estimates, and the reliability of the channel state estimates. We develop an online scheduling algorithm for a multichannel multiuser network that employs predictive link adaptation, and generalize it to incorporate imperfect channel state estimates. We apply this heuristic together with performance bounds to the offline version of the problem to evaluate the performance with varying prediction qualities. Our results suggest that it may be possible to reap most of the potential channel-aware scheduling benefits with a small look-ahead and imperfect channel state estimates. Thus, a modest consumption of resources for channel prediction and link adaptation may result in a significant throughput improvement, with only marginal gains through further enhancement of the prediction quality. Our results can provide meaningful guidelines in deciding what level of system resource consumption is justified for channel quality estimation and link adaptation.","Intelligent networks,
Wireless networks,
Throughput,
State estimation,
Scheduling algorithm,
Bandwidth,
Processor scheduling,
Guidelines,
Computer science,
Downlink"
Extracting subtle facial expression for emotional analysis,"Humans convey emotions with subtle facial expressions. The interpretation of these expressions not only depends on that expression, but on how it developed. Exposing this development enables a better analysis of the ambiguity experienced when analyzing the underlying emotion. Subtle facial expressions can be objectively measured in terms of action units as defined in the facial action coding system. This paper presents a facial expression language based on FACS which provides a continues stream of expressions for measuring the growth of expression. The development of the emotion surprise which requires two action units is used as a test case","Gold,
Humans,
Psychology,
Computer science,
Decoding,
Large-scale systems,
Feeds,
Displays,
Area measurement,
Costs"
Remote repair of operating system state using Backdoors,"Backdoors is a novel system architecture that enables remote monitoring and recovery/repair of the software state of a computer system without using its processors or relying on its OS resources. We have implemented a Backdoors prototype in the FreeBSD kernel using Myrinet NICs for remote access to the target machine. In a previous paper, we have shown how Backdoors can be used for recovery of ""good"" OS and application state from a failed system on other healthy systems. In this paper, we describe how Backdoors can be used to detect and repair damage to the OS state of a computer system. We present two case studies of remote repair of an OS subject to resource depletion (fork bomb and memory hog) to the point where it cannot perform useful work and local repair is impossible. We show that our prototype detects OS resource exhaustion efficiently and it successfully repairs the affected system.","Operating systems,
Remote monitoring,
Computerized monitoring,
Condition monitoring,
Computer science,
Computer architecture,
Prototypes,
Educational institutions,
Software prototyping,
Kernel"
DICOM data handling for Geant4-based medical physics application,"We developed a DICOM interface for Geant4-based medical physics applications. Geant4 simulation toolkit has been adopted to radiotherapy, hadrontherapy, PEM, PET, MRT and so on, in purpose of physical full-simulation of accurate particle propagation and interaction with not only medical devices but also patient tissues. DICOM is commonly utilized to transport digital images such as patient image, information and so on. We succeeded in retrieving various manufacturers' DICOM data stream, constructing Geant4 geometry and simulating particle interaction with patient tissues in a Geant4 application with the DICOM interface. In addition, the DICOM interface can store outcomes of the simulation successively as a DICOM data stream; consequently commercially or freely available DICOM viewers come to be usable to visualize results. In this paper, we describe feature and availability of the DICOM interface.","DICOM,
Data handling,
Physics,
Biomedical imaging,
Medical simulation,
Streaming media,
Positron emission tomography,
Digital images,
Information retrieval,
Virtual manufacturing"
Advanced slicing of sequential and concurrent programs,"Program slicing is a technique to identify statements that may influence the computations in other statements. Despite the ongoing research of almost 25 years, program slicing still has problems that prevent a widespread use: Sometimes, slices are too big to understand and too expensive and complicated to be computed for real-life programs. The presented thesis shows solutions to these problems: It contains various approaches which help the user to understand a slice more easily by making it more focused on the user's problem. All of these approaches have been implemented in the VALSOFT system and thorough evaluations of the proposed algorithms are presented. The underlying data structures used for slicing are program dependence graphs. They can also be used for different purposes: A new approach to clone detection based on identifying similar subgraphs in program dependence graphs is presented; it is able to detect modified clones better than other tools. In the theoretical part, this thesis presents a high-precision approach to slice concurrent procedural programs despite the fact that optimal slicing is known to be undecidable. It is the first approach to slice concurrent programs that does not rely on inlining of called procedures.","Cloning,
Data structures,
Programming profession,
Computer science,
Concurrent computing,
Java,
User interfaces,
Software maintenance,
Data mining,
Data flow computing"
A modeling approach for burn scar assessment using natural features and elastic property,"A modeling approach is presented for quantitative burn scar assessment. Emphases are given to: 1) constructing a finite-element model from natural image features with an adaptive mesh and 2) quantifying the Young's modulus of scars using the finite-element model and regularization method. A set of natural point features is extracted from the images of burn patients. A Delaunay triangle mesh is then generated that adapts to the point features. A three-dimensional finite-element model is built on top of the mesh with the aid of range images providing the depth information. The Young's modulus of scars is quantified with a simplified regularization functional, assuming that the knowledge of the scar's geometry is available. The consistency between the relative elasticity index and the physician's rating based on the Vancouver scale (a relative scale used to rate burn scars) indicates that the proposed modeling approach has high potential for image-based quantitative burn scar assessment.","Elasticity,
Finite element methods,
Magnetic resonance imaging,
Mesh generation,
Skin,
Computer science,
Magnetic noise,
Biomedical measurements,
Motion measurement,
Ultrasonic variables measurement"
Single-dimension software pipelining for multi-dimensional loops,"Traditionally, software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops. We propose a three-step approach, called single-dimension software pipelining (SSP), to software pipeline a loop nest at an arbitrary loop level. The first step identifies the most profitable loop level for software pipelining in terms of initiation rate or data reuse potential. The second step simplifies the multidimensional data-dependence graph (DDG) into a 1-dimensional DDG and constructs a 1-dimensional schedule for the selected loop level. The third step derives a simple mapping function which specifies the schedule time for the operations of the multidimensional loop, based on the 1-dimensional schedule. We prove that the SSP method is correct and at least as efficient as other modulo scheduling methods. We establish the feasibility and correctness of our approach by implementing it on the IA-64 architecture. Experimental results on a small number of loops show significant performance improvements over existing modulo scheduling methods that software pipeline a loop nest from the innermost loop.",
MODD: a new decision diagram and representation for multiple output binary functions,"This paper presents a new decision diagram (DD), called MODD, for multiple output binary and multiple-valued functions. This DD is canonic and can be made minimal with respect to a given variable order. Unlike other reported DDs, our approach can represent arbitrary combination of bits at the word-level. The preliminary results show that our representation can result in considerable memory saving.","Galois fields,
Boolean functions,
Circuits,
Computer science,
Data structures,
Algebra,
Binary decision diagrams,
Design automation,
Automatic testing,
Europe"
Evidence of the need for social intelligence in rescue robots,"This study investigates data collected from operating an Inuktun robot in an urban search and rescue (USAR) confined space training exercise task at Virginia Beach Training Center. Data was collected from coding approximately one hour of video. The video had no sound so all analysis is based on the video feed. Indicators of communication, gestures, physical interactions with the robot, and robot movements were analyzed. The findings indicate that the robot emerges as a virtual presence for the support of the team outside of the confined space. The team members spontaneously responded socially to the robot despite the robot not being engineered to have a social intelligence. This confirms numerous studies in the cognitive science, psychology, and affective computing literature that robots need a social interface regards of domain.","Intelligent robots,
Orbital robotics,
Human robot interaction,
Mobile robots,
Plasma welding,
Intelligent structures,
Psychology,
Computer interfaces,
Humanoid robots,
Anthropomorphism"
Breeding software test cases for complex systems,"The potential cost savings from handling software errors within a development cycle, rather than subsequent cycles, has been estimated at 38.3 billion dollars. Such figures emphasize that current testing methods are inadequate, and that helping reduce software bugs and errors is an important area of research with a substantial payoff. This paper reports on research using genetic algorithms for test case generation for systems level testing, building on past work at the unit testing level. The goals of the paper are to explore the use of genetic algorithms for testing complex distributed systems, as well as to develop a framework or vocabulary of important environmental attributes that characterize complex systems failures. In addition, preliminary visualization techniques that might help software developers to understand and uncover complex systems failures are explored.","Software testing,
System testing,
Computer aided software engineering,
Costs,
Automatic testing,
Computer bugs,
Genetic algorithms,
Humans,
Productivity,
Vocabulary"
Dynamic transitive closure via dynamic matrix inverse: extended abstract,"We consider dynamic evaluation of algebraic functions such as computing determinant, matrix adjoint, matrix inverse and solving linear system of equations. We show that in the dynamic setup the above problems can be solved faster than evaluating everything from scratch. In the case when rows and columns of the matrix can change we show an algorithm that achieves O(n/sup 2/) arithmetic operations per update and O(1) arithmetic operations per query. Next, we describe two algorithms, with different tradeoffs, for updating the inverse and determinant when single entries of the matrix are changed. The fastest update for the first tradeoff is O(n/sup 1.575/) arithmetic operations per update and O(n/sup 0.575/) arithmetic operations per query. The second tradeoff gives O(n/sup 1.495/) arithmetic operations per update and O(n/sup 1.495/) arithmetic operations per query. We also consider the case when some number of columns or rows can change. We use dynamic determinant computations to solve the following problems in the dynamic setup: computing the number of spanning trees in a graph and testing if an edge in a graph is contained in some perfect matching. These are the first dynamic algorithms for these problems. Next, with the use of dynamic matrix inverse, we solve fully dynamic transitive closure in general directed graphs. The bounds on arithmetic operations for dynamic matrix inverse translate directly to time bounds for dynamic transitive closure. Thus we obtain the first known algorithm with O(n/sup 2/) worst-case update time and constant query time and two algorithms for transitive closure in general digraphs with subquadratic update and query times. Our algorithms for transitive closure are randomized with one-sided error. We also consider for the first time the case when the edges incident with a part of vertices of the graph can be changed.","Arithmetic,
Equations,
Heuristic algorithms,
Linear systems,
Vectors,
Informatics,
Tree graphs,
Testing,
Tin,
Data preprocessing"
Community-driven adaptation: automatic content adaptation in pervasive environments,"Mobile devices are increasingly being used to access Web content but lack the resources for proper presentation to the user. To address this problem, content is typically adapted to be more suitable for a mobile environment. Community-driven adaptation (CDA) is a novel approach to automatic content adaptation for mobile devices that adapts content based on feedback from users. CDA groups users into communities based on common characteristics, and assumes that users of the same community have similar adaptation requirements. CDA learns how to adapt content by observing how members of a community alter adapted content to make it more useful to them. Experiments that consider the idealized case, where all users perform the same task, show that CDA can reduce wastage of network bandwidth by up to 90% and requires less user interaction to correct bad adaptation decisions compared with existing approaches to automatic content adaptation.","Mobile computing,
Feedback,
Bandwidth,
Personal digital assistants,
Computer displays,
Computer science,
Cellular phones,
Pervasive computing,
User interfaces,
Computer interfaces"
Power-aware on-demand routing protocol for MANET,"We present a power-aware on-demand routing protocol called PAOD, which aims to maximize the system lifetime of MANET. In PAOD, the source is able to anticipate the traffic of the request, and each node on the route makes energy reservation according to the anticipated traffic. Since a node knows not only its physical residual energy, but also the expected energy possibly to be consumed in the future, it will report its energy status more accurately. A new cost function is used in route selection, which takes both shortest-hop and maximum-lifetime into consideration. Some filter mechanisms are proposed based on an energy threshold function, which can remarkably reduce the overhead costs, and improve the performance of PAOD. A novel metric - the standard deviation of node energy is used in the simulation experiments to evaluate the balance of energy consumption in the candidate protocols. Simulation results show that, the energy consumption in PAOD is more balanced than in the common on-demand protocols, and PAOD is with much longer system lifetime, along with higher delivery rate than the common ones.","Routing protocols,
Mobile ad hoc networks,
Atherosclerosis,
Cost function,
Energy consumption,
Base stations,
Energy efficiency,
Network topology,
Computer science,
High performance computing"
On the implementation of an efficient FPGA-based CFAR processor for target detection,"Real-time performance of adaptive digital signal processing algorithms is required in many applications but it often means a high computational load for many conventional processors. In this paper, we present a configurable hardware architecture for adaptive processing of noisy signals for target detection based on Constant False Alarm Rate (CFAR) algorithms. The architecture has been designed to deal with parallel/pipeline processing and to be configured for three version of CFAR algorithms, the Cell-Average, the Max and the Min CFAR. The proposed architecture has been implemented on a Field Programmable Gate Array (FPGA) device providing good performance improvements over software implementations. FPGA implementation results are presented and discussed.","Object detection,
Signal processing algorithms,
Computer architecture,
Hardware,
Algorithm design and analysis,
Field programmable gate arrays,
Working environment noise,
Backscatter,
Background noise,
Digital signal processing"
Highly reliable analog MEMS varactors,"In this paper we report experimental results on the capacitive range, stability and reliability of low-cost and high-performance analog MEMS varactors. Our varactors are contact-less, parallel-plate devices with a tuning range of nearly 3:1 and high maximum capacitance value of 272 fF. Total lack of ohmic and dielectric contacts considerably enhances the robustness of our proposed varactor, which does not require a hermetically sealed package and can be therefore produced at a very low cost. Reliability measurements of package-free varactors exposed to a regular laboratory environment for over three months are presented. Furthermore, non-linear effects for structures undergoing large deflections are experimentally addressed in this paper by demonstrating optimized varactors with nearly-ideal C - V curves.","Micromechanical devices,
Varactors,
Capacitance,
Packaging,
Hermetic seals,
Radiofrequency microelectromechanical systems,
Microswitches,
Analog computers,
Computer science,
Stability"
Automatic generation of noise-free time-activity curve with gated blood-pool emission tomography using deformation of a reference curve,"This paper describes a new method for assessing clinical parameters from a noisy regional time-activity curve (TAC) in tomographic gated blood-pool ventriculography. This method is based on a priori knowledge on the shape of a TAC, and shape approximation. The rejection method was used to generate different random Poisson deviates, covering standard count levels, of six representative TACs in order to test and compare the proposed method with harmonic and multiharmonic reconstruction methods. These methods were compared by evaluating four clinical parameters: time of end systole, amplitude, peak ejection and filling rates. Overall, the accuracy of assessment of these parameters was found to be better with the method described in this paper than with standard multiharmonic fits.","Noise generators,
Tomography,
Shape,
Filling,
Motion analysis,
Volume measurement,
Biophysics,
Nuclear medicine,
Performance analysis,
Filtering"
Chebyshev series for designing RF pulses employing an optimal control approach,"Magnetic resonance imaging (MRI) provides bidimensional images with high definition and selectivity. Selective excitations are achieved applying a gradient and a radio frequency (RF) pulse simultaneously. They are modeled by the Bloch differential equation, which has no closed-form solution. Most methods for designing RF pulses are derived from approximation of this equation or are based on iterative optimization methods. The approximation methods are only valid for small tip angles and the optimization-based algorithms yield better results, but they are computationally intensive. To improve the solutions and to reduce processing time, a method for designing RF pulses using a pseudospectral approach is presented. The Bloch equation is expanded in Chebyshev series, which can be solved using a sparse linear algebraic system. The method permits three different formulations derived from the optimal control theory, minimum distance, minimum energy, or minimum time, which are solved as algebraic constrained minimization problems. The results were validated through simulated and real experiments of 90/spl deg/ and 180/spl deg/ RF pulses. They show improvements compared to the corresponding solutions obtained using the Shinnar-Le Roux method. The minimum time formulation produces the best performance for 180/spl deg/ pulses, reducing the excitation length in 4% and the RF pulse energy in 3%.","Chebyshev approximation,
Radio frequency,
Optimal control,
Magnetic resonance imaging,
Design methodology,
Optimization methods,
Differential equations,
Closed-form solution,
Iterative methods,
Approximation methods"
An efficient data dissemination method in wireless sensor networks,"We propose a novel data dissemination scheme in wireless sensor networks which achieves energy efficiency by avoiding query flooding and extra data transmission on data storage elsewhere. Moreover, our method achieves further energy efficiency by reducing dissemination of redundant data. In addition, services for data retrieval in multiple levels of detail are provided. Our scheme consists of a localized data-source-forming algorithm and a hierarchical multi-resolution querying scheme which includes a localized node-selection algorithm and a set of registration points (geographical locations) for each type of task. Simulation results show that the performance of our method is much better than early methods like directed diffusion.","Intelligent networks,
Wireless sensor networks,
Energy efficiency,
Information retrieval,
Temperature sensors,
Sensor phenomena and characterization,
Energy storage,
Computer science,
Memory,
Temperature measurement"
Geometrical Computations Explain Projection Patterns of Long-Range Horizontal Connections in Visual Cortex,"Neurons in primary visual cortex respond selectively to oriented stimuli such as edges and lines. The long-range horizontal connections between them are thought to facilitate contour integration. While many physiological and psychophysical findings suggest that collinear or association field models of good continuation dictate particular projection patterns of horizontal connections to guide this integration process, significant evidence of interactions inconsistent with these hypotheses is accumulating. We first show that natural random variations around the collinear and association field models cannot account for these inconsistencies, a fact that motivates the search for more principled explanations. We then develop a model of long-range projection fields that formalizes good continuation based on differential geometry. The analysis implicates curvature(s) in a fundamental way, and the resulting model explains both consistent data and apparent outliers. It quantitatively predicts the (typically ignored) spread in projection distribution, its nonmonotonic variance, and the differences found among individual neurons. Surprisingly, and for the first time, this model also indicates that texture (and shading) continuation can serve as alternative and complementary functional explanations to contour integration. Because current anatomical data support both (curve and texture) integration models equally and because both are important computationally, new testable predictions are derived to allow their differentiation and identification.",
Overdetermined least-squares aberration estimates using common-midpoint signals,"As medical ultrasound imaging moves to larger apertures and higher frequencies, tissue sound-speed variations continue to limit resolution. In geophysical imaging, a standard approach for estimating near-surface aberrating delays is to analyze the time shifts between common-midpoint signals. This requires complete data-echoes from every source/receiver pair in the array. Unfocused common-midpoint signals remain highly correlated in the presence of delay aberrations; there is also tremendous redundancy in the data. In medical ultrasound, this technique has been impaired by the wide-angle, random-scattering nature of tissue. This has made it difficult to estimate azimuth-dependent aberration profiles or to harness the full redundancy in the complete data. Prefiltering the data with two-dimensional fan filters mitigates these problems, permitting highly overdetermined, least-squares solutions for the aberration profiles at many steering angles. In experiments with a tissue-mimicking phantom target and silicone rubber aberrators at nonzero stand-off distances from a one-dimensional phased array, this overdetermined, fan-filtering algorithm significantly outperformed other phase-screen algorithms based on nearest-neighbor cross-correlation, speckle brightness maximization, and common-midpoint signal analysis. Our results imply that there is still progress to be made in imaging with single-valued focusing operators. It also appears that the signal-to-noise penalty for using complete data sets is partially compensated by the overdetermined nature of the problem.","Ultrasonic imaging,
Biomedical imaging,
High-resolution imaging,
Delay estimation,
Signal analysis,
Phased arrays,
Acoustic imaging,
Apertures,
Frequency,
Image resolution"
Unilateral fixtures for sheet-metal parts with holes,"In this paper, we introduce unilateral fixtures , a new class of fixtures for sheet-metal parts with holes. These fixtures use cylindrical jaws with conical grooves that facilitate part alignment; each jaw provides the equivalent of four point contacts. The fixtures are unilateral in the sense that their actuating mechanisms are restricted to one side/surface of the part, facilitating access to the other side/surface for assembly or inspection. We present a two-phase algorithm for computing unilateral fixtures. Phase I is a geometric algorithm that assumes the part is rigid and applies two-dimensional (2-D) and three-dimensional (3-D) kinematic analysis of form closure to identify all candidate locations for pairs of primary jaws. We prove three new grasp properties for 2-D and 3-D grips at concave vertices and define a scale-invariant quality metric based on the sensitivity of part orientation to infinitesimal relaxation of jaw position. Phase II uses a finite element method to compute part deformation and to arrange secondary contacts at part edges and interior surfaces. For a given sheet-metal part, given as a 2-D surface embedded in 3-D with e edges, n concavities and m mesh nodes, Phase I takes O(e+n/sup 4/3/log/sup 1/3/n+glogg) time to compute a list of g pairs of primary jaws ranked by quality. Phase II computes the location of r secondary contacts in O(grm/sup 3/) time. Note to Practitioners-This paper was motivated by the problem of holding sheet-metal parts for automobile bodies but it also applies to other sheet-metal components that have cut or stamped holes. Existing approaches to fixturing such parts generally have contacting mechanisms on both sides of the sheet that restrict access for welding or inspection. This paper suggests a new approach using pairs of grooved cylinders, activated from only one side of the part (hence ""unilateral""). These cylinders mate with opposing corners of holes in the sheet and push apart to hold the sheet in tension, thus acting as both locators and clamps. In this paper, we mathematically characterize the mechanics and conditions for a unilateral fixture to hold a given part. We then show how such fixtures can be efficiently computed; this can allow a computer-aided design (CAD) system (with finite element capability) to automatically generate and propose unilateral fixtures for a given part. Preliminary physical experiments suggest that this approach is feasible but it has not yet been incorporated into a CAD system nor tested in production. In future research, we will address the design of unilateral fixtures that hold two or more parts simultaneously for welding.",
Scaling all-to-all multicast on fat-tree networks,"In this paper, we study the all-to-all multicast operation. Strategies for all-to-all multicast need to be different for small and large messages. For small messages, the major issue is the minimization of software overhead, where as for large messages, the issue is network contention. Many modern large parallel computers use the fat-tree interconnection topology. We therefore analyze network contention on fat-tree networks and develop strategies to optimize collective multicast using known contention free communication schedules on fat-tree networks in the design of two strategies. We evaluate performance of these strategies with up to 256 nodes (1024 processors) on an alpha cluster. We present schemes that perform well when a contiguous chunk of nodes is not available. For large messages, many of our strategies have two times better throughput than native MPI. We also demonstrate that the software overhead of a collective operation is a small fraction of the total completion time in the presence of the communication coprocessor. We therefore compare the performance of the studied strategies using both metrics (i) completion time, and (it) computation overhead.","Network topology,
Bandwidth,
Design optimization,
Quantum computing,
Costs,
Computer science,
Concurrent computing,
Processor scheduling,
Throughput,
Coprocessors"
On building parallel & grid applications: component technology and distributed services,"Software component frameworks are well known in the commercial business application world and now this technology is being explored with great interest as a way to build large-scale scientific application on parallel computers. In the case of grid systems, the current architectural model is based on the emerging Web services framework. We describe progress that has been made on the common component architecture model (CCA) and discuss its success and limitations when applied to problems in grid computing. Our primary conclusion is that a component model fits very well with a services-oriented grid, but the model of composition must allow for a very dynamic (both in space and it time) control of composition. We note that this adds a new dimension to conventional service workflow and it extends the ""inversion of control"" aspects of must component systems.","Application software,
Component architectures,
Programming profession,
Concurrent computing,
Web services,
Software libraries,
Software performance,
Computer science,
Business,
Large-scale systems"
Design of part feeding and assembly processes with dynamics,"We introduce computational support tools for the analysis and design of systems with multiple frictional contacts, with a focus on applications to part feeding and assembly processes. The tools rely on dynamic models of the processes. We describe two approaches to modeling, the Stewart-Trinkle model (1996) and the Song-Pang-Kumar model (2003), that allow the designer to experiment with different geometric, material and dynamic properties and optimize the design for performance. In order to accommodate contact transitions, we introduce a smooth cone model for friction. We illustrate the models and the design process by describing the design optimization of a part feeder.","Solid modeling,
Assembly systems,
Design optimization,
Fixtures,
Prototypes,
Computer science,
Friction,
Process design,
Manufacturing processes,
Materials handling"
Land cover classification of SSC image: unsupervised and supervised classification using ERDAS Imagine,"NASA's Earth Sciences program is primarily focused on providing high quality data products to its science community. NASA also recognizes the need to increase its involvement with the general public, including areas of information and education. The main objective of this study is to classify the vegetation, man-made structures, and miscellaneous objects from the Satellite Image of NASA Stennis Space Center (SSC), Mississippi, and USA, by using the software, ERDAS Imagine 8.5. The ERDAS Image software performs the classification of an image for identification of terrestrial features based on the spectral analysis. For classification of the SSC image, the multispectral data was used for categorization of terrestrial objects, vegetation and shadows of the trees. These are two ways to classify pixels into different categories: Supervised and unsupervised. The classification of unsupervised data through ERDAS Image helped in identifying the terrestrial objects in the Study Image (SSC). The spectral pattern present within the data for each pixel was used as the numerical basis for categorization. The first analysis of the Image SSC involved the use of generalized Unsupervised Classification with 4 categories (Grass, Trees, Man-Made and Unknown). The result of the Unsupervised Image was used to create another image by using Supervised classification. The key difference between the two images is the ability of supervised image to decipher similar images, such as the roofs of the buildings and the shadows of the trees. Image stacking was conducted to create a fully classified image to separate shadow, grass, man-made, and trees. This poster will describe the procedures for viewing and measuring image, Computer-guided (Unsupervised) and User-guided (Supervised) Procedures will be described on image stacking to view each classification one at a time and stack them into a complete Classified Image. The application of unsupervised and supervised classification in agriculture will be discussed by giving examples of measurement of field reflectance of two classes of giant salvinia [green giant salvinia (green foliage) and senesced giant salvinia (mixture of green and brown foliage)], and invasive aquatic weed in Texas.","Classification tree analysis,
NASA,
Vegetation mapping,
Stacking,
Geoscience,
Educational products,
Educational programs,
Satellites,
Software performance,
Spectral analysis"
A computational framework for supporting software inspections,"Software inspections improve software quality by the analysis of software artifacts, detecting their defects for removal before these artifacts are delivered to the following software life cycle activities. Some knowledge regarding software inspections have been acquired by empirical studies. However, we found no indication that computational support for the whole software inspection process using appropriately such knowledge is available. This paper describes a computational framework whose requirements set was derived from knowledge acquired by empirical studies to support software inspections. To evaluate the feasibility of such framework, two studies have been accomplished: one case study, which has shown the feasibility of using the framework to support inspections, and an experimental study that evaluated the supported software inspection planning activity. Preliminary results of this experimental study suggested that unexperienced subjects are able to plan inspections with higher defect detection effectiveness, and in less time, when using this computational framework.","Inspection,
Software quality,
Proposals,
Systems engineering and theory,
Computer science,
Costs,
Decision making,
Software performance,
Processor scheduling,
Guidelines"
A practical cross-layer mechanism for fairness in 802.11 networks,"Many companies, organizations and communities are providing wireless hotspots that provide networking access using 802.11b wireless networks. Since wireless networks are more sensitive to variations in bandwidth and environmental interference than wired networks, most networks support a number of transmission rates that have different error and bandwidth properties. Access points can communicate with multiple clients running at different rates, but this leads to unfair bandwidth allocation. If an access point communicates with a mix of clients using both 1 mb/s and 11 mb/s transmission rates, the faster clients are effectively throttled to 1 mb/s as well. This happens because the 802.11 MAC protocol approximate ""station fairness"", with each station given an equal chance to access the media. We provide a solution to provide ""rate proportional fairness"", where the 11 mb/s stations receive more bandwidth than the 1 mb/s stations. Unlike previous solutions to this problem, our mechanism is easy to implement, works with common operating systems and requires no change to the MAC protocol or the stations.","Intelligent networks,
Bandwidth,
Bit rate,
Wireless networks,
Encoding,
Media Access Protocol,
Wireless LAN,
Computer science,
Wireless sensor networks,
Interference"
A multi-agent approach for peer-to-peer-based information retrieval systems,,"Information retrieval,
Search engines,
Protocols,
Peer to peer computing,
Computer science,
Information analysis,
Joining processes,
Network topology,
System performance,
Collaborative work"
Open laboratory for robotics education,"Laboratories are key components in the learning process of applied matters. The laboratory enables students to acquire methodologies, work habitude, knowledge on equipment operation and experience, in conditions as near as possible to their future professional activity. The evolution of communication and information technologies opens new possibilities in educational methods. The purpose of this paper is to present a Web based system for the implementation of a robotics laboratory with didactic finalities. The laboratory is to be accessible indifferently and simultaneously, in situ or via Internet. The system presented aims at providing access to the laboratory at any time, from everywhere, without space problems, paying special attention to safety requirements. For these reasons we call it an Open laboratory.","Laboratories,
Educational robots,
Robot sensing systems,
Robot programming,
Mobile robots,
Programming profession,
Orbital robotics,
Educational programs,
Computer science education,
Control engineering computing"
A model for conflict resolution between coverage and cost in cellular wireless networks,"The antenna placement problem, or cell planning problem, involves locating and configuring infrastructure for cellular wireless networks. Many authors have looked at computationally efficient methods to create feasible or optimised cell plans. However in most approaches the tension between conflicting objectives is generally addressed implicitly rather than explicitly. In this paper, we present a model for resolving the most fundamental tension, between service coverage and cost of base stations. The model is general in its approach, and its abstract representation means that state-of-the-art multiple objective optimisation techniques can readily be applied. We use a synthesised test problem to obtain typical results which demonstrate the tension between objectives and the diminishing return (in terms of service coverage) for additional investment.","Costs,
Intelligent networks,
Cellular networks,
Wireless networks,
Bit rate,
Quality of service,
Optimization methods,
Base stations,
Computer science,
System testing"
Achieving one-hop DHT lookup and strong stabilization by passing tokens,"Recent research has demonstrated that if network churn is not excessively high, it becomes entirely reasonable for a distributed hash table (DHT) to store a global lookup table at every node to achieve one-hop lookup. We present a novel algorithm for maintaining global lookup state in a DHT with a Chord-like circular address space. In our DHT, events are disseminated with a parallelized token-passing algorithm using dynamically-constructed dissemination trees rooted at the source of the events. We show that we are able to achieve good one- and two-hop routing performance at a modest cost in bandwidth. Furthermore, our scheme is bandwidth-adaptive, and automatically detects and repairs global address space inconsistencies.","Peer to peer computing,
Routing,
Table lookup,
Bandwidth,
Computer science,
Artificial intelligence,
Laboratories,
Heuristic algorithms,
Costs,
Telecommunication traffic"
Mining Pinyin-to-character conversion rules from large-scale corpus: a rough set approach,"The paper introduces a rough set technique for solving the problem of mining Pinyin-to-character (PTC) conversion rules. It first presents a text-structuring method by constructing a language information table from a corpus for each pinyin, which it will then apply to a free-form textual corpus. Data generalization and rule extraction algorithms can then be used to eliminate redundant information and extract consistent PTC conversion rules. The design of our model also addresses a number of important issues such as the long-distance dependency problem, the storage requirements of the rule base, and the consistency of the extracted rules, while the performance of the extracted rules as well as the effects of different model parameters are evaluated experimentally. These results show that by the smoothing method, high precision conversion (0.947) and recall rates (0.84) can be achieved even for rules represented directly by pinyin rather than words. A comparison with the baseline tri-gram model also shows good complement between our method and the tri-gram language model.","Large-scale systems,
Data mining,
Natural languages,
Natural language processing,
Computer science,
Smoothing methods,
Merging,
Context modeling,
Speech recognition,
Error analysis"
Cost-driven selection of parity trees,"We discuss the problem of parity tree selection for lossless compaction of the output responses of a circuit. Earlier methods assume off-chip storage of the correct compacted responses and therefore minimize the number of necessary parity trees. In contrast, our method targets on-chip generation of the correct compacted responses and therefore minimizes the actual implementation cost of the corresponding parity prediction functions. We present a systematic search approach that exploits the correlation between the hardware cost of a function and its entropy, in order to select parity trees that minimize the incurred cost, while achieving lossless compaction. Experimental results demonstrate that our method achieves significant hardware reduction over methods that minimize the number of parity trees.",
Toward a software testing and reliability early warning metric suite,"The field reliability is measured too late for affordably guiding corrective action to improve the quality of the software. Software developers can benefit from an early warning of their reliability while they can still affordably react. This early warning can be built from a collection of internal metrics. An internal metric, such as the number of lines of code, is a measure derived from the product itself. An external measure is a measure of a product derived from assessment of the behavior of the system. For example, the number of defects found in test is an external measure. The ISO/IEC standard states that [i]nternal metrics are of little value unless there is evidence that they are related to external quality. Internal metrics can be collected in-process and more easily than external metrics. Additionally, internal metrics have been shown to be useful as early indicators of externally-visible product quality. For these early indicators to be meaningful, they must be related (in a statistically significant and stable way) to the field quality/reliability of the product. The validation of such metrics requires the convincing demonstration that (1) the metric measures what it purports to measure and (2) the metric is associated with an important external metric, such as field reliability, maintainability, or fault-proneness. Software metrics have been used as indicators of software quality and fault proneness. There is a growing body of empirical results that supports the theoretical validity of the use of higher-order early metrics, such as OO metrics defined by Chidamber-Kemerer (CK) and the MOOD OO metric suites as predictors of field quality. However, general validity of these metrics (which are often unrelated to the actual operational profile of the product) is still open to criticism.",
Implications of an open system approach to vehicle health management,"The Boeing Company has pursued the development of an open, software reference architecture for vehicle health management (VHM) systems. Boeing's role as a system integrator has motivated this effort to reduce the development and integration cost of VHM systems and to achieve a higher level of performance that includes totally integrated diagnostics and prognostic capabilities. The development of this architecture was begun by an industry consortium lead by Boeing under a Navy Dual Use Science and Technology Program. This program was entitled open system architecture for condition based maintenance (OSACBM), and it has been continued under the auspices of the machinery information management open systems alliance (MIMOSA). The rewards and challenges of introducing this VRM open architecture into the system development and business processes are described.","Open systems,
Vehicles,
Software development management,
Costs,
Computer architecture,
Machinery,
Information management,
Intellectual property,
Data processing,
Health information management"
On the (im)possibility of cryptography with imperfect randomness,"We investigate the feasibility of a variety of cryptographic tasks with imperfect randomness. The kind of imperfect randomness we consider are entropy sources, such as those considered by Santha and Vazirani, Chor and Goldreich, and Zuckerman. We show the following: (1) certain cryptographic tasks like bit commitment, encryption, secret sharing, zero-knowledge, non-interactive zero-knowledge, and secure two-party computation for any non-trivial junction are impossible to realize if parties have access to entropy sources with slightly less-than-perfect entropy, i.e., sources with imperfect randomness. These results are unconditional and do not rely on any un-proven assumption. (2) On the other hand, based on stronger variants of standard assumptions, secure signature schemes are possible with imperfect entropy sources. As another positive result, we show (without any unproven assumption) that interactive proofs can be made sound with respect to imperfect entropy sources.","Cryptography,
Entropy,
Cryptographic protocols,
Computer science,
Security,
Engineering profession,
Computational modeling,
Polynomials"
Automated Web service composition using semantic Web technologies,"We present an architecture to facilitate automated discovery, selection, and composition of semantically-described heterogeneous services using semantic Web technologies. Our framework has three main features which distinguish it from other work in this area. First, we propose a dynamic, adaptive, and highly fault-tolerant service discovery and composition algorithm. Second, we distinguish between different levels of granularity of loosely coupled workflows. Finally, our framework allows the user to specify and refine a high-level objective.","Web services,
Semantic Web,
Concrete,
Fault tolerance,
Computer science,
Computer architecture,
Service oriented architecture,
Manuals,
Artificial intelligence,
Process planning"
Fair Attribution of Functional Contribution in Artificial and Biological Networks,"This letter presents the multi-perturbation Shapley value analysis (MSA), an axiomatic, scalable, and rigorous method for deducing causal function localization from multiple perturbations data. The MSA, based on fundamental concepts from game theory, accurately quantifies the contributions of network elements and their interactions, overcoming several shortcomings of previous function localization approaches. Its successful operation is demonstrated in both the analysis of a neurophysiological model and of reversible deactivation data. The MSA has a wide range of potential applications, including the analysis of reversible deactivation experiments, neuronal laser ablations, and transcranial magnetic stimulation “virtual lesions”, as well as in providing insight into the inner workings of computational models of neurophysiological systems.",
Research on Vacuum Micro-Gripper of Intelligent Micromanipulation Robots,"This paper presents a new vacuum micro-gripper (as an actuator of the micromanipulator) for using in micro-assembly tasks. The gripper is able to pick and place 100-300um sized targets. The micro-gripper consists of a vacuum unit and a control unit. With a proportional valve as a pressure adjuster and a pressure sensor as a pressure detector, the vacuum unit has a simple structure. With a PC as the upper layer controller and a MCU as the bottom layer controller, the control unit has two-layered control architecture. The necessary conditions to realize pick, hold and place operations are derived by comparing the forces acting on micro-targets. For solving the difficulty of place a micro-target, a controller based on fuzzy logic is designed to control the working pressure accurately. With MATLAB's Fuzzy Logic Toolbox, simulation experiments are performed to validate the performance of the fuzzy PD controller","Intelligent robots,
Fuzzy logic,
Pressure control,
Intelligent actuators,
Micromanipulators,
Grippers,
Valves,
Detectors,
Computer languages,
Fuzzy control"
Hierarchical tournament selection genetic algorithm for the vehicle routing problem with time windows,"The vehicle routing problem with time windows (VRPTW) is a well-known and complex combinatorial problem, which has received considerable attention in recent years. The VRPTW benchmark problems of Solomon (1987) have been most commonly chosen to evaluate and compare all exact and heuristic algorithms. A genetic algorithm and a set partitioning two phases approach has obtained competitive results in terms of total travel distance minimization. However, a great number of heuristics has used the number of vehicles as the first objective and travel distance as the second, subject to the first. This paper proposes a three phases approach considering both objectives. Initially, a hierarchical tournament selection genetic algorithm is applied. It can reach all best results in number of vehicles of the 56 Solomon's problems explored in the literature. After then, the two phase approach, the genetic and the set partitioning, is applied to minimize the travel distance as the second objective.","Genetic algorithms,
Vehicles,
Routing,
Computer science,
Testing,
Heuristic algorithms,
Partitioning algorithms,
Minimization methods,
Time factors,
Customer service"
Articulated models from video,"Past research on model-based tracking of articulated targets has neglected to address the problems of model-acquisition and initialization. However, for model-based approaches to ever become practical and autonomous, these important issues need to be addressed Towards this goal, this paper-presents a model-acquisition framework for acquiring articulated models directly from monocular video. Both structure, shape, and appearance of articulated models are estimated In addition, the initialization problem is solved by estimating pose information for at least one frame of a sequence, allowing subsequent model-based tracking. The presented work is based on basic assumptions and hence not restricted towards specific types of targets. It has in particular the ability to process human as well as non-human targets and makes no assumptions with respect to the structure of the kinematic tree or complexity. This work hence presents a set of systematic solutions to the problems of model-acquisition and initialization that bridge the gap between state of the art model-based tracking approaches and practical applications.","Target tracking,
Shape,
Humans,
Computer vision,
Motion analysis,
Cameras,
Video sequences,
Motion segmentation,
Computer science,
Kinematics"
A landscape with games in the background,"An overview of applications of two player path-forming games to verification and synthesis is given. Several extensions of the standard model of finite games with regular winning conditions are discussed. One direction is that of considering non-regular winning conditions. The other concerns the ways games are played, in particular probabilistic and multi-player games.","Cost accounting,
Game theory,
Automata,
Stress,
Robustness,
Proposals,
Probabilistic logic,
Computer science"
An extended kernel for generalized multiple-instance learning,"The multiple-instance learning (MIL) model has been successful in areas such as drug discovery and content-based image-retrieval. Recently, this model was generalized and a corresponding kernel was introduced to learn generalized MIL concepts with a support vector machine. While this kernel enjoyed empirical success, it has limitations in its representation. We extend this kernel by enriching its representation and empirically evaluate our new kernel on data from content-based image retrieval, biological sequence analysis, and drug discovery. We found that our new kernel generalized noticeably better than the old one in content-based image retrieval and biological sequence analysis and was slightly better or even with the old kernel in the other applications, showing that an SVM using this kernel does not overfit despite its richer representation.","Kernel,
Image retrieval,
Content based retrieval,
Image analysis,
Image sequence analysis,
Drugs,
Support vector machines,
Shape,
Computer science,
Information retrieval"
On the cognitive informatics foundations of software engineering,"Software is the object of study in software engineering and computer science. Software is an intellectual artifact that provides a solution for a repeatable computer application, which enables existing tasks to be done easier, faster, and smarter, or which provides innovative applications for the industries and daily life. This paper attempts to explore the basic characteristics of software and to search the informatics and cognitive foundations of software in supplement to theories of computer science. In this paper, the nature of software is characterized by its informatics, behavioral, mathematical, and cognitive properties. Then, the cognitive informatics foundations of software engineering are developed on the basis of the informatics laws of software and software engineering psychology.","Cognitive informatics,
Software engineering,
Computer industry,
Computer science,
Psychology,
Computer aided instruction,
Manufacturing industries,
Software measurement,
Quality assurance,
Large-scale systems"
Mobile agents: can they assist with context awareness?,"This position paper argues that the mobile agents paradigm is a useful and important technology enabling pervasive and ubiquitous computing. Context awareness drives adaptability of pervasive computing systems. It is asserted that mobile agents capable of discovering, extracting, interpreting and validating context will make significant contribution to increasing efficiency, flexibility and feasibility of pervasive computing systems.","Context awareness,
Mobile agents,
Pervasive computing,
Intelligent agent,
Application software,
Software agents,
Mobile computing,
Context-aware services,
Wireless sensor networks,
Computer science"
A personalized courseware recommendation system based on fuzzy item response theory,"With the rapid growth of computer and Internet technologies, e-learning has become a major trend in the computer assisted teaching and learning field currently. In past years, many researchers made efforts in developing e-learning systems with personalized learning mechanism to assist on-line learning. However, most of them focused on using learner's behaviors, interests, or habits to provide personalized e-learning services. These systems usually neglected to concern if learner's ability and the difficulty of courseware are matched each other. Generally, recommending an inappropriate courseware might result in learner's cognitive overhead or disorientation during a learning process. To promote learning efficiency and effectiveness, we present a personalized courseware recommendation system (PCRS) based on the proposed fuzzy item response theory (FIRT), which can recommend courseware with appropriate difficult level to learner through learner gives a fuzzy response of understanding percentage for the learned courseware. Experiment results show that applying the proposed fuzzy item response theory to Web-based learning can achieve personalized learning and help learners to learn more effectively and efficiently.","Courseware,
Fuzzy systems,
Electronic learning,
Feedback,
Databases,
Chaos,
Educational technology,
Educational institutions,
Computer science education,
Electronic mail"
Scene-adaptive transform domain video partitioning,"An adaptive mechanism for video partitioning in the transform domain is proposed. Different quantitative measures for motion complexity and activity levels in a scene are defined, based on which a video scene can be consistently categorised into identifiable classes. Further, the video quality, as measured by the mean square error (MSE) is related to certain parameters used in video partitioning. Adaptability is realized by tailoring the parameters of the video partitioning algorithm to the specific characteristics of the video scene, as embodied in the video scene class and the video quality. Experimental results are included.","Layout,
Video compression,
Partitioning algorithms,
Video sequences,
Proposals,
Robustness,
Computer science,
Motion measurement,
Mean square error methods,
Indexing"
Efficient density-based clustering of complex objects,"Nowadays, data mining in large databases of complex objects from scientific, engineering or multimedia applications is getting more and more important. In many different application domains, complex object representations along with complex distance functions are used for measuring the similarity between objects. Often, not only these complex distance measures are available but also simpler distance functions which can be computed much more efficiently. Traditionally, the well known concept of multi-step query processing which is based on exact and lower-bounding approximative distance functions are used independently of data mining algorithms. In this paper, we demonstrate how the paradigm of multi-step query processing can be integrated into the two density-based clustering algorithms DBSCAN and OPTICS resulting in a considerable efficiency boost. Our approach tries to confine itself to /spl epsiv/-range queries on the simple distance functions and carries out complex distance computations only at that stage of the clustering algorithm where they are compulsory to compute the correct clustering result. In a broad experimental evaluation based on real-world test data sets, we demonstrate that our approach accelerates the generation of flat and hierarchical density-based clusterings by more than one order of magnitude.","Clustering algorithms,
Query processing,
Data mining,
Multimedia databases,
Application software,
Image databases,
Acceleration,
Computer science,
Data engineering,
Integrated optics"
Dynamic approximate all-pairs shortest paths in undirected graphs,"We obtain three dynamic algorithms for the approximate all-pairs shortest paths problem in unweighted undirected graphs: 1) For any fixed /spl epsiv/ > 0, a decremental algorithm with an expected total running time of O(mn), where m is the number of edges and n is the number of vertices in the initial graph. Each distance query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 1 + /spl epsiv/. The algorithm uses O(n/sup 2/) space; 2) For any fixed integer k /spl ges/ 1, a decremental algorithm with an expected total running time of O(mn). Each query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 2k - 1. This algorithm uses, however, only O(m + n/sup 1+1/k/) space. It is obtained by dynamizing techniques of Thorup and Zwick. In addition to being more space efficient, this algorithm is also one of the building blocks used to obtain the first algorithm; 3) For any fixed /spl epsiv/, /spl delta/ > 0 and every t /spl les/ m/sup 1/2-/spl delta//, a fully dynamic algorithm with an expected amortized update time of O(mn/t) and worst-case query time of O(t). The stretch of the returned distances is at most 1+/spl epsiv/. All algorithms can also be made to work on undirected graphs with small integer edge weights. If the largest edge weight is b, then all bounds on the running times are multiplied by b.","Heuristic algorithms,
Computer science,
Shortest path problem"
Speed up of an analytical algorithm for nonuniform attenuation correction by using PC video/graphics card architecture,"A major task in quantitative SPECT (single photon emission computed tomography) reconstruction is compensation for object-specific attenuation, which is usually nonuniform. Mathematically this task is expressed as the inversion of the attenuated Radon transform. Novikov had derived an explicit inversion formula for the attenuated Radon transform for parallel-beam collimation geometry. In our previous work, we extended his work to variable focusing fan-beam (VFF) collimators. A ray-driven analytical inversion formula for VFF reconstruction with nonuniform attenuation was derived. The drawback of ray-driven methods is that they are time consuming. In this work, we proposed a fast implementation method, which includes algorithm optimization and acceleration by the texture-mapping architecture of PC graphics/video card. We further investigated the noise properties and associated artifacts of the analytical inversion formula. The artifacts were remarkably reduced when more projections were sampled to mitigate the problem of wide bandwidth of the discrete Hilbert transform. The reconstruction from noisy data demonstrated the accuracy and robustness of the presented ray-driven analytical inversion formula with dramatic speed acceleration by the PC graphics/video card.",
Diagnosis of Complex Systems: Bridging the Methodologies of the FDI and DX Communities,,"Fault detection,
Bridges,
Fault diagnosis,
Space technology,
Aircraft manufacture,
Computer science,
Terminology,
Humans,
Hardware"
On-chamber readout system for the ATLAS MDT muon spectrometer,"The ATLAS MDT Muon Spectrometer is a system of approximately 380 000 pressurized cylindrical drift tubes of 3 cm diameter and up to 6 m in length. These monitored drift tubes (MDTs) are precision glued to form superlayers, which in turn are assembled into precision chambers of up to 432 tubes each. Each chamber is equipped with a set of mezzanine cards containing analog and digital readout circuitry sufficient to read out 24 MDTs per card. Up to 18 of these cards are connected to an on-chamber DAQ element referred to as a chamber service module (CSM). The CSM multiplexes data from the mezzanine cards and outputs this data on an optical fiber which is received by the off-chamber DAQ system. Thus, the chamber forms a highly self-contained unit with DC power in and a single optical fiber out. The MDTs, due to their length, require a terminating resistor at their far end to prevent reflections. The readout system has been designed so that thermal noise from this resistor remains the dominant noise source of the system. This level of noise performance has been achieved and maintained in large scale on-chamber tests.","Mesons,
Spectroscopy,
Optical noise,
Data acquisition,
Optical fibers,
Resistors,
Monitoring,
Assembly,
Circuits,
Optical reflection"
The development of an intelligent Web-based rapid prototyping manufacturing system,"A rapid prototyping (RP) machine system which combines a PC-based controller with the thermal-extrusion method is presented. The proposed RP system offers a three-axis platform, an extrusion head, a temperature controller, and a PC-based control system. Low-cost acrylonitrile-butadiene-styrene (ABS) pellets/powder are used for thermal extrusion, although the system is easily adapted to other not-too-dissimilar materials. In order to improve the quality of RP part, the Taguchi method was used to analyzing the process parameters of the proposed RP system. Based on the experimental results, the proposed RP mechatronics system can produce good quality RP parts. The RP software technique includes slicing, support, tool path, and motion code generation. This paper also presents a new adaptive slicing algorithm for RP system. According to this algorithm, the three-dimensional (3-D) computer-aided design (CAD) model can be sliced with different thickness automatically by comparing the contour circumference or the center of gravity of the contour with those of the adjacent layer. With this adaptive slicing method, the part can be fabricated faster than it uses uniform slicing method with identical accuracy. Finally, the intelligent web-based RP system allows remote users to upload an STL file, to directly building up of the physical model, and to monitor the actual fabrication process from a charged coupled devices (CCD) camera located in the RP machine itself. The user does not need to buy an expensive RP machine; instead, he can rent time and uses the machine remotely via the Internet.

Note to Practitioners-RP has being used as an essential augmented tool between CAD and computer-aided manufacturing (CAM) for product manufacturing. RP uses layered manufacturing technology to produce complicated prototypes directly from a CAD model. As imaging, materials, and processing techniques improve, RP functions such as assembly fit, tooling masters, prototype tools, and prototype parts are made possible. RP technology that can construct parts of multiple materials, colors, and even parts composed of composite materials will soon be available. In this paper, an RP machine system which combines PC-based controller with thermal extrusion method is presented. Low-cost ABS pellets are used for thermal extrusion, although the system is easily adapted to other not-too-dissimilar materials. Based on the experimental results, the proposed RP mechatronics system can produce good quality RP parts. In this paper, a new adaptive slicing algorithm is developed to decrease fabrication time without much reducing the model accuracy. According to this algorithm, the 3-D CAD model can be sliced with different thickness automatically. With this adaptive slicing method, the part can be fabricated much faster than it uses traditional uniform slicing method. Finally, the intelligent web-based RP system has been developed which allows remote users to upload a CAD file of the part, direct building up of the physical model, and monitoring of the actual fabrication process from a CCD camera located in the RP machine itself. The greatest advantage is that user does not need to buy an expensive RP machine; instead he rents time and uses the machine remotely via the Internet. This really drastically reduces the production development cycle for the highly competitive time-to-market challenge.","Prototypes,
Manufacturing systems,
Control systems,
Machine intelligence,
Fabrication,
Computer aided manufacturing,
Temperature control,
Mechatronics,
Design automation,
Intelligent systems"
Approximate solutions for partially observable stochastic games with common payoffs,"Partially observable decentralized decision making in robot teams is fundamentally different from decision making in fully observable problems. Team members cannot simply apply single-agent solution techniques in parallel. Instead, we must turn to game theoretic frameworks to correctly model the problem. While partially observable stochastic games (POSGs) provide a solution model for decentralized robot teams, this model quickly becomes intractable. We propose an algorithm that approximates POSGs as a series of smaller, related Bayesian games, using heuristics such as QMDP to provide the future discounted value of actions. This algorithm trades off limited look-ahead in uncertainty for computational feasibility, and results in policies that are locally optimal with respect to the selected heuristic. Empirical results are provided for both a simple problem for which the full POSG can also be constructed, as well as more complex, robot-inspired, problems.","Stochastic processes,
Robot sensing systems,
Game theory,
Parallel robots,
Costs,
Orbital robotics,
Decision making,
State-space methods,
Permission,
Computer science"
Reinforcement learning for autonomic network repair,We report on our efforts to formulate autonomic network repair as a reinforcement-learning problem. Our implemented system is able to learn to efficiently restore network connectivity after a failure.,"Learning,
Testing,
Cost function,
Power system restoration,
Computer science,
Decision making,
Fault diagnosis,
Performance evaluation,
Dynamic programming,
Process planning"
Model-based testing and maintenance,"This paper presents a semantic software development model (SSDM) for object-oriented software. It organizes all the information generated during the software development lifecycle including requirements, design, implementation, testing, and maintenance. Based on SSDM, software testing and maintenance can be carried out in a more systematic, efficient and complete manner, and can be enhanced by a set of proactive rules defined.","Software testing,
Object oriented modeling,
System testing,
Software maintenance,
Programming,
Software quality,
Software engineering,
Real time systems,
Computer science,
Life testing"
Automated synthesis of multitolerance,"We concentrate on automated synthesis of multitolerant programs, i.e., programs that tolerate multiple classes of faults and provide a (possibly) different level of fault-tolerance to each class. We consider three levels of fault-tolerance: (1) failsafe, where in the presence of faults, the synthesized program guarantees safety, (2) nonmasking, where in the presence of faults, the synthesized program recovers to states from where its safety and liveness are satisfied, and (3) masking where in the presence of faults the synthesized program satisfies safety and recovers to states from where its safety and liveness are satisfied. We focus on the automated synthesis of finite-state multitolerant programs in high atomicity model where the program can read and write all its variables in an atomic step. We show that if one needs to add failsafe (respectively, nonmasking) fault-tolerance to one class of faults and masking fault-tolerance to another class of faults then such addition can be done in polynomial time in the state space of the fault-intolerant program. However, if one needs to add failsafe fault-tolerance to one class of faults and nonmasking fault-tolerance to another class of faults then the resulting problem is NP-complete. We find this result to be counterintuitive since adding failsafe and nonmasking fault-tolerance to the same class of faults (which is equivalent to adding masking fault-tolerance to that class of faults) can be done in polynomial time, whereas adding failsafe fault-tolerance to one class of faults and nonmasking fault-tolerance to a different class of faults is NP-complete.","Fault tolerance,
Safety,
Fault tolerant systems,
Network synthesis,
Polynomials,
Software engineering,
Laboratories,
Computer science,
State-space methods,
Engineering profession"
A parallel loop self-scheduling on grid computing environments,"Internet computing and grid technologies promise to change the way we tackle complex problems. They will enable large-scale aggregation and sharing of computational, data and other resources across institutional boundaries. And harnessing these new technologies effectively will transform scientific disciplines ranging from high-energy physics to the life sciences. In this paper, a grid computing environment is proposed and constructed on multiple PC clusters by using Globus Toolkit (GT) and SUN Grid Engine (SGE). The experimental results are also conducted by using the matrix multiplication to demonstrate the performance. On the other hand, the approaches to deal with scheduling and load balancing on multiple heterogeneous PC clusters computer system are not mature. Self-scheduling schemes which are suitable for parallel loops with independent iterations on heterogeneous cluster computer system have been designed in the past. However, these schemes, such as FSS, GSS and TSS, can not achieve load balancing in extremely heterogeneous environment. We propose a heuristic approach based upon a two-phase scheme to solve parallel regular loop scheduling problem on an extremely heterogeneous grid computing environment.","Grid computing,
Processor scheduling,
Load management,
Internet,
Large-scale systems,
Physics,
Sun,
Search engines,
Concurrent computing,
Frequency selective surfaces"
Using permutations instead of student's t distribution for p-values in paired-difference algorithm comparisons,The paired-difference t-test is commonly used in the machine learning community to determine whether one learning algorithm is better than another on a given learning task. This paper suggests the use of the permutation test instead because it calculates the exact p-value instead of an estimate. The permutation test is also distribution free and the time complexity is trivial for the commonly used 10-fold cross-validation paired-difference test. Results of experiments on real-world problems suggest it is not uncommon to see the t-test estimate deviate up to 30-50% from the exact p-value.,"Testing,
Machine learning algorithms,
Machine learning,
Computer science,
Probability,
Robustness,
Statistical analysis,
Packaging"
Approximate selection queries over imprecise data,"We examine the problem of evaluating selection queries over imprecisely represented objects. Such objects are used either because they are much smaller in size than the precise ones (e.g., compressed versions of time series), or as imprecise replicas of fast-changing objects across the network (e.g., interval approximations for time-varying sensor readings). It may be impossible to determine whether an imprecise object meets the selection predicate. Additionally, the objects appearing in the output are also imprecise. Retrieving the precise objects themselves (at additional cost) can be used to increase the quality of the reported answer. We allow queries to specify their own answer quality requirements. We show how the query evaluation system may do the minimal amount of work to meet these requirements. Our work presents two important contributions: first, by considering queries with set-based answers, rather than the approximate aggregate queries over numerical data examined in the literature; second, by aiming to minimize the combined cost of both data processing and probe operations in a single framework. Thus, we establish that the answer accuracy/performance tradeoff can be realized in a more general setting than previously seen.","Probes,
Costs,
Computer science,
Uncertainty,
Query processing,
Aggregates,
Data processing,
Databases"
Speech emotion classification with the combination of statistic features and temporal features,"For classifying speech emotion, most previous systems used either statistical features or temporal features exclusively. However, these two distinct feature representations appear to be concerned with different aspects of emotion, and should be combined in the task. This work proposes a classification scheme that enables the combination of them both. In the scheme, GMM and HMM are first performed to model the statistical features and temporal features respectively. Then the GMM likelihoods and HMM likelihoods are used as features in a further procedure. Finally, a weighted Bayesian classifier and MLP are applied to accomplish the classification. Experiments on a Chinese speech corpus have demonstrated that the scheme could improve the classification accuracy greatly. More detailed analysis indicated that these two feature representations could compensate each other efficiently in the classification.","Speech,
Statistics,
Hidden Markov models,
Feature extraction,
Computer science,
Emotion recognition,
Support vector machines,
Support vector machine classification,
Coordinate measuring machines,
Bayesian methods"
Buddy-finder: a proposal for a novel entertainment application for GSM,"Since the first deployment of cellular phones, we have witnessed an almost global revolution in the development of wireless communication. This success has been evident for the GSM technology, whose popularity has exploded during the 1990s thanks to its affordable price, digital quality and a pervasive innovative application as SMS. However, the same positive condition seems to lack in the next generations of wireless communication, as UMTS, thus slowing down their market penetration. Therefore, both mature and recent mobile technology could benefit from the introduction of a novel application able to shake the market. To this aim, mobile location could be used to facilitate the diffusion of many novel services for cellular systems. We present in this paper, an innovative location based application that involves only a minor impact on the current GSM architecture, while offering to customers an appealing means to obtain multimedia entertainment.","Proposals,
GSM,
Cellular phones,
Application software,
Wireless communication,
Costs,
Computer science,
3G mobile communication,
Boosting,
Message systems"
Compound Critiques for Conversational Recommender Systems,"Recommender systems bring together ideas from information retrieval and filtering, user profiling, adaptive interfaces and machine learning in an attempt to offer users more personalized and responsive search systems. Conversational recommenders guide a user through a sequence of iterations, suggesting specific items, and using feedback from users to refine their suggestions in subsequent iterations. Different recommender systems look for different types of feedback from users. In this paper we examine the role of critiquing, a form of feedback in which the user indicates a preference over a particular feature of a recommended item. For example, when shopping for a PC a user might indicate that they like the current suggestion but they are looking for something ""cheaper""; ""cheaper"" is a critique over the price feature of the PC case. Sometimes it is useful to critique multiple features simultaneously (compound critiques). In this paper we describe how a recommender can automatically discover useful compound critiques during the recommendation session and how these critiques can be used to improve recommendation efficiency.","Recommender systems,
Feedback,
Information retrieval,
Machine learning,
Read-write memory,
Computer science,
Educational institutions,
User centered design,
Adaptive filters,
Information filtering"
A novel morphological-based carotid artery contour extraction,"Ultrasound imaging provides an inexpensive tool for monitoring the blood flow within the carotid artery. The precipitation of plaque on the wall of the carotid artery has a great influence on the blood flow within the artery. Due to the poor quality and the presence of noise in ultrasound images, manual extraction of carotid artery walls is a tedious and time-consuming task. In this paper, a novel carotid artery contour extraction is proposed. The proposed scheme consists of four major stages. These stages are: pre-processing stage, quantization stage, morphological contour detection stage, and finally a contour enhancement stage. Experimental results over a set of sample images showed that the proposed scheme produces accurate contours. Thus the proposed scheme can be used as an effective tool in monitoring and detecting plaque precipitation on the walls of the carotid artery.","Carotid arteries,
Ultrasonic imaging,
Cardiac arrest,
Heart,
Data mining,
Deformable models,
Computer science,
Quantization,
Morphology,
Coronary arteriosclerosis"
Wallop: designing social software for co-located social networks,"Technology is increasingly being incorporated into people's day-to-day social relationships, particularly for people whose friendships occupy the center of their social lives. In the following paper we discuss a co-located social group's tendency to integrate planning and re-experiencing around social events with tools for persistent conversations. Through a questionnaire study we found that emails and mailing lists were used as much as phone conversations to plan social activities, and that said usage was positively correlated with measures of friendship satisfaction, sense of community, and percentage of time spent socializing. In response to our observations, we designed a sharing and communication application, Wallop, to enrich the co-located social group's planning and sharing around social events. Wallop provides both communication and social awareness tools, enabling users to build conversations in the context of shared content and browse their implicit social networks. Initial responses to Wallop from a focus group and limited deployment to test users have been positive.","Technology social factors,
Communication system software,
Electronic mail"
A novel CLB architecture to detect and correct SEU in LUTs of SRAM-based FPGAs,"This work proposes a new CLB architecture for FPGAs that can detect and correct single event upset (SEU) faults in the LUTs. A methodology for mapping logical functions onto the LUTs is presented that exploits the features of the proposed CLB architecture to detect and correct the SEU faults in the LUTs. Experimental results obtained by mapping standard benchmark circuits on the proposed architecture indicate that on an average, 96% of the SEU in the LUTs can be detected without employing any redundancy. Further, by using duplication with comparison (DWC) techniques it is shown that 100% of the SEU in the LUTs can be detected for any circuit that is mapped on the proposed architecture; and for the benchmark circuits, on an average, 96% of the SEU in the LUTs can be automatically (without any user intervention or reconfiguration) corrected.","Table lookup,
Field programmable gate arrays,
Single event upset,
Circuit faults,
Routing,
Computer architecture,
Computer science,
Fault detection,
Logic testing,
Fault tolerant systems"
Design and implementation of a MAC scheme for wireless ad-hoc networks based on a cooperative game framework,"Due to their dynamic topologies, providing quality of service (QoS) in wireless/mobile ad-hoc networks introduces major new challenges to the research community. Today, the only commercially available ad-hoc network products are those based on the widely deployed IEEE 802.11 distributed coordination function (DCF). However DCF is a random access scheme, which not only cannot provide any guarantees, is also well known to suffer from a problem of fairness. In this paper, the bandwidth allocation problem in the medium access control (MAC) layer of ad-hoc networks is modelled as a constrained maximization problem. Based on duality, the problem is further modelled as a cooperative game and an algorithm to solve this problem is provided, and the discussion is centered on the design and implementation issues of the algorithm.","Ad hoc networks,
Channel allocation,
Media Access Protocol,
Quality of service,
Network topology,
Bandwidth,
Power system modeling,
Computer science,
Partial response channels,
Electronic mail"
Multi-robot area exploration with limited-range communications,"This paper proposes a reliable and efficient multi-robot coordination algorithm to accomplish an area exploration task given that the communication range of each robot is limited. This algorithm is based on a distributed bidding model to coordinate the movement of multiple robots. Two measures are developed to accommodate the limited-range communications. First, the distances between robots are considered in the bidding algorithm so that the robots tend to stay close to each other. Second, a novel coding mechanism is introduced in the map representation to reduce the exchanged data volume when new communication links are formed. Simulation results show the effectiveness of the use of the nearness measure in the coordination algorithm as well as the new coding mechanism to reduce the amount of map data exchanged. By handling the limited communication range we can make the coordination algorithms more practical in multi-robot applications.","Robot kinematics,
Robot sensing systems,
Algorithm design and analysis,
Mobile communication,
Mobile robots,
Robustness,
Costs,
Computer science,
Hazardous materials,
Concurrent computing"
Power save mechanisms for multi-hop wireless networks,"In this paper, we discuss power save mechanisms that allow hosts to go to sleep to conserve energy. When sleeping hosts need to receive packets from other hosts, these hosts must somehow wake up the sleeping hosts. The paper discusses a busy-tone mechanism for this purpose, and an approach to improve the mechanism by using multiple busy tones. We generalize this protocol to develop the notion of multi-level power save mechanisms, and present some examples to illustrate this notion.","Spread spectrum communication,
Wireless networks,
Sensor phenomena and characterization,
Media Access Protocol,
Delay,
Access protocols,
Switches,
Space exploration,
Computer science,
Computer networks"
Embedding information within dynamic visual patterns,There exist many scenarios in which the automatic distinction between human and machines is desirable. This paper describes a texture-image based approach to encode text information in such a way that machine vision algorithms will experience difficulties while humans can extract the embedded text effortlessly. We exploit both static and dynamic visual patterns to diversify the image content and discourage automatic processing. Examples of the resulting patterns are presented and their properties are discussed,"Humans,
Testing,
Computer science,
Machine vision,
Radio access networks,
Data mining,
Information technology,
Information retrieval,
Computer networks,
Hardware"
Establishing structured support for programming students,"This paper describes the structure, implementation, and impact evaluation of a programming support centre for engineering and computing students. The main focus of this centre is to provide a positive, supportive atmosphere where students can voluntarily seek one-to-one assistance with programming difficulties. The support offered is specifically structured to nurture and leverage each student's motivation for taking a programming course whilst providing them with individually tailored advice and practical help. A qualitative and quantitative evaluation of the centre's operation is presented, together with analysis of statistics on student motivation. The results of this research suggest that the newly developed programming support centre has had a positive impact on student learning.","Programming profession,
Computer science,
Educational institutions,
Educational programs,
Atmosphere,
Statistical analysis,
Computer displays,
Computer science education,
Continuing education,
Information management"
ProtoMon: embedded monitors for cryptographic protocol intrusion detection and prevention,"Intrusion detection systems (IDS) are responsible for monitoring and analyzing host or network activity to detect intrusions in order to protect information from unauthorized access or manipulation. There are two main approaches for intrusion detection: signature-based and anomaly-based. Signature-based detection employs pattern matching to match attack signatures with observed data making it ideal for detecting known attacks. However, it cannot detect unknown attacks for which there is no signature available. Anomaly-based detection uses machine-learning techniques to create a profile of normal system behavior and uses this profile to detect deviations from the normal behavior. Although this technique is effective in detecting unknown attacks, it has a drawback of a high false alarm rate. In this paper, we describe our anomaly-based IDS designed for detecting malicious use of cryptographic and application-level protocols. Our system has several unique characteristics and benefits, such as the ability to monitor cryptographic protocols and application-level protocols embedded in encrypted sessions, a very lightweight monitoring process, and the ability to react to protocol misuse by modifying protocol response directly.","Cryptographic protocols,
Intrusion detection,
Cryptography,
Communication system security,
Pattern matching,
Computer displays,
Computer science,
Computerized monitoring,
Information analysis,
Protection"
A novel approach of palm-line extraction,"Palm-lines, including the principal lines and wrinkles, can describe a palmprint clearly. This paper presents a novel approach of palm-line extraction for the online palmprints. This approach is composed of two stages: coarse-level extraction stage and fine-level extraction stage. In the first stage, morphological operations are used to extract palm-lines in different directions. In the second stage, for each extracted line, a recursive process is devised to further extract and trace the palm-line using the local information of the extracted part. Experimental results show that the proposed approach is suitable for palm-line extraction.","Data mining,
Biometrics,
Fingerprint recognition,
Iris,
Feature extraction,
Morphological operations,
Computer science,
Information geometry,
Solid modeling,
Shape"
Dynamic data integration using Web services,"We address the problem of large-scale data integration, where the data sources are unknown at design time, are from autonomous organisations, and may evolve. Experiments are described involving a demonstrator system in the field of health services data integration within the UK. Current Web services technology has been used extensively and largely successfully in these distributed prototype systems. The work shows that Web services provide a good infrastructure layer, but integration demands a higher level ""broker"" architectural layer; the paper identifies eight specific requirements for such an architecture that have emerged from the experiments, derived from an analysis of shortcomings which are collectively due to the static nature of the initial prototype. The way in which these are being met in the current version in order to achieve a more dynamic integration is described.",
Image and feature co-clustering,"The visual appearance of an image is closely associated with its low-level features. Identifying the set of features that best characterizes the image is useful for tasks such as content-based image indexing and retrieval. In this paper, we present a method which simultaneously models and clusters large sets of images and their low-level visual features. A computational energy function suited for co-clustering images and their features is first constructed and a Hopfield model based stochastic algorithm is then developed for its optimization. We apply the method to cluster digital color photographs and present results to demonstrate its usefulness and effectiveness.","Indexing,
Prototypes,
Bipartite graph,
Histograms,
Pixel,
Image retrieval,
Content based retrieval,
Stochastic processes,
Clustering algorithms,
Computer science"
Self-checking code-disjoint carry-select adder with low area overhead by use of add1-circuits,"In this paper, a new code-disjoint totally self-checking carry-select adder, with low area overhead, is proposed which is based on the simplified design of carry-select adders by use of add1-circuits as proposed in (T. Y. Chang et al., IEE Elec. Lett., vol.34, no.22, p.2101-2103, 1998) and (Y. Kim et al., ISCAS, p.218-221, 2001). The area overhead for the proposed adder is only 40% of a traditional carry-select adder without error detection.","Adders,
Circuit faults,
Latches,
Registers,
Crosstalk,
Fault detection,
Computerized monitoring,
Computer science,
Fault tolerance,
Voltage"
Bandwidth-aware co-allocating meta-schedulers for mini-grid architectures,"The interaction of simultaneously co-allocated jobs can often create contention in the network infrastructure of a dedicated computational grid. This contention can lead to degraded job run-time performance. We present several bandwidth-aware co-allocating meta-schedulers. These schedulers take into account inter-cluster network utilization as a means by which to mitigate this impact. We make use of a bandwidth-centric parallel job communication model that captures the time-varying utilization of shared inter-cluster network resources. By doing so, we are able to evaluate the performance of grid scheduling algorithms that focus not only on node resource allocation, but also on shared inter-cluster network bandwidth.","Resource management,
Bandwidth,
Switches,
Processor scheduling,
Computer architecture,
Computer networks,
Grid computing,
Laboratories,
Computer science education,
Degradation"
Scalability in adaptive multi-metric overlays,"Increasing application requirements have placed heavy emphasis on building overlay networks to efficiently deliver data to multiple receivers. A key performance challenge is simultaneously achieving adaptivity to changing network conditions and scalability to large numbers of users. In addition, most current algorithms focus on a single performance metric, such as delay or bandwidth, particular to individual application requirements. We introduce a two-fold approach for creating robust, high-performance overlays called adaptive multimetric overlays (AMMO). First, AMMO uses an adaptive, highly-parallel, and metric-independent protocol, TreeMaint, to build and maintain overlay trees. Second, AMMO provides a mechanism for comparing overlay edges along specified application performance goals to guide TreeMaint transformations. We have used AMMO to implement and evaluate a single-metric (bandwidth-optimized) tree similar to Overcast and a two-metric (delay-constrained, cost-optimized) overlay.","Scalability,
Peer to peer computing,
Probes,
Delay,
Bandwidth,
Protocols,
Computer science,
Measurement,
Data engineering,
Drives"
Automated formal verification of scheduling process using finite state machines with datapath (FSMD),"This paper presents a methodology for the formal verification of scheduling during High-Level Synthesis(HLS). A notion of functional equivalence between two Finite State Machines with Datapath (FSMDs) is defined, on the basis of which we propose a methodology to verify scheduling. The functional equivalence between the behavioral specification and the scheduled Control-Data Flow Graph (CDFG) - that is the result of scheduling - is established using their FSMD models. The equivalence conditions are mathematically modeled and implemented in the higher-order specification language of theorem proving environment PVS, integrated with a HLS tool. The proof of correctness of the design is subsequently verified by the PVS proof checker.","Formal verification,
Automata,
High level synthesis,
Processor scheduling,
Computer science,
Flow graphs,
Mathematical model,
Specification languages,
Libraries,
Computer applications"
Evolution Spectrographs: visualizing punctuated change in software evolution,"Software evolution is commonly characterized as a slow process of incremental change. Researchers have observed that software systems also exhibit characteristics of punctuation (sudden and discontinuous change) during their evolution. We analyze punctuated evolution from the perspective of structural change. We developed a color-coded visualization technique called the Evolution Spectrograph (ESG). ESG can be applied to highlight conspicuous changes across a historical sequence of software releases. We describe evolution spectrographs and present the empirical results from our studies of three open source software systems: OpenSSH, PostgreSQL, and Linux. We show that punctuated change occurred in the evolution of these three systems, and we validate our empirical results by examining related software documents such as change logs and release notes.","Visualization,
Evolution (biology),
Open source software,
Computer architecture,
Software systems,
Software architecture,
Sequences,
Computer science,
Linux,
Genetic mutations"
Sampling and aliasing consequences of quarter-detector offset use in helical CT,"In this paper, the sampling and aliasing consequences of employing a quarter-detector-offset (QDO) in helical computed tomography (CT) are analyzed. QDO is often used in conventional CT to reduce in-plane aliasing by eliminating data redundancies to improve radial sampling. In helical CT, these same redundancies are exploited to improve longitudinal sampling and so it might seem ill-advised to employ QDO. The relative merit of the two geometries for helical CT is studied by conducting a multidimensional sampling analysis of projection-space sampling as well as a Fourier crosstalk analysis of crosstalk among the object's Fourier basis components. Both a standard fanbeam helical CT geometry and a hypothetical parallel-beam CT geometry, which helps illuminate the more complicated fanbeam results, are analyzed. Using the sampling analysis, it was found that the use of QDO leads to very different spectral tiling than arise when not using QDO. However, due to the shape of the essential support of the projection data spectra that arises in practice, both configurations lead to very similar or identical amounts of spectral overlap. This perspective also predicts the spatially variant longitudinal aliasing that has been observed in helical CT. The crosstalk results were consistent with those of the multidimensional sampling analysis. Thus, from the standpoint of aliasing and crosstalk, no compelling difference is found between the two geometries.","Sampling methods,
Computed tomography,
Crosstalk,
Geometry,
Multidimensional systems,
Image sampling,
Detectors,
Image reconstruction,
Image analysis,
Radiology"
Digital watermarking capacity research,"Image watermarking capacity is an evaluation of how much information can be hidden in a digital image. Watermarking capacity research studies how to transmit more watermark information. In watermarking schemes, the image is considered as a communication channel to transmit messages. Watermark power should be constrained according to the content of the image. We analyse the shortcomings of some previous watermarking capacity methods and present a watermarking capacity method using the noise visibility function, and discuss the watermarking capacity of blind watermarking and non-blind watermarking.","Watermarking,
Pixel,
Digital images,
Information analysis,
Humans,
Educational institutions,
Computer science,
Communication channels,
Decoding,
Robustness"
PSO embedded evolutionary programming technique for nonconvex economic load dispatch,"This paper proposes a hybrid method that integrates the main features of particle swarm optimization (PSO) and evolutionary programming (EP) for solution of nonconvex economic load dispatch (ELD) problems having nonlinearities like valve point loadings. Algorithms based on PSO, Evolutionary programming (EP) and PSO embedded EP techniques have been developed and tested on a practical nonconvex ELD problem with valve point loading effects considered in the cost functions. Numerical results show that all the algorithms are capable of finding feasible near global solutions within a reasonable time but PSO embedded EP-algorithm with Gaussian mutation appears to outperform the other two in terms of convergence speed, solution time and quality of solution.",
Multi-agent simulation of collaborative strategies in a supply chain,"The bullwhip effect is the amplification of the order variability in a supply chain. This phenomenon causes important financial cost due to higher inventory levels and agility reduction. In this paper, we study, for each company in a supply chain, the individual incentive to collaborate to reduce this problem. To achieve this, we simulate a supply chain inspired by the Quebec forest industry, in which each company is an agent that uses one of three ordering schemes. Each ordering scheme represents a level of collaboration. One run of the simulation is done with fifty (50) weeks for each of the 36 = 729 combinations of these 3 ordering schemes among the 6 companies of the simulation. In each run, we evaluate each company¿s inventory holding and backorder costs. These outcomes are used to build a game in the normal form, which is next analyzed using Game Theory. In particular, we find two Nash equilibria incurring the minimum cost of the supply chain. We also note that there are no Nash equilibria in which some companies do not collaborate: collaborating companies have no incentive to stop collaboration.",
Size-based scheduling policies with inaccurate scheduling information,"Size-based scheduling policies, such as shortest remaining processing time (SRPT), have been studied since the 1960s and have been applied in various areas, including packet networks and Web server scheduling. SRPT has been proven to be optimal in the sense that it yields - compared to any other conceivable strategy - the smallest mean value of occupancy and therefore also of waiting and delay time. One important prerequisite to applying size-based scheduling is to know the sizes of all jobs in advance, which are unfortunately not always available. No work has been done to study the performance of size-based scheduling policies when only inaccurate scheduling information is available. We study the performance of SRPT and fair sojourn protocol (FSP) as a function of the correlation coefficient between the actual job sizes and estimated job sizes. We have developed a simulator that supports both M/G/l/m and G/G/n/m queuing models. The simulator can be driven by trace data or synthetic data produced by a workload generator we have developed that allows us to control the correlation. The simulations show that the degree of correlation has a dramatic effect on the performance of SRPT and FSP, and that a reasonably good job size estimator makes both SRPT and FSP outperform processor sharing (PS) in both mean response time and slowdown.",
Fuzzy rough sets: beyond the obvious,"Rough set theory was introduced in 1982. Soon it was combined with fuzzy set theory, giving rise to a hybrid model, involving fuzzy sets and fuzzy relations, which appears to be a natural, elegant generalization. In this paper we reveal that in the fuzzification process an important step seems to be overlooked. The most fascinating part is that this forgotten step arises from the true essence of fuzzy set theory: namely, that an element can belong to a given degree to more than one fuzzy set at the same time.","Fuzzy sets,
Rough sets,
Fuzzy set theory,
Set theory,
Uncertainty,
Mathematical model,
Mathematics,
Computer science,
Information systems,
Fuzzy systems"
Designing high performance DSM systems using InfiniBand features,"Software DSM systems do not perform well because of the combined effects of increase in communication, slow networks and the large overhead associated with processing the coherence protocol. Modern interconnects like Myrinet, Quadrics and InfiniBand offer reliable, low latency (around 5.0 /spl mu/s point-to-point), and high-bandwidth (10.0 Gbps in 4X InfiniBand). These networks also support efficient memory- based communication primitives like RDMA-Read and RDMA-Write, which allow remote reading and writing of data respectively without receiver intervention. These supports can be leveraged to reduce overhead in a software DSM system. In this paper, we propose a new scheme NGDSM with two components ARDMAR and DRAW for page fetching and diffing, respectively. These components employ RDMA and atomic operations to efficiently implement the coherency scheme. The scheme NGDSM evaluated on an 8-node InfiniBand cluster using SPLASH-2 and TreadMarks applications shows better parallel speedup and scalability.","Protocols,
Modems,
Delay,
Computer networks,
Scalability,
High performance computing,
Information science,
Coherence,
Telecommunication network reliability,
Magnetic heads"
Scalable cooperative latency estimation,"This paper discusses SCoLE, a scalable system to estimate Internet latencies. SCoLE is based on GNP, which models Internet latencies in an n-dimensional Euclidean space. In contrast to GNP and other GNP-based systems, however, SCoLE does not employ any global space whose parameters must typically be negotiated by the participating hosts. Instead, it allows each host to construct its ""'private"" space and model interhost latencies in that space. The private space parameters as well as the modeling algorithm can be adjusted on a per-host basis, which improves system flexibility. More importantly, the mutual independence of private spaces results in higher SCoLE scalability, which is bound neither by the global negotiation of space parameters nor by global knowledge of any kind. We show that latency estimates performed in different private spaces are highly correlated. This allows SCoLE to be used in large-scale applications where consistent latency estimates need to be performed simultaneously by many independent hosts.","Delay,
Economic indicators,
Internet,
Scalability,
Peer to peer computing,
Computer science,
Force measurement,
Probes,
Joining processes,
Solid modeling"
Human behavior recognition for cooking support robot,"The recent development in information technology is making electrical household appliances computerized and networked. If the environment surrounding us could recognize our activities indirectly by sensors, novel services, which respect our activities, can be possible. In this paper, we propose a human activity recognition system, which infers the next human action by taking into account past human behavior, observed so far. We also developed a cooking support robot, which suggests what the human should do next, by voice and gesture.","Humans,
Intelligent sensors,
Intelligent robots,
Robot vision systems,
Information technology,
Home appliances,
Computer networks,
Robot sensing systems,
Sensor systems,
Mobile robots"
The Goodman-Kruskal coefficient and its applications in genetic diagnosis of cancer,"Increasing interest in new pattern recognition methods has been motivated by bioinformatics research. The analysis of gene expression data originated from microarrays constitutes an important application area for classification algorithms and illustrates the need for identifying important predictors. We show that the Goodman-Kruskal coefficient can be used for constructing minimal classifiers for tabular data, and we give an algorithm that can construct such classifiers.",
Map building without odometry information,The map building methods usually employed by mobile robots are based on the assumption that an estimate of the position of the robot can be obtained from odometry readings. In this paper we propose three methods that build a geometrical global map by integrating partial maps without using any odometry information. We focus on the problem of integrating a sequence of partial maps that specifies the order in which the partial maps must be integrated. Experimental results show the effectiveness of our approach in different types of environments.,"Buildings,
Mobile robots,
Robot sensing systems,
Computer science,
Scattering,
Resumes,
Simultaneous localization and mapping,
Robot localization,
Machinery,
Kalman filters"
Quality-based adaptive resource management architecture (QARMA): a CORBA resource management service,"Summary form only given. We describe the quality-based adaptive resource management architecture, QARMA, a framework for resource management within CORBA. QARMA consists of three major components: the system repository service, the resource management service, and the enactor service. QARMA serves as a basis for integration of existing CORBA services and management mechanisms into a single, coherent framework for resource management. QARMA supports the management of a wide variety of applications developed using various development paradigms, easily integrates with other management and infrastructure components that already exist as CORBA services, and is easily extended to allow the use of new resource management mechanisms as they become available.","Resource management,
Yarn,
Processor scheduling,
Computer architecture,
Computer science,
Standardization,
Real time systems,
Middleware,
Distributed computing,
Load management"
Simultaneous localization and mapping using multiple view feature descriptors,"We propose a vision-based SLAM algorithm incorporating feature descriptors derived from multiple views of a scene, incorporating illumination and viewpoint variations. These descriptors are extracted from video and then applied to the challenging task of wide baseline matching across significant viewpoint changes. The system incorporates a single camera on a mobile robot in an extended Kalman filter framework to develop a 3D map of the environment and determine egomotion. At the same time, the feature descriptors are generated from the video sequence, which can be used to localize the robot when it returns to a mapped location. The kidnapped robot problem is addressed by matching descriptors without any estimate of position, then determining the epipolar geometry with respect to a known position in the map.","Simultaneous localization and mapping,
Mobile robots,
Orbital robotics,
Computer science,
Lighting,
Robot vision systems,
Cameras,
Computational geometry,
Silicon,
Bonding"
A framework for contractual resource sharing in coalitions,"We develop a framework for specifying and reasoning about policies for sharing resources in coalitions, focussing here on a particular, common type of contract in which coalition members agree to make available some total amount of specified resource over a given time period. The main part of the framework is a policy language with two basic elements: 'obligations' (of a member enterprise to provide a total amount of resource over a given time period) express the coalition policy, and 'entitlements' (granted by an enterprise to other coalition members) express the local policies of the coalition members. We discuss the conditions under which a local policy can be said to be in compliance with, or meet, the obligations of a coalition policy, and the conditions under which an obligation, and by extension a contract, can be said to be violated or fulfilled.",
Transformed Social Interaction: Decoupling Representation from Behavior and Form in Collaborative Virtual Environments,"Computer-mediated communication systems known as collaborative virtual environments (CVEs) allow geographically separated individuals to interact verbally and nonverbally in a shared virtual space in real time. We discuss a CVE-based research paradigm that transforms (i.e., filters and modifies) nonverbal behaviors during social interaction. Because the technology underlying CVEs allows a strategic decoupling of rendered behavior from the actual behavior of the interactants, conceptual and perceptual constraints inherent in face-to-face interaction need not apply. Decoupling algorithms can enhance or degrade facets of nonverbal behavior within CVEs, such that interactants can reap the benefits of nonverbal enhancement or suffer nonverbal degradation. Concepts underlying transformed social interaction (TSI), the ethics and implications of such a research paradigm, and data from a pilot study examining TSI are discussed.",
Extraction of fixed dimension patterns from varying duration segments of consonant-vowel utterances,"Classification models based on multilayer perceptron (MLP) or support vector machine (SVM) have been commonly used for complex pattern classification tasks. These models are suitable for classification of fixed dimension patterns. However, durations of consonant-vowel (CV) utterances vary not only for different classes, but also for a particular CV class. It is necessary to develop a method for representing the CV utterances by patterns of fixed dimension. For CV utterances, vowel onset point (VOP) is the instant at which the consonant part ends and the vowel part begins. Important information necessary for classification of CV utterances is present in the region around the VOP. A segment of fixed duration around the VOP can be processed to extract a pattern of fixed dimension to represent a CV utterance. Accurate detection of vowel onset points is important for recognition of CV utterances of speech. In this paper, we propose an approach for detection of VOP, based on dynamic time alignment between a reference pattern of a CV class and the pattern of an utterance of that class. The results of studies show that the hypothesised VOPs using the proposed approach have less deviation from their actual locations.","Support vector machines,
Support vector machine classification,
Multilayer perceptrons,
Pattern classification,
Speech recognition,
Laboratories,
Computer science,
Production,
Neural networks"
SRB: shared running buffers in proxy to exploit memory locality of multiple streaming media sessions,"With the falling price of the memory, an increasing number of multimedia servers and proxies are now equipped with a large DRAM memory space. Caching media objects in the memory of a proxy helps to reduce network traffic, disk I/O bandwidth requirement, and data delivery latency. The running buffer approach and its alternatives are representative techniques to cache streaming data in the memory. However, there are two limits in the existing techniques. First, although multiple running buffers for the same media object co-exist in a given processing period, data sharing among the multiple buffers is not considered. Second, user access patterns are not insightfully considered in the buffer management. In this paper, we propose two techniques based on shared running buffers (SRB) in the proxy to address these limits. Considering user access patterns and characteristics of the requested media objects, our techniques adoptively allocate memory buffers to fully utilize the currently buffered data of streaming sessions, with the aim to reduce both the server load and the network traffic. Experimentally comparing with several existing techniques, we show that the proposed techniques have achieved significant performance improvement by effectively using the shared running buffers.","Streaming media,
Network servers,
Bandwidth,
Delay,
Telecommunication traffic,
Computer science,
Mobile computing,
Educational institutions,
Laboratories,
Multimedia systems"
Online training of SVMs for real-time intrusion detection,"To break the strong assumption that most of the training data for intrusion detectors are readily available with high quality, conventional SVM, robust SVM and one-class SVM are modified respectively in virtue of the idea from online support vector machine (OSVM) in this paper, and their performances are compared with that of the original algorithms. Preliminary experiments with 1998 DARPA BSM data set indicate that the modified SVMs can be trained online and the results outperform the original ones with less support vectors (SVs) and training time without decreasing detection accuracy. Both of these achievements benefit an effective online intrusion detection system significantly.","Support vector machines,
Intrusion detection,
Support vector machine classification,
Training data,
Detectors,
Robustness,
Information science,
Computer networks,
Earth,
Computer security"
Impact of data transformations on memory bank locality,"High-energy consumption presents a problem for sustainable clock frequency and deliverable performance. In particular, memory energy consumption of array-intensive applications can be overwhelming due to poor cache locality. One option for reducing memory energy is to adopt a banked memory architecture, where memory space is divided into banks and each bank can be powered down if it is not in active use. An important issue here is the bank access pattern, which determines opportunities for saving energy. In this paper, we present a compiler-based data layout transformation strategy for increasing the effectiveness of a banked memory architecture. The idea is to transform the array layouts in memory in such a way that two loop iterations executed one after another access the data in the same bank as much as possible; the remaining banks can be placed into a low-power mode. Our simulation-based experiments with nine array-intensive applications show significant savings in memory energy consumption.","Energy consumption,
Costs,
Memory architecture,
SDRAM,
Computer science,
Power engineering and energy,
Clocks,
Frequency,
Application software,
Banking"
Optimal self-dual codes over F/sub 2//spl times/F/sub 2/ with respect to the Hamming weight,"In this paper, we study optimal self-dual codes and type IV self-dual codes over the ring F/sub 2//spl times/F/sub 2/ of order 4. We give improved upper bounds on minimum Hamming and Lee weights for such codes. Using the bounds, we determine the highest minimum Hamming and Lee weights for such codes of lengths up to 30. We also construct optimal self-dual codes and type IV self-dual codes.","Hamming weight,
Upper bound,
Linear code,
Algebra,
Error correction codes,
Computer science,
Informatics,
Cryptography,
Galois fields"
Hybrid self-learning fuzzy PD+I control of unknown linear and nonlinear systems,"A human being is capable of learning how to control many complex systems without knowing the mathematical model behind such systems, so there must exist some way to imitate that behavior with a machine. A novel hybrid self-learning controller is proposed that is capable of learning how to control unknown linear and nonlinear processes incorporating a human-like learning behavior. The controller is comprised of a Fuzzy PD controller plus a conventional I controller and its corresponding gains are tuned using a human-like learning algorithm in order to reach specified goals of steady-state error (SSE), settling time (Ts) and percentage of overshooting (PO). Among the systems tested are first and second order linear systems, nonlinear pendulum and the nonlinear equations of Van der Pol, Rayleigh and Damped Mathieu. Analysis and simulation of a second order linear and nonlinear pendulum is provided to demonstrate that the proposed controller has excellent results.",
Exploring the relationship between personal and public annotations,"Today people typically read and annotate printed documents even if they are obtained from electronic sources like digital libraries. If there is a reason for them to share these personal annotations online, they must re-enter them. Given the advent of better computer support for reading and annotation, including tablet interfaces, do people ever share their personal digital ink annotations as is, or do they make substantial changes to them? What can we do to anticipate and support the transition from personal to public annotations? To investigate these questions, we performed a study to characterize and compare students' personal annotations as they read assigned papers with those they shared with each other using an online system. By analyzing over 1,700 annotations, we confirmed three hypotheses: (1) only a small fraction of annotations made while reading are directly related to those shared in discussion; (2) some types of annotations - those that consist of anchors in the text coupled with margin notes - are more apt to be the basis of public commentary than other types of annotations; and (3) personal annotations undergo dramatic changes when they are shared in discussion, both in content and in how they are anchored to the source document. We then use these findings to explore ways to support the transition from personal to public annotations.","Software libraries,
Collaborative work,
Computer interfaces,
Ink,
User interfaces,
Brushes,
Computer science,
Information retrieval,
Documentation,
Human factors"
AMUN - autonomic middleware for ubiquitous environments applied to the smart doorplate project,"We envision future office buildings that partly or fully implement a flexible office organization. These organizational principles save office space, but require a sophisticated software system that is highly dynamic, scalable, context-aware, self-configuring, self-optimizing and self-healing. We therefore propose an autonomic middleware approach for ubiquitous in-door environments.","Middleware,
Pervasive computing,
Monitoring,
Computer science,
Software systems,
Context awareness,
Scalability,
Information analysis,
Runtime environment,
Computer architecture"
I/O placement for FPGAs with multiple I/O standards,"In this paper, we present the first exact approach to solve the constrained input/output (I/O) placement problem for field programmable gate arrays (FPGAs) that support multiple I/O standards. We derive a compact integer linear program formulation for the constrained I/O placement problem. The size of the integer linear program derived is independent of the number of I/O objects to be placed and, hence, is scalable to very large design instances. For example, for a Xilinx Virtex-E FPGA, the number of integer variables required is never more than 32 and is much smaller for practical design instances. Extensive experimental results using a noncommercial integer linear program solver shows that it only takes seconds to solve the resultant integer linear program in practice. In addition, we also propose a new overall placement flow to place both core logic and I/Os.","Field programmable gate arrays,
Programmable logic arrays,
Communication standards,
Logic arrays,
Clocks,
Standards organizations,
Heuristic algorithms,
Testing,
Operating systems,
Computer science"
A system for ensuring data integrity in grid environments,"Data integrity has to become one of the central concerns of large-scale distributed computing systems such as the grid, whose primary products are the results of computation. In order to maintain the integrity of this data, the system must be resilient to diverse attacks and tampering. The system should also encourage positive influences on its integrity in addition to discouraging or eliminating negative ones. In this paper we develop a model of trust for grid participants based on the use of reputation systems and associated feedback mechanisms.",
Collaborative learning patterns: assisting the development of component-based CSCL applications,"The creation of a framework of software components and their associated software design patterns would provide great benefits for the development of reusable, flexible, and customizable component-based CSCL applications. The development of such a framework implies that software developers have a proper understanding of the key concepts and principles of the domain of interest. The achievement of this understanding is particularly difficult in the CSCL domain, where there is a big separation among abstractions used by educational science experts and those used by software developers. In order to alleviate this problem, we propose, justifies, and illustrates the use of the so-called collaborative learning patterns: detailed descriptions of well-accepted types of collaborative learning activities defined by collaborative learning experts. We also present the initial steps that would be followed so that software developers identify software components applicable to several types of component-based CSCL applications. All this proposal is illustrated with the jigsaw and pyramid collaborative learning patterns and their use in the development of a real CSCL application according to the unified process software development methodology.","Collaborative work,
Application software,
Software reusability,
Software engineering,
Software design,
Programming,
Collaborative software,
Petroleum,
Proposals,
Software systems"
Path diminution in disjoint multipath routing for mobile ad hoc networks,"An ad hoc network is a collection of mobile nodes connected through multi-hop wireless links without the required intervention of any centralized access point or existing infrastructure. Design of disjoint multipath routing protocols in an ad hoc environment is a challenging task. We have considered disjoint multipath routing schemes with path diminution in an ad hoc environment. Also, we have discussed the trade-off between the number of multiple paths searched and route establishment overheads.","Intelligent networks,
Mobile ad hoc networks,
Ad hoc networks,
Routing protocols,
Telecommunication traffic,
Network topology,
Ash,
Computer science,
Mobile computing,
Spread spectrum communication"
Delay optimal low-power circuit clustering for FPGAs with dual supply voltages,"This paper presents a delay optimal FPGA clustering algorithm targeting low power. We assume that the configurable logic blocks of the FPGA can be programmed using either a high supply voltage (high-Vdd) or a low supply voltage (low-Vdd). We carry out the clustering procedure with the guarantee that the delay of the circuit under the general delay model is optimal, and in the meantime, logic blocks on the non-critical paths can be driven by low-Vdd to save power. We explore a set of dual-Vdd combinations to find the best ratio between low-Vdd and high-Vdd to achieve the largest power reduction. Experimental results show that our clustering algorithm can achieve power savings by 20.3% on average compared to the clustering result for an FPGA with a single high-Vdd. To our knowledge, this is the first work on dual-Vdd clustering for FPGA architectures.","Delay,
Field programmable gate arrays,
Clustering algorithms,
Algorithm design and analysis,
Integrated circuit interconnections,
Permission,
Computer science,
Low voltage,
Logic circuits,
Logic design"
GCap: Graph-based Automatic Image Captioning,"Given an image, how do we automatically assign keywords to it? In this paper, we propose a novel, graph-based approach (GCap) which outperforms previously reported methods for automatic image captioning. Moreover, it is fast and scales well, with its training and testing time linear to the data set size. We report auto-captioning experiments on the ""standard"" Corel image database of 680 MBytes, where GCap outperforms recent, successful auto-captioning methods by up to 10 percentage points in captioning accuracy (50% relative improvement).",
POSE: getting over grainsize in parallel discrete event simulation,"Parallel discrete event simulations (PDES) encompass a broad range of analytical simulations. Their utility lies in their ability to model a system and provide information about its behavior in a timely manner. Current PDES methods provide limited performance improvements over sequential simulation. Many logical models for applications have fine granularity making them challenging to parallelize. In POSE, we examine the overhead required for optimistically synchronizing events. We have designed an object model based on the concept of visualization and new adaptive optimistic methods to improve the performance of finegrained PDES applications. These novel approaches exploit the speculative nature of optimistic protocols to improve single-processor parallel over sequential performance and achieve scalability for previously hard-to-parallelize fine-grained simulations.","Discrete event simulation,
Object oriented modeling,
Computational modeling,
Synchronization,
Safety,
Analytical models,
Clocks,
System recovery,
Computer science,
Computer simulation"
Song-specific bootstrapping of singing voice structure,"We present some experiments in the semi-automatic extraction of singing voice structure. The main characteristic of the proposed approach is that the segmentation is performed specifically for each individual song using a process we call bootstrapping. In bootstrapping, a small random sampling of the song is annotated by the user. This annotation is used to learn the song-specific voice characteristics and the trained classifier is subsequently used to classify and segment the whole song. We present experimental results on a collection of pieces with jazz singers that show the potential of this approach and compare it with the traditional approach of using multiple songs for training. It is our belief that the idea of song-specific bootstrapping is applicable to other types of music and audio computer-supported annotation.","Music information retrieval,
Character recognition,
Speech recognition,
Sampling methods,
Data mining,
Content based retrieval,
Computer science,
Speech analysis,
Signal processing,
Instruments"
3D face recognition using local shape map,"This paper firstly proposes a new scheme for description of 3D local shape, termed local shape map (LSM). The LSM for a point on the surface is a 2D histogram constructed from mapping 3D coordinates of surface's points within a sphere centralized at this point into a 2D space. The similarity metric for LSM is given. Secondly, application of LSM to 3D face recognition is presented, which is performed by incorporating LSM into an ad-hoc voting method. It does not require registering two face models. With a dataset of 31 facial range images for six subjects, in presence of significant change in pose and slight variation in expression, the equal error rate of 2.98% is obtained. The experimental results also demonstrate the performance of 3D face recognition using LSM outperforms that using spin image.","Face recognition,
Shape,
Image recognition,
Histograms,
Probes,
Computer science,
Voting,
Error analysis,
Humans,
Lighting"
Electronic sourcing with multi-attribute auctions,"Multi-attribute reverse auctions have been proposed as market institutions for ""electronic request for quotation"" buying processes. The choice of a multi-attribute auction institution for corporate sourcing is a challenge for the auctioneer in terms of utility maximization and allocational efficiency. The authors report on a computer based laboratory experiment in a sole sourcing scenario of a single, indivisible object and investigate whether a multi-attribute reverse English and a multi-attribute reverse Vickrey auction institution lead to identical outcomes with respect to the buyer's utility, suppliers' profits and allocational efficiency. The results show no significant difference in suppliers' profits. However, the English auction institution leads to both higher allocational efficiency and buyer's utility and is thus recommendable to corporate buyers. The documented breakdown of the outcome equivalence of the two auction institutions is attributed to bidders deviating from the dominant bidding strategy. The deviations are analyzed and explained by learning effects.",
Users inventing ways to enjoy new mobile services - the case of watching mobile videos,"The introduction and marketing of third-generation mobile services has not been enough to make them a commercial success. User involvement and user innovations are apparently needed before such success can be achieved. We handed out mobile phones with video capability to test users to see what kind of meaningful contexts they might find for watching streaming mobile video. There were at least two different contexts in which they considered it natural to view mobile video. Firstly, they were able to avoid boring situations by entertaining themselves. Secondly, mobile video made it possible to share experiences with other people. Singing karaoke together or watching cartoons with children offered fun moments and represented a type of use not frequently found in visions of mobile entertainment.",
An agent design pattern classification scheme: capturing the notions of agency in agent design patterns,"Agent technology is increasingly being used to develop software systems in different domains. Hence particular design problems are recurring in different multiagent systems development projects. In order to enhance the reuse of proven solutions to recurring design problems, agent design patterns are being identified and documented. Understandably, most of the available work on agent design patterns reflect object oriented concepts (classes, objects, inheritance, etc) and exhibit an implementation bias. It is preferable for these patterns to reflect the notions of agency and be described at the right level of abstraction. In this paper, we present a new agent design pattern classification scheme that is structured to better reflect the notions of agency and allow varying levels of abstraction in describing agent design patterns. We also show how four different agent design patterns fit into our classification scheme.","Pattern classification,
Software systems,
Software engineering,
Application software,
Computer science,
Multiagent systems,
Buildings,
Software design,
Software agents,
Organizing"
Computer modeling the ATLAS trigger/DAQ system performance,"In this paper, simulation (""computer modeling"") of the Trigger/data acquisition (DAQ) system of the ATLAS experiment at the LHC accelerator is discussed. The system will consist of a few thousand end nodes, which are interconnected by a large Ethernet network. The nodes will run various applications under the Linux operating system (OS). Predictions for the latency, throughput and queue development in various places have been obtained. Results are presented with respect to the application of traffic shaping to reduce the probability of possible frame loss (which may cause severe performance degradation).",
A study of host-based IDS using system calls,"Intrusion detection systems (IDS) are complimentary to other security mechanisms such as access control and authentication. While signature based IDS are limited to known attacks only, anomaly based IDS are capable of detecting novel attacks. However, anomaly based systems usually trade performance for efficiency. We analyze various anomaly based IDS and list the strengths and weaknesses of different schemes. We conclude that the abstract stack model proposed by D. Wagner and D. Dean (see Proc. IEEE Symp. on Security and Privacy, 2001) shows best performance in detecting various types of attacks, while it suffers from substantial runtime overhead owing to its non deterministic nature. In a recently published approach utilizing code instrumentation, J.T. Giffin et al. (see Proc. NDSS Conf., 2004) minimize the runtime overhead while approaching the detection capability of the abstract stack model.","Intrusion detection,
Information security,
Runtime,
Information systems,
Computer science,
Computer security,
Access control,
Authentication,
Design engineering,
Computer bugs"
Smart download on the go: a wireless Internet application for music distribution over heterogeneous networks,"The maturing distributed file sharing technology implemented by Napster has first enabled the dissemination of musical content in digital forms, permitting to costumers an ubiquitous reach to stored music files from around the world. In the post-Napster era, the Apple iTunes online music service has hit a record share of 16.7% in the MP3 player market. This is only the most prominent example of the success of digital music distribution based on packet network technologies. However, to the best of our knowledge, the most noteworthy aspect of the success of digital music distribution is that little about this music delivery technology is really new. To deeply change the trend of this technology business, we claim that wireless technologies must come on the scene. In particular, the digital music delivery model may take benefit by the integration of the wired Internet with a plethora of several, alternative wireless technologies, such as, for example, WiFi, WPAN and 3G. In this challenging context, we have developed a wireless Internet application designed to support the distribution of digital music to handheld devices. The main novelty of our software application amounts to its ability in providing a seamless music delivery service even in the presence of horizontal and vertical handoffs. We have taken measurements from real-world experiments that show the efficacy of the system we have developed.","IP networks,
Multiple signal classification,
Computer science,
Application software,
Handheld computers,
Web and internet services,
Cellular phones,
Peer to peer computing,
Digital audio players,
Layout"
Synchronous collaboration in distance education: a case study on a science course,"This paper describes our experience with introduction of synchronous collaborative problem solving activities in the frame of a distance learning computer science undergraduate course of the Hellenic Open University (HOU). Groups of students worked collaboratively at a distance in order to build a flowchart of an algorithm to a given problem. The technological and organization issues involved, the first findings of analysis of peer student interaction during this study, as well as some general implications for distance education are discussed.","Collaboration,
Computer aided software engineering,
Computer science,
Peer to peer computing,
Collaborative work,
Computer aided instruction,
Distance learning,
Flowcharts,
Bandwidth,
Protocols"
"Enabling social dimensions of learning through a persistent, unified, massively multi-user, and self-organizing virtual environment","Existing online learning experiences lack the social dimension that characterizes learning in the real world. This social dimension extends beyond the traditional classroom into the university's common areas where learners build knowledge and understanding through serendipitous and collaborative exchanges both within and across traditional subject area boundaries. A next generation virtual learning environment (VLE) can address the limitations of current online systems by providing a richer social context for online learning. We describe the end-user properties of a highly-scalable self-organizing Croquet-based VLE that fosters dynamic group learning experiences and the development of communities of practice. This proposed VLE provides the capacity to merge the institutional infrastructure for academic computing, enterprise-level networks, Squeak/Croquet-based content authoring, and the educational principles of constructivist pedagogy.",
The influence of grid shape and asynchronicity on cellular evolutionary algorithms,"In This work we study cellular evolutionary algorithms, a kind of decentralized heuristics, and the importance of the induced exploration/exploitation balance on different problems. It is shown that, by choosing synchronous or asynchronous update policies, the selection pressure, and thus the exploration/exploitation tradeoff, can be influenced directly, without using additional ad hoc parameters. Synchronous algorithms of different neighborhood-to-topology ratio, and asynchronous update policies are applied to a set of benchmark problems. Our conclusions show that the update methods of the asynchronous versions, as well as the ratio of the decentralized algorithm, have a marked influence on its convergence and on its accuracy.","Shape,
Evolutionary computation,
Content addressable storage,
Information systems,
Grid computing,
Computer science,
Convergence,
Genetics,
Topology,
Testing"
Comparing static and dynamic measurements and models of the Internet's AS topology,"Capturing a precise snapshot of the Internet's topology is nearly impossible. Recent efforts have produced autonomous-system (AS) level topologies with noticeably divergent characteristics, even calling into question the widespread belief that the Internet's degree distribution follows a power law. In turn, this casts doubt on Internet modeling efforts, since validating a model on one data set does little to ensure validity on another data set, or on the (unknown) actual Internet topology. We examine six metrics-three existing metrics and three of our own-applied to two large publicly-available topology data sets. Certain metrics highlight differences between the two topologies, while one of our static metrics and several dynamic metrics display an invariance between the data sets. Invariant metrics may capture properties inherent to the Internet and independent of measurement methodology, and so may serve as better gauges for validating models. We continue by testing nine models-seven existing models and two of our own-according to these metrics applied to the two data sets. We distinguish between growth models that explicitly add nodes and links over time in a dynamic process, and static models that add all nodes and links in a batch process. All existing growth models show poor performance according to at least one metric, and only one existing static model, called Inet, matches all metrics well. Our two new models-growth models that are modest extensions of one of the simplest existing growth models-perform better than any other growth model across all metrics. Compared with Inet, our models are very simple. As growth models, they provide a possible explanation for the processes underlying the Internet's growth, explaining, for example, why the Internet's degree distribution is more skewed than baseline models would predict",
Job fairness in non-preemptive job scheduling,"Job scheduling has been a much studied topic over the years. While past research has studied the effect of various scheduling policies using metrics such as turnaround time, slowdown, utilization etc., there has been little research on how fair a nonpreemptive scheduling policy is. We propose an approach to assessing fairness in nonpreemptive job scheduling. Our basic model of fairness is that no later arriving job should delay an earlier arriving job. We quantitatively assess the fairness of several job scheduling strategies and propose a new strategy that seeks to improve fairness.","Processor scheduling,
Computer science,
Production,
High performance computing,
Computational modeling,
Delay effects,
Runtime,
Dynamic scheduling,
Parallel processing,
Sections"
Decomposing polygon meshes by means of critical points,"Polygon mesh is among the most common data structures used for representing objects in computer graphics. Unfortunately, a polygon mesh does not capture high-level structures, unlike a hierarchical model. In general, high-level abstractions are useful for managing data in applications. In this paper, we present a method for decomposing an object represented in polygon meshes into components by means of critical points. The method consists of steps to define the root vertex of the object, define a function on the polygon meshes, compute the geodesic tree and critical points, decide the decomposition order, and extract components using backwards flooding. We have implemented the method. The preliminary results show that it works effectively and efficiently. The decomposition results can be useful for applications such as 3D model retrieval and morphing.","Shape,
Computer vision,
Geophysics computing,
Floods,
Humans,
Partitioning algorithms,
Computer science,
Data structures,
Computer graphics,
Application software"
Lower bounds for testing bipartiteness in dense graphs,"We consider the problem of testing bipartiteness in the adjacency matrix model. The best known algorithm, due to Alon and Krivelevich, distinguishes between bipartite graphs and graphs that are /spl epsi/-far from bipartite using 0(1//spl epsi//sup 2/) queries. We show that this is optimal for non-adaptive algorithms, up to polylogarithmic factors. We also show a lower bound of /spl Omega/(1//spl epsi//sup 3/2/) for adaptive algorithms.",
"""Age of computers""; an innovative combination of history and computer game elements for teaching computer fundamentals","Age of computers (AoC) is an online multiplayer game used in teaching of computer fundamentals in a M.Sc. study in computer science. It supplements traditional auditorium lectures by a rich set of problems in a computer game like environment. The story of the game is linked to the historical periods (epochs) of computers, and the content is organized in rooms or places on a map. A chat window for each historical period is used for communication with other students and teaching assistants. The first version of AoC was used in a class with 250 students. Almost all students agreed that ""AoC is more motivating than traditional exercises"", and a majority claimed that they learn more through AoC than by traditional exercises. We are currently working on an improved version of AoC for the 2004 fall semester based on the feedback from an extensive questionnaire.","History,
Education,
Telecommunication computing,
Embedded computing,
Games,
Computer science,
Feedback,
Collaborative work,
Information science,
Computer aided instruction"
Performance analysis of audio codecs over real-time transmission protocol (RTP) for voice services over Internet protocol,"The real-time transmission of voice on a IP network is one of the key techniques in multimedia communication. This paper aims at evaluating the performance of a set of audio codecs best suitable for real-time voice transmission using the real-time transmission protocol (RTP) over an IP network. Voice is encoded using an audio codec and transmission is simulated by means of a RTP media simulator for an IP network. The client/server programming style is used for receiver and transmitter, in both design and implementation. Performance is measured based on the voice quality and objective quality measurements are presented.","Protocols,
Performance analysis,
Codecs,
Web and internet services,
IP networks,
Internet telephony,
Streaming media,
Bandwidth,
Multimedia communication,
Computer science"
Eliciting Truthful Feedback for Binary Reputation Mechanisms,"Reputation mechanisms offer an efficient way of building the necessary level of trust in electronic markets. Feedback about an agent's past behavior can be aggregated into a measure of reputation, and used by other agents for taking trust decisions. Unfortunately, true feedback cannot be automatically assumed. In the absence of Trusted Third Parties, the mechanism has to make it rational for agents to truthfully share reputation information. In this paper we describe two mechanisms that can be used in decentralized environments for eliciting true feedback. The mechanisms are accompanied by examples inspired by real scenarios.","Feedback,
Humans,
Games,
Decision making,
Artificial intelligence,
Laboratories,
Computer science,
Consumer electronics,
Internet,
Business communication"
A stream-weight optimization method for audio-visual speech recognition using multi-stream HMMs,"For multi-stream HMM that are widely used in audio-visual speech recognition, it is important to automatically and properly adjust stream weights. This paper proposes a stream-weight optimization technique based on a likelihood-ratio maximization criterion. In our audiovisual speech recognition system, video signals are captured and converted into visual features using HMM-based techniques. Extracted acoustic and visual features are concatenated into an audio-visual vector. A multi-stream HMM is obtained from audio and visual HMM. Experiments are conducted using Japanese connected digit speech recorded in real-world environments. Applying the MLLR (maximum likelihood linear regression) adaptation and our optimization method, we achieve a 29% absolute accuracy improvement and a 76% relative error rate reduction compared with the audio-only scheme.","Streaming media,
Optimization methods,
Speech recognition,
Hidden Markov models,
Automatic speech recognition,
Maximum likelihood linear regression,
Acoustic noise,
Robustness,
Maximum likelihood estimation,
Computer science"
Hardness of approximating the shortest vector problem in lattices,"Let p > 1 be any fixed real. We show that assuming NP /spl nsube/ RP, it is hard to approximate the shortest vector problem (SVP) in l/sub p/ norm within an arbitrarily large constant factor. Under the stronger assumption NP /spl nsube/ RTIME(2/sup poly(log n)/), we show that the problem is hard to approximate within factor 2/sup log n1/2 - /spl epsi// where n is the dimension of the lattice and /spl epsi/> 0 is an arbitrarily small constant. This greatly improves all previous results in l/sub p/ norms with 1 < p < /spl infin/. The best results so far gave only a constant factor hardness, namely, 2/sup 1/p/ - /spl epsi/ by Micciancio and p/sup 1 - /spl epsi// in high l/sub p/ norms by Khot. We first give a new (randomized) reduction from closest vector problem (CVP) to SVP that achieves some constant factor hardness. The reduction is based on BCH codes. Its advantage is that the SVP instances produced by the reduction behave well under the augmented tensor product, a new variant of tensor product that we introduce. This enables us to boost the hardness factor to 2/sup log n1/2-/spl epsi//.","Lattices,
Tensile stress,
Polynomials,
Paper technology,
History,
Books,
Gaussian processes,
Geometry,
Linear programming,
Computer science"
JAPARA - a Java parallel random number generator library for high-performance computing,"Summary form only given. Random number generators are one of the most common numerical library functions used in scientific applications. The standard random number generator provided within Java is fine for most purposes, however it does not adequately meet the needs of large-scale scientific applications, such as Monte Carlo simulations. Previous work has addressed some of these problems by extending the standard random API in Java and providing an implementation that includes a choice of several different generator algorithms. One issue that was not addressed in this work was concurrency. Implementations of the standard Java random number generator use synchronized methods to support the use of the generator across multiple Java threads, however this is a sequential bottleneck for parallel applications. Here we present a proposal for further extending the standard API to support parallel generation of random number streams, which we have implemented in JAPARA, a Java parallel random number generator library for high-performance computing.","Java,
Random number generation,
Libraries,
Concurrent computing,
Large-scale systems,
Yarn,
Cryptography,
Hardware,
Computer science,
Australia"
A framework for interactive visualization of component-based software,"We advocate the use of visual tooling for the development and maintenance of component-based software systems. Our contribution is twofold. First, we demonstrate how an interactive visualization tool effectively supports understanding large component based software. Secondly, we show how to design such a tool in order to make it applicable for a wide range of component systems and investigation goals. We demonstrate our approach by several visualization scenarios for real-world systems.",
Using entropy impurity for improved 3D object similarity search,"Similarity search in 3D object databases is becoming an important problem in multimedia retrieval, with many practical applications. We investigate methods for improving the effectiveness in a retrieval system that implements multiple feature extraction algorithms to choose from. Our techniques are based on the entropy impurity measure, widely used in the context of decision trees. We propose a method for the a priori estimation of individual feature vector performance, given a query. We then define two approaches that use this estimator to improve the retrieval effectiveness. Our experimental results show that significant improvements are achievable using these methods",
Modeling uncertainties in publish/subscribe systems,"In the publish/subscribe paradigm, information providers disseminate publications to all consumers who have expressed interest by registering subscriptions. This paradigm has found wide-spread applications, ranging from selective information dissemination to network management. However, all existing publish/subscribe systems cannot capture uncertainty inherent to the information in either subscriptions or publications. In many situations, exact knowledge of either specific subscriptions or publications is not available. Moreover, especially in selective information dissemination applications, it is often more appropriate for a user to formulate her search requests or information offers in less precise terms, rather than defining a sharp limit. To address these problems, this paper proposes a new publish/subscribe model based on possibility theory and fuzzy set theory to process uncertainties for both subscriptions and publications. Furthermore, an approximate publish/subscribe matching problem is defined and algorithms for solving it are developed and evaluated.",
Modelling SAMIPS: a synthesisable asynchronous MIPS processor,"The last fifteen years have witnessed a resurgence of interest in asynchronous digital design techniques as they promise to liberate VLSI systems from clock skew problems, offer the potential for low power and high performance and encourage a modular design philosophy which makes incremental technological migration a much easier task. This activity has revealed a need for modelling and simulation techniques suitable for the asynchronous design style. The concurrent process algebra communication sequential processes (CSP) is increasingly advocated as particularly suitable for this purpose. This paper discusses the modelling of SAMIPS, a synthesisable asynchronous MIPS processor core, in Balsa, a CSP-based, asynchronous hardware description language and synthesis tool.","Computational modeling,
Synchronization,
Protocols,
Clocks,
Computer simulation,
Hardware,
Very large scale integration,
Logic design,
Telecommunication computing,
Computer science"
Adjacent orientation vector based fingerprint minutiae matching system,"Minutia matching is the most popular approach to fingerprint recognition. We analyzed a novel fingerprint feature named adjacent orientation vector, or AOV, for fingerprint matching. In the first stage, AOV is used to find possible minutiae pairs. Then one minutiae set is rotated and translated. This is followed by a preliminary matching to ensure reliability as well as a fine matching to overcome possible distortion. Such method has been deployed to a payroll and security access information system and its workability is encouraging. The information system aims to offer a highly secured and automated identification system for payroll tracking as well as authorized access to working areas.","Fingerprint recognition,
Pattern matching,
Information systems,
Image matching,
Computer science,
Information security,
Workability,
Bifurcation,
Noise robustness,
Filter bank"
Program comprehension for Web services,"Web services provide programmatic interaction between organisations within large heterogeneous distributed systems. Using recent experiences of constructing and enhancing a data integration system for the health domain, based on Web services, we draw conclusions about new problems for program comprehension. These derive from the fundamentally dynamic and distributed nature of the environment. We suggest several key research topics for program comprehension, arguing that these are crucial if software constructed from Web services is to be supportable over a long period. Finally, we briefly summarise some wider conclusions about understanding Web services at the application domain level.",
A real options approach for prioritization of a portfolio of information technology projects: a case study of a utility company,"The valuation of information technology (IT) investments is particularly challenging because it is characterized by long payback periods, uncertainty, and changing business conditions. Corporate budgeting methods use accounting-based criteria like return on investment (ROT), internal rate of return (IRR), and payback period which were designed for projects with no option features. However, the uncertainties underlying IT investment decisions and the inability of traditional discounted cash flow (DCF) methods to incorporate the impact of flexibility on project valuation, force executives to rely on gut instinct when finalizing IT investment decisions. Real options analysis (ROA) has been suggested as a capital budgeting tool because it explicitly accounts for the value of future flexibility in management decision making. This paper deals with the application of a nested real options model to evaluate and prioritize a portfolio of information technology projects. It elaborate the valuation and prioritization of a real-world portfolio of IT initiatives under consideration for funding. It is illustrated using real world data from EnergyCo, a large utility facing challenges on many fronts due to uncertainties surrounding energy deregulation and Internet-based energy trading.",
Steps toward derandomizing RRTs,"We present two new motion planning algorithms, based on the rapidly exploring random tree (RRT) family of algorithms. These algorithms represent the first work in the direction of derandomizing RRTs; this is a very challenging problem due to the way randomization is used in RRTs. In RRTs, randomization is used to create Voronoi bias, which causes the search trees to rapidly explore the state space. Our algorithms take steps to increase the Voronoi bias and to retain this property without the use of randomization. Studying these and related algorithms would improve our understanding of how efficient exploration can be accomplished, and would hopefully lead to improved planners. We give experimental results that illustrate how the new algorithms explore the state space and how they compare with existing RRT algorithms.",
A Suite of Metamodels as a Basis for a Classification of Visual Languages,"Metamodeling frameworks for the definition and management of visual languages allow the implementation of visual environments based on some abstract notion of visual entity and of relations among them. We propose a suite of metamodels able to accommodate most commonly used visual paradigms, built as progressive specialisation of a root meta-meta model. This approach facilitates the design and implementation of new, general purpose as well as domain specific, visual languages by allowing the progressive construction of language-independent service layers","Concrete,
Unified modeling language,
Computer science,
Operations research,
Metamodeling,
Environmental management,
Visualization,
Computational complexity,
Humans,
Computer languages"
A replicated experiment of usage-based and checklist-based reading,"Software inspection is an effective method to detect faults in software artefacts. Several empirical studies have been performed on reading techniques, which are used in the individual preparation phase of software inspections. Besides new experiments, replications are needed to increase the body of knowledge in software inspections. We present a replication of an experiment, which compares usage-based and checklist-based reading. The results of the original experiment show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. We present the data of the replication together with the original experiment and compares the experiments. The main result of the replication is that it confirms the result of the original experiment. This replication strengthens the evidence that usage-based reading is an efficient reading technique.","Inspection,
Fault detection,
Testing,
Communication system software,
Computer science,
Software metrics,
Bioreactors,
Springs,
Software engineering"
A Bayesian morphometry algorithm,"Most methods for structure-function analysis of the brain in medical images are usually based on voxel-wise statistical tests performed on registered magnetic resonance (MR) images across subjects. A major drawback of such methods is the inability to accurately locate regions that manifest nonlinear associations with clinical variables. In this paper, we propose Bayesian morphological analysis methods, based on a Bayesian-network representation, for the analysis of MR brain images. First, we describe how Bayesian networks (BNs) can represent probabilistic associations among voxels and clinical (function) variables. Second, we present a model-selection framework, which generates a BN that captures structure-function relationships from MR brain images and function variables. We demonstrate our methods in the context of determining associations between regional brain atrophy (as demonstrated on MR images of the brain), and functional deficits. We employ two data sets for this evaluation: the first contains MR images of 11 subjects, where associations between regional atrophy and a functional deficit are almost linear; the second data set contains MR images of the ventricles of 84 subjects, where the structure-function association is nonlinear. Our methods successfully identify voxel-wise morphological changes that are associated with functional deficits in both data sets, whereas standard statistical analysis (i.e., t-test and paired t-test) fails in the nonlinear-association case.","Bayesian methods,
Image analysis,
Magnetic analysis,
Brain,
Atrophy,
Performance analysis,
Biomedical imaging,
Medical tests,
Performance evaluation,
Magnetic resonance"
Rainbow: architecture-based self-adaptation with reusable infrastructure,"Software-based systems today are increasingly expected to dynamically self-adapt to accommodate resource variability, changing user needs, and system faults. Recent work uses closed-loop control based on external models to monitor and adapt system behavior at run time. Taking this externalized approach, the Rainbow framework we have developed uses software architectural models to dynamically monitor and adapt a running system. A key goal and primary challenge of this framework is to support the reuse of adaptation strategies and infrastructure across different systems. We show that the separation of a generic adaptation infrastructure from system-specific adaptation knowledge makes this reuse possible.",
Learning OWL ontologies from free texts,"Ontology based approach has been popularized by current semantic Web researches. However, ontology building by hand has proven to be a very hard and error-prone task and become the bottleneck of ontology acquiring process. WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics. The paper proposes an ontology learning approach, which uses WordNet lexicon resources to build a standard OWL ontology model. The approach can the automation of ontology building and be very useful in ontology-based applications.","OWL,
Ontologies,
Semantic Web,
Buildings,
Automation,
Humans,
Knowledge engineering,
Laboratories,
Computer science education,
Educational institutions"
A study on optimal hierarchy in multi-level hierarchical mobile IPv6 networks,"Hierarchical mobile IPv6 (HMIPv6) is an enhanced mobile IPv6 in order to reduce signaling overhead and to support seamless handoff in IP-based wireless/mobile networks. To support more scalable services, HMIPv6 can be organized as a multilevel hierarchy architecture (i.e, tree structure). However, since the multi-level HMIPv6 results in additional packet processing overhead, it is necessary to consider the overall cost and to find the optimal level to minimize the overall cost. In this paper, we investigate this problem, namely the design of a multi-level HMIPv6 with optimal hierarchy. To do this, we formulate the location update cost and the packet delivery cost in the multi-level HMIPv6. Based on the formulated cost functions, we present the optimal hierarchy level in the multi-level HMIPv6 to minimize the total cost. In addition, we investigate the effects of session-to-mobility ratio (SMR) on the total cost and the optimal hierarchy. The numerical results, which show various relationships among network size, optimal hierarchy, and SMR, can be utilized to design an optimal HMIPv6 network.","Intelligent networks,
Cost function,
Mobile computing,
Mobile radio mobility management,
Land mobile radio cellular systems,
Tree data structures,
Analytical models,
Computer science,
Next generation networking,
Advertising"
On encoding and enumerating threshold functions,"In this paper, we deal with encoding and enumerating threshold functions defined on n-dimensional binary inputs. The paper specifies situations in which the unique characterization of functions from a given class is preserved by usage of an appropriate set of discrete moments. Moreover, sometimes such a characterization (coding) is optimal with respect to the number of necessary bit rate per coded function. By estimating the number of possible values of the discrete moments used, several upper bounds (for different classes of threshold functions) are derived, some of which are better than those previously known.",
PSO approaches to coevolve IPD strategies,"This paper investigates two different approaches using particle swarm optimisation (PSO) to evolve strategies for iterated prisoner's dilemma (IPD). Strategies evolved by the lesser known binary PSO algorithm are compared to strategies evolved by neural networks that were trained using PSO. Evolved strategies are compared against well-known game theory strategies, with positive results. The presence of noise during IPD interactions are also investigated, and evolved strategies are compared against the same well-known game theory strategies in a noisy environment.","Game theory,
Artificial neural networks,
Computer science,
Africa,
Neural networks,
Working environment noise,
Genetic algorithms,
Particle swarm optimization,
Mathematical model,
Environmental economics"
The use of mobile computing to support SMS dialogues and classroom discussions in a literature course,"In this paper we present a communication and discussion toolkit based on sending short messages designed for use in schools. This toolkit is the result of a student's project at the University of Duisburg-Essen in collaboration with a local secondary school. Our approach uses PDAs in a wireless network to build an environment which emulates sending of short messages with mobile phones. The messages created by the students are collected in a database and establish a base for a discussion and analysis later on using Cool Modes, a graph based modeling and discussion system.","Mobile computing,
Mobile communication,
Personal digital assistants,
Mobile handsets,
Educational institutions,
Databases,
Educational technology,
Technological innovation,
Design engineering,
Computer science"
Hanging Services: an investigation of context-sensitivity and mobile code for localised services,"As Web service technology evolves, the idea of context-aware services gains more interest. An idea is that different sets of services will dynamically drop into the mobile users' devices depending on their contexts. To do this effectively requires location modelling and representation as well as spontaneity in downloading and executing the service interface on a mobile device. This paper introduces the concept and an implementation of hanging services that supports proactive and ad hoc context-aware services in mobile environments. This system works on top of an 802.11b wireless network. The prototype implementation is done using Web services and highly compact mobile code applications using Microsoft .NET compact framework.",
Information fusion in face identification,"Information fusion of multi-modal biometrics has attracted much attention in recent years. However, this paper focuses on the information fusion in single models, that is, the face biometric. Two different representation methods, gray level intensity and Gabor feature, are exploited for fusion. We study the fusion problem in face recognition at both the face representation level and the confidence level. At the representation level, both the PCA feature fusion and the LDA feature fusion are considered, while at the confidence level, the sum rule and the product rule are investigated. We show through experiments on FERET face database and our own face database that appropriate information fusion can improve the performance of face recognition and verification. This suggests that gray level intensity and Gabor feature compensate for each other, based on the feasible fusion.","Face recognition,
Spatial databases,
Pixel,
Feature extraction,
Biometrics,
Fingerprint recognition,
Computer science,
Research and development,
Content addressable storage,
Principal component analysis"
Technology factors in corporate adoption of mobile cell phones: a case study analysis,"This paper studies the technology factors that companies consider important in deciding to adopt and deploy wireless devices designed for mobile telephony and information services, the extent of current use of wireless cell phones, the extent of existing utilization and/or planning for Web-enabled cell phone use, the constraining factors in their deployment decisions, and how such decisions are made, and the practical technology implications for decision-making. Wireless cell phone technology has complex characteristics that often make the process of corporate adoption decisions difficult. This study seeks to shed light on this process to help decision makers. The conceptual model combines the TAM and innovation adoption/diffusion models, adding the factors of security, cost, reliability, digital standards, technology product suitability, and future Web connectivity. Case study methodology is utilized for five manufacturing and technology firms. A key finding is that the most important technology decision factors are security, reliability, and Web connectivity. Customer service was the most important non-technology factor. Although the current uses are dominated by voice, Web-enabled capability dominates future decision-making.",
LessLog: a logless file replication algorithm for peer-to-peer distributed systems,"Summary form only given. The technique of replicating frequently-accessed files to other nodes has been widely used in a high-performance distributed system to reduce the load of the nodes hosting these files. Traditional file replication algorithms rely on the analysis of client-access logs to determine the location of the replicated nodes. We present LessLog, a loglessfile replication algorithm, developed for a peer-to-peer distributed system. We first construct a lookup tree for each node. LessLog uses bitwise operations to determine the location of the replicated node without any client-access history. In addition, each replication is guaranteed to reduce the workload of the replicating node by half. A fault-tolerant LessLog model is also presented. The experimental results show that LessLog successfully and efficiently reduces the load of overloaded nodes.",
A dynamic HLR location management scheme for PCS networks,"In this paper, a dynamic HLR (home location register) scheme for location management in PCS (personal communications service) networks is presented. The proposed scheme provides a dynamic copy of mobile terminal location information in the nearest (current) HLR database. A modified table lookup procedure is also proposed for determining the current HLR easily. It allows the location registration and call delivery to be performed efficiently. An analytical model is developed for studying the performance of the proposed scheme. The performance study shows that the proposed scheme significantly reduces the system overhead for location management in PCS networks.","Personal communication networks,
Computer science,
Databases,
Computer network management,
Cities and towns,
Analytical models,
Wireless communication,
Routing,
Information retrieval,
Base stations"
Allocate fair payoff for cooperation in wireless ad hoc networks using Shapley Value,"Summary form only given. In wireless mobile ad hoc networks (MANET), energy is a scarce resource. Though cooperation is the basis of network services, due to the limited energy reserve of each node, there is no guarantee any given protocols would be followed by nodes managed by different authorities. Instead of treating the selfish nodes as a security concern and trying to eliminate them, we propose a novel way to encourage cooperative works - rewarding service providers according to their contributions. Nodes in a MANET can form coalitions to reduce aggregate transmission power on each hop along a route. The payment of each node in a coalition is determined by using Shapley Value, a well-known concept in game theory for allocating payoff for each member in a cooperative coalition. We present the contribution reward routing protocol with Shapley Value (CAP-SV). It achieves the objective of truthfulness. The performance of CAP-SV is studied by simulations using ns-2. Analysis and experimental results show a routing protocol with the consideration of the incentives of individual nodes stimulates cooperation and improves network lifetime without significantly diminishing the performance of the whole network.",
A robust visual tracking system for patient motion detection in SPECT: hardware solutions,"The goal of our project is to devise a robust method to track and compensate patient motion by combining an emission data based approach with a visual tracking system (VTS) that provides an independent estimate of motion. In a previous study, we used the Polaris infra-red system (NDI Inc., Ontario, Canada) as the gold standard to show that the VTS can accurately track motion without having the limitations of the Polaris. In the present work, we present the latest hardware configuration of the VTS and our solution for temporal synchronization between the SPECT and the optical acquisitions. The current version of the VTS includes stereo imaging with sets of optical network cameras with lighting, a SPECT/VTS calibration phantom, a black stretchable garment with reflective spheres to track chest motion, and a computer to control the cameras. The computer also stores the JPEG files generated by the optical cameras with timing synchronization to the list-mode acquisition of events on our SPECT system. Five Axis PTZ 2130 network cameras (Axis Communications AB, Lund, Sweden) were used to track motion of spheres with a highly retro-reflective coating using stereo methods. The calibration phantom is comprised of seven reflective spheres, and radioactivity can be added to the tip of the mounts holding the spheres. This phantom is used to determine the transformation to be applied to convert the motion detected by the VTS into the SPECT coordinates system. Synchronization is assessed in two ways. First, optical cameras are aimed at a digital clock and the elapsed time estimated by the cameras is compared to the actual time shown by the clock in the images. Synchronization is also assessed by moving a radioactive and reflective sphere three times during concurrent VTS and SPECT acquisitions and comparing the time at which motion occurred in the optical and SPECT images. The results show that optical and SPECT images stay synchronized within a 150 ms range. The 100Mbit network load is less than 10%, and the computer's CPU load is between 15 and 25%; thus, the VTS can be improved by adding more cameras or by increasing the image size and/or resolution while keeping an acquisition rate of 30 images per second per camera","Robustness,
Tracking,
Motion detection,
Hardware,
Cameras,
Synchronization,
Imaging phantoms,
Calibration,
Computer networks,
Optical computing"
Evolution of emergent behaviors for shooting game characters in Robocode,"Various digital characters, which are automatic and intelligent, are attempted with the introduction of artificial intelligence or artificial life. Since a character's behavior is designed by a developer, the style can be static and simple. Even complex patterns designed by a developer cannot satisfy various users and easily make them feel tedious. A game should maintain various and complex character's behaviors, but it is not easy for the developer to design them. In this paper, we adopt genetic algorithm to produce various and excellent behavior-styles for characters especially focusing on Robocode which is one of the promising simulators for artificial intelligence.","Artificial intelligence,
Games,
Intelligent robots,
Robotics and automation,
Genetic algorithms,
Service robots,
Computer science,
Automatic control,
Character generation,
Animation"
An Empirical Study of Feature Selection for Text Categorization based on Term Weightage,"This paper proposes a local feature selection (FS) measure namely, Categorical Descriptor Term (CTD) for text categorization. It is derived based on classic term weighting scheme, TFIDF. The method explicitly chooses feature set for each category by only selecting set of terms from relevant category. Although past literatures have suggested that the use of features from irrelevant categories can improve the measure of text categorization, we believe that by incorporating only relevant feature can be highly effective. The experimental comparison is carried out between CTD and five well-known feature selection measures: Information Gain, Chi-Square, Correlation Coefficient, Odd Ratio and GSS Coefficient. The results also show that our proposed method can perform comparatively well with other FS measures, especially on collection with highly overlapped topics.",
StoryMapper: a multimedia tool to externalize knowledge,"People with different skills and knowledge can contribute in future decisions and best-practices. In order to help us, there are different mechanisms to represent and organize knowledge. In this paper, we present and discuss a computer tool that it has been developed to support the externalization of tacit knowledge through a storytelling activity. The tool uses conceptual maps to represent knowledge. The group members may synchronous or asynchronously contribute to the development of a story.","Computer science,
Collaborative work,
Knowledge representation,
Vehicles,
Joining processes,
Organizing,
Collaborative software,
Knowledge management,
Memory management,
Environmental factors"
Genetic algorithm application on the job shop scheduling problem,"Based on the concepts of operation template and virtual job shop, this paper attempts to solve several job shop scheduling problems with different scale and analyzes the relationship among the population size, mutation probability, the number of evolving generations and the complexity of the undertaking problem visually by using the trend chart of the fitness curves. This visual analysis could provide some referencing information for the adjustment of genetic algorithm running parameters.",
"Performance analysis of the Globus Toolkit Monitoring and Discovery Service, MDS2","Monitoring and information services form a key component of a distributed system, or grid. A quantitative study of such services can aid in understanding the performance limitations, advise in the deployment of the monitoring system, and help evaluate future development work. To this end, we examined the performance of the Globus Toolkit/spl reg/ Monitoring and Discovery Service (MDS2) by instrumenting its main services using NetLogger. Our study shows a strong advantage to caching or prefetching the data, as well as the need to have primary components at well-connected sites.","Performance analysis,
Instruments,
Laboratories,
Prefetching,
Production systems,
Mathematics,
Computer science,
Computerized monitoring,
Access protocols,
Aggregates"
A hypersphere method for plant leaves classification,"The recognition of the tens of thousands of kinds of plants on Earth is an important and difficult task. If we use a nearest neighbour classifier to solve it, millions of training data are needed to obtain a high correct recognition rate. However, the corresponding classification process is quite time-consuming. We propose a new approach using a moving median centers hypersphere technique to recognize different plant leaves. With this new method, we can successfully decrease the classification time, and reduce the storage size without sacrificing the classification accuracy. Experimental results using real data show that this method is really efficient and effective in classifying different plant leaves.","Neural networks,
Training data,
Machine intelligence,
Multimedia computing,
Biomedical signal processing,
Computer science,
Geoscience,
Earth,
Plants (biology),
Agriculture"
Learning mixture models with the regularized latent maximum entropy principle,"This paper presents a new approach to estimating mixture models based on a recent inference principle we have proposed: the latent maximum entropy principle (LME). LME is different from Jaynes' maximum entropy principle, standard maximum likelihood, and maximum a posteriori probability estimation. We demonstrate the LME principle by deriving new algorithms for mixture model estimation, and show how robust new variants of the expectation maximization (EM) algorithm can be developed. We show that a regularized version of LME (RLME), is effective at estimating mixture models. It generally yields better results than plain LME, which in turn is often better than maximum likelihood and maximum a posterior estimation, particularly when inferring latent variable models from small amounts of data.","Entropy,
Maximum likelihood estimation,
Parametric statistics,
Machine learning,
Computer science,
Inference algorithms,
Robustness,
Yield estimation,
Iterative algorithms,
State estimation"
Voice capacity in IEEE 802.11 networks,"We are currently witnessing a boom in the deployment and usage of wireless local area networks (WLANs). Once only seen within the enterprise, WLANs are increasingly making their way into residential, commercial, industrial, and public areas. The recent efforts of carriers to integrate WLANs into their wide-area service offerings are testimony to their growing role in the future of wireless networking. Voice continues to be the most predominant wireless application, and, in order to integrate fully with existing and future cellular systems, WLANs must deliver a high quality voice service. We present an experimental evaluation of VoIP capacity in an IEEE 802.11b network. Moreover, we identify and quantify the contributing factors that limit the capacity to less than 10 voice calls per access point.","Intelligent networks,
Wireless LAN,
Wideband,
DSL,
Codecs,
Internet,
Computer science education,
Testing,
Collaboration,
Airports"
Generating realistic images from hydrothermal plume data,"Most data used in the study of seafloor hydrothermal plumes consists of sonar (acoustic) scans and sensor readings. Visual data captures only a portion of the sonar data range due to the prohibitive cost and physical infeasibility of taking sufficient lighting and video equipment to such extreme depths. However, visual images are available from research dives and from the recent IMAX movie, volcanoes of the deep sea. In this application paper, we apply existing lighting models with forward scattering and light attenuation to the 3D sonar data in order to mimic the visual images available. These generated images are compared to existing visual images. This can help the geoscientists understand the relationship between these different data modalities and elucidate some of the mechanisms used to capture the data.",
A flexible IO scheme for grid workflows,"Summary form only given. Computational grids have been proposed as the next generation computing platform for solving large-scale problems in science, engineering, and commerce. There is an enormous amount of interest in applications, called grid workflows in which a number of otherwise independent programs are run in a ""pipeline"". In practice, there are a number of different mechanisms that can be used to couple the models, ranging from loosely coupled file based IO to tightly coupled message passing. We propose a flexible IO architecture that provides a wide range of mechanisms for building grid workflows without the need for any source code modification and without the need to fix them at design time. Further, the architecture works with legacy applications. We evaluate the performance of our prototype system using a workflow in computational mechanics.","Grid computing,
High performance computing,
Application software,
Distributed computing,
Power engineering computing,
Instruments,
Large-scale systems,
Business,
Message passing,
Computer architecture"
The Waterbed Effect in Spectral Estimation,"In this lecture note, we present a textbook-like derivation of the waterbed effect result in Ninness (2003). Compared with Ninness, our analysis is much simpler and yet slightly more general in that it is not limited to rational spectral densities as in Ninness (it is noted, however, that Ninness has also derived a closed-form expression for the finite-m CRB on the relative variance of /spl Phi/(/spl omega/, /spl theta//spl circ/), which we do not).","Reactive power,
Frequency estimation,
Shape,
Control systems,
USA Councils,
Computer science,
Closed-form solution,
Taylor series"
Exact Pareto-optimal coordination of two translating polygonal robots on an acyclic roadmap,"We present an algorithm that computes the complete set of Pareto-optimal coordination strategies for two translating polygonal robots in the plane. A collision-free acyclic roadmap of piecewise-linear paths is given on which the two robots move. The robots have a maximum speed and are capable of instantly switching between any two arbitrary speeds. Each robot would like to minimize its travel time independently. The Pareto-optimal solutions are the ones for which there exist no solutions that are better for both robots. The algorithm computes exact solutions in time O(mn/sup 2/ log n), in which m is the number of paths in the roadmap, n is the number of coordination space vertices. An implementation is presented.",
Cluster-based support vector machines in text-independent speaker identification,"Based on statistical learning theory, support vector machines (SVM) is a powerful tool for various classification problems, such as pattern recognition and speaker identification etc. However, training SVM consumes large memory and long computing time. This work proposes a cluster-based learning methodology to reduce training time and the memory size for SVM. By using k-means based clustering technique, training data at boundary of each cluster were selected for SVM learning. We also applied this technique to text-independent speaker identification problems. Without deteriorating recognition performance, the training data and time can be reduced up to 75% and 87.5% respectively.","Support vector machines,
Support vector machine classification,
Training data,
Error correction,
Upper bound,
Computer science,
Power engineering and energy,
Electronic mail,
Statistical learning,
Pattern recognition"
Comparison of scheduling heuristics for grid resource broker,"We consider parallel task scheduling problems for hierarchical decentralized systems that consist of homogeneous computational resources such as clusters, PCs and supercomputers, and are geographically dispersed. We concentrate on two-level hierarchy scheduling: at the first level, the broker allocates computational tasks to the resource. At the second level, each resource schedules the tasks assigned to it using heuristics based, for instance, on strip-packing algorithms. The allocation strategies and efficiency of proposed hierarchical scheduling algorithms are discussed.","Resource management,
Processor scheduling,
Personal communication networks,
Supercomputers,
Scheduling algorithm,
Dynamic scheduling,
Control systems,
Mesh generation,
Concurrent computing,
Clustering algorithms"
Evaluation of 802.11a for streaming data in ad-hoc networks,"Advances in communication and processing have made ad-hoc networks of wireless devices a reality. One application is home entertainment systems where multiple Home-to-Home (H20) devices collaborate as peers to stream audio and video clips to a household. In this study, we investigate the feasibility of IEEE 802.11a protocol in combination with both TCP and UDP to realize a H20 device. Challenges include lossy connections, unfair allocation of bandwidth between multiple simultaneous transmissions, and the exposed node limitation [22], [19], [13], [4]. Our primary contribution is an empirical study of 802.11a to quantify these factors and their significance. Our multi-dimensional experimental design consists of the following axes: distance between participating devices, number of intermediate H20 devices used to route a stream from a producing H20 device to a consuming H2O device, and simultaneous number of active streams in the same radio range. Both operating system and application level routing were considered_ Obtained results demonstrate the following lessons. First, with a multi-hop UDP transmission, in the absence of congestion control, transient bottlenecks result in a high loss rate. Hence, a transport protocol with congestion control is essential for streaming of continuous media within a H2O cloud. Second, 802.11a does not drop TCP connections in the presence of many competing transmissions (802.11b drops connections [22]). Third, we observed fairness when transmitting several hundred Megabytes (MB) of data, among multiple competing 1- hop TCP and UDP flows. Fourth, while there is unfair allocation of bandwidth with an exposed node, the observed bandwidths are sufficient to stream a high-quality video clip (with a 4 Mbps display bandwidth requirement). These results indicate streaming of data is feasible with an ad-hoc network of wireless devices employing the 80211a protocol.",
Finding all hops shortest paths,"In this letter, we introduce and investigate a new problem referred to as the All Hops Shortest Paths (AHSP) problem. The AHSP problem involves selecting, for all hop counts, the shortest paths from a given source to any other node in a network. We derive a tight lower bound on the worst-case computational complexities of the optimal comparison-based solutions to AHSP.","Routing,
Cost function,
Computational complexity,
Quality of service,
Computer science education,
Educational technology,
Computer networks,
Electronic mail"
Electrohydrodynamic flow and its effect on ozone transport in corona radical shower reactor,"New arguments supporting the supposition that the ozone is transported along a corona discharge radical shower (CDRS) reactor by the electrohydrodynamic (EHD) flow are presented. The arguments are based on the analysis of the corona discharge, which is a precursor of the EHD flow in the CDRS reactor, and on the measurements of velocity field of the EHD flow in the CDRS reactor by the particle image velocimetry (PIV). The obtained velocity flow structures and the possible causes of the ozone transport in the CDRS, i.e., diffusion, additional gas flow, EHD flow, and convection by the main flow, were discussed basing on the conservation equations for the EHD flow. The discussion showed that the EHD flow plays a dominant role in the ozone transport. This is also supported by the results of a simple phenomenological model for one-dimensional description of EHD-induced ozone transport in the CDRS reactor. The results of the computer simulation based on this model explained the main features of the measured ozone distribution in the CDRS reactor, establishing the EHD flow as the main cause of the ozone transport from the discharge region upstream, i.e., against the main flow.","Electrohydrodynamics,
Corona,
Inductors,
Image analysis,
Velocity measurement,
Particle measurements,
Fluid flow,
Equations,
Computer simulation,
Fluid flow measurement"
Rewriting History to Exploit Gain Time,"With modern processors and more dynamic application requirements it is becoming increasingly difficult to produce tight upper bounds on the worst-case execution time of real-time tasks. As a result, at run-time, considerable spare CPU capacity (termed gain time) becomes available that must be usefully employed if cost effective real-time systems are to be engineered. In this paper we introduce a scheme by which gain time is exploited by retrospectively reassigning execution time from a task’s own budget to the gain time that later become available. As a result of changing the system’s execution history, spare capacity is immediately reallocated and hence preserved. The proposed scheme is shown to work with fixed priority dispatching, the use of servers to provide temporal firewalls, and other capacity sharing approaches. Evaluations are provided via simulations.","History,
Real time systems,
Upper bound,
Runtime,
Quality of service,
Computational modeling,
Computer science,
Application software,
Costs,
Dispatching"
Scalable cryptographic key management in wireless sensor networks,"We propose a scalable key management scheme for sensor networks consisting of a large-scale random deployment of commodity sensor nodes that are anonymous. The proposed scheme relies on a location-based virtual network infrastructure and is built upon a combinatorial formulation of the group key management problem. Primary features of our scheme include autonomously computing group keys, and dynamically computing, using a simple hash function, the mapping of nodes to group keys. The scheme scales well in the size of the network and supports dynamic setup and management of arbitrary structures of secure group communications in large-scale wireless sensor network.","Cryptography,
Intelligent networks,
Wireless sensor networks,
Computer network management,
Management training,
Large-scale systems,
Computer networks,
Computer science,
Network servers,
Communication system security"
Performance evaluation of service discovery strategies in ad hoc networks,"Service discovery is an important and necessary component of ad hoc networks. To fit within the context of such networks, a post-query model with several service discovery strategies named post-query strategies (Barbeau and Kranakis (2003)) have been proposed, which focus on locating services over an ad hoc network. Each strategy consists of a sequence of post-query protocols executed in rounds. These strategies include: the greedy, incremental, uniform memoryless and with memory post-query strategies. Inspired by these proposals, we define the conservative post-query strategy. This paper represents the performance evaluation of these five strategies in combination with the DSR protocol and DSDV protocol: based on the simulation results.","Intelligent networks,
Ad hoc networks,
Network servers,
Routing protocols,
Context-aware services,
Context modeling,
Proposals,
Bluetooth,
Computer science,
Spread spectrum communication"
Pruning local feature correspondences using shape context,"We propose a novel approach to improve the distinctiveness of local image features without significantly affecting their robustness with respect to image deformations. Local image features have proven to be successful in computer vision tasks involving partial occlusion, background noise, and various types of image deformations. However, the relatively high number of outliers that have to be rejected from the correspondences set, formed during the search for similar features, still plagues this approach. The task of rejecting outliers is usually based on estimating the global spatial transform suffered by the features in the correspondences set. This presents two problems: (i) it cannot properly deal with non-rigid objects, and (ii) it is sensitive to a high number of outliers. Here, we address these problems by combining typical local features with shape context. A performance evaluation shows that this new semi-local feature generally provides higher distinctiveness and robustness to image deformations, thus potentially increasing the inlier/outlier ratio in the correspondences set. Also, we show that in wide baseline stereo matching, and non-rigid motion applications, the use of the novel semi-local feature not only provides robustness to non-rigid deformations, but also produces a higher inlier/outlier ratio than the standard Hough clustering of the global spatial transform of parameters.","Application software,
Computer vision,
Shape measurement,
Histograms,
Noise robustness,
Background noise,
Stereo vision,
Computer science,
Object recognition,
Filters"
Some applications of Grobner bases,"Previously (see ibid., vol.6, no.2, 2004), we discussed the geometry of linear and algebraic systems. We also defined ideals and bases so that we could introduce the concept of Grobner bases for algebraic system solving. In this article, we give more details about Grobner bases and describe their main application (algebraic system solving) along with some surprising derived ones: inclusion of varieties, automatic theorem-proving in geometry, expert systems, and railway interlocking systems.","Polynomials,
Dictionaries,
Algebra"
Cognitive process during program debugging,"Program debugging is a critical and complex activity in software engineering. Accurate and fast debugging leads to high quality software and a short time-to-market. Debugging involves a very demanding cognitive process. In a case study, we found all six levels of Bloom's taxonomy of cognitive learning, from ""knowledge"" through ""comprehension"", ""application"", ""analysis"", ""synthesis"", and ""evaluation"". The involvement of the higher levels of Bloom's taxonomy, such as synthesis and evaluation, indicates that program debugging is a difficult cognitive task. This fact may explain the difference between novices and experts in debugging effectiveness.","Taxonomy,
Programming profession,
Psychology,
Software engineering,
Time to market,
Computer science,
Software debugging,
Software quality,
Testing,
Medical diagnostic imaging"
Communications for improving policy computation in distributed POMDPs,"Distributed Partially Observable Markov Decision Problems (POMDPs) are emerging as a popular approach for modeling multiagent teamwork where a group of agents work together to jointly maximize a reward function. Since the problem of finding the optimal joint policy for a distributed POMDP has been shown to be NEXP-Complete if no assumptions are made about the domain conditions, several locally optimal approaches have emerged as a viable solution. However, the use of communicative actions as part of these locally optimal algorithms has been largely ignored or has been applied only under restrictive assumptions about the domain. In this paper, we show how communicative acts can be explicitly introduced in order to find locally optimal joint policies that allow agents to coordinate better through synchronization achieved via communication. Furthermore, the introduction of communication allows us to develop a novel compact policy representation that results in savings of both space and time which are verified empirically. Finally, through the imposition of constraints on communication such as not going without communicating for more than K steps, even greater space and time savings can be obtained.","Distributed computing,
Teamwork,
Uncertainty,
Observability,
Permission,
Multiagent systems,
Performance loss,
Humans"
Non-detrimental Web application security scanning,"The World Wide Web has become a sophisticated platform capable of delivering a broad range of applications. However, its rapid growth has resulted in numerous security problems that current technologies cannot address. Researchers from both academic and private sector are devoting a considerable amount of resources to the development of Web application security scanners (i.e., automated software testing platforms for Web application security auditing) with some success. However, little is known about their potential side effects. It is possible for an auditing process to induce permanent changes in an application's state. Due to this potential, we have so far avoided large-scale empirical evaluations of our Web Application Vulnerability and Error Scanner (WAVES). we introduce a testing methodology that allows for harmless auditing, define three testing modes - heavy, relaxed, and safe modes, and report our results from two experiments. In the first, we compared the coverage and side effects of the three scanning modes using 5 real-world Web applications chosen from the 38 found vulnerable in a previous static verification effort. In the second, we used the relaxed mode to conduct a 48-hour test involving 1120 random Web sites, of which 55 were found to be vulnerable.","Application software,
National security,
Information security,
Web sites,
Filtering,
Inspection,
Information science,
Computer science,
Software testing,
Large-scale systems"
Derivatives for multiple-valued functions induced by Galois field and Reed-Muller-Fourier expressions,"In classical mathematics, Newton-Leibniz differential operators determine the coefficients in the Taylor series. At the same time, there are relationships between the Fourier coefficients of a (differentiable) function and its derivative. By analogy, Boolean differential operators are viewed as coefficients of Taylor-MacLaurin series-like expressions for switching functions, usually denoted as Reed-Muller expressions. Spectral interpretation of these expressions, permits us to relate the Boolean difference to the coefficients in Fourier series-like expressions for switching functions. This paper considers these two possible ways of the introduction of differential operators for multiple-valued (MV) functions. We defined the logic derivatives and Gibbs derivatives for MV functions as coefficients in the Taylor-MacLaurin series for MV functions and through relationships to Fourier series-like coefficients, respectively.",
MatrixPro - a tool for demonstrating data structures and algorithms ex tempore,"Algorithm animation has been researched since early 1980's and many different visualization systems have been developed. However, most of them have remained as research prototypes and almost none have gained wide acceptance by teachers as classroom demonstration tools. One of the key reasons for this has been that preparing animations has been too laborious. In this paper, we demonstrate a new tool, MatrixPro, in which animations are generated in terms of visual algorithm simulation. The user can graphically invoke ready-made operations available in the library to simulate the working of real algorithms. Since the system understands the semantics of the operations, the teacher can ex tempore demonstrate the execution of algorithms with different input sets, or work with ""what-if"" questions students ask in lectures. Such an approach lowers considerably the step for adopting algorithm visualization as a regular lecture tool.","Data structures,
Animation,
Data visualization,
Computer science,
Graphical user interfaces,
Design engineering,
Prototypes,
Libraries,
Education,
History"
A path compression technique for on-demand ad-hoc routing protocols,"Ad-hoc on-demand routing protocols like AODV establish and maintain routes on-demand. However, the paths established by on-demand protocols, which are optimal during the route establishment phase, become sub-optimal over time due to node mobility. We propose a path compression algorithm (PCA) that optimizes the established routes when feasible without incurring extra overhead. By utilizing a promiscuous mode of operation, nodes 'hear' hop counts embedded in the IP optional header of data packets to find shorter paths. PCA avoids unnecessary aggressive and ephemeral route updates, which improves the protocol performance. Routing path optimality is important as the optimal path reduces the packet drop ratio, the end-to-end delay, and the energy dissipation of end-to-end data transmission. We conducted simulations using GloMoSim to evaluate the performance of our algorithm and compared it with AODV and a related protocol in the literature - SHORT. Our simulation results show that PCA achieves high delivery rate, low control overhead, low end-to-end delivery and low average hop count compared to both AODV and SHORT.","Routing protocols,
Principal component analysis,
Ad hoc networks,
Delay,
Data communication,
Laboratories,
Computer science,
Compression algorithms,
Energy dissipation,
Communication system control"
DNA-based matching of digital signals,"Adleman with his pioneering work set the stage for the new field of bio-computing research (Science, vol.266, p.1021-1024, 1994). His main idea was to use actual chemistry to solve problems that are either unsolvable by conventional computers, or require an enormous amount of computation. The main focus of our research is to consider the application of molecular computing to the domain of digital signal processing (DSP). In this paper, we consider matching problems that arise in signal processing applications and are amenable to a DNA-based solution. Digital data are encoded in DNA sequences using a sophisticated codeword set that satisfies the noise tolerance constraint (NTC) that we introduce. NTC, one of the main contributions of our work, takes into account the presence of noise in digital signals by exploiting the annealing between non-perfect complementary sequences. We propose an algorithm to map binary values into DNA codewords by satisfying a number of constraints, including the NTC. Using that algorithm, we retrieved 128 codewords that enables us to use a DNA based approach to digital signal matching.","DNA,
Sequences,
Chemistry,
Application software,
Digital signal processing,
Biomedical signal processing,
Annealing,
Signal processing algorithms,
Temperature,
Chemicals"
The selective read-out processor for the CMS electromagnetic calorimeter,"This paper describes the selective read-out processor (SRP) proposed for the electromagnetic calorimeter (ECAL) of the CMS experiment at LHC (CERN). The aim is to reduce raw ECAL data to a level acceptable by the CMS DAQ system. For each positive level 1 trigger, the SRP is guided by trigger primitive generation electronics to identify ECAL regions with energy deposition satisfying certain programmable criteria. It then directs the ECAL read-out electronics to apply predefined zero suppression levels to the crystal data, depending whether the crystals fall within these regions or not. About 200 of high speed 1.6 Gbit/s I/O channels, asynchronous operation at up to 100 kHz level 1 trigger rate, a stringent real-time requirement of 5 /spl mu/s latency and flexibility in choice of selection algorithms are the main challenges of the SRP application. The architecture adopted for the SRP is based on modern parallel optic pluggable modules and high density FPGA devices with embedded processors and multi-gigabit transceivers. Implementation studies to validate proposed solutions are presented. The performance of envisaged selection algorithms is investigated with the CMS detector simulation software. The robustness of optical communication channels is estimated via direct measurements and calculations. The feasibility to perform data reduction operations within the allocated timing budget is verified by running a representative SRP firmware on a development board with a Xilinx Virtex2Pro FPGA device.",
Dynamic rate-selection for extending the lifetime of energy-constrained networks,"Wireless networks have a constraint on their functional lifetime. This is due to the limited energy capacity of batteries powering the wireless nodes. For extending the lifetime of such battery-operated networks, we present a scheme for dynamically selecting the transmission rate for each node in the network. The transmission rate is based on the available energy budget in each node's battery. The goal is to increase the network capability of delivering more packets. The rate selection for each node is subject to satisfying a QoS timing constraint on the packet delivery time. Through adaptively varying each node's rate, we extended the lifetime 10 times on average more transmitting at a maximum rate and delivered on average 7.5 times more data packets. When compared with a scheme that transmits data at a lower rates independent of the battery levels, our scheme delivers up to 12% more packets for the same available total energy.","Batteries,
Routing,
Wireless networks,
Energy consumption,
Quality of service,
Computer science,
Timing,
Energy efficiency,
Energy conservation,
Transceivers"
Fast and high quality overlap repair for patch-based texture synthesis,"Patch-based texture synthesis has proven to produce high quality textures faster than pixel-based approaches. Previous algorithms differ in how the regions of overlap between neighboring patches are treated. We present an approach that produces higher quality overlap regions than simple blending of patches or computing good boundaries, however, that is faster than resynthesizing invalid pixels using a classical per-pixel synthesis algorithm: we use a k-nearest neighbor (knn) data structure, obtained from the input texture in a precomputation step. Results from our implementation show that the algorithm produces high-quality textures, where the time complexity of the synthesis stage is linear in the number of resynthesized pixels and, therefore, scales well with the size of the input texture","Computer errors,
Computer science,
Data structures,
Principal component analysis,
Surface texture,
Markov random fields,
Acceleration,
Fourier transforms,
Pixel,
Shape"
Range cube: efficient cube computation by exploiting data correlation,"Data cube computation and representation are prohibitively expensive in terms of time and space. Prior work has focused on either reducing the computation time or condensing the representation of a data cube. We introduce range cubing as an efficient way to compute and compress the data cube without any loss of precision. A new data structure, range trie, is used to compress and identify correlation in attribute values, and compress the input dataset to effectively reduce the computational cost. The range cubing algorithm generates a compressed cube, called range cube, which partitions all cells into disjoint ranges. Each range represents a subset of cells with the same aggregation value, as a tuple which has the same number of dimensions as the input data tuples. The range cube preserves the roll-up/drill-down semantics of a data cube. Compared to H-cubing, experiments on real dataset show a running time of less than one thirtieth, still generating a range cube of less than one ninth of the space of the full cube, when both algorithms run in their preferred dimension orders. On synthetic data, range cubing demonstrates much better scalability, as well as higher adaptiveness to both data sparsity and skew.",
Face hallucination with pose variation,"Face hallucination is used to synthesize a high-resolution facial image from a low-resolution input. In this paper, we present a framework for face hallucination with pose variation. We derive a texture model consisting of a set of linear mappings between the Gabor wavelet features of the facial images of every two possible poses. Given a low-resolution facial image, its pose is first estimated using a SVM classifier. Then, the Gabor wavelet feature corresponding to the frontal face is computed by our texture model and the low-resolution frontal facial image is reconstructed from its Gabor wavelet feature by a novel algorithm we propose. Finally, the high-resolution face can be hallucinated by one of the hallucination approaches for frontal faces. Our framework is demonstrated by extensive experiments with high-quality hallucinated results.","Shape,
Support vector machines,
Support vector machine classification,
Image reconstruction,
Face recognition,
Computer science,
Surveillance,
Cameras,
Equations,
Image recognition"
Proxy-RED: an AQM scheme for wireless local area networks,"Wireless access points act as bridges between wired and wireless networks. Since the actually available bandwidth in wireless networks is much smaller than the bandwidth in wired networks, there is a disparity in channel capacity which makes the access point a significant network congestion point in the downstream direction. A current architectural trend in wireless local area networks (WLAN) is to move functionality from access points to a centralized gateway in order to reduce cost and improve features. In this paper, we study the use of RED, a well known active queue management (AQM) scheme, and explicit congestion notification (ECN) to handle bandwidth disparity between the wired and the wireless interface of an access point Then, we propose the proxy-RED scheme, as a solution for reducing the AQM overhead from the access point. Simulations-based performance analysis indicates that the proposed proxy-RED scheme improves overall performance of the network. In particular, the proxy-RED scheme significantly reduces packet loss rate and improves goodput for a small buffer, and minimizes delay for a large buffer size","Wireless LAN,
Bandwidth,
Throughput,
Wireless networks,
Electronic mail,
Quality of service,
Computer science,
Bridges,
Channel capacity,
Cost function"
AGILE: a general approach to detect transitions in evolving data streams,"In many applications such as e-commerce, system diagnosis and telecommunication services, data arrives in streams at a high speed. It is common that the underlying process generating the stream may change over time, either as a result of the fundamental evolution or in response to some external stimulus. Detecting these changes is a very challenging problem of great practical importance. The overall volume of the stream usually far exceeds the available main memory and access to the data stream is typically performed via a linear scan in ascending order of the indices of the records. In this paper, we propose a novel approach, AGILE, to monitor streaming data and to detect distinguishable transitions of the underlying processes. AGILE has many advantages over the traditional Hidden Markov Model, e.g., AGILE only requires one scan of the data.","Hidden Markov models,
Monitoring,
Data mining,
Intrusion detection,
Computer science,
Application software,
Telecommunication services,
Data analysis,
Change detection algorithms,
Process design"
Systematic static shadow detection,A systematic static shadow detection algorithm for color images is presented in this paper. The image is modeled by an undirected graph and the shadow detection is achieved through maximizing the graph probability using the EM algorithm. Further analysis shows the connection between our model and the relaxation-labeling (RL) model. Experiments clearly indicate that our method is superior to a state-of-the-art shadow detection algorithm.,
Genetic tuning on fuzzy systems based on the linguistic 2-tuples representation,"Linguistic fuzzy modeling allows us to deal with the modeling of systems building a linguistic model clearly interpretable by human beings. However, in this kind of modeling the accuracy and the interpretability of the obtained model are contradictory properties directly depending on the learning process and/or the model structure. Thus, the necessity of improving the linguistic model accuracy arises when complex systems are modeled. To solve this problem, one of the research lines of this framework in the last years has leaded up to the objective of giving more accuracy to the linguistic fuzzy modeling, without losing the associated interpretability to a high level. In this work, a new post-processing method of fuzzy rule-based systems is proposed by means of an evolutionary lateral tuning of the linguistic variables, with the main aim of obtaining fuzzy rule-based systems with a better accuracy and maintaining a good interpretability. To do so, this tuning considers a new rule representation scheme by using the linguistic 2-tuples representation model which allows the lateral variation of the involved labels. As an example of application of these kinds of systems, we analyze this approach considering a real-world problem.",
Measurement of disturbances radiated from personal computers in the WLAN frequency band,"Wireless LAN networks have become widely used in various environments. Since WLAN terminals are normally located nearby personal computers (PCs), electromagnetic disturbances emitted from the PCs potentially degrade the quality of the WLAN communication links. To investigate the effects of the PC disturbances on the wireless links, it is necessary to evaluate and model the characteristics of the radiated emissions from the PCs. We have therefore conducted measurements of the PC disturbances in the WLAN frequency band. The experimental results show that the harmonics of the base clock of a PC are major components of the disturbances in the WLAN frequency band above 2 GHz. It is also found that the harmonic spectrum is not a line spectrum but has a certain bandwidth. This is due to dithered clocking, which is a technique for reducing the spectral amplitude of the harmonics by intentionally sweeping the fundamental frequency of the clock signal. These findings imply that the PC disturbance in the WLAN band can be simulated by frequency modulated CW.","Frequency measurement,
Microcomputers,
Wireless LAN,
Personal communication networks,
Clocks,
Electromagnetic measurements,
Electromagnetic radiation,
Degradation,
Bandwidth,
Frequency modulation"
Particle Swarm Optimization for Constrained Layout Optimization,"Taking the layout problem of satellite cabins as the background, the authors make a study of the optimal layout problem of circle groups in a circular container with performance constraints of equilibrium and inertia, which belong to NP-Hard problem. This paper extends the heuristic called ""Particle Swarm Optimization"" (PSO) to deal with the Constrained Layout Optimization Problem, proposes the particle presentation for this problem, and compares the PSO with GA. By the contrast experiments of three test suits, it has been proved that the PSO is a more effective method for Constrained Layout Optimization problem.","Constraint optimization,
Satellites,
Containers,
NP-hard problem,
Design optimization,
Iron,
Sun,
Optimal control,
Computer science,
Testing"
Keyword fusion to support efficient keyword-based search in peer-to-peer file sharing,"Peer-to-peer (P2P) computing has become a popular distributed computing paradigm thanks to abundant computing power of modern desktop workstations and widely available network connectivity via the Internet. Although P2P file sharing provides a scalable alternative to conventional server-based approaches, providing efficient file search in a large scale dynamic P2P system remains a challenging problem. In this paper, we propose a set of mechanisms to provide a scalable keyword-based file search in distributed hash table (DHT)-based P2P systems. In particular, we address the problem induced by common keywords that are associated with a large number of files and thus require excessive storage consumptions from the hosting peers. Our proposed architecture, called keyword fusion, adaptively unburdens the peers overloaded with excessive storage consumptions due to common keywords and reduces network bandwidth consumption by transforming users' queries to contain more focused search terms. Through trace-driven simulations, we show that keyword fusion can reduces the storage consumption of the top 5% most loaded nodes by 50% and decrease the search traffic by up to 68% even in the modest scenarios of combining two keywords.","Peer to peer computing,
Distributed computing,
Computer networks,
Power engineering computing,
Workstations,
Large-scale systems,
Bandwidth,
Telecommunication traffic,
Intrusion detection,
Computer science"
A network layer based architecture for service discovery in mobile ad hoc networks,"Service discovery is an integral pail of constructing a self-configuring mobile ad hoc network (MANET). While several service discovery protocols have been developed, most of them are designed for infrastructure-based networks and thus not suitable to be used in MANET. On the other hand, service discovery protocols that have been designed for MANET suffer from problems with two critical issues. Firstly, they have limited scalability due to the extensive use of broadcast communication. Secondly, they usually lack context aware selection mechanisms. In this paper, we propose a network layer supported comprehensive service discovery solution that addresses the above issues and provides solutions in two parts. First we discuss a location aware network layer routing protocol that groups mobile nodes into clusters where a gateway at each cluster is responsible for routing. Then we propose a service discovery protocol that utilizes directories for service discovery that interact with lower network layer gateway configuration. Our service discovery solution includes an agent-based context aware service selection.",
Decomposition Methods for Linear Support Vector Machines,"In this letter, we show that decomposition methods with alpha seeding are extremely useful for solving a sequence of linear support vector machines (SVMs) with more data than attributes. This strategy is motivated by Keerthi and Lin (2003), who proved that for an SVM with data not linearly separable, after C is large enough, the dual solutions have the same free and bounded components. We explain why a direct use of decomposition methods for linear SVMs is sometimes very slow and then analyze why alpha seeding is much more effective for linear than nonlinear SVMs. We also conduct comparisons with other methods that are efficient for linear SVMs and demonstrate the effectiveness of alpha seeding techniques in model selection.",
Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality,,"Consumer electronics,
Learning,
Business,
Permission,
Computer science"
Real-time eye detection using face-circle fitting and dark-pixel filtering,"Real-time eye tracking is a challenging problem. A face-circle fitting (FCF) method is proposed to confirm the extracted face region. Once the face is detected, a dark-pixel filter (DPF) is used to perform eye detection and tracking. Furthermore, The proposed system has been implemented on a personal computer with a PC-camera and can perform eye detection and tracking in real-time. The correct eye identification rate is as high as 92% during the actual operation","Face detection,
Filtering,
Filters,
Humans,
Eyes,
Flowcharts,
Frequency,
Computer science,
Real time systems,
Biometrics"
Architecture modeling language based on UML2.0,"Existing ADLs (architecture description languages) have an advantage of formally specifying the architecture of component-based systems. But ADLs have not come into extensive use in industries since ADL users should learn a distinct notation specific to architecture, and ADLs do not address all stakes of development process that is becoming diversified everyday. On the other hand, UML is a de facto standard general modeling language for software developments as UML provides a consistent notation and various supporting tools during the whole software development cycle. A number of researches on architecture modeling based on UML have been progressed. In particular, many research results have been introduced that specialize UML by its extension mechanism in order to explicitly represent core architecture concepts that UML does not fully support. UML2.0 embraces much more concepts that are important to architecture modeling than UML1.x. In this paper, we examine architecture modeling elements that can be represented in UML2.0 and discuss how to extend and specialize UML2.0 in order to make it more suitable for representing architectures.","Unified modeling language,
Computer architecture,
Object oriented modeling,
Standards development,
Programming,
Computer science,
Architecture description languages,
Software standards,
Software architecture,
Vocabulary"
A further exploration of teaching ethics in the software engineering curriculum,"The importance of teaching topics related to ethics within software engineering programs is highlighted especially in the light of the guiding principles for the Software Engineering volume of the Computing Curricula 2001 and the requirements of professional bodies when accrediting programs. A new work that extends the original investigation is outlined. The work centers on the why, what and how questions concerned with current ethics education in software engineering programs. Details of innovative strategies that are being employed to support the teaching of ethics in professional programs are also provided.",
A discrete PSO method for generalized TSP problem,"A novel discrete particle swarm optimization (PSO) method is presented to solve the generalized traveling salesman problem (GTSP). The ""generalized vertex"" is employed to represent the problem, by which the GTSP and TSP can be handled in a uniform style. An uncertain searching strategy and local searching techniques are also employed to accelerate the convergent speed. Numerical results show the effectiveness of the proposed method.","Particle swarm optimization,
Traveling salesman problems,
Educational institutions,
Dynamic programming,
Biological cells,
Computer science,
Information technology,
Mathematics,
Acceleration,
Application software"
Web visualization of geo-spatial data using SVG and VRML/X3D,"Data visualization is an important technique that helps in understanding and analysis of complex data. Most of current-web-based geo-spatial data visualization products use two-dimensional visualization. A tool that uses the combination of two-and three-dimensional visualization can provide extra spatial dimension as well as flexible usability. Current Web technologies: scalable vector graphics (SVG), virtual reality modeling language (VRML), and extensible 3D (X3D) provide a necessary foundation for such a tool. In this paper, we present a general framework for effective integration and Web-based presentation of complex heterogeneous spatial-temporal data sets. This framework facilitates interpretation, exploration and analysis of large volume of data with significant geo-spatial and temporal characteristics. Advantages of this approach include improved visualization of geo-spatial and temporal raw data; better navigation and selection of data; and intuitive user interface. A Web-based road traffic visualization system illustrates this approach using the Twin Cities road traffic data.","Data visualization,
Computer science,
Data analysis,
Graphics,
Usability,
Navigation,
Roads,
Internet,
Virtual reality,
User interfaces"
Co-evolution of complementary formal and informal requirements,"Agent-oriented Conceptual Modelling (AoCM, as exemplified by the i* notation by E. Yu (1995)), represents an interesting approach to modelling early phase requirements that is particularly effective in capturing organizational contexts, stake-holder intentions and rationale. There are significant benefits in using formal methods for the development of computer systems and improving their quality. We propose a methodology which permits the use of these two otherwise disparate approaches in a complementary and synergistic fashion for requirements engineering.","Strontium,
Context modeling,
Laboratories,
Proposals,
Computer architecture,
Computer science,
Software quality,
Emergency services,
Disaster management,
Project management"
Dense motion estimation using regularization constraints on local parametric models,"This paper presents a method for dense optical flow estimation in which the motion field within patches that result from an initial intensity segmentation is parametrized with models of different order. We propose a novel formulation which introduces regularization constraints between the model parameters of neighboring patches. In this way, we provide the additional constraints for very small patches and for patches whose intensity variation cannot sufficiently constrain the estimation of their motion parameters. In order to preserve motion discontinuities, we use robust functions as a regularization mean. We adopt a three-frame approach and control the balance between the backward and forward constraints by a real-valued direction field on which regularization constraints are applied. An iterative deterministic relaxation method is employed in order to solve the corresponding optimization problem. Experimental results show that the proposed method deals successfully with motions large in magnitude, motion discontinuities, and produces accurate piecewise-smooth motion fields.","Motion estimation,
Parametric statistics,
Image segmentation,
Image motion analysis,
Robustness,
Intelligent systems,
Intelligent sensors,
Machine intelligence,
Information systems,
Computer science"
View-dependent streaming of progressive meshes,"Multiresolution geometry streaming has been well studied in recent years. The client can progressively visualize a triangle mesh from the coarsest resolution to the finest one while a server successively transmits detail information. However, the streaming order of the detail data usually depends only on the geometric importance, since basically a mesh simplification process is performed backwards in the streaming. Consequently, the resolution of the model changes globally during streaming even if the client does not want to download detail information for the invisible parts from a given view point. In this paper, we introduce a novel framework for view-dependent streaming of multiresolution meshes. The transmission order of the detail data can be adjusted dynamically according to the visual importance with respect to the client's current view point. By adapting the truly selective refinement scheme for progressive meshes, our framework provides efficient view-dependent streaming that minimizes memory cost and network communication overhead. Furthermore, we reduce the per-client session data on the server side by using a special data structure for encoding which vertices have already been transmitted to each client. Experimental results indicate that our framework is efficient enough for a broadcast scenario where one server streams geometry data to multiple clients with different view points.",
High capacity associative memories and small world networks,"Models of associative memory usually have full connectivity or if diluted, random symmetric connectivity. In contrast biological neural systems have predominantly local, non-symmetric connectivity. Here we investigate sparse networks of threshold units, trained with the perceptron learning rule. The units are arranged in a small world network, with short path-lengths but cliquish connectivity. The connectivity may be symmetric or non-symmetric. The results show that the small-world networks with non-symmetric weights perform well as associative memories. It is also shown that in highly dilute networks with random connectivity, it is symmetry of the weights, rather than symmetry of the connectivity matrix, that causes poor performance.",
A near lower-bound complexity algorithm for compile-time task-scheduling in heterogeneous computing systems,"Task scheduling is in general an NP-complete problem. For this reason a huge number of heuristics have been presented in the literature to obtain near optimal schedules. These heuristics mainly target homogeneous computing systems, while a few of them target heterogeneous systems. The heterogeneous heuristics presented so far target computing machines with different capabilities, while almost none of them handle heterogeneous communication systems. This paper presents a novel task scheduling algorithm called the heterogeneous critical tasks reverse duplicator (HCTRD), which targets both heterogeneous computation and communication systems. The algorithm is based on list-scheduling and task-duplication in a bounded number of machines, and aims to achieve high performance and near lower bound complexity.","Optimal scheduling,
Processor scheduling,
Scheduling algorithm,
Computer science,
NP-complete problem,
Computational efficiency,
Cost function,
Finishing,
Random number generation"
Variable radii connected sensor cover in sensor networks,"One of the useful approaches to exploit redundancy in a sensor network is to actively keep only a small subset of sensors that are sufficient to cover the region required to be monitored. The set of active sensors should also form a connected communication graph, so that they can autonomously respond to application queries and/or tasks. Such a set of active sensors is known as a connected sensor cover, and the problem of selecting a minimum connected sensor cover has been well studied when the transmission radius and sensing radius of each sensor is fixed. In this article, we address the problem of selecting a minimum energy-cost connected sensor cover, when each sensor node can vary its sensing and transmission radius; larger sensing or transmission radius entails higher energy cost. For the above problem, we design various centralized and distributed algorithms, and compare their performance through extensive experiments. One of the designed centralized algorithms (called CGA) is shown to perform within an O(log n) factor of the optimal solution, where n is the size of the network. We have also designed a localized algorithm based on Voronoi diagrams which is empirically shown to perform very close to CGA, and due to its communication-efficiency, results in significantly prolonging the network lifetime.","Intelligent networks,
Costs,
Algorithm design and analysis,
Signal processing algorithms,
Wireless sensor networks,
Logistics,
Computer science,
Computerized monitoring,
Distributed algorithms,
Signal processing"
UTACO: a unified timing and congestion optimization algorithm for standard cell global routing,"Timing performance and routability are two main goals of global routing. These two targets are mutually conflicting if we view and handle their effects independently. In this paper, we adopt a shadow price mechanism to incorporate the two issues into one unified objective function. We formulate global routing as a multicommodity flow problem. The objective function is the slack of congestion with the clock period as the delay limit from registers and inputs to registers and outputs. The multicommodity flow is expressed by a linear-programming formulation as a primal problem. We then convert the primal problem into a dual formulation using the shadow price as the variables. The shadow price of a net is the sum of its congestion price and timing price. The primal and dual formulation offers theoretical upper and lower bounds of the routing solution. Throughout the optimization process, the difference of the two bounds reduces, which provides the user's insight into the quality of the solutions. Based on the new formulation, this paper presents the UTACO algorithm for standard cell global routing.","Timing,
Routing,
Costs,
Delay,
Clocks,
Computer science,
Very large scale integration,
Wire,
Optimization,
Joining processes"
An ant system based exploration-exploitation for reinforcement learning,"In this paper, we develop a novel exploration-exploitation strategy for reinforcement learning based on ant colony system. Most of the exploration-exploitation strategies use some statistics extracted from a single simulated trajectory. The novel strategy uses some statistics extracted from multiple simulated trajectories obtained from a swarm of ants. We show that the strategy preserves the convergence property of Q-learning.","Learning,
Ant colony optimization,
Statistics,
Biological system modeling,
Biological information theory,
Computer science,
Computational modeling,
Casting,
Birds,
Marine animals"
Stabilizing route panoramas,"The route panorama (RP) has been proposed as a new digital medium to record and visualize cityscapes along a route. It is a compact, continuous and complete visual representation of scenes collected from a sequence of slit views with a camera moving along a smooth path. In real scene acquisition, a camera may suffer from vehicle shaking and the obtained route panoramas are jagged and waved. To improve the quality of route panoramas, we develop an algorithm to stabilize them. By referring the continuous linear features, we use median filters to smooth the 2D route panorama. By setting the slit properly with respect to the camera, we can reduce the influence from the vehicle shaking in the RP without matching consecutive video frames. We have rectified long distance route panoramas to a moderate level for virtual tours and visual navigation.","Cameras,
Layout,
Vehicles,
Nonlinear filters,
Navigation,
Image segmentation,
Video sequences,
Image matching,
Smoothing methods,
Computer science"
Chinese handwriting-based writer identification by texture analysis,"Handwriting-based writer identification is a hot research field in pattern recognition. Given that the handwritings of different people are often visually distinctive, we regard the global text and single character of Chinese as textures and thus apply techniques of texture analysis to them. We present two methods of texture analysis on Chinese off-line handwriting writer identification, one is text-independent and the other is text-dependent. Further, the combination of these two methods is also applied on our application. Through experiments, ideal results are achieved.",
An improved collaborative filtering method for recommendations' generation,"Among the recommender system technologies, collaborative filtering system, which employs statistical techniques to find a set of customers who have a history of agreeing with the target user, has achieved widespread success on the e-commerce site. Although collaborative filtering system overcomes almost all the shortcomings of content-based systems, it is still reported having some limitations just like sparsity and scalability. In this paper, clustering using representatives algorithm is used to generate a new cluster-product matrix from original matrix. Based on the new matrix, traditional way is adopted to find the nearest neighbors. And at last a formula is given to generate the top-N recommendations. The experiment results suggest that the improved collaborative filtering method can increase the accuracy of the recommendations and the efficiency of the system.","Collaboration,
Filtering,
Computer science,
Recommender systems,
Scalability,
Collaborative work,
Cities and towns,
Educational institutions,
Marketing and sales,
Application software"
SOSS: smart object-based storage system,"Hints for storage system come from three aspects: the first is in combination with accurate file or directory attribute layout. The second is from existing file system and depends on user input. The third is from file content analysis. However in the SOSS (smart object-based storage system), object, a new fundamental storage component different from traditional storage unit (file or block), is introduced, and object provides ample hints for storage system, which can help with designing more intelligent (or smarter) storage system. This paper gives a brief introduction about SOSS, and some methods adopted in SOSS to achieve intelligence, such as pattern recognition method, predicting object properties method and an adaptive cache replacement policy (all are based on object attribute), are studied.","Pattern recognition,
Intelligent networks,
Fabrics,
Storage automation,
File systems,
Laboratories,
Data storage systems,
Computer science education,
Educational technology,
Cache storage"
On improving service differentiation under bursty data traffic in wireless networks,"The fair queuing model has been widely used to provide QoS for flows sharing a wireless channel. In fluid fair queuing, a flow cannot reclaim its service loss due to absence. As a result, fair queuing models that emulate fluid fair queuing cannot provide good service differentiation under bursty data traffic. On the other hand, strict priority queuing (SPQ) can provide good service differentiation at the cost of QoS provision. To achieve both service differentiation and QoS provision, we propose a new service model called absence compensation fair queuing. The basic idea is to allow a flow to get compensation of its service loss due to absence. Since the proposed service model is based on fair queuing, QoS provision is guaranteed. We first verify these properties by analysis, and then evaluate the performance compared to weighted fair queuing (WFQ) and SPQ. Simulation results show that our service model can provide much better service differentiation than WFQ, and outperforms SPQ in terms of QoS provision.",
Implement role based access control with attribute certificates,"Nowadays more and more activities are performed over the Internet. But as more people are involved in the transaction circle, security and authorization control becomes one of the biggest concerns. Hence, We are motivated by the need 10 manage and to enforce a strong authorization mechanism in large-scale web-environment. Role based access control (RBAC) provides some flexibility to security management. Public key infrastructure (PKI) can provide a strong authentication. Privilege management infrastructure (PMI) as a new technology can provide strong authorization. In order to satisfy mentioned security requirements, we have established a role based access control infrastructure and developed a prototype that uses X.509 public key certificates (PKCs) and attribute certificates (ACs). Access control is performed by access control policies that are written in XML. Policies and roles are stored in ACs. PKCs and AO are all stored in LDAP servers. A new solution for policy management is described. The main components of the prototype are administration tool and access control engine. The access control engine provides a service that mediates the data between the users and the resources, which is also responsible for authentication and authorization. The administration tool can create key pairs, PKCs and ACs, manage users' information, and so on.","Access control,
Authorization,
Security,
Public key,
Authentication,
Permission,
Internet,
Technology management,
Prototypes,
Computer science"
A key assignment scheme for controlling access in partially ordered user hierarchies,"A key assignment scheme whose security is based on solving discrete logarithms is proposed to work out a solution on the access control problem in an arbitrary partially ordered user hierarchy. Each user is assigned a secret key used to efficiently derive his successors' secret key and assigned an encryption key at the same time used to encrypt his information items or files only. Thus, any user can freely change his own encryption key for some security reasons without caring about those security classes with lower clearances to make their information items been reenciphered. And moreover, make a security class be added into or deleted from the hierarchy without changing any issued keys.","Information security,
Access control,
Computer security,
Cryptography,
Data security,
Computer science,
Information management,
National security,
Application software,
Military computing"
An Access Control Model for Web Services in Business Process,"Business process describes a set of services that span enterprise boundaries and are provided by enterprises that see each other as partners. Web services is widely accepted and adopted to construct business process. Web services are built in exposed environment and open to security threats. When a web service contained in a business process is authorized to illegal users, it will cause economic loss of the service provider. Although there exist some standards for security of Web services and access control for services in distributed systems are well studied, there is a lack of comprehensive approach in access control for web services, especially in business process. In this paper, an extended RBAC model, called WS-RBAC, is proposed to secure web services in business process. The model takes web services in business process as protected objects and extends the classical RBAC model. Next, The software architecture of WS-RABC is presented. This paper also presents how to specify business process in the model and the authorization constraints of WS-RBAC based on WS-Policy.",
How to sanitize data?,"Balancing the needs of a data analyst with the privacy needs of a data provider is a key issue when data is sanitized. We treat both the requirements of the analyst and the privacy expectations as policies, and compose the two policies to detect conflicts. The result can be applied to an intermediate data representation to sanitize the relevant pans of the data. We conclude that this method has promise, but more work is needed to determine its effectiveness and limits.","Data privacy,
Telecommunication traffic,
Computer security,
Data analysis,
Computer science,
Conferences,
Collaborative work,
International collaboration,
Diseases,
Contracts"
Robust nose detection in 3D facial data using local characteristics,"The problem of detecting the feature points arises in many fields of science and engineering. In this paper, we focus on the 3D face range data and propose a robust scheme to solve a specific problem, i.e. locating the nose lip and nose ridge using the local statistic features and included angle curve. This work is very significant to 3D face modelling, recognition and registration. The key features of our method are the fully automated processing, the ability to deal with noisy and incomplete input data, the immunity to the rotation and translation and the adaptability to the different resolution. The experimental results in different databases fully show the robust and feasibility of the proposed method.","Nose,
Face detection,
Statistics,
Computer vision,
Spatial databases,
Mouth,
Noise robustness,
Shape,
Pattern recognition,
Automation"
A controlled experiment for evaluating a metric-based reading technique for requirements inspection,"Natural language requirements documents are often verified by means of some reading technique. Some recommendations for defining a good reading technique point out that a concrete technique must not only be suitable for specific classes of defects, but also for a concrete notation in which requirements are written. Following this suggestion, we have proposed a metric-based reading (MBR) technique used for requirements inspections, whose main goal is to identify specific types of defects in use cases. The systematic approach of MBR is basically based on a set of rules as ""if the metric value is too low (or high) the presence of defects of type de fType/sub 1/,...de fType/sub n/ must be checked"". We hypothesised that if the reviewers know these rules, the inspection process is more effective and efficient, which means that the defects detection rate is higher and the number of defects identified per unit of time increases. But this hypotheses lacks validity if it is not empirically validated. For that reason the main goal is to describe a controlled experiment we carried out to ascertain if the usage of MBR really helps in the detection of defects in comparison with a simple checklist technique. The experiment result revealed that MBR reviewers were more effective at detecting defects than checklist reviewers, but they were not more efficient, because MBR reviewers took longer than checklist reviewers on average.","Inspection,
Concrete,
Computer languages,
Computer science,
Natural languages,
Programming,
Proposals,
Bioreactors,
System testing,
Error analysis"
Implementation of a PC-based Robot Controller with Open Architecture,"The demand for increased capability and flexibility has led to a dramatic increase in the use of controllers based on open architecture. Knowledge from past research on open architecture system indicates that a component-based philosophy should be the solution. In this paper, a PC-based open architecture robot control system is presented and the technologies related to component-ware are investigated. By assembling controller from off-the-shell hardware and software components, the benefits of reduced cost and improved robustness have been realized. CORBA is exploited in software development to ensure the extensibility, scalability and portability of the software. Finally, by applying to the Movemaster-EX robot, the performance of the system is evaluated","Robot control,
Computer architecture,
Hardware,
Control systems,
Automatic control,
Programming,
Scalability,
Communication system control,
Application software,
Computer industry"
"Testing, optimization, and games","We discuss algorithmic problems arising in the testing of reactive systems, i.e. systems that interact with their environment. The goal is to design test sequences so that we can deduce desired information about the given system under test, such as whether it conforms to a given specification model, or whether it satisfies given requirement properties. Test generation can be approached from different points of view - as an optimization problem of minimizing cost and maximizing the effectiveness of the tests; as a game between tester and system under test; or as a learning problem. We touch on some of these aspects and related algorithmic questions.","System testing,
Automata,
Logic testing,
Computer science,
Cost function,
Automatic testing,
Software testing,
Software systems,
Hardware"
A distributed implementation of sequential consistency with multi-object operations,"Sequential consistency is a consistency criterion for concurrent objects stating that the execution of a multiprocess program is correct if it could have been produced by executing the program on a mono-processor system, preserving the order of the operations of each individual process. Several protocols implementing sequential consistency on top of asynchronous distributed systems have been proposed. They assume that the processes access the shared objects through basic read and write operations. We consider the case where the processes can invoke multiobject operations which can read or write several objects in a single operation atomically. It proposes a particularly simple protocol that guarantees sequentially consistent executions in such a context. The previous sequential consistency protocols, in addition to considering only unary operations, assume either full replication or a central manager storing copies of all the objects. In contrast, the proposed protocol has the noteworthy feature that each object has a separate manager. Interestingly, this provides the protocol with a versatility dimension that allows deriving simple protocols providing sequential consistency or atomic consistency when each operation is on a single object.",
A novel dynamic load balancing library for cluster computing,"In the last few years, research advances in dynamic scheduling at application and runtime system levels have contributed to improving the performance of scientific applications in heterogeneous environments. This paper presents the design and implementation of a library as a result of an integrated approach to dynamic load balancing. This approach combines the advantages of optimizing data migration via novel dynamic loop scheduling strategies with the advances in object migration mechanisms of parallel runtime systems. The performance improvements obtained by the use of this library have been investigated by its use in two scientific applications: the N-body simulations, and the profiling of automatic quadrature routines. The experimental results obtained underscore the significance of using such an integrated approach, as well as the benefits of using the library especially in cluster applications characterized by irregular and unpredictable behavior.","Load management,
Libraries,
Dynamic scheduling,
Application software,
Computational modeling,
Computer science,
Runtime environment,
Workstations,
Delay,
Processor scheduling"
Using passive traces of application traffic in a network monitoring system,"Adaptive grid applications require up-to-date network resource measurements and predictions to help steer their adaptation to meet performance goals. To this end, we are interested in monitoring the available bandwidth of the underlying networks in the most accurate and least obtrusive way. Bandwidth is either measured by actively injecting data probes into the network or by passively monitoring existing traffic, but there is a definite trade-off between the active approach, which is invasive, and the passive approach, which is rendered ineffective during periods of network idleness. We are developing the Wren bandwidth monitoring tool, which uses packet traces of existing application traffic to measure available bandwidth. We demonstrate that the principles supporting active bandwidth tools can be applied to passive traces of the LAN and WAN traffic generated by high-performance grid applications. We use our results to form a preliminary characterization of the application traffic required by available bandwidth techniques to produce effective measurements. Our results indicate that a low overhead, passive monitoring system supplemented with active measurements can be built to obtain a complete picture of the network's performance.",
Clustering search engine query log containing noisy clickthroughs,"Query clustering is a technique for discovering similar queries on a search engine. In this paper, we present a query clustering method based on the agglomerative clustering algorithm. We first present an overview of the agglomerative clustering algorithm proposed by Beeferman and Berger (2000). We point out a weakness of the method caused by noisy user clicks and propose an improved clustering algorithm. Our results show that in general the agglomerative clustering algorithm can cluster similar queries effectively and that our improved algorithm can successful eliminate noisy clicks and produce cleaner query clusters.","Search engines,
Clustering algorithms,
Clustering methods,
Internet,
Iterative algorithms,
Computer science,
Web pages,
Bipartite graph"
New fast and accurate heuristics for inference of large phylogenetic trees,"Summary form only given. Inference of phylogenetic trees comprising thousands of taxa using maximum likelihood is computationally extremely expensive. We present simple heuristics which yield accurate trees for simulated as well as real data and reduce execution time. The new heuristics have been implemented in a program called RAxML which is freely available. Furthermore, we present a distributed version of our algorithm which is implemented in an MPI-based prototype. This prototype is being used to implement an http-based seti@home-like version of RaxML. We compare our program with PHYML and MrBayes which are currently the fastest and most accurate programs for phylogenetic tree inference. Experiments are conducted using 50 simulated 100 taxon alignments as well as real-world alignments with up to 1000 sequences. RAxML outperforms MrBayes for real-world data both in terms of speed and final likelihood values. Furthermore, for real data RAxML outperforms PHYML by factor 2-8 and yields better final trees due to its more exhaustive search strategy. For synthetic data MrBayes is slightly more accurate than RAxML and PHYML but significantly slower.",
Performability modeling of mobile software systems,"An increasing number of applications operate in heterogeneous computing environments, often with mobile components. Methodologies that help developers assess the ability of such applications to meet their performance requirements throughout the software life-cycle are needed. In particular, early in the design phases, analysis techniques are critical for ensuring the future system's behavior, evaluating and comparing design alternatives. A performability evaluation is the most appropriate means to assess the expected system's ability to perform, including the effects of component failures and repairs. This paper focuses on model-based analysis of performability of mobile software systems. We propose a general methodology that starts from design artifacts expressed in a UML-based notation. Inferred performability models are based on the stochastic activity networks notation. The viability of the proposed approach is demonstrated through its application in a case study.","Software systems,
Application software,
Performance analysis,
Mobile agents,
Software performance,
Performance evaluation,
Degradation,
Computer science,
Mobile computing,
Stochastic processes"
Topology control for MANETs,"We deal with the problem of topology control for mobile ad hoc networks. Based on an asymptotic result on k-connectivity, a simple scheme for topology control is proposed. The scheme is used to generate several topologies under highly mobile conditions. Through simulation and analysis we show that this scheme results in high connectivity even under mobile conditions as the number of nodes increase. We also show that even though the scheme uses varying transmit power, the overall power consumption in the network remains relatively the same, and is less than using constant transmit power as the number of nodes increases. In order to use the scheme, the nodes need to he uniformly distributed. We prove that under periodic boundary conditions the stationary distribution of nodes, moving according to (a variant of) the random waypoint model, is uniform.","Mobile ad hoc networks,
Network topology,
Electronic mail,
Energy consumption,
Wireless networks,
Mathematics,
Statistics,
Computer science,
Analytical models,
Boundary conditions"
"Identifying ""representative"" workloads in designing MpSoC platforms for media processing","Workload design is a well recognized problem in the domain of microprocessor design. Different program characteristics that influence the selection of a representative workload include microarchitecture-centric properties such as cache miss rates, instruction mix and accuracy of branch prediction. However, properties of a workload that are pertinent to the context of system-level design of multiprocessor SoC platforms are very different. Till date, the problem of ""representative workload design"" in this specific context has not been sufficiently addressed. This work represents an attempt to address this problem in the specific case of SoC platform design for multimedia processing. Towards this, we present a method to characterize properties of multimedia workload that are relevant to SoC platform design. Based on such a characterization, we present a technique for classifying different multimedia streams. Finally, we show the utility of such a classification through a case study involving the design of a multiprocessor SoC platform for MPEG-2 decoding.","Streaming media,
Process design,
Microarchitecture,
System-level design,
Intelligent networks,
Design engineering,
Laboratories,
Computer science,
Electronic mail,
System-on-a-chip"
"Role-playing, group work and other ambitious teaching methods in a large requirements engineering course","Requirements engineering is offered to students at the faculty of information technology in the University of Technology, Sydney. It is a core subject in the Graduate Certificate in Information Technology and Graduate Diploma. This paper outlines the subject's objectives, the topics discussed. Details of the lecturing methods used are also presented, namely: role-playing and peer-assessment within a group environment. The paper also describes the main features of the online support tool utilised in addition to summarising the lessons learnt. The main aim is to highlight both successful practices and those that can be improved further.","Education,
Information technology,
Maintenance engineering,
Australia,
Costs,
Software quality,
Programming,
Conferences,
Systems engineering and theory"
Integer reversible transformation to make JPEG lossless,"JPEG, as an international image coding standard based on DCT and Huffman entropy coder, is still popular in image compression applications although it is lossy. JPEG-LS, standardized for lossless image compression, however, employs an encoding technique different from JPEG. This paper presents an integer reversible implementation to make JPEG lossless. It uses the framework of JPEG, and just converts DCT and color transform to be integer reversible. Integer DCT is implemented by factoring the float DCT matrix into a series of elementary reversible matrices and each of them is directly integer reversible. Our integer DCT integrates lossy and lossless schemes nicely, and it supports both lossy and lossless compression by the same method. Our JPEG can be used as a replacement for the standard JPEG in either encoding or decoding or both. Experiments show that the performance of JPEG with our integer reversible DCT is very close to that of the original standard JPEG for lossy image coding, and more importantly, with our transform, it can compress images losslessly.","Image coding,
Discrete cosine transforms,
Transform coding,
Color,
Computer science,
Code standards,
Entropy,
Application software,
Image converters,
Matrix converters"
On stabilization of linear systems with stochastic disturbances and input saturation,"It is well-known that for linear systems internal asymptotic stability implies external stability in the sense that when the external input is in L/sub p/ then also the state will be in L/sub p/. However, for the control of linear systems with saturation where the controlled system is nonlinear this implication is no longer directly applicable. Several people have studied the effect of external inputs in L/sub p/ either directly or in the context of ISS as introduced by Sontag. In this paper we study the effect of external stochastic disturbances on linear systems with input saturation and we establish that when we can achieve internal global asymptotic stability then we can also achieve a bounded variance for the state.","Linear systems,
Stochastic systems,
Asymptotic stability,
Control systems,
Feedback,
Nonlinear control systems,
Open loop systems,
Discrete time systems,
Mathematics,
Computer science"
Enhancing intelligent transportation systems to improve and support homeland security,"The surface transportation system plays a crucial role in responding to natural disasters and other catastrophic incidents. We propose new techniques to enhance ITS (intelligent transportation systems) to improve and support homeland security. In particular, we propose two evacuation algorithms, all-links and fastest-links, and perform simulation studies to compare their performances. These algorithms are part of a smart traffic evacuation management system (STEMS) being developed to provide rapid and efficient response to human-caused threats and disasters, by creating dynamic evacuation plans based on incident location and scope.","Intelligent transportation systems,
Terrorism,
Disaster management,
Traffic control,
Safety,
Crisis management,
Computer science,
Helium,
Technology management,
US Department of Transportation"
A first step towards formal verification of security policy properties for RBAC,"Considering the current expansion of IT-infrastructure, the security of the data inside this infrastructure becomes increasingly important. Therefore, assuring certain security properties of IT-systems by formal methods is desirable. So far in security, formal methods have mostly been used to prove properties of security protocols. However, access control is an indispensable part of security inside a given IT-system, which has not yet been sufficiently examined using formal methods. The paper presents an example of a RBAC security policy having the dual control property. This is proved in a first-order linear temporal logic (LTL) that has been embedded in the theorem prover Isabelle/HOL by the authors. Thus, the correctness of the proof is assured by Isabelle/HOL. The authors consider first-order LTL a good formalism for expressing RBAC authorisation constraints and deriving properties from given RBAC security policies. Furthermore, it might also be applied to safety-related issues in similar manner.","Formal verification,
Data security,
Access control,
Access protocols,
Logic,
Hospitals,
Mathematics,
Computer science,
Computer security,
Authorization"
A fuzzy multidimensional model for supporting imprecision in OLAP,The use of OLAP technology in new knowledge fields and the merge of data from different sources have made appeared new requirements for models to support this technology. What we propose in this paper is a new multidimensional model that can manage imprecision both in the dimensions and the facts. This enables the multidimensional structure to model the imprecision of the data as a result of the integration of data from different sources or even information from experts. This is done by means of fuzzy logic.,"Multidimensional systems,
Fuzzy logic,
Computer science,
Artificial intelligence,
Proposals,
Database languages,
Knowledge management,
Technology management,
Decision support systems,
XML"
Identity based proxy-signcryption scheme from pairings,An identity-based cryptosystem is a novel type of public cryptographic scheme in which the public keys of the users are their identities or strings derived from their identities. A signcryption is a primitive that provides private and authenticated delivery of messages between two parties. Proxy signature schemes are variations of ordinary digital signature schemes and have been shown to be useful in many applications. We proposed an identity-based proxy-signcryption scheme from pairings. Also we analyze the proposed scheme from efficiency and security points of view. Heuristic arguments have been given for those security properties. We have shown that the proxy-signcryption scheme is as efficient as ordinary identity-based signcryption schemes under certain circumstances.,
DeW: a dependable Web services framework,"Web services (WSs) correspond to conceptual entities with well defined interfaces published by different organizations. For example, with businesses, a WS might correspond to a business process to be invoked by other WSs and Internet applications. To increase availability of a WS, an organization might replicate it across different nodes. This study focuses on data intensive applications that: (a) expose a conceptual entity as a Web service (WS); and (b) disperse copies of their WSs across the nodes of a distributed environment to enhance both performance and availability. We describe the design and implementation of a dependable Web services (DeW) framework to realize physical-location-independence. Physical-location-independence means a plan will execute as long as a copy of its referenced WSs is available. This concept enables the client proxy objects to continue operation in the presence of both failures and WS migrations that balance system load.","Web services,
Business communication,
Availability,
Animation,
Context modeling,
Computer science,
Internet,
IP networks,
Computational modeling,
Concurrent computing"
Towards peer-to-peer double auctioning,"P2P systems constitute nowadays an increasingly important part of the online world that needs or will soon need all kinds of e-commerce services and applications that are normally available today on client-server platforms. When designing new e-commerce solutions in P2P, particular attention must be paid to masking autonomy of the peers and the lack of any central authorities as two most important problems. In this paper, we explore possibilities to bring e-commerce into P2P and propose a double auctioning mechanism that does not rely on the existence of central authorities, auctioneer in particular, and is amenable to implementation in P2P environments. The mechanism has good economic properties such as, for example, fast convergence towards efficient trading through intuitive and simple bidding strategies.","Peer to peer computing,
Environmental economics,
Mechanical factors,
Distributed computing,
Computer architecture,
Internet,
Distributed information systems,
Laboratories,
Technology management,
Electronic commerce"
Nash equilibria in parallel downloading with multiple clients,"Recently, the scheme of parallel downloading has been proposed as a novel approach to expedite the reception of a large file from the Internet. Experiments with a single client have shown that the client can improve its performance significantly by using the scheme. Simulations and experiments with multiple clients using the scheme have been conducted in [Gkantsidis, C et al., (2003), Koo, S et al., (2003)] to investigate the impact that this technique might have on the network if it is widely adopted. Contrast to the methodology used in [Gkantsidis, C et al., (2003), Koo, S et al., (2003)], we formulate parallel downloading as a noncooperative game. Within this framework, we present a characterization of the traffic configuration at Nash equilibrium in a general network, and analyze its properties in a specific network. We also establish the dynamic convergence to equilibrium from an initial nonequilibrium state for a specific network. Finally, we investigate the efficiency of Nash equilibrium from the point of view of the clients and the system respectively, i.e., downloading latencies perceived by individual clients and total latencies over all connections. We find that although the traffic configuration at Nash equilibrium is optimal from the point of view of the clients, it may be bad from the point of view of the system.",
Analysis and visualization of index words from audio transcripts of instructional videos,"We introduce new techniques for extracting, analyzing, and visualizing textual contents from instructional videos of low production quality. Using automatic speech recognition, approximate transcripts (/spl ap/75% word error rate) are obtained from the originally highly compressed videos of university courses, each comprising between 10 to 30 lectures. Text material in the form of books or papers that accompany the course are then used to filter meaningful phrases from the seemingly incoherent transcripts. The resulting index into the transcripts is tied together and visualized in 3 experimental graphs that help in understanding the overall course structure and provide a tool for localizing certain topics for indexing. We specifically discuss a transcript index map, which graphically lays out key phrases for a course, a textbook chapter to transcript match, and finally a lecture transcript similarity graph, which clusters semantically similar lectures. We test our methods and tools on 7 full courses with 230 hours of video and 273 transcripts. We are able to extract up to 98 unique key terms for a given transcript and up to 347 unique key terms for an entire course. The accuracy of the Textbook Chapter to Transcript Match exceeds 70% on average. The methods used can be applied to genres of video in which there are recurrent thematic words (news, sports, meetings, etc.).",
On interactions between routing and detailed placement,"The main goal of This work is to develop deeper insights into viable placement-level optimization of routing. Two primary contributions are made. First, an experimental framework in which the viability of predictive models of routing congestion for optimization during detailed placement can be evaluated, is developed. The main criteria of consideration in these experiments is how (un)reliably various models from the literature detect routing hot-spots. We conclude that such models appear to be too unreliable for detailed placement optimization. Second, motivated by the first result, we present a unified combinatorial framework in which cell placement and exact routing structures are captured and optimized; the framework relies on the trunk-decomposition of global routing structures and optimization is performed by a generalized optimal interleaving algorithm (Hur and Lillis, 2000). A proof of concept implementation of this framework is studied in the FPGA domain. The technique can reduce the number of channels at maximum density by almost 45% on average with maximum reduction of 68% for optimized global routing.","Routing,
Predictive models,
White spaces,
Computer science,
Field programmable gate arrays,
Design optimization,
Simulated annealing,
Interleaved codes,
Digital systems,
Logic design"
Engaging undergraduate students with robotic design projects,This paper describes our experiences developing robotics design projects for undergraduate students in our electrical and computer engineering curriculum at Georgia Tech. Several low-cost alternatives for developing robot-based design projects and designing the associated electronics and sensors to control them are included.,"Educational robots,
Robot sensing systems,
Circuits,
Motor drives,
DC motors,
Mobile robots,
Plastics,
Electrical engineering computing,
Hardware,
Radio frequency"
From peer assessment towards collaborative learning,"Peer assessment is one form of innovative assessment which aims to integrate learning and assessment. Peer assessment can also be seen as a special type of collaborative learning, where the task can be specified by using so called collaboration scripts. This implies the application of a more or less rigid schema (collaboration script) for structuring the task. Although peer assessment has positive effects on knowledge acquirement there is a lack of web-based collaborative peer assessment systems. Thus we have implemented a peer assessment application for review and discussion of artifacts. In our approach it is new that students use presentation recording tools to create personal presentations. It is possible to use these continuous documents for peer assessment. With this approach we lift restrictions to submit only static documents for peer review.",
Modified particle swarm optimization based on space transformation for solving traveling salesman problem,"A modified particle swarm optimization was proposed to solve traveling salesman problem (TSP). The algorithm searched in the Cartesian continuous space, and constructed a mapping from continuous space to discrete permutation space of TSP, thus to implement the space transformation. Moreover, local search technique was introduced to enhance the ability to search, and chaotic operations were employed to prevent the particles from falling into local optima prematurely. Finally four benchmark problems in TSPLIB were tested to evaluate the performance of the algorithm. Experimental results indicate that the algorithm can find high quality solutions in a comparatively short time.","Particle swarm optimization,
Space exploration,
Traveling salesman problems,
Space technology,
Cities and towns,
Chaos,
Educational institutions,
Computer science,
Benchmark testing,
Random number generation"
Information leak in the Chord lookup protocol,"In peer-to-peer (P2P) systems, it is often essential that connected systems (nodes) relay messages which did not originate locally, on to the greater network. As a result, an intermediate node might be able to determine a large amount of information about the system, such as the querying tendencies of other nodes. This represents an inherent security issue in P2P networks. Therefore, we ask the following question: through the observation of the network traffic in a P2P network, what kind of information can an adversarial node learn about another node in the same network? We study this question in the case of a specific P2P system - Chord. We also study the effects of the parameters of Chord (such as finger-table size) and the various enhancements to Chord (such as location caching and data caching) on the amount of information leaked.",
Power-aware routing for energy conserving and balance in ad hoc networks,"In this paper, we proposed an energy conserving routing protocol in mobile ad hoc network. The goal of our protocol is to reduce power consumption in transmission and hence to increase the lifetime of the whole network. To achieve energy conservation, the transmission power is controlled to the minimum level that packets can be correctly received. To find a proper route, we take into account both the transmission power and the remaining energy of the mobile hosts along the path. We also proposed a route caching strategy to increase the cache efficiency. Simulation results show that our protocol can conserve 10% to 20% more energy than dynamic source routing does. Also, our protocol also have not only longer network lifetime but also lower standard deviation on remaining energy among hosts.",
Computational physics for undergraduates: The CPUG degree program at Oregon State University,"We presently are experiencing historically rapid advances in science, technology, and education driven by a dramatic increase in computer use and power. In the past, educators were content to have undergraduates view scientific computation as black boxes (an abstraction of a device in which only its externally visible behavior is considered, not its implementation) and have them wait for graduate school to learn what's inside. Our increasing reliance on computers makes this less true today, and much less likely to be true in the future. To adjust to the growing importance of computing in all of science, Oregon State University's Physics Department now offers a four-year, research-rich curriculum leading to a bachelor's degree in computational physics. The five computational courses developed for this program act as a bridge connecting physics with the computation, mathematics, and computational science communities.",
A mix route algorithm for mix-net in wireless mobile ad hoc networks,"Providing anonymous connection service in mobile ad hoc networks is a challenging task. In addition to security concerns, performance concerns must be addressed properly as well. Chaum's mix method (Comms. of the ACM, vol.24(2), p.84-88, 1981) can effectively thwart an adversary's attempt at tracing packet routes and can hide the source and/or destination of packets. However, applying the mix method in ad hoc networks may cause significant performance degradation due to its non-adaptive mix route selection algorithm. We propose a dynamic mix routing algorithm to find topology-dependent mix routes for anonymous connections. Its effectiveness in improving network performance is validated by simulation results. We also address the potential degradation of anonymity due to dynamic mix routing.",
"Preliminary results of a data acquisition sub-system for distributed, digital, computational, APD-based, dual-modality PET/CT architecture for small animal imaging","A new highly integrated data acquisition (DAQ) system for a combined, APD-based, dual-modality PET/CT scanner, implementing both analog and digital electronics on the same board, has been fabricated and tested. The DAQ system was designed to achieve high-precision (<1 ns) coincidence detection in PET and high-rate event counting in CT imaging using the same detectors and electronics. One DAQ board holds 64 parallel detector channels that can be sampled directly at the output of the charge-sensitive preamplifiers (CSP) at a rate of 100 MHz with free-running analog-to-digital converters (ADC) from Maxim. Digital signal processing is performed in field programmable gate arrays (FPGAs) from Xilinx. Independent 500 V voltage regulators are mounted on board for optimum biasing of individual APDs coupled to phoswich detectors. The DAQ board has been fabricated on a 12 copper layers printed circuit board (PCB). The low-noise analog front-end is directly interfaced on board through differential CSP outputs to the high-speed digital circuits for optimum coupling and noise immunity. The board shows excellent electrical characteristics with all circuitry powered up, featuring a mean signal to noise and distortion ratio (SINAD) of 47.7 dB over all 64 channels when supplied with a 10 MHz sine waveform at its input. Initial performance characteristics with BGO/LSO phoswich detectors are reported.","Data acquisition,
Distributed computing,
Positron emission tomography,
Computed tomography,
Computer architecture,
Animals,
Detectors,
Electronic equipment testing,
Event detection,
Field programmable gate arrays"
Statistical classification of buried objects from spatially sampled time or frequency domain electromagnetic induction data,"Methods for classifying objects based on spatially sampled electromagnetic induction data taken in the time or frequency domain are developed and analyzed. To deal with nuisance parameters associated with the position of the object relative to the sensor as well as the object orientation, a computationally tractable physical model explicit in these unknowns is developed. The model is also parameterized by a collection of decay constants (or equivalently Laplace-plane poles) whose values in theory are independent of object position and orientation. These poles are used as features for classification. The overall algorithm consists of two stages. First, we estimate the values of the unknown parameters and then we do classification. Classification is done by comparing either the raw data or some low-dimensional collection of features extracted from the data to entries in a library. The library can be constructed using either simulated or calibration data. A maximum likelihood method is developed and analyzed for the problem of joint pole, location, and orientation parameter determination. Here we examine and compare two classification schemes. The first classification method is based on data residuals generated from estimated signal parameters. This scheme performs well in low SNR cases. The second is based on estimated pole values themselves, which performs well in high SNR cases. We validate our methods on both simulated and field data taken from frequency and time domain sensors.",
Multi-clock timed networks,"We consider verification of safety properties for parameterized systems of timed processes, so called timed networks. A timed network consists of a finite state process, called a controller, and an arbitrary set of identical timed processes. In a previous work, we showed that checking safety properties is decidable in the case where each timed process is equipped with a single real-valued clock. It was left open whether the result could be extended to multi-clock timed networks. We show that the problem becomes undecidable when each timed process has two clocks. On the other hand, we show that the problem is decidable when clocks range over a discrete time domain. This decidability result holds when processes have any finite number of clocks.","Clocks,
Protocols,
Automatic control,
Automata,
Counting circuits,
Encoding,
Process control,
Safety devices,
Control systems,
Timing"
Unlifted loop subdivision wavelets,"In this paper, we propose a new wavelet scheme for loop subdivision surfaces. The main idea enabling our wavelet construction is to extend the subdivision rules to be invertible, thus executing each inverse subdivision step in the reverse order makes up the wavelet decomposition rule. As opposed to other existing wavelet schemes for loop surfaces, which require solving a global sparse linear system in the wavelet analysis process, our wavelet scheme provides efficient (linear time and fully in-place) computations for both forward and backward wavelet transforms. This characteristic makes our wavelet scheme extremely suitable for applications in which the speed for wavelet decomposition is critical. We also describe our strategies for optimizing free parameters in the extended subdivision steps, which are important to the performance of the final wavelet transform. Our method has been proven to be effective, as demonstrated by a number of examples.",
Using information visualization for accessing learning object repositories,"Learning objects are entities that may be used for learning, education or training. Nowadays they are often stored in learning object repositories (LORs), such as the Ariadne Knowledge Pool System (KPS) as stated in E. Duval et al. (2001), Merlot and EdNa. Typically users can search for learning objects in those LORs by filling out an electronic form that enables them to compose Boolean combinations of search criteria. More research is needed on novel access paradigms to enable more effective and flexible access to these repositories. We investigate how we can use information visualization techniques for this purpose. We discuss the use of three existing information visualization techniques that we applied to the ARIADNE Knowledge Pool System (KPS).","Visualization,
Filling,
Computer science,
Computer science education,
Art,
History,
Information filtering,
Information filters"
Recovery in Web service applications,"Web service technology is changing the Internet to a platform of applications collaboration and integration. Reliability is a critical requirement for this platform. The main goal of failure recovery is to assure that minimum work is lost and normal execution can be continued. We look at the issue of failure recovery in Web services management systems, and propose an infrastructure to implement failure recovery capabilities in the Web services management systems.","Web services,
Context-aware services,
Application software,
Web and internet services,
Computer bugs,
Outsourcing,
Helium,
Computer science,
Collaborative work,
Software standards"
A data warehouse conceptual data model,"In this short paper we will briefly introduce a data warehouse conceptual data model, which gracefully extends the standard entity-relationship (ER) conceptual data model with multi-dimensional aggregated entities. The conceptual data model has a clear model-theoretic semantics grounded on the extension of the standard ER semantics with the QMD logic-based multi-dimensional data model.","Data warehouses,
Data models,
Erbium,
Computer science,
Databases,
Multidimensional systems,
Logic,
Telephony,
Aggregates,
Sun"
Cluster computing in the classroom and integration with computing curricula 2001,"With the progress of research on cluster computing, many universities have begun to offer various courses covering cluster computing. A wide variety of content can be taught in these courses. Because of this variation, a difficulty that arises is the selection of appropriate course material. The selection is complicated because some content in cluster computing may also be covered by other courses in the undergraduate curriculum, and the background of students enrolled in cluster computing courses varies. These aspects of cluster computing make the development of good course material difficult. Combining experiences in teaching cluster computing at universities in the United States and Australia, this paper presents prospective topics in cluster computing and a wide variety of information sources from which instructors can choose. The course material is described in relation to the knowledge units of the Joint IEEE Computer Society and the Association for Computing Machinery (ACM) Computing Curricula 2001 and includes system architecture, parallel programming, algorithms, and applications. Instructors can select units in each of the topical areas and develop their own syllabi to meet course objectives. The authors share their experiences in teaching cluster computing and the topics chosen, depending on course objectives.",
High-performance MAC for high-capacity wireless LANs,"The next-generation wireless technologies, e.g., 802.11n and 802.15.3a, offer a physical-layer speed at least an-order-of-magnitude higher than the current standards. However, direct application of current MACs leads to high protocol overhead and significant throughput degradation. In this paper, we propose ADCA, a high-performance MAC that works with high-capacity physical layer. ADCA exploits two ideas of adaptive batch transmission and opportunistic selection of high-rate hosts to simultaneously reduce the overhead and improve the aggregate throughput. It opportunistically favors high-rate hosts by providing higher access probability and more access time, while ensuring each low-rate host certain minimum amount of channel access time. Simulations show that the ADCA design increases the throughput by 112% and reduces the average delay by 55% compared with the legacy DCF. It delivers more than 100 Mbps MAC-layer throughput as compared with 35 Mbps offered by the legacy MAC","Wireless LAN,
Local area networks,
Throughput,
Media Access Protocol,
Physical layer,
Aggregates,
Bandwidth,
Computer science,
Educational institutions,
Laboratories"
Frown gives game away: affect sensitive systems for elementary mathematics,"An important factor in the success of human one-to-one tutoring is the tutor's ability to identify and respond to affective cues. However, current intelligent tutoring systems model students solely on their cognitive, and not affective, state. Automated facial expression analysis makes it possible for computers to identify affective cues by analysing images of learners' facial expressions. A new generation of intelligent tutoring systems, affective tutoring systems, is being developed at Massey University in New Zealand. An important component of this system is a facial expression analysis component that was implemented using a fuzzy approach. This fuzzy facial expression analysis approach and future directions are discussed in this paper","Mathematics,
Intelligent systems,
Humans,
Image analysis,
Machine intelligence,
State estimation,
Intelligent agent,
Chaos,
Computer science,
Software systems"
Query routing and processing in schema-based P2P systems,"Recently, the peer-to-peer (P2P) paradigm has emerged, mainly by file sharing systems such as Napster and Gnutella and in terms of scalable distributed data structures. Due to the decentralization, P2P systems promise an improved robustness and scalability and therefore open also a new view on data integration solutions. However, several design and technical challenges arise in building scalable P2P-based integration systems. We address one of them: the problem of distributed query processing. We discuss strategies of query decomposition and routing based on different kinds of routing indexes and present results of an experimental evaluation.","Query processing,
Peer to peer computing,
Routing,
XML,
Algebra,
Hoses,
Computer science,
Automation,
Data structures,
Robustness"
A CASE tool platform using an XML representation of Java source code,"Recent IDEs have become more extensible tool platforms but do not concern themselves with how other tools running on them collaborate with each other. They compel developers to use proprietary representations or the classical abstract syntax tree (AST) to build source code tools. Although these representations contain sufficient information, they are neither portable nor extensible. This paper proposes a tool platform that manages commonly used, fined-grained, information about Java source code by using an XML representation. Our representation is suitable for developing tools which browse and manipulate actual source code since the original code is annotated with tags based on its structure and retained within the tags. Additionally, it exposes information resulting from global semantic analysis, which is never provided by the typical AST. Our proposed platform allows the developers to extend the representation for the purpose of sharing or exchanging various kinds of information about the source code, and also enables them to build new tools by using existing XML utilities","Computer aided software engineering,
XML,
Java,
Software tools,
Collaborative tools,
Collaborative software,
Information analysis,
Computer science,
Information systems,
Application software"
Phylogenetic reconstruction from arbitrary gene-order data,"Phylogenetic reconstruction from gene-order data has attracted attention from both biologists and computer scientists over the last few years. So far, our software suite GRAPPA is the most accurate approach, but it requires that all genomes have identical gene content, with each gene appearing exactly once in each genome. Some progress has been made in handling genomes with unequal gene content, both in terms of computing pair-wise genomic distances and in terms of reconstruction. In this paper, we present a new approach for computing the median of three arbitrary genomes and apply it to the reconstruction of phylogenies from arbitrary gene-order data. We implemented these methods within GRAPPA and tested them on simulated datasets under various conditions as well as on a real dataset of chloroplast genomes; we report the results of our simulations and our analysis of the real dataset and compare them to reconstructions made by using neighbor-joining and using the original GRAPPA on the same genomes with equalized gene contents. Our new approach is remarkably accurate both in simulations and on the real dataset, in contrast to the distance-based approaches and to reconstructions using the original GRAPPA applied to equalized gene contents.",
Indexing and retrieval of 3D models by unsupervised clustering with hierarchical SOM,"A hierarchical indexing structure for 3D model retrieval based on the hierarchical self organizing map (HSOM) is proposed. The proposed approach organizes the database into a hierarchy so that head models are partitioned by coarse features initially and finer scale features are used in lower levels. The aim is to traverse a small subset of the database during retrieval. This is made possible by exploiting the multi-resolution capability of spherical wavelet features to successively approximate the salient characteristics of the head models, which are encoded in the form of weight vectors associated with the nodes at different levels (from coarse to fine) of the HSOM. To avoid premature commitment to a possibly erroneous model class, search is propagated from a subset of nodes at each level, which is selected based on a fuzzy membership measure between the query feature vector and weight vector, instead of taking the winner-take-all approach. Experiments show that, in addition to efficiency improvement, model retrieval based on the HSOM approach is able to achieve a much higher accuracy compared with the case where no indexing is performed.","Indexing,
Spatial databases,
Information retrieval,
Shape,
Application software,
Organizing,
Head,
Computer aided manufacturing,
Computer science,
Internet"
Using representative-based clustering for nearest neighbor dataset editing,"The goal of dataset editing in instance-based learning is to remove objects from a training set in order to increase the accuracy of a classifier. For example, Wilson editing removes training examples that are misclassified by a nearest neighbor classifier so as to smooth the shape of the resulting decision boundaries. This paper revolves around the use of representative-based clustering algorithms for nearest neighbor dataset editing. We term this approach supervised clustering editing. The main idea is to replace a dataset by a set of cluster prototypes. A clustering approach called supervised clustering is introduced for this purpose. Our empirical evaluation using eight UCI datasets shows that both Wilson and supervised clustering editing improve accuracy on more than 50% of the datasets tested. However, supervised clustering editing achieves four times higher compression rates than Wilson editing.","Nearest neighbor searches,
Clustering algorithms,
Prototypes,
Computer science,
Shape,
Testing,
H infinity control,
Algorithm design and analysis,
Data mining,
Impurities"
A median based interpolation algorithm for deinterlacing,"In this paper, state-of-the-art interpolation algorithms for deinterlacing within a single frame are investigated. Based on Chen's directional measurements and on the median filter, a novel interpolation algorithm for deinterlacing is proposed. By efficiently estimating the diagonal and vertical directional correlations of the neighboring pixels, the proposed method performs better than existing techniques on different images and video sequences, for both subjective and objective measurements. Additionally, the proposed method has a simple structure with low computation complexity, which therefore makes it simple to implement in hardware.","Interpolation,
Video sequences,
Hardware,
Motion detection,
HDTV,
Image quality,
Equations,
Computer science,
Software engineering,
Australia"
Prosody recognition in male infant-directed speech,"Robots designed to learn from and interact with humans require an intuitive method for humans to communicate with them. Normal human speech is very difficult to process, requiring many kinds of complex analysis for robots to interpret it. An intermediate method for communication is recognition of prosody, the affective content of speech. Using prosody recognition, a human interacting with a robot can reward or punish its actions by scolding or praising it. In this project, prosody recognition of male voices is performed by feature-based analysis of sound files containing short utterances, which were recorded from subjects who were directed to emulate infant-directed speech, which generally contains exaggerated prosody (Breazeal, C and Aryanada, L, 2000). The features used are extracted from the energy and pitch contours in the preprocessing stage. The classifier discriminates amongst four affective classes of speech and neutral utterances. The four classes are prohibition, attentional bids, approval, and soothing, while the neutral utterances are speech, which carries none of the above affective intents. Discrimination is performed using a multistage k-nearest neighbor classifier. The five-way single-stage classifier operates at 62.5 accuracy on the entire male speech data set, while the female single-stage classifier classifies 66.7 percent correctly. Chi-square analysis resulted in a p of less than or equal to 0.001 for each. The data seem to indicate that while female voice data may be somewhat easier to classify than male, fundamental differences that make male utterances unsuitable for classification do not exist.","Speech recognition,
Speech analysis,
Human robot interaction,
Pediatrics,
Speech processing,
Computer science,
Performance analysis,
Feature extraction,
Laboratories,
Interactive systems"
A model driven approach for software systems reliability,"The main contribution of this research is to provide platform-independent means to support reliability design following the principles of a model driven approach. The contribution aims to systematically address dependability concerns from the early to the late stages of software development. MDA appears to be a suitable framework to assess these concerns and, therefore, semantically integrate analysis and design models into one environment.","Software systems,
Unified modeling language,
Computer architecture,
Software engineering,
Software safety,
Software design,
Software testing,
Computer science,
Educational institutions,
Reliability engineering"
Combination methods in microarray analysis,"Microarray technology and experiment can produce thousands or tens of thousands of gene expression measurements in a single cellular mRNA sample. Selecting a list of informative differential genes from these measurement data has been the central problem for microarray analysis. Many methods to identify informative genes have been proposed in the past. However, due to the complexity of biological systems, each proposed method seems to perform nicely in a particular data set or specific experiment. It remains a great challenge to come up with a selection method for a wider spectrum of experiments and a broader variety of data sets. In this paper, we take the approach of method combination using data fusion and rank-score graph which have been used successfully in other application domains such as information retrieval, pattern recognition and tracking, and molecular similarity search. Our method combination is efficient and flexible and can be extended to become a general learning system for microarray gene expression analysis.","Gene expression,
Information retrieval,
Pattern recognition,
Data analysis,
Computer science,
Information systems,
Head,
Neck,
Surgery,
Medical diagnostic imaging"
Adaptive data broadcasting in asymmetric communication environments,"We present a new adaptive broadcast dissemination model to support flexible responses to client requests. Several features distinguish our model. First, client queries do not target individual documents, but specify the required information by attributes. Second, clients are satisfied by responses that are sufficiently close to the desired information. Finally, the server in our model solicits randomized feedback from clients to adapt its broadcast program to client needs. Our simulation results show that our model captures the interest patterns of clients more efficiently and more accurately and scales very well with the number of clients, while reducing overall client average waiting times.","Broadcasting,
Databases,
Feedback,
Processor scheduling,
Computer science,
Power system modeling,
Mobile communication,
Feeds,
Traffic control,
Information systems"
Reaching through learned forward model,"This paper presents a learning approach for a humanoid to reach objects in its environment. Instead of assuming that the exact forward kinematics of the arm is given, we address the reaching problem by first learning forward kinematics with a RBFN through autonomously gathered training samples. The learnt forward model is subsequently used to construct Jacobian matrices to incrementally generate straight reaching trajectory exhibited by humans. We show that if the learning parameters are set appropriately, a RBFN trained on a small number of samples corrupted by perception noise can still lead to high reaching accuracy. The size of the training set can be further reduced without severe performance degradation if limited visual feedback is used to aid reaching after the end effector has been moved into the neighborhood of the desired object.","Kinematics,
Humans,
Feedback,
Jacobian matrices,
Robots,
Sun,
Computer science,
Degradation,
End effectors,
Motion control"
Dynamic update of shortest path tree in OSPF,"The shortest path tree (SPT) construction is a critical issue to the high performance routing in an interior network using link state protocols, such as open shortest path first (OSPF) and IS-IS. In this paper, we propose a new efficient algorithm for dynamic SPT update to avoid the disadvantages (e.g. redundant computation) caused by static SPT update algorithms. The new algorithm is based on the understanding of the update procedure to reduce redundancy. Only significant elements that contribute to the construction of new SPT from the old one are focused on. The efficiency of our algorithm is improved because it only pay attention to the edges really count for the update process. The running time for the proposed algorithm is maximum reduced, which is shown through experimental results. Furthermore, our algorithm can be easily generalized to solve the SPT updating problem in a graph with negative weight edges and applied to the scenario of multiple edge weight changes.","Routing protocols,
Heuristic algorithms,
Computer networks,
Network topology,
Computer science,
IP networks,
Costs,
Delay,
Buildings,
Telecommunication traffic"
Scaling up support vector data description by using core-sets,"Support vector data description (SVDD) is a powerful kernel method that has been commonly used for novelty detection. While its quadratic programming formulation has the important computational advantage of avoiding the problem of local minimum, this has a runtime complexity of O(N/sup 3/), where N is the number of training patterns. It thus becomes prohibitive when the data set is large. Inspired from the use of core-sets in approximating the minimum enclosing ball problem in computational geometry, we propose An approximation method that allows SVDD to scale better to larger data sets. Most importantly, the proposed method has a running time that is only linear in N. Experimental results on two large real-world data sets demonstrate that the proposed method can handle data sets that are much larger than those that can be handled by standard SVDD packages, while its approximate solution still attains equally good, or sometimes even better, novelty detection performance.","Kernel,
Quadratic programming,
Support vector machines,
Event detection,
Support vector machine classification,
Machine learning,
Covariance matrix,
Computer science,
Electronic mail,
Packaging"
Core-bored search-and-rescue applications for an agile limbed robot,"A custom version of the TerminatorBot is described for core bored inspection during search-and-rescue operations. ""Core bored inspection"" refers to visual inspection of a void by passing a small camera through an access hole into the void. This is the classic ""camera-on-a-stick"" approach. Sometimes the access hole occurs naturally. Sometimes a suspected void has no access hole. To gain access, a hole is bored through the rubble with a coring tool, hence the term ""core-bored inspection"". In either case, the camera, once inside, can articulate to look around, but is limited to fine-of-sight. Occlusions can prevent a thorough inspection or force using/boring another hole. A small, agile robotic device could augment the use of such cameras. We propose the TerminatorBot as a prototype limbed robot for studying such applications.","Inspection,
Robot sensing systems,
Concrete,
Probes,
Boring,
Slabs,
Application software,
Computer science,
Navigation,
Process planning"
Reuse of a geometric model for shape approximation,"A robust procedure of three-dimensional shape modeling for an object approximation is proposed. A solid model is first prepared or reused for the prototype and then transferred to the model that approximates the shape of the object. Vertex geometry of the prototype is modified based on the multidirectional silhouettes and light stripe pattern projection. Topology of the mesh, shape and connectivity of the vertices, of the prototype is conserved throughout the modeling process. Stable meshing and accurate shape approximation are achieved, starting from a simple or a dedicated model with desired meshes. A dedicated male sculpture and a simple cylinder with quadrilateral meshes are demonstrated for the shape approximation. This eliminates the laborious modeling procedures, reusing a proven or simple and stable model, and the complicated camera calibrations.","Solid modeling,
Coordinate measuring machines,
Image reconstruction,
Prototypes,
Cameras,
Lighting,
Shape measurement,
Computer science,
Robustness,
Topology"
Edge-valued decision diagrams for multiple-valued functions,"In this paper, we provide a brief review of various decision diagrams (DDs) with attributed edges, and then we propose a method for construction of various edge-valued diagrams for multiple-valued logic functions. We consider construction of both edge-valued diagrams with additive and the multiplicative attributes at the edges. We illustrate the application of related algorithms by constructing examples of decision diagrams for quaternary functions.",
Driving pattern prediction for an energy management system of hybrid electric vehicles in a specific driving course,"Energy management systems that consider a whole driving pattern have been proposed to improve the fuel consumption and the emissions of hybrid electric vehicles. In this research, we propose a new energy management system that includes a driving pattern prediction system. The proposed method pay attention to a commuting to work because a commuting occupies the large portion of mileage and are repeated day-to-day. Therefore the database of driving patterns can be simply constructed by the proposed clustering method, and the future driving pattern can be predicted from the database. In this paper, it is shown that the actual driving pattern is changed by external factors, and we propose an improve method of driving pattern prediction. The experimental result shows the availability of the proposed system.","Energy management,
Hybrid electric vehicles,
Databases,
Medical services,
Torque,
Prediction methods,
Batteries,
Pattern matching,
Computer science,
Fuels"
An evolutionary snake algorithm for the segmentation of nuclei in histopathological images,"This paper addresses the problem of automatic segmentation of nuclei in histopathological images. A novel method, inspired from active contour models is proposed. An evolutionary based approach, which guarantees convergence to global minimum energies has been used to solve the combinatorial optimization problem of snakes. The computational complexity, often associated with evolutionary approaches, has been reduced by short cutting the natural evolution step by means of replacing standard mutation with an oriented stochastic mutation process. Results have shown the efficiency of this method both in terms of accuracy and fast computation.","Image segmentation,
Convergence,
Cancer,
Active contours,
Genetic mutations,
Pathology,
Shape,
Dynamic programming,
Computer science,
Stochastic processes"
TDM-based coordination function (TCF) in WLAN for high throughput,"IEEE 802.11 has become a dominant standard for wireless local area network (WLAN), and the users' demand for high throughput of IEEE 802.11 is continuously increasing. We propose a new medium access control scheme, TDM-based coordination function (TCF), which can be used when all the stations are within the radio transmission range. TCF uses information on the number of active stations explicitly to eliminate the contention period in the DCF (distributed coordination function) of IEEE 802.11. TCF can improve overall throughput and provide fair sharing of resources among active stations. Also, TCF is simple and can be implemented distributively. Simulation is performed to compare TCF with DCF and FCR (fast collision recovery) in terms of performance.","Wireless LAN,
Throughput,
Media Access Protocol,
Data communication,
Intelligent networks,
Computer science,
Wireless communication,
Communications technology,
Wireless networks,
Bluetooth"
XML and computational science,"The Extensible Markup Language (XML) is a specification for document interchange that the World Wide Web Consortium (W3C) developed in 1998. In many ways, XML is the lingua franca among programming language enthusiasts, and proponents argue that it could potentially solve the multitude of data management and analysis problems the entire computing industry currently faces. XML might make a real difference, especially in computing, engineering, and the mathematical sciences, in part because we can use it with different languages. The author presents some background and lightweight examples of XML usage, describes some XML component frameworks along with their purpose and applicability to computational science, and discusses some technical obstacles to overcome for the language to be taken seriously in computational science.","XML,
Computational modeling,
Web sites,
Writing,
Astrophysics,
Scientific computing"
Visual Tracking Using Depth Data,"A method is presented for robust tracking in highly cluttered environments. The method makes effective use of 3D depth sensing technology, resulting in illumination-invariant tracking. A few applications using tracking are presented including face tracking and hand tracking.","Target tracking,
Robustness,
Stereo vision,
Computer vision,
Head,
Image edge detection,
Computer science,
Application software,
Face detection,
Shape"
Vehicular communication - a candidate technology for traffic safety,"Research in traffic safety has indicated that in-vehicle safety systems provide a better service to drivers when they use data about nearby vehicles. For supplying such information, vehicular communication can be employed. However, the development of in-vehicle safety systems is currently at an early stage, and the applicability of the communication between vehicles for improving traffic safety is still under investigation. This paper provides an analysis of the role of communication for implementing traffic safety services. We survey approaches to vehicular communication and identify the most appropriate alternative for developing active safety systems that implement safety functions such as collision warning and collision avoidance.","Vehicle safety,
Sensor systems,
Laser radar,
Remotely operated vehicles,
Vehicle driving,
Computer science,
Road accidents,
Collision avoidance,
Mobile robots,
Radar detection"
Collaborative information systems and business process design using simulation,"The information systems (IS) community promotes the idea that IS analyst should have a clear understanding of the way the organization operates before attempting to propose an IS solution. It is argued that to take a complete advantage of the underlying information technology (IT), organizations should first identify any process flaw and then propose a suitable IT solution. Similarly, many process design approaches claim that business process (BP) design should be done considering the advantages provided and the limitations imposed by the underlying (IT). Despite this fact research in these domains provides little indication of which mechanisms or tools can help BP and IS analyst to understand the complex relationships amongst these two areas. This paper describes the insights gained during a UK funded research project, namely ASSESS-IT that aimed to depict the dynamic relationships between IT and BP using simulation. One of the major limitations of the ASSESS-IT project is that it looked at relationship between BP and IT as a three layered structure, namely BP, IS and computer networks (CN), and did not explore in detail the relationships between BP and IS alone. This paper uses the outcomes derived from this project and suggests that, is some cases, the relationship between BP and IT could be analyzed by looking at the relationship between BP and IS alone. It then proposes an alternative simulation framework, namely BPISS, that provides the guideline to develop simulation models that portray BP and IS behavior performance measurements, offering in this way an alternative mechanism that can help BP and IS analyst to understand in more detail the dynamic interactions between BP and IS domains.","Collaboration,
Information systems,
Process design,
Analytical models,
Information analysis,
Information technology,
Computational modeling,
Computer networks,
Guidelines,
Measurement"
Mosaicing with Parallax using Time Warping,"2D image alignment methods are applied successfully for mosaicing aerial images, but fail when the camera moves in a 3D scene. Such methods can not handle 3D parallax, resulting in distorted mosaicing. General egomotion methods are slow, and do not have the robustness of 2D alignment methods. We propose to use the x-y-t space-time volume as a tool for depth invariant mosaicing. When the camera moves on a straight line in the x direction, a y-t slice of the space-time volume is a panoramic mosaic, while a x-t slice is an EPI plane. Time warping, which is a resampling of the t axix, is used to form straight feature lines in the EPI planes. This process will simultaneously give best panoramic mosaic in the y-t slices. This approach is as robust as 2D alignment methods, while giving depth invariant motion (""ego motion""). Extensions for two dimensional camera motion on a plane are also described, with applications for 2D mosaicing, and for image based rendering such as ""light field"".","Cameras,
Layout,
Robustness,
Rendering (computer graphics),
Photography,
Computer science,
Cities and towns,
Inspection,
Contracts,
Motion control"
A formal semantics of UML sequence diagram,"We present a formal semantics of UML sequence diagram. In abstract syntax form, a well-formed sequence diagram corresponds to an ordered hierarchical structure tree. The static semantics of a sequence diagram is to check whether it is consistent with the class diagram declaration as well as with its well-formed tree structure. Meanwhile, the dynamic semantics is defined in terms of the state transitions that are carried out by the method invocations in the diagram. When a message is executed, it must be consistent with system state, i.e., object diagram and the state diagrams of its related objects. The semantics clearly captures the consistency between sequence diagram with class diagram and state diagram. Therefore, it is useful to develop the model consistent checking functions in UML CASE tools. And it also can be used to reason about the correctness of a design model with respect to a requirement model.","Unified modeling language,
Object oriented modeling,
Helium,
Mathematics,
Computer science,
Tree data structures,
Computer aided software engineering,
Visualization,
Software systems,
Object oriented programming"
Revisiting ERP systems: benefit realization,"Many companies initially implemented their ERP systems to solve Y2K and disparate systems issues. These same companies are now looking at a how to strategically leverage their investment in these systems through the implementation ""second wave"" functionality. This paper identifies the expected and actual benefits of ""second wave"" implementations. In addition it identifies barriers, which limit the benefit realization. The findings reinforce that ERP implementations are people focused projects which rely heavily on change management for success.","Enterprise resource planning,
Companies,
Information systems,
Australia,
Investments,
Marketing and sales,
Manufacturing industries,
Computer aided manufacturing,
Asset management,
Financial management"
Guest Editors' Introduction: Dependable Agent Systems,It is well known that building dependable software systems for dynamic environments is difficult. It is also well known that building large-scale distributed software systems is difficult. The relatively few attempts to combine these two tasks confirm that successfully building large-scale distributed systems with predictable dependability properties is exceptionally difficult. The articles in this special issue of IEEE Intelligent Systems deal with this issue and discuss an emerging and exciting new approach to building these most challenging kinds of systems.,
SOT: secure overlay tree for application layer multicast,"Application layer multicast (ALM) has been proposed to overcome current limitations in IP multicast. We address, for the first time, offering data confidentiality in ALM. To achieve data confidentiality, data encryption keys are shared among the multicast group members. Observe that in this system, a node may need to continuously reencrypt packets before forwarding them downstream. Furthermore, keys have to be changed whenever there is a membership change, leading to rekey processing overhead at the nodes. For a large and dynamic group, these reencryption and rekeying operations incur high processing overhead at the nodes. We introduce a scalable scheme called secure overlay tree (SOT) which clusters ALM peers so as to localize rekeying within a cluster and to limit reencryption at cluster boundaries, thereby minimizing the total nodal processing overhead. We describe the operations of SOT and compare its nodal processing overhead with two other basic approaches, namely, host-to-host encryption and whole group encryption. We show that there exists an optimal cluster size to minimize the total nodal processing overhead. SOT achieves substantial reduction in nodal processing overhead with little cost in network performance in terms of network stress and delay.","Cryptography,
Peer to peer computing,
Application software,
Computer science,
Costs,
Stress,
IP networks,
Videoconference,
Protection,
Unicast"
An edge in time saves nine: LP rounding approximation algorithms for stochastic network design,"Real-world networks often need to be designed under uncertainty, with only partial information and predictions of demand available at the outset of the design process. The field of stochastic optimization deals with such problems where the forecasts are specified in terms of probability distributions of future data. In this paper, we broaden the set of models as well as the techniques being considered for approximating stochastic optimization problems. For example, we look at stochastic models where the cost of the elements is correlated to the set of realized demands, and risk-averse models where upper bounds are placed on the amount spent in each of the stages. These generalized models require new techniques, and our solutions are based on a novel combination of the primal-dual method truncated based on optimal LP relaxation values, followed by a tree-rounding stage. We use these to give constant-factor approximation algorithms for the stochastic Steiner tree and single sink network design problems in these generalized models.","Intelligent networks,
Approximation algorithms,
Stochastic processes,
Algorithm design and analysis,
Costs,
Uncertainty,
Bandwidth,
Computer science,
Probability distribution,
Joining processes"
Symbolic execution of program paths involving pointer structure variables,"Many white-box testing methods are based on the analysis of program paths. For these methods, an important problem is to determine the feasibility of a given path, and find appropriate input data to execute the path if it is feasible. The symbolic execution of program paths is studied. An approach is presented, which translates a path involving pointer and structure variables to a path involving simple variables and array variables only. The later is then analyzed with constraint solving techniques. An implementation of the translation tool is described with some examples. Preliminary experimental results show that the approach is quite efficient and applicable to paths in typical programs.","Flow graphs,
Input variables,
Laboratories,
Computer science,
Software testing,
Graph theory,
Software quality,
Arithmetic"
Software failure rate and reliability incorporating repair policies,"Reliability of a software application, its failure rate and the residual number of faults in an application are the three most important metrics that provide a quantitative assessment of the failure characteristics of an application. Typically, one of many stochastic models known as software reliability growth models (SRGMs) is used to describe the failure behavior of an application in its testing phase, and obtain an estimate of the above metrics. In order to ensure analytical tractability, SRGMs are based on an assumption of instantaneous repair and thus the estimates of the metrics obtained using SRGMs tend to be optimistic. In practice, fault repair activity consumes a nonnegligible amount of time and resources. Also, repair may be conducted according to many policies which are reflective of the schedule and budget constraints of a project. A few research efforts that have sought to incorporate repair into SRGMs are restrictive, since they consider only one of the several SRGMs, model the repair process using a constant rate, and provide an estimate of only the residual number of faults. These techniques do not address the issue of estimating application failure rate and reliability in the presence of repair. In this paper we present a generic framework which relies on the rate-based simulation technique in order to provide the capability to incorporate various repair policies into the finite failure nonhomogeneous Poisson process (NHPP) class of software reliability growth models. We also present a technique to compute the failure rate and the reliability of an application in the presence of repair. The potential of the framework to obtain quantitative estimates of the above three metrics taking into consideration different repair policies is illustrated using several scenarios.","Application software,
Software reliability,
Stochastic processes,
Phase estimation,
Computer science,
Reliability engineering,
Software testing,
Computational modeling,
Information technology,
Environmental economics"
A novel information hiding scheme based on BTC,"The general hiding strategy is to embed the confidential image such as a weaponry plan, called secret image in this paper, into an in-confidential image. However, we have found that sometimes people also transmit the compression codes generated by a compression technique, such as VQ or BTC, via the Internet in order to save the transmission time. The key point of this paper is to embed the secret image into the compression codes generated by the BTC algorithm, such as means and the bitmap of each block. The experimental results have confirmed that the proposed approach would indeed allow legal receivers to retrieve the secret images without distorting them. Although the original means and bitmaps are modified in order to hide the secret images, the difference between the PSNRs generated by original BTC and our scheme is insignificant.","Image coding,
Cryptography,
Decoding,
Internet,
Protection,
Computer science,
Law,
Legal factors,
Bandwidth,
Data communication"
Towards the verification and validation of online learning systems: general framework and applications,"Online adaptive systems cannot be certified using traditional testing and proving methods, because these methods rely on assumptions that do not hold for such systems. In this paper, we discuss a framework for reasoning about online adaptive systems, and see how this framework can be used to perform the verification of these systems. In addition to the framework, we present some preliminary results on concrete neural network models.","Learning systems,
Adaptive systems,
Neural networks,
Control systems,
Adaptive control,
Aerospace control,
Programmable control,
Intelligent sensors,
Fault tolerant systems,
Aerodynamics"
A time interval based consistency control algorithm for interactive groupware applications,"Traditional concurrency control methods such as locking and serialization are not suitable for distributed interactive applications that demand fast local response. Operational transformation (OT) is the standard solution to concurrency control and consistency maintenance in group editors, an important class of interactive groupware applications. It generally trades consistency for local responsiveness, because human users can often tolerate temporary inconsistencies but do not like their interactions be lost or nondeterministically blocked. This paper presents a time interval based operational transformation algorithm (TIBOT) that overcomes the various limitations of previous related work. Our approach guarantees content convergence and is significantly more simple and efficient than existing approaches. This is achieved in a pure replicated architecture by using a linear clock and by posing some constraints on communication that are reasonable for the application domain.","Collaborative software,
Collaborative work,
Application software,
Concurrency control,
Distributed computing,
Sun,
Delay,
Convergence,
Computer networks,
Computer science"
A user-level framework for scheduling within service execution environments,"To support application-specific quality of service for hosted services, a client of a compute utility requires the ability to schedule the processor resources supplied to its service. We present a user-level scheduling framework that operates in tandem with a standard kernel scheduler to support user-level policies for sharing processor resources. The scheduler operates by sampling the resource consumption of processes and limiting which processes are eligible for scheduling by the kernel. We present a Unix implementation of this framework and show that it can accurately control the rate of execution of compute-bound processes, with low computational overhead, despite its user-level operation. Finally, we demonstrate the scheduler's ability to enforce differentiated qualities of service for a Web-based message board service.","Processor scheduling,
Kernel,
Quality of service,
Operating systems,
Single machine scheduling,
Computer science,
Sampling methods,
Discussion forums,
Computer networks,
Web server"
Progress in the development of anthropomorphic fluidic hands for a humanoid robot,"New lightweight anthropomorphic hands are presented for application in a humanoid robot. These hands possess 13 to 15 degrees of freedom and are driven by flexible fluidic actuators that are integrated in the finger joints. The compact design of the hands contains a pressure unit that is housed in the metacarpus. Alternatively, the pressure unit can be mounted externally, which leads to a further mass reduction. The new design and its performance shall be described in the present article.","Anthropomorphism,
Humanoid robots,
Actuators,
Humans,
Mobile robots,
Grasping,
Manipulators,
Application software,
Fingers,
Collaboration"
Towards supporting fine-grained access control for Grid resources,"The heterogeneous nature and independent administration of geographically dispersed resources in a Grid demand the need for access control using fine-grained policies. In this paper, we investigate the problem of fine-grained access control in the context of resource allocation in the Grid, as we believe it is the first and key step in developing access control methods specifically tailored for Grid systems. To perform this access control, we design a security component (to be part of a meta-scheduler service) that finds the list of nodes where a user is authorized to run his/her jobs. The security component is designed in an effort to reduce the number of rules that need to be evaluated for each user request. We believe such a fine-grained policy-based access control would help the adoption of the Grid to a higher extent into new avenues such as desktop Grids, as the resource owners are given higher flexibility in controlling access to their resources. Similarly, Grid users get a higher flexibility in choosing the resources in which their jobs must execute.","Access control,
Resource management,
Security,
Scheduling,
Computer architecture,
Computer science,
Job design,
Collaboration,
Organizing,
Conferences"
Scalable and efficient parallel algorithms for Euclidean distance transform on the LARPBS model,"A parallel algorithm for Euclidean distance transform (EDT) on linear array with reconfigurable pipeline bus system (LARPBS) is presented. For an image with n/spl times/n pixels, the algorithm can complete EDT transform in O(n log n/c(n) log d(n)) time using n/spl middot/d(n)/spl middot/c(n) processors, where c(n) and d(n) are parameters satisfying 1/spl les/c(n)/spl les/n, and 1","Parallel algorithms,
Euclidean distance,
Pixel,
Phase change random access memory,
Computer science,
Image processing,
Optical arrays,
Computer Society,
Pipelines,
Image analysis"
Identifying significant genes from microarray data,"Microarray technology is a recent development in experimental molecular biology which can produce quantitative expression measurements for thousands of genes in a single, cellular mRNA sample. These many gene expression measurements form a composite profile of the sample, which can be used to differentiate samples from different classes such as tissue types or treatments. However, for the gene expression profile data obtained in a specific comparison, most likely only some of the genes will, be differentially expressed between the classes, while many other genes have similar expression levels. Selecting a list of informative differential genes from these data is important for microarray data analysis. In this paper, we describe a framework for selecting informative genes, called ranking and combination analysis (RAC), which combines various existing informative gene selection methods. We conducted experiments using three data sets and six existing feature selection methods. The results show that the RAC framework is a robust and efficient approach to identify informative gene for microarray data. The combination approach on two selecting methods almost always performed better than the less efficient individual, and in many cases, better than both. More significantly, when considering all three data sets together, the combination approach, on average, outperforms each individual feature selection method. All of these indicate that RCA might be a viable and feasible approach for the microarray gene expression analysis.","Data analysis,
Biomedical measurements,
Gene expression,
Biomedical engineering,
Biomedical computing,
Computer science,
Biomedical informatics,
Medical diagnostic imaging,
Information science,
Information systems"
An automated feedback system for computer organization projects,"This paper describes a system, built and refined over the past five years, that automatically analyzes student programs assigned in a computer organization course. The system tests a student's program, then e-mails immediate feedback to the student to assist and encourage the student to continue testing, debugging, and optimizing his or her program. The automated feedback system improves the students' learning experience by allowing and encouraging them to improve their program iteratively until it is correct. The system has also made it possible to add challenging parts to each project, such as optimization and testing, and it has enabled students to meet these challenges. Finally, the system has reduced the grading load of University of Michigan's large classes significantly and helped the instructors handle the rapidly increasing enrollments of the 1990s. Initial experience with the feedback system showed that students depended too heavily on the feedback system as a substitute for their own testing. This problem was addressed by requiring students to submit a comprehensive test suite along with their program and by applying automated feedback techniques to help students learn how to write good test suites. Quantitative iterative feedback has proven to be extremely helpful in teaching students specific concepts about computer organization and general concepts on computer programming and testing.","Computer testing,
Automatic programming,
Software testing,
Data processing,
Computer science education"
Improving Generalization Performance of Natural Gradient Learning Using Optimized Regularization by NIC,"Natural gradient learning is known to be efficient in escaping plateau, which is a main cause of the slow learning speed of neural networks. The adaptive natural gradient learning method for practical implementation also has been developed, and its advantage in real-world problems has been confirmed. In this letter, we deal with the generalization performances of the natural gradient method. Since natural gradient learning makes parameters fit to training data quickly, the overfitting phenomenon may easily occur, which results in poor generalization performance. To solve the problem, we introduce the regularization term in natural gradient learning and propose an efficient optimizing method for the scale of regularization by using a generalized Akaike information criterion (network information criterion). We discuss the properties of the optimized regularization strength by NIC through theoretical analysis as well as computer simulations. We confirm the computational efficiency and generalization performance of the proposed method in real-world applications through computational experiments on benchmark problems.",
Simulating the Webgraph: a comparative analysis of models,"The Webgraph is the directed graph produced by the World Wide Web's hyperlinked structure: its nodes are static html pages, and its edges are the hyperlinks between two pages. Since the early '90s, the Web has grown exponentially - a trend we expect will continue. Today's Webgraph has several billion edges, but in spite of its size, it exhibits a well-defined structure characterized by several properties. In the past few years, several research papers have reported these properties and proposed various random graph models. We simulated several of these models and compared them against a 300-million-node sample of the Webgraph provided by the Stanford WebBase project (http://www-diglib.stanford.edu//spl sim/testbed/doc2/WebBase/). All the software we developed to perform this comparison is free to download from the European Research Project COSIN Web site (www.cosin.org). Over the past six years, computer scientists, economists, mathematicians, and physicists have extensively studied the Webgraph's properties. All this research was motivated primarily by the need to efficiently mine the huge quantities of information on the Web - information that is often distributed among several pages. The first major discovery concerned in-degree, an intuitive and simplistic measure of page importance. (Each node in a directed graph is characterized by in-degree and out-degree - the number of incoming and outgoing links, respectively).",
Dimensionality reduction of face images for gender classification,"Data in most real world applications are high dimensional and learning algorithms like neural networks have problems in handling high dimensional data. However, the intrinsic dimension is often much less than the original dimension of the data. We use a fractal based method to estimate the intrinsic dimension and show that a nonlinear projection method called curvilinear component analysis can effectively reduce the original dimension to the intrinsic dimension. We apply this approach for dimensionality reduction of the face images data and use neural network classifiers for gender classification.",
Error estimation and optimization of the method of auxiliary sources (MAS) for scattering from a dielectric circular cylinder,"This article presents a rigorous error estimation of the method of auxiliary sources (MAS) when applied to the solution of the electromagnetic scattering problem involving dielectric objects. The geometry investigated herein is a circular, dielectric cylinder of infinite length. The MAS matrix is inverted analytically, via advanced eigenvalue analysis, and an exact expression for the boundary condition error owing to discretization is derived. Furthermore, an analytical formula for the condition number of the linear system is also extracted, explaining the irregular behavior of the computational error resulting from numerical matrix inversion. Also, the effects of the dielectric parameters on the error are fully investigated. Finally, the optimal location of the auxiliary sources is determined on the grounds of error minimization.","Dielectrics,
Electromagnetic scattering,
Boundary conditions,
Optimization,
Electric fields"
Mobile agents: What about them? Did they deliver what they promised? Are they here to stay? (panel),"Mobile Agents have brought around a new way to perform computations and develop distributed application and it is now struggling for a visible position in the area of distributed and wireless computing. This panel explores the issues that affect and direct the acceptance or not of this computing paradigm, i.e., mobile agents technologies.","Mobile agents,
Java,
Distributed computing,
Application software,
Computer applications,
Mobile computing,
IP networks,
Artificial intelligence,
Relational databases,
Computer science"
Program structuring algorithms for dynamically reconfigurable parallel systems based on redundant connection switches,"A new program graph structuring algorithm for dynamically reconfigurable multi-processor systems based on the look-ahead dynamic connection reconfiguration is presented. This architectural model enables elimination of connection reconfiguration time overheads. It consists in preparing link connections in advance in redundant connection switches in parallel with program execution. An application program is partitioned into sections, which are executed using such connections. Parallel program scheduling in this environment incorporates graph partitioning problem. The new algorithm is based on list scheduling and a new iterative clustering heuristics for graph partitioning. The experimental results are presented, which compare performance of several graph partitioning heuristics for such environment.","Heuristic algorithms,
Switches,
Communication switching,
Communication system control,
Control systems,
Scheduling algorithm,
Costs,
Partitioning algorithms,
Computer science,
Process control"
A self-stabilizing directed diffusion protocol for sensor networks,"We design a self-stabilizing communication protocol in a sensor network, based on the directed diffusion method [1]. A request for data from an initiator node is broadcast in the network, and the positive answers from the sensors are forwarded back to the initiator (following a Shortest-Path- Tree (SPT) construction rooted at the initiator.) The sensor nodes, starting from an arbitrary state and following our protocol, establish reliable communication in the network in a finite number of steps. Any number of initiators and any number of different requests at a time per initiator are allowed, but we limit the number of entries in the interest cache as the memory of a sensor node is limited.","Protocols,
Telecommunication network reliability,
Sensor systems,
Network topology,
Wireless sensor networks,
Computer science,
Back,
Optical sensors,
Infrared sensors,
Intrusion detection"
Evolving sparse direction maps for maze pathfinding,"A genetic algorithm is used to solve a class of maze pathfinding problems. In particular, we find a complete set of paths directing an agent from any position in the maze towards a single goal. To this end, we define a sparse direction map, wherein the maze is divided into sectors, each of which contains a direction indicator. Maps are evolved using a simple genetic algorithm. The fitness function samples the efficacy of the map from random starting points, this estimating the likelihood that agents find the goal. The framework was effective in evolving successful maps for three different mazes of varying size and complexity, resulting in interesting and lifelike agent behavior suitable for games, but not always the shortest paths.","Genetic algorithms,
Filling,
Counting circuits,
Computer science,
Delay"
TWISTER: an immersive autostereoscopic display,"This paper describes in detail the design, development, and evaluation of the TWISTER III (Telexistence Wide-angle Immersive STEReoscope) and demonstrates how this system can display immersive three-dimensional full-color and live motion pictures without the need for special eye-wear. The device works as a cylindrical display by rotating 30 display units around an observer and presenting time-varying patterns, while immersive autostereoscopic vision is achieved by employing a ""rotating parallax barrier"" method. After explaining the principle, we discuss the designs and implementations for maximum performance in various aspects of the display. We also evaluate the display.","Virtual reality,
Motion pictures,
Holography,
Optical arrays,
Computer displays,
Light emitting diodes,
Information science,
Three dimensional displays,
Holographic optical components,
Optical sensors"
On the list and bounded distance decodibility of Reed-Solomon codes,"For an error-correcting code and a distance bound, the list decoding problem is to compute all the codewords within a given distance to a received message. The bounded distance decoding problem is to find one codeword if there is at least one codeword within the given distance, or to output the empty set if there is not. Obviously the bounded distance decoding problem is not as hard as the list decoding problem. For a Reed-Solomon code [n, k]/sup q/, a simple counting argument shows that for any integer 0 < g < n, there exists at least one Hamming ball of radius n - g, which contains at least (/sup n//sub g/)/q/sup g-k/ many codewords. Let g(n, k, q) be the smallest positive integer g such that (/sup n//sub g/)/q/sup g-k/ < 1. One knows that k /spl les/ g(n, k, q) /spl les/ /spl radic/nk /spl les/ n. For the distance bound up to n- /spl radic/nk;, it is well known that both the list and bounded distance decoding can be solved efficiently. For the distance bound between n - /spl radic/nk and n - g(n, k, q), we do not know whether the Reed-Solomon code is list, or bounded distance decodable, nor do we know whether there are polynomially many codewords in all balls of the radius. It is generally believed that the answers to both questions are no. There are public key cryptosystems proposed recently, whose security is based on the assumptions. In this paper, we prove: (1) List decoding can not be done for radius n - g(n, k: q) or larger, otherwise the discrete logarithm over F/sub qg(m, k, q)-k/ is easy. (2) Let h and g be positive integers satisfying q /spl ges/ max(g/sup 2/, (h-l)/sup 2+/spl epsiv//) and g /spl ges/ (4//spl epsiv/ + 2)(h + 1) for a constant /spl epsiv/ > 0. We show that the discrete logarithm problem over F/sub qh/ can be efficiently reduced by a randomized algorithm to the bounded distance decoding problem of the Reed-Solomon code [q, g - h]/sub q/ with radius q - g. These results show that the decoding problems for the Reed-Solomon code are at least as hard as the discrete logarithm problem over finite fields. The main tools to obtain these results are an interesting connection between the problem of list-decoding of Reed-Solomon code and the problem of discrete logarithm over finite fields, and a generalization of Katz's theorem on representations of elements in an extension finite field by products of distinct linear factors.","Decoding,
Reed-Solomon codes,
Error correction codes,
Galois fields,
Computer science,
Hamming distance,
Computer errors,
Public key cryptography,
Security,
Engineering profession"
Metric incremental clustering of nominal data,We present an algorithm/or clustering nominal data that is based on a metric on the set of partitions of a finite set of objects; this metric is defined starting from a lower valuation of the lattice of partitions. The proposed algorithm seeks to determine a clustering partition such that the total distance between this partition and the partitions determined by the attributes of the objects has a local minimum. The resulting clustering is quite stable relative to the ordering of the objects.,
Haptic tele-surgery simulation,"In the last decades, with the rapid development of electronic and computer technologies, advanced medical equipment has been introduced to facilitate medical diagnosis and treatment. Yet, medical education and training approaches have not benefited much from these developments. As the kinds of surgical operations increase, specialized procedures become more and more sophisticated. Thus, the medical specialists have to be well-trained before they perform surgery on real human beings. The purpose of this research is to develop a collaborative haptic simulation architecture for tracheotomy surgery and its training program. In this simulation, there will be two scenarios. In the first, two doctors operate collaboratively from different locations. In the second scenario, the trainer coaches the trainee on how to perform the surgery successfully in a tele-mentoring manner.","Haptic interfaces,
Surgery,
Medical simulation,
Medical diagnostic imaging,
Collaboration,
Computational modeling,
Biomedical equipment,
Medical diagnosis,
Medical treatment,
Computer science education"
Multiple communicating autonomous underwater vehicles,"This paper addresses the problems associated with missions using multiple cooperating autonomous underwater vehicles. In such missions, inter-vehicle communication is an important requirement upon which coordination strategy and intelligence decision making functions can be built. This paper presents briefly our current modeling and simulation work on characterizing the vehicle communication performance, together with some of the preliminary results. Even with much simplification of scenario modeling, simulation results exhibit intricate communication characteristics, and they can vary significantly as a function of mission requirements and scenarios.","Underwater vehicles,
Humans,
Intelligent vehicles,
Automotive engineering,
Costs,
Intelligent sensors,
Acoustic propagation,
Mobile communication,
Oceans,
Computer science"
Static identification of delinquent loads,"The effective use of processor caches is crucial to the performance of applications. It has been shown that cache misses are not evenly distributed throughout a program. In applications running on RISC-style processors, a small number of delinquent load instructions are responsible for most of the cache misses. Identification of delinquent loads is the key to the success of many cache optimization and prefetching techniques. We propose a method for identifying delinquent loads that can be implemented at compile time. Our experiments over eighteen benchmarks from the SPEC suite shows that our proposed scheme is stable across benchmarks, inputs, and cache structures, identifying an average of 10% of the total number of loads in the benchmarks we tested that account for over 90% of all data cache misses. As far as we know, this is the first time a technique for static delinquent load identification with such a level of precision and coverage has been reported. While comparable techniques can also identify load instructions that cover 90% of all data cache misses, they do so by selecting over 50% of all load instructions in the code, resulting in a high number of false positives. If basic block profiling is used in conjunction with our heuristic, then our results show that it is possible to pin down just 1.3% of the load instructions that account for 82% of all data cache misses.","Benchmark testing,
Computer science,
Prefetching,
Application software,
Optimizing compilers,
Parallel processing,
Hardware,
Computer aided instruction,
Runtime,
Robustness"
Formal verification coverage: computing the coverage gap between temporal specifications,"Existing methods for formal verification coverage compare a given specification with a given implementation, and evaluate the coverage gap in terms of quantitative metrics. We consider a new problem, namely to compare two formal temporal specifications and to find a set of additional temporal properties that close the coverage gap between the two specifications. In this paper we present: (1) the problem definition and motivation, (2) a methodology for computing the coverage gap between specifications, and (3) a methodology for representing the coverage gap as a collection of temporal properties that preserve the syntactic structure of the target specification.","Formal verification,
Formal specifications,
Logic,
Computer science,
Strategic planning"
The second level trigger of the ATLAS experiment at CERN's LHC,"The ATLAS trigger reduces the rate of interesting events to be recorded for off-line analysis in three successive levels from 40 MHz to /spl sim/100 kHz, /spl sim/2 kHz and /spl sim/200 Hz. The high level triggers and data acquisition system are designed to profit from commodity computing and networking components to achieve the required performance. In this paper, we discuss data flow aspects of the design of the second level trigger (LVL2) and present results of performance measurements.","Large Hadron Collider,
Detectors,
Magnetic field measurement,
Data acquisition,
Energy measurement,
Particle measurements,
Mesons,
Computer networks,
High performance computing,
Physics"
Trade-off between coverage and data reporting latency for energy-conserving data gathering in wireless sensor networks,"We propose a novel energy-conserving data gathering strategy for wireless sensor networks. The proposed strategy is based on a trade-off between coverage and data reporting latency with an ultimate goal of maximizing the network's lifetime. The basic idea is to select in each round only a minimum of k sensors as data reporters which are sufficient for a desired sensing coverage given by the users or applications. Such a selection of the minimum data reporters also reduces the amount of traffic flow to the data gathering point in each round, and thus avoids network congestion as well as channel interference/contention. The proposed strategy includes three schemes for the minimum k-sensor selection. Using these schemes, we evaluate such fundamental issues as event detection integrity and data reporting latency, which can be critical in deploying the proposed data gathering strategy. Simulation results demonstrate that the average data reporting latency is hardly affected and the real-time event detection ratio is greater than 80% when the desired sensing coverage is at least 80%. It is also shown that the sensors can conserve a significant amount of energy with a small trade-off, and that the higher the network density, the higher is the energy conservation rate without any additional computation cost.",
A method of generalized projections (MGP) ghost correction algorithm for interleaved EPI,"Investigations into the method of generalized projections (MGP) as a ghost correction method for interleaved EPI are described. The technique is image-based and does not require additional reference scans. The algorithm was found to be more effective if a priori knowledge was incorporated to reduce the degrees of freedom, by modeling the ghosting as arising from a small number of phase offsets. In simulations with phase variation between consecutive shots for n-interleaved echo planar imaging (EPI), ghost reduction was achieved for n=2 only. With no phase variation between shots, ghost reduction was obtained with n up to 16. Incorporating a relaxation parameter was found to improve convergence. Dependence of convergence on the region of support was also investigated. A fully automatic version of the method was developed, using results from the simulations. When tested on invivo 2-, 16-, and 32-interleaved spin-echo EPI data, the method achieved deghosting and image restoration close to that obtained by both reference scan and odd/even filter correction, although some residual artifacts remained.","Hospitals,
Convergence,
Magnetic resonance imaging,
Radiology,
Magnetic field measurement,
Testing,
In vivo,
Image restoration,
Magnetic separation,
Filters"
An event-driven clustering routing algorithm for wireless sensor networks,"Wireless sensor networks (WSNs) are an area of emerging networking research. One obstacle is its limited supply of energy. Therefore, minimizing energy consumption and maximizing system lifetime have been a major design goal for WSNs. This paper presents an energy-efficient event-driven clustering routing algorithm (EDC algorithm) based on unique features of the event-driven data model of WSNs. The algorithm can decide which nodes would become cluster-head nodes according to the maximum remainder energy of nodes, which are sensing an event occurred and are firstly switched to the active state if their several components are in the sleeping state. This strategy can keep sensor nodes with lower remainder energy out of being used up quickly. Detailed simulations of sensor network environments demonstrate that EDC algorithm saves node energy, prolongs system lifetime, and improves evenness of dissipated network energy.","Clustering algorithms,
Wireless sensor networks,
Sensor phenomena and characterization,
Routing protocols,
Data models,
Energy efficiency,
Educational institutions,
Sensor systems,
Wireless application protocol,
Computer science"
Eliminating replica selection - using multiple replicas to accelerate data transfer on grids,"Data-intensive, high-performance computing applications often require the efficient transfer of terabytes or even petabytes of data in wide-area, distributed computing environments. To increase the efficiency of wide area data movement, researchers have devised various techniques such as TCP tuning, multiple streams and asynchronous I/O. This paper adopts an approach to increase performance further by exploiting replica-level parallelism in grids. rFTP, a grid data transferring tool, improves the data transfer rate and reliability on grids by utilizing multiple replica sources concurrently. Experiments on the NPACI grid show as much as a 2.02/spl times/ speedup over a single data source by adaptively retrieving partial data segments from 4 replicas using the data provided by NWS.","Acceleration,
Distributed computing,
Information retrieval,
Weather forecasting,
Throughput,
Leg,
Computer science,
Computer applications,
Grid computing,
Application software"
Efficient authenticated encryption schemes with public verifiability,"An authenticated encryption scheme allows messages to be encrypted and authenticated simultaneously. C. Ma and K. Chen proposed such a scheme with public verifiability (see Electronics Letters, vol.39, no.3 p.281-2, 2003). That is, in their scheme, the receiver can efficiently prove to a third party that a message has indeed originated from a specific sender. We first identify two security weaknesses in the Ma-Chen authenticated encryption scheme. Then, based on the Schnorr signature, we proposed an efficient and secure improved scheme such that all the desired security requirements are satisfied.","Public key,
Information security,
Digital signatures,
Protocols,
Computer security,
Computer science,
Data security,
Authentication,
Public key cryptography,
Certification"
Operation tables for scheduling in the presence of incomplete bypassing,"Register bypassing is a powerful and widely used feature in modern processors to eliminate certain data hazards. Although complete bypassing is ideal for performance, bypassing has significant impact on cycle time, area, and power consumption of the processor. Due to the strict constraints on performance, cost and power consumption in embedded processors, architects need to evaluate and implement incomplete register bypassing mechanisms. However traditional data hazard detection and/or avoidance techniques used in retargetable schedulers breaks down in the presence of incomplete bypassing. We present the concept of operation tables, which can be used to detect data hazards, even in the presence of incomplete bypassing. Furthermore our technique integrates the detection of both data, as well as resource hazards, and can be easily employed in a compiler to generate better schedules. Our experimental results on the popular Intel XScale embedded processor platform show that even with a simple intra-basic block scheduling technique, we achieve up to 20% performance improvement over fully optimized GCC generated code on embedded applications from the MiBench suite.","Delay,
Processor scheduling,
Hazards,
Energy consumption,
Registers,
Program processors,
Permission,
Logic,
Embedded computing,
Computer science"
Human factors in extracting business rules from legacy systems,"Business rules are operational rules based on data that business organizations follow to perform various activities. Systems that implement business rules are updated when the organization change to meet new business needs or to follow the change of laws. Over time, the documentation of systems would become inconsistent with system behaviour. Thus software code becomes a more reliable source than any available documentation to obtain business rules. Currently, extracting business rules from legacy systems is heavily dependent on human interaction and steering. Though there are tools to assist the extraction, it is not a fully automated process. So it is very important to analyze the human factors, which have never been summarized or analyzed in this field. This work would analyze human factors in every procedure of extracting business rules from legacy systems. These human factors have been proved in extracting business rules from a financial legacy system.","Human factors,
Data mining,
Documentation,
Organizational aspects,
Sun,
Helium,
Educational institutions,
Computer science,
Costs,
Business process re-engineering"
Low-complexity high-performance GFSK receiver with carrier frequency offset correction,"This paper presents an implementation of a GFSK receiver, based on matched filtering of a sequence of K successive bits. This enables improved detection and superior BER performance but requires 2/sup K/ matched filters of considerable complexity. Exploiting redundancy by performing phase propagation of successive single-bit stages, we propose an efficient receiver implementation. Results presented highlight the benefits of the proposed method in terms of computational cost and performance compared to standard methods. We also address carrier frequency offset, and suggest a blind algorithm for its elimination. Performance results are exemplarily shown for a Bluetooth system.","Frequency,
Matched filters,
AWGN,
Filter bank,
Bluetooth,
Filtering,
Redundancy,
Computational efficiency,
Character generation,
Computer science"
"An efficient, secure and delegable micro-payment system","We propose a new efficient and secure micro-payment scheme, named e-coupons, which can provide the users the facility of delegating their spending capability to other users or their own devices like Laptop, PDA, Mobile Phone, and such service access points. The scheme has the promise of becoming an enabler for various Internet-based services involving unit-wise payment. We give flexibility to the users to manage their spending capability across various access points for a particular service without obtaining an authorization for each and every access point from a facilitating bank. This flexibility which is not present in the existing micro-payment schemes is essential for accessing ubiquitous e-services and other Internet-based applications. The facility of delegation introduces a slight overhead in respect of the proof or verification of the delegated authorization and security provided to the payments. The payoff from the facility of delegation takes away the burden of the overhead. We discuss the design of the protocol and provides a basic analysis of the performance of the system.","Web and internet services,
Security,
Costs,
Network servers,
Authorization,
Authentication,
Subscriptions,
Computer science,
Portable computers,
Personal digital assistants"
Video content representation on tiny devices,"The perceptual satisfaction of a user watching video on a tiny mobile device is constrained by the display capability and network bandwidth. To maximize the user's perceptual satisfaction in this constrained environment, we propose a new method to represent the video content adaptively in real-time on tiny devices according to the user's attention. First, a sampling based dynamic attention model is proposed to obtain and maintain the user's attention in the video streams. Second, based on the most attended regions and sequences extracted, the attention based representation is introduced to achieve a higher perceptual satisfaction on a small display. Experiments with users show the effectiveness of our proposed method in a video surveillance application.","Streaming media,
Sampling methods,
Computer displays,
Bandwidth,
Human factors,
Computer science,
Computer networks,
Mobile computing,
Video surveillance,
Handheld computers"
Bidding in P2P content distribution networks using the lightweight currency paradigm,"This paper presents a micropayment-based architecture for P2P content distribution networks using the lightweight currency protocol. Under this architecture, autonomous peers form a dynamic overlay network that evolves as peers buy and sell documents with the lightweight currency protocol (LCP). By adopting a payment-based system, member peers are forced to contribute resources in order to obtain benefits, thus eliminating the free-rider problem. We present a document search mechanism that is based on bidding, with the use of micropayments.","Intelligent networks,
Peer to peer computing,
Web services,
Computer architecture,
Computer science,
Resource management,
Costs,
Internet,
Access protocols,
Frequency"
A type-2 fuzzy embedded agent for ubiquitous computing environments,We describe a novel system for learning and adapting type-2 fuzzy controllers for intelligent agents that are embedded in ubiquitous computing environments (UCEs). Our type-2 agents operate non intrusively in an online life long learning manner to learn the user behaviour so as to control the UCE on the user's behalf. We have performed unique experiments in which the type-2 intelligent agent has learnt and adapted online to the user's behaviour during a stay of five days in the intelligent dormitory (iDorm) which is a real UCE test bed. We show how our type-2 agent deals with the uncertainty and imprecision present in UCEs to give a very good performance that outperform the type-1 fuzzy agents while using a smaller number of rules.,"Ubiquitous computing,
Uncertainty,
Fuzzy sets,
Intelligent agent,
Pervasive computing,
Embedded computing,
Ambient intelligence,
Computer science,
Fuzzy control,
Fuzzy systems"
Optimized slowdown in real-time task systems,"Slowdown factors determine the extent of slowdown a computing system can experience based on functional and performance requirements. Dynamic voltage scaling (DVS) of a processor based on slowdown factors can lead to considerable energy savings. We address the problem of computing slowdown factors for dynamically scheduled tasks with specified deadlines. We present an algorithm to compute a near optimal constant slowdown factor based on the bisection method. As a further generalization, for the case of tasks with varying power characteristics, we present the computation of near optimal slowdown factors as a solution to convex optimization problem using the ellipsoid method. The algorithms are practically fast and have the same time complexity as the algorithms to compute the feasibility of a task set. Our simulation results show on an average 20% energy gains over known slowdown techniques using static slowdown factors and 40% gains with dynamic slowdown.","Real time systems,
Energy consumption,
Processor scheduling,
Frequency,
Embedded computing,
Threshold voltage,
Embedded system,
Delay,
Computer science,
Dynamic voltage scaling"
Data partitioning with a realistic performance model of networks of heterogeneous computers,"Summary form only given. The article presents a performance model of a network of heterogeneous computers that takes account of the heterogeneity of memory structure and other architectural differences. Under this model, the speed of each processor is represented by a function of the size of the problem whereas standard models use single numbers to represent the speeds of the processors. We prove that this model is more realistic than the standard ones when the network includes computers with significantly different memory structure. We formulate a problem of partitioning of an n-element set over p heterogeneous processors using this advanced performance model and give its efficient solution of the complexity O(p/sup 2//spl times/log/sub 2/n).","Computer networks,
Distributed computing,
Computer science,
Educational institutions,
Parallel architectures,
Concurrent computing,
Design engineering,
Algorithm design and analysis,
Distributed processing"
Can online behavior unveil deceivers? - an exploratory investigation of deception in instant messaging,"Deception has been extensively studied in many disciplines in social science. With the increasing use of instant messaging (IM) in both informal communication and performing tasks in work place, deception in IM is emerging as an important issue. In this study, we aimed to explore the online behavior of deception in a group IM setting. The empirical results from triadic groups showed that two types of online behavior under investigation could significantly differentiate deceivers from truth-tellers. The findings can potentially broaden our knowledge of behavioral indicators of deception in human interaction and improve deception detection in cyberspace.","Humans,
Context,
Information systems,
Computer mediated communication,
Communications technology,
Face,
Employment,
Feedback,
Collaboration"
Predicting the end of an atrial fibrillation episode: the physionet challenge,"The PhysioNet Challenge 2004 addresses two differenr goals: to separate the persistent atrial $brillation (A F) from the paroxysmal AF (event I ) and, in case of paroxysmal AF, to identify the one-minute ECG strip just before the termination of the AF episode (event 2). Both events were approached through the separation of the atrial activity by the ventricular one in the ECG recordings (I-minute, two leads, 128 Hz). This separation was obtained through two different methods: a) QRST cancellation through cross-channel adaptive filtering; b) beat classification and class avernged beat subtraction. For event I , the averaged RR (index of ventricular uch'vity) was put into relationship with the Dominant Atrial Frequency (DAF) (index of atrial activity). A linear classifier was evaluated separating the RWDAF plane into the N-type and T-type regions. The best score was 95% on learning sets and 27/30 on testing set A. For event 2, once the S-type and T-type signals were joined for each patient using a QRST correlation method, sign9cative parameters were singled out in the DAFs during the penultimate and last two seconds of the S-rype and T-type recordings. Criteria based on rhe DAF trend of each signal in its last seconds and criteria based on the DAF comparison between S-rype and T-type signals were jointly used. The best score was 80% on learning sets and iBR0 on testing set B. 1.","Atrial fibrillation,
Electrocardiography,
Rhythm,
Testing,
Strips,
Cardiology,
Contracts,
Blood,
Physiology,
Computer science"
Sentiment classification using phrase patterns,"This paper presents a phrase pattern-based method in classifying sentiment orientation of text. That is to analyze whether the text expresses a favorable or unfavorable sentiment for a specific subject. In our method, we construct some phrase patterns and calculate their sentiment orientation by unsupervised learning algorithm. When we classify a document, we first add special tags to some words in the text, then match the tags within a sentence with some phrase patterns to get the sentiment orientation of the sentence. At last, we add up the sentiment orientation of each sentence. We classify the text according to this summation. The method achieves an accuracy rate of 86% when used to evaluate sports reviews from some Websites.","Natural language processing,
Computer science,
Unsupervised learning,
Pattern matching,
Feedback,
Information filtering,
Information filters,
Internet,
Information technology,
Text categorization"
The pulse protocol: sensor network routing and power saving,"We present a performance evaluation of the pulse protocol operating in a sensor network. In this work, a number of modifications are made to the original pulse protocol to provide efficient operation in a sensor network environment. The pulse protocol utilizes a periodic flood (the pulse) initiated by a single node (the pulse source) to provide both routing and synchronization to the network. This periodic pulse forms a pro-actively updated spanning tree rooted at the pulse source. Nodes communicate by forwarding packets through this tree. In addition, nodes are able to synchronize with the periodic pulse, allowing idle nodes to power off their radios a large percentage of the time when they are not required for packet forwarding. This results in substantial energy savings. A new mechanism called intermediate wake-up periods is introduced in this work in order to reduce the energy costs of low delay applications. Through simulation we explore the performance of both the protocol and the modifications with respect to energy efficiency and delay.","Routing protocols,
Delay,
Access protocols,
Costs,
Energy efficiency,
Energy consumption,
Timing,
Floods,
Kirk field collapse effect,
Computer science"
Security patterns: a method for constructing secure and efficient inter-company coordination systems,"As the Internet, intranets and other wide-area open networks grow, novel techniques for building distributed systems, notably mobile agents, are attracting increasing attention. This is particularly the case for inter-company system coordination applications. A key difficulty in constructing such systems is to meet the security requirements while at the same time respecting the requirements for efficient implementation. We propose a method that addresses this problem and show an application of the method to a real implemented system, the environmentally conscious product (ECP) design support system. Our approach enables developers to specify several candidate system behaviors that satisfy the security requirements. We use patterns for this purpose. Patterns are abstract templates of system behavior fragments. The patterns include agent migrations, communications between applications and security procedures. We model the performance data associated with each pattern. Developers can then select an efficient implementation using this model to compare the performance data of the candidates. We evaluate our approach with a significant real-world example, the ECP design support system that essentially requires inter-company system coordination.","Data security,
Communication system security,
Companies,
Informatics,
Mobile agents,
Product design,
Collaboration,
Joining processes,
National security,
Computer science"
An empirical evaluation of interval estimation for Markov decision processes,"This work takes an empirical approach to evaluating three model-based reinforcement-learning methods. All methods intend to speed the learning process by mixing exploitation of learned knowledge with exploration of possibly promising alternatives. We consider /spl epsi/-greedy exploration, which is computationally cheap and popular, but unfocused in its exploration effort; R-Max exploration, a simplification of an exploration scheme that comes with a theoretical guarantee of efficiency; and a well-grounded approach, model-based interval estimation, that better integrates exploration and exploitation. Our experiments indicate that effective exploration can result in dramatic improvements in the observed rate of learning.","Computer science,
State-space methods,
Arm,
Learning,
Convergence,
Mathematical model,
Artificial intelligence,
Sampling methods,
Pursuit algorithms"
Tool-supported unobtrusive evaluation of software engineering process conformance,"Software engineers face the dilemma of verifying process conformance and having the Hawthorne effect distorting the results or not doing it and as a consequence not being able to know if the proposed process is actually carried out. We addressed this issue by classifying and proposing the use of some approaches based on unobtrusive observation. These approaches include cognitive labs, remote monitoring and metrics collection. In addition, a tool supporting the application of perspective based reading (PER) for requirements inspection has been used and is also presented to exemplify metrics collection. Among other features, PBR Tool unobtrusively collects metrics in order to monitor if the reading technique is faithfully applied. Two studies evaluating the feasibility of such a tool are also reported.","Software engineering,
Bioreactors,
Productivity,
Remote monitoring,
Systems engineering and theory,
Computer science,
Inspection,
Fault detection,
Noise generators,
Signal to noise ratio"
An RFID based technique for handling object distribution and recalls in pervasive transaction environments,"A completely visible pervasive transaction environment where it is possible to link all related distributions of physical objects and trace their mobility through their entire life process, has been elusive. With the emergence of radio frequency identification (RFID) based object tags, it is now practicable to automatically collect information pertaining to the object's place, time, transaction, etc. We propose an architecture for pervasive real-time tracking of object distribution and subsequently present an efficient mechanism for performing recall of RFID tagged objects that were previously distributed but turned out defective. Applying the concepts of scale free networks and epidemic theory, we mathematically model the distribution and recall process and get a stochastic estimate of the average spread of object distributions and the number of recall messages required. Our simulation results validate the analysis and the small average number of recall messages obtained proves the efficacy of the scheme.","Radiofrequency identification,
RFID tags,
Stochastic processes,
Intelligent networks,
Computer science,
Mathematical model,
Analytical models,
Globalization,
Supply chains,
Supply chain management"
Towards synchronous collaborative software engineering,"CAISE, a collaborative software engineering architecture, provides extensible real-time support for collaboration between participating tools and users. The architecture maintains a semantic project model constructed incrementally from software artifacts as they are developed; this model is used to determine the impact of changes at a semantic level. This information is relayed to developers, providing them with awareness of others' locations, and alerting them to potential conflicts and the need for closer collaboration. We use examples from CAISE-based tools to illustrate the potential of real-time collaborative software engineering to enhance awareness of other developers' actions.","Collaborative software,
Software engineering,
Computer architecture,
Collaborative tools,
Collaborative work,
Maintenance engineering,
Software tools,
Java,
Visualization,
Computer science"
Fuzzy semantic labeling for image retrieval,The paper proposes a fuzzy image labeling method that assigns multiple semantic labels together with confidence measures to each region in an image. The confidence measures are derived from the distance of the region to hyperplanes constructed by support vector machines. Test results show that this method yields higher classification accuracy and retrieval precision than crisp labeling methods that are based on crisp classification,"Labeling,
Image retrieval,
Support vector machines,
Support vector machine classification,
Testing,
Neural networks,
Computer science,
Drives,
Nearest neighbor searches,
Partitioning algorithms"
ChipPower: an architecture-level leakage simulator,"Leakage power is projected to be one of the major challenges in future technology generations. The temperature profile, process variation, and transistor count all have strong impact on the leakage power distribution of a processor. We have built a simulator to estimate the dynamic/leakage power for a VLIW architecture considering dynamic temperature feedback and process variation. The framework is based on architecture similar to the Intel Itanium IA64 and is extended to simulate its power when implemented in 65nm technology. Our experimental results show that leakage power will become more than 50% of the power budget in 65nm technology. Moreover, without including the process variation, the total leakage power will be underestimated by as much as 30%.","Temperature,
Power generation,
Feedback,
Gate leakage,
Computational modeling,
Threshold voltage,
Clocks,
Subthreshold current,
Thermal management,
Computer science"
Facial event classification with task oriented dynamic Bayesian network,"Facial events include all activities of face and facial features in spatial or temporal space, such as facial expressions, face gesture, gaze and furrow happening, etc. Developing an automated system for facial event classification is always a challenging task due to the richness, ambiguity and dynamic nature of facial expressions. This paper presents an efficient approach to real-world facial event classification. By integrating dynamic Bayesian network (DBN) with a general-purpose facial behavior description language, a task-oriented stochastic and temporal framework is constructed to systematically represent and classify facial events of interest. Based on the task oriented DBN, we can spatially and temporally incorporate results from previous times and prior knowledge of the application domain. With the top-down inference, the system can make active selection among multiple visual channels to identify the most effective sensory channels to use. With the bottom-up inference from observed evidences, the current facial event can be classified with a desired confident level via the belief propagation. We applied the task-oriented DBN framework to monitoring driver vigilance. Experimental results demonstrate the feasibility and efficiency of our approach.","Bayesian methods,
Facial features,
Face detection,
Robustness,
Gold,
Face recognition,
Application software,
Infrared detectors,
Computer science,
Stochastic systems"
Real-time adaptive and trajectory-optimized manipulator motion planning,"While there has been a large body of literature addressing offline path planning for manipulators, there is relatively less study on real-time motion planning that occurs as a manipulator moves in an environment with unknown obstacles or unknown changes. This paper introduces a unified and general motion planning approach based on evolutionary computation that is suitable for both offline and real-lime adaptive motion planning for manipulators under various optimization criteria and manipulator constraints in environments with obstacles or changes not known a priori. The implementation and testing results demonstrate the effectiveness and efficiency of the approach.","Trajectory,
Path planning,
Motion planning,
Mobile robots,
Manipulator dynamics,
Computer science,
Sampling methods,
Roads,
Orbital robotics,
Constraint optimization"
RSCS: a parallel simplex algorithm for the Nimrod/O optimization toolset,"This paper describes a method of parallelisation of the popular Nelder-Mead simplex optimization algorithms that can lead to enhanced performance on parallel and distributed computing resources. A reducing set of simplex vertices are used to derive search directions generally closely aligned with the local gradient. When tested on a range of problems drawn from real-world applications in science and engineering, this reducing set concurrent simplex (RSCS) variant of the Nelder-Mead algorithm compared favourably with the original algorithm, and also with the inherently parallel multidirectional search algorithm (MDS). All algorithms were implemented and tested in a general-purpose, grid-enabled optimization toolset.","Optimization methods,
Distributed computing,
Design optimization,
Computer science,
Testing,
Design engineering,
Process design,
Software engineering,
Algorithm design and analysis,
Numerical simulation"
Efficient and robust query processing in dynamic environments using random walk techniques,"Many existing systems for sensor networks rely on state information stored in the nodes for proper operation (e.g., pointers to parent in a spanning tree, routing information, etc). In dynamic environments, such systems must adopt failure recovery mechanisms, which significantly increase the complexity and impact the overall performance. We investigate alternative schemes for query processing based on random walk techniques. The robustness of this approach under dynamics follows from the simplicity of the process, which only requires the connectivity of the neighborhood to keep moving. In addition we show that visiting a constant fraction of sensor network, say 80%, using a random walk is efficient in number of messages and sufficient for answering many interesting queries with high quality. Finally, the natural behavior of a random walk, also provide the important properties of load-balancing and scalability.","Robustness,
Query processing,
Sensor phenomena and characterization,
Sensor systems,
Computer science,
Routing,
Computer networks,
Distributed computing,
Permission,
Intelligent networks"
Concurrency control in mobile database systems,We propose an concurrency control mechanism (CCM) for mobile database systems (MDS) that ensures epsilon serializability and report its performance.,
Nonlinear instabilities and nonstationarity in human heart-rate variability,"The human cardiovascular system exhibits complex and interesting dynamics, including heart-rate fluctuations. The heart rate fluctuates irregularly - a phenomenon called heart-rate variability. Here, we present a short review of our experience with the analysis of heart-rate variability. Initially, we focused on the assessment of the risk of sudden cardiac death due to cardiac arrest. During this research, we became aware of intermittency in heart-rate variability. We identify the intermittency as type I and describe how simple models helped us understand the differences between our results and textbook properties of this phenomenon. We also mention the use of modified van der Pol oscillators for the modeling of heart-rate variability. Finally, we review our latest research on the modeling of the heart tissue's properties using the true shape of the ventricles reconstructed from clinical electro-physiological measurement data.","Humans,
Heart rate variability,
Time measurement,
Entropy,
Time series analysis,
Plasma measurements,
Cardiovascular system,
Fractals,
Pathology,
Particle measurements"
From aspectual requirements to proof obligations for aspect-oriented systems,"Aspect-oriented software development (AOSD) techniques support systematic modularization and composition of crosscutting concerns. Though AOSD techniques have been proposed to handle crosscutting concerns at various stages during the software life cycle, there is a traceability gap between the aspects at the requirements level and those at later development stages. It is not clear what proof obligations about an aspect-oriented implementation follow from the initial aspectual requirements. This work presents PROBE, a framework for generation of proof obligations for aspect-oriented systems from the initial aspectual requirements and associated trade-offs. The abstract proof obligations are expressed in standard linear temporal logic. Key components of the framework include an extended ontology with parametric temporal formulas and functions, and extensive treatment of conflicts among requirements. The resultant temporal logic assertions, grouped into specifications of aspect implementations, can then be instantiated in terms of the implementation and verification tools.","Programming,
Logic,
Java,
Design engineering,
Computer science,
Probes,
Ontologies,
Bridges,
Security,
Real time systems"
Robust feature matching across widely separated color images,We present a novel method for feature matching across widely separated color images. The proposed approach is robust and can support various correspondence based algorithms e.g. the recovery of epipolar geometry. Our algorithm extends an existing gray-scale corner detector to color. The feature matching algorithm robustly segments the area around the feature into significant color regions using the mean shift mode estimator. The recovered data structures are matched under all possible rotations and the best rotation and its corresponding matches are selected. The results of the matching algorithm are used for recovery of the epipolar geometry from wide base line stereo image pairs. The algorithm has been tested extensively yielding good results over a wide range of scenes and viewpoints. A small subset of these results is presented in the paper.,"Robustness,
Color,
Image segmentation,
Iterative algorithms,
Detectors,
Computer vision,
Geometry,
Stereo vision,
Testing,
Computer science"
Multidimensional integration: partition and conquer,"Understanding the behavior of particles subjected to forces is a basic theme in physics. The simplest system is a set of particles confined to motion along a line, but even this type of system presents computational challenges. For the harmonic oscillator, a particle is subjected to a force directed toward the origin and proportional to the distance between the particle and the origin. The resulting potential is V(x) = 1/2 /spl alpha/x/sup 2/ where /spl alpha/ is a constant. This system is quite thoroughly understood, and quantities of interest can be computed in closed form. Alternatively, the Ginzburg-Landau anharmonic potential, - 1/2 /spl alpha/x/sup 2/ + 1/4 /spl beta/x/sup 4/ (/spl alpha/ and /spl beta/ are constant), is related to solution of the Schrodinger equation, and quantities of interest are computed approximately. One method of obtaining such approximations is numerical integration, which is our focus in this assignment.","Multidimensional systems,
Polynomials,
Temperature,
Partitioning algorithms,
Integral equations,
Physics computing,
Investments,
Hypercubes,
Approximation methods"
Implicit concept-based image indexing and retrieval,"This paper focuses on implicit concept-based image indexing and retrieval (ICIIR), and the development of an improved method for the indexing and retrieval of images. The method involves the development of techniques to enable components of an image to be categorised on the basis of their relative importance with the image. Thus the storage of images involves an implicit, rather than an explicit, indexing scheme. Retrieval of images will then be effected by application of an algorithm based on the categorisation, which will allow relevant images to be identified and retrieved accurately and efficiently.",
Fisher kernel for tree structured data,"We introduce a kernel for structured data, which is an extension of the Fisher kernel used for sequences. In our approach, we extract the Fisher score vectors from a Bayesian network, specifically a hidden tree Markov model, which can be constructed starting from the training data. Experiments on a QSPR (quantitative structure-property relationship) analysis, where instances are naturally represented as trees, allow a first test of the approach.","Kernel,
Hidden Markov models,
Bayesian methods,
Human resource management,
Computer science,
Data mining,
Tree graphs,
Training data,
Testing,
Bioinformatics"
Multilevel plane wave time domain-based global boundary kernels for two-dimensional finite difference time domain simulations,"Time domain boundary integrals are used to impose global transparent boundary conditions in two-dimensional finite difference time domain solvers. Augmenting classical methods for imposing these conditions with the multilevel plane wave time domain scheme reduces the computational cost of enforcing a global transparent boundary condition from O(Ñs2 Ñt2) to O(ÑS Ñt log Ñs log Ñt); here Ñs and Ñt denote the number of equivalent source boundary nodes and their time samples used to integrate external fields, respectively. Numerical results demonstrate that for thin and concave material objects, plane wave time domain-accelerated global transparent boundary kernels outperform perfectly matched layer-based absorbing boundary schemes without loss of accuracy.","Time-domain analysis,
Finite difference methods,
Kernel,
Magnetic fields,
Boundary conditions,
Electromagnetic scattering"
Scheduling for VoIP service in cdma2000 1x EV-DO,"Recently cdma2000 1x EV-DO (HDR) system has begun to be deployed in some countries to support high data rate services in cellular networks. The system is originally designed to support data services, but now is expected to serve some real-time traffic including VoIP. For VoIP service with delay hound and low loss requirements, we propose a frame structure considering delay bound and a scheduling algorithm reflecting channel conditions. To schedule VoIP, we adopt the maximal rate algorithm and the proportionally fair algorithm. The proportionally fair algorithm (PF) was known to be appropriate for elastic-traffic, however, from simulation results, we conclude that the PF algorithm with the channel test is an appropriate scheduling scheme to provide QoS of VoIP. When the required slot portion of VoIP is 75%, the loss rate is about 1% on the average and 3% in the worse case. On the other hand, the maximal rate algorithm shows twice of the loss rate for the same delay bound and load. Additionally we propose a simple admission control scheme for VoIP service that controls the average portion of slots occupied by VoIP packets.","Scheduling algorithm,
Quality of service,
Downlink,
Land mobile radio cellular systems,
Real time systems,
Multiaccess communication,
Delay effects,
Processor scheduling,
Computer science,
Electronic mail"
Design and implementation of an extensible rule processing system for wearable computing,"In wearable computing environments, a user brings and uses his/her own computer to acquire various services wherever he/she goes. This type of computer, which acts as a service platform for wearable computing, needs autonomy and simplicity of services, flexibility of services/devices configuration, and power saving functions. Since most conventional wearable computing systems do not fulfil all of these requirements, we propose A-WEAR, which is a rule-based wearable computing system. We employ an event-driven rule as a behavior description language of A-WEAR to achieve autonomy and simplicity. Furthermore, we employ a plug-in mechanism to achieve flexibility and power saving. Using our system, we can easily provide and use various services for wearable computing environments.","Wearable computers,
Computer displays,
Power system management,
Portable computers,
Energy management,
Information science,
Watches,
Books,
Battery management systems,
Engines"
Balancing the tradeoffs between data accessibility and query delay in ad hoc networks,"In mobile ad hoc networks, nodes move freely and link/node failures are common. This leads to frequent network partitions, which may significantly degrade the performance of data access in ad hoc networks. When the network partition occurs, mobile nodes in one network are not able to access data hosted by nodes in other networks. In this paper, we deal with this problem by applying data replication techniques. Existing data replication solutions in both wired or wireless networks aim at either reducing the query delay or improving the data accessibility. As both metrics are important for mobile nodes, we propose schemes to balance the tradeoffs between data accessibility and query delay under different system settings and requirements. Simulation results show that the proposed schemes can achieve a balance between these two metrics and provide satisfying system performance.","Intelligent networks,
Ad hoc networks,
Delay,
Wireless networks,
Mobile ad hoc networks,
IP networks,
Mobile communication,
Computer science,
Electronic mail,
Degradation"
Using temporal logics of knowledge in the formal verification of security protocols,"Temporal logics of knowledge are useful for reasoning about situations where the knowledge of an agent or component is important, and where change in this knowledge may occur over time. Here we use temporal logics of knowledge to reason about security protocols. We show how to specify part of the Needham-Schroeder protocol using temporal logics of knowledge and prove various properties using a clausal resolution calculus for this logic.","Logic,
Authentication,
Cryptographic protocols,
Information security,
Public key,
Calculus,
Niobium,
Formal verification,
Computer security,
Computer science"
Gene ontology friendly biclustering of expression profiles,"The soundness of clustering in the analysis of gene expression profiles and gene function prediction is based on the hypothesis that genes with similar expression profiles may imply strong correlations with their functions in the biological activities. Gene ontology (GO) has become a well accepted standard in organizing gene function categories. Different gene function categories in GO can have very sophisticated relationships, such as 'part of' and 'overlapping'. Until now, no clustering algorithm can generate gene clusters within which the relationships can naturally reflect those of gene function categories in the GO hierarchy. The failure in resembling the relationships may reduce the confidence of clustering in gene function prediction. In this paper, we present a new clustering technique, smart hierarchical tendency preserving clustering (SMTP-clustering), based on a bicluster model, tendency preserving cluster (TP-Cluster). By directly incorporating gene ontology information into the clustering process, the SMTP-clustering algorithm yields a TP-cluster tree within which any subtree can be well mapped to a part of the GO hierarchy. Our experiments on yeast cell cycle data demonstrate that this method is efficient and effective in generating the biological relevant TP-clusters.","Ontologies,
Gene expression,
Clustering algorithms,
Computer science,
Data analysis,
Biology,
Organizing,
Fungi,
Cells (biology),
DNA"
On the estimation of rapidly time-varying channels,"Relying on the basis expansion model (BEM) for rapidly time-varying channels, we propose a novel training-based BEM channel estimation approach. While the existing approach applies a rectangular window to the received sequence and employs a BEM with a period equal to the window length, the novel approach applies a general window to the received sequence and employs a BEM with a period equal to a multiple of the window length. Simulation results show that these extensions can significantly improve the channel estimation performance.","Estimation,
Abstracts"
Region-based image retrieval using color coherence region vectors,"Color histograms are used to compare images in many applications. Their advantages are efficiency, and insensitivity to small changes in camera viewpoint. However, color histograms lack spatial information, so images with very different appearances can have similar histograms. This paper presents a new image retrieval algorithm based on color coherence region vectors (CCRV) image segmentation and its perceptual importance of different regions. Several features known to influence human visual attention are evaluated for each region of a segmented image to produce an importance value for region. Results shown indicate that images retrieval of perceptually important regions correlate well with human perception and the algorithm support region-based image retrieval.","Image retrieval,
Image segmentation,
Shape,
Humans,
Content based retrieval,
Histograms,
Computer science,
Application software,
Cameras,
Information retrieval"
A resilient network that can operate under duress: to support communication between government agencies during crisis situations,"The work in this paper is motivated by the weaknesses in communication networks that were observed among government agencies while responding to the emergency situation posed by the attack on the World Trade Center. The paper proposes a self-healing and self-managing architecture for supporting electronic communication between government agencies in crisis situations when the communication infrastructure is partially disabled. The architecture that we propose consists of independent services with standard interfaces and variable addresses. The services discover each other as required in real time by matching the standard interfaces. Disabled services are automatically pruned from the network and new services seamlessly replace the existing services at alternate network nodes. Complex operations can be performed using these services by integrating the services into existing workflows. The architecture allows for redundancy in the system as well as for requisitioning of additional services when the performance degrades due to a higher than normal load, which causes duress in the system. The paper presents the architecture for such a system as well as a model for simulating such a system under various scenarios of duress.","Government,
Computer networks,
Computer hacking,
Data security,
Computer security,
Information security,
Network servers,
Computer crime,
Business communication,
Communication networks"
A review of evolutionary multiobjective optimization applications in the area of production research,Evolutionary computation methods have been used extensively in the past for the solution of manufacturing optimization problems. This work examines the impact of the fast-growing evolutionary multiobjective optimization field in this area of research. A considerable number of significant applications are reported for a wide range of relevant optimization problems. The review of these applications leads to a number of conclusions and establishes directions for future research.,"Production,
Optimization methods,
Evolutionary computation,
Job shop scheduling,
Scheduling algorithm,
Application software,
Processor scheduling,
Robustness,
Computer science,
Educational institutions"
An electronic diary software for ecological momentary assessment (EMA) in clinical trials,"Experience sampling is a research technique in which participants record their behaviors and moods in real time in their day-to-day environments. Ecological momentary assessment (EMA) is a refinement of experience sampling in which randomly timed assessments are combined with event-contingent assessments. EMA data are more reliable if obtained with handheld computers than with paper diaries. Robust EMA applications sufficiently flexible and easily adaptable for our various research efforts, including a study of polydrug use we are now undertaking at our outpatient treatment/research clinic, were not available. We developed transactional electronic diary (TED), a highly configurable EMA program. TED provides investigators the capability to conduct close assessment and monitoring of symptoms and intensity of drug withdrawal in participants. Data collected via TED is subsequently incorporated into our human research information system (HuRIS) for further analysis.","Clinical trials,
Sampling methods,
Mood,
Handheld computers,
Robustness,
Application software,
Monitoring,
Drugs,
Humans,
Information systems"
Distinguishing mislabeled data from correctly labeled data in classifier design,"We have developed a method for distinguishing between correctly labeled and mislabeled data sampled from video sequences and used in the construction of a facial expression recognition classifier. The novelty of our approach lies in training a single, optimal classifier type (a support vector machine, or SVM) on multiple representations of the data, involving different ""discriminating"" subspaces. Results of a preliminary study on the discrimination of ""high stress"" vs. ""low stress"" facial expression data by this method confirms that our novel approach is able to distinguish subproblems where labeling is highly reliable from those where mislabeling can lead to high error rates. In helping detect data subsamples which yield misleading classification results, the method is also a rapid, highly efficient cross-validated approach for eliminating outliers.","Support vector machines,
Support vector machine classification,
Acoustic noise,
Labeling,
Speech,
Neural networks,
Voting,
Bagging,
Computer science,
Video sequences"
A computational interpretation of open induction,"We study the proof-theoretic and computational properties of open induction, a principle which is classically equivalent to Nash-Williams' minimal-bad-sequence argument and also to (countable) dependent choice (and hence contains full classical analysis). We show that, intuitionistically, open induction and dependent choice are quite different: Unlike dependent choice, open induction is closed under negative- and A-translation, and therefore proves the same /spl pi//sub 2//sup 0/-formulas (over not necessarily decidable, basic-predicates) with classical or intuitionistic arithmetic. Via modified realizability we obtain a new direct method for extracting programs from classical proofs of /spl pi//sub 2//sup 0/-formulas using open induction. We also show that the computational interpretation of classical countable choice given by S. Berardi et al. (1998) can be derived from our results.","Arithmetic,
Computer science,
Set theory,
Logic,
Topology"
Facilitating automated search for Web services,"Recent advances in the area of Web services have enabled inter-organization sharing of data and data-oriented software services. The challenges of developing software in a service-oriented development environment include search, retrieval, and integration of services with client applications. Such applications can be dynamic in nature and may vary depending on current availability of services or on the current relationship between client and service organizations. As such, applications must be able to quickly locate and integrate different potential service components. In this paper we describe an approach for automating the process of searching for Web services using signature matching and describe a new signature match criteria called the contains match.","Web services,
Application software,
Databases,
Data engineering,
Software reusability,
Computer science,
Availability,
Engineering profession,
Search engines,
Indexing"
LODS: locality-oriented dynamic scheduling for on-chip multiprocessors,"Current multiprocessor SoC applications like network protocol codes,multimedia processing and base-band telecom circuits have tight time-to-market and performance constraints, which require an efficient design cycle. Consequently, automated techniques such as those oriented towards exploiting data locality are critical. In this paper, we demonstrate that existing loop scheduling techniques provide performance improvements even on on-chip multiprocessors, hut they fall short of generating the hest results since they do not take data locality into account as an explicit optimization parameter. Based on this observation, we propose a data locality-oriented loop-scheduling algorithm. The idea is to assign loop iterations to processors in such a fashion that each processor makes maximum reuse of the data it accesses.","Dynamic scheduling,
Processor scheduling,
Optimizing compilers,
Programming profession,
Permission,
Computer science,
Design engineering,
Application software,
Protocols,
Telecommunications"
Adaptive RTS/CTS mechanism for IEEE 802.11 WLANs to achieve optimal performance,"In IEEE 802.11 WLANs, there are two types of access modes which perform very differently under different channel or traffic conditions: the basic access mode CSMA/CA and and an RTS/CTS based mechanisms. In seeking to balance between the use of the different modes, the contribution of this paper is threefold. Firstly, we present our development of a new method capable of calculating the access delay via an analytical model. Secondly, we report a comparison of the throughput and delay performance of basic and RTS/CTS mechanisms and the analysis of which mode should be chosen to obtain better throughput and delay performance under different conditions. Finally, we propose an adaptive RTS/CST mechanism that adjusts the RTS threshold adaptively in order to achieve optimal performance in a centralized or distributed way. Numerical results, and simulation results obtained using NS2 are provided.","Wireless LAN,
Delay,
Multiaccess communication,
Analytical models,
Throughput,
Computer science,
Performance analysis,
Numerical simulation,
Collision avoidance,
Space stations"
Enhancing broadcast operations in ad hoc networks with two-hop connected dominating sets,"We introduce the three-hop horizon pruning (THP) algorithm to make broadcast operations more efficient in ad hoc networks using contention-based MAC protocols. THP builds a two-hop connected dominating set (TCDS) of the network, which is a set of nodes such that every node in the network is within two hops from some node in the dominating set. Efficiency of broadcast operations is attained by implementing forwarding schemes that take advantage of a TCDS. More specifically, every node provides its one-hop neighbors with a list specifying one or more tuples, each with the identifier of a one-hop neighbor and a bit indicating if that neighbor dominates any two-hop neighbor. To forward a broadcast packet, a node tries to obtain the smallest subset of forwarders, which are one-hop neighbors that use some of the node's two-hop neighbors to reach any node that is three hops away. After such a selection of forwarders, the node broadcasts its packet with a header specifying the list of forwarders, and each forwarder in turn repeats the process.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Computer science,
Media Access Protocol,
Relays,
Routing,
Operating systems,
Topology,
Signal processing"
AVT-NBL: an algorithm for learning compact and accurate naive Bayes classifiers from attribute value taxonomies and data,"In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT) - hierarchical groupings of attribute values - to learn compact, comprehensible, and accurate classifiers from data - including data that are partially specified. This paper describes AVT-NBL, a natural generalization of the naive Bayes learner (NBL), for learning classifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples.","Taxonomy,
Data mining,
Ontologies,
Learning,
Robustness,
Artificial intelligence,
Laboratories,
Computer science,
Application software,
Training data"
An architecture for coordinating multiple self-management systems,"A common approach to adding self-management capabilities to a system is to provide one or more external control modules, whose responsibility is to monitor system behavior, and adapt the system at run time to achieve various goals (configure the system, improve performance, recover from faults, etc.). An important problem arises when there is more than one such self-management module: how can one make sure that they are composed to provide consistent and complementary benefits? In this paper we describe a solution that introduces a self-management coordination architecture and infrastructure to support such composition. We focus on the problem of coordinating self-configuring and self-healing capabilities, particularly with respect to global configuration and incremental repair. We illustrate the approach in the context of a self-managing video teleconference system that composes two preexisting adaptation modules to achieve synergistic benefits of both.","Control systems,
Conference management,
Computerized monitoring,
Automatic control,
Costs,
Computer architecture,
Computer science,
Teleconferencing,
Environmental management,
Condition monitoring"
Packet scheduling over a shared wireless link for heterogeneous classes of traffic,"We propose a quality of service (QoS) management framework and a packet scheduling scheme over a shared wireless link for heterogeneous classes of traffic. The QoS management framework is developed based on the characterization of different QoS requirements of heterogeneous traffic classes. The framework comprises two subsequent packet scheduling modules and load control modules. Based on the QoS management framework, we propose an effective packet scheduling scheme that accounts for the wireless channel, status of packet queues, the degree of QoS satisfaction, and fairness among sessions. Simulation results show that the proposed packet scheduling scheme provides better performance than the previously proposed schemes in light or moderate system load, and that it provides fair sharing of QoS degradation in overload.",
Investigations in applying metrics to multi-view architecture models,"The goal of our research is to develop industry-proof software architecture and design metrics. We identify a number of problems that arise in computing software architecture and design metrics in industrial settings that were not encountered in computing source-code metrics. These problems include the absence of a single, unifying representation for architectures and they arise from the fact that architecture diagrams are used in an informal manner. We describe our approach towards defining metrics for architectures and designs which are represented in the 4+1 views paradigm using UML. We report our experiences with architectural metrics in industrial settings.","Computer architecture,
Software architecture,
Computer industry,
Software design,
Costs,
Mathematical model,
Mathematics,
Computer science,
Unified modeling language,
Programming"
Quantifying the variance in application reliability,"A notable drawback of the existing architecture-based reliability assessment techniques is that they only obtain a point estimate of application reliability and do not attempt to quantify the variance in the estimate. The variance in the reliability estimate of an application represents the risk associated with the estimate. Ideally, the variance should be zero, but in practice it is inevitable due to the following two factors: (i) variances in the reliability estimates of components comprising the application, and (ii) architectural characteristics of the application. Quantifying the variance in the reliability estimate of an application provides an indication of the degree of risk associated with the estimate, and can also suggest an appropriate variance reduction strategy. We present a technique to quantify the variance in the reliability estimate of an application based on its architecture. Our technique generates analytical functions which express the mean and variance of application reliability in terms of the means and variances of the component reliabilities as well as the architectural characteristics of the application. Through a case study, we illustrate how the analytical functions generated using our technique could be used to evaluate the impact of individual components on the mean and the variance in the application reliability.","Application software,
Object oriented modeling,
Software systems,
Computer architecture,
Programming,
Analysis of variance,
Software reliability,
Computer science,
Reliability engineering,
Character generation"
Validating astrophysical simulation codes,"Astrophysical simulations model phenomena that can't be fully reproduced terrestrially. Validation then requires carefully devising feasible experiments with the relevant physics. The authors describe validating simulations against experiments that probe fluid instabilities, nuclear burning, and radiation transport, and then discuss insights from - and the limitations of - these tests.","Laboratories,
Discrete event simulation,
Computational modeling,
Testing,
Astrophysics,
Numerical models,
Numerical simulation,
Computational fluid dynamics,
Physics computing,
Aerospace simulation"
Systematic reviews,"During the past few years, I have become interested in evidence-based software engineering [5]. The evidence-based paradigm began in medicine and has totally revolutionised medical research [7]. It is now being adopted by a number of humancentered disciplines such as nursing, social policy, education etc. Although I have some reservations about whether software engineering can adopt fully the evidencebased paradigm, I am convinced that we should adopt certain aspects of the approach immediately. In particular, I believe we should adopt systematic review in place of ad hoc literature reviews, and recognise that a systematic review is a research method in its own right. Currently, all PhD students need to conduct a literature review as a part of their research but such reviews are seldom performed with the rigour now being required in other disciplines (including Information Systems Research, [6]). A systematic review is rigorous method of synthesising existing research related to a topic of interest. Quantitative meta-analysis is one form of systematic review, but it is not mandatory to provide a quantitative summary. Systematic reviews aim to provide a synthesis that is complete (with respect to defined criteria) and unbiased. Systematic reviews must be rigorous i.e. be conducted according to a well-defined procedure and open i.e. the research process must be reported in a manner is understandable to other researchers.",
Methods for interrupting a wearable computer user,"A wearable computer equipped with a head-mounted display allows its user to receive notifications and advice that is readily visible in her field of view. While needless interruption of the user should be avoided, there are times when the information is of such importance that it must demand the user's attention. As the user is mobile and likely interacts with the real world when these situations occur, it is important to know in what way the user can be notified without increasing her cognitive workload more than necessary. To investigate ways of presenting information without increasing the cognitive workload of the recipient, an experiment was performed testing different approaches. The experiment described in this paper is based on an existing study of interruption of people in human-computer interaction, but our focus is instead on finding out how this applies to wearable computer users engaged in real world tasks.",
How to verify dynamic properties of information systems,"EB/sup 3/ is an established formal technique, based on process algebra, for specifying Information Systems (IS) that have both complex state and event based features; as yet, EB/sup 3/ has no tool support. Another formal technique called CSP /spl par/ B uses two existing analysis tools, FDR and the B-Toolkit, to support the verification of state/event based systems. However the CSP /spl par/ B approach has never been applied to this specialised domain. In this paper we use a specification pattern of EB/sup 3/ to motivate a new style of specification in CSP /spl par/ B appropriate for IS. We demonstrate this using an example system and show that the verification of its dynamic properties is now amenable to tool support.","Information systems,
Libraries,
Computer science,
Algebra,
Data structures,
State-space methods,
Unified modeling language,
Formal specifications,
Specification languages,
Software engineering"
An investigation of an evolutionary approach to the opening of Go,"The game of Go can be divided into three stages; the opening, the middle, and the end game. In this paper, evolutionary neural networks, evolved via an evolutionary strategy, are used to develop opening game playing strategies for the game. Go is typically played on one of three different board sizes, i.e., 9/spl times/9, 13/spl times/13 and 19/spl times/19. A 19/spl times/19 board is the standard size for tournament play but 9/spl times/9 and 13/spl times/13 boards are usually used by less-experienced players or for faster games. This work focuses on the opening, using a 13/spl times/13 board. A feed forward neural network player is played against a static player (Gondo), for the first 30 moves. Then Gondo takes the part of both players to play out the remainder of the game. Two experiments are presented which indicate that learning is taking place.","Neural networks,
Computer science,
Feeds,
Feedforward neural networks,
Cognitive science,
Humans,
Hardware,
Algorithm design and analysis,
Databases,
Artificial neural networks"
Performance analysis of an improved tensor based correspondence algorithm for automatic 3D modeling,"3D modeling of a free-form object involves the acquisition of multiple views (range images) of the object to cover its entire surface. These views are then registered in a common coordinate basis by establishing correspondence between them. Our tensor based correspondence algorithm can automatically establish correspondence between overlapping view pairs and register them. In this paper we present an improved version of our tensor based automatic correspondence and registration algorithm and integrate it with a global registration and a surface reconstruction algorithm to make an efficient, automatic and complete 3D modeling framework. We also present a quantitative analysis of this automatic correspondence algorithm based on our results according to the following criteria: robustness to resolution, efficiency, robustness to the amount of overlap and noise.","Performance analysis,
Tensile stress,
Algorithm design and analysis,
Reconstruction algorithms,
Noise robustness,
Software algorithms,
Computer science,
Software engineering,
Australia,
Surface reconstruction"
Collaborative note taking,"Collaborative note taking enables students in a class to take notes on their PDAs and share them with their ""study group"" in real-time. Students receive instructor's slides on their PDAs as the instructor displays them. As the individual members of the group take notes pertaining to the slide being presented, their notes are automatically sent to all members of the group. In addition, to reduce their typing, students can use text they receive from other students and from the instructors slides to construct their notes. This system has been used in actual practice for a graduate level course on wireless mobile computing.","Collaboration,
Personal digital assistants,
Computer science,
Portable computers,
Displays,
Mobile computing,
Information management,
Organizing,
Educational technology,
Computer science education"
A robust packet scheduling algorithm for proportional delay differentiation services,"The proportional delay differentiation (PDD) model is an important approach for relative differentiated services provisioning on the Internet. It aims to maintain pre-specified packet queueing-delay ratios between different classes of traffic at each hop. Existing PDD packet scheduling algorithms are able to achieve the goal in long time-scales when the system is highly utilized. The paper presents a new PDD scheduling algorithm, called Little's average delay (LAD), based on a proof of Little's law. It monitors the arrival rate and the cumulative delays of the packets from each traffic class, and schedules the packets according to their transient queueing properties so as to achieve the desired class delay ratios in both short and long time-scales. Simulation results show that, in comparison with other PDD scheduling algorithms, LAD can provide no worse level of service quality in long time-scales and more accurate and robust control over the delay ratio in short time-scales. In particular, LAD outperforms its main competitors significantly when the desired delay ratio is large.","Scheduling algorithm,
Robustness,
Delay,
Diffserv networks,
Traffic control,
Web and internet services,
Robust control,
Quality of service,
Maintenance engineering,
Computer science"
Dissecting computer fraud: from definitional issues to a taxonomy,"Computer frauds, while less dramatic than crimes of violence, can inflict significant damage at community, organizational or individual level. In order to properly quantify and mitigate the risk, computer frauds needs to be well understood. In this paper, in a conceptual-analytical research approach, we propose a dissection of computer fraud. First, we look into the elements of an offense, the act of fraud in general then explain what is and what is not computer fraud. Next, from a prevention perspective, we propose a taxonomy of computer fraud with respect to perpetration platform, and to perpetration method. We believe that our contributions extend the existing knowledge of the phenomenon, and can assist those fighting computer fraud to better understand it and to design means of preventing and reporting it.","Computer crime,
Taxonomy,
Law,
Legal factors,
Uncertainty,
Costs"
Incorporating software process in an undergraduate software engineering curriculum: challenges and rewards,"Milwaukee School of Engineering has one of the first ABET-accredited software engineering (SE) programs in the United States. We describe our experiences in incorporating the core elements of the software engineering process throughout the undergraduate SE program. These elements are integrated vertically as well as horizontally throughout the curriculum, starting with an introductory process course in the sophomore year and culminating in a three-quarter software development laboratory course sequence and a two-quarter capstone project in the junior and senior years. The challenges encountered while using this approach are also discussed.","Software engineering,
Programming,
Laboratories,
Production,
Software maintenance,
Software quality,
Software systems,
Algorithm design and analysis,
Software algorithms,
Software architecture"
Measurement-based construction of locality-aware overlay networks,"One important aspect of constructing an overlay network is how to exploit network locality in the underlying network. In this paper, we propose a scalable protocol for constructing an overlay network that takes account of locality of network hosts. The constructed overlay network can significantly decrease the communication cost between end-hosts. Our simulation results show that the average distance between a pair of hosts in the constructed overlay network is only about 11% of the one in a traditional, randomly connected overlay network. Furthermore, our proposed overlay considered to be more scalable than tree-based or mesh-based overlays.","Costs,
Scalability,
Protocols,
Routing,
Telecommunication traffic,
Network topology,
Tree data structures,
Computer science,
Computer networks,
Peer to peer computing"
Grammar-based lossless universal refinement source coding,"A sequence y=(y/sub 1/,...,y/sub n/) is said to be a coarsening of a given finite-alphabet source sequence x=(x/sub 1/,...,x/sub n/) if, for some function /spl phi/, y/sub i/=/spl phi/(x/sub i/) (i=1,...,n). In lossless refinement source coding, it is assumed that the decoder already possesses a coarsening y of a given source sequence x. It is the job of the lossless refinement source encoder to furnish the decoder with a binary codeword B(x|y) which the decoder can employ in combination with y to obtain x. We present a natural grammar-based approach for finding the binary codeword B(x|y) in two steps. In the first step of the grammar-based approach, the encoder furnishes the decoder with O(/spl radic/nlog/sub 2/n) code bits at the beginning of B(x|y) which tell the decoder how to build a context-free grammar G/sub y/ which represents y. The encoder possesses a context-free grammar G/sub x/ which represents x; in the second step of the grammar-based approach, the encoder furnishes the decoder with code bits in the rest of B(x|y) which tell the decoder how to build G/sub x/ from G/sub y/. We prove that our grammar-based lossless refinement source coding scheme is universal in the sense that its maximal redundancy per sample is O(1/log/sub 2/n) for n source samples, with respect to any finite-state lossless refinement source coding scheme. As a by-product, we provide a useful notion of the conditional entropy H(G/sub x/|G/sub y/) of the grammar G/sub x/ given the grammar G/sub y/, which is approximately equal to the length of the codeword B(x|y).",
A hybrid approach for mining maximal hyperclique patterns,"A hyperclique pattern [H. Xiong et al. (2003)] is a new type of association pattern that contains items which are highly affiliated with each other. More specifically, the presence of an item in one transaction strongly implies the presence of every other item that belongs to the same hyperclique pattern. We present a new algorithm for mining maximal hyperclique patterns, which are desirable for pattern-based clustering methods [H. Xiong et al. (2004)]. This algorithm exploits key advantages of both the depth first search (DFS) strategy and the breadth first search (BFS) strategy. Indeed, we adapt the equivalence pruning method, one of the most efficient pruning methods of the DFS strategy, into the process of the BFS strategy. As demonstrated by our experimental results, the performance of our algorithm can be orders of magnitude faster than standard maximal frequent pattern mining algorithms, particularly at low levels of support.","Data mining,
Clustering algorithms,
Computer science,
Itemsets,
Association rules,
Clustering methods,
Probability,
Government,
Algorithm design and analysis,
Large-scale systems"
End-Users' Mental Models of Concepts Critical to Web Application Development,"We report an empirical study of nonprogrammers' mental models regarding particular concerns in Web application development such as input validation, database lookup, and overview-detail relationships. The goal of the study was to understand how nonprogrammers think about the data and logic underlying a Web application. In continuing work, we are using this understanding as a basis for the design of tools and development resources that are intuitive and easy to use. The current paper describes the empirical work that was done and discusses its implications for the design of end-user Web development tools that could be used to develop Web applications of intermediate complexity","Cognitive science,
Programming profession,
HTML,
Productivity,
Computer languages,
Information retrieval,
Image databases,
Functional programming,
Logic design,
Research and development management"
Popularity-wise proxy caching for interactive streaming media,"Most of the current proxy caching algorithms for streaming video media assume that users favor the beginning of the media object. However, this assumption is questionable in highly interactive scenarios, such as e-learning, where some parts of the video other than the prefix can also be popular. A new segment-based proxy caching algorithm, named popularity-wise caching, is proposed for highly interactive streaming. It is designed to deal with arbitrary popularity distribution of media content. Simulations are performed using synthetic traces with different kinds and levels of user interactivity. The results show that the performance of current segment-based caching (such as exponential caching and soccer caching) degrade with increasing user interactivity, while popularity-wise caching can provide the lowest user startup latency for interactive requests and highest bandwidth saving for the backbone network.","Streaming media,
Electronic learning,
Computer science,
Degradation,
Delay,
Bandwidth,
Spine,
Acceleration,
Australia Council,
Algorithm design and analysis"
The CERN CMS tracker control system,"Due to the high integration level of the experiments planned for the Large Hadron Collider (LHC) of the European Organization for Nuclear Research (CERN), the data acquisition and the control systems need complex developments both in hardware and software. The purpose of this paper is to describe the control system of a sub-detector of one of the CERN experiments, the tracker of the compact muon solenoid (CMS). The CMS tracker control system is based on dedicated hardware and software. The hardware is based on the front end controller (FEC), an interface board that hosts token rings for the communication with the control and communication unit (CCU) modules. These in turn contain dedicated I/O channels for the front end readout and control chips. The software is built in layers: one device driver, a C++ dedicated application program interface (API) plus a database for the storage of all the information needed for the front end electronics. This system will also be adopted in some other CMS sub-detectors.","Collision mitigation,
Control systems,
Communication system control,
Hardware,
Large Hadron Collider,
Data acquisition,
Mesons,
Solenoids,
Token networks,
Embedded software"
Behind the rules: XP experiences,"Agile processes such as XP (extreme programming) have been recognised for their potential benefits of improving software. During adoption of the XP process, teams can misapply the XP principles by following them verbatim, ignoring the context in which they are applied. In this paper we document our experiences where naive applications of XP principles were altered in recognition of context. We detail our observations of how teams ""looked behind"" the rules and began fitting XP to the problem rather than attempting to fit the problem to XP. We conclude by reflectively focusing on how this transformation occurred and suggest that it is buying into the XP ethos that drives this change of perspective on the XP process and principles.","Educational institutions,
Testing,
Computer science,
Application software,
Functional programming,
Computer bugs,
Programming profession,
Cost function,
Code standards,
Standards development"
Data Centric Cache Measurement on the Intel ltanium 2 Processor,"Processor speed continues to increase faster than the speed of access to main memory, making effective use of memory caches more important. Information about an application’s interaction with the cache is therefore critical to performance tuning. To be most useful, tools that measure this information should relate it to the source code level data structures in an application. We describe how to gather such information by using hardware performance counters to sample cache miss addresses, and present a new tool named Cache Scope that does this using the Intel Itanium 2 performance monitors. We present experimental results concerning Cache Scope’s accuracy and perturbation of cache behavior. We also describe a case study of using Cache Scope to tune two applications, achieving 24% and 19% reductions in running time.","Hardware,
Counting circuits,
Data structures,
Instruments,
Monitoring,
Logic,
Computer science,
Educational institutions,
Application software,
Software performance"
Efficient Bayesian methods for updating and storing uncertain search information for UAVs,"Algorithms for the autonomous decision and control of unmanned aerial vehicles (UAV's) require access to accurate information about the state of the environment in order to perform well. However, this information is oftentimes uncertain and dynamically changing. An efficient method to store and retrieve this information in such circumstances is provided in this paper. Bayesian methods are used to take probabilistic information about reports of object detections and to incorporate this information into an information base, which includes probabilities of the location and probabilities for an object's existence. This allows for the discrimination of false/real objects, with the easy extension into false/real targets. The resulting process is suited for cooperative search by a team of UAVs.","Bayesian methods,
Unmanned aerial vehicles,
Object detection,
Software algorithms,
Software agents,
Computer science,
Information retrieval,
Search problems,
Constraint theory,
Collision avoidance"
Perceptual digital watermark of images using ridgelet transform,"A perceptually based watermarking technique for image is proposed. It provides a new visual model, which can exactly evaluate the just noticeable distortion (JND) tolerance of human visual system. The watermarking algorithm is designed to address the issue of how to improve the robustness and the security of the embedded watermark according to the image content In order to improve the robustness and transparency, the method is first developed to determine significant ridgelet subbands and to select a couple of significant ridgelet coefficients in these subbands automatically, then the watermark bits are inserted in the visual significant ridgelet coefficients without exceeding the maximum strength of JND. With the spread of spectrum technique and the secret key protection, the security of the embedded watermark is also guaranteed very well. Experiments show that the proposed method can achieve a better tradeoff between the robustness and transparency.","Watermarking,
Robustness,
Wavelet transforms,
Image processing,
Visual system,
Information security,
Image coding,
Wavelet domain,
Lungs,
Computer science"
Local smoothing for manifold learning,"We propose methods for outlier handling and noise reduction using weighted local linear smoothing for a set of noisy points sampled from a nonlinear manifold. Weighted PCA is used as a building block for our methods and we suggest an iterative weight selection scheme for robust local linear fitting together with an outlier detection method based on minimal spanning trees to further improve robustness. We also develop an efficient and effective bias-reduction method to deal with the ""trim the peak and fill the valley"" phenomenon in local linear smoothing. Synthetic examples along with several image data sets are presented to show that manifold learning methods combined with weighted local linear smoothing give more accurate results.","Smoothing methods,
Noise reduction,
Computer vision,
Principal component analysis,
Learning systems,
Manifolds,
Computer science,
Application software,
Independent component analysis,
Nearest neighbor searches"
A novel distributed control architecture for shared protection,"The paper proposes a novel distributed control architecture for shared protection with reduced complete routing information, which aims to initiate a graceful compromise between the amount of link-state dissemination and the performance impairment due to the incompleteness of routing information. We first give explicit and comprehensive descriptions on a number of reported routing information dissemination scenarios for shared protection. A novel framework of link-state dissemination for facilitating shared protection, called reduced complete routing scenario, is introduced, in which the singular value decomposition (SVD) transformation is adopted to deal with the information reduction. We show, through simulation, that the proposed scheme can achieve a higher throughput and a better estimation in reconstruction of the spare provision matrix than the other schemes taking the same complexity of link-state dissemination.","Distributed control,
Protection,
Resource management,
Routing protocols,
Costs,
Scalability,
Information management,
Computer architecture,
Informatics,
Information science"
Cooperative multiagent systems for the optimization of urban traffic,"The risks and benefits of trusting others in a cooperative context is discussed and the notion of social rationality is used to establish these ideas in the realm of autonomous rational agents (utility maximizers). A traffic simulation is introduced to test the ideas presented in this work. The simulation consists of traffic lights controlled by cooperative autonomous agents, each of whom is given mechanisms to implement social rationality. Results using cooperative traffic agents are compared to results of control simulations where non-cooperative agents were deployed. Results predictably show a loss of local efficiency and a gain of global efficiency with the cooperative agents in comparison to their non-cooperative counterparts.","Multiagent systems,
Traffic control,
Testing,
Collaborative work,
Autonomous agents,
Lighting control,
Collaboration,
Computer science,
Niobium,
Computational modeling"
Virtual mission operation framework,"The virtual mission operation framework (VMOF) is one of the project lifecycle engineering process improvement efforts initiated by the Institutional Technology Infrastructure program at JPL. The VMOF is composed of three frameworks: a model integration framework, a simulation framework, and a visualization framework. The model integration framework interfaces with spacecraft system design, mission design, and structure design. The simulation framework interfaces with the operation scenario design, environmental phenomena science, and science payload system design. The visualization framework interfaces with the flight system testbed, the ground system, and the science analysis. The three frameworks of the VMOF collaborate to create a comprehensive virtual mission operation that enables a ""validation-in-the-loop"" system design process and lifecycle-continuous science-return validation. This paper discusses the technical approaches for each framework implementation, challenges and approaches involved in streamlining mission information access, and on-going activities toward enabling model-based engineering design in a collaborative distributed environment.",
FPGA architecture extensions for preemptive multitasking and hardware defragmentation,"The focus in This work is put onto extensions to typical FPGA hardware architectures in order to support some operating system functions. In particular, we examine the problem of preemptive multitasking and hardware defragmentation on a reconfigurable system based on FPGAs with a dynamically changing set of hardware tasks that can be replaced considering internal states.","Field programmable gate arrays,
Multitasking,
Hardware,
Registers,
Computer architecture,
Computer science,
Software systems,
Operating systems,
Logic devices,
Reconfigurable logic"
Vega Grid and CSCW: two approaches to collaborative computing,"This work looks at computer supported cooperative work from a grid computing viewpoint. We point out the trend of network computing and outline its implications and new CSCW requirements. We discuss key pieces of grid technology that are relevant to collaboration, focusing on the Vega Grid research work conducted at Institute of Computing Technology, Chinese Academy of Sciences. We also discuss an application example from China Railways.",
Underwater Robot Localization using Artificial Visual Landmarks,"Accurate localization for underwater robot is a big challenge in the mobile robot community. We use acoustic and visual-based approaches to solve the problem. In this paper, we focuses on the artificial landmark visual-based localization. The robot carries the camera to localize its position by calculating the camera's viewpoint through looking at the landmarks. The position of landmarks is known in a world-centered coordinate system (WCCS). A method is presented in this paper to recover the camera's viewpoint with minimum three feature points and a single image from one camera. Two steps are used in this paper: first step is to calculate the feature points' 3D coordinates in a camera centered coordinate system (CCCS); second step is to obtain a closed-form solution through the geometric transformations to map the 3D points from CCCS to WCCS. The algorithm is robust and efficient. It uses the fewest feature points required so far to deal with the same problem",
IP address handoff in the MANET,"When compared with a fixed host that is connected to a hardwired network, a mobile nude in the MANET may change its IP address more frequently due to the deployment of autoconfiguration, global connectivity, and hierarchical addressing schemes. When an IP address changes, the performance of unicast routing protocols and real-time communications may degrade, and privacy may be compromised within the MANET. Although there have been some autoconfiguration algorithms proposed for the assignment of unique IP addresses to mobile nodes, the overhead resulting from address changes has not been carefully examined. Based on studies of the overhead caused by address change, an IP address handoff solution, which extends the unicast routing protocol and network address translation (NAT) scheme, is proposed in the paper. The proposed approach is able to offset the overhead of broken routing fabrics and on-going communications, which is supported by our analysis and a prototype implementation.","Mobile ad hoc networks,
Corporate acquisitions,
Unicast,
Mobile communication,
Computer science,
Mobile computing,
Routing protocols,
Network address translation,
Space technology,
Local area networks"
Linear Response Algorithms for Approximate Inference in Graphical Models,"Belief propagation (BP) on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. However, it does not prescribe a way to compute joint distributions over pairs of distant nodes in the graph. In this article, we propose two new algorithms for approximating these pairwise probabilities, based on the linear response theorem. The first is a propagation algorithm that is shown to converge if BP converges to a stable fixed point. The second algorithm is based on matrix inversion. Applying these ideas to gaussian random fields, we derive a propagation algorithm for computing the inverse of a matrix.",
Pocket SCORM,"With advanced technologies, computer devices have become smaller in size but still with powerful computing ability. As a result, there are growing amount of people start to take distance education courses not only by using their PCs but also using mobile devices such as pocket PCs. Because pocket PCs are easy to carry, these devices are considered as a proper platform for distance education. We focus on the issues of transferring the current PC based sharable content object reference model (SCORM) to pocket PC based. We also introduce the prototype version of pocket SCORM run-time environment (RTE) which has been developed in our lab. Proposed pocket SCORM architecture is able to operate, even when the mobile device is physically disconnected from the network, without losing any students' learning record. Collected records are then be sent back to learning management system (LMS), runs at the sever side, after the device is back on on-line.","Portable computers,
Distance learning,
Personal communication networks,
Least squares approximation,
Courseware,
Industrial training,
Power engineering computing,
Computer industry,
Computer science education,
Power engineering education"
Particle swam optimization for image registration,"This paper discusses the particle swam optimization for image registration. The term particle swarm optimization (PSO) refers to a relatively new family of algorithms that may be used to find optimal (or near optimal) solutions to numerical and qualitative problems. It is easily implemented and has proven both very effective and quick when applied to a diverse set of optimization problems. During the past several years, PSO has been successfully applied to multidimensional optimization problems, artificial neural nework training, and multiobjective optimization problems. Presently PSO technique is used for registration, which is a fundamental task in image processing that, is used to match two or more pictures taken. In this we choose a point-mapping technique because it reduces the complexity of registration algorithms, increases the precision of the optimal transformation and permits to eliminate a big number of aberrant matches. A modified Particle Swarm Optimizer, which deals with permutation problems.","Image registration,
Particle swarm optimization,
Image processing,
Particle tracking,
Computer science,
Stochastic processes,
Birds,
Marine animals,
Educational institutions,
Multidimensional systems"
Event reconstruction in time warp,"In optimistic simulations, checkpointing techniques are often used to reduce the overhead caused by state saving. In this paper, we propose event reconstruction as a technique with which to reduce the overhead caused by event saving, and compare its memory consumption and execution time to the results obtained by dynamic checkpointing. As the name implies, event reconstruction reconstructs input events and anti-events from the differences between adjacent states, and does not save input events in the event queue. For simulations with fine event granularity and small state size, such as the logic simulation of VLSI circuitry, event reconstruction can yield an improvement in execution time as well as a significant reduction in memory utilization when compared to dynamic checkpointing. Moreover, this technique facilitates load migration because only the state queue needs to be moved from one processor to another.","Checkpointing,
Circuit simulation,
Very large scale integration,
Discrete event simulation,
Costs,
Logic circuits,
Time warp simulation,
Heuristic algorithms,
Frequency,
Computer science"
O(/spl radic/log n) approximation to SPARSEST CUT in O/spl tilde/(n/sup 2/) time,"We show how to compute O(/spl radic/log n)-approximations to SPARSEST CUT and BALANCED SEPARATOR problems in O/spl tilde/(n/sup 2/) time, thus improving upon the recent algorithm of Arora, Rao and Vazirani (2004). Their algorithm uses semidefinite programming and required O/spl tilde/(n/sup 4.5/) time. Our algorithm relies on efficiently finding expander flows in the graph and does not solve semidefinite programs. The existence of expander flows was also established by Arora, Rao, and Vazirani.","Approximation algorithms,
Partitioning algorithms,
Computer science,
Particle separators,
Clustering algorithms,
Algorithm design and analysis,
Routing,
NP-hard problem,
Graph theory,
Phase change random access memory"
Multi-dimensional quorum sets for read-few write-many replica control protocols,"We describe d-spaces, a replica control protocol defined in terms of quorum sets on multi-dimensional logical structures. Our work is motivated by asymmetrical access patterns, where the number of read accesses to data are dominant relative to update accesses, i.e. where the protocols should be read-few write-many. D-spaces are optimal with respect to quorum group sizes. The quality of the tradeoff between read efficiency and update availability is not matched by existing quorum protocols. We also propose a novel scheme for implementing d-spaces that combines caching and local information to provide a best-effort form of global views. This allows quorum reconfiguration to be lightweight without impacting access latencies, even when the rate of membership changes is very high.",
Adaptive cache-driven request distribution in clustered EJB systems,"This paper presents an algorithm for request distribution in clustered EJB systems. A classification for EJB requests is first introduced, on which request distribution is based. The objective is to achieve load balancing and to enhance the caching performance in the EJB containers. The algorithm is based on periodically collecting traffic statistics from the EJB containers, and then solving a constrained optimization problem that finds the best distribution strategies for each request type. An event-driven simulator is built for the performance evaluation, where the TPC-W benchmark is used as the workload. Simulation results show that for relatively small sizes of the containers' caches, the algorithm outperforms the random and round robin algorithms, currently used in clustered EJB systems. In addition, it scales better with the cluster size and can adapt to the varying load patterns.","Containers,
Round robin,
Clustering algorithms,
Load management,
Computer science,
Internet,
Web server,
Logic,
Databases,
Adaptive systems"
Processing dynamic PDEVS models,"Structural changes, i.e. the creation and deletion of components, and the change of interactions are salient features of adaptive systems. To model and specify these systems, variable structure models are required, i.e. models that entail in their own description the possibility to change their structure. To execute these models, a simulator with a clear semantic of intertwining structural and non-structural changes is required. In JAMES (Java-based agent modeling environment for simulation), different simulator components, e.g., for paced, unpaced, sequential, and parallel simulation, support the continuous use of models and simulation from specification to testing and a composition of the simulation engine on demand. Two types of simulator components for variable structure models are developed and integrated into the simulation layer; the implications are discussed.","Biological system modeling,
Software systems,
Computational modeling,
Computer simulation,
Analytical models,
Computer science,
System testing,
Context modeling,
Software design,
Telecommunication computing"
The Scientific Evaluation of Music Information Retrieval Systems: Foundations and Future,,
Testing object-oriented software,"The best approach to testing object-oriented software depends on many factors: the application-under-test, the development approach, the organization of the development and quality assurance teams, the criticality of the application, the development environment and the implementation language(s), the use of design and language features, project timing and resource constraints. Nonetheless, we can outline a general approach that works in stages from independent consideration of classes and their features to consideration of their interactions. A coherent strategy would include three main phases: intraclass, interclass, and system and acceptance testing.","Software testing,
Encapsulation,
System testing,
Object oriented modeling,
Computer science,
Concurrent computing,
Technology planning,
Software quality,
Quality assurance,
Application software"
Toward self-stabilizing operating systems,"This work presents several approaches for designing self-stabilizing operating systems. The first approach is based on periodical automatic reinstalling of the operating system and restart. The second, reinstalls the executable portion of the operating system and uses predicates on the operating system state (content of variables) to ensure that the operating system does not diverge from its specifications. The last approach presents an example of a tailored self-stabilizing very-tiny operating system. Prototypes using the Intel Pentium processor were composed.","Operating systems,
Robustness,
Application software,
Monitoring,
Computer errors,
Error correction,
Microprocessors,
Computer science,
Software prototyping,
Prototypes"
Trading Variance Reduction with Unbiasedness: The Regularized Subspace Information Criterion for Robust Model Selection in Kernel Regression,"A well-known result by Stein (1956) shows that in particular situations, biased estimators can yield better parameter estimates than their generally preferred unbiased counterparts. This letter follows the same spirit, as we will stabilize the unbiased generalization error estimates by regularization and finally obtain more robust model selection criteria for learning. We trade a small bias against a larger variance reduction, which has the beneficial effect of being more precise on a single training set. We focus on the subspace information criterion (SIC), which is an unbiased estimator of the expected generalization error measured by the reproducing kernel Hilbert space norm. SIC can be applied to the kernel regression, and it was shown in earlier experiments that a small regularization of SIC has a stabilization effect. However, it remained open how to appropriately determine the degree of regularization in SIC. In this article, we derive an unbiased estimator of the expected squared error, between SIC and the expected generalization error and propose determining the degree of regularization of SIC such that the estimator of the expected squared error is minimized. Computer simulations with artificial and real data sets illustrate that the proposed method works effectively for improving the precision of SIC, especially in the high-noise-level cases. We furthermore compare the proposed method to the original SIC, the cross-validation, and anempirical Bayesian method in ridge parameter selection, withgood results.",
Algorithmic fusion of gene expression profiling for diffuse large B-cell lymphoma outcome prediction,"Many different methods and techniques have been investigated for the processing and analysis of microarray gene expression profiling datasets. It is noted that the accuracy and reliability of the results are often dependent on the measurement approaches applied, and no single measurement so far is guaranteed to generate a satisfactory result. In this paper, an algorithmic fusion approach is presented for extracting genes that are predictive to clinical outcomes (survival-fatal) of diffuse large B-cell lymphoma on a set of microarray data for gene expression profiling. The approach integrates a set of measurements from different aspects in terms of the discrepancy indications and merit expectations of the gene expression patterns with respect to the clinical outcomes. A combination of statistical and nonstatistical criteria, continuous and discrete parameterizations, as well as model-based and modeless evaluations is applied in the approach. By integrating these measurements, a set of genes that are indicative to the clinical outcomes are better captured from the gene expression profiling dataset.","Gene expression,
Neoplasms,
Data mining,
Statistical analysis,
Fusion power generation,
Supervised learning,
Computer science,
Pathology,
Diseases,
Principal component analysis"
QoS support for USB 2.0 periodic and sporadic device requests,"As the universal serial bus (USB) 2.0 becomes a major driving force in the peripherals market now, the variety of USB devices and their request types also impose challenging issues on resource allocation of bus bandwidth. This research aims at the proposing of a USB-compliant system architecture and real-time scheduling algorithms for the resource allocation of USB 2.0 and 1.1 device requests jointly in a quality-of-service (QoS) fashion. Periodic requests, such as isochronous and interrupt transfers, are guaranteed with preservation of bus bandwidth and schedulability tests. Sporadic requests, such as control and bulk transfer, are serviced with probabilistic performance guarantees. The capability of this work is demonstrated with performance evaluations over a Linux system prototype, for which we have encouraging results.","Universal Serial Bus,
Job shop scheduling,
Real time systems,
Linux,
Bandwidth,
Resource management,
Scheduling algorithm,
Quality of service,
Testing,
Computer science"
Low static-power frequent-value data caches,"Static energy dissipation in cache memories will constitute an increasingly larger portion of total microprocessor energy dissipation due to nanoscale technology characteristics and the large size of on-chip caches. We propose to reduce the static energy dissipation of an on-chip data cache by taking advantage of the frequent values (FV) that widely exist in a data cache memory. The original FV-based low-power cache design aimed at only reducing dynamic power, at the cost of a 5% slowdown. We propose a better design that reduces both static and dynamic cache power, and that uses a circuit design that eliminates performance overhead. A designer can utilize our architecture by simulating an application and then synthesizing the FVs into an application-specific cache design when values will not change, or by simulating and then writing to an FV-cache with configuration registers when values could change. Furthermore, we describe hardware that can dynamically determine FVs and write to the configuration registers completely transparently. Experiments on 11 Spec 2000 benchmarks show that, in addition to the dynamic power savings, 33% static energy savings for data caches can be achieved.","Energy dissipation,
Cache memory,
Circuit synthesis,
Costs,
Circuit simulation,
Registers,
Energy consumption,
Computer science,
Power engineering and energy,
Embedded computing"
Image-based rendering and modeling in video-endoscopy,"We present algorithms for image-based rendering and modeling in video-endoscopy. Our algorithms are aimed at alleviating two common viewing problems in video-endoscopy: that the scope view is often distorted and oriented wrongly (a dis-orientation problem), and that the scope view is acquired from a viewpoint that deviates from the surgeon's (a dis-association problem). Our solutions strive to alleviate these problems to arrive at a more ""open-surgery-like"" and ""surgeon-centered"" display. We present experimental results based on real endoscopic video to illustrate our image enhancement procedures.",
A table presentation system for database and Web applications,"Report generation is arguably the most important task for database applications. It is especially important for e-commerce applications, since dynamic Web pages consisting of database reports are essential to almost every transaction. When a report is generated, the layout of the data sources must be converted to the layout of the report. This conversion is called data transformation. It has been shown that data transformation can be simplified by using table operations. In this paper, we propose a set of new extendable table operations. We show that these operations can be applied visually so as to support visual programming for data transformation. We also implement a visual application called table presentation system (TPS) to demonstrate how these operations are used. With the aid of TPS, one can create sophisticated database reports without writing data transformation programs. Therefore the complexity of data transformation is greatly reduced.","Web pages,
Transaction databases,
Application software,
Computer science,
Writing,
Visual databases,
HTML,
Civil engineering,
Chemical engineering,
Mathematics"
Grid Resource Specification Language based on XML and its usage in resource registry meta-service,"Resources are the basis of a grid, and their description is important for a grid user to discover and access resources. Grid resource describing is divided into two parts: resource description and resource specification. In this paper, an XML-based Grid Resource Specification Language is defined. A grid resource registry meta-service is then designed based on GRSL. A resource registry meta-service is implemented and tested. The test and tryout indicate that GRSL results in a powerful extensible framework for grid resource specification and discovery.","Specification languages,
XML,
Service oriented architecture,
Resource management,
Testing,
Application software,
Web services,
Helium,
Computer science,
Communication networks"
Semantics based verification and synthesis of BPEL4WS abstract processes,"We introduce a logic model to formally specify the semantics of workflows and their composite tasks described as BPEL4WS abstract processes. Based on the model, we present a set of inference rules to deduce the strongest postcondition and weakest precondition of a workflow and demonstrate that automatic workflow verification is possible due to the restrictions on data manipulation in an abstract process. We then sketch an algorithm that automatically synthesizes a workflow given its specification and a task library.","Web services,
Inference algorithms,
Computer science,
Libraries,
Computational modeling,
Art,
Logic,
Control system synthesis,
Workflow management software,
Disaster management"
Toward an integrated human-centered knowledge-based collaborative decision making system,"Collaboration and the use of knowledge in collaborative decision-making are fundamental to all organizational processes, and have been the subject of research in management and in decision and computer sciences for years. More recently, it has become necessary to integrate human-centered designs and groupwork practices with other aspects of collaborative systems to ensure higher levels of human involvement in task processes. Human-centered systems not only allow greater involvement, but require higher user friendliness, end-user satisfaction, learnability, and so on. In addition to the aforementioned advantages, human involvement enables the system to capture a wider context, create more knowledge and enhance its collaborative and decision making abilities. Therefore, it is necessary to develop a good balance and integration of the human aspects with enabling technologies for knowledge and decision making. This paper presents human-centered decision-making system (HUDS), an architecture for collaborative decision making in a knowledge based system with active human involvement. We discuss its architecture and concentrate on its main features, namely, decision making, knowledge management, and human computer interaction. We also present case studies that can be used to validate our architecture and to show how information and knowledge flow through the designed system. This architecture can be implemented in a variety of networked scenarios which form the basis of our future work and further development.",
Learning to play like a human: case injected genetic algorithms for strategic computer gaming,"We use case injected genetic algorithms to learn how to competently play computer strategy games. Strategic computer games involve long range planning across complex dynamics and imperfect knowledge presented to players requires them to anticipate opponent moves and adapt their strategies accordingly. This work addresses the problem of acquiring and using knowledge from human players for such games. Specifically, we learn general routing information from a human player and use case-injected genetic algorithms to incorporate this acquired knowledge in subsequent planning. Results from a strike planning game show that with an appropriate representation, case injection effectively biases the genetic algorithm toward producing plans that contain important strategic elements used by human players.","Humans,
Computer aided software engineering,
Genetic algorithms,
Game theory,
Asset management,
Strategic planning,
Resource management,
Fatigue,
Computer science,
Routing"
ATXOP: an adaptive TXOP based on the data rate to guarantee fairness for IEEE 802.11e wireless LANs,"The IEEE 802.11e is an extension of the IEEE 802.11 to provide quality of service (QoS) to applications requiring real time services. However, when the IEEE 802.11e WLANs are used with the multi-rate scheme, they suffer an unfairness problem. In this paper, we propose a scheme called adaptive transmission opportunity (ATXOP) to solve the unfairness problem in the IEEE 802.11e networks adopting the multi-rate scheme. Our simulation results show that the proposed ATXOP scheme provides better fairness.","Wireless LAN,
Local area networks,
Quality of service,
Uninterruptible power systems,
Sections,
Media Access Protocol,
Computer science,
Application software,
Internet telephony,
Access protocols"
Continuous speech recognition by adaptive temporal radial basis function,"This work presents the adaptive temporal radial basis function ""ATRBF"" applied to continuous speech recognition, in particular the recognition of phonemes. ATRBF combines features from time delay neural network ""TDNN"" and the advantages of radial basis function ""RBF"". The capacity to detect the acoustic features and their independent temporal report of the temporal localisation is inspired from the TDNN model. The main use of radial basis functions is both their speed of treatment and few parameters to adjust for the training phase, which encourages applying this model to new tasks in most delicate cases. The algorithm automatically selects the significant RBF centres and estimates the weights and delay at the same time. The adaptability is obviously when applying this approach in speech recognition, especially for phoneme recognition. This resemble to what would make a human brain in like situation.","Speech recognition,
Delay effects,
Radial basis function networks,
Computer science,
Automatic speech recognition,
Computer vision,
Shape,
Biological neural networks,
Humans,
Multilayer perceptrons"
SPIDER: software for protein identification from sequence tags with de novo sequencing error,"For the identification of novel proteins using MS/MS, de novo sequencing software computes one or several possible amino acid sequences (called sequence tags) for each MS/MS spectrum. Those tags are then used to match, accounting amino acid mutations, the sequences in a protein database. If the de novo sequencing gives correct tags, the homologs of the proteins can be identified by this approach and software such as MS-BLAST is available for the matching. However, de novo sequencing very often gives only partially correct tags. The most common error is that a segment of amino acids is replaced by another segment with approximately the same masses. We developed a new efficient algorithm to match sequence tags with errors to database sequences for the purpose of protein and peptide identification. A software package, SPIDER, was developed and made available on Internet for free public use. This work describes the algorithms and features of the SPIDER software.","Proteins,
Technical Activities Guide -TAG,
Sequences,
Peptides,
Amino acids,
Computer errors,
Spatial databases,
Software packages,
Mass spectroscopy,
Computer science"
Replicating Web applications on-demand,"Many Web-based commercial services deliver their content using Web applications that generate pages dynamically based on user profiles, request parameters etc. The workload of these applications are often characterized by a large number of unique requests and a significant fraction of data updates. Hosting these applications drives the need for systems that replicates both the application code and its underlying data. We propose the design of such a system that is based on on-demand replication, where data units are replicated only to servers that access them often. This reduces the consistency overhead as updates are sent to a reduced number of servers. The proposed system allows complete replication transparency to the application, thereby allowing developers to build applications unaware of the underlying data replication. We show that the proposed techniques can reduce the client response time by a factor of 5 in comparison to existing techniques for a real-world e-commerce application used in the TPC-W benchmark. Furthermore, we evaluate our strategies for a wide range of workloads and show that on-demand replication performs better than centralized and fully replicated systems by reducing the average latency of read/write data accesses as well as the amount of bandwidth utilized to maintain data consistency.","Delay,
Network servers,
Application software,
Databases,
Computer science,
Performance evaluation,
Bandwidth,
Books,
Web and internet services,
Web pages"
Quantifying non-functional requirements: a process oriented approach,"In this work, we propose a framework for quantifying non-functional requirements (NFRs). This framework uses quality characteristics of the execution domain, application domain and component architectures to refine qualitative requirements into quantifiable ones. Conflicts are resolved during the refinement process and more concrete non-functional requirements are produced.","Computer science,
Statistics,
Hardware,
Security,
Software engineering,
Systems engineering and theory"
Non-vanishing basin of attraction with respect to a parametric variation and center manifold,"When the origin of a nonlinear system having a parameter is locally asymptotically stable, the size of its basin of attraction may also depend on the parameter. If the parameter variation is confined within a compact interval and if the origin is locally exponentially stable for each parameter, then it is well-known that the radius of basin of attraction does not shrink to zero under the variation of parameter. We relax this condition up to the case where the origin loses exponential stability for a certain parameter. This is done by focusing on the behavior of the system on a parametrized center manifold. In addition, by introducing the concept of stability with respect to a positively invariant subset of the state-space, the proposed analysis is applicable to the case where the system experiences bifurcation of splitting or merging equilibria, which often appears in biological systems. The stability property considered here is called for with the boundary layer system in the singular perturbation context, or with the system having slowly varying inputs. The main results establish that the non-vanishing basin of attraction (relative to a positively invariant set) is guaranteed by confirming the similar property just for the reduced order system on a parametrized center manifold.","Nonlinear systems,
Stability analysis,
Bifurcation,
Merging,
Biological systems,
Reduced order systems,
Asymptotic stability,
Computer science,
Manifolds"
WebGPSS: the first two hours of simulation education,In this paper we present seven short lessons used for introducing management science students to discrete event simulation. It has been used both as the only element of such simulation in courses that devote only two classroom hours to this topic and as the introduction in courses that are devoted almost completely to simulation.,"Educational programs,
Logic,
Educational institutions"
Efficient secure multicast with well-populated multicast key trees,"Secure group communications is the basis for many recent multimedia and Web technologies. In order to maintain secure and efficient communications within a dynamic group, it is essential that the generation and management of group key(s) be secure and efficient with realtime response. Typically, a logical key hierarchy is used for distribution of group keys to users so that whenever users leave or join the group, new keys are generated and distributed using the key hierarchy. In this paper, we propose well-populated multicast key tree (WPMKT), an efficient technique to handle group dynamics in the key tree and maintain the tree balanced with minimal cost. In WPKT, subtrees are swapped in a way that keeps the key tree balanced and well populated. A t the same time, rekeying overhead due to reorganization is kept at a minimum. Another advantage of WPKT is that rebalancing has no effect on the internal key structure of the swapped subtrees. Results from simulation studies show that under random user deletion, our approach achieves one order of magnitude in overhead less than existing approaches. Under clustered sequential user deletion, our approach achieves almost a linear growth with tree size under individual rebalancing. For periodic rebalancing, we achieved almost half the overhead introduced by other approaches.","Data security,
Cryptography,
Computer science,
Costs,
Command and control systems,
Technology management,
Authentication,
Clustering algorithms"
Array decomposition-fast multipole method for finite array analysis,"An innovative approach is presented for analyzing finite arrays of regularly spaced elements. We review the recently proposed Array Decomposition Method, which exploits the block-Toeplitz property of regularly spaced arrays for significant storage reduction. To further reduce storage, in this paper we incorporate a multipole expansion to treat distant element interactions. The suggested approach overcomes the matrix storage bottleneck associated with integral equation methods, resulting in fixed and minimal matrix storage for any sized array (on the same order as the storage of a single array element). Hence, fast and rigorous analysis of very large finite arrays can be accomplished with limited resources.","Finite element analysis,
Antenna arrays,
Matrix decomposition,
Integral equations,
Electromagnetics,
Electromagnetic scattering,
Memory management"
An implementation of self-protected mobile agents,"Traditional approaches to mobile agent code protection rely on platform-based cryptographic services, often based on PKI solutions. In this paper, we discuss some of their shortcomings, and propose a new architecture for secure mobile agents that addresses two outstanding issues: inter-platform portability and agent code protection. Existing solutions often introduce heavyweight frameworks, requiring a major reengineering of legacy systems and severely impeding portability and code reuse. We show how preexisting, as well as new, agent systems can be robustly secured against a wide variety of external attacks, while minimising the impact on the code base of both the agent and its host platform(s). The paper closes with a discussion of our implementation of the proposed mechanisms, as an extension to the well-known JADE platform.","Mobile agents,
Protection,
Application software,
Computer architecture,
Cryptography,
Information security,
Computer science,
Impedance,
Robustness,
Paper technology"
ARDSR: an anycast routing protocol for mobile ad hoc network,"Anycast service is an effective implement method of distributed services in ad hoc network. It can simplify the configuration and management of distributed services. It can further improve the robustness of ad hoc networks to use anycast services in highly dynamic topology environment. According to the features of ad hoc network, a new anycast routing protocol ARDSR is proposed in this paper. This protocol has a lot of advantages such as balancing traffic load, conserving network bandwidth and saving host energy. It accomplishes an effective anycast routing in dynamic ad hoc networks. The simulation results show that the protocol can get good performance in dynamic network environment.","Routing protocols,
Mobile ad hoc networks,
Ad hoc networks,
Network topology,
Multicast protocols,
Network servers,
Radio propagation,
Bandwidth,
Unicast,
Computer science"
A novel direction chain code-based image retrieval,"The direction chain code has been widely used to encode the boundary lines for its simplicity and low storage requirement. One problem with the chain code is that it lacks of robustness to the rotation and scaling of the image content, which limited its use in image retrieval. In this paper, we propose a novel shape descriptor called minimize sum statistical direction code (MSSDC), which has the advantages of being invariant to the position and rotation of the image content. It is proportional to the image scaling. A new distance measure called direction entropy (DE) is presented to measure the similarity of shape information embedded in the MSSDC. Experiment demonstrates the efficiency of our proposed algorithm.","Image retrieval,
Shape measurement,
Clocks,
Content based retrieval,
Entropy,
Computer science,
Electronic mail,
Robustness,
Humans,
Histograms"
"An application of heterogeneous agents to fabricate large, realistic corporate transaction data sets for data mining tool testing and evaluation",,"Data mining,
Testing,
Decision making,
Algorithm design and analysis,
Multiagent systems,
Demography,
Planning,
Permission,
Application software,
Computer science"
A model for evaluating learning objects,"The growing international interest in the field of reusable learning objects suggests that the learning objects' approach is an innovative one, leveraging the development and deployment of e-learning content in new and interesting ways. Although there are numerous development projects based on the learning objects' approach, there are few studies that have provided guidelines for determining return on investment on learning objects. We believe that without such studies it is difficult to determine the usability and effectiveness of learning objects, and so the current huge amount of development effort will not scale. To deal with this shortcoming, we propose a new model for evaluating learning objects. In this model, four major aspects of learning objects are evaluated: content design, back-end delivery, front-end presentation, and the learning process itself. We argue that these aspects are closely linked, and show how each one of them plays an important role in the development life cycle of a learning object.","Object oriented modeling,
Computer science,
Usability,
Electronic learning,
Guidelines,
Investments,
Educational technology,
Educational institutions,
Software tools,
Building materials"
Privacy-preserving association rule mining in large-scale distributed systems,"Data privacy is a major concern that threatens the widespread deployment of data Grids in domains such as health-care and finance. We propose a unique approach for obtaining knowledge, by way of a data mining model, from a data Grid, while ensuring that the data is cryptographically safe. This is made possible by an innovative, yet natural generalization for the accepted trusted third party model and a new privacy-preserving data mining algorithm that is suitable for Grid-scale systems. The algorithm is asynchronous, involves no global communication patterns, and dynamically adjusts to changes in the data or to the failure and recovery of resources. To the best of our knowledge, this is the first privacy-preserving mining algorithm to possess these features. Simulations of thousands of resources prove that our algorithm quickly converges to the correct result while using reasonable communication. The simulations also prove that the effect of the privacy parameter on both the convergence time and the number of messages, is logarithmic.","Association rules,
Data mining,
Large-scale systems,
Statistics,
Data privacy,
Databases,
Clustering algorithms,
Grid computing,
Radio access networks,
Computer science"
The influence of QoS routing on the achievable capacity in TDMA-based ad hoc wireless networks,"The issue of providing QoS guarantees in an ad hoc wireless network is a challenging problem. Irrespective of the nature of the routing and reservation protocol used in the QoS scheme, there is an inherent limitation on the kind of QoS guarantees that can be provided. Unlike existing studies which analyze the transport capacity, we focus on the achievable capacity. The framework that we assume is that of a TDMA-based network. In this paper, we investigate the achievable capacity and the influence of routing protocols on it. The metrics that we consider are the call acceptance probability and the system saturation probability. We derive general bounds for the case of multiple-classes of users in the network. These bounds indicate the number of calls of the highest priority class that can be admitted into the network. Simulation studies were performed to study the effect of load, hopcount, and the routing protocol on the call acceptance. The increase of the call acceptance with the introduction of load-balancing highlights the importance of load-balancing in enhancing the system performance.","Intelligent networks,
Wireless networks,
Routing protocols,
Telecommunication traffic,
Bandwidth,
Markov processes,
Computer science,
System performance,
Estimation theory,
Capacity planning"
FolkMusic: a mobile peer-to-peer entertainment system,"In this paper we present the design of FolkMusic, a mobile peer-to-peer entertainment system. The work reported in this paper builds on the current trends towards: 1) edutainment software, 2) increase in use of peer-to-peer technologies, and, 3) the current trend towards mobile computing solutions. Further on, the research reported in this paper builds on prior research on ""folk computing"" for which mobile ad-hoc peer-to-peer solutions are a focal concern. Overall, this paper contributes to research on MANETs (Mobile Ad-hoc Networks) and peer-to-peer (P2P) computing by illustrating how mobile peer-to-peer technologies can be applied to both the area of entertainment as well as for educational purposes. Further on, the FolkMusic system contributes to current technical efforts made on ad-hoc networks by providing a good example of how Internet technologies and similar stationary and stable infrastructures can be combined with highly dynamic and wireless networks.","Peer to peer computing,
Mobile computing,
Cities and towns,
Ad hoc networks,
Legged locomotion,
Sprites (computer),
Informatics,
Computer networks,
Educational technology,
IP networks"
Calibration of active antenna arrays using a sky brightness model,Low-frequency arrays (120-400 MHz) can be calibrated using a sky model based on sky maps. For a single antenna the variation of measured power with the Earth's rotation is needed to separate receiver gain and noise temperature. For an array an instantaneous calibration can be made using normalized correlations or normalized beam power for which the receiver gain cancels out. Examples of the sky model for various antenna orientations are shown along with measured data used to perform a calibration. The effects of unmodeled mutual coupling of the received signals and receiver noise on the calibration method are analyzed and are found to result in errors of only a few percent in an array of 5 × 5 crossed dipoles designed to observe the deuterium line at 327 MHz.,"Antenna arrays,
Calibration,
Antenna measurements,
Temperature measurement,
Dipole antennas,
Brightness"
Virtual watersheds: simulating the water balance of the Rio Grande Basin,"Managers of water resources in arid and semi-arid regions must allocate increasingly variable surface water supplies and limited groundwater resources. This challenge is leading to a new generation of detailed computational models that can link multiple sources to a wide range of demands. Detailed computational models of complex natural-human systems can help decision makers allocate scarce natural resources such as water. This article describes a virtual watershed model, the Los Alamos Distributed Hydrologic System (LADHS), which contains the essential physics of all elements of a regional hydrosphere and allows feedback between them. Unlike real watersheds, researchers can perform experiments on virtual watersheds, produce them relatively cheaply (once a modeling framework is established), and run them faster than real time. Furthermore, physics-based virtual watersheds do not require extensive tuning and are flexible enough to accommodate novel boundary conditions such as land-use change or increased climate variability. Essentially, virtual watersheds help resource managers evaluate the risks of alternatives once uncertainties have been quantified.","Atmospheric modeling,
Computational modeling,
Water resources,
Resource management,
Application software,
Spatial resolution,
Physics,
Feedback,
Boundary conditions,
Plasma welding"
Condensation tracking through a Hough space,"Recent work has shown tracking groups of lines through the parameter space represented by a Hough accumulator array to be efficient and insensitive to both occlusion and changes in illumination. Previous methods have, however, been less robust in the presence of distractions are a challenge to trackers of all types, but the problem is heightened in the Hough-based approach as short collinear segments give the same response as an equal length connected line. We describe the application of the condensation algorithm to tracking peaks through a Hough accumulator array, producing a tracking technique that is efficient and robust under illumination changes, occlusion and distractions.","Lighting,
Robustness,
Motion estimation,
Image sequences,
Optical computing,
Milling machines,
Computer science,
Image segmentation,
Computer vision,
Object detection"
Sparse representation from a winner-take-all neural network,"We introduce an incremental algorithm for independent component analysis (ICA) based on maximization of sparseness criteria. We propose using a new sparseness measure criteria function. The learning algorithm based on this criteria leads to a winner-take-all learning mechanism. It avoids the optimization of high order nonlinear function or density estimation, which have been used by other ICA methods. We show that when the latent independent random variables are super-Gaussian distributions, the network efficiently extracts the independent components.","Neural networks,
Independent component analysis,
Vectors,
Convergence,
Optimization methods,
Data mining,
Maximum likelihood estimation,
Machine learning algorithms,
Computer science,
Electronic mail"
Some effects of system information in instructions for use,"An experiment was carried out to investigate whether it is useful to add system information to procedural information in instructional text. It was assumed that readers of instructions construct both a procedural and a system mental model, and that the latter enables the readers to infer possible missing information in procedural instructions. Moreover, it was assumed that system information would increase the cognitive load during reading and practicing, and that it would affect the appreciation of the instructions as well as the self-efficacy of the reader. The participants in the experiment read instructions and practiced with a fictitious machine before performing a number of tasks and answering a questionnaire. The results indicate that system information increased the cognitive load during reading and decreased self-efficacy, while the instructional text with system information was judged as more difficult. The effect on performance is limited: system information leads to faster performance for correctly completed tasks.","Cognitive science,
Internal stresses,
Spreadsheet programs,
Manuals,
Mice"
Peer learning with Lego Mindstorms,"Reciprocal peer learning involves students learning from, and with, each other. This paper details a peer learning centred course where small teams of students design and develop a multifunctional robot using Lego Mindstorms/spl trade/. In particular, it describes how students were introduced to the concept of peer learning and outlines how the learning environment was managed and sustained. Particular emphasis is placed on acknowledging and rewarding peer collaboration as part of the assessment procedures, thus encouraging active student engagement with the peer learning process.","Computer science,
Educational robots,
Environmental management,
Collaboration,
Application software,
Collaborative work,
Peer to peer computing,
Computer applications,
Fellows,
Welding"
PARADE: parametric delay evaluation under process variation [IC modeling],"Under manufacturing process variation, the circuit delay varies with process parameters. For delay test and timing verification under process variation, it is necessary to model the variational delay as a function of process variables. However, conventional methods to generate such functions are either slow or inaccurate. In this paper, we present a number of new methods for fast parametric delay evaluation under process variation. Our methods are either based on explicit delay formulae or based on characterized lookup tables, and are significantly faster than conventional methods of comparable accuracy. Due to the efficiency of our method, we can accurately model any path delay as a function of multiple interconnect and device process variables in large circuits. Experimental results on ISCAS85 circuits show that the path delay error predicted by our methods is about 1% of that computed by the RSM using SPICE, where the path delay variation is within /spl plusmn/10%.","Integrated circuit interconnections,
Performance analysis,
Table lookup,
Timing,
Delay effects,
Computer science,
Manufacturing processes,
Circuit testing,
SPICE,
Very large scale integration"
A new technique for leakage reduction in CMOS circuits using self-controlled stacked transistors,"In CMOS circuits, the reduction of the threshold voltage due to voltage scaling leads to increase in sub threshold leakage current and hence, static power dissipation. We propose a novel technique called LECTOR for designing CMOS gates which significantly cuts down the leakage current without increasing the dynamic power dissipation. In the proposed technique, we introduce two leakage control transistors (a p-type and a n-type) within the logic gate for which the gate terminal of each leakage control transistor (LCT) is controlled by the source of the other. In this arrangement, one of the LCT's is always ""near its cut-off voltage"" for any input combination. This increases the resistance of the path from V/sub dd/ to ground leading to significant decrease in leakage currents. The gate-level netlist of the given circuit is first converted into a static CMOS complex gate implementation and then LCTs are introduced to obtain a leakage controlled circuit. The significant feature of LECTOR is that it works effectively in both active and idle states of the circuit, resulting in better leakage reduction compared to other techniques. Further, the proposed technique overcomes the limitations posed by other existing methods for leakage reduction. Experimental results indicate an average leakage reduction of 79.4% for MCNC '91 benchmark circuits.","Circuits,
Threshold voltage,
Power dissipation,
Leakage current,
Dynamic voltage scaling,
Sleep,
Very large scale integration,
Nanomaterials,
Computer science,
Power engineering and energy"
A perceptive multirobot system for monitoring electro-magnetic fields,"Sensor networks are distributed systems that detect phenomena to produce detailed environmental assessments. The use of multirobot systems for carrying sensors around the environment represents a solution that has recently received considerable attention. In this paper we present an architecture for a multirobot system that performs tasks related to environmental perception; in particular, it is devoted to monitoring electro-magnetic fields.","Multirobot systems,
Monitoring,
Sensor systems,
Sensor phenomena and characterization,
Mobile robots,
Current measurement,
Position measurement,
Navigation,
Tin,
Safety"
Spectral analysis of random graphs with skewed degree distributions,"We extend spectral methods to random graphs with skewed degree distributions through a degree based normalization closely connected to the normalized Laplacian. The normalization is based on intuition drawn from perturbation theory of random matrices, and has the effect of boosting the expectation of the random adjacency matrix without increasing the variances of its entries, leading to better perturbation bounds. The primary implication of this result lies in the realm of spectral analysis of random graphs with skewed degree distributions, such as the ubiquitous ""power law graphs"". Mihail and Papadimitriou (2002) argued that for randomly generated graphs satisfying a power law degree distribution, spectral analysis of the adjacency matrix simply produces the neighborhoods of the high degree nodes as its eigenvectors, and thus miss any embedded structure. We present a generalization of their model, incorporating latent structure, and prove that after applying our transformation, spectral analysis succeeds in recovering the latent structure with high probability.","Spectral analysis,
Computer science,
Laplace equations,
Social network services,
Frequency,
Boosting,
Distributed power generation,
Power generation,
Intelligent systems,
Information systems"
A complexity measure for ontology based on UML,"UML is a good tool to represent ontologies. When using UML for ontology development, one of the principal goals is to assure the quality of ontologies. UML class diagrams provide a static modeling capability that is well suited for representing ontologies, so the structural complexity of a UML class diagram is one of the most important measures to evaluate the quality of the ontologies. This paper uses weighted class dependence graphs to represent given class diagrams, and then presents a structure complexity measure for the UML class diagrams based on entropy distance. It considers complexity of both classes and relationships between the classes, and presents rules for transforming complexity value of classes and different kinds of relations into weighted class dependence graphs. This method can measure the structure complexity of class diagrams objectively.","Ontologies,
Unified modeling language,
Object oriented modeling,
Computer science,
Software quality,
Laboratories,
Semantic Web,
Resource description framework,
Artificial intelligence,
Knowledge representation"
A game theory approach to pairwise classification with support vector machines,"Support Vector Machines (SVM) for pattern recognition are discriminant binary classifiers. One of the approaches to extend them to multi-class case is pairwise classification. Pairwise comparisons for each pair of classes are combined together to predict the class or to estimate class probabilities. This paper presents a novel approach, which considers the pairwise S VM classification as a decision-making problem and involves game theory methods to solve it. We prove that in such formulation the solution in pure minimax strategies is equivalent to the solution given by standard fuzzy pairwise SVM method. On the other hand, if we use mixed strategies we formulate new linear programming based pairwise SVM method for estimating class probabilities. We evaluate the performance of the proposed method in experiments with several benchmark datasets, including datasets for optical character recognition and multi-class text categorization problems.","Game theory,
Support vector machines,
Support vector machine classification,
Pattern recognition,
Decision making,
Minimax techniques,
Computer science,
Linear programming,
Probability,
Optical character recognition software"
A comparative investigation into two pointing systems for use with wearable computers while mobile,"Target selection is a task carried out by many wearable computer users. Conventional desktop pointing devices such as mice are not appropriate for the wearable user as they are designed for use within the conventional desktop paradigm. Although many pointing systems have been devised for use with wearable computers, little empirical research has been carried out. This research investigates two different target selection systems: a touch screen stylus and an off-table mouse. This research takes a novel approach and evaluates users while moving and stationary. Twenty participants wore a wearable computer and selected targets while stationary and while mobile, input times and the participants' task load were recorded.","Wearable computers,
Mobile computing,
Mice,
Application software,
Military computing,
Legged locomotion,
Computer science,
Human computer interaction,
Medical services,
Biomedical equipment"
Efficient methods for generating optimal single and multiple spaced seeds,"Biologists highly rely on good algorithms for finding homologous regions in bimolecular sequences. An advanced homology search program named PatternHunter has recently been developed, unlike the well-known program BLAST using a consecutive model, it utilizes a spaced seed model to attain higher sensitivity. We have developed a new program, which extends PatternHunter from a single spaced model to a multiple spaced model. In this paper, we describe methods for finding optimal single and multiple spaced models.","Hidden Markov models,
Computer science,
Biological system modeling,
Pattern matching,
Heuristic algorithms,
Dynamic programming,
Chaos,
Biology,
Databases,
Sensitivity analysis"
CAD challenges in BioMEMS design,,"Design automation,
Micromachining,
Micromechanical devices,
Nanobioscience,
In vitro,
Permission,
Prototypes,
Jacobian matrices,
Computer science,
Laboratories"
End-to-end measurements on performance penalties of IPv4 options,"IP version 4 specifies options that extend the basic IP header and also allow new functions to be added to IP without breaking existing implementations. Since options must always be inspected at routers, it is generally believed that routers prioritize ordinary packets over packets carrying options in a way that significantly impacts the performance of options. This article presents an experiment based on end-to-end probing using UDP packets, capturing the performance penalties associated with IPv4 options. Analysis of experiment results quantifies the impact of IPv4 options on forwarding performance in terms of delay, jitter and loss rate. From the analysis it can be concluded that there is a slight increase in delay and jitter and a severe increase in loss rate.","Jitter,
Internet,
Delay,
Computer science,
Streaming media,
Hardware,
Performance analysis,
Performance loss,
Routing,
Quality of service"
A semantic classification model for e-catalogs,"Electronic catalogs (or e-catalogs) hold information about the goods and services offered or requested by the participants, and consequently, form the basis of an e-commerce transaction. Catalog management is complicated by a number of factors and product classification is at the core of these issues. Classification hierarchy is used for spend analysis, customs regulation, and product identification. Classification is the foundation on which product databases are designed, and plays a central role in almost all aspects of management and use of product information. However, product classification has received little formal treatment in terms of underlying model, operations, and semantics. We believe that the lack of a logical model for classification introduces a number of problems not only for the classification itself but also for the product database in general. In this paper, we try to understand what it means to classify products and present how best to represent classification schemes so as to capture the semantics behind the classifications and facilitate mappings between them. We believe the model proposed in this paper satisfies the requirements and challenges that have been raised by previous works.","Electronic commerce,
Supply chain management,
Electronic catalog,
Product design,
Databases,
Resource management,
Enterprise resource planning,
Manufacturing processes,
Research and development,
Computer science"
"Why so few women, still?","""Why are there so few women in engineering?"" queried the e-mail from a male engineering student. As I listed the reasons put forth by various experts, the thought occurred to me: We've been working on this issue for 50 years now. If we really knew the answer, we would have solved it already. I've read lots of research on this topic and conducted many recruitment and outreach programs over the years to try to interest more women in studying engineering, and the conclusion I've reached is what I call ""Jill's Theory"": Engineering in the United States suffers from a huge image problem.","Engineering profession,
Switches,
Dentistry,
Portable computers,
Psychology,
Cellular phones,
Companies,
Back,
Engineering students,
Engineering education"
Improving modeling of other agents using tentative stereotypes and compactification of observations,"We investigate possible improvements to modeling other agents based on observed situation-action pairs and the nearest neighbor rule. Tentative stereotype models allow for good predictions of a modeled agent's behavior even after few observations. Periodic revaluation of the chosen stereotype and the potential for switching between different stereotypes or to the observation based model aids in dealing with very similar (but not identical) stereotypes and agents that do not conform to any stereotype. Finally, compactification of observations keeps the application of the model efficient by reducing comparisons within the nearest neighbor rule. Our experiments show that stereotyping significantly improves cases where using just the original method performs badly and that revaluation and switching fortify stereotyping against the potential risk of using an incorrect stereotype. Compactification shows good potential for improving efficiency, but is sometimes at risk of losing important observations.",
Co-Grid: an efficient coverage maintenance protocol for distributed sensor networks,"Wireless sensor networks often face the critical challenge of sustaining long-term operation on limited battery energy. Coverage maintenance protocols can effectively prolong network lifetime by maintaining sufficient sensing coverage over a region using a small number of active nodes while scheduling the others to sleep. We present a novel distributed coverage maintenance protocol called the coordinating grid (Co-Grid). In contrast to existing coverage maintenance protocols which are based on simpler detection models, Co-Grid adopts a distributed detection model based on data fusion that is more consistent with many distributed sensing applications. Co-Grid organizes the network into coordinating fusion groups located on overlapping virtual grids. Through coordination among neighboring fusion groups, Co-Grid can achieve comparable number of active nodes as a centralized algorithm, while reducing the network (re-)configuration time by orders of magnitude. Co-Grid is especially suitable for large and energy-constrained sensor networks that require quick (re-)configuration in response to node failures and environmental changes. We validate our claims by both theoretical analysis and simulations.","Protocols,
Sensor phenomena and characterization,
Wireless sensor networks,
Event detection,
Batteries,
Permission,
Computer science,
Maintenance engineering,
Scheduling,
Real time systems"
An arithmetical hierarchy of the law of excluded middle and related principles,"The topic of this paper is relative constructivism. We are concerned with classifying nonconstructive principles from the constructive viewpoint. We compare, up to provability in intuitionistic arithmetic, subclassical principles like Markov's principle, (a function-free version of) weak Konig's lemma, Post's theorem, excluded middle for simply existential and simply universal statements, and many others. Our motivations are rooted in the experience of one of the authors with an extended program extraction and of another author with bound extraction from classical proofs.","Logic,
Computer science"
Trust-preserving set operations,"We describe a method for performing trust-preserving set operations by untrusted parties. Our motivation for this is the problem of securely reusing content-based search results in peer-to-peer networks. We model search results and indexes as data sets. Such sets have value for answering a new query only if they are trusted. In the absence of any system-wide security mechanism, a data set is trusted by a node a only if it was generated by some node which is trusted by a. Our main contributions are a formal definition of the problem as well as an efficient scheme that solves this problem by allowing untrusted peers to perform set operations on trusted data sets while also producing unforgeable proofs of correctness. This is accomplished by requiring trusted nodes to sign appropriately-defined digests of generated sets; each such digest consists of an RSA accumulator and a Bloom filter. The scheme is general, and has other applications as well. We give an analysis demonstrating the low overhead of the scheme, and we include experimental data which confirm the analysis.","Peer to peer computing,
Performance analysis,
Costs,
Computer science,
Educational institutions,
Data security,
Filters,
Cryptography,
Routing"
Cooperative embodied communication emerged by interactive humanoid robots,"Research on humanoid robots has produced various uses for their body properties in communication. In particular, mutual relationships of body movements between a robot and a human are considered to be important for smooth and natural communication, as they are in human-human communication. We have developed a semi-autonomous humanoid robot system that is capable of cooperative body movements with humans using environment-based sensors and switching communicative units. And we conducted an experiment using this robot system and verified the importance of cooperative behaviors in a route-guidance situation where a human gives directions to the robot. This result indicates that the cooperative body movements greatly enhance the emotional impressions of human in a route-guidance situation. We believe these results allow us to develop interactive humanoid robots that sociably communicate with humans.","Humanoid robots,
Robot sensing systems,
Intelligent robots,
Human robot interaction,
Laboratories,
Sensor systems,
Communication switching,
Computer science,
Adaptive systems,
Educational institutions"
Tight lower bounds for certain parameterized NP-hard problems,"Based on the framework of parameterized complexity theory, we derive tight lower bounds on the computational complexity for a number of well-known NP-hard problems. We start by proving a general result, namely that the parameterized weighted satisfiability problem on depth-t circuits cannot be solved in time n/sup o(k)/poly(m), where n is the circuit input length, m is the circuit size, and k is the parameter, unless the (t - l)-st level W[t $1] of the W-hierarchy collapses to FPT. By refining this technique, we prove that a group of parameterized NP-hard problems, including weighted SAT, dominating set, hitting set, set cover, and feature set, cannot be solved in time n/sup o(k)/poly(m), where n is the size of the universal set from which the k elements are to be selected and m is the instance size, unless the first level W[l] of the W-hierarchy collapses to FPT. We also prove that another group of parameterized problems which includes weighted q-SAT (for any fixed q /spl ges/ 2), clique, and independent set, cannot be solved in time n/sup o(k)/ unless all search problems in the syntactic class SNP, introduced by Papadimitriou and Yannakakis, are solvable in subexponential time. Note that all these parameterized problems have trivial algorithms of running time either n/sup k/ poly(m) or O(n/sup k/).",
On the representation of contact states between curved objects,"Information of high-level, topological contact states is useful and even necessary for a wide range of applications, including many robotic applications. A contact state between two polyhedral objects can be effectively represented as a contact formation in terms of a set of principal contacts between faces, edges, and vertices of the two objects. However, little is done to characterize and represent contact states between curved objects. In order to facilitate the representation of contact states between such objects, we introduce a novel approach to segment the boundary of curved objects based on monotonic changes of curvatures, which we call the curvature monotonic segmentation. We specifically apply this approach to curved 2D and 3D objects with boundary curves or surfaces represented by algebraic polynomials of degrees up to 2. The segmentation yields curvature monotonic faces and edges (or pseudo edges), and vertices (or pseudo vertices). With these faces, (pseudo) edges, and (pseudo) vertices, we effectively extend the concept of contact formation to curved objects to represent high-level, topological contact states between such objects with the same desirable characteristics as the contact formations between polyhedral objects.","Robots,
Polynomials,
Application software,
Biological system modeling,
Computer science,
Cities and towns,
Haptic interfaces,
Robustness,
Solids,
Geometry"
Ciset: a generalization of fuzzy sets,In this paper we extend fuzzy theory to handle both positive and negative values. This would enable us to extend database theory and expert systems to integrate the supporting and the opposing facts based on the subject matter and thus obtain a complete picture of the situation.,"Fuzzy sets,
Weapons,
Lattices,
Computer science,
Databases,
Expert systems,
Brightness,
Production"
Pipelined multipliers for reconfigurable hardware,"Summary form only given. Reconfigurable devices used in digital signal processing applications must handle large amounts of data in vector form. Most signal processing algorithms use multiplication extensively; thus, the hardware must support this operation to achieve high performance. However, mapping a multiplier on traditional fine-grain devices produces a complex structure whose performance is limited by the routing overhead. In this paper, we present a novel pipelined multiplier structure suitable for medium-grain and coarse-grain reconfigurable cell arrays. We first implement an unsigned n-bit multiplier using m-bit cells. Then, we show how the same structure can work with two's-complement data with small changes to the configuration. The structure requires [n/m]/sup 2/ cells, but can execute vector operations in a pipelined fashion. We also discuss the benefits of using a hierarchical design for large multipliers.","Hardware,
Pipeline processing,
Clocks,
Computer science,
USA Councils,
Distributed processing,
Broadcasting,
Throughput,
Digital signal processing"
Performance modeling of distributed hybrid architectures,"Hybrid architectures are systems where a high performance general purpose computer is coupled to one or more special purpose devices (SPDs). Such a system can be the optimal choice for several fields of computational science. Configuring the system and finding the optimal mapping of the application tasks onto the hybrid machine often is not straightforward. Performance modeling is a tool to tackle and solve these problems. We have developed a performance model to simulate the behavior of a hybrid architecture consisting of a parallel multiprocessor where some nodes are the host of a GRAPE board. GRAPE is a very high performance SPD used in computational astrophysics. We validate our model on the architecture at our disposal, and show examples of predictions that our model can produce.","Computer architecture,
Application software,
Computational modeling,
Hardware,
Pipelines,
Astrophysics,
Kernel,
High performance computing,
Predictive models,
Quantum computing"
Extending record and playback technologies to support cooperative learning,"We have long-term experience with developing and employing multimedia materials for on-campus and distance education. We also are assessing the efficacy of cooperative learning where groups of learners explore, with guidance from an instructor, the learning environment and construct models of meaning based on their shared learning experiences. Our core technologies capture and store classroom events, but are record-and-playback technologies focused on delivering content to individual learners. We describe an extension of our technology, Cooperative learning in MANIC (CLIMANIC), which allows groups of learners and teachers to collaborate and communicate. We describe our current assessment of CLIMANIC and future plans for more extensive evaluation.","Collaboration,
Streaming media,
Courseware,
Laboratories,
Distance learning,
Computer aided instruction,
Multimedia systems,
Collaborative tools,
Computer science,
Educational technology"
An algebraic approach to the complexity of propositional circumscription,Every logical formalism gives rise to two fundamental problems: model checking and inference. Circumscription is one of the most important and well studied formalisms in the realm of nonmonotonic reasoning. The model checking and inference problem for propositional circumscription has been extensively studied from the viewpoint of computational complexity. We use a new approach based on algebraic techniques to study the complexity of the model checking and inference problems for propositional variable circumscription in a unified way. We prove that there exists a dichotomy theorem for the complexity of the inference problem in propositional variable circumscription. We also study the model checking and inference problem for propositional variable circumscription in many-valued logics using the same algebraic techniques. In particular we prove dichotomy theorems for the complexity of model checking and inference for propositional variable circumscription in the case of 3-valued logic.,
RACE: time series compression with rate adaptivity and error bound for sensor networks,"Sensor networks usually have limited energy and transmission capacity. It is beneficial to reduce the data volume for dissemination in a sensor network that monitors continuous physical processes in order to reduce energy consumption. Data compression schemes in use should be able to adapt to limited bandwidth while preserving high data quality. We propose a wavelet-based, error aware compression algorithm that is targeted to achieving these goals. It is called RACE (rate adaptive compression with error bound). It can adjust its maximum normalized error to current network capacity. Additionally, errors due to multiple passes of compression during multi-hop relaying are additive and thus can be estimated easily upon data reconstruction. Moreover, during data dissemination, error ranges can be narrowed through an opportunistic patching process when excess bit rate is available. Consequently, the performance is less subject to the volatility of physical processes. The algorithm has been evaluated in various aspects and demonstrated to be effective in rate adaptivity, error range narrowing, and preservation of statistical interpretation.","Bandwidth,
Relays,
Monitoring,
Bit rate,
Sensor phenomena and characterization,
Energy conservation,
Computer errors,
Computer science,
Capacitive sensors,
Computer displays"
Four metrics for efficiently comparing attributed trees,"We address the problem of comparing attributed trees and propose four novel distance metrics centered around the notion of a maximal similarity common subtree, and hence can be computed in polynomial time. We experimentally validate the usefulness of our metrics on shape matching tasks, and compare them with edit-distance.","Tree graphs,
Polynomials,
Pattern recognition,
Time measurement,
Computer science,
Shape,
Pattern matching,
Computer vision,
Layout,
Multidimensional systems"
Personalized face verification system using owner-specific cluster-dependent LDA-subspace,"We propose an owner-specific cluster-dependent linear discriminant analysis (OSCD-LDA) method, and apply it to develop a personalized face verification system. Before the owner enrollment, our system first divides all the training face images into a number of clusters, each containing a subset of face images having similar characteristics. Once the owner completes the enrollment procedure, the system assigns the owner to the cluster that contains faces most similar to the owner's training faces. Then, the system uses the training faces in this most similar cluster to determine the OSCD-LDA subspace for computing the matching score. This OSCD-LDA subspace can be considered as a personalized subspace, trained specifically for this owner in order to best discriminate this particular owner from other non-owners. Our experimental results have shown that the proposed OSCD-LDA method outperforms the conventional LDA method, and can reduce false acceptance rate and false rejection rate by about 40 percent when using the XM2VTS database.","Linear discriminant analysis,
Principal component analysis,
Authentication,
Computer science,
Information science,
Electronic mail,
Databases,
Biometrics,
Face recognition,
Robustness"
High-level methods for quantum computation and information,"Quantum information and computation is concerned with the use of quantum-mechanical systems to carry out computational and information-processing tasks (Nielsen and Chunag, 2000). In the few short years that this approach has been studied, a number of remarkable concepts and results have emerged, most notably:a couple of spectacular algorithms and a number of information protocols, exemplified by quantum teleportation, which exploit quantum entanglement in an essential fashion. The current tools available for developing quantum algorithms and protocols are deficient on two main levels: firstly, they are too low-level and at a more fundamental level, the standard mathematical framework for quantum mechanics (which is essentially due to von Neumann (1932)) is actually insufficiency comprehensive for informatic purposes. In joint work with Bob Coecke, we have recently made some striking progress in addressing both these points. They have recast the von Neumann formalism at a more abstract and conceptual level, using category theory.",
Using fuzzy theory for effort estimation of object-oriented software,"Estimating software effort and costs is a very important activity that includes very uncertain elements. The concepts of the fuzzy set theory has been successfully used for extending metrics such as FP and reducing human influence in the estimation process. However, when we consider object-oriented technologies, other models, such as the use case model, are used to represent the specification in the early stages of development. New metrics based on this model were proposed and the application of the fuzzy set theory in this context is also very important. This work introduces the metric FUSP (fuzzy use case size points) that allows gradual classifications in the estimation by using fuzzy numbers. Results of a study case show some advantages and limitations of the proposed metric.","Estimation theory,
Object oriented modeling,
Fuzzy set theory,
Costs,
Computer science,
Humans,
Context modeling,
Uncertainty,
Fuzzy systems,
Lab-on-a-chip"
Location-aware cache replacement for mobile environments,"Traditional cache replacement policies rely on the temporal locality of users' access pattern to improve cache performance. These policies, however, are not ideal in supporting mobile clients. As mobile clients can move freely from one location to another, their access pattern not only exhibits temporal locality, but also exhibits spatial locality. In order to ensure efficient cache utilisation, it is important to take into consideration the location and movement direction of mobile clients when performing cache replacement. In this paper. we propose a mobility-aware cache replacement policy, called MARS, suitable for wireless environments. MARS takes into account important factors (e.g. client access rate, access probability, update probability and client location) in order to improve the effectiveness of onboard caching for mobile clients. Test results show that MARS consistently outperforms existing cache replacement policies and significantly improves mobile clients' cache hit ratio.",
A systemC-based modular design and verification framework for C-model reuse in a HW/SW-codesign design flow,"Rising the level of abstraction in system modelling allows early verification of the system functionality, reducing the risk of long redesign cycles. Moving to a new flow introducing systemC as SDL allows the reuse of existing high-level C-models. A framework is presented that allows C-model integration and the connection of modules located at different levels of abstraction without the need to implement the communication or introduce adaptors to translate between the abstraction levels. The focus of the approach lies on high acceptance by the designers coming from a C and HDL based top-down design methodology.","Design methodology,
Software systems,
Process design,
Concurrent computing,
Electrical equipment industry,
Industrial control,
Computer science,
Hardware design languages,
Time to market,
Embedded software"
An execution slice and inter-block data dependency-based approach for fault localization,"Localizing a fault in a program is a complex and time-consuming process. In this paper we present a novel approach using execution slice and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, the likelihood of a piece of code containing a specific fault is inversely proportional to the number of successful tests that execute it. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.","Debugging,
Fault diagnosis,
Software testing,
Computer bugs,
Computer science,
Application software,
Life testing,
Programming"
SimSnap: fast-forwarding via native execution and application-level checkpointing,"As systems become more complex, conducting cycle-accurate simulation experiments becomes more time consuming. Most approaches to accelerating simulations attempt to choose simulation points, such that the performance of the program portions modeled in detail are representative of whole-program behavior. To maintain or build the correct architectural state, ""fast-forwarding"" models a series of instructions before a desired simulation point. This fast-forwarding is usually performed by functional simulation: modeling the effects of instructions without all the details of pipeline stages and individual /spl mu/-ops. We present another fast-forwarding technique, SimSnap, that leverages native execution and application-level checkpointing. We demonstrate the viability of our approach by moving checkpointed versions of SPLASH-2 benchmarks between an Alpha 21264 system and SimpleScalar Version 4.0 Alpha-Sim. Reduction in experiment times is dramatic, with minimal perturbation of benchmark programs.","Checkpointing,
Computational modeling,
Computer simulation,
Analytical models,
Discrete event simulation,
Timing,
Statistics,
Application software,
Computer science,
Acceleration"
Exposing memory access regularities using object-relative memory profiling,"Memory profiling is the process of characterizing a program's memory behavior by observing and recording its response to specific input sets. Relevant aspects of the program's memory behavior may then be used to guide memory optimizations in an aggressively optimizing compiler. In general, memory access behavior has eluded meaningful characterization because of confounding artifacts from memory allocators, linker data layout, and OS memory management. Since these artifacts may change from run to run, memory access patterns may appear different in each run even for the same input set. Worse, regular memory access behavior such as linked list traversals appear to have no structure. We present object-relative translation and decomposition techniques to eliminate these artifacts and to expose previously obscured memory access patterns. To demonstrate the potential of these ideas, we implement two different memory profilers targeted at different sets of applications. These profilers outperform the existing ones in terms of profile size and useful information per byte of data. The first profiler is a lossless profiler, called WHOMP, which uses object-relativity to achieve a 22% better compression than the previously best known scheme. The second profiler, called LEAP, uses lossy compression to get highly compact profiles while providing useful information to the targeted applications. LEAP correctly characterizes the memory alias rates for 56% more instruction pairs than the previously best known scheme with a practical running time.","Random access memory,
Optimizing compilers,
Memory management,
Computer science,
Prefetching,
Libraries,
Probes"
An intent-driven planner for multi-agent story generation,,"Character generation,
Computer science,
Computer science education,
Process planning,
Humans,
Engines"
Cryptanalysis of log-in authentication based on circle property,"A scheme of user authentication had been proposed to authenticate users based on the secret data stored inside a smart card and the property of an n-dimensional circle. We investigate the security of the scheme and show that the scheme is insecure under the dictionary attack, the impersonation attack, and the attack of impersonating the central authority. Due to the insecurity under various attacks, we suggest that a cryptographic scheme should be provably secure.","Authentication,
Smart cards,
Dictionaries,
Data security,
Law,
Legal factors,
Cryptography,
Mathematics,
Computer science,
Entropy"
Approximate fingerprint matching using kd-tree,"Fast and robust fingerprint matching is a challenging task today in fingerprint-based biometric systems. A fingerprint matching algorithm compares two given fingerprints and returns either a degree of similarity or a binary decision. Minutiae-based fingerprint matching is the most well-known and widely used method. This paper reveals a new technique of fingerprint matching, using an efficient data structure, combining the minutiae representation with the individual usefulness of each minutia, to make the matching more powerful. Experimental results exhibit the strength of this method.","Fingerprint recognition,
Image matching,
Fingers,
Robustness,
Pattern matching,
Data mining,
Bifurcation,
Gray-scale,
Computer science,
Educational institutions"
A static task scheduling heuristic for homogeneous computing environments,"List-based scheduling is generally accepted as an attractive approach to static task scheduling in a homogeneous environment, since it pairs low complexity with good results. We present a low complexity algorithm based on list-scheduling and task-duplication on a bounded number of fully connected homogeneous machines. The algorithm is called critical unlisted parents with fast duplicator (CUPFD). The CUPFD algorithm consists of two phases: the listing phase, which is a simple listing heuristic based on list-scheduling, and a low complexity machine assigning phase based on task-duplication. The experimental work has shown that CUPFD outperformed on average all other higher complexity algorithms.","Processor scheduling,
Scheduling algorithm,
Distributed computing,
High performance computing,
Computer science,
Digital communication,
Computer applications,
Computational efficiency,
Costs,
Search methods"
Unobtrusiveness and efficiency in idle cycle stealing for PC grids,"Summary form only given. Studies have shown that for a significant fraction of the time desktop PCs and workstations are under-utilized. To exploit these idle resources, various desktop/workstation grid systems have been developed. The ultimate goal of such systems is to maximize efficiency of resource usage while maintaining low obtrusiveness to machine owners. To this end, we created a new fine-grain cycle stealing approach and conducted a performance comparison study against the traditional coarse-grain cycle stealing. We developed a prototype of fine-grain cycle stealing, the Linger-Longer system, on a Linux cluster. The experiments on a cluster of desktop Linux PCs with benchmark applications show that, overall, fine-grain cycle stealing can improve efficiency of idle cycle usage by increasing the guest job throughput by 50% to 70%, while limiting obtrusiveness with no more than 3% of host job slowdown.","Workstations,
Contracts,
Computer science,
Personal communication networks,
Prototypes,
Linux,
Throughput,
Biological system modeling,
Computational modeling,
Computer simulation"
Efficient and scalable barrier over Quadrics and Myrinet with a new NIC-based collective message passing protocol,"Summary form only given. Modern interconnects often have programmable processors in the network interface that can be utilized to offload communication processing from host CPU. We explore different schemes to support collective operations at the network interface and propose a new collective protocol. With barrier as an initial case study, we have demontrated that much of the communication processing can be greatly simplified with this collective protocol. Accordingly, we have designed and implemented efficient and scalable NIC-based barrier operations over two high performance interconnects, Quadrics and Myrinet. Our evaluation shows that, over a Quadrics cluster of 8 nodes with ELan3 network, the NIC-based barrier operation achieves a barrier latency of only 5.60/spl mu/s. This result is a 2.48 factor of improvement over the Elanlib tree-based barrier operation. Over a Myrinet cluster of 8 nodes with LANai-XP NIC cards, a barrier latency of 14.20/spl mu/s over 8 nodes is achieved. This is a 2.64 factor of improvement over the host-based barrier algorithm. Furthermore, an analytical model developed for the proposed scheme indicates that a NIC-based barrier operation on a 1024-node cluster can be performed with only 22.13/spl mu/s latency over Quadrics and with 38.94/spl mu/s latency over Myrinet. These results indicate the potential for developing high performance communication subsystems for next generation clusters.","Message passing,
Protocols,
Delay,
Hardware,
Broadcasting,
Laboratories,
Computer networks,
Network interfaces,
Mathematics,
Computer science"
An efficient algorithm for lossless compression of IEEE float audio,"Audio data in the IEEE float format is used by the professional quality audio editors as the format of choice during the editing process and also for storing the intermediate results. The proposed algorithm transforms the floating-point values into integer values, in a completely portable way across architectures and compilers. The transform maintains all the important properties of the original values, such as magnitude relations, linear temporal relations and, most importantly, compressibility. It produces a sequence of integers and an additional binary stream used for lossless reconstruction of the original values. Furthermore, the proposed algorithm can be successfully applied to other areas such as medical, science and space data compression. Our algorithm obtains on average 7% better compression than the existing multimedia-aware compression algorithms, while being also faster.","Signal processing algorithms,
Quantization,
Signal to noise ratio,
Data compression,
Hardware,
Signal processing,
Phase change materials,
Computer science,
Streaming media,
Compression algorithms"
Complex square root with operand prescaling,"We propose a radix-r digit-recurrence algorithm for complex square-root. The operand is prescaled to allow the selection of square-root digits by rounding of the residual. This leads to a simple hardware implementation. Moreover, the use of digit recurrence approach allows correct rounding of the result. The algorithm, compatible with the complex division, and its design are described at a high-level. We also give rough comparisons of its latency and cost with respect to implementation based on standard floating-point instructions as used in software routines for complex square root.","Hardware,
Arithmetic,
Quantum computing,
Computer science,
Algorithm design and analysis,
Delay,
Costs,
Software standards,
Computer aided instruction,
Singular value decomposition"
A reliable and efficient MAC layer broadcast (multicast) protocol for mobile ad hoc networks,"Broadcast/multicast is a key service for mobile ad hoc networks. A great number of applications rely on a reliable and efficient MAC layer broadcast. The IEEE 802.11 broadcast protocol, which is based on carrier sense multiple access with collision avoidance (CSMA/CA), does not offer any MAC layer recovery on broadcast frames. Consequently, the increasing probability of lost frames may deteriorate the quality of broadcast/multicast services offered at upper layers. In this paper, we first formulate the broadcast problem as an optimization problem and show that it is NP-hard, even if the upper layer service is periodical beacons. An approximation algorithm with a guaranteed approximation ratio is also suggested. Then, a reliable and efficient MAC layer broadcast protocol, named broadcast protocol with busy tone (BPBT), is proposed. BPBT applies a busy tone to solve the hidden terminal problem. Finally, BPBT is compared with previous protocols for performance evaluation by simulation.","Multicast protocols,
Media Access Protocol,
Broadcasting,
Mobile ad hoc networks,
Access protocols,
Multimedia communication,
Mobile communication,
Ad hoc networks,
Computer network reliability,
Computer science"
Patch panel: enabling control-flow interoperability in ubicomp environments,"Ubiquitous computing environments accrete slowly over time rather than springing into existence all at once. Mechanisms are needed for incremental integration- the problem of how to incrementally add or modify behaviors in existing ubicomp environments. Examples include adding new input modalities and choreographing the behavior of existing independent applications. The iROS event heap, via its publish-subscribe coordination mechanism, provides the foundation for interoperation through event intermediation, but does not directly provide facilities for expressing these intermediations. The patch panel provides a general facility for retargeting event flow. Intermediations can be expressed as simple event translation mappings or as more complex finite-state machines. We describe an implemented prototype of the patch panel, including examples of its use drawn from real life applications in production use in the iRoom ubiquitous computing environment.","Pervasive computing,
Ubiquitous computing,
Space technology,
Printers,
Computer science,
Cameras,
Image converters,
Printing,
Communication system control,
Publish-subscribe"
The effect of music on the perception of display rate and duration of animated sequences: an experimental study,"It's currently impossible, even on modern graphics hardware to compute high fidelity graphics of complex scenes in real time. As we are producing the graphics for human observers it may be possible to exploit limitations of the human perceptual system to improve the quality/rendering time ratio. When confronted with multisensory input the human has to divide his/her cognitive resources between the different sensory stimuli. We present an independent samples experiment on the influence of musical tempo and emotional suggestiveness of music on the perception of motion and time duration in a computer graphics environment. The purpose is to investigate whether music would be a significant distractor, allowing us to render at a slower frame rate without any perceivable difference for the user. No overall main effect of fast tempo/exciting music was revealed, while slow tempo/relaxing music resulted in longer duration estimations and slower perceived temporal rates","Multiple signal classification,
Animation,
Rendering (computer graphics),
Computer graphics,
Humans,
Frequency,
Hardware,
Layout,
Computer displays,
Computer science"
Failure detection and membership management in grid environments,"Failure detectors are an integral part of any fault tolerant distributed system and hence have been a well-studied area. However, earlier proposed failure detectors fail to perform efficiently when applied to grid environments. Most of the earlier proposed detectors were either designed for local area networks or to handle small number of nodes and hence lack in areas such as scalability, efficiency, running times etc. In this paper we propose a highly scalable failure detector protocol that is aided by a membership management service. The membership management service is essential to make the failure detector transparent to changes in the system. Using a distributed heartbeat mechanism, for an unreliable failure detector, we have overcome the shortcomings of similar schemes proposed earlier. It realizes scalability by reducing context switching requirements and achieves faster failure detection. The membership management protocol handles membership issues with a worst case complexity of O(n) where n is the number of heartbeat groups. Note that n is much smaller than the total number of nodes in the grid. The algorithm is also shown to be failure resilient and scalable.","Environmental management,
Detectors,
Protocols,
Heart beat,
Local area networks,
Scalability,
Large-scale systems,
Computer science,
Fault detection,
Fault tolerant systems"
Quantum weak coin-flipping with bias of 0.192,"We present a family of protocols for flipping a coin over a telephone in a quantum mechanical setting. The family contains protocols with n + 2 messages for all n > 1, and asymptotically achieves a bias of 0.192. The case n = 2 is equivalent to the protocol of Spekkens and Rudolph with bias 0.207, which was the best known protocol. The case n = 3 achieves a bias of 0.199, and n = 8 achieves a bias of 0.193. The analysis of the protocols uses Kitaev's description of coin-flipping as a semidefinite program. We construct an analytical solution to the dual problem which provides an upper bound on the amount that a party can cheat.","Protocols,
Quantum mechanics,
Telephony,
Collaboration,
Quantum computing,
Upper bound,
Security,
Quantum entanglement,
Optimized production technology"
Approximate temporal aggregation,"Temporal aggregate queries retrieve summarized information about records with time-evolving attributes. Existing approaches have at least one of the following shortcomings: (i) they incur large space requirements, (ii) they have high processing cost and (iii) they are based on complex structures, which are not available in commercial systems. We solve these problems by approximation techniques with bounded error. We propose two methods: the first one is based on multiversion B-trees and has logarithmic worst-case query cost, while the second technique uses off-the-shelf B- and R-trees, and achieves the same performance in the expected case. We experimentally demonstrate that the proposed methods consume an order of magnitude less space than their competitors and are significantly faster, even for cases that the permissible error bound is very small.","Costs,
Databases,
Information retrieval,
Computer science,
Space technology,
Aggregates,
Telecommunications,
Terminology,
Call conference,
Query processing"
Performance advantage and use of a location based handover algorithm,"In this paper we evaluate the performance of all types of handover procedures within UMTS, GSM and between the two of them. We estimate the time needed for measurements to determine the next cell during the handover preparation as well as the overall handover delay. To reduce the preparation time we suggest using the built in localization mechanisms of GSM and UMTS instead of measuring the reception level of surrounding cells. We discuss the suitability of the different localization mechanisms for the determination of the next cell. An acceleration of the handover procedures in GSM and UMTS is desirable in order to enhance the quality of calls. In particular time-savings are critical in scenarios where the duration of stay for a user within one cell is small, as it is the case e.g. for users moving with high speed. Moreover time-savings in the handover procedures can be used to help integrating an additional authentication during handovers between GSM and UMTS.","3G mobile communication,
GSM,
Time measurement,
Authentication,
Delay estimation,
Base stations,
Mobile communication,
Protection,
Frequency conversion,
Computer science"
Iso-shaping rigid bodies for estimating their motion from image sequences,"In many medical imaging applications, due to the limited field of view of imaging devices, acquired images often include only a part of a structure. In such situations, it is impossible to guarantee that the images will contain exactly the same physical extent of the structure at different scans, which leads to difficulties in registration and in many other tasks, such as the analysis of the morphology, architecture, and kinematics of the structures. To facilitate such analysis, we developed a general method, referred to as iso-shaping, that generates structures of the same shape from segmented image sequences. The basis for this method is to automatically find a set of key points, called shape centers, in the segmented partial anatomic structure such that these points are present in all images and that they represent the same physical location in the object, and then trim the structure using these points as reference. The application area considered here is the analysis of the morphology, architecture, and kinematics of the joints of the foot from magnetic resonance images acquired at different joint positions and load conditions. The accuracy of the method is analyzed by utilizing ten data sets for iso-shaping the tibia and the fibula via four evaluative experiments. The analysis indicates that iso-shaping produces results as predicted by the theoretical framework.","Motion estimation,
Image sequences,
Image analysis,
Magnetic analysis,
Morphology,
Kinematics,
Shape,
Image segmentation,
Biomedical imaging,
Image sequence analysis"
Requirements engineering process improvement based on an information model,"Requirements engineering (RE) process improvement methods typically work with explicit process models describing activities and document flow between the stakeholders involved, and with explicit document definitions. In complex, multi-project contexts, however, the RE process is better characterized as intertwining of design, negotiation, and sense-making. In the first part of This work, we present the concepts of a workshop-based RE process improvement technique suitable for a multi-project context. In the second part, we show the experiences made in an industrial case study conducted with Nokia smart traffic products. The major innovations of our approach are: (i) instead of a process model, an information model is created, which focuses solely on the responsibilities of stakeholders with regard to the major documents; (ii) instead of document details, only the major point of view of the documents is defined.","Context modeling,
Traffic control,
Software engineering,
Computer science,
Technological innovation,
Customer satisfaction,
Design engineering,
Bridges"
Frequency-domain analysis of analog single-event transients (ASETs) based on energy spectral density,"We present an analysis of analog single-event transients (ASETs) in the frequency domain. Through an overall error energy defined here, a metric of part susceptibility and the impact on surrounding system components can be determined.","Frequency domain analysis,
Transient analysis,
Operational amplifiers,
Circuit simulation,
Pulse measurements,
Circuit testing,
Energy measurement,
Computer errors,
Space vector pulse width modulation,
Fourier transforms"
Adaptive fault-tolerant wormhole routing with two virtual channels in 2D meshes,"An adaptive fault-tolerant wormhole routing algorithm based on a convex fault model in 2D meshes is presented. With the algorithm, a normal routing message, when blocked by faulty processors, would detour along some special polygons around the fault region. The result is that the proposed algorithm can tolerate convex faults with only two virtual channels per physical channel regardless of the overlapping of the boundaries of different fault regions. The convex fault model used does not include any nonfaulty processors and the proposed algorithm is deadlock-free.","Fault tolerance,
System recovery,
Routing protocols,
Computer science,
Solid modeling,
Information systems,
Delay,
Throughput,
Design methodology,
Algorithm design and analysis"
"JeCo, a Collaborative Learning Tool for Programming","Classroom pair-programming activities have been found to support novice student's learning during basic programming courses. However, there are very few tools that could support pair or group programming in distance education courses. Here, we explain the new concept of collaborative program visualization and present a tool to support it, called JeCo, that can help students work together on a platform that supports both collaborative authoring and program visualization","Collaborative work,
Visualization,
Programming profession,
Collaborative tools,
Collaborative software,
Distance learning,
Software tools,
Computer science,
Writing,
Assembly"
Dual power management for network connectivity in wireless sensor networks,"Summary form only given. As the energy consumption in wireless sensor nodes is dominated by the radio transmission circuitry, the network configuration must be designed to minimize the power consumption by transmission radios. Sensor nodes are generally equipped with short-range radios that require low power consumption. But the current technology allows each node to adjust its transmission power. We consider the dual power radios in which the radio of each node can be assigned high-or low-power during the network initializing stage. Our primary goal in such an assignment is to minimize the overall power consumption by radios to maximize the network lifetime while maintaining the full network connectivity, the most fundamental network functionality. Using a graph-theoretic approach, we formulate the problem as a ""minimum subgraph"" problem, show its NP-completeness, establish upper and lower bounds on the optimum solution, and present a near-optimal heuristic algorithm and simulation results.",
Model Management Through Graph Transformation,"Model management offers a higher level interface than current techniques for metadata management, and generic operators drastically reduce amount of programming for metadata applications. The interactive nature of generic model management operators inevitably demands an intuitive representation. This paper proposes a visual representation for model management operators based on graph transformation. Graph transformation formalisms, as the theoretic foundation of many visual programming languages, can formally represent model management operators by visual and intuitive expressions. By using graphical representations, users can easily comprehend and manipulate the operators and desired outputs","Data models,
Computer languages,
XML,
Humans,
Computer science,
Application software,
Relational databases,
Data engineering,
Design engineering,
Information management"
ASPIRE: automated systematic protocol implementation robustness evaluation,"Network protocol implementations are susceptible to problems caused by their lack of ability to handle invalid inputs. We present ASPIRE: automated systematic protocol implementation robustness evaluation, an automated approach to pro-actively test protocol implementations by observing their responses to faulty protocol data units (PDUs) or messages. In contrast to existing approaches, we sample the faulty PDU space in a systematic manner, thus allowing us to evaluate protocol implementations in the face of a wider variety of faulty PDUs. We use a pruning strategy to reduce, from exponential, the size of the faulty PDU set to polynomial in the number of fields of a PDU. We have implemented the ASPIRE algorithms and evaluated them on implementations of HTTP (Apache, Google Web Server (GWS), and Microsoft IIS) and SMTP (Sendmail and Microsoft Exchange) protocols. Our results show that Apache, GWS, and IIS, although implementing the same protocol specification, behave differently on faulty HTTP PDUs; Sendmail and exchange are different in handling our faulty SMTP PDUs.","Protocols,
Robustness,
Automatic testing,
Web server,
Vehicle crash testing,
Computer crashes,
Internet,
Computer science,
Educational institutions,
System testing"
Subspace analysis and optimization for AAM based face alignment,"Active appearance models (AAM) is very powerful for extracting objects, e.g. faces, from images. It is composed of two parts: the AAM subspace model and the AAM search. While these two parts are closely correlated, existing efforts treated them separately and had not considered how to optimize them overall. In this paper, an approach is proposed to optimize the subspace model while considering the search procedure. We first perform a subspace error analysis, and then to minimize the AAM error we propose an approach which optimizes the subspace model according to the search procedure. For the subspace error analysis, we decomposed the subspace error into two parts, which are introduced by the subspace model and the search procedure respectively. This decomposition shows that the optimal results of AAM can be achieved only by optimizing both of them jointly rather than separately. Furthermore, based on this error decomposition, we develop a method to find the optimal subspace model according to the search procedure by considering both the two decomposed errors. Experimental results demonstrate that our method can find the optimal AAM subspace model rapidly and improve the performance of AAM significantly.","Active appearance model,
Facial animation,
Shape,
Educational institutions,
Computer science,
Error analysis,
Face recognition,
Image reconstruction,
Asia,
Facial features"
Teaching renewable energy using multimedia,"In this paper an implementation of an electronic-learning system in the area of renewable energy resources is presented. According to written scenarios six teaching modules in this area of the science are realized. The control of the studying and testing modules is accomplished with the help of a Petri network. The structure of the teaching modules is presented in a few examples. This project, which is a subject of a university studying program, is going to be implemented in a special multimedia laboratory as part of the lecture and exercises of the subject of renewable energy resources.","Renewable energy resources,
Electronic learning,
Multimedia systems,
Power system reliability,
Educational technology,
Density estimation robust algorithm,
Management training,
Educational programs,
Computer science education,
Power system simulation"
Reconstruction of medical images by perspective shape-from-shading,"Shape-from-shading (SfS) is a fundamental problem in computer vision; it is based upon the image irradiance equation. Recently, the authors proposed to solve the image irradiance equation under the assumption of perspective projection rather than the common orthographic one. The solution was a modification of the fast marching method of Kimmel and Sethian. This paper presents an application of this novel perspective algorithm to reconstruction of medical images. We focus on gastrointestinal endoscopy and compare the two versions of the fast marching method (orthographic vs. perspective). The examples and comparison show that, unlike orthographic SfS, perspective SfS is robust and can be utilized for real-life applications.","Image reconstruction,
Biomedical imaging,
Equations,
Gastrointestinal tract,
Computer science,
Computer vision,
Application software,
Endoscopes,
Robustness,
Layout"
A scheme for the assignment of unique addresses to support self-organization in wireless sensor networks,"We propose a scheme for the assignment of unique addresses to facilitate self-organization in a wireless sensor network. The field of wireless sensor networks has experienced tremendous growth in the last few years, and their usage is rapidly increasing; however there still remain several issues that hinder the usability of such networks. One such issue is self-organization, where the nodes form a network by collaboration with each other without using manual intervention and using little external help. We propose a scheme to facilitate sensor nodes to organize themselves into a network. Our scheme uses unique addresses and requires only (3N-2) messages to assign addresses to all N nodes. The scheme is based on the concept of hierarchical levels and repeated patterns in order to achieve scalability. The paper describes our scheme and the advantages of using it.","Intelligent networks,
Wireless sensor networks,
Humans,
Usability,
Energy efficiency,
Scattering,
Computer science,
Telephony,
Collaboration,
Scalability"
PET reconstruction with system matrix derived from point source measurements,"The quality of images reconstructed by statistical iterative methods depends on an accurate model of the relationship between image space and projection space through the system matrix. A method of acquiring the system matrix on the CPS Innovations the HiRez scanner was developed. The system matrix was derived by positioning the point source in the scanner field of view and processing the response in projection space. Such responses include geometrical and detection physics components of the system matrix. The response is parameterized to correct point source location and to smooth projection noise. Special attention was paid to span concepts of HiRez scanner. The projection operator for iterative reconstruction was constructed, taking into account estimated response parameters. The computer generated and acquired data were used to compare reconstruction obtained by the HiRez standard software and produced by better modeling. Results showed that the better resolution and noise property can be achieved.","Positron emission tomography,
Image reconstruction,
Robot kinematics,
Technological innovation,
Physics,
Software standards,
Solid modeling,
Iterative methods,
Position measurement,
Parameter estimation"
Sammon's nonlinear mapping using geodesic distances,"Sammon's nonlinear mapping (NLM) is an iterative procedure to project high dimensional data into low dimensional configurations. This paper discusses NLM using geodesic distances and proposes a mapping method GeoNLM. We compare its performance through experiments to the performances of NLM and Isomap. It is found that both GeoNLM and Isomap can unfold data manifolds better than NLM. GeoNLM outperforms Isomap when the short-circuit problem occurs in computing the neighborhood graph of data points. In turn, Isomap outperforms GeoNLM if the neighborhood graph is correctly constructed. These observations are discussed to reveal the features of geodesic distance estimation by graph distances.","Principal component analysis,
Computer science,
Geophysics computing,
Indexing,
Pattern analysis,
Data mining,
Information retrieval,
Optimization methods,
Eigenvalues and eigenfunctions,
Covariance matrix"
Effective network monitoring,"Various network monitoring and performance evaluation schemes generate considerable amount of traffic, which affects network performance. In this paper we describe a method for minimizing network monitoring overhead based on shortest path tree (SPT) protocol. We describe two different variations of the problem: the A-problem and the E-problem, and show that there is a significant difference between them. We prove that finding optimal solutions is NP-hard for both variations, and propose a theoretically best possible heuristic for the A-problem and three different heuristics for the E-problem, one of them being also theoretically best possible. We show that one can compute in polynomial time an O(ln|V|)-approximate solution for each of these problems. Then, we analyze the performance of our heuristics on large graphs generated using Waxman and power-law models as well as on real ISP topology maps. Experiment results show more than 80% improvement when using our heuristics on real topologies over the naive approaches","Telecommunication traffic,
Network topology,
Computerized monitoring,
Computer science,
Protocols,
Polynomials,
Performance analysis,
Power generation,
Bandwidth,
Knowledge management"
Shrinking: another method for surface reconstruction,"We present a method to reconstruct a pipe or a canal surface from a point cloud (a set of unorganized points). A pipe surface is defined by a spine curve and a constant radius of a swept sphere, while a variable radius may be used to define a canal surface. In this paper, by using the shrinking and moving least-squares methods, we reduce a point cloud to a thin curve-like point set which will be approximated to the spine curve of a pipe or canal surface. The distance between a point in the thin point cloud and a corresponding point in the original point set represents the radius of the pipe or canal surface.","Surface reconstruction,
Irrigation,
Clouds,
Rough surfaces,
Surface roughness,
Solid modeling,
Optimization methods,
Shape,
Surface fitting,
Computer science"
Why one example is not enough for an image query,"For over a decade, query-by-one-example has been a popular query paradigm for multimedia information retrieval. We show, by analyzing feature-to-semantics mapping, that such a paradigm cannot realistically lead to a scalable, satisfactory query performance. More specifically, we cluster a small image dataset based on the images' perceptual features, and show that these image clusters are not coherent to the semantic categories of the images. Though some image categories are well separated from the others in the input space formed by the perceptual features, most categories are colocated in more than one cluster. For a query-concept that is mixed with others in a number of clusters, the query-by-one-example paradigm simply lacks information to identify clearly the target query-concept, and hence cannot achieve satisfactory query results.",
Experience-based admission control (EBAC),"Classical admission control approaches take either descriptor or measurement based information about the traffic into account without relating them to each other. We propose a experience-based AC (EBAC) which uses an empirical percentile of the effective reservation utilization to determine a suitable overbooking factor. In this paper, we show the impact of different measurement time scale resolutions and different quantiles on the performance of the system. We propose aging mechanisms for statistic collection to make the system adaptive to traffic mixes that change over time. We illustrate their effectiveness by simulation results.",
Web services-based data management: evaluating the performance of UDDI registries,"With the advancement of Web services technologies, online businesses have the ability to offer their capabilities to larger, lesser known communities of potential collaborators. Universal Description, Discovery, and Integration (UDDI) specification and supporting technologies support open frameworks for businesses to store, advertise and retrieve pertinent services. Many researchers investigate approaches that ubiquitously create higher-level processes by composing services discovered in and retrieved from UDDI registries. However, there are few studies that consider the impact of registry performance on future service automation. This work focuses on evaluating the performance of UDDI registries considering variability and concurrent load of publish and inquiry requests.","Web services,
Concurrent computing,
Large-scale systems,
Web server,
Computer science,
Collaboration,
Automation,
Software engineering,
Companies,
Distributed databases"
Model selection of SVMs using GA approach,"A new automatic search methodology for model selection of support vector machines, based on the GA-based tuning algorithm, is proposed to search for the adequate hyperparameters of SVMs. In our method, each chromosome indicates a group of hyperparameters, and the population is a collection of chromosomes. Experimental results show that our method performs superiorly on time cost, performance and stability. Our algorithm requires only the evaluation of an objective function to guide its search with no additional derivative or auxiliary knowledge required. In addition, the encoding of chromosomes makes the implementation of multiple hyperparameters tuning simpler.","Support vector machines,
Kernel,
Biological cells,
Costs,
Genetic algorithms,
Computer science,
Electronic mail,
Stability,
Encoding,
Machine learning"
Spectral methods for cross correlations of geometric sequences,"Families of sequences with low pairwise shifted cross correlations are desirable for applications such as code-division multiple-access (CDMA) communications. Often such sequences must have additional properties for specific applications. Several ad hoc constructions of such families exist in the literature, but there are few systematic approaches to such sequence design. We introduce a general method of constructing new families of sequences with bounded pairwise shifted cross correlations from old families of such sequences. The bounds are obtained in terms of the maximum cross correlation in the old family and the Walsh transform of certain functions.","Multiaccess communication,
Galois fields,
Associate members,
Computer science,
Codes"
Compression of words over a partially commutative alphabet,"Concurrency is a fundamental concept in computer science which is concerned with the study of systems involving multiple processes. The order of events in a concurrent system is unpredictable because of the independence of events occurring in the individual processes. Trace theory is a successful model for the execution of concurrent processes which employs congruence classes of words over partially commutative alphabets. These congruence or interchange classes generalize the more familiar notions of strings and type classes. Motivated by recent work in the areas of program profiling and compression of executable code, we consider a rate distortion problem in which the objective is to reproduce a string which is equivalent to the original string. This leads to a generalization of Kolmogorov complexity and a new graph entropy called the interchange entropy. We provide some of the basic properties of the interchange entropy. We also consider some universal compression schemes for this problem and show that for a large collection of dependence alphabets we can asymptotically attain the interchange entropy.",
A dynamic resource reservation scheme with mobility prediction for wireless multimedia networks,"To reduce the call blocking probability (CBP) and call dropping probability (CDP) of real-time and non-real-time traffic in wireless multimedia networks, resource reservation is frequently employed to achieve both the above-mentioned goal as well as to avoid long latency of path rebuilding. However, pure resource reservation may lead to inefficient resource utilization and increase of CBP and CDP due to frequent handoff in wireless networks. To reduce unnecessary resource reservation, the prediction of moving direction can be incorporated to enhance the pure resource reservation. Therefore, a dynamic resource reservation scheme with mobility prediction is proposed in this paper. In this scheme, a resource request with available range rather than a fixed resource request is specified by each traffic to make the reservation scheme flexible. Using simulation results, we show that mobility prediction can dynamically adjust resource reservation and effectively enhance system resource utilization. Moreover, the available interval of resource request gives flexibility to resource reservation which also improves the system performance.","Quality of service,
Resource management,
Computer science,
Electronic mail,
Microelectronics,
Technological innovation,
Computer industry,
Telecommunication traffic,
Delay,
Wireless networks"
Techniques for providing software engineering education to working professionals,"As the pace of change of technology increases, software engineering professionals are experiencing a pressure to maintain currency in recent developments in the software engineering field. Providing effective education to this population of working professionals is of increasing importance. This paper is an initial investigation of effective education techniques that may be employed when teaching working professionals. The paper will attempt to answer two main questions: 1) What techniques does prior research indicate might be effective in teaching working professionals? 2) How might these techniques be employed in software engineering courses for working professionals? Survey-based feedback from prior software courses taught to working professionals will be used to provide baseline insight into techniques in current courses and to suggest directions for future efforts. This feedback is taken from multiple courses taught to adult working professional students spanning several semesters and two U.S. institutions.","Software engineering,
Education,
Feedback,
Economic indicators,
Educational institutions,
Computer industry,
Time factors,
Timing"
Spatial multipath location aided ad hoc routing,"Mobile ad-hoc networks are infrastructure-free networks of mobile nodes that communicate with each other wirelessly. There are several ad hoc routing algorithms at present that utilize position information (usually in two dimensional terms) to make routing decisions at each node. We consider routing algorithms that use 3D positional information, particularly a hybrid extension to LAR that works in 3D. We propose a new hierarchical, zone-based 3D routing algorithm, based on GRID by Liao, Tseng and Sheu (2001). Our new algorithm called ""hyper-GRID"" is a hybrid algorithm that uses multipath routing (alternate path caching) in 3D. We propose replacing LAR with multipath LAR (MLAR) in GRID. We have implemented MLAR and are validating MLAR through simulation using ns-2 and studying its efficiency, scalability and other properties. We use a random waypoint mobility model and compare our MLAR approach versus LAR, AODV and AOMDV in both 2D and 3D for a range of traffic and mobility scenarios","Routing,
Ad hoc networks,
Scalability,
Floods,
Computer science,
Educational institutions,
Testing,
Traffic control,
Costs,
Bandwidth"
Security analysis of mandatory access control model,"Mandatory access control (MAC) model is an important security model. Based on the lattice model of security level and Bell-LaPadula model the definition of MAC security model is formally described in detail. The equivalent MAC security model described by colored Petri nets (CPN) is proposed. According to the state reachability graph, four security properties of MAC security model, i.e. the access temporal relations, the reachability of objects when subject accesses them, hidden security holes due to the dynamic security level, the indirect reasoning of confidential information flow between different objects, are explored at length. In addition, an example of the security model is illustrated and the conclusions show that the security model based on Petri nets is not only a concise graphic analysis method, but also suited to be formally verified. This model can efficiently improve the whole security policies during the system security design and implementation.","Access control,
Information security,
Computer security,
Petri nets,
Lattices,
Information systems,
Authorization,
Information analysis,
Computer science,
Graphics"
Affective factors and student achievement: a quantitative and qualitative study,"The affective domain can be used to support the internalization of cognitive content and foster the development of curriculum and industry-related interests, attitudes, values, and practices. During a two-year period, using validated instruments, the authors measured student interest, value, effort, perceived competence, lack of pressure, student-peer belonging, and student-faculty belonging. Initial findings included a positive correlation between each affective factor and course grade, a significant decrease in the levels of affective factors over the course term, and a lessening of those decreases with the use of specific affective objectives and instructional strategies. The current study built upon these initial results by incorporating new quantitative and qualitative data for each affective factor. The paper reports on the results of these analyses and offers practical suggestions and instructional guidelines based upon the findings. These findings appear to be broadly applicable throughout our curriculum and could extend to other science, mathematics, engineering, and technology disciplines.","Computer industry,
Guidelines,
Mathematics,
Educational institutions,
Mobile computing,
Instruments,
Pressure measurement,
Programming profession,
Educational programs,
Communication standards"
"Finding ""early"" indicators of UML class diagrams understandability and modifiability","Given the relevant role that models obtained in the early stages play in the development of OO systems, in the recent years special attention has been paid to the quality of such models. Adhering to this fact, the main objective of this work is to obtain ""early"" indicators of UML class diagrams understandability and modifiability. These indicators will allow OO designers to improve the quality of the diagrams they model and hence contribute improving the quality of the OO systems, which are finally delivered. The empirical data were obtained through a controlled experiment and its replication we carried out for obtaining prediction models of the Understandability and Modifiability Time of UML class diagrams based on a set of metrics previously defined for UML class diagrams structural complexity and size. The obtained results, reveal that the metrics that count the number of methods (NM), the number of attributes (NA), the number of generalizations (NGen), the number of dependencies (NDEP), the maximum depth of the generalization hierarchies (MaxDIT) and the maximum height of the aggregation hierarchies (MaxHAgg) could influence the effort needed to maintain UML class diagrams.","Unified modeling language,
Predictive models,
Software quality,
Computer science,
Size control,
Size measurement,
Software measurement,
Software maintenance,
Computer architecture,
Spine"
Reliability models and evaluation of internal BGP networks,"The performance of global Internet communication is significantly influenced by the reliability and the stability of Internet routing systems, especially the border gateway protocol (BGP), the de facto standard for inter-domain routing. In this paper, we investigate the reliability of BGP sessions and the internal BGP (IBGP) networks in the environment of unreliable physical and routing layers. The reliability analysis of IBGP networks is difficult, because IBGP sessions may be correlated to each other by the shared underlying physical links and TCP enables IBGP sessions to tolerate certain level of network failures. In this paper, we first investigate the failure probability of IBGP sessions and its relation to BGP timers and TCP retransmission behaviors. The result of this investigation is a simple modification of TCP that increases the robustness of IBGP sessions significantly. Second, we present a novel reliability model to measure the resilience of the whole IBGP networks. This model is of great importance for studying the function loss of IBGP operations and it also provides the theory basis for IBGP network optimization in terms of reliability",
Autonomous query-driven index mining,"Index tuning as part of database tuning is the task of selecting and creating indexes with the goal of reducing query processing times. However, in dynamic environments with various ad-hoc queries it is difficult to identify potentially useful indexes in advance. We investigate an approach addressing this problem by deciding about index creation automatically at runtime in order to speed up processing of subsequent queries. We present a cost model taking into account the benefits of indexes for an evolving query workload and discuss strategies for choosing indexes to be created in a space-limited environment.","Indexes,
Query processing,
Costs,
Transaction databases,
Computer science,
Runtime,
Business,
Automation,
Delay,
Throughput"
Car plate character extraction under complicated environment,"License plate recognition (LPR) system has many applications. Character extraction is a key step in LPR system. The proposed character extraction method in this paper is mainly according to plate's geometrical characteristic and supplementary with the scan of projection image. Subsequently, locate the plate character sequence, then do more jobs on false characters elimination and verify the validity of extracted characters. The above methods lead to an ideal character extraction result, which leads higher recognition accuracy of the LPR system","Character recognition,
Licenses,
Image edge detection,
Computer science,
Vehicles,
Software systems,
Image recognition,
Labeling,
Application software,
Monitoring"
Assignment testers: towards a combinatorial proof of the PCP-theorem,"In this work, we look back into the proof of the PCP theorem, with the goal of finding new proofs that are ""more combinatorial"" and arguably simpler. For that, we introduce the notion of an assignment tester, which is a strengthening of the standard PCP verifier, in the following sense. Given a statement and an alleged proof for it, while the PCP verifier checks correctness of the statement, the assignment-tester checks correctness of the statement and the proof. This notion enables composition that is truly modular, i.e., one can compose two assignment-testers without any assumptions on how they are constructed. A related notion was independently introduced in (Ben-Sasson et. al. STOC 04). We provide a toolkit of (non-trivial) generic transformations on assignment testers. These transformations may be interesting in their own right, and allow us to present the following two main results: 1. The first is a new proof of the PCP theorem. This proof relies on a rather weak assignment tester given as a ""black box"". From this, we construct combinatorially the full PCP. An important component of this proof is a new combinatorial aggregation technique (i.e., a new transformation that allows the verifier to read fewer, though possibly longer, ""pieces"" of the proof). An implementation of the black-box tester can be obtained from the algebraic proof techniques that already appear in L. Babai et al., 1991 and U. Feige et al., 1991. Obtaining a combinatorial implementation of this tester would give a purely combinatorial proof for the PCP theorem, which we view as an interesting open problem. 2. Our second construction is a ""standalone"" combinatorial construction showing NP /spl sube/ PCP (S. Arora et al., 1998). This implies, for example, that approximating max-SAT is quasi-NP-hard. This construction relies on a transformation that makes an assignment tester ""oblivious"": so that the proof locations read are independent of the statement that is being proven. This eliminates, in a rather surprising manner, the need for aggregation in a crucial point in the proof.","Testing,
Computer science,
Polynomials,
National electric code,
Career development,
Mathematics,
Heart,
Circuits"
Study of flow in tip-clearance turbomachines using large-eddy simulation,"A powerful computational technique, large-eddy simulation, helps researchers study the detailed flow dynamics in the tip-gap region of hydraulic turbomachines. LES also helps researchers investigate ways to mitigate undesirable effects, such as cavitation, which can lead to reduced performance, increased noise, and structural vibration and erosion.",
Communication breakdown: analyzing CPU usage in commercial Web workloads,"There is increasing concern among developers that future Web servers running commercial workloads may be limited by network processing overhead in the CPU as 10Gb Ethernet becomes prevalent. We analyze CPU usage of real hardware running popular commercial workloads, with an emphasis on identifying networking overhead. Contrary to much popular belief, our experiments show that network processing is unlikely to be a problem for workloads that perform significant data processing. For the dynamic Web serving workloads we examine, networking overhead is negligible (3% or less), and data processing limits performance. However, for Web servers that serve static content, networking processing can significantly impact performance (up to 25% of CPU cycles). With an analytical model, we calculate the maximum possible improvement in throughput due to protocol offload to be 50% for the static Web workloads.","Electric breakdown,
Data processing,
Protocols,
Web server,
Analytical models,
Throughput,
Network servers,
Java,
Intelligent networks,
Computer science"
A rate-constrained key-frame extraction scheme for channel-aware video streaming,"The paper presents an adaptive rate-constrained key-frame selection scheme for channel-aware realtime video streaming applications. The proposed method dynamically determines the target number of key-frames by estimating the channel conditions according to feedback information. A two-step sequential key-frame selection scheme is then utilized to select a target number of key-frames by first finding the optimal allocation of the key-frame budget among the video shots in a video clip using an analytical model, and then selecting most representative key-frames in each shot according to the allocation. The feature information used for key-frame selection is extracted offline and stored in the server as metadata for realtime streaming and transcoding. Experimental results show that the proposed method can achieve good performance with acceptable complexity.",
Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects,"There has been extensive research in the area of data mining over the last decade, but relatively little research in algorithmic mining. Some researchers shun the idea of incorporating explicit knowledge with a Genetic Program environment. At best, very domain specific knowledge is hard wired into the GP modeling process. This work proposes a new approach called the Genetically Engineerable Evolvable Program (GEEP). In this approach, explicit knowledge is made available to the GP. It is considered breadth-based, in that all pieces of knowledge are independent of each other. Several experiments are performed on a NASA-based data set using established equations from other researchers in order to predict software defects. All results are statistically validated.","Genetic engineering,
Genetic programming,
Machine learning algorithms,
Software algorithms,
Computer science,
Lakes,
Software performance,
Optical filters,
Software engineering,
Data mining"
Self-grading in a project-based software engineering course,"The Web application design and development course offered at Rensselaer at Hartford uses a project-based approach where students construct a Web application of their own choosing. For the past two years, the instructor of this course has used a student self-grading approach where, as part of defining the requirements for the project, students also define grade specifications for project grades of 'A', 'B', and 'C'. We discuss the motivation and approach to self grading and reports on a survey-based study used to determine student attitude toward the self-defined project and the self-grading approaches. Results of the survey indicate that students are moderately satisfied with the self-grading approach when used in conjunction with the student-defined project.","Software engineering,
Application software,
Concrete,
Position measurement,
Ethics,
Web page design,
Service oriented architecture,
Instruments,
Educational products,
Product design"
Using a victim buffer in an application-specific memory hierarchy,"Customizing a memory hierarchy to a particular application or applications is becoming increasingly common in embedded system design, with one benefit being reduced energy. Adding a victim buffer to the memory hierarchy is known to reduce energy and improve performance on average, yet victim buffers are not typically found in commercial embedded processors. One problem with such buffers is, while they work well on average, they tend to hurt performance for many applications. We show that a victim buffer can be very effective if it is considered as a parameter in designing a memory hierarchy, like the traditional cache parameters of total size, associativity, and line size. We describe experiments on PowerStone and MediaBench benchmarks, showing that having the option of adding a victim buffer to a direct-mapped cache can reduce memory-access energy by a factor of 3 in some cases. Furthermore, even when other cache parameters are configurable, we show that a victim buffer can still reduce energy by 43%. By treating the victim buffer as a parameter, meaning the buffer can be included or excluded, we can avoid performance overhead of up to 4% on some examples. We discuss the victim buffer in the context of both core-based and pre-fabricated platform based design approaches.",
Intrusion detection solution to WLANs,"With the increasing popularity of the wireless network, the security issue for mobile users could be even more serious than it can be expected. Here it is need to search for new architecture and mechanisms to protect the wireless networks and mobile computing application. This paper focuses on intrusion detection and security consideration in wireless local area networks (WLANs) and also presents intrusion detection for WLANs, which gives a wireless intrusion detection system and its architecture consideration. A wireless IDS is similar to a standard, wired IDS, but has additional deployment requirements as well as some unique features specific to WLAN intrusion and misuse detection. Meanwhile, we discuss some issues with wireless networks and Intrusion detection solution to wireless networks.","Intrusion detection,
Wireless LAN,
Wireless networks,
Communication system security,
Monitoring,
Computer science,
Computer architecture,
Mobile computing,
Computer networks,
Mobile communication"
Filtering of color map images by context tree modeling,"We propose a method for filtering raster map images by context tree modeling. This is a two-pass method. At the first pass, the filter utilizes statistical information about the image spatial structure and stores the statistics in a tree structure. At the second pass, these statistics are used in actual filtering to calculate the probability of the current pixel in its neighborhood. We test this method on a set of map images. We use different noise models to evaluate the performance of the proposed filter. Finally we compare the performance results for our method with the vector median filter. The proposed method does not destroy the object borders and outperforms the vector median filter for a moderate level of noise.","Context modeling,
Information filtering,
Information filters,
Probability,
Statistics,
Color,
Arithmetic,
Computer science,
Tree data structures,
Testing"
Architecture and first prototype tests of the Clear-PEM electronics systems,"The Clear-PEM detector system is a compact positron emission mammography scanner with about 12,000 channels aiming at high sensitivity and good spatial resolution. Front-end, trigger and data acquisition electronics are crucial components of this system. Front-end system is implemented as a data-driven synchronous design that identifies and multiplexes the analogue signals (channels) whose associated energy is above a pre-defined threshold. The trigger and data acquisition logic uses digitized front-end data streams and computes the pulses amplitude and timing. Based on this information it generates a coincidence trigger signal that is used to initiate the conditioning and transfer processes of the corresponding data towards the data acquisition computer. To minimize dead-time, data acquisition electronics architecture makes extensive use of pipeline processing structures and de-randomizer memories with multi-event capacity. The system operates at 100 MHz clock frequency and is capable to sustain a data acquisition rate of 1 million events per second with efficiency above 95%, under a total single photon background rate of 10 MHz. The basic component of the front-end system is a low-noise amplifier-multiplexer chip presently under development The off-detector system is designed around a dual-bus crate backplane for fast intercommunication among system modules. The trigger and data acquisition logic is implemented in large FPGAs with 4 million gates. Monte Carlo simulation results of the trigger efficiency, as well as results of hardware simulations are presented, showing the correctness of the design and implementation approach.","Electronic equipment testing,
Prototypes,
System testing,
Data acquisition,
Signal processing,
Logic,
Detectors,
Radioactive decay,
Mammography,
Spatial resolution"
BlobTree trees,"In recent years several methods for modeling botanical trees have been proposed. The geometry and topology of tree skeletons can be well described by L-systems; however, there are several approaches to modeling smooth surfaces to represent branches, and not all of the observed phenomena can be represented by current methods. Many tree types exhibit nonsmooth features such as branch bark ridges and collars. In this research a hierarchical implicit modeling system is used to produce models of branching structures that capture smooth branching, branch collars and branch bark ridges. The BlobTree provides several techniques to control the combination of primitives, allowing both smooth and nonsmooth effects to be intuitively combined in a single blend volume. Irregular effects are implemented using precise contact modeling, constructive solid geometry and space warping. We show that smooth blends can be obtained, without noticeable bulging, using summation of distance based implicit surfaces. L-systems are used to create the branching structure allowing botanically based simulations to be used as input","Surface fitting,
Geometry,
Convolution,
Solid modeling,
Topology,
Skeleton,
Phase change materials,
Tree graphs,
Computer graphics,
Computer science"
Semi-supervised mixture-of-experts classification,"We introduce a mixture-of-experts technique that is a generalization of mixture modeling techniques previously suggested for semi-supervised learning. We apply the bias-variance decomposition to semi-supervised classification and use the decomposition to study the effects from adding unlabeled data when learning a mixture model. Our empirical results indicate that the biggest gain from adding unlabeled data comes from the reduction of the model variance, whereas the behavior of the bias error term heavily depends on the correctness of the underlying model assumptions.","Semisupervised learning,
Computer science,
Machine learning,
Performance analysis,
Error correction,
Information filtering,
Drugs,
Costs,
Machine learning algorithms,
Degradation"
Stop-word removal algorithm for Arabic language,"Summary form only given. We have designed and implemented an efficient stop-word removal algorithm for Arabic language based on a finite state machine (FSM). An efficient stop-word removal technique is needed in many natural language processing application such as: spelling normalization, stemming and stem weighting, Question answering systems and in information retrieval systems (IR). Most of the existing stop-word removal techniques bases on a dictionary that contains a list of stop-word, it is very expensive, it takes too much time for searching process and required too much space to store these stop-words. The new Arabic removal stop-word technique has been tested using a set of 242 Arabic abstracts chosen from the Proceedings of the Saudi Arabian National Computer conferences, and another set of data chosen from the holy Q'uran, and it gives impressive results that reached approximately to 98%.","Computational Intelligence Society,
Computer science,
Algorithm design and analysis,
Automata,
Natural language processing,
Information retrieval,
Dictionaries,
Testing,
Abstracts"
Grids of grids of simple services,"The paper describes how to build systems from service-oriented grids that let you build new grids by composing and adapting existing collections (libraries) of grids. The paper also suggests some best practices for deciding how to architect services and package systems. Often we consider grids as providing seamless access to a set of resources. The author agrees but also proposes that the resulting grid architecture can consist of many small grids. This reflects the many different overlapping community types and resource collections that naturally form individual grids. Each individual grid can have a seamless elegant environment - this even could be a criterion for defining basic grids - but a composite grid would amalgamate multiple subgrids and provide a resultant heterogeneous environment. In other words, we don't want just a few grids but a large number composed, divided, and overlapped to support dynamic communities and requirements.","Packaging,
Java,
Software packages,
Distributed computing,
Algorithms,
Software libraries,
Software systems,
Security,
Delay,
Computer interfaces"
Global vs. local model checking: a comparison of verification techniques for infinite state systems,"Global and local model checking procedures follow radically different paradigms: while global approaches are based on fixpoint computation, local approaches are related to deduction and induction. For the verification finite state systems, this may result in different runtimes. For the verification of infinite state systems, however the differences are far more important. Since most problems are undecidable for such systems, it may be the case that one of the procedures does not terminate. In this paper we compare global and local procedures for model checking p-calculus properties of infinite state systems. In particular we show how they can benefit from each other and present appropriate extensions.","Data structures,
Runtime,
Real time systems,
Communication system control,
Control systems,
Power system modeling,
Computer science,
Protocols,
Hardware,
Circuits"
Introducing EAI and service components into process management,"Workflow has grown to be a primary method in managing processes. Improving the ability to support heterogeneous applications for workflow management system (WFMS) will promote the more popular application of workflow technology in wide areas. Moreover, as e-service is becoming as a focus in business process management, workflow technology is facing the challenge to support e-services in a proper way. We import the enterprise application integration (EAI) technology into workflow enactment service to support heterogeneous applications in a uniform way; and also we introduce some service-related components into workflow reference model to support e-services in processes. A WFMS prototype based on our extended workflow model is also presented. Semantic knowledge is used in this prototype to make service registration and discovery.","Application software,
Technology management,
Search engines,
Workflow management software,
Prototypes,
Large-scale systems,
Internet,
Monitoring,
Educational institutions,
Computer science"
Towards understanding the nature of high frequency backscatter from cells and tissues: an investigation of backscatter power spectra from different concentrations of cells of different sizes,"During cell death, a series of structural changes occur within the cell. We have shown that cell ensembles and tissues undergoing structural changes associated with various cell death pathways can be detected using high-frequency ultrasound. In our effort to understand better the nature of backscatter from collections of cells (which emulate tissues), we have collected raw RF backscatter data from cells of two different sizes (human acute myeloid leukemia, AML, cells and transformed prostate cells) in solutions for a series of concentrations or in pellet form. It was found that the backscatter power (as measured by the mid-band fit) increased by /spl sim/3 dB for both cell types in dilute solutions for which the volumetric concentration was doubled for a specific range of cell concentrations (which was dependent on cell size). In pellet form, the backscatter power from the prostate cell pellets was /spl sim/12-14 dB greater than the AML cell pellets. A comparison of the spectral slopes also strongly suggests a change in the scattering source contributions when the cells are in pellets: the spectral slope was negative for all concentrations for prostate cells imaged at 40 MHz, but positive when measured in pellets. This is consistent with an increased contribution to the backscatter of smaller sized scatterers (such as the cell nucleus) that manifests itself only when the cells are in pellets but not in solution. These data are compared to theoretical predictions and their significance discussed.","Backscatter,
Frequency,
Scattering,
Ultrasonic imaging,
Power measurement,
Ultrasonic transducers,
Mathematics,
Physics,
Computer science,
Biomedical imaging"
Medical and information technologies converge,"Information technology offers medical science tools to collect, process, store, and communicate clinical data. Healthcare institutions have adapted standards-based data communication technologies that allow easy implementation of communications infrastructure. As clinical and information technologies have converged, two trends have emerged: the widespread use of commercial off-the-shelf hardware and software and the use of standards-based communication technologies. Technical support for these complex systems requires an integrated, ""end-to-end"" view and staff who are knowledgeable of both clinical and computer technologies. In this article, examples of new computerized medical devices are discussed as well as the support and support staff implications of the ever-growing influence of IT on clinical systems.","Biomedical imaging,
Medical diagnostic imaging,
Medical services,
Hardware,
Communications technology,
Information systems,
Clinical diagnosis,
Microprocessors,
Data communication,
Personal communication networks"
Residual vibration suppression using initial value compensation for repetitive positioning,"This paper presents a novel residual vibration suppression methodology for the repetitive fast-response and high-precision positioning in machine tool drives. In sequential positioning motions, as the interval period of position references becomes shorter, the residual vibration in response due to undesired initial values deteriorates the positioning accuracy, since the positioning controller is generally designed on the condition that initial state variables are zero. In this research, an initial value compensation (IVC) approach is proposed under the theoretical study on effects of the initial values on the position transient response. The IVC can appropriately assign poles and zeros of the transfer characteristic of position output for the initial values by applying an additional input corresponding to the initial state variables, enabling the response to be residual vibration free. The desired positioning performance, as a result, can be achieved in repetitive motions with arbitrary interval period. The effectiveness of the proposed compensation has been verified by numerical simulations and experiments using a positioning device of industrial machine tools.","Machine tools,
Frequency,
Vibration control,
Servomechanisms,
Machinery production industries,
Servomotors,
Computer science,
Paper technology,
Drives,
Motion control"
Using invisible watermarks to protect visibly watermarked images,"Physical visible watermarks have been widely used for centuries. Now digital visible watermarks such as electronic logos find their applications in digital library, video broadcasting, and other multimedia services. Several visible watermarking techniques have been proposed in the literature, and meanwhile, some problems with visible watermarks are also under investigation. Among these problems, watermark removal and unauthorized insertion are two major concerns. We propose using an invisible watermark in visibly watermarked images to overcome these problems. When a visibly watermarked image is in question, the invisible watermark can provide appropriate ownership information. We first investigate what kind of invisible watermark is needed, and then, focus on the details of the invisible watermarking technique. The experiments have shown that the proposed algorithm can provide a very effective protection for visibly watermarked images.","Watermarking,
Protection,
Discrete wavelet transforms,
Partial response channels,
Automatic control,
Computer science,
Cities and towns,
Application software,
Software libraries,
Multimedia communication"
Constant modulus algorithm aided soft decision-directed blind space-time equalization for SIMO channels,"Smart antenna aided broadband beamforming plays an increasingly important role in wireless communications. The paper investigates blind space-time equalization/equalizers (STE) designed for single-input multi-output (SIMO) systems. Specifically, the constant modulus algorithm (CMA) and a soft decision-directed (SDD) scheme, originally derived for low-complexity blind equalization of a single-input single-output (SISO) channel, are combined for employment in the SIMO scenario.",
A teaching prototype for educating IT security engineers in emerging environments,"Many enterprises today do not have an information security program or anyone on staff who can help them manage the escalating security risks. Computer science, information technology (IT) and information systems see an urgent need to train students in Internet security but have limited or untrained resources available to enhance their curricula. Perceiving that students best learn information security by doing it, the IT program at Brigham Young University elected to initiate a security emphasis in its instruction by creating security courses and an experimental security laboratory. Students collaboratively prepare lectures and labs as part of an IT security teaching model. Their enthusiasm in using the security lab as a playground follows a pedagogical approach of active learning and persistent student-led teams. Students have architected, administered and operated two lab concepts, namely, (a) an isolated Sandbox security tab, and (b) a prototype network that could serve as a simple model for a small university and which does have external Internet connectivity that offers features for extensive wired and wireless deployment. Students created the security best practices and student security team used to manage this on-going, active experimental environment that served as the test-bed for evaluating security projects that deal with network hardening, vulnerability and intrusion prevention issues. This security teaching model served, and continues to serve, as a prototype for facilitating the education and preparation of urgently needed security engineers in emerging environments worldwide.","Education,
Prototypes,
Design engineering,
Information security,
Risk management,
Computer science,
Information technology,
Management information systems,
Internet,
Computer security"
Forming agents for business process orchestration,"Distributed component-based services and semantic Web services are promising technologies for next generation inter-enterprise integration. The dynamic nature of this domain presents a complex problem for potential software agent-based approaches that support this cross-organizational integration. Currently, there are few studies that measure the impact of the dynamic environmental effects on service composition. On an on-going basis, composite services or workflow processes of Web services may be constantly changing in terms of responsiveness of services, accessibility of services and their meta-information, business process schema changes, etc. This paper describes an approach, model, and supporting software toward the efficient formation of agent teams and interaction protocols for business process orchestration in response to certain environmental conditions.","Web services,
Simple object access protocol,
Environmental management,
Computer science,
Semantic Web,
Current measurement,
Delay,
Access protocols,
Electronic commerce,
Virtual enterprises"
An approach to compile-time task scheduling in heterogeneous computing systems,"List-based scheduling is generally accepted as an attractive approach to static task scheduling since it combines low complexity with good results. Although a large number of scheduling heuristics have been presented in the literature, most of them target only homogeneous computing systems and almost none of them target heterogeneous communication systems. In this paper we present a simple scheduling algorithm based on list-scheduling and task-duplication on a bounded number of heterogeneous machines called Heterogenous Critical Nodes with Fast Duplicator (HCNFD). The suggested algorithm supports both heterogeneous computation and communication environments. The analysis and experiments have shown that HCNFD outperforms on average all other higher complexity algorithms.","Processor scheduling,
Scheduling algorithm,
Distributed computing,
High performance computing,
Computer science,
Algorithm design and analysis,
Computer applications,
Computational efficiency,
Cost function,
Performance evaluation"
Robust tree-based multicasting in ad hoc networks,"We examine on-demand multicasting in ad hoc networks. We study a wide range of simulation scenarios and identify key limitations of MAODV. Based on our findings, we propose a number of changes in MAODV, and call the resulting protocol robust multicasting in ad hoc networks using trees (ROMANT). We compare ROMANT to MAODV and ODMRP, for a wide range of simulation scenarios. Our results indicate that ROMANT effectively eliminates the limitations of MAODV. Moreover, it provides comparable or better packet delivery ratio than ODMRP at only a fraction of the overhead incurred by ODMRP.","Robustness,
Intelligent networks,
Ad hoc networks,
Multicast protocols,
Telecommunication traffic,
Routing protocols,
Computer science,
Computational modeling,
Mobile ad hoc networks,
Mobile communication"
Dynamic Resource Selection For Service Composition in The Grid,"While numerous efforts have focused on service composition in the Grid environment, service selection among similar services from multiple providers has not been addressed. In particular, all service composition work done so far are based on a given selection of services under a well set environment. As a result, uncertainty (e.g., server load, network traffic, computation time of the services due to changing memory and other unexpected conditions) under a real, dynamic environment has never been considered. This paper prototypes the service selection under a Grid environment and proposes an uncertainty framework to address the issue. Experimental results show that our considerations are valid and our preliminary solution works well in our Globus Grid network.","Web services,
Uncertainty,
Computer science,
Network servers,
Telecommunication traffic,
Prototypes,
Logic programming,
Costs,
Computer networks,
Ontologies"
Ian Foster on recent changes in the grid community,,"Web services,
Resource management,
Convergence,
Laboratories,
Computer science,
Open source software,
Distributed computing,
Data security,
Books,
Software tools"
An XOR based Reed-Solomon algorithm for advanced RAID systems,"In this paper, a simple codec algorithm, based on Reed-Solomon (RS) codes, is proposed for erasure correcting in RAID (redundant array of independent disks) level 6 systems. Unlike conventional RS codes, this scheme, with a mathematical reduction method, called reduced static-checksum table approach, could improve coding performance, including encoding and decoding procedures. This scheme uses current industrial RAID-5 controllers as well as the regular hardware, and is without extra cost in adding any new equipment. Moreover, our algorithm is able to expand to correct multiple failed-disks while others, EvenOdd codes for example, cannot do so. Also, this scheme performs all computations with only simple exclusive-OR (XOR) operators, the same as EvenOdd codes. For most RAID architectures, this new XOR-based RS code could adapt to implementation in terms of reliability, flexibility and lower cost.","Reed-Solomon codes,
Codecs,
Costs,
Error correction,
Encoding,
Decoding,
Hazards,
Control systems,
Computer science,
Electrical equipment industry"
Surface normals and height from non-Lambertian image data,"It is well known that many surfaces exhibit reflectance that is not well modelled by Lambert's law. This is the case not only for surfaces that are rough or shiny, but also those that are matte and composed of materials that are particle suspensions. As a result, standard Lambertian shape-from-shading methods cannot be applied directly to the analysis of rough and shiny surfaces. In order to overcome this difficulty, we consider how to reconstruct the Lambertian component for rough and shiny surfaces when the object is illuminated in the viewing direction. To do this we make use of the diffuse reflectance models described by Oren and Nayar, and by Wolff. Our experiments with synthetic and real-world data reveal the effectiveness of the correction method, leading to improved surface normal and height recovery.","Rough surfaces,
Surface roughness,
Surface topography,
Reflectivity,
Surface reconstruction,
Brightness,
Light scattering,
Computer science,
Suspensions,
Image reconstruction"
Facial expression features extraction based on Gabor wavelet transformation,"Features extraction is a key step in facial expression recognition system. In order to extract facial expression features that are subject-independent and robust to illumination variety, this paper introduces a facial expression features extraction algorithm. Given a still image containing facial expression information, preprocessors are executed firstly, which include expression sub-regions segmentation, grayscale and scale normalization. Secondly, expression feature vectors of the expression sub-regions are extracted by Gabor wavelet transformation to form elastic graph for expression. Finally, the features of six basic expressions shown by different subjects under different illumination conditions are extracted and compared each other. The experimental results show that expression features can be extracted effectively based on Gabor wavelet transformation, which is insensitive to illumination variety and individual difference.",
Practical aspects of efficient forward selection in decomposable graphical models,We discuss efficient forward selection in the class of decomposable graphical models. This subclass of graphical models has a number of desirable properties. The contributions of This work are twofold. First we improve an existing algorithm by addressing cases previously not considered. Second we extend the algorithm to reflect model graphs with multiple disconnected components. We further present experimental results that apply this approach to a real dataset and discuss its properties. We belief that the presented approach is applicable to a wide area of fields and problems.,"Graphical models,
Probability distribution,
Inference algorithms,
Large-scale systems,
Computer science,
Polynomials,
Pattern recognition,
Data mining,
Information retrieval,
Testing"
Dynamic task scheduling in computing cluster environments,"In this study, a cluster-computing environment is employed as a computational platform. In order to increase the efficiency of the system, a dynamic task scheduling algorithm is proposed, which balances the load among the nodes of the cluster. The technique is dynamic, nonpreemptive, adaptive, and it uses a mixed centralised and decentralised policies. Based on the divide and conquer principle, the algorithm models the cluster as hyper-grids and then balances the load among them. Recursively, the hyper-grids of dimension k are divided into grids of dimensions k - 1, until the dimension is 1. Then, all the nodes of the cluster are almost equally loaded. The optimum dimension of the hyper-grid is chosen in order to achieve the best performance. The simulation results show the effective use of the algorithm. In addition, we determined the critical points (lower bounds) in which the algorithm can to be triggered.","Processor scheduling,
Dynamic scheduling,
Clustering algorithms,
Computer networks,
Resource management,
Computational modeling,
Computer science,
Educational institutions,
Scheduling algorithm,
Workstations"
CAMEO: Camera Assisted Meeting Event Observer,"Static cameras are pervasive in a variety of environments. However it remains a challenging problem to extract and reason about high-level features from real-time and continuous observation of an environment. In this paper, we present CAMEO, the Camera Assisted Meeting Event Observer, which is a physical awareness system designed for use by an agent-based electronic assistant. CAMEO is an inexpensive high-resolution omnidirectional vision system designed to be used in meeting environments. The multiple camera design achieves the desired high image resolution and lower cost that can be achieved when compared to traditional omnicameras that make use of a single camera and mirror solution.","Cameras,
Humans,
Firewire,
Optical character recognition software,
Streaming media,
Face,
Workstations,
Computer science,
Machine vision,
Image resolution"
A quantitative study and estimation models for extensible instructions in embedded processors,"Designing extensible instructions is a computationally complex task, due to the large design space each instruction is exposed to. One method of speeding up the design cycle is to characterize instructions and estimate their peculiarities during a design exploration. In this paper, we study and derive three estimation models for extensible instructions: area overhead, latency, and power consumption under a wide range of customization parameters. System decomposition and regression analysis are used as the underlying methods to characterize and analyze extensible instructions. We verify our estimation models using automatically and manually generated extensible instructions, plus extensible instructions used in large real-world applications. The mean absolute error of our estimation models arc as small as: 3.4% (6.7% max.) for area overhead, 5.9% (9.4% max.) for latency, and 4.2% (7.2% max.) for power consumption, compared to estimation through the time consuming synthesis and simulation steps using commercial tools. Our estimation models achieve an average speedup of three orders of magnitude over the commercial tools and thus enable us to conduct a fast and extensive design space exploration that would otherwise not be possible. The estimation models are integrated into our extensible processor tool suite.",
A statistical fault coverage metric for realistic path delay faults,"The path delay fault model is the most realistic model for delay faults. Testing all the paths in a circuit achieves 100% delay fault coverage according to traditional path delay fault coverage metrics. These metrics result in unrealistically low fault coverage if only a subset of paths is tested, and the real test quality is not reflected. For example, the traditional path delay fault coverage of any practical test for circuit c6288 is close to 0 because this circuit has an exponential number of paths. In this paper, a statistical and realistic path delay fault coverage metric is presented. Then the quality of several existing test sets (path selection methods) is evaluated in terms of local and global delay faults using this metric, in comparison with the transition fault and traditional path delay fault coverage metrics.","Circuit faults,
Circuit testing,
Fault detection,
Electrical fault detection,
Delay effects,
Computer science,
Timing,
Benchmark testing,
Circuit optimization,
Manufacturing processes"
A Web based conversational case-based recommender system for ontology aided metadata discovery,"Locating resources of interest in a large resource-intensive environment is a challenging problem. In this paper we present research on addressing this problem through the development of a recommender system to aid in metadata discovery. Our recommender approach uses conversational case-based reasoning (CCBR), with semantic Web markup languages providing a standard form for case representation. We present our initial efforts in designing and developing ontologies for an Earthquake Simulation Grid, to use these to guide case retrieval, discuss how these are exploited in a prototype application, and identify future steps for this approach.","Recommender systems,
Ontologies,
Semantic Web,
Large-scale systems,
Grid computing,
Computer science,
Markup languages,
Earthquakes,
Virtual prototyping,
Availability"
Managing user-centric adaptive services for pervasive computing,"Pervasive computing environments need to exhibit highly adaptive behavior to meet the changing task requirements and operational context of visiting mobile users. However this must be balanced with the need of resource owners to meet their goals in administering how users use their resources. This presents challenges of how to manage adaptive systems and how such management should be exercised by people, both average pervasive computing users and administrators of pervasive computing resources. This paper presents some of the issues involved in reconciling dynamic user-centric adaptation with the management of autonomic systems to meet high-level management policies. It discusses our architectural approach and presents some initial research results in addressing these issues.","Pervasive computing,
Resource management,
Programmable control,
Adaptive control,
Personal digital assistants,
Data engineering,
Computer science,
Educational institutions,
Mobile computing,
Adaptive systems"
Time domain vocal tract length normalization,"Recently, the speaker normalization technique VTLN (vocal tract length normalization), known from speech recognition, was applied to voice conversion. So far, VTLN has been performed in frequency domain. However, to accelerate the conversion process, it is helpful to apply VTLN directly to the time frames of a speech signal. In this paper, we propose a technique which directly manipulates the time signal. By means of subjective tests, it is shown that the performance of voice conversion techniques based on frequency domain and time domain VTLN are equivalent in terms of speech quality, while the latter requires about 20 times less processing time.",
Detection of bronchovascular pairs on HRCT lung images through relational learning,"The identification of bronchovascular pairs on high resolution computer tomography (HRCT) images provides valuable diagnostic information in patients with suspected airway diseases. Classification of a bronchovascular pair primarily formed by two structures, namely a bronchus and a vessel, is based on relations. Therefore, classifications based on simple attributes are insufficient for the recognition of bronchovascular pairs. To address this, we make use of relations and inductive learning from examples. Relations of potential bronchovascular pairs are extracted using image analysis and used for learning within FOIL, a first order relational learning system. The system was tested on 47 images using the learned classifier and its performance was visually validated with the help of radiologists in our team.","Lungs,
Computed tomography,
Shape,
Image segmentation,
Diseases,
Learning systems,
Image recognition,
Context modeling,
Computer science,
Statistics"
Energy generalized LVQ with relevance factors,"Input feature ranking and selection represent a necessary preprocessing stage in classification, especially when one is required to manage large quantities of data. We introduce a weighted generalized LVQ algorithm, called energy generalized relevance LVQ (EGRLVQ), based on the Onicescu's informational energy. EGRLVQ is an incremental learning algorithm for supervised classification and feature ranking.","Clustering algorithms,
Prototypes,
Euclidean distance,
Computer science,
Vector quantization,
Bayesian methods,
Training data,
Iterative algorithms,
Convergence,
Data structures"
Customer-centric network upgrade strategy: maximizing investment benefits for enhanced service quality,"With the ever increasing demand for network resources, network operators and Internet service providers are under constant pressure to accommodate more network bandwidth and offer better service quality via periodic network upgrades, Given a budget constraint, a sound network upgrade decision should maximize investment benefit which is contingent on the degree of customer satisfaction. This paper presents a customer-centric approach in making network upgrade decisions, where customer satisfaction is the key evaluation criterion. Network performance is related to customer's perceived service quality and component upgrades are assessed based on their profitability. As demonstrated using a case scenario, our approach results in effective upgrade decisions that enhance service quality, improve customer satisfaction, and maximize revenue.","Investments,
Customer satisfaction,
Performance analysis,
Web and internet services,
Bandwidth,
Computer science,
Profitability,
Process planning,
Strategic planning,
Large-scale systems"
An integral flow-based energy-efficient routing algorithm for wireless sensor networks,"Sensor networks consist of a large number of sensor nodes performing distributed sensing and event detection. As sensor nodes are energy-constrained, energy-efficient routing is essential for increasing the lifetime of a sensor network. In Gandham, SR et al. (2003), we proposed an ILP-based method for routing in sensor networks with multiple mobile base stations. The ILP-based method does not guarantee integral routes and bounds on running time. In this paper, we consider static base stations and propose an algorithmic approach to obtain integral energy-efficient routes. We propose to split the lifetime of a sensor network into equal periods of time referred to as rounds and model the energy constrained routing during a round as polynomial-time solvable flow problems. The flow information from an optimum solution to a flow problem is then used as a basis for an energy-efficient routing protocol. Through simulations, we demonstrate that our routing algorithm performs significantly better than the shortest path based algorithms and consumes less energy than the ILP-based method. In addition, we present an algorithm to determine, a priori, lower bound on the lifetime of the sensor network.","Energy efficiency,
Wireless sensor networks,
Base stations,
Routing protocols,
Computer science,
Event detection,
Polynomials,
Capacitive sensors,
Computer network management,
Integral equations"
Comparing several coverage criteria for detecting faults in logical decisions,"Many testing coverage criteria, including decision coverage and condition coverage, are well-known to be inadequate for software characterised by complex logical decisions, such as those in safety-critical software. In the past decade, more sophisticated testing criteria have been advocated. In particular, compliance of MC/DC has been mandated in the aviation industry for the approval of airborne software. On the other hand, the MUMCUT criterion has been proved to guarantee the detection of certain faults in logical decisions in irredundant disjunctive normal form. We analyse and empirically evaluate the ability of test sets satisfying these testing criteria in detecting faults in logical decisions. Our results show that MC/DC test sets are effective, but they may still miss some faults that can almost always be detected by test sets satisfying the MUMCUT criterion.","Fault detection,
Logic testing,
Software safety,
Software testing,
Computer industry,
Embedded software,
Terminology,
Computer science,
Information technology,
Councils"
Study of a highly accurate and fast protein-ligand docking based on molecular dynamics,"Summary form only given. Few methods use molecular dynamics simulations based on atomically detailed force fields to study the protein-ligand docking process because they are considered too time demanding despite their accuracy. We present a docking algorithm based on molecular dynamics simulations which has a highly flexible computational granularity. We compare the accuracy and the time required with well-known, commonly used docking methods like AutoDock, DOCK, FlexX, ICM, and GOLD. We show that our algorithm is accurate, fast and, because of its flexibility, applicable even to loosely coupled distributed systems like desktop grids for docking.","Computational modeling,
Grid computing,
Protein engineering,
Physics,
Biological system modeling,
Gold,
Distributed computing,
Concurrent computing,
Computer science,
Biology"
Context-based adaptive control in autonomous systems,"In this paper we introduce the concept of a context as a tool for adaptive control in autonomous systems. We provide a mechanism to represent the context information, and apply it to devise a practical methodology to prioritize the operations in an autonomous system. We illustrate the methodology using an intrusion management system and an autonomous battlefield theater. The context specification information is used as input parameters to the decision support mechanism that controls the operations in an autonomous system. Our specification of the context allows it to change dynamically, as the system operations progress. A context that changes dynamically thus provides a modified set of input parameters to the operations control mechanism, so that the forward-looking behavior of the autonomous system can be modulated in an adaptive manner.","Adaptive control,
Control systems,
Context modeling,
Network servers,
Computer science,
Programmable control,
Intrusion detection,
Data mining,
Databases,
Telemetry"
Hierarchical route optimization for nested mobile network,"One of the issues in designing a mobile network with MR-HA bidirectional tunnel is to solve the route optimization problem in the nested mobile networks. Since the aggregated hierarchy of mobile networks becomes a single nested mobile network, in order to forward packets to the nested mobile network nodes, multiple levels of bidirectional nested tunnels are required. However, the multiple levels of bidirectional tunnels lead to an undesirable overhead. We propose a hierarchical mechanism that allows direct packet tunneling between HA and MR without nested tunnels.","Bidirectional control,
Routing,
Mobile computing,
Proposals,
Design optimization,
Tunneling,
IP networks,
Protocols,
Mobile radio mobility management,
Computer science"
Nonlinear CA based scalable design of on-chip TPG for multiple cores,This paper reports an efficient design of test pattern generators (TPGs) for a chip having multiple cores. It is built around nonlinear cellular automata (CA) based pseudo-random pattern generator (PRPG). The modular and cascadable structure of proposed n-cell PRPG can be utilized to construct the (n+1)-cell PRPG without sacrificing the pseudo-randomness quality. The efficiency of such a scalable PRPG structure is demonstrated in designing the on-chip TPGs for a VLSI chip implementing multiple cores.,"Design engineering,
Educational institutions,
Logic testing,
Information technology,
Computer science,
Test pattern generators,
Very large scale integration,
Hardware"
Guaranteed broadcasting using SPON: supervised P2P overlay network,"One important application for dynamic networks is broadcast or multicast communication, where a single message is sent to all members of a dynamic set of recipients. This functionality is required by many data-sharing protocols, as well as applications including media streaming, online gaming, teleconferencing, and distance education. This paper proposes a tree-based network called SPON which manages group updates and supports efficient broadcasting. SPON uses a supervisor peer to maintain the network during node arrivals and departures and routes broadcasts using direct connections between nodes.","Broadcasting,
Peer to peer computing,
Network servers,
Telecommunication network reliability,
Streaming media,
Distance learning,
Computer science,
Application software,
Multicast communication,
Computer network reliability"
Composite ports for an architecture-oriented assembling of components,"Explicit architecture-oriented modelling could help in coping with the complexity of today's field-bus systems. However, there are few practicable concepts for extending wide spread programming techniques and standards of an architecture-driven design. A new component composition model is outlined that seeks to enable collaboration-based assembly in order to develop applications of field-devices and repositories for reusing of components. It takes into account the specific properties of this application area building up a port-based composition, which is a well known and established method of modelling automation systems.","Assembly,
Connectors,
Computer science,
Collaboration,
Information systems,
Design automation,
Large-scale systems,
Collaborative work,
Mechanical engineering,
Information technology"
Performance improvement of TCP with delayed ACKs in IEEE 802.11 wireless LANs,"We use a Markov renewal reward approach to analyse the throughput achieved by a TCP connection spanning a multi-hop wireless network that uses the IEEE 802.11 MAC protocol. The significance of the paper is twofold: (a) the method used in the paper is simple and can be used for performance evaluation of wireless LANs in general, and, (b) the analytical results suggest that there can be a significant gain in TCP performance due to ACK thinning if the TCP receiver uses the delayed acknowledgement option of TCP. This is in accordance with other simulation based studies on similar lines.","Delay,
Wireless LAN,
Local area networks,
Wireless networks,
Protocols,
Throughput,
Spread spectrum communication,
Analytical models,
Performance analysis,
Computer science"
Blood pressure estimation using neural networks,"Oscillometry is an indirect method to determine blood pressure. An inflatable and debatable cuff is placed on arm to observe oscillations at different pressure levels. Thus, an envelope obtained from the oscillations is related to the blood pressure. In our work, we extract few features from the oscillometric waveforms, and estimate blood pressure using feedforward neural networks. Feature strength is evaluated by computing the standard deviation of the errors. The results are compared with the traditional maximum amplitude pressure algorithm. A large noninvasively collected database is used for this purpose.","Blood pressure,
Neural networks,
Artificial neural networks,
Mercury (metals),
Arteries,
Computer science,
Feedforward neural networks,
Nonlinear systems,
Nonlinear control systems,
Control systems"
GERT: an empirical reliability estimation and testing feedback tool,"Software testing is an integral part of the software development process. Some software developers, particularly those who use the Extreme Programming test-driven development practice, continuously write automated tests to verify their code. We present a tool to complement the feedback loops created by continuous testing. The tool combines static source code metrics with dynamic test coverage for use throughout the development phase to predict a reliability estimate based on a linear combination of these values. Implemented as an open source plug-in to the Eclipse IDE, the tool facilitates the rapid transition between unit test case completions and testing feedback. The color-coded results highlight inadequate testing efforts as well as weaknesses in overall program structure. To illustrate the tool's efficacy, we share the results of its use on university software engineering course projects.","Software testing,
Automatic testing,
Software reliability,
Automatic programming,
Feedback loop,
Software engineering,
State feedback,
Computer science,
Phase estimation,
Software tools"
A hybrid security framework of mobile code,"Mobile code can potentially be malicious. To protect the local system against malicious mobile code, a hybrid security framework of mobile code is proposed, which combines different static and dynamic techniques to provide a general solution to mobile code security. For a given mobile code and a set of security policies that the code needs to enforce, a static analysis tool is used to verify the mobile code against the policy. If the static analysis shows that the mobile code will never violate the policy, nothing needs to do; otherwise it never rejects the code simply but adds dynamic checks to enforce the policy when necessary. Several static analysis optimizing algorithms is also proposed to improve performance of dynamic enforcement.",
Learning static occlusions from interactions with moving figures,"We present a simple and efficient algorithm for determining the position of static occluding bodies within a scene viewed by one or more static cameras. All information about the occluding bodies is derived from the perimeter of a figure moving through the scene. Once the positions of the occlusions are learned, successful reasoning about the observability of future figures in the scene is demonstrated. The method is extended to derive an estimate of the 3D position of the occluding bodies from multiple views. Several experimental results are described.","Layout,
Cameras,
Humans,
Observability,
Shape measurement,
Stereo vision,
Computer science,
Gold,
Application software,
Computer vision"
The segmentation of the body of the tongue based on the improved snake algorithm in traditional Chinese medicine,"The segmentation of the body of tongue is a premise to establishing a system of automatic diagnosis by the features of the tongue in traditional Chinese medicine, whose qualities affect the performance of tongue diagnosis. In order to overcome two key difficulties with the initialization and boundary concavities, a new active contour model is introduced as a segmentation method. We present an automatic initialization of snake by the feature of tongue in the HSV space. Multi-resolution wavelet (Gabor) decomposition is applied in an active contour model generating the edge energy of the image. Experiments on real tongue images show the good performance of this method.","Tongue,
Biomedical imaging,
Medical diagnostic imaging,
Active contours,
Image segmentation,
Educational institutions,
Computer science,
Artificial intelligence"
Variable-length contexts for PPM,"This paper presents a PPM variation which combines traditional character based processing with string matching. Such an approach can effectively handle repetitive data and can be used with practically any algorithm from the PPM family. The algorithm, inspired by its predecessors, PPM/sup */ and PPMZ, searches for matching sequences in arbitrarily long, variable-length, deterministic contexts. The experimental results show that the proposed technique may be very useful, especially in combination with relatively low order (up to 8) models, where the compression gains are often significant and the additional memory requirements are moderate.","Computer science,
Predictive models,
Compression algorithms,
Testing,
Statistics,
XML,
Data compression,
Context awareness"
Towards proactive computer-system forensics,"We examine principles and approaches for proactive computer-system forensics. Proactive computer-system forensics is the design, construction and configuring of systems to make them most amenable to digital forensics analyses in the future. The primary goals of proactive computer-system forensics are system structuring and augmentation for automated data discovery, lead formation, and efficient data preservation. We propose: (1) using the Neyman-Pearson Lemma to proactively build online forensics tests with the best possible critical regions for hypothesis testing, and (2) using classical stopping rules for sequential hypothesis testing to determine which users are deviating from standard usage behavior and should be the focus of more investigative resources. Here the focus is on security breaches by the employees or stakeholders of an organization. The main measurements are event-driven logs of program executions.","Computer crime,
Sequential analysis,
Data security,
Computer security,
Intrusion detection,
Computer science,
Data mining,
Digital forensics,
Programming profession,
Personnel"
Protocol-based business process modeling and enactment,"Business processes are conventionally modeled as monolithic flows that capture the desired business logic. However, developing process flows is challenging. Because a flow specifies what its participants should do, it restricts the autonomy of its participants, thus limiting their ability to exploit opportunities or accommodate exceptions according to their business preferences. We take a dual perspective wherein business processes are modeled as compositions of (instantiated) business protocols. Each business protocol specifies interactions among its partners; each protocol serves a unique business purpose, e.g., processing a payment or shipping an item. Thus, modularizing a monolithic business process via business protocols allows clear separation of concerns for modeling and enacting the process. We develop an approach in which protocols are compiled into local skeletal flows for each participant that can be fleshed out with local business logic as needed. Such flows are naturally distributed but can be enacted using commercial business flow engines. Thus, our protocol-based approach combines the benefit of improved modeling with simplified implementations.",
A Scalable Peer-to-Peer System for Music Information Retrieval,,
The generalized spherical homeomorphism theorem for digital images,"The spherical homeomorphism conjecture, proposed by Shattuck and Leahy in 2001, serves as the backbone of their algorithm to correct the topology of magnetic resonance images of the human cerebral cortex. Using a canonical image-thickening technique and the authors' previously proven ""spherical homeomorphism theorem for surfaces,"" we formulate and prove a spherical homeomorphism theorem which is valid for all digital images when utilizing the (26,6)-connectivity rule.","Digital images,
Topology,
Cerebral cortex,
Magnetic resonance,
Humans,
Spine,
Magnetic noise,
Image resolution,
Tree graphs,
Rendering (computer graphics)"
Functional summarization of gene product clusters using Gene Ontology similarity measures,"The paper addresses the problem of constructing a functional summarization of groups of gene products that are found by clustering a database of such products annotated by the Gene Ontology. Our method builds the ""most representative term"" (MRT) for each cluster in three increasingly sensitive ways. Initially, we perform crisp hierarchical clustering using BLAST and our novel fuzzy measure similarities and find the MRTs as the terms of highest frequency in the description of the gene products. Using weights from the fuzzy partition matrix generated by a relational fuzzy clustering algorithm, we show how more specific MRTs can be made. Finally, weighting these memberships by the information content of each term further increases the specificity of the functional annotation of the clusters.","Ontologies,
Clustering algorithms,
Proteins,
Frequency,
Databases,
Fuzzy sets,
Electric variables measurement,
Engineering management,
Biomedical informatics,
Computer science"
Classification of computerized learning tools for introductory programming courses: learning approach,"Learning programming is a difficult task since programming requires new concepts in thinking and creative skills in problem solving. A number of learning tools and environments have been built to assist both teachers and students in introductory programming courses. In this study, we have established a classification for these tools. Tools are divided into four categories: A) integrated development interface; B) visualization; C) virtual learning environments; and D) systems for submitting, managing, and testing of exercises. The classification is based on a review of existing tools, both commercial and freely available. Guidelines for the selection of a suitable tool are discussed.",
Coupons: wide scale information distribution for wireless ad hoc networks,"Integrating ad hoc networks into the Internet requires a number of difficult technical challenges to be overcome. In particular, ad hoc networks must not only overcome intermittent connectivity, but they also need a strong incentive mechanism to encourage users to participate in the cooperative relay of data traffic. We believe that inherent in solving these problems is the development of new applications that might, in fact, be more easily deployed in an ad hoc environment than in a traditional fixed network infrastructure. To this end, we develop and evaluate the idea of ""coupons"" for wide-scale information distribution in ad hoc networks. ""Coupons"" provide a simple incentive to nodes for relaying a piece of information. By using mechanisms on top of basic flooding to control distribution efficiently, the concept provides an elegant solution for scalable data dissemination with reduced network costs.","Ad hoc networks,
IP networks,
Relays,
Costs,
Mobile ad hoc networks,
Floods,
Intelligent systems,
Intelligent networks,
Computer science,
Telecommunication traffic"
Plone and content management,"This paper looks at Plone, one of the best content management systems. Plone is distributed under a free open-source license: the cost of getting started is only limited to the time you have available to set up the software on a server. Plone is written in Python and uses the Zope application server infrastructure; it runs on most modern operating systems. Plone can be customized for the maintenance of content - entirely over the Web.","Content management,
HTML,
XML,
Operating systems,
Calendars,
Collaboration,
Education,
Dictionaries,
Speech,
Vocabulary"
Proof reuse for deductive program verification,"We present a proof reuse mechanism for deductive program verification calculi. After a program amendment, it reuses a previous proof incrementally (one proof step at a time), employing a similarity measure for the points (formulas, terms, programs) where a rule is applied The method is flexible, as the reuse mechanism does not need knowledge about particularities of the target programming language or individual calculus rules. It also allows reuse of proof steps even if the situation in the new proof is merely similar but not identical to the template. Upon reaching a significant change in the program, the reuse process stops, and genuinely new proof steps have to be provided Reuse resumes automatically if another (unaffected) part of the proof template becomes pertinent. Our method has been successfully implemented within the KeY system to reuse correctness proofs for Java programs.","Calculus,
Computer languages,
Application software,
Computer science,
Time measurement,
Resumes,
Java,
Software engineering"
Does the common criteria paradigm have a future? [security and privacy],"In IEEE Security & Privacy's July/August 2003 issue, the author discussed the then upcoming 4th International Common Criteria Conference, which was held in Stockholm in September. Reviewing the CD of the presentations, however, he is left with the strong impression that, while a good idea when promulgated five years ago (after five years' effort by the six founding nations), the CC enterprise might have run out of gas. The proceedings' record and the author's own sampling of discussions on the CC Forum suggest the management structure and process need considerable revamping if the CC is to have a future.","National security,
Privacy,
Certification,
Costs,
Knowledge management,
Project management,
Information security,
Computer security,
Computer science education"
PARES: a software tool for computer-based testing and evaluation used in the Greek higher education system,"This paper presents a prototype software platform, namely Platform for Adaptive and Reliable Evaluation of Students (PARES), for student testing and evaluation. The motivation has been to provide educators in the Greek higher education system a useful tool for both carrying out tests using minimal resources and for keeping the students actively involved during a semester. PARES has been developed in Java for launching it in the WWW. A pilot study using PARES at the Technological Educational Institution of Kavala is presented. A statistical evaluation of PARES is also described. A future extension, based on a synergy of machine-learning with state feedback-control techniques, for adaptive student learning is outlined.","Software tools,
Software testing,
System testing,
Computer science education,
Software prototyping,
Java,
World Wide Web,
Educational technology,
Educational institutions,
Machine learning"
A call admission protocol for cellular networks that supports differentiated fairness,"Call admission protocols play a central role in determining the performance of any network. The call admission protocol must decide either to accept a call or reject it; at the same time, it must deal with different classes of calls that have different bandwidth and quality of service (QoS) requirements, and different priorities. It must also maintain some form of fairness (depending on QoS) and maintain a reasonable utilization of the channel. We assume a cellular system and we present a new call admission protocol. Our protocol is simple to implement, and it can support differentiated fairness in call acceptance. We also present a Markov chain representation of a system using our proposed protocol. Finally, we present simulation results in order to compare our protocol to previous protocols and show that our protocol can achieve the required differentiated fairness without sacrificing channel utilization.","Protocols,
Land mobile radio cellular systems,
Bandwidth,
Quality of service,
Telecommunication traffic,
Wireless networks,
Traffic control,
Diffserv networks,
Computer science,
Nuclear magnetic resonance"
Biologically-Inspired Face Detection: Non-Brute-Force-Search Approach,"We present a biologically-inspired face detection system. The system applies notions such as saliency, gist, and gaze to localize a face without performing blind spatial search. The saliency model consists of highly parallel low-level computations that operate in domains such as intensity, orientation, and color. It is used to direct attention to a set of conspicuous locations in an image as starting points. The gist model, computed in parallel with the saliency model, estimates holistic image characteristics such as dominant contours and magnitude in high and low spatial frequency bands. We are limiting its use to predicting the likely head size based on the entire scene. Also, instead of identifying face as a single entity, this system performs detection by parts and uses spatial configuration constraints to be robust against occlusion and perspective.","Face detection,
Layout,
Concurrent computing,
Humans,
Computer science,
Frequency estimation,
Head,
Face recognition,
Biology,
Biological system modeling"
Support vector clustering combined with spectral graph partitioning,We propose a new support vector clustering (SVC) strategy by combining (SVC) with spectral graph partitioning (SGP). SVC has two main steps: support vector computation and cluster labeling using adjacency matrix. Spectral graph partitioning (SGP) method is applied to the adjacency matrix to determine the cluster labels. It is feasible to combine multiple adjacency matrices computed using different parameters. A novel multi-resolution combination method is proposed for cluster labeling using the SGP for the purpose of boosting the clustering performance.,"Static VAr compensators,
Labeling,
Clustering methods,
Computer science,
Pattern recognition,
Machine learning,
Joining processes,
Kernel,
Boosting,
Learning systems"
Optimal placement of Web proxies for tree networks,"Placement of Web proxy servers is an important avenue to save network bandwidth, alleviate server load, and reduce latency experienced by users. The general problem of Web proxy placement is to compute the optimal locations for placing k Web proxies in a network such that the objective concerned is minimized or maximized. We address this problem for tree networks and propose a novel mathematical model for it. In our model, we consider maximizing the overall access gain as our objective and formulate this problem as an optimization problem. The optimal placement is obtained using a computationally efficient dynamic programming-based algorithm. Applying our mathematical model, we also present a solution to Web proxy placement for autonomous systems (ASes), as a natural extension of the solution for tree networks. Our algorithms have been implemented. The simulation results show that our model significantly outperforms the random placement model.","Network servers,
Delay,
Heuristic algorithms,
Web server,
Bandwidth,
Mathematical model,
Internet,
Instruments,
Information science,
Computer networks"
Improving modeling of other agents using stereotypes and compactification of observations,,
Affine invariant multiscale wavelet-based shape matching algorithm,"In this paper, a multiscale wavelet-based algorithm for matching stand-alone shapes is developed. The algorithm uses the Dyadic Wavelet Transform (DWT) to decompose a shape¿s boundary into multi-scale levels. Features are extracted by calculating the curve moment invariants of the approximation coefficients. If the measured dissimilarity is small, then the shapes are globally similar. Local similarity is investigated by calculating the normalized cross correlation of the 1-D triangle area representation of the detail coefficients. The presented algorithm not only finds similar shapes, but it also can easily distinguish between seemingly similar shapes. The algorithm is invariant to the affine transformation and to the starting point variation of the shape contour.","Wavelet transforms,
Shape measurement,
Noise shaping,
Feature extraction,
Approximation algorithms,
Physics,
Computer science,
Discrete wavelet transforms,
Image segmentation,
Data mining"
"Semantic services for grid-based, large-scale science","The process of large-scale science must evolve to facilitate the next steps of scientific discovery. Grid technology and semantic tools will be valuable in dealing with the complex multidisciplinary simulation and data environments that next-generation science will require. We envision semantic Web-like tools that automatically check the validity of sequences of composed operations and data, and automatically construct intermediate steps in a loosely specified sequence. These tools should also automatically construct sequences of operations that are consistent with a discipline model representing permitted relationships among simulation and analysis operations and data for particular disciplines, such as climatology or high-energy physics. These tools are called semantic services.",
Scalable self-stabilization via composition,"Objections to the practical use of stabilization have centered around problems of scale. Because of potential interferences between actions, global reasoning over the entire system is in general necessary. The complexity of this task increases dramatically as systems grow in size. Alternatives to dealing with this complexity focus on reset and composition. For reset, the problem is that any fault, no matter how minor, will cause a complete system reset with potentially significant lack of availability. For existing compositional alternatives, including compositional reset, severe restrictions on candidate systems are imposed. To address these issues, we give a framework for composition in which global reasoning and detailed system knowledge are not necessary, and which apply to a significantly wider range of systems than has hitherto been possible. We explicitly identify for each component which other components it can corrupt. Additionally, the correction of one component often depends on the prior correction of one or more other components, constraining the order in which correction can take place. Given appropriate component stabilizers such as detectors and correctors, we offer several ways to coordinate system correction, depending on what is actually known about the corruption and correction relations. By reducing the design of and reasoning about stabilization to local activities involving each component and the neighbors with which it interacts, the framework is scalable. Reset is generally avoided by using the correction relation to check and correct only where necessary. By including both correction and corruption relations, the framework subsumes and extends other compositional approaches. Though not directly a part of this work, we mention tools and techniques that can be used to help calculate the dependency and corruption relations and to help create the necessary stabilizers. To illustrate the theory, we show how this framework has been applied in our work in sensor networks.","Interference,
Information science,
Detectors,
Local activities,
Temperature sensors,
Network servers,
Computer errors,
Computer bugs,
Wireless networks,
Contracts"
Toward the use of local monitoring and network-wide correction to achieve QoS guarantees in mobile ad hoc networks,"This paper presents the exploration of novel mechanisms toward providing quality of service (QoS) guarantees in mobile ad hoc networks. We study mechanisms that provide differentiated services to packets of varying priority traffic flows. These mechanisms do not require any central coordination and do not depend on any specific protocols at the physical, MAC, or network layers. Nodes independently monitor the rates of the highest priority flows and signal corrective mechanisms when these rates fall outside of specified local bounds. Triggering conditions for network-wide corrective mechanisms are designed to trade-off rapid reactive response to local QoS violations with control packet overhead. A range of corrective mechanisms are explored that attempt to maintain reactive response while improving total network utilization, including resources consumed by lower priority traffic. We provide simulation results that demonstrate the effectiveness of monitoring, reactive triggering, and basic and advanced corrective mechanisms. We discuss the extension of these novel mechanisms to a complete QoS solution for mobile ad hoc networks.","Monitoring,
Intelligent networks,
Mobile ad hoc networks,
Quality of service,
Traffic control,
Telecommunication traffic,
Routing protocols,
Communication system traffic control,
Media Access Protocol,
Computer science"
Web-based mathematics education: MeML design and implementation,"The Web-based mathematics education framework (WME) aims to create a Web for mathematics education. WME empowers mathematics teachers, learning content developers, as well as dynamic mathematics computation and education service providers, to deliver an unprecedented mathematics learning environment to students and educators. Main WME components include the Mathematics Education Markup Language (MeML), the MeML processor (Woodpecker, browser plug-in), and on-Web Mathematics Education Services. MeML provides effective and expressive markup elements to represent and structure mathematics education pages that may also contain XHTML and MathML elements. Woodpecker enables regular Web browsers to process MeML pages and to interact with a wide variety of mathematics computation and education services deployed on the Web.",
"Glitch elimination by gate freezing, gate sizing and buffer insertion for low power optimization circuit","One of the major factors contributing to the power dissipation in CMOS digital circuits is the switching activity. Many of such switching activities include spurious pulses, called glitches. In this paper, we propose a new method of glitch reduction by gate freezing, gate sizing, and buffer insertion. The proposed method unifies gate freezing, gate sizing, and buffer insertion into a single optimization process to maximize the glitch reduction. The effectiveness of our method is verified experimentally using LGSynth91 benchmark circuits with a 0.5/sup um/ standard cell library. Our optimization method reduces glitches by 65.64% and the power by 31.03% on average.","Optimization methods,
Power dissipation,
Delay,
CMOS digital integrated circuits,
Capacitance,
Computer science,
Switching circuits,
Software libraries,
Circuit synthesis,
Digital circuits"
Expanding education through active space collaboration,"We introduce a system called e-Fuzion based on using computing devices, such as Tablet PCs, that empower students and teachers with better technologies for educational interaction. We propose an e-Fuzion integration with ubiquitous computer environments of smart devices and active spaces using a system called Gaia. At present, e-Fuzion facilitates electronic communication in a seamless, integrated classroom, giving students additional ways to interact with faculty and each other. E-Fuzion encourages students to participate actively even in large scale class settings. By integrating e-Fuzion into an active space, we enhance the effectiveness of the existing system by taking advantage of the ubiquitous nature of smart devices and their unique communication capabilities. The end result is that students are able to interact, record data and share ideas more quickly than in the traditional classroom.",
Speed control of an interior permanent magnet synchronous motor using belbic (brain emotional learning based intelligent controller),"Among ac drives, the permanent magnet synchronous motor has been gaining popularity owing to its high torque to current ratio, large power to weight ratio, high efficiency, high power factor and robustness. Because of its technical and economic advantages, Permanent Magnet Synchronous Motor drive technology is a serious contender for replacing the existing technologies. In this paper we report the utilization of a novel controller (BELBlC) based on emotion processing mechanism in brain. Our results show superior control characteristics especially very fast response, simple implementation and robustness with respect to disturbances and manufacturing imperfections. Our proposed method enables the designer to shape the response in accordance with the multiple objectives of his/her choice.","Velocity control,
Permanent magnet motors,
Torque control,
Rotors,
AC motors,
Machine vector control,
Synchronous motors,
Process control,
Magnetic materials,
Control theory"
A process component model for enterprise business knowledge reuse,"To respond quickly to the market, the organization must be able to create, manage and optimize dynamic business process. Building up dynamic process using multi predefined process units can lead to higher efficiency in fulfilling complex business goals. Among all valuable information business process, current business process management systems only reuse the workflow model, which is of a relatively lower level, and cannot fully support process unit composition. This paper suggests that business process should be regarded as a kind of knowledge, and reuse of this knowledge should cover a much wider range. A business knowledge reuse framework is discussed. We define process component as a unit for process knowledge management and reuse. The ontology of process component is expatriated in detail. Process component model is a key factor in realizing process knowledge reuse and organizational learning.",
Local recovery solutions from multi-link failures in MPLS-TE networks with probable failure patterns,"MPLS-TE (traffic engineering) fast reroute proposes a local protection mechanism to reroute protected TE LSPs (label switched paths) quickly onto precomputed and signaled bypass tunnels. The paper explores the case of multiple network element failure scenarios. The undesired complexity inherent in the multiple failure scenario originates from the fact that those failure scenarios are more disruptive, and may require multiple bypass tunnels to cope with them. The paper adapts the MPLS local recovery schemes to multi-failure scenarios, while controlling the number of bypass tunnels that are required. This is achieved by mapping multi-failure scenarios onto probable failure patterns (PFPs). PFPs are characterized by their probability (or frequency) of occurrence during the network lifetime. A number of bypass tunnels is then computed to cope effectively with the PFPs according to their frequency or probability of occurrence. It is shown that by properly choosing how the PFPs are grouped, and how the corresponding bypass tunnels are computed, it is possible to trade the required number of bypass tunnels for their average length and outage probability, i.e., the probability that the local recovery scheme cannot cope with the occurrence of a multi-failure pattern.","Intelligent networks,
Multiprotocol label switching,
Tellurium,
Protection,
Telecommunication traffic,
Frequency,
Optical fiber networks,
Laboratories,
Computer science,
Mission critical systems"
Spatial prediction based intra-coding [video coding],"According to the H.264 video coding standard, spatial prediction is used for intra block coding. The luma prediction may be based on a 4/spl times/4 block, for which there are nine prediction modes, or a 16/spl times/16 macroblock., for which there are four prediction modes. For chroma prediction, there are also four prediction modes. In this paper, a new method is proposed for improving the intra prediction algorithm, the first step is to apply direction to the results of the DC prediction and the second step is to use simplified modes to reduce computational complexity.",
Atomizer: a dynamic atomicity checker for multithreaded programs,"Summary form only given. Ensuring the correctness of multithreaded programs is difficult, due to the potential for unexpected interactions between concurrent threads. We focus on the fundamental noninterference property of atomicity and present a dynamic analysis for detecting atomicity violations. This analysis combines ideas from both Lipton 's theory of reduction and earlier dynamic race detectors such as Eraser. Experimental results demonstrate that this dynamic atomicity analysis is effective for detecting errors due to unintended interactions between threads. In addition, the majority of methods in our benchmarks are atomic, supporting our hypothesis that atomicity is a standard methodology in multithreaded programming.","Yarn,
Interleaved codes,
Interference,
Testing,
Computer science,
Java,
Code standards,
Educational institutions,
Detectors,
Software standards"
Peer-to-peer reputations,"Summary form only given. Peer-to-peer (P2P) networks are designed with an assumption that the nodes in a P2P network will cooperate each other. In the absence of any common goals shared by the nodes of a P2P network, external motivation to cooperate and be trustworthy is required. Digital reputations can be used to inject trust among the autonomous nodes of a network and motivate the nodes to contribute resources. We summarize a self-certification scheme for the identification of peers using digital certificates similar to SDSI certificates, techniques to mitigate the problem of 'a consortium of liars' and an elicitation-storage protocol for procuring and storing recommendations.","Peer to peer computing,
Design engineering,
Computer science,
Protocols,
History,
Voting,
Computer network management,
Identity management systems,
Humans,
Computer networks"
An unconditional study of computational zero knowledge,"We prove a number of general theorems about CZK, the class of problems possessing computational zero knowledge proofs. Our results are unconditional, in contrast to most previous works on CZK which rely on the assumption that one-way functions exist. We establish several new characterizations of CZK, and use these characterizations to prove results such as: 1) Honest-verifier CZK equals general CZK. 2) Public-coin CZK equals private-coin CZK. 3) CZK is closed under union (and more generally, ""monotone formula closure""). 4) CZK with imperfect completeness equals CZK with perfect completeness. 5) Any problem in CZK /spl cap/ NP can be proven in computational zero knowledge by a BPP/sup NP/ prover. 6) CZK with black-box simulators equals CZK with general, non-black-box simulators. The above equalities refer to the resulting class of problems (and do not necessarily preserve other efficiency measures such as round complexity). Our approach is to combine the conditional techniques previously used in the study of CZK with the unconditional techniques developed in the study of SZK, the class of problems possessing statistical zero knowledge proofs. To enable this combination, we prove that every problem in CZK can be decomposed into a problem in SZK together with a set of instances from which a one-way function can be constructed.","Computational modeling,
Knowledge engineering,
Cryptographic protocols,
Polynomials,
Computer science"
SEKEN: secure and efficient key exchange for sensor networks,"Wireless sensor networks are edging closer to widespread feasibility with recent research showing promising results in developing and adapting new mechanisms to suit their environment. Secure communication between these distributed wireless devices is a desired characteristic, especially in scenarios where these sensors would be used for military and other mission-critical operations. This paper highlights some of the research challenges for extending secure communications over these resource-limited devices and points out why current protocols do not scale well in this unique application realm. A new key setup protocol (SEKEN) is proposed that neatly fits into the requirements of these device types. The performance of SEKEN is then analyzed against some other possible key setup mechanisms. Our initial results confirm that it performs better under most of the conditions anticipated for general wireless sensor networks.","Wireless sensor networks,
Sensor phenomena and characterization,
Protocols,
Intelligent sensors,
Biosensors,
Power system security,
Computer science,
Information security,
Sensor systems,
Communication system security"
AGORA: an architecture for strategyproof computing in grids,"Grids enable the sharing of resources and problem solving in multi-organizational environments. The problem of resource management in such systems is very complex because the participants, resource owners and users, have their own requirements and objectives that need to be considered when making allocation decisions. To address this issue novel protocols that take into account the self-interest and incentives of these participants need to be developed. These kinds of protocols in which the participants maximize their own utilities only if they report their true parameters and follow the rules are called strategyproof protocols. In this paper we propose AGORA, an architecture for strategyproof computing in grids and present several strategyproof mechanisms for resource allocation that can be deployed on this architecture.","Computer architecture,
Grid computing,
Resource management,
Protocols,
Distributed computing,
Cost accounting,
Computer science,
Problem-solving,
Instruments,
Memory"
Object tracking using incremental Fisher discriminant analysis,"This work presents a novel object tracking algorithm using incremental Fisher linear discriminant (FLD) algorithm. The sample distribution of the target class is modeled by a single Gaussian and the non-target background class is modeled by a mixture of Gaussians. To a facilitate a multiclass classification problem, we recast the classic FLD algorithm in which the number of classes does not need to be pre-determined. The most discriminant projection matrix that best separates the samples in the projected space is computed using FLD at each frame. Based on the current target location, an efficient sampling algorithm is used to predict the possible locations in the next frame. Using the current projection matrix computed by FLD, the most likely candidate which is closed to the center of the target class in the projected space is selected. Since the FLD is repeatedly computed at each frame, we develop an incremental and efficient method to compute the projection matrix based on the previous results. Experimental results show that our tracker is able to follow the target with large lighting, pose and expression variation.","Target tracking,
Sampling methods,
Computer science,
State estimation,
Image motion analysis,
Motion estimation,
Performance analysis,
Algorithm design and analysis,
Support vector machines,
Boosting"
Nontree routing for reliability and yield improvement [IC layout],"We propose to introduce redundant interconnects for manufacturing yield and reliability improvement. By introducing redundant interconnects, the potential for open faults is reduced, at the cost of increased potential for short faults. Overall, manufacturing yield and fault tolerance can be improved. We focus on a postprocessing, tree-augmentation approach, which can be easily integrated in current physical design flows. Our contributions are as follows. 1) We formulate the problem as a variant of the classical two-edge-connectivity augmentation problem in which we take into account such practical issues as wirelength increase budget, routing obstacles, and the use of Steiner points. 2) We show that an optimum solution can always be found on the Hanan grid defined by the terminals and the corners of the feasible routing region. 3) We give a compact integer program formulation which is solved in practical runtime by the commercial optimization package CPLEX for nets with up to 100 terminals. 4) We give a well-scaling greedy algorithm which has a practical runtime up to 1000 terminals, and comes on the average within 1%-2% of the optimum computed by CPLEX. 5). We give a comprehensive experimental study comparing the solution quality and runtime of our methods with the best methods reported in the literature.","Routing,
Runtime,
Bridge circuits,
Computer science,
Manufacturing,
Steiner trees,
SPICE,
Delay,
Costs,
Fault tolerance"
Humanoid robot presentation controlled by multimodal presentation markup language MPML,"We have developed a multimodal presentation markup language, called MPML. In our previous studies, we have succeeded to make attractive multimodal presentation with animated virtual characters easily. Then we have combined the MPML with a two-legged humanoid robot, instead of the animated character on 2D screen. It enables an end-user to control freely the humanoid robot presenter for his/her own Web-based multimodal presentation. The humanoid robot introduces the multimedia contents with a voice pointing at a screen using a laser pointer. A single MPML program can generate both animated character presentation on 2D screen and humanoid robot presentation in 3D space. We also show empirically how controllable and expressive the presentation is by means of the humanoid robot.","Humanoid robots,
Robot control,
Markup languages,
Animation,
HTML,
Humans,
Information science,
Character generation,
TV,
Robot programming"
Vulnerability analysis of AIS-based intrusion detection systems via genetic and particle swarm red teams,"Artificial immune systems (AISs) are biologically inspired problem solvers that have been used successfully as intrusion detection systems (IDSs). In this paper we compare a genetic hacker with 12 evolutionary hackers based on particle swarm optimization (PSO) that have been effectively used as vulnerability analyzers (red teams) for AIS-based IDSs. Our results show that the PSO-based red teams that use Clerc's constriction coefficient outperform those that do not. Our results also show that the three types of red teams (genetic, basic PSO, and PSO with the constriction coefficient) have distinct search behaviors that are complimentary. This result suggests that red teams based on genetic swarms may hold the most promise.","Intrusion detection,
Genetics,
Particle swarm optimization,
Detectors,
Computer hacking,
Telecommunication traffic,
Computer science,
Artificial immune systems,
Software,
Statistical analysis"
Layered priority encoded transmission for video streaming to heterogeneous clients,"A scheme based on priority encoded transmission (PET), in which a movie is encoded and transmitted in parallel over multiple layers is presented in this paper. Clients can ""tune in"" to a suitable number of layers according to their bandwidth constraints, and begin watching the movie with commensurate delays. This scheme is extended to layered PET transmissions over multiple channels to support heterogeneous clients. The schemes can be implemented both using MDS block codes or the recently discovered fountain codes. For a movie of segments the layers and the channels will be having an algorithm to determine the optimal channel-to-layer mapping","Streaming media,
Motion pictures,
Bandwidth,
Broadcasting,
Delay,
Positron emission tomography,
Computer science,
Multimedia communication,
Watches,
Costs"
On the semantics of aggregation and generalization in learning object contracts,"When machine-understandability is required to build software modules that automatically retrieve and combine learning objects, learning object relationships should be carefully considered, as they raise important semantic issues that influence runtime behaviour. In this paper, we analyse how learning object relationships have an effect on learning object contracts and look for analogies with the object-oriented paradigm. Being some of the most common relationships, we focus on the commitments that aggregation and generalization impose on learning object contracts.","Contracts,
Machine learning,
Runtime,
Computer science,
Best practices,
Least squares approximation,
Concrete,
Books,
Automation,
Certification"
Computer sciences at Prudue University-1962 to 2000,"Purdue University established the first academic department of computer sciences in 1962. The events leading to its establishment are chronicled and its first 35 years of development presented. Nationally, computer science departments experienced a steady increase in size and activities, which were reflected in Purdue's experience. The two periods of crises in the department's history were triggered by the two periods of national enrollment explosion in the number of undergraduate majors.","Mathematics,
Statistics,
Biology computing,
Computer science education,
Educational institutions,
Meeting planning,
Laboratories,
Proposals,
Computer science,
History"
An agent-based framework for testing Web applications,"Software testing in general and Web applications testing in particular are knowledge-driven, labor intensive activities, which are best performed by intelligent, autonomous agents. The proposed framework is based on the Belief-Desire-Intention (BDI) model of rational agents and the Unified Modeling Language (UML). We describe how Web applications testing can be modeled and reasoned using the framework.","Application software,
Unified modeling language,
Software testing,
Intelligent agent,
Logic testing,
Navigation,
Variable speed drives,
Computer science,
Knowledge engineering,
Performance evaluation"
A study of intrinsic Crystal-pixel light-output spread for discrete scintigraphic imagers modeling,"This paper is focused on the discrete scintillation imaging devices, consisting of crystal arrays and metal-channel dynode Hamamatsu 1"" and 2"" square position sensitive photomultiplier tubes (PSPMTs). These devices are suitable for nuclear medicine based high resolution applications, and, particularly, for single photon emission computed tomography (SPECT). The model of scintillation light distribution (SLD) previously developed was able to distinguish the responses from crystal-pixels with different side, but it was not detailed enough to explain the influence of crystal-thickness. For this reason the experimental data were reviewed to find a new and more adequate analytical model. The improved SLD model explains the influence both of crystal-side and crystal-thickness on the scintillation light-output spread. The SLD expression is quite simple and its spread depends only on one q-parameter. This expression is well adaptable over the range of examined crystal arrays. Furthermore, in the considered experiments, the SLD q-parameter was found linearly dependent on crystal-pixel shape factor S/V(S=blind-surfacearea,V=volume). An overview of discrete scintillation imager simulator (DISIS) computer code is reported. Major outcomes of this work are: 1) the improved expression of SLD, which consolidates the DISIS performances, and 2) a tool for local SLD-spread control in the imager field of view (FOV).","Superluminescent diodes,
Photonic crystals,
High-resolution imaging,
Photomultipliers,
Nuclear medicine,
Single photon emission computed tomography,
Analytical models,
Adaptive arrays,
Shape,
Computational modeling"
An efficient dynamic modeling methodology for general type of hybrid robotic systems,"In this paper, we deal with the kinematic and dynamic modeling of hybrid robotic systems that are constructed by combination of parallel and serial modules or series of parallel modules. Up to now, open-tree structure has been generally employed for dynamic modeling of hybrid robotic systems. However, it requires not only expensive computation as the complexity of the system increases, but also must perform dynamic modeling for the whole manipulator again even if the partial portion of the robot structure is changed. Therefore, we propose an efficient dynamic modeling methodology for hybrid robotic systems. In the proposed method, initially the local dynamics of each of modules are obtained with respect to its independent joint coordinates and then the dynamics of the hybrid robot is calculated utilizing the concept of virtual joints that are attached to the base of each module of interest The virtual joints are assigned to have the appropriate number of DOFs in the operational space to represent the motion of all the proximal modules from the module of interest to the ground. For general multiple module-based hybrid robots, a recursive dynamic formulation of the proposed method is derived and the usefulness of the method is verified by comparing the computational efficiency of both the proposed method and the existing method.",
K-edge connected neighborhood graph for geodesic distance estimation and nonlinear data projection,"Nonlinear data projection based on geodesic distances requires the construction of a neighborhood graph that spans all data points so that the geodesic distance between any pair of data points could be estimated by the graph distance between the pair. This paper proposes an approach for constructing a k-edge connected neighborhood graph. The approach works by repeatedly extracting minimum spanning trees from the complete Euclidean graph of all data points. The constructed neighborhood graph has the following properties: (1) it is k-connected; (2) each point connects to its k-nearest neighbors; (3) if the graph is cut into two partitions, the cut edges contain k-shortest edges between the two partitions. Experiments show that the presented approach works well for clustered data and outperforms the nearest neighbor approaches used in Isomap for evenly distributed data.","Data mining,
Nearest neighbor searches,
Tree graphs,
Pattern recognition,
Euclidean distance,
Computer science,
Data processing,
Humans,
Visual perception,
Level measurement"
Experimental performance evaluation of job scheduling and processor allocation algorithms for grid computing on metacomputers,"Summary form only given. Scheduling is a fundamental issue in achieving high performance on metacomputers and computational grids. The job scheduling problem for grid computing on metacomputers has been studied as a combinatorial optimization problem. In this paper, we compare the performance of various job scheduling and processor allocation algorithms for grid computing on metacomputers. We evaluate the performance of 128 combinations of two job scheduling algorithms, four initial job ordering strategies, four processor allocation algorithms, and four metacomputers by extensive simulation. It is found that the combination of LJF and MEET or LMF yields the best performance, and the choice of FCFS and LS depends on the range of job sizes.","Processor scheduling,
Scheduling algorithm,
Grid computing,
Artificial intelligence,
High performance computing,
Computer networks,
Gold,
Bandwidth,
Computer science,
Computational modeling"
Utilising push and pull mechanism in wireless e-health environment,"The emerging of wireless computing motivates radical changes of how information is derived. Our paper concerns with developing push and pull based application in wireless environment. We use a simplified e-health (hospital) context to demonstrate some effective uses of pull-based and push-based mechanisms. The pull mechanism involves information retrieval from a database, and update information in the database. The push mechanism is classified into three scenarios, (i) aperiodic push, (ii) periodic push - global recipients and (iii) periodic push - selective recipients.","Hospitals,
Databases,
Network servers,
Context,
Computer science,
Information retrieval,
Unicast,
Broadcasting,
Communication system control,
Software engineering"
A data readout system with high-speed serial data link for balloonborne X-ray detectors,"Data size of recent balloonborne detector is growing. High speed data readout is one of the key technologies for those detectors. A parallel data bus has been used for sending data with high speed. However, it causes problems in noise, mechanical size and power consumption. We apply the serial data link for our apparatus as a replacement of the parallel data link to solve those problems. We designed the hardware and firmware based on the SpaceWire protocol. Because of simple and flexible routing protocol of SpaceWire, the system was compact and has very high modularity. The protocol is implemented on Altera's FPGAs, APEX20KE and Cyclone. The speed of the serial data link was 48-53 Mbps. The maximum data rate to the computer was 4.5 MB/s.","X-ray detectors,
Logic,
X-ray detection,
Energy consumption,
Educational institutions,
Data acquisition,
Hardware,
Microprogramming,
Routing protocols,
Field programmable gate arrays"
"The use patterns of large, interactive display surfaces: Case studies of media design and use for blueboard and MERboard","During the past several years we have been developing large, interactive display surfaces for collaboration uses in a variety of work settings. People in small work groups can easily create, annotate and share media with their partners. The Blueboard, developed at IBM Research, is a large display system for groups to use in exchanging information in a lightweight, informal collaborative way. It began as a large, ubiquitously placed display surface for walk-by use in a corporate setting and has evolved in response to task demands and user needs. At NASA, the MERboard is being designed to support surface operations for the upcoming Mars exploration rover missions. The MERboard extends the design to support the collaboration requirements for viewing, annotating, linking and distributing information for the science and engineering teams that will operate two rovers on the surface of Mars. Here we examine differing implementations of the same idea: a collaborative information tool that began from the same design goals, but which grew into somewhat different systems under the evolutionary pressures of the NASA and IBM task environments. Lessons about how media are designed, task requirements for collaborative use, information flow requirements and work practice drive the evolution of a system are illustrated.","Computer aided software engineering,
Collaborative work,
NASA,
Collaboration,
Plasma displays,
Mars,
Large screen displays,
Radiofrequency identification,
Keyboards,
Joining processes"
3D shape matching using collinearity constraint,"In this paper, a novel algorithm is proposed to carry out automatic 3D shape matching with 3D shapes represented as sets of points. After the possible matches between the 3D shapes have been determined by the tradition ICP criterion, the novel approach employs the collinearity constraint to eliminate false matches based on a statistical model. A comparative study based on real range images has shown that the proposed algorithm is accurate, robust and efficient for the automatic matching of overlapping 3D shapes.","Shape,
Computer science,
Iterative closest point algorithm,
Cameras,
Robot vision systems,
Robotics and automation,
Iterative algorithms,
Educational institutions,
Robustness,
Laser modes"
Single-sided CZT strip detectors,"We report progress in the study of thick CZT strip detectors for 3-D imaging and spectroscopy and discuss two approaches to device design. Unlike double-sided strip detectors, these devices feature both row and column contacts implemented on the anode surface. This electron-only approach circumvents problems associated with poor hole transport in CZT that normally limit the thickness and energy range of double-sided strip detectors. The work includes laboratory and simulation studies aimed at developing compact, efficient, detector modules for 0.05 to 1 MeV gamma radiation measurements while minimizing the number and complexity of the electronic readout channels. These devices can achieve similar performance to pixel detectors for both 3-D imaging and spectroscopy. The low channel count approach can significantly reduce the complexity and power requirements of the readout electronics. This is particularly important in applications requiring large area detector arrays. We show two single-sided strip detector concepts. One, previously reported, features rows established with collecting contacts and columns with noncollecting contacts. Another, introduced here, operates on a charge sharing principle and establishes both rows and columns with collecting contacts on the anode surface. In previous work using the earlier strip detector concept we reported simulations and measurements of energy and spatial resolution for prototype 5- and 10-mm-thick CZT detectors. We now present the results of detection efficiency and uniformity measurements conducted on 5-mm-thick detectors using a specific configuration of the front-end electronics and event trigger. We discuss the importance of the detector fabrication processes when implementing this approach.","Strips,
Spectroscopy,
Anodes,
Event detection,
Computer vision,
Laboratories,
Gamma ray detection,
Gamma ray detectors,
Radiation detectors,
Gamma rays"
Real-time road lane recognition using fuzzy reasoning for AGV vision system,"The automatic guided vehicle (AGV) vision system is an important research area in computer vision. In order to recognize the road lane quickly and effectively, this paper presents an algorithm using fuzzy reasoning based on the Hough transform to solve this problem, which improves the entire system's real-time performance. After our tests on the test vehicle, this method can speed up the road lane recognition velocity phenomenally, and it also can improve the stability in driving.","Real time systems,
Fuzzy reasoning,
Machine vision,
Road vehicles,
Remotely operated vehicles,
Testing,
Cameras,
Educational institutions,
Computer science,
Computer vision"
Process management in supply chains - a new Petri-net based approach,"Supply chain process management (SCPM) refers to the design and control of interrelated production, logistics and information processes on an operational level. In this paper, we present an innovative approach to SCPM, based on a new type of high level Petri nets, so called XML-nets. With the proposed XML-nets, the capabilities of SCPM can be significantly enhanced in comparison to existing methods. By employing XML-nets, not only single phases, i.e. modelling, analysis and control, but SCPM as a whole can be improved. XML-nets facilitate an integral approach to SCPM by using a consistent and comprehensible methodology throughout all relevant phases. A further application potential results from applying the widely used XML standard: A SCPM software application can be directly linked to different transactional systems, in order to exchange relevant (intra and inter-organisational) process data. After a detailed introduction to supply chain process modelling with XML-nets, the advantages of the proposed methodology is clearly outlined on the basis of a practical modelling example. Thereupon we demonstrate the architecture and functions of an XML-net based prototype software tool supporting SCPM.","Supply chain management,
Supply chains,
Application software,
Production,
Logistics,
Petri nets,
XML,
Software standards,
Computer architecture,
Software prototyping"
Language and compiler design for streaming applications,"Summary form only given. We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly important. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to improve the performance of streaming applications via stream-specific analysis and optimizations. We motivate, describe and justify the language features of StreamIt, which include a structured model of streams, a messaging system for control, and a natural textual syntax.","Programming profession,
Application software,
Productivity,
Robustness,
Optimizing compilers,
Streaming media,
Cellular phones,
Program processors,
Computer science,
Artificial intelligence"
Modular analysis of systems composed of semiautonomous subsystems,"This paper reviews a proposal for the modular analysis of Petri nets and its applicability to factory automation systems. It presents new algorithms to harness this modular analysis in the determination of reachable states with specified partial markings, to determine possible deadlocks, both global and local, and also liveness. These algorithms have been implemented in a prototype tool which has then been used to solve a problem in factory automation which, even for relatively simple configurations, can lead to state spaces beyond the capabilities of many analysis tools.",
Network-aware multicasting for video-on-demand services,"In this paper, we propose a network-aware multicast scheme to supply instantaneous VOD (video-on-demand) services. Being different from traditional VOD systems, this system is implemented based on peer-to-peer grid. By taking advantage of the large storage and the powerful processing capability in client-side devices, the user host serves as both a client and a non-dedicated video server. The system capability is enhanced due to the contribution of user hosts. Peer groups are formed as a collection of peers that share similar interest, and workload may be fairly apportioned among autonomous groups. In this VOD system, the adaptive video delivery mainly employs a new dynamic buffering algorithm and an improved video multicast strategy to achieve an optimal utilization of system resources, and the network-aware adaptation makes the system more robust. In cooperating with each other, the relevant servers can supply instantaneous video services for local users.",
Rendering implicit flow volumes,"Traditional flow volumes construct an explicit geometrical or parametrical representation from the vector field. The geometry is updated interactively and then rendered using an unstructured volume rendering technique. Unless a detailed refinement of the flow volume is specified for the interior, information inside the underlying flow volume is lost in the linear interpolation. These disadvantages can be avoided and/or alleviated using an implicit flow model. An implicit flow is a scalar field constructed such that any point in the field is associated with a termination surface using an advection operator on the flow. We present two techniques, a slice-based three-dimensional texture mapping and an interval volume segmentation coupled with a tetrahedron projection-based renderer, to render implicit stream flows. In the first method, the implicit flow representation is loaded as a 3D texture and manipulated using a dynamic texture operation that allows the flow to be investigated interactively. In our second method, a geometric flow volume is extracted from the implicit flow using a high dimensional isocontouring or interval volume routine. This provides a very detailed flow volume or set of flow volumes that can easily change topology, while retaining accurate characteristics within the flow volume. The advantages and disadvantages of these two techniques are compared with traditional explicit flow volumes.","Visualization,
Rendering (computer graphics),
Computer graphics,
Surface texture,
Geometry,
Aerodynamics,
Computer science,
Vectors,
Interpolation,
Manipulator dynamics"
Axiomatization of trace semantics for stochastic nondeterministic processes,"We give a complete axiomatization of trace distribution precongruence for probabilistic nondeterministic processes based on a process algebra that includes internal behavior and recursion. The axiomatization is given for two different semantics of the process algebra that are consistent with the alternating model of Hansson and the nonalternating model of Segala, respectively. It is shown that the two semantics coincide up to trace distribution precongruence.",
Learning with lecture recordings: key issues for end-users,"Computer-based recording of live presentations has become a widespread method of producing learning materials in both higher education and companies. The success of such contents depends on their acceptance by the end-users working with the materials. From their point of view, the support of certain features and, thus, the choice of formats used for network delivery and replay are crucial issues. We present a list of key factors from the end-user's perspective and report on a user study carried out to evaluate a number of formats used for delivering recorded presentations to learners.","Streaming media,
Computer science education,
Research and development,
Conducting materials,
System testing,
Data structures,
Portable computers,
ISDN,
Audio recording,
Video recording"
Lie algebra template tracking,Visual cues are often very difficult to track. We use an effective least squares estimation of the Lie algebra parameters to find the affine transformation involved in a visual region tracking. These parameters represent the geodesis of the optimal transformation orbit. Our experiments validate the effectiveness of the method.,"Algebra,
Matrix decomposition,
H infinity control,
Computer science,
Laboratories,
Least squares approximation,
Karhunen-Loeve transforms,
Pixel,
Jacobian matrices,
Optimization methods"
On mobility and context of work: exploring mobile police work,"This article aims to propose some elements for a theory of mobility. Mobility is the structural attribute of an age that heavily relies on information and mobile devices as identified by observed, cross-contextual research. It pervades most work and social organizations in various cultural and institutional expressions. Organizational structures rely heavily on the relationships of construction and utilization of information and on the context of interaction of the various actors. This paper explores issues of mobility within work activities of two distinct roles in a police force in the UK. Departing from the concept of mobility as interaction, this article seeks to put forth a more comprehensive theory of how to study the phenomenon of mobility within work organizations and across various roles. It advances the idea that mobility is linked strongly to work conditions and that in order to increase such state within organizations, we must use a triangulated analysis to understand both the relation with the environment of work as well as the relation with information.",
A simulation tool to help learning of object oriented programming basics,"In this paper we present the OOP-Anim learning environment. It was developed to help our students to learn the basic concepts of object oriented programming and to develop their programming capabilities using this paradigm. To achieve those goals students must practice intensively the development and debugging of programs. We believe this environment can help, since it uses animation to facilitate program understanding and error detection/correction. This debugging process has a lot of educational potential, as students can learn when correcting their own mistakes. When they reach a working solution, their experience and confidence normally improves, facilitating further learning. In the paper we describe the environment main features, some possible uses and the educational advantages associated with that utilization.","Object oriented modeling,
Object oriented programming,
Programming profession,
Dynamic programming,
Animation,
Error correction,
Java,
Debugging,
Algorithm design and analysis,
Books"
Applying model checking to workflow verification,"Model checking is a technique for the verification of temporal logic specifications in state-transition systems. It can be applied to software, at design stages as well as at source code level. The latter in particular is constricted by the large space requirements of model checking. However, model checking is a promising technique for the improvement of software quality. We examined the applicability of existing model checking methods and tools to software in general and with a focus on e-commerce software systems developed at Intershop. Although model checking is currently not applicable to all domains, it is useful for certain restricted fields of application. One of these domains are Intershop's e-commerce systems, consisting of so called pipelines. We show the source code related verification concept for these pipelines and is part of an overall concept for the verification and quality assurance of Intershop's products.","Logic,
Software systems,
Software design,
Power system modeling,
Explosions,
Software tools,
Pipelines,
Context modeling,
Computer science,
Poles and towers"
An approach to help select trustworthy Web services,"The emerging paradigm of Web services opens a new way of Web application design and development to quickly develop and deploy Web applications by integrating independently published Web services components to conduct new business transactions. However, how to choose the most appropriate Web services components remains a challenge. In this paper we propose a user-centered, mobile agent-based, fault injection-equipped, and assertion-oriented approach to assist Web services requesters to select trustworthy Web services components. Our approach can largely enhance the efficiency of Web services components selection process, and provide a more cost-efficient method to help service requesters make better decisions","Web services,
Web and internet services,
Simple object access protocol,
Computer science,
Application software,
Systems engineering and theory,
Software measurement,
Software systems,
Assembly,
Delay effects"
Integrative science: biosignal processing and modeling,"Coupling computational modeling and information processing in biology and medicine is a major challenge for better comprehending structures and functions of living systems. Signal processing should extract the relevant information required to explore complex organization levels, at all space and time scales. Advances coming from applied physics and mathematics are challenged by extremely hot topics in biology and medicine. The biomedical scene has proven to be the most difficult to address due to the fact that biomedical processes involve nonGaussian, nonlinear, and nonstationary components. This paper provides some clues on processing schemes such as time and frequency transforms, blind signal separation, independent component analysis, empirical mode decomposition, particle methods and Kernel methods that may help in lessening the ambiguity about the observed components of the mixtures to be handled and, this way, facilitating their matching with models.",
"Low energy, highly-associative cache design for embedded processors","Many embedded processors use highly associative data caches implemented using a CAM-based tag search. When high-associativity is desirable, CAM designs can offer performance advantages due to fast associative search. However, CAMs are not energy efficient. This paper describes a CAM-based cache design which uses prediction to reduce energy consumption. A last used prediction is shown to achieve an 86% prediction accuracy, on average. A new design integrating such predictor in the CAM tag store is described. A 30% average D-cache energy reduction is demonstrated for the MiBench programs with little additional hardware or impact on processor performance. Even better results can be achieved with another predictor design which increases prediction accuracy. Significant static energy reduction is also possible using this approach for the RAM data store.","Process design,
Computer aided manufacturing,
CADCAM,
Read-write memory,
Energy consumption,
Energy efficiency,
Accuracy,
Delay,
Hardware,
Computer science"
Privacy-preserving data mining on data grids in the presence of malicious participants,"Data privacy is a major threat to the widespread deployment of data grids in domains such as health care and finance. We propose a novel technique for obtaining knowledge - by way of a data mining model - from a data grid, while ensuring that the privacy is cryptographically secure. To the best of our knowledge, all previous approaches for solving this problem fail in the presence of malicious participants. In this paper we present an algorithm which, in addition to being secure against malicious members, is asynchronous, involves no global communication patterns, and dynamically adjusts to new data or newly added resources. As far as we know, this is the first privacy-presenting data mining algorithm to possess these features in the presence of malicious participants. Simulations of thousands of resources prove that our algorithm quickly converges to the correct result. The simulations also prove that the effect of the privacy parameter on the convergence time is logarithmic.","Data mining,
Data privacy,
Statistics,
Statistical distributions,
Distributed databases,
Radio access networks,
Computer science,
Medical services,
Investments,
Law"
Evaluating GML support for spatial databases,"This work presents a study on Geography Markup Language (GML), the issues that arise from using GML for spatial databases and solutions that have been proposed. We concentrate on three aspects of GML, including storage, parsing, and querying. GML is an XML encoding for storing geographic data. It has been developed by the OpenGIS as a medium for uniform geographic data storage and exchange among diverse applications. The underlying concepts of XML therefore can also be applied to GML data. This results in both advantages and disadvantages, which are discussed In the work. GML is a comparatively new language in the field of geographic information systems and still in its developmental stage. Most of the data processing techniques need to be further developed in order for GML to be an efficient medium for geographic data storage and processing.","Spatial databases,
XML,
Markup languages,
Encoding,
Database systems,
Computer science,
Geography,
Memory,
Geographic Information Systems,
Data processing"
Reducing coverage collection overhead with disposable instrumentation,"Testers use coverage data for test suite quality assessment, stopping criteria definition, and effort allocation. However, as the complexity of products and testing processes increases, the cost of coverage data collection may grow significantly, jeopardizing its potential application. We present two techniques to mitigate this problem based on the concept of ""disposable coverage instrumentation"": coverage instrumentation that is removed after its usage. The idea is to reduce coverage collection overhead by removing instrumentation probes after they have been executed. We have extended a Java virtual machine to support these techniques, and show their potential through empirical studies with the Specjvm98 and Specjbb2000 benchmarks. The results indicate that the techniques can reduce coverage collection overhead between 18% and 97% over existing techniques.","Instruments,
Probes,
Java,
Costs,
Virtual machining,
Computer science,
Data engineering,
Quality assessment,
Benchmark testing,
Fluid flow measurement"
Admission control for variable bit rate traffic using variable service interval in IEEE 802.11e WLANs,"The IEEE 802.11 working group is currently working on the standard IEEE 802.11e and introduces the hybrid coordination function (HCF) to provide better QoS support to real-time traffic. A reference design of simple scheduling and admission control algorithm is proposed in a TGe consensus proposal. However, this scheduling and admission control unit only consider the mean data rate and mean packet size. The rate and packet size variation are not taken into account. Thus, it is only efficient to CBR traffic and the packet loss rate of VBR traffic may be very high. In W. F. Fan et al. (Aug. 2004), we analyzed the packet loss rate of the reference scheme and proposed a new method to determine the effective TXOP duration for admission control so that the packet loss rate of VBR flows can be guaranteed. In both reference and our proposed scheme, all stations use fixed schedule service interval (SI) which is the minimum of all maximum service interval of all admitted flows. Thus, maximum packet delay of all stations is limited by the most stringent SI and some traffic with larger delay bound may be over-guaranteed. Also, the efficiency of the admission control scheme in W. F. Fan et al. (Aug. 2004), becomes lower than that of the reference one. In this paper, we extend our admission control scheme by using variable service interval to improve the efficiency about 20% - 30% and avoid over guarantee on packet delay. Also, the packet loss rate of VBR traffic can be guaranteed","Admission control,
Bit rate,
Wireless LAN,
Delay,
Quality of service,
Media Access Protocol,
Traffic control,
Access protocols,
Scheduling algorithm,
Computer science"
A novel self-configuration mechanism for heterogeneous P2P networks,"We propose a simple and efficient mechanism to self-configure semi-structured overlay networks for heterogeneous peer-to-peer systems with no global knowledge. The overlay network is constructed as a hierarchical graph based on peer power which is an aggregate measure of available resources. We use index based, random walk JOIN and LEAVE protocols to build the overlay network in which node degree and workload correspond to its peer power. We also propose an index based query scheme for peers to search documents in the network. Performance studies demonstrate that the proposed self-configuration protocol and search scheme are efficient and viable. The self-configured P2P network has significantly lower query cost than traditional Gnutella-like P2P systems. We further propose a caching assisted query mechanism where query results are cached by intermediate nodes along the search path, thereby exploiting the surplus storage power of the peers and further improving query efficiency. Caching assisted query permits a quicker response to subsequent queries for popular data without incurring the excessive overhead of globally replicated directory information. Our experimental study reveals that the caching assisted query provides an additional reduction of up to 40% in messages needed per query.","Peer to peer computing,
Protocols,
Costs,
Network topology,
Scalability,
Computer science,
Aggregates,
Power measurement,
Cache storage,
Delay"
Characteristics of large group support systems,"In this paper, we present an evaluation of large group support systems and literature in three fields: group support systems (GSS), computer supported cooperative work, and online self-organizing social systems. In addition to literature, we investigated over 25 large group systems (18 are presented here). Since GSS research has typically specialized in smaller groups, this research serves to categorize groupware applications and introduce GSS researchers to large groupware concepts and the large groupware landscape.","Collaborative software,
Collaborative work,
Recommender systems,
International collaboration,
Application software,
Internet,
Information systems,
Concurrent computing,
Urban planning,
Sorting"
Safety tactics for software architecture design,"The influence of architecture in assurance of system safety is being increasingly recognised in mission-critical software applications. Nevertheless, most architectural strategies have not been developed to the extent necessary to ensure safety of these systems. Moreover, many software safety standards fail to discuss the rationale behind the adoption of alternative architectural mechanisms. Safety has not been explicitly considered by existing software architecture design methodologies. As a result, there is little practical guidance on how to address safety concerns in 'shaping' a 'safe' software architecture. This work presents a method for software architecture design within the context of safety. This method is centred upon extending the existing notion of architectural tactics to include safety as a consideration. The approach extends existing software architecture design methodologies and demonstrates the true value of deployment of specific protection mechanisms. The feasibility of this method is demonstrated by an example.","Software safety,
Software architecture,
Software design,
IEC standards,
Mission critical systems,
Application software,
Software standards,
Design methodology,
Protection,
Computer science"
Broadband optical wireless Internet: delay optimization,"In this paper, we investigate the link flexibility constraints imposed on the deployment of the optical wireless Internet. We formulate the topology control problem under link delay conditions, and accordingly propose a new optimization algorithm for networks with two-transceiver nodes. Simulation results show that networks applying the proposed algorithm achieve a reduction in average packet delay up to 55% over networks with rigid topologies.","Internet,
Delay,
High speed optical techniques,
Network topology,
Telecommunication network topology,
IP networks,
Transceivers,
Optical fiber networks,
Computer science,
Wireless networks"
A design of small-area automatic wheelchair,"There are lots of handicaps and elders in the world. Many of them are not able to move as easily as normal people. It is useful if we develop an automatic wheelchair to help them move more freely. In this paper we proposed a design of small-area automatic wheelchair to help handicaps or elders be able to move easily in a small area. The most concern in this project is low cost with acceptable performance rather than high velocity or high accuracy. The design integrates several technologies to apply on the wheelchair, including wireless positioning, automatic mobile technology, and wireless communication. On the other hand, we proposed a design that duplicable information can be computed on server side, and information which is not duplicable can be computed in constant time. Therefore, our design of small-area automatic wheelchair can cost very little. We also proposed a design of building map information to make the whole design more complete.",
Adaptive averaging for improved SNR in real-time coronary artery MRI,"A technique has been developed for combining a series of low signal-to-noise ratio (SNR) real-time magnetic resonance (MR) images to produce composite images with high SNR and minimal artifact in the presence of motion. The main challenge is identifying a set of real-time images with sufficiently small systematic differences to avoid introducing significant artifact into the composite image. To accomplish this task, one must: 1) identify images identical within the limits of noise; 2) detect systematic errors within such images with sufficient sensitivity. These steps are achieved by evaluating the correlation coefficient (CC) between regions in prospective images and a template containing the anatomy of interest. Images identical within noise are selected by comparing the measured CC values to the theoretical distribution expected due to noise. Sensitivity for systematic error depends on the SNR of the CC(=SNR/sub CCmax/), which in turn depends on the noise, and the template size and structure. By varying the template size, SNR/sub CCmax/ may be altered. Experiments on phantoms and coronary artery images demonstrate that the SNR/sub CCmax/ necessary to avoid introducing significant artifact varies with the target composite SNR. The future potential of this technique is demonstrated on high-resolution (/spl sim/0.9 mm), reduced field-of-view real-time coronary images.",
On evaluating the trade-offs between broadcasting and multicasting in ad hoc networks,"Multicasting in ad hoc networks has received a lot of attention for the important application of disseminating information to multiple recipients. Most multicast protocols require the creation and maintenance of a structure (such as a tree or a mesh) for distributing information to the group members. In contrast, broadcast schemes are simple schemes which aim to distribute the information to all or a fraction of the nodes in the network without having a structural framework. While the creation/maintenance of the structure could potentially be cumbersome and heavyweight, multicast does offer benefits in terms of restricting the number of nodes that perform rebroadcasts. We argue that it is not a given that multicast is a better choice for group communications in all possible scenarios and that there could be circumstances wherein the use of a simple broadcast based technique would be more advantageous. In support of this claim, we study various scenarios to evaluate and quantify the trade-offs between broadcasting and multicasting. In particular, we perform simulation experiments using the on-demand multicast routing protocol and the simple broadcast algorithm as candidate protocols for multicasting and broadcasting, respectively. These protocols have been shown to be the elite protocols in their classes in prior work. Our results demonstrate that multicasting is preferable only under conditions of moderate mobility and with multicast group sizes smaller than 40%.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Multicast protocols,
Routing protocols,
Communication system control,
Computer science,
Application software,
Casting,
Multicast algorithms"
Embedding semantic annotations into dynamic Web contents,"The semantic Web explosion has not yet been accompanied by efficient search engines or intelligent applications that can exploit semantic query capabilities. This is the reason why the amount of semantically annotated Web pages is still very small compared to the total amount of information on the Web. In this sense, there have been several approaches that deal with the semantic annotation of static Web pages. However, they do not support the annotation of dynamically generated Web content. This paper offers two main contributions: first, the automatic generation of a function that dynamically calculates annotations for a Web document; secondly, a novel approach for annotating dynamic Web documents by annotating the queries enveloped in each document and their results.",
Providing an e-learning platform in a university context - balancing the organisational frame for application service providing,"Universities have severe difficulties in using e-learning applications successfully due to organisational problems to provide them. Providing a Web-based learning environment is an enormous effort not only from the didactical and organisational perspective, but also from an administrative and technical point of view. In this paper, we critically review two case studies for making software available in a university context by applying a model for application service providing. ASP in the university context is a different business than ASP between companies. A much more flexible relation between service provider and consumer needs to be established. We describe the necessary process that is based on an evolutionary approach consisting of repeated loops that help to balance the organisational frame for application service providing on the four different levels: application domain, organisation, services, and contracts.","Electronic learning,
Context-aware services,
Application specific processors,
Application software,
Hardware,
Network servers,
Contracts,
Production systems,
Informatics,
Context modeling"
Retrieving imaged documents in digital libraries based on word image coding,"A great number of documents are scanned and archived in the form of digital images in digital libraries, to make them available and accessible in the Internet. Information retrieval in these imaged documents has become a growing and challenging problem. For this purpose, a word image coding technique is proposed in this paper, and a Web-based system for efficiently retrieving imaged documents from digital libraries is described. Some image preprocessing is first carried out offline to extract word objects from imaged documents stored in the digital library. Then each word object is represented by a string of feature codes. As a result, each document image is represented by a series of feature code strings of its words, which are stored in a feature code file. Upon receiving a user's request, the server converts the query word into feature code string using the same conversion mechanism as is used in producing feature codes for the underlying imaged documents. Searching is then performed among those feature code files generated offline. An inexact string matching technique, with the ability of matching a word portion, is applied to match the query word with the words in the documents, and then the occurrence frequency of the query word in each corresponding document is calculated for relevant ranking. Preliminary experimental results with some imaged documents of students' theses in the digital library of our university show that the proposed approach is efficient and promising for retrieving imaged documents, with potential applications to digital libraries.","Image retrieval,
Software libraries,
Image coding,
Information retrieval,
Image converters,
Data mining,
Optical character recognition software,
Document image processing,
Computer science,
Digital images"
Session-based service discovery in peer-to-peer communications,"The paper presents an approach for service discovery in session-based peer-to-peer communications. It is an approach by which service discovery can be securely extended into the Internet domain for spontaneous collaborative applications, secure in that services are only available during the period of time that the session is in place. An implementation of the approach is presented that demonstrates the integration of the IETF service location protocol (SLP) with the IETF session initiation protocol (SIP). In order to do this, SIP SDP (session description protocol) specifications had to be developed for SLP and a peer-to-peer model for SLP was also developed. The proposed methodology can also be used to share services from local wireless personal area networks (WPAN) like Bluetooth. In order to demonstrate this, a Bluetooth-SLP gateway was developed as well as a SIP SDP for Bluetooth OBEX profiles.","Peer to peer computing,
Web and internet services,
Multimedia communication,
Broadcasting,
Bluetooth,
Multicast protocols,
Access protocols,
Information technology,
Computer science,
Collaboration"
Light weight space leaping using ray coherence,"We present a space leaping technique for accelerating volume rendering with very low space and run-time complexity. Our technique exploits the ray coherence during ray casting by using the distance a ray traverses in empty space to leap its neighboring rays. Our technique works with parallel as well as perspective volume rendering, does not require any preprocessing or 3D data structures, and is independent of the transfer function. Being an image-space technique, it is independent of the complexity of the data being rendered. It can be used to accelerate both time-coherent and noncoherent animation sequences.","Acceleration,
Data structures,
Casting,
Rendering (computer graphics),
Transfer functions,
Computer graphics,
Image generation,
Coherence,
Runtime,
Computer science"
Engineering a method for wide audience requirements elicitation and integrating it to software development,"Consumer oriented information systems development has become increasingly important matter, as more and more complex information systems are targeted towards consumer markets. We argue that developing IS for non-organizational users creates new problems, which IS and requirement engineering (RE) community should attend to. First of all, the elicitation of requirements becomes more difficult as usually consumers do not explicitly know what they want, and it is difficult for them to express their ideas. To support different views of product development, such as project management and design, the method should present requirements in a 'rich enough' way to avoid overloading management, but in the same time giving designers the detailed information they need. Last but not the least, the results of requirements engineering should be easy to integrate to the software development process. To this end, we have constructed a new RE method and its support environment within Metaedit+ Meta CASE tool. We based our method on critical success chains (CSC) method, which supports top-down approach for planning, but also provides for wide participation of IS customers to get rich information. CSC aggregates the results of many individual interviews into meaningful graphical models of what is critically important about a potential system. In our work, CSC is extended with customer segmentation and lead user concepts from marketing.",
A UML based approach for modeling and implementing multi-agent systems,,"Unified modeling language,
Multiagent systems,
Object oriented modeling,
Java,
Power system modeling,
Computer languages,
Permission,
Computer science,
Application software,
Appropriate technology"
"Size, shape, orientation, speed, and duration of GPS equatorial anomaly scintillations","GPS L1 C/A signal scintillation data were collected at the equatorial anomaly over a period of three months using five receivers spaced on magnetic east-west and north-south axes to examine the speed, orientation, shape, width, and duration of GPS scintillation fade patterns. The nighttime speeds were primarily eastward in the range of 100-200 m/s with a significant spread to both larger values and negative (westward) values as expected, given the known behavior of ionospheric drifts and GPS signal path movement. The characteristic velocity was found to be small so that the true velocity was equal to the apparent velocity to a very good approximation. The orientation of the scintillation fade patterns was organized by a simple projection model of the magnetic field along the GPS signal path onto the horizontal plane when the signal paths were aligned no closer than 60° from the magnetic field. The shape of the scintillation fade pattern was greatly elongated in the magnetic north-south direction, and no change could be detected over a distance of 1 km. The east-west widths of the scintillation fade patterns were variable, but after normalizing to the elevation angle, accounting for the fade orientation, and eliminating signal paths within 60° of the magnetic field, an organized scale length of about 450 m was determined. The duration of the scintillation fade patterns was examined using the optimal cross-correlation amplitude as a measure of change. For a 5 s duration, 49% of the optimal cross-correlation amplitudes exceed a value of 0.8.","Receivers,
Global Positioning System,
Ionosphere,
Shape,
Geophysical measurements,
Fluctuations,
Plasmas"
Transient stability analysis of large aluminum stabilized superconductor by 2D and 3D finite element analysis,"Very-large-current composite superconductors are used in SMES coils and fusion applications. These superconductors have large cross-sectional areas of high purity aluminum to improve their stability. Once a normal zone is initiated in such superconductors the current transfers from the superconducting strands to the aluminum stabilizer according to the Maxwell's equations and the temperature distribution. However, the time constant of current diffusion in the aluminum stabilizer is very long as electrical resistivity of aluminum is very low. Therefore, excess Joule heating is generated in a small region of aluminum stabilizer near superconducting strands, and the temperature increases locally. Some 2D numerical analyses have been carried out in order to investigate the transient stability of the superconductor applied to the helical coil of LHD in National Institute for Fusion Science. But, as the performance of computers have improved, huge numerical simulations are new feasible. So we wrote a 3D finite element analysis code ourselves to carry out some now analyses that we compared with 2D analysis.","Transient analysis,
Stability analysis,
Aluminum,
Finite element methods,
Superconducting coils,
Superconducting magnetic energy storage,
Samarium,
Maxwell equations,
Temperature distribution,
Electric resistance"
A relay-based multirate protocol in infrastructure wireless LANs,"In this paper, we introduce a relay-based adaptive auto rate (RAAR) protocol that can find a suitable relay node for data transmission between transmitter and receiver. In RAAR, a MH can dynamically adjust its modulation scheme to achieve the maximal throughput according to the transmission distance and the channel condition. Evaluation results show that this scheme can provide significant improvements on throughput for nodes located at the fringe of the AP's transmission range, thus remarkably improving overall system performance.","Relays,
Wireless application protocol,
Wireless LAN,
Local area networks,
Throughput,
Computer science,
Information management,
Data communication,
Transmitters,
System performance"
Deterring password sharing: user authentication via fuzzy c-means clustering applied to keystroke biometric data,"This work describes a clustering-based system to enhance user authentication by applying fuzzy techniques to biometric data in order to deter password sharing. Fuzzy c-means is used to train personal, per-keyboard profiles based on the keystroke dynamics of users when entering passwords on a keyboard. These profiles use DES encryption taking the actual passwords as key and are read at logon time by the access control mechanism in order to further validate the identity of the user. Fuzzy values obtained from membership functions applied to the input (i.e., keystroke latencies) are compared against profile values, and a match, within a certain precision threshold /spl gamma/, will grant access to the user. With this technique, even when user A shares password P/sub A/ with user B, B will still be denied access unless he is capable of mimicking the keystroke dynamics of A. We describe the motivation, design, and implementation of a prototype whose results indicate the accuracy level and feasibility of the approach.","Authentication,
Bioinformatics,
Keyboards,
Biometrics,
Information security,
Fuzzy systems,
Access control,
Cryptography,
Intelligent systems,
Identity-based encryption"
Mining association rules from relations on a parallel NCR teradata database system,"Data mining from relations is becoming increasingly important with the advent of parallel database systems. In this paper, we propose a new algorithm for mining association rules from relations. The new algorithm is an enhanced version of the SETM algorithm of M. Houtsma and A. Swami (1995), and it reduces the number of candidate itemsets considerably. We implemented and evaluated the new algorithm on a parallel NCR teradata database system. The new algorithm is much faster than the SETM algorithm, and its performance is quite scalable.","Data mining,
Association rules,
Database systems,
Itemsets,
Relational databases,
Transaction databases,
Computer science,
Data engineering,
Performance analysis,
Algorithm design and analysis"
NIC-based offload of dynamic user-defined modules for Myrinet clusters,"Many of the modern networks used to interconnect nodes in cluster-based computing systems provide network-interface cards (NICs) that offer programmable processors. Substantial research has been done with the focus of offloading processing from the host to the NIC processor. However, the research has primarily focused on the static offload of specific features to the NIC, mainly to support the optimization of common collective and synchronization-based communications. We describe the design and implementation of a framework based on MP1CH-GM to support the dynamic NIC-based offload of user-defined modules for Myrinet clusters. We evaluate our implementation on a 16-node cluster using a NIC-based version of the common broadcast operation and we find a maximum factor of improvement of 1.2 with respect to total latency as well as a maximum factor of improvement of 2.2 with respect to average CPU utilization under conditions of process skew. In addition, we see that these improvements increase with system size, indicating that our NIC-based framework offers enhanced scalability when compared to a purely host-based approach.","Computer networks,
Microprogramming,
Laboratories,
Delay,
Computer science,
Broadcasting,
Multiprocessor interconnection networks,
Contracts,
Debugging,
Production systems"
"Microprocessors: from theory to practice, a didactic experience","The study of microprocessors in computer or electronic engineering programs faces a great problem: the migration from theory to practice, from the abstract concepts to laboratory experimentation. In this work we present the learning methodology adopted by us in the last years in the microprocessor course. In this methodology the student is driven from the classroom theory to laboratory experimentation. Several tools are applied, as the hardware description language for microcomputers systems modeling and FPGA implementations, microcontroller experiments based on proto-o-board assemblies and more complex microprocessor applications based on electronics kits previously developed in the computer engineering program. This methodology applied in this course has been gradually improved through the time, and the improvement process continues, although we believe to have reached a format that integrates theory and practice, where the students have control of the process, involving microprocessors and its applications.","Microprocessors,
Laboratories,
Application software,
Hardware design languages,
Microcomputers,
Modeling,
Field programmable gate arrays,
Microcontrollers,
Assembly systems,
Process control"
Use of dynamic emulation of mechanical loads in the design of adjustable speed applications,"A rapid prototyping procedure is presented in the paper. An approach to the dynamic emulation of mechanical loads, the ""hardware-in-the-loop"" procedure where the applied prototype is reused for many different applications using the same drive, is explained and analyzed. Instead of the actual load, a torque driven machine is introduced. The load torque, which it produces, is calculated in such a manner, that the input/output behavior of the experimental system resembles one of the actual (emulated) one. A simple feedforward scheme, combined with a feedback controller, is used fore the calculation. In contrast to the existing references the method is simple and requires no torque measurement. The method is used for the evaluation of the control algorithms, regardless the control and estimation methods used. Algorithms are developed with the use of Matlab/Simulink and experiments are executed with the use of the dSPACE system.","Emulation,
Nonlinear dynamical systems,
Application software,
Prototypes,
Inverse problems,
Computer science,
Adaptive control,
Torque measurement,
Open loop systems,
Variable speed drives"
Electromigration-dependent parametric yield estimation,"We define and investigate the problem of electromigration faults caused by spot defects during the VLSI manufacturing process. Analysis is given for a simple layout, and simulations are presented and discussed for a more complicated case. It is shown that in some cases, electromigration-dependent parametric faults can make a significant contribution to the total yield estimation.","Yield estimation,
Wire,
Very large scale integration,
Electromigration,
Circuit faults,
Manufacturing processes,
Frequency estimation,
Computer science,
Computational modeling,
Analytical models"
Multicast-based inference of network-internal loss,"The use of multicast traffic as measurement probes is efficient and effective to infer network-internal characteristics. We propose a new statistical approach to infer network internal link loss performance from end-to-end measurements. Incorporating with the procedure of topology inference, we present an inference algorithm that can infer loss rates of individual links in the network when it infers the network topology. It is proved that the loss rate inferred by our approach is consistent with the real loss rate as the number of probe packets tends to infinity. The approach is also extended to general trees case for loss performance inference. Loss rate-based scheme on topology inference is built in view of correct convergence to the true topology for general trees.",
Empirical evaluation of the fault-detection effectiveness of smoke regression test cases for GUI-based software,"Daily builds and smoke regression tests have become popular quality assurance mechanisms to detect defects early during software development and maintenance. In previous work, we addressed a major weakness of current smoke regression testing techniques, i.e., their lack of ability to automatically (re)test graphical user interface (GUI) event interactions - we presented a GUI smoke regression testing process called daily automated regression tester (DART). We have deployed DART and have found several interesting characteristics of GUI smoke tests that we empirically demonstrate in this paper. We also combine smoke tests with different types of test oracles and present guidelines for practitioners to help them generate and execute the most effective combinations of test-case length and test oracle complexity. Our experimental subjects consist of four GUI-based applications. We generate 5000-8000 smoke tests (enough to be run in one night) for each application. Our results show that: (1) short GUI smoke tests with certain test oracles are effective at detecting a large number of faults; (2) there are classes of faults that our smoke test cannot detect; (3) short smoke tests execute a large percentage of code; and (4) the entire smoke testing process is feasible to do in terms of execution time and storage space.","Software testing,
Computer aided software engineering,
Graphical user interfaces,
Automatic testing,
Application software,
Software maintenance,
Smoke detectors,
Programming profession,
Computer science,
Educational institutions"
Software modeling techniques for a first course in software engineering: a workshop-based approach,"Computer science studies in Spain are organized in such a way that software engineering courses normally appear in the last years. This situation establishes a programming-first approach in the overall curricular structure, and the resulting situation produces several unwanted side effects. One of the main problems is that students lack the abstraction capability they need in order to model software. For this reason, the practical classes of the first software engineering course have been organized in workshop sessions, where each session is devoted to a concrete modeling technique. A workshop session is built around a problem published on the teacher's website. The session is divided into two parts. In the first part, the students are organized into work groups, and they give an initial solution to the proposed modeling problem. In the second part of the session, one volunteer group presents its solution, and afterwards, in a moderated debate, the rest of the students discuss the proposed solution. Finally, the volunteer group writes a workshop session report that contains the final agreed-on solution. This approach has been successfully applied to the software modeling practical classes of a first software engineering course during the last five years, achieving important improvements in the students' abstraction capabilities, which is much needed in software modeling.","Computer science education,
Software engineering,
Engineering education"
Analyzing the secure overlay services architecture under intelligent DDoS attacks,"Distributed denial of service (DDoS) attacks are currently major threats to communication in the Internet. A secure overlay services (SOS) architecture has been proposed to provide reliable communication between clients and a target under DDoS attacks. The SOS architecture employs a set of overlay nodes arranged in three hierarchical layers that controls access to the target. Although the architecture is novel and works well under simple congestion based attacks, we observe that it is vulnerable under more intelligent attacks. We generalize the SOS architecture by introducing more flexibility in layering to the original architecture. We define two intelligent DDoS attack models and develop an analytical approach to study the impacts of the number of layers, number of neighbors per node and the node distribution per layer on the system performance under these two attack models. Our data clearly demonstrate that performance is indeed sensitive to the design features and the different design features interact with each other to impact overall system performance.","Computer crime,
Availability,
System performance,
Web and internet services,
Communication system control,
Performance analysis,
Resilience,
Communication system security,
Computer science,
Computer architecture"
Lattice problems in NP /spl cap/ coNP,"We show that the problems of approximating the shortest and closest vector in a lattice to within a factor of /spl radic/n lie in NP intersect coNP. The result (almost) subsumes the three mutually-incomparable previous results regarding these lattice problems: Banaszczyk (1993), Goldreich and Goldwasser (2000), and Aharonov and Regev (2003). Our technique is based on a simple fact regarding succinct approximation of functions using their Fourier transform over the lattice. This technique might be useful elsewhere - we demonstrate this by giving a simple and efficient algorithm for one other lattice problem (CVPP,) improving on a previous result of Regev (2003). An interesting fact is that our result emerged from a ""dequantization"" of our previous quantum result in (Aharanov and Regev, 2003). This route to proving purely classical results might be beneficial elsewhere.","Lattices,
Computer science"
A novel approach to cartoon style rendering of an image with an approximated crayon texture,"We present an efficient approach to cartoon style rendering of an image with an approximated crayon texture. The algorithm generates a crayon texture map, with two crayon textures (illuminated color texture and shadowed color texture) combined into one. The crayon texture is generated using crayon stroke generation technique. The texture is then mapped onto the image in cartoon style using per-vertex mapping concept. This algorithm results in an improved transition boundary between the two texture colors, comparable to the normal cartoon shaded image without crayon texture. If the image generated by this algorithm is animated, then the transition boundary would move logically with the object as compared to our earlier algorithm (described in the text), where transition boundary changed drastically with slight movement of an object in the image. The present approach can be used to generate variations in cartoon shading easily and efficiently. We have also attempted to approximate the effect of distribution of wax in the resulting image.",
Reducing multiplier energy by data-driven voltage variation,"Design of portable battery operated multimedia devices requires energy-efficient multiplication circuits. This paper proposes a new technique to reduce power consumption of digital multipliers. In contrast to related methods which concentrate on transition activity reduction, we focus on dynamic reduction of supply voltage. Two implementation schemes capable of dynamically adjusting a double voltage supply to input data variation are presented. Simulations show that using these schemes we can reduce energy consumption of 16/spl times/16 bit multiplier in DCT computation by 33.4% and 25.2% on average without any speed degradation and as low as 4.7% area overhead.","Voltage,
Circuits,
Energy consumption,
Delay,
Discrete cosine transforms,
Digital signal processing,
Energy dissipation,
Optimization methods,
Clocks,
Computer science"
The fractal nature of Bezier curves,"Fractals are attractors - fixed points of iterated function systems. Bezier curves are polynomials - linear combinations of Bernstein basis functions. The de Casteljau subdivision algorithm is used here to show that Bezier curves are also attractors. Thus, somewhat surprisingly, Bezier curves are fractals. This fractal nature of Bezier curves is exploited to derive a new rendering algorithm for Bezier curves.","Fractals,
Polynomials,
Gaskets,
Convergence,
Solid modeling,
Computer science"
Interleaving harmonic broadcasting and receiving scheme with loss-anticipation delivery,"With the growth of broadband networks, video-on-demand (VoD) has become realistic. Many significant broadcasting schemes have been proposed to reduce the bandwidth requirement for stored popular videos, but they cannot be used for provide reliable delivery over lossy channels perfectly. Herein, we propose a new broadcasting scheme, called the interleaving harmonic broadcasting (IHB) scheme, which guarantees continuous playback, and mitigates the effect of packet losses. Further, in comparison with the poly-harmonic broadcasting (PHB), cautions harmonic broadcasting (CHB) scheme, reliable periodic broadcasting (RPB) scheme and second chance broadcasting (SCB) scheme, the IHB outperforms on required bandwidth, maximum required buffers and maximum disk transfer rate.","Interleaved codes,
Broadcasting,
Multimedia communication,
Bandwidth,
Video on demand,
Harmonic analysis,
Streaming media,
Computer science,
Information management,
Broadband communication"
Depicting shape features with directional strokes and spotlighting,"This paper presents a new algorithm and technique for rendering triangular surfaces in pen-and-ink edge-based strokes. Our technique integrates two very important illustration strategies for depicting shape features: selection of drawing direction and the use of light. Drawing direction is given by four stroke directional fields. For lighting, we introduce the idea of ""spotlight silhouettes"" for fast illumination computation, with target tone matched by adaptive stroke length adjustment. Stroke style is achieved by path perturbation and noise-based weight control. Our technique also allows visual effects of reverse tone values and depth cueing. Examples with models from anatomy and archeology demonstrate the capabilities of our system","Shape,
Ink,
Lighting,
Visual effects,
Anatomy,
Computer science,
Noise shaping,
Weight control,
Solids,
Optical attenuators"
An evaluation of approaches to classification rule selection,"In this paper a number of classification rule evaluation measures are considered. In particular the authors review the use of a variety of selection techniques used to order classification rules contained in a classifier, and a number of mechanisms used to classify unseen data. The authors demonstrate that rule ordering founded on the size of antecedent works well given certain conditions.","Association rules,
Data mining,
Computer science,
System testing,
Chromium"
The design and performance of configurable component middleware for distributed real-time and embedded systems,"QoS-enabled component middleware solutions can help reduce the programming complexity of configuring real-time aspects, such as priorities and rates of invocation. However, few empirical studies have been conducted to guide distributed real-time and embedded (DRE) system developers in choosing among alternative configuration mechanisms and performance optimization techniques in practice. This paper makes three contributions to research on QoS-enabled component middleware for DRE systems in the context of the component-integrated ACE ORB (CIAO). First, it describes the design of CIAO's static component configuration mechanisms, which enhance configurability by avoiding features that are not supported by key real-time platforms, while reducing run-time overhead and footprint. Second, it compares the performance of dynamic and static configuration mechanisms in CIAO to help guide the selection of suitable configuration mechanisms based on specific requirements of each DRE system. Third, it presents an empirical comparison of CIAO's static configuration mechanisms to the static configuration mechanisms in Boeing's PRISM avionics component middleware solution.",
On the modeling and optimization of discontinuous network congestion control systems,"AIMD is a widely-used network congestion control scheme. Despite its discontinuous control behavior, the majority of contemporary literature employed a statistically-averaged continuous model to approximate AIMD without considering its discontinuity. The design of a discontinuous control system must be based on rules that are entirely different from that of continuous control systems. Ignoring discontinuity issues results in great discrepancy between analytical models and the practice. In this paper we use the sliding mode control (SMC) theory to investigate congestion control without ignoring its discontinuity. Based on the SMC theory, the design of discontinuous (congestion) control systems must consider the relative degree and zero dynamics of the system, in order to guarantee asymptotic stability. This framework can precisely reflect the behavior of the control rules and the controlled objective of a congestion control system. We show that the relative degree of the control system of rate-based, AIMD flow-control algorithms is two. That is, to apply sound control principles to the design of AIMD algorithms, one should use both the queue length error and its first order time derivative to construct the switching function of the control model of an active queue management scheme. Based on the SMC model, one can quantify the tradeoffs among the convergence speed, the amount of throttling adjustments, and the degree of oscillations. We show quantitatively that one can guarantee stability conditions, drastically reduce oscillation of AIMD without significant loss of fairness and stability, and quantitative understanding of the tradeoffs among oscillation, delay and fairness.",
Achieving packet-level quality of service through scheduling in multirate WLANs,"Wireless packet scheduling has been a popular paradigm to achieve packet-level QoS in terms of fairness and throughput in the presence of channel errors. However, the current design does not anticipate the multi-rate capability offered by the IEEE 802.11a/b/g physical layer, thus suffering significant performance degradation in 802.11 WLANs. In this paper, we propose multirate wireless fair scheduling (WMFS). In MWFS, each flow is granted a temporal fair share of the channel, in contrast to the throughput fair share adopted by existing algorithms. Therefore, each flow receives services in proportion to its perceived transmission rate, and high-rate flows are able to opportunistically exploit their good channel conditions and receive more services. MWFS also renovates the compensation model in order to allow for error-prone flows to catch up, thus ensuring fairness for all flows over error-prone channels. We demonstrate the effectiveness of MWFS through both simulations and analysis. Especially, WMFS achieves system throughput 159% of state-of-the-art scheduling algorithms in simulated scenarios.",
Year,,
A battery aware medium access control (BAMAC) protocol for ad hoc wireless networks,"One of the challenging issues in the energy-constrained ad hoc wireless networks is to find ways that increase their lifetime. Squeezing maximum energy from the battery of the nodes of these networks requires, the communication protocols to be designed such that they are aware of the state of the batteries. Traditional ad hoc wireless MAC protocols are designed without considering the battery state. We propose a novel battery aware medium access control (BAMAC) protocol that takes benefit of the chemical properties of the batteries to provide the longest life for the mobile nodes' battery. The proposed protocol extends the battery lifetime consuming 70% and 21% less percentage nominal capacity of the battery per packet transmission compared to the IEEE 802.11 and the DWOP (distributed wireless ordering protocol) respectively. A discrete chain Markov model is used to theoretically analyze the battery behavior. We have provided a detailed simulation study on the performance and the results show that BAMAC outperforms DWOP and IE EE 802.11 MAC protocols, in terms of power consumption, fairness, and lifetime of the nodes.",
Handwriting analysis of pre-hospital care reports,"Emergency health care facilities lack automated medical form recognition systems necessary for efficient epidemiological and health surveillance analysis. The task is to extract handwritten text from the New York State (NYS) Pre-Hospital Care Report (PCR) and determine its ASCII translation. Our approach hybridizes image processing and semantic lexicon pruning to compensate for the otherwise enormous lexicon size. In this paper we expand on our IEEE CBMS 2001 paper, which provided a conceptual overview, by probing into our recognizer design and performance measurements.","Biomedical imaging,
Image segmentation,
Measurement,
Computer displays,
Handwriting recognition,
Venus,
Text analysis,
Computer science,
Biomedical engineering,
Medical services"
Extreme Programming in global software development,"Reliable communication is essential for the success of global collaborative software development efforts. Software development organizations and methodologies must be tailored to avoid communications issues which may result in misunderstood requirements or missed project milestones. Extreme Programming (XP) was adopted for the Lattice/sup /spl reg// Trading System reengineering project - a globally distributed software development effort. Customers in Boston worked with the offshore development in Hangzhou. Business knowledge and requirements were transferred iteratively throughout the project duration. Steps in the development process were executed at different sites at different times, depending on skill set match. An iterative development process reduced communication risk as development for one phase and communication for the next phase could be conducted in parallel. This approach allowed the team to avoid most of the communication delay and improved the quality of communication as well.",
A possibilistic Petri-Nets-based service discovery,"The focus of this paper is to propose a service-oriented multi-agent framework to better integrate Web services and agents. In the making of this framework, there are four main components: (1) a possibilistic Petri-Nets (PPN) based service matchmaking mechanism for service discovery, (2) a service description language based on PPN for describing agent services and Web services, (3) a set of ontologies to facilitate the semantic matchmaking, and (4) an extension of tModels to bridge the Web services registered in UDDI to the PPN matchmaker.","Web and internet services,
Web services,
Ontologies,
Variable speed drives,
Software engineering,
Computer science,
Bridges,
Software agents,
Communication standards,
Standards development"
Zone-based hierarchical routing in two-tier backbone ad hoc networks,"An ad hoc network is usually assumed to be homogeneous, such that each mobile node uses the same radio capability. However, a homogenous ad hoc network suffers from poor scalability. Nodes that use different radio powers at different tiers can be used to solve this problem. This study proposes zone-based hierarchical routing in two-tier ad hoc networks (HRTT). The hierarchical structure of a wireless network is typically organized in clusters, so establishing stable clusters to manage every node is important. HRTT provides a stable clustering scheme according to the rate of degree variations of nodes. Stable nodes are selected as the cluster heads to manage the other nodes. The cluster heads become the backbone nodes (BNs), which use large power to transmit packets and form the backbone network. In the proposed maintenance scheme, nodes exchange their state messages with neighbors and determine whether a new cluster head is required. Routing is classified into two types - low tier routing and high tier routing - to lighten the backbone network. HRTT yields promising results concerning the hierarchical structure, according to simulations.","Routing,
Intelligent networks,
Spine,
Ad hoc networks,
Delay,
Scalability,
Wireless networks,
Power engineering and energy,
Computer science,
Mobile computing"
A research and mentoring program for undergraduate women in computer science,"This paper describes a new program for female undergraduate computer science students. The program uses recognized strategies for engaging women in computer science. It includes multi-faceted mentoring, community building activities, and a research program with significant educational components. The research component gives women an opportunity to work in research teams under the direction of a female faculty member who serves as role model. While there are other programs that allow students to work with female faculty on their research, this research program was designed to develop our female students. The research team presents its research at an appropriate conference each year. The team members also reach out to other students by participating in recruiting activities and presenting their research to high school students at local career days. Our program is currently in its second year. This paper describes the program and changes that we have made based on what we learned during the first year.",
A hybrid method for robust car plate character recognition,"Image based car plate recognition is an indispensable part of intelligent traffic system. The quality of the images taken for car plates, especially for Chinese car plates, however, may sometimes be very poor, due to the operating conditions. Furthermore, there exist some ""similar"" characters, such as ""8"" and ""B"", ""7"" and ""T"" and so on. They are less distinguishable because of noises and/or distortions. To achieve robust and high recognition performance, in this paper, a two-stage hybrid recognition system combining statistical and structural recognition method is proposed. Car plate image are skew corrected and normalized before recognition. In the first stage, four statistical subclassifiers recognize the input character independently, and the recognition results are combined using the Bayes method. If the output of the first stage contains characters that belong to prescribed sets of similarity characters, structure recognition method is used to further classify these character images. Experiments show that our recognition system is very efficient and robust. As part of an intelligent traffic system, the system has been in successful commercial use",
A novel carrier based PWM method in three phase four wire inverters,"This paper presents a new carrier based PWM method to control the three phase four wire inverters. An equivalent circuit model of two-level inverter has been proposed, presenting a redundancy function in the zero sequence voltage. Its application has been used to explain simply the characteristics of the three phase four wire inverter topologies. The advantage from this method is a simple and flexible PWM control, and its possible applications to higher level inverters. Several specific characteristics have been also introduced.","Pulse width modulation inverters,
Wire,
Space vector pulse width modulation,
Zero voltage switching,
Circuit topology,
Leg,
Mathematical model,
Moon,
Computer science,
Equivalent circuits"
Using reflection to introduce self-tuning technology into DBMSs,The increasing complexity of database management systems (DBMSs) and their workloads means that manually managing their performance has become a difficult and time-consuming task. Autonomic computing systems have emerged as a promising approach to dealing with this complexity. Current DBMSs have begun to move in the direction of autonomic computing with the introduction of parameters that can be dynamically adjusted. A logical next step is the introduction of self-tuning technology to diagnose performance problems and to select the dynamic parameters that must be adjusted. We introduce a method for automatically diagnosing performance problems in DBMSs and then describe how this method can be incorporated into current DBMSs using the concept of reflection. We demonstrate the feasibility of our approach with a proof-of-concept implementation for DB2 universal database.,"Reflection,
Quality of service,
Database systems,
Availability,
Knowledge management,
Resource management,
Condition monitoring,
Computer science,
Mission critical systems,
Management information systems"
On the hardware design of an elliptic curve cryptosystem,"We present a hardware architecture for an elliptic curve cryptography system performing the three basic cryptographic schemes: DH key generation, encryption and digital signature. The architecture is described by using hardware description languages, specifically Handel C and VHDL. Because of the sequential nature of the cryptographic algorithms, they are written in Handel C language. The critical part of the cryptosystem is a module performing the scalar multiplication operation. This module has been written in VHDL to let further improvements. The points of the elliptic curve are represented in projective coordinates working over the two-characteristic finite field and using polynomial basis. A prototype of this hardware architecture is implemented on a Xilinx Virtex II FPGA device.","Hardware,
Elliptic curve cryptography,
Public key cryptography,
Security,
Elliptic curves,
Computer science,
Computer architecture,
DH-HEMTs,
Digital signatures,
Galois fields"
Dynamic optimality - almost [competitive online binary search tree],"We present an O(lg lg n)-competitive online binary search tree, improving upon the best previous (trivial) competitive ratio of O(lg n). This is the first major progress on Sleator and Tarjan's dynamic optimality conjecture of 1985 that O(1)-competitive binary search trees exist.","Binary search trees,
Data structures,
Computer science,
Tree data structures,
Artificial intelligence,
Laboratories,
Information science,
Read-write memory"
An efficient random key pre-distribution scheme,"Any key pre-distribution (KPD) scheme is inherently a trade-off between complexity and security. By sacrificing some security (KPD schemes need some assurance of the ability to limit sizes of attacker coalitions), KPD schemes gain many advantages. We argue that random KPD schemes, in general, perform an ""advantageous"" trade-off which renders them more suitable for practical large scale deployments of resource constrained nodes. We introduce a novel random KPD scheme, hashed random preloaded subsets (HARPS), which turns out to be a generalization of two other random KPD schemes, random preloaded subsets (RPS) and a scheme proposed by T. Leighton and S. Micali (LM). All three schemes have probabilistic measures for the ""merit"" of the system. We analyze and compare the performance of the three schemes. We show that HARPS has significant advantages over other KPD schemes, and in particular, over RPS and LM.","Intrusion detection,
Privacy,
Information security,
Performance analysis,
Mobile ad hoc networks,
Authentication,
Computer science,
Information science,
Computer security,
Large-scale systems"
Quantum pseudo-telepathy and the kochen-specker theorem,"There are different approaches to proving the impossibility of classical hidden-variable explanations of quantum-mechanical behavior. Whereas Kochen and Specker proved that a three-or higher-dimensional quantum-mechanical system cannot be ""classically"" prepared for all possible alternative measurements in a consistent way, Bell showed that the behavior of certain two-partite systems is nonlocal, i.e., inexplicable by shared classical information. We show a close connection between deterministic manifestations of such nonlocality-called ""pseudotelepathy"" games-and Kochen and Specker's theorem: Every such game leads to a Kochen-Specker contradiction, and vice versa","Quantum mechanics,
Game theory,
Hilbert space,
Quantum entanglement,
Computer science,
Extraterrestrial measurements,
History,
Mechanical variables measurement,
Paramagnetic resonance,
Control systems"
Exact analysis of a class of GI/G/1-type performability models,"We present an exact decomposition algorithm for the analysis of Markov chains with a GI/G/1-type repetitive structure. Such processes exhibit both M/G/1-type & GI/M/1-type patterns, and cannot be solved using existing techniques. Markov chains with a GI/G/1 pattern result when modeling open systems which accept jobs from multiple exogenous sources, and are subject to failures & repairs; a single failure can empty the system of jobs, while a single batch arrival can add many jobs to the system. Our method provides exact computation of the stationary probabilities, which can then be used to obtain performance measures such as the average queue length or any of its higher moments, as well as the probability of the system being in various failure states, thus performability measures. We formulate the conditions under which our approach is applicable, and illustrate it via the performability analysis of a parallel computer system.","Performance analysis,
Risk analysis,
Length measurement,
Computer science,
Hardware,
Software quality,
Algorithm design and analysis,
Open systems,
High performance computing,
Performance evaluation"
Service differentiation at transport layer via TCP Westwood low-priority (TCPW-LP),"An end-to-end ""foreground/background"" priority scheme is useful for end hosts to utilize the residual capacity left unused by high-priority foreground applications. Several end-to-end prioritization schemes, such as TCP-LP (Low Priority) and TCP-Nice, have been proposed, however, the residual capacity cannot be fully utilized by these schemes. We propose TCP Westwood Low Priority (TCPW-LP), a scheme that maximizes the utilization of residual capacity without intrusion on coexisting foreground flows. TCPW-LP employs an ""Early Window Reduction"" mechanism to reduce its congestion window as a reaction to incipient congestion. To achieve high efficiency, the reaction is based on the estimation whether the congestion is caused by the foreground traffic or not. Simulation results show that TCPW-LP appropriately defers to foreground flows. Further, under a wide range of buffer capacity and link error losses, TCPW-LP better utilizes the residual capacity than other proposed priority schemes or even TCP Reno.","Traffic control,
Delay,
Transport protocols,
Communication system traffic control,
Bandwidth,
Laboratories,
National electric code,
Computer science,
Application software,
Web and internet services"
An efficient deadlock-free tree-based routing algorithm for irregular wormhole-routed networks based on the turn model,"We proposed an efficient deadlock-free tree-based routing algorithm, the DOWN/UP routing, for irregular wormhole-routed networks based on the turn model. In a tree-based routing algorithm, hot spots around the root of a spanning tree and the uneven traffic distribution are the two main facts degrade the performance of the routing algorithm. To solve the hot spot and the uneven traffic distribution problems, in the DOWN/UP routing, it tries to push the traffic downward to the leaves of a spanning tree as much as possible and remove prohibited turn pairs with opposite directions in each node, respectively. To evaluate the performance of DOWN/UP routing, the simulation is conducted. We have implemented the DOWN/UP routing along with the L-turn routing on the IRFlexSim0.5 simulator. Irregular networks that contain 128 switches with 4-port and 8-port configurations are simulated. The simulation results show that the proposed routing algorithm outperforms the L-turn routing for all test samples in terms of the degree of hot spots, the traffic load distribution, and throughput.","System recovery,
Routing,
Tree graphs,
Telecommunication traffic,
Traffic control,
Throughput,
Network topology,
Communication system traffic control,
Sun,
Computer science"
Semantic driven program analysis,"The paper presents an approach to extract and to analyze the semantic content (i.e., problem and solution domain semantics) of existing software systems to support program understanding and software various maintenance tasks, such as: recovery of traceability links between documentation and source code, identification of abstract data types in legacy code, and identification of high-level concept clones in software. The semantic information is derived from the comments, documentation, and identifier names associated with the source code using information retrieval methods. The paper advocates for the use of latent semantic indexing as the underlying support for the semantic driven analysis. The presented results are based on the author's doctoral dissertation (Marcus, 2003).","Documentation,
Software systems,
Software maintenance,
Data mining,
Information analysis,
Cognitive science,
Information retrieval,
Indexing,
Computer science,
Cloning"
The MAGIC-5 Project: medical applications on a GRID infrastructure connection,"The MAGIC-5 Project aims at developing computer aided detection (CAD) software for medical applications on distributed databases by means of a GRID infrastructure connection. The use of automatic systems for analyzing medical images is of paramount importance in the screening programs, due to the huge amount of data to check. Examples are: mammographies for breast cancer detection, computed-tomography (CT) images for lung cancer analysis, and the positron emission tomography (PET) imaging for the early diagnosis of the Alzheimer disease. The need for acquiring and analyzing data stored in different locations requires a GRID approach of distributed computing system and associated data management. The GRID technologies allow remote image analysis and interactive online diagnosis, with a relevant reduction of the delays actually associated to the screening programs. From this point of view, the MAGIC-5 Collaboration can be seen as a group of distributed users sharing their resources for implementing different virtual organizations (VO), each one aiming at developing screening programs, tele-training, tele-diagnosis and epidemiologic studies for a particular pathology.","Medical services,
Biomedical equipment,
Image analysis,
Distributed computing,
Cancer,
Positron emission tomography,
Grid computing,
Distributed databases,
Image databases,
Biomedical imaging"
Adaptive Two-Pass Median Filter Based on Support Vector Machines for Image Restoration,"In this letter, a novel adaptive filter, the adaptive two-pass median (ATM) filter based on support vector machines (SVMs), is proposed to preserve more image details while effectively suppressing impulse noise for image restoration. The proposed filter is composed of a noise decision maker and two-pass median filters. Our new approach basically uses an SVM impulse detector to judge whether the input pixel is noise. If a pixel is detected as a corrupted pixel, the noise-free reduction median filter will be triggered to replace it. Otherwise, it remains unchanged. Then, to improve the quality of the restored image, a decision impulse filter is put to work in the second-pass filtering procedure. As for the noise suppressing both fixed-valued and random-valued impulses without degrading the quality of the fine details, the results of our extensive experiments demonstrate that the proposed filter outperforms earlier median-based filters in the literature. Our new filter also provides excellent robustness at various percentages of impulse noise.",
A hybrid parameterization method for NURBS,"The problem of the parameterization of data points in NURBS curve/surface has been considered by amount of researchers. In this study, a new parameterization method considered as hybrid parameterization method is proposed. Like universal method, this method uses the property of ¿at each span index, there is exactly one maximum value of the rational B-spline basis function¿. By taking the maximum rational B-spline basis functions as initial values, the centripetal method is applied to generate the parameter values of hybrid parameterization. This approach inherits the advantages of universal and centripetal methods such as allowing having multiple knots value. The proposed method gives better object shaped relevant to the other methods.",
"Educational design, evaluation, & development of platforms for learning","Systemic reform in undergraduate engineering education is critical to improving student ability and understanding. Electrical engineering and computer science at Oregon State University has worked in collaboration with university science and math education researchers to implement large-scale curriculum reform based on a platform for learning/spl trade/. To successfully approach such a large systemic problem and introduce major education reform, an approach called design research has been used. Design research involves a team of education designers that manage a series of iterative cycles of design, implementation, and evaluation. Each cycle provides the empirical evidence needed to improve instruction, and refine the education theory related to platforms for learning. The design research process has brought a much richer and expansive understanding of the platforms for learning concept and engineering education in general. In part concepts like cross-cutting competencies (which include enhancing community building, student innovation and design skills, depth, breadth and professionalism), educational hardware design, and horizontal and vertical inter-class connections have been better understood through the research. This paper summarizes the design research process as it is used at OSU to reform engineering education. Findings specific to a platform for learning and generally applicable to engineering education are discussed. Finally, implementation changes that resulted from the design research process are presented.","Engineering education,
Process design,
Computer science education,
Electrical engineering,
Computer science,
Collaborative work,
Large-scale systems,
Buildings,
Technological innovation,
Hardware"
Digital libraries and educational practice: a case for new models,"Educational digital libraries can benefit from theoretical and methodological approaches that enable lessons learned from design and evaluation projects performed in one particular setting to be applied to other settings within the library network. Three promising advances in design theory are reviewed - reference tasks, design experiments, and design genres. Each approach advocates the creation of 'intermediate' constructs as vehicles for knowledge building and knowledge sharing across design and research projects. One purpose of an intermediate construct is to formulate finer-grained models that describe and explain the relationship between key design features and the cognitive and social dimensions of the context of use. Three models are proposed and used as thought experiments to analyze the utility of these approaches to educational digital library design and evaluation: digital libraries as cognitive tools, component repositories, and knowledge networks.","Software libraries,
Computer aided software engineering,
Educational technology,
Atmospheric modeling,
Buildings,
Computer science,
Performance evaluation,
Vehicles,
Context modeling,
Human factors"
Constrained optimization problem solving using estimation of distribution algorithms,Two variants of estimation of distribution algorithm (EDA) are tested solving several continuous optimization problems with constraints. Numerical experiments are conducted and comparison is made between constraint handling using several types of penalty and repair operators in case of both elitist and nonelitist implementation of the EDA's. Graphical display and animations of representative runs of the best and worst performers proved useful in enhancing the understanding of how such algorithms work.,"Constraint optimization,
Problem-solving,
Electronic design automation and methodology,
Testing,
Mechanical engineering,
Evolutionary computation,
Probability distribution,
Computer science,
Displays,
Animation"
An induction learning approach for building intrusion detection models using genetic algorithms,"Building and updating an effective intrusion detection system is complex engineering knowledge. A method of learning the intrusion detection rules based on Genetic Algorithms is presented in order to realize the automation of the detection models. The same attributes of an intrusion can be found through the heuristic search in the network data space. In our experiments the characters of an attack, such as smurf, are summarized inductively through the datasets of the 1998 DARPA Intrusion Detection Evaluation Program. The effectiveness and robustness of the approach are proved.",
State-space reduction techniques in agent verification,,"Multiagent systems,
State-space methods,
Logic programming,
Mars,
NASA,
Computer languages,
Java,
Contracts,
Computer science,
Explosions"
A statistical prediction-based scheme for energy-aware multimedia data streaming,"The proliferation of multimedia-capable mobile devices and ubiquitous high-speed network technologies to deliver multimedia objects has fueled the demand of mobile streaming multimedia. A necessary criterion for the mass acceptance of mobile devices is acceptable battery life of these devices. This paper explores linear prediction-based client-side strategies to reduce the wireless network interface card (WNIC) energy consumption by transitioning the WNIC to a lower power consuming sleep state. The basic idea of this strategy is to selectively choose proper periods of time to suspend communication by switching the WNIC to sleep state. A linear prediction-based time series forecasting technique is used to predict future no-data intervals. Simulation results show that linear prediction-based strategy gives better results than those based on simple averaging [Surendar Chandra et al., (2002)].","Streaming media,
Energy consumption,
Batteries,
Wireless networks,
Mobile computing,
Communication switching,
Network interfaces,
Computer science,
Sleep,
Multimedia computing"
An architectural approach to mobility - the handover case study,"Community is a formal approach to software architecture. Its main characteristics are: a precise, yet intuitive mathematical semantics based on categorical diagrams; a clear separation between computation, coordination, and distribution (including mobility); and a simple state-based language, inspired by Unity, to describe behaviour. This paper discusses the applicability of this approach to location-aware systems through the modelling of the GSM handover protocol, namely the way communication with a moving cellular phone passes from one station to another. The case study was developed with the Community Workbench, a tool that animates distributed and mobile architectural models.",
Exploring agent-supported simulation brokering on the semantic Web: foundations for a dynamic composability approach,"Federated simulations address the need for interoperability, as well as the improvement of reuse and composability. The focal goal in a federated simulation is to facilitate composable simulations by standardizing interfaces to assure technical interoperability among disparate simulations. Yet, existing federated simulation infrastructures neither facilitate substantive interoperability nor are dynamically extensible. Emergent Web services technologies hold out the potential to significantly improve the development of interoperable, extensible, and dynamically composable federations. As such, recent initiatives (i.e., XMSF) are urging the use of open standards that can be applied within an extensible framework for next generation modeling and simulation applications. We discuss how the realization of multimodel and multisimulation formalisms in terms of semantic Web and agent technologies may bring new vistas to demonstrate runtime model discovery, instantiation, composition, and interoperation.","Semantic Web,
Runtime,
Ontologies,
Computational modeling,
Computer simulation,
Microstrip,
Service oriented architecture,
Laboratories,
Computer science,
Information technology"
Multi-class active learning for video semantic feature extraction,"Active learning has been demonstrated to be a useful tool to reduce human labeling effort for many multimedia applications, especially for those handling large video collections. However, most of the previous work on active learning has focused on only binary classification, which greatly limits the applicability of active learning. We present a multi-class active learning approach which extends active learning from binary classification to multi-class classification using a unified representation with margin-based loss functions. The experimental results on the TREC03 semantic feature extraction task shows that the proposed active learning approach works effectively even with a significantly reduced amount of labeled data.","Feature extraction,
Humans,
Labeling,
Training data,
Computer science,
Machine learning,
Information retrieval,
Supervised learning,
Sampling methods,
Costs"
A distributed hierarchical routing protocol for non-GEO satellite networks,"Satellite will continue play a major role in the implementation of the Next Generation Internet (NGI), with characters of large geographical coverage, unique broadcasting capabilities, and high-capacity channel. However, the dynamic network topology and limited onboard resources of satellite networks that involve nongeostationary satellites turn routing into a very challenging problem. In this paper, a distributed hierarchical routing protocol (DHRP) for nongeostationary satellite networks is developed which calculates routing tables efficiently using the collected link state advertisement. The protocol generates optimal paths with low implementation complexity and communicational overhead. The performance of DHRP is evaluated through analysis and simulations.","Routing protocols,
Satellite broadcasting,
Network topology,
Internet,
Low earth orbit satellites,
IP networks,
Artificial satellites,
Satellite constellations,
Computer science,
Broadcast technology"
Dynamic Shannon coding,"This paper presents a new algorithm, called dynamic Shannon coding, that uses at most (H + 1)m + O(nlogm) bits to encode string S. The key idea is to smooth the relative frequencies of characters when computing their weights. It also shows that dynamic Shannon coding can be easily modified to restrict the maximum length of any codeword in the encoding produced. The analysis of dynamic Shannon coding is much simpler than the analysis of dynamic Huffman coding.",
"A comparison among four SVM classification methods: LSVM, NLSVM, SSVM and NSVM","Support vector machines (SVMs) are powerful tools for providing solutions to classification and function approximation problems. The comparison among the four classification methods is conducted. The four methods are Lagrangian support vector machine (LSVM), finite Newton Lagrangian support vector machine (NLSVM), smooth support vector machine (SSVM) and finite Newton support vector machine (NSVM). The comparison of their algorithm in generating a linear or nonlinear kernel classifier, accuracy and computational complexity is also given. The study provides some guidelines for choosing an appropriate one from four SVM classification methods in a classification problem.","Support vector machines,
Support vector machine classification,
Lagrangian functions,
Kernel,
Iterative algorithms,
Newton method,
Mathematics,
Computer science,
Computational complexity,
Guidelines"
An enhanced buffer management scheme for fast handover protocol,"The integration of IEEE 802.11 wireless LAN and mobile IP technologies offers an affordable and high bandwidth solution for host mobility. When applying the hierarchical mobile IP for fast handover, the performance bottleneck on mobile IP and potential disconnection during handoff period can be greatly improved. However, the fast handover protocol suffers several problems such as scalability and QoS support for lacking a buffer management mechanism. We propose an enhanced buffer management scheme for fast handover. By means of the proposed scheme, we are able to improve the buffer utilization on routers as well as to support QoS services during a handoff process. Using the ns-2 simulator, we have demonstrated the significance and effectiveness of our proposed scheme.","Delay,
Wireless LAN,
Quality of service,
Internet,
Access protocols,
Computer science,
Computer science education,
Educational programs,
Mobile computing,
Educational technology"
A decision theoretic approach for task coordination in social robots,"We present a new method for designing and implementing socially interactive mobile robots built around a 3-layer hybrid control architecture. Our main contribution is at the deliberative level, where we introduce multiply sectioned Markov decision processes (MS-MDPs) as a mechanism for efficient task specification, policy generation and execution. Using MS-MDPs, we partition the task into a number of subtasks, each assigned to an MDP, such that each one can be specified and solved independently. Each MDP controls one aspect of the global task, and they all are executed concurrently, coordinated implicitly by common state variables. We validate our approach by presenting experiments performed using our robot HOMER, the human oriented messenger robot. HOMER is a stereo-vision guided mobile robot designed for performing a message delivery task, which allows for rich and complex robot-human interactions using a multi-modal interface. HOMER's deliberative layer includes 3 MDPs: the navigator, the dialogue manager and the gesture generator. Together they coordinate 10 behaviors for accomplishing the message delivery task.","Robot kinematics,
Mobile robots,
Navigation,
Human robot interaction,
Orbital robotics,
Computer science,
Educational institutions,
Design methodology,
Computer architecture,
Model driven engineering"
Fiber orientation mapping using generalized diffusion tensor imaging,"Generalized diffusion tensor imaging uses tensors of arbitrary ranks to model the angular variations in the diffusivities measured by magnetic resonance imaging (MRI) methods. However, a diffusivity profile alone is not readily capable of producing distinct fiber orientations. In this work, we show how it is possible to get the displacement probability profile for water molecules from the higher rank diffusion tensors and validate the technique via simulations of one, two and three fiber systems. Finally, we present fiber orientation results for an image from an excised rat brain.","Diffusion tensor imaging,
Tensile stress,
Equations,
Magnetic resonance imaging,
Attenuation,
Physics,
Computer science,
Biochemistry,
Gain measurement,
Brain modeling"
Software engineering education needs adequate modeling tools,"Teaching graphical modeling languages with industrial tools is not always satisfying, since the focus of these tools lies on professional development rather than education. We present a family of modeling tools devoted explicitly to teaching and built upon a common framework. We also report on the evaluation of first teaching experiences with these tools.","Software engineering,
Education,
Software systems,
Unified modeling language,
Software tools,
Educational institutions,
Computer industry,
Communication system security,
Maintenance,
Computer architecture"
Integration of simulation and geographic information systems: modeling traffic flow in inland waterways,"This paper describes the integration of geographic information systems (GIS) with simulation modeling of traffic flow on inland waterways. Two separate modeling efforts are described: (a) GIS/AutoMod modeling of barge traffic on the Ohio River, and (b) GIS/Arena modeling of the transit of ocean-going vessels through the Panama Canal. These modeling efforts demonstrate the benefits that accrue both to modeling realism and to the initialization process with discrete-event models of traffic flow on these waterways.","Geographic Information Systems,
Traffic control,
Rivers,
Boats,
Delay,
Computational modeling,
Industrial engineering,
Computer science,
Moon,
Water resources"
A new approach to clustering biological data using message passing,"Clustering algorithms are widely used in bioinformatics to classify data, as in the analysis of gene expression and in the building of phylogenetic trees. Biological data often describe parallel and spontaneous processes. To capture these features, we propose a new clustering algorithm that employs the concept of message passing. Message passing clustering (MPC) allows data objects to communicate with each other and produces clusters in parallel, thereby making the clustering process intrinsic. We have proved that MPC shares similarity with hierarchical clustering (HC) but offers significantly improved performance because it takes into account both local and global structure. We analyzed 35 sets of simulated dynamic gene expression data, achieving a 95% hit rate in which 639 genes out of total 674 genes were correctly clustered. We have also applied MPC to a real data set to build a phylogenetic tree from aligned mycobacterium sequences. The results show higher classification accuracies as compared to traditional clustering methods such as HC.","Message passing,
Clustering algorithms,
Phylogeny,
Gene expression,
Bioinformatics,
Algorithm design and analysis,
Clustering methods,
Couplings,
Computer science,
Pathology"
Generation of attribute value taxonomies from data for data-driven construction of accurate and compact classifiers,"Attribute value taxonomies (AVT) have been shown to be useful in constructing compact, robust, and comprehensible classifiers. However, in many application domains, human-designed AVTs are unavailable. We introduce AVT-learner, an algorithm for automated construction of attribute value taxonomies from data. AVT-learner uses hierarchical agglomerative clustering (HAC) to cluster attribute values based on the distribution of classes that co-occur with the values. We describe experiments on UCI data sets that compare the performance of AVT-NBL (an AVT-guided naive Bayes learner) with that of the standard naive Bayes learner (NBL) applied to the original data set. Our results show that the AVTs generated by AVT-learner are competitive with human-gene rated AVTs (in cases where such AVTs are available). AVT-NBL using AVTs generated by AVT-learner achieves classification accuracies that are comparable to or higher than those obtained by NBL; and the resulting classifiers are significantly more compact than those generated by NBL.","Taxonomy,
Instruction sets,
Robustness,
Data mining,
Ontologies,
Artificial intelligence,
Laboratories,
Computer science,
Application software,
Clustering algorithms"
Requirement analysis and implementation of palm-based multimedia museum guide systems,"A multimedia museum guide system is developed and presented in the paper. In contrast with conventional cassette audio tape exhibition tours, the presented museum guide system is implemented on palm-based personal digital assistants (PDAs), so as to provide a multimedia touring experience for visitors. Moreover, the museum guide system is capable of being aware of the current position of the visitor, and then automatically retrieves the related multimedia information of the exhibit that the visitor is gazing at. As the visitor walks up to another exhibit, without any clicks or operations, the screen on the PDA will be changed to the related multimedia information of that exhibit automatically. Basically, the developed museum guide system plays the role of a personal guide assistant, so as to construct the mobile learning environment in museums.",
Ranking function optimization for effective Web search by genetic programming: an empirical study,"Web search engines have become indispensable in our daily life to help us find the information we need. Although search engines are very fast in search response time, their effectiveness in finding useful and relevant documents at the top of the search hit list needs to be improved. In this paper, we report our experience applying genetic programming (GP) to the ranking function discovery problem leveraging the structural information of HTML documents. Our empirical experiments using the Web track data from recent TREC conferences show that we can discover better ranking functions than existing well-known ranking strategies from IR, such as Okapi, Ptfidf. The performance is even comparable to those obtained by support vector machine.","Web search,
Genetic programming,
Search engines,
Information systems,
Computer science,
Internet,
Electronic mail,
Delay,
HTML,
Support vector machines"
An IP traceback mechanism for reflective DoS attacks,"We present a new ICMP message and an automatic process capable of tracing reflective DoS attacks back to attack agents. The newly designed ICMP message carries the packet routing history and is signed by each forwarding router. After receiving the loaded ICMP messages, attack targets can identify the border routers of reflectors in the first flooding path and then use an ICMP message to inform accountable border routers to continue the traceback process to find the attack agents. In this paper, we propose an automatic, efficient, and secure traceback process across domains and discuss some limitations of the protocol.","Computer crime,
Routing,
Floods,
History,
Filtering,
Computer science,
Protocols,
Distributed computing,
Computerized monitoring,
Condition monitoring"
Solving the FMS scheduling problem by critical ratio-based heuristics and the genetic algorithm,"This paper addresses the FMS scheduling problem. The objective concerned here is maximizing the meet-due-date rate. The authors propose two rules for job sequencing and job dispatching, two common subtasks in solving this problem. These two rules are designed based on the critical ratio values of jobs. We also propose a mechanism to obtain better performance than the stand-alone scheduling process via genetic algorithms. With the nature of design of the proposed job sequencing rule, the genetic algorithm is designed not only to improve the schedule quality but also to save computation time. All the proposed rules and idea are carefully examined through several different scenarios.","Flexible manufacturing systems,
Genetic algorithms,
Job shop scheduling,
Processor scheduling,
Job design,
Algorithm design and analysis,
Dispatching,
Manufacturing industries,
Guidelines,
Computer science"
"Efficient analysis of electromagnetic scattering and radiation from patches on finite, arbitrarily curved, grounded substrates","A precorrected-fast Fourier transform (FFT) accelerated surface integral equation approach formulated using the homogeneous medium Green's function is presented for the analysis of patch arrays on finite, arbitrarily shaped, grounded substrate. The integral equation is solved by the method of moments, and the precorrected-FFT method is applied to reduce the memory requirement and computational complexity of the solution procedure. The memory required for this algorithm is O(N1.5), and the computational complexity is NiterN1.5 log N, where N is the number of unknowns and Niter is the iteration number. Numerical results are presented to demonstrate the accuracy and capability of the method.","Integral equations,
Mathematical model,
Substrates,
Dielectrics,
Manganese,
Transmission line matrix methods,
Electromagnetic scattering"
Supporting dynamic QoS in Linux,"This work is an application of the variable-rate execution (VRE) model in Linux to support dynamic quality of service (QoS). Based on conventional time-sharing scheduling algorithms, Linux does not adequately support QoS requirements. The VRE scheduler can assign a specified execution rate to any application, and dynamically adjust the execution rate during runtime. Rate controller components are introduced to adjust a task's execution rate based on predefined rules and runtime feedbacks, such as the suspension time, the queue length, and so on. A significant feature of this work is its ability to support legacy applications at the binary level. On conventional operating systems, millions of applications have been built under time-sharing schedulers, which we call legacy applications. Under the VRE model, a legacy application can obtain a guaranteed variable execution rate. We also designed a simple default rate controller for legacy multimedia applications. The Linux kernel was slightly modified in our implementation to achieve reconfigurability. Both the VRE scheduler and the default rate controller are implemented as Linux loadable modules, which can be dynamically loaded into the kernel to replace the Linux scheduler or change the behavior of the scheduler. We provide a set of interfaces for users to design and use their own schedulers and rate controllers.","Linux,
Quality of service,
Time sharing computer systems,
Dynamic scheduling,
Runtime,
Operating systems,
Kernel,
Computer science,
Application software,
Scheduling algorithm"
Application-tailored database systems: a case of aspects in an embedded database,"Current techniques used to design and implement database systems do not provide support for efficient implementation of crosscutting concerns in the database software, e.g., failure detection, database policies such as concurrency control and scheduling, and synchronization. Aspect-oriented software development (AOSD) is a new technique that provides an efficient way of modularizing crosscutting concerns in software systems. In This work we evaluate the effectiveness of applying AOSD to database systems thereby paving way for successful application of aspect languages to the database domain. Our focus is on embedded database systems, as a representative for a class of database systems. We show, by analyzing and re-engineering one commercial well-known embedded database system (Berkeley database), that aspect-orientation has promise, especially in enabling development of tailorable, maintainable, and evolvable database systems.","Database systems,
Computer aided software engineering,
Programming,
Application software,
Concurrency control,
Software systems,
Spatial databases,
Software maintenance,
Computer science,
Job shop scheduling"
Automated data mapping specification via schema heuristics and user interaction,"Data transformation problems are very common and are challenging to implement for large and complex datasets. We describe a new approach for specifying data mapping transformations between XML schemas using a combination of automated schema analysis agents and selective user interaction. A graphical tool visualizes parts of the two schemas to be mapped and a variety of agents analyze all or parts of the schema, voting on the likelihood of matching subsets. The user can confirm or reject suggestions, or even allow schema matches to be automatically determined, incrementally building up to a fully-mapped schema. An implementation of the mapping specification can then be generated.",
Feature selection for visual gesture recognition using hidden Markov models,"Hidden Markov models have become the preferred technique for visual recognition of human gestures. However, the recognition rate depends on the set of visual features used, and also on the number of states of the hidden variable. It is difficult to determine a priori the optimal set of features and number of states. We analyse experimentally the use of different features for gesture recognition in an office environment. We considered a set of seven gestures that include interaction with other objects, such as writing, using the mouse, opening a drawer, etc. We use a single camera to detect and track the hand of the user based on adaptive colour histograms. From tracking the hand in a video sequence we obtain several features. The features considered include position and velocity in polar and Cartesian coordinates, and the trajectory represented as a chain code. Given that these features are continuous, we discretized them into a set of symbols using vector quantization. We then tested the recognition rate using HMMs with different: (i) number of discrete symbols; (ii) number of hidden states, (Hi) combination of features. The results show a high variation on the recognition rate depending on these parameters, from below 50% to more than 95%. The best performance (97%) was obtained by using the magnitude and orientation in polar coordinates, 64 discrete symbols and 10 states.","Hidden Markov models,
Humans,
Feature extraction,
Cameras,
Speech recognition,
Data mining,
Image recognition,
Writing,
Mice,
Histograms"
Network robotics: a framework for dynamic distributed architectures,"In this paper we present the networked robotics approach to dynamic architecture creation. Building on our prior work we highlight the ease at which system and architecture creation can be moved from the single robot domain to the cooperative/multiple robotic domain; indeed under the networked robotic framework there are no differences between the two, as a multiple, cooperative, robotic architecture simply emerges from a richer network environment (the module pool). Essentially task-driven architectures are instantiated on an as needed basis, allowing conceptualised designs to be run wherever a suitable framework (i.e. a module pool) exists. In this paper we demonstrate this framework applied in the creation of a multi robot system, highlighting the flexibility and robustness of the approach, and the experimental freedom inherent within it. The approach has been tested experimentally.","Computer architecture,
Intelligent robots,
Sensor systems,
Mobile robots,
Robot sensing systems,
Control systems,
Computer science,
Systems engineering and theory,
Buildings,
Robustness"
Flexible power scheduling for sensor networks,"We propose a distributed on-demand power-management protocol for collecting data in sensor networks. The protocol aims to reduce power consumption while supporting fluctuating demand in the network and provide local routing information and synchronicity without global control. Energy savings are achieved by powering down nodes during idle times identified through dynamic scheduling. We present a real implementation on wireless sensor nodes based on a novel, two-level architecture. We evaluate our approach through measurements and simulation, and show how the protocol allows adaptive scheduling and enables a smooth trade-off between energy savings and latency. An example current measurement shows an energy savings of 83% on an intermediate node.","Protocols,
Energy consumption,
Energy management,
Costs,
Processor scheduling,
Communication system operations and management,
Computer network management,
Power system management,
Computer science,
Wireless sensor networks"
Vision-based camera motion recovery for augmented reality,"We address the problem of tracking the 3D position and orientation of a camera, using the images it acquires while moving freely in unmodeled, arbitrary environments. This task has a broad spectrum of useful applications in domains such as augmented reality and video post production. Most of the existing methods for vision-based camera tracking are designed to operate in a batch, off-line mode, assuming that the whole video sequence to be tracked is available before tracking commences. Typically, such methods operate noncausally, processing video frames backwards and forwards in time as they see fit. Furthermore, they resort to optimization in very high dimensional spaces, a process that is computationally intensive. For these reasons, batch methods are inapplicable to tracking in online, time-critical applications such as video see-through augmented reality. This paper puts forward a novel feature-based approach for camera tracking. The proposed approach operates on images continuously as they are acquired, has realistic computational requirements and does not require modifications of the environment. Sample experimental results demonstrating the feasibility of the approach on video images are also provided","Cameras,
Augmented reality,
Layout,
Tracking,
Computer science,
Production,
Video sequences,
Application software,
Motion estimation,
Jitter"
Debugging support for Charm++,"Summary form only given. This paper describes a parallel debugger and the related debugging support implemented for CHARM++, a data-driven parallel programming language. Because we build extensive debugging support into the parallel runtime system, applications can be debugged at a very high level.","Debugging,
Parallel programming,
Programming profession,
Application software,
Concurrent computing,
Communication system control,
Control systems,
Parallel machines,
Computer science,
Data mining"
Reflection processes in the teaching and learning of human aspects of software engineering,We illustrate how reflection is introduced into the teaching and learning of the human aspects of software engineering. We start with explaining the rationale for a reflective mode of thinking and its fitness to the field of software engineering. Then we outline in detail the agenda of a course that deals with human aspects of software engineering. It is suggested that the intertwining of a reflective mode of thinking into the education of software engineers in general and especially into a course that focuses on human aspects of software engineering enhance students' understanding of the essence of the discipline as well as their professional performance in the field.,"Humans,
Software engineering,
Programming,
Software systems,
Educational technology,
Computer science,
Reflection,
Application software,
Computer science education,
Cities and towns"
Aesthetics and inspiration for visualization design: bridging the gap between art and science,"Most information visualization research and design is being performed in computer science and engineering labs by technically trained individuals. Within the digital arts and design communities, on the other hand, there is a parallel discourse regarding artistic visualization aesthetics and interaction design experimentation. Unfortunately, there has been limited exchange and collaboration between infoVis researchers and artists. This paper provides pointers to resources which can provide aesthetic and conceptual inspiration for visualization design in order to bridge the gap in visualization discourse between science, art, technology, and design. The examples given, intended to generate ideas for visualization metaphors and interaction models, are drawn from experiments in algorithmic art, architecture, artificial life, and the natural environment.","Visualization,
Subspace constraints"
"Cyber security exercises: testing an organization's ability to prevent, detect, and respond to cyber security events","The digital age has transformed how today's organizations operate. The production and delivery of essential goods and services takes place through complex and interconnected business processes that in turn rely on a set of interdependent infrastructures. These infrastructures and their supporting information systems transcend individual organizations. However, information systems security research is largely under the purview of computer science and engineering departments, and consequently often focuses on technological issues while overlooking the pervasive nature of information systems in today's society. This has generated calls for a new approach to information systems security; one that employs a socio-organizational perspective that includes not only individual organizations but entire industry sectors and government agencies as well. This paper presents one such approach, the use of scenario-based exercises in addressing security issues common to large organizations, industry sectors, and various levels of government. Lessons learned from illustrative examples of such exercises, as well as suggestions to help organizations conduct their own exercise, are discussed.","Computer security,
Event detection,
Information systems,
Information security,
System testing,
Computer science,
Government,
Production,
Business,
Management information systems"
Multiwavelets in watermarking spectral images,"A watermarking method for spectral images is proposed in this study. Multiwavelets are used in computing the transform domain of a spectral image. The visual, gray-scale watermark is transformed with the scalar wavelet transform and the transformed watermark is embedded in the three-dimensional transform domain of the spectral image. The strength of the watermark in embedding is controlled by perceptual constraints and the reconstruction quality. The experiments indicate, that the robustness of the proposed method against attacks like PCA/wavelet compression is better than in embedding in the transform domain obtained with the scalar wavelets","Watermarking,
Wavelet transforms,
Wavelet domain,
Multiresolution analysis,
Wavelet analysis,
Low pass filters,
Filtering,
Information technology,
Computer science,
Gray-scale"
Reducing cognitive load,"This paper explores issues related to cognitive load in the contexts of learning, information filtering, user modeling, categorization and personal information organizing behavior. We incorporate expertise from the fields of information science, educational psychology and computer science to report research that can ultimately influence the design of personalized adaptive systems.","Information filtering,
Psychology,
Organizing,
Internet,
Context modeling,
Information science,
Computer science,
Adaptive systems,
Computer applications,
Application software"
Genetic design of fuzzy knowledge bases - a study of different approaches,"The objective of this work is to design, implement and test two different genetic fuzzy systems approaches with the purpose of analyzing the performance of both when applied to classification problems. In the first approach the fuzzy sets are defined previously by fuzzy clustering and the rule base is automatically generated and optimized using genetic algorithms. In the second approach the data base is the object of genetic algorithm learning, instead of the rule base. In this case, the rule base is generated by means of an auxiliary method (Wang & Mendell). Investigations of both methods developed earlier by the authors are described and then, the results of the comparison experiments performed in the present work are presented. The methods have been selected for investigation with the objective of analyzing the performance and the size of the resulting knowledge bases generated through genetic algorithms applied to different KB components.","Fuzzy systems,
Genetic algorithms,
Fuzzy sets,
Performance analysis,
Algorithm design and analysis,
Computer science,
System testing,
Data mining,
Power generation,
Uncertainty"
Data Fusion with Different Accuracy,"This paper presents criteria to evaluate different data fusion approaches. A new fusion method for two data with different accuracy is also presented. This approach is an extension of weighted average, which can solve some problem that cannot be handled by maximum likelihood approach. Simulation result is compared with other three fusion algorithms. Comparison shows that it is better than all weighted average approaches and it is the best of these four approaches","Fuses,
Random variables,
Data engineering,
Information science,
Remote sensing,
Gaussian distribution,
Sensor systems"
SCALLOP: a scalable and load-balanced peer-to-peer lookup protocol for high-performance distributed systems,"Many large-scale servers are implemented in a peer-to-peer distributed system. Achieving rapid response time in such a system relies on a scalable lookup protocol to efficiently locate the requested items and a load-balancing mechanism to avoid the hot spot problem by evenly distributing lookup requests. We present a peer-to-peer lookup protocol that addresses both issues. Our lookup protocol uses the technique of distributed hash table (DHT) to store in each node O(logN) routing information in an N-node distributed system. Unlike other DHT-based peer-to-peer protocols, our protocol constructs a balanced lookup tree to avoid hot spots. We compare our protocol with a commonly used peer-to-peer lookup protocol. The experimental results show that our protocol reduces up to 51% of the lookup requests on heavily loaded nodes. Furthermore, our protocol reduces the total lookup requests and thus delivers better performance.","Peer to peer computing,
Table lookup,
Computer science,
Routing protocols,
Costs,
Workstations,
High-speed networks,
Web server,
Network servers,
Centralized control"
Fast model-based penetration testing,"Traditional approaches to security evaluation have been based on penetration testing of real systems, or analysis of formal models of such systems. The former suffer from the problem that the security metrics are based on only a few of the possible paths through the system. The latter suffer from the inability to analyze detailed system descriptions due to the rapid explosion of state space sizes, which render the models intractable for tools such as model checkers. We propose an approach to obtain statistically valid estimates of security metrics by performing repeated penetration testing of detailed system models. We make use of importance sampling techniques to help reduce the variance of our estimates, and achieve relative error bounds quickly. We validate our approach by estimating security metrics of a large model with more than 2/sup 1700/ possible states.","State-space methods,
State estimation,
System testing,
Information security,
Explosions,
Performance evaluation,
Monte Carlo methods,
Computer science,
Automatic testing,
Analytical models"
Turning Segways into soccer robots,"The Segway human transport (HT) is a one person dynamically self-balancing transportation vehicle. The Segway robot mobility platform (RMP) is a modification of the HT capable of being commanded by a computer for autonomous operation. With these platforms, we are investigating human/robot coordination in adversarial environments through the game, Segway soccer. The players include robots (RMPs) and humans (riding HTs). The rules of the game are a combination of soccer and ultimate Frisbee rules. In this paper, we provide two contributions. First, we examine the capabilities and limitations of the Segway and describe the mechanical systems necessary to create a robot Segway soccer player. Second, we provide a detailed analysis of several ball manipulation/kicking systems and the implementation results of the CM-RMP pneumatic ball manipulation system.","Turning,
Robot kinematics,
Mobile robots,
Human robot interaction,
Intelligent robots,
Cognitive robotics,
Mechanical systems,
Computer science,
Vehicles,
Mathematical model"
A change impact dependency measure for predicting the maintainability of source code,"We first articulate the theoretic difficulties with the existing metrics designed for predicting software maintainability. To overcome the difficulties, we propose to measure a purely internal and objective attribute of code, namely change impact dependency, and show how it can be modeled to predict real change impact. The proposed base measure can be further elaborated for evaluating software maintainability.",
Multiple constant multiplication by time-multiplexed mapping of addition chains,,"Circuit synthesis,
Hardware,
Adders,
Digital signal processing,
Libraries,
Delay,
Costs,
Discrete Fourier transforms,
Permission,
Computer science"
On the impact of noise sensitivity on performance in 802.11 based ad hoc networks,The IEEE 802.11 Medium Access Control (MAC) layer plays a crucial role on the overall throughput obtained in a mobile ad hoc network. We show that the virtual carrier sense mechanism as designed and used in 802.11 could have crippling effect on distant but competing transmissions. We propose a modification to mitigate this situation and show using simulations that the proposed modification provides as much as 50% higher User Datagram Protocol (UDP) throughput for static wireless networks and 10-25% higher throughput for mobile ad hoc networks.,"Intelligent networks,
Ad hoc networks,
Peer to peer computing,
Routing protocols,
Mobile ad hoc networks,
Media Access Protocol,
Throughput,
Computer science,
Wireless application protocol,
Wireless sensor networks"
Incorporating goal recognition into human-machine collaboration,"Goal recognition techniques provide a novel way to make human-machine collaboration applicable. After describing a goal recognition algorithm, this paper focuses on how to incorporate goal recognition into practical systems. Specifically, several important properties, namely, partially observing, explanation and the ability to handle ambiguity have been exploited. This method does not use a plan library as traditional plan recognition methods do. So it also does not suffer the problems such as hand coding of a plan library and searching the plan library space of exponential size. Finally, theoretical and empirical results of the method have been achieved, which show the accuracy, efficiency and scalability of the method.","Man machine systems,
Collaboration,
Libraries,
Software agents,
Computer science,
Collaborative software,
Software systems,
Intelligent agent,
Humans,
Polynomials"
Animator: a tool for the animation of parallel coordinates,"In this paper, an uncommon use of parallel coordinates is illustrated using the Animator software. Animator is used to plot the parallel coordinates of objects in multi-dimensional space. Subsequently, the Animator software is used to animate the movement of individual objects, in this multi-dimensional space over time. Initial empirical studies of this technique for the visualization of data from Neurophysiological research, multi-dimensional spike train datasets, have shown that the technique is useful. Thus, Animator was developed for public access and is now freely available (including source code) from the Visualization Lab at the University of Plymouth, www.plymouth.ac.uk/infovis.","Animation,
Data visualization,
Computer science,
Intelligent systems,
Hoses,
Multidimensional systems"
On designing CO$T: a new approach and programming environment for distributed problem solving based on evolutionary computation and anytime algorithms,"In This work we present a unified view of AI inspired by ideas from evolutionary computation as design of bounded rational agents. The approach specifies optimal programs rather than optimal actions, and is based on process algebras and anytime algorithms. The search method described in This work is so general than many other search algorithms, including evolutionary search methods, become its special case. We present a practical design of the programming language and environment targeting real-time complex domains. As AI systems move into more complex domains, all problems become real-time, because the agent never have long enough time to solve the decision problem exactly.","Algorithm design and analysis,
Programming environments,
Problem-solving,
Evolutionary computation,
Genetic programming,
Turing machines,
Computer science,
Artificial intelligence,
Search methods,
Genetic algorithms"
Multi-class SVM with negative data selection for Web page classification,"Support vector machine (SVM) has been demonstrated its excellent performance in terms of solving document classification problem. In this paper, SVM with one-against-all structure is applied to solve Web page classification problems with multi-class. However, the main problem of SVM with one-against-all structure is that the negative data might be too huge so that the training time obviously increase. To solve this problem, a negative data selection method is presented to reduce a large amount of negative data for SVM. Experimental results show that the training time is obviously reduced. Moreover, the proposed method also keeps a high accuracy rate for Web page classification.","Support vector machines,
Support vector machine classification,
Web pages,
Machine learning,
Educational institutions,
Computer science,
Data engineering,
Electronic mail,
Internet,
Search engines"
Detection of the number of signals using a multiple hypothesis test,"This work presents a novel approach to detect multiple signals embedded in noisy observations of a sensor array. We formulate the detection problem as a multiple hypothesis test. To control the global level of the multiple test, we apply the false discovery rate (FDR) criterion recently suggested by Benjamini and Hochberg instead of the classical familywise error rate (FWE) criterion. The proposed method is tested by the simulated data. Results show that the FDR-controlling procedure is more powerful than the FWE- controlling procedure. The performance improvement is most significant for a large number of signals and low SNRs.","Testing,
Sensor arrays,
Signal detection,
Error analysis,
Signal to noise ratio,
Covariance matrix,
Computer science,
Information science,
Telecommunication control,
Signal processing"
Developing training programs for systems architecting,"Six years ago the Aerospace Corporation undertook the development of the aerospace systems architecting program. With the passage of several offerings we have learned a number of lessons that apply to the development of programs in systems architecting and to its practice. The course effectively uses an intellectual core of material in ill-structured problem solving and conceptual design to address areas usually regarded as soft or more art than science. While this material is non-traditional for engineering educational programs, often being seen as too soft or non-analytical, it has nevertheless proven possible to teach effectively. The approach to teaching this material has required adaptation over course cycles, but the nature of the adaptations often applies to real business projects as well as education.",
Supporting efficient keyword-based file search in peer-to-peer file sharing systems,"Peer-to-peer (P2P) computing has become a popular distributed computing paradigm thanks to the abundant computing power of modern desktop workstations and widely available network connectivity. Although P2P file sharing provides a scalable alternative to conventional server-based approaches, providing efficient file search in a large-scale dynamic P2P system remains a challenging problem. We propose a set of mechanisms to provide a scalable keyword-based file search in distributed hash table (DHT) based P2P systems. Our proposed architecture, called keyword fusion, balances unfair storage consumptions at peers and transforms users' queries to contain focused search terms. Through trace-driven simulations, we show that keyword fusion can reduce the storage consumption of the top 5% most loaded nodes by 50% and decrease the search traffic by up to 67%, even in a modest scenario of combining two keywords.","Peer to peer computing,
Distributed computing,
Computer networks,
Power engineering computing,
Workstations,
Large-scale systems,
Computer science,
Power engineering and energy,
Traffic control,
IP networks"
On the effect of localization errors on geographic face routing in sensor networks,"In the absence of localization errors, geographic routing - using a combination of greedy forwarding and face routing - has been shown to work correctly and efficiently. The effects of location errors on geographic routing have not been studied before. In this work we provide a detailed analysis of the effects of location errors on the correctness and performance of geographic routing in static sensor networks. First, we perform a micro-level behavioural analysis to identify the possible protocol error scenarios and their conditions and bounds. Then, we present results from an extensive simulation study of GPRS and GHT to quantify the performance degradation due to location errors (of 10% of the radio range or less) can in fact lead to incorrect (non-recoverable) geographic routing with noticeable performance degradation. We then introduce a simple modification for face routing that eliminates probable errors and leads to near perfect performance.","Intelligent networks,
Routing protocols,
Computer errors,
Error correction,
Performance analysis,
Degradation,
Permission,
Computer science,
Computer networks,
Algorithm design and analysis"
A comparative study of encodings to design combinational logic circuits using particle swarm optimization,"This paper extends our original proposal to use particle swarm optimization (PSO) to design combinational logic circuits (Coello Coello et al., 2003) in which a binary representation was adopted. In this case, we study the impact of the representation adopted. For that sake, we adopt 2 integer representations (one of which is proposed by us) and we compare them with respect to our previous binary representation and with respect to a multiobjective genetic algorithm that uses an integer encoding. For our comparative study, we adopted several combinational logic circuits of one and several outputs whose designs have been previously studied in the specialized literature. Our results indicate that PSO can be a competitive algorithm for circuit design when using one of the integer representations proposed.",
Tuning SoC platforms for multimedia processing: identifying limits and tradeoffs,"We present an analytical framework to identify the tradeoffs and performance impacts associated with different SoC platform configurations in the specific context of implementing multimedia applications. ""Configurations"" in this case might include sizes of different on-chip buffers and scheduling mechanisms (or associated parameters) implemented on the different processing elements of the platform. Identifying such tradeoffs is difficult because of the bursty nature of on-chip traffic arising out of multimedia processing and the high variability in their execution requirements, which result in a highly irregular design space. We show that this irregularity in the design space can be precisely captured using an abstraction called variability characterization curves.","Processor scheduling,
Time division multiple access,
System-on-a-chip,
Multimedia systems,
Costs,
Permission,
Hardware,
Computer networks,
Laboratories,
Computer science"
Identifying stakeholders and their preferences about NFR by comparing use case diagrams of several existing systems,"We present a method to identify stakeholders and their preferences about non-functional requirements (NFR) by using use case diagrams of existing systems. We focus on the changes about NFR because such changes help stakeholders to identify their preferences. Comparing different use case diagrams of the same domain helps us to find the changes that can occur. We utilize the goal-question-metrics (GQM) method to identify variables that characterize NFR. Thus, we can systematically represent changes about NFR using the variables. The use cases that represent system interactions help us to bridge the gap between goals and metrics (variables). Thus, we can easily construct measurable NFR. In order to illustrate and evaluate our method, we applied our method to an application domain of the mail user agent (MUA) system.","Computer aided software engineering,
Airplanes,
Bridges,
Computer science,
Postal services"
Bounds on the reliability of distributed systems with unreliable nodes & links,"The reliability of distributed systems & computer networks in which computing nodes and/or communication links may fail with certain probabilities have been modeled by a probabilistic network. Computing the residual connectedness reliability (RCR) of probabilistic networks under the fault model with both node & link faults is very useful, but is an NP-hard problem. Up to now, there has been little research done under this fault model. There are neither accurate solutions nor heuristic algorithms for computing the RCR. In our recent research, we challenged the problem, and found efficient algorithms for the upper & lower bounds on RCR. We also demonstrated that the difference between our upper & lower bounds gradually tends to zero for large networks, and are very close to zero for small networks. These results were used in our dependable distributed system project to find a near-optimal subset of nodes to host the replicas of a critical task.","Telecommunication network reliability,
Computer network reliability,
Upper bound,
Computer networks,
Heuristic algorithms,
Computer science,
Distributed computing,
NP-hard problem,
Floors"
News sports video shot classification with sports play field and motion features,"In this paper a novel sports news video shot classification method has been proposed. First two features based on motion and color are constructed and extracted from video shots: play field color ratio for specific types of sports, background motion and consistency ratio, then they are combined to generate an 11-dimension shot feature to feed into a C4.5 decision tree for shot classification. Based on our video data sets-the sports news video from the CNN Headline News video used in the TRECVID 2003, 7 predefined video shot classes were defined: 4 types of sports field video (basketball, baseball, ice hockey and golf) and sports news lead-in/lead-out, text and others. Sports news video segments from 15 half-hour CNN News video were used for the training and testing. A performance of average precision and recall 88%, 82% has been achieved, respectively. The proposed method can be further developed and used to search news video for individual sports news and sports highlights.","Cellular neural networks,
Ice,
Histograms,
Cameras,
Computer science,
Data mining,
Decision trees,
Classification tree analysis,
Testing,
Indexing"
Combining Head-Mounted and Projector-Based Displays for Surgical Training,"We introduce and present preliminary results for a hybrid display system combining head-mounted and projector-based displays. Our work is motivated by a surgical training application where it is necessary to simultaneously provide both a highfidelity view of a central close-up task (the surgery) and visual awareness of objects and events in the surrounding environment. In this article, we motivate the use of a hybrid display system, discuss previous work, describe a prototype along with methods for geometric calibration, and present results from a controlled human subject experiment. This article is an invited resubmission of work presented at IEEE Virtual Reality 2003. The article has been updated and expanded to include (among other things) additional related work and more details about the calibration process.",
Bayesian Networks Structure Learning and Its Application to Personalized Recommendation in a B2C Portal,"Web Intelligence (WI) is a new and active research field in current AI and IT. Personalized recommendation in an intelligent B2C portal is an important research topic in WI. In this paper, we first investigate the architecture of a B2C portal from the viewpoint of conceptual levels of WI. Aiming at data mining of knowledge-level in a B2C portal, we present a new improved learning algorithm of Bayesian Networks, which consists of two major contributions, namely, making the best of lower order Conditional Independence (CI) tests and accelerating search process by means of sort order for parent nodes. By a number of experiments on ALARM datasets, we find that the proposed algorithm is both more efficient and effective than others. We have applied this algorithm to a commodity recommendation system in a B2C portal. Our experimental results demonstrate that the recommendation method based on a Customer Shopping Model (CSM) produced by the new algorithm outperforms some traditional ones in rates of coverage and precision.","Bayesian methods,
Intelligent networks,
Portals,
Inference algorithms,
Artificial intelligence,
Nearest neighbor searches,
Clustering algorithms,
Application software,
Educational institutions,
Computer science"
Two stochastic dynamic programming problems by model-free actor-critic recurrent-network learning in non-Markovian settings,"We describe two stochastic non-Markovian dynamic programming (DP) problems, showing how the posed problems can be attacked by using actor-critic reinforcement learning with recurrent neural networks (RNN). We assume that the current state of a dynamical system is ""completely observable"", but that the rules, unknown to our decision-making agent, for the current reward and state transition depend not only on current state and action, but on possibly the ""entire history"" of past states and actions. This should not be confused with problems of ""partially observable Markov decision processes (POMDPs)"", where the current state is only deduced from either partial (observable) state alone or error-corrupted observations. Our actor-critic RNN agent is capable of finding an optimal policy, while learning neither transitional probabilities, associated rewards, nor by how much the current state space must be augmented so that the Markov property holds. The RNN's recurrent connections or context units function as an ""implicit"" history memory (or internal state) to develop ""sensitivity"" to non-Markovian dependencies, rendering the process Markovian implicitly and automatically in a ""totally model-free"" fashion. In particular, using two small-scale longest-path problems in a stochastic non-Markovian setting, we discuss model-free learning features in comparison with the model-based approach by the classical DP algorithm.","Stochastic processes,
Dynamic programming,
Recurrent neural networks,
Learning,
Electronic mail,
History,
Context modeling,
Projectiles,
Computer science,
Computer industry"
Shape representation and classification using the Poisson equation,"Silhouettes contain rich information about the shape of objects that can be used for recognition and classification. We present a novel approach that allows us to reliably compute many useful properties of a silhouette. Our approach assigns for every internal point of the silhouette a value reflecting the mean time required for a random walk beginning at the point to hit the boundaries. This function can be computed by solving Poisson's equation, with the silhouette contours providing boundary conditions. We show how this function can be used to reliably extract various shape properties including part structure and rough skeleton, local orientation and aspect ratio of different parts, and convex and concave sections of the boundaries. In addition to this we discuss properties of the solution and show how to efficiently compute this solution using multi-grid algorithms. We demonstrate the utility of the extracted properties by using them for shape classification.","Shape,
Poisson equations,
Skeleton,
Computer vision,
Boundary conditions,
Application software,
Computer science,
Data mining,
Object recognition,
Belts"
Handwriting-based learning materials on a tablet PC: a prototype and its practical studies in an elementary school,"This paper presents handwriting-based learning materials (HLMs) on a tablet PC and their practical studies in an elementary school. We designed HLMs to be created using HLM components or a HLM template for teachers to create their original materials. Practical studies demonstrated that HLMs could be effectively used when children expressed and presented their ideas in a classroom and when they practiced calculations and characters by hand. We also investigated the application of a handwriting recognition technique to automatically check children's handwritten answers to drill materials. Using questionnaires, we confirmed that automatic checking not only saved time but also improved the children's motivation to learn. HLMs were enthusiastically embraced by both field teachers and children.","Prototypes,
Educational institutions,
Handwriting recognition,
Application software,
Computer science education,
Personal communication networks,
Laboratories,
Engines,
Computer interfaces,
Keyboards"
Performance comparison of cache invalidation strategies for Internet-based mobile ad hoc networks,"Internet-based mobile ad hoc network (IMANET) combines a mobile ad hoc network (MANET) and the Internet to provide universal information accessibility. Although caching frequently accessed data items in mobile terminals (MTs) improves the communication performance in an IMANET, it brings a critical design issue when data items are updated. We analyze several push and pull-based cache invalidation strategies for IMANETS. A global positioning system (GPS) based connectivity estimation (GPSCE) scheme is first proposed to assess the connectivity of an MT for supporting any cache invalidation mechanism. Then, we propose a pull-based approach, called aggregate cache based on demand (ACOD) scheme, to find the queried data items efficiently. In addition, we modify two push-based cache invalidation strategies, proposed for cellular networks, to work in IMANETs. These are a modified timestamp (MTS) scheme, and an MTS with updated invalidation report (MTS+UIR) scheme. Simulation results indicate that our proposed strategy provides high throughput, low query latency, and low communication overhead, and thus, is a viable approach for implementation in IMANETS.","IP networks,
Mobile ad hoc networks,
Aggregates,
Mobile communication,
Personal digital assistants,
Land mobile radio cellular systems,
Network topology,
Global Positioning System,
Delay,
Computer science"
On the performance of learning machines for bankruptcy detection,"Predicting the financial health of companies is a problem of great importance to various stakeholders in the increasingly globalized economy. We apply several learning machines methods to the problem of bankruptcy prediction of private companies. Financial data obtained from Diana, a database containing 780,000 financial statements of French companies, are used to perform experiments. Classification accuracy is evaluated with respect to artificial neural networks, linear genetic programming and support vector machines. We analyze both type I (bankrupted companies misclassified as healthy) and type II (healthy companies misclassified as bankrupted) errors on three datasets containing balanced and unbalanced class distribution. Linear genetic programming has the best accuracy in the balanced data while support vector machines is more stable for the unbalanced dataset. Our results, though preliminary in nature, demonstrate the tremendous potential of using learning machines in solving important economics problems such as predicting bankruptcy with accuracy","Machine learning,
Genetic programming,
Support vector machines,
Artificial neural networks,
Economic forecasting,
Databases,
Physics computing,
Biomedical informatics,
Computer science,
Read only memory"
Implementation of Walsh function generator of order 64 using LUT cascades,"This paper introduces an application of lookup table (LUT) cascades to a design of a Walsh function generator for 64 outputs. By using LUT cascades, the Walsh functions of order 64 can be easily generated. Such function generator can be used in LSI testing, CDMA, image and signal processing.","Signal generators,
Table lookup,
Testing,
Multiaccess communication,
Very large scale integration,
Signal processing,
Logic functions,
Design engineering,
Computer science,
Application software"
Failure analysis of open faults by using detecting/un-detecting information on tests,"Recently, manufacturing defects including opens in the interconnect layers have been increasing. Therefore, a failure analysis for open faults has become important in manufacturing. Moreover, the failure analysis for open faults under BIST environment is demanded. Since the quality of the failure analysis is engaged by the resolution of locating the fault, we propose the method for locating single open fault at a stem, based on only detecting/un-detecting information on tests. Our method deduces candidate faulty stems based on the number of detections for single stuck-at fault at each fan-out branches, by performing single stuck-at fault simulation with both detecting and un-detecting tests. To improve the ability of locating the fault, the method reduces the candidate faulty stems based on the number of detections for multiple stuck-at faults at fanout branches of the candidate faulty stem, by performing multiple stuck-at fault simulation with detecting tests.","Failure analysis,
Fault detection,
Circuit faults,
Built-in self-test,
Software testing,
Performance evaluation,
Very large scale integration,
Integrated circuit interconnections,
Computer science,
Computer aided manufacturing"
