Title,Abstract,Keywords
Cloud Computing and Grid Computing 360-Degree Compared,"Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",
In defense of Nearest-Neighbor based image classification,"State-of-the-art image classification methods require an intensive learning/training stage (using SVM, Boosting, etc.) In contrast, non-parametric Nearest-Neighbor (NN) based image classifiers require no training time and have other favorable properties. However, the large performance gap between these two families of approaches rendered NN-based image classifiers useless.","Image classification,
Image databases,
Quantization,
Support vector machines,
Support vector machine classification,
Boosting,
Neural networks,
Classification tree analysis,
Degradation,
Rendering (computer graphics)"
Lost in quantization: Improving particular object retrieval in large scale image databases,"The state of the art in visual object retrieval from large databases is achieved by systems that are inspired by text retrieval. A key component of these approaches is that local regions of images are characterized using high-dimensional descriptors which are then mapped to ldquovisual wordsrdquo selected from a discrete vocabulary.This paper explores techniques to map each visual region to a weighted set of words, allowing the inclusion of features which were lost in the quantization stage of previous systems. The set of visual words is obtained by selecting words based on proximity in descriptor space. We describe how this representation may be incorporated into a standard tf-idf architecture, and how spatial verification is modified in the case of this soft-assignment. We evaluate our method on the standard Oxford Buildings dataset, and introduce a new dataset for evaluation. Our results exceed the current state of the art retrieval performance on these datasets, particularly on queries with poor initial recall where techniques like query expansion suffer. Overall we show that soft-assignment is always beneficial for retrieval with large vocabularies, at a cost of increased storage requirements for the index.","Quantization,
Information retrieval,
Image retrieval,
Large-scale systems,
Image databases,
Visual databases,
Architecture,
Buildings,
Vocabulary,
Costs"
HC-MAC: A Hardware-Constrained Cognitive MAC for Efficient Spectrum Management,"Radio spectrum resource is of fundamental importance for wireless communication. Recent reports show that most available spectrum has been allocated. While some of the spectrum bands (e.g., unlicensed band, GSM band) have seen increasingly crowded usage, most of the other spectrum resources are underutilized. This drives the emergence of open spectrum and dynamic spectrum access concepts, which allow unlicensed users equipped with cognitive radios to opportunistically access the spectrum not used by primary users. Cognitive radio has many advanced features, such as agilely sensing the existence of primary users and utilizing multiple spectrum bands simultaneously. However, in practice such capabilities are constrained by hardware cost. In this paper, we discuss how to conduct efficient spectrum management in ad hoc cognitive radio networks while taking the hardware constraints (e.g., single radio, partial spectrum sensing and spectrum aggregation limit) into consideration. A hardware-constrained cognitive MAC, HC-MAC, is proposed to conduct efficient spectrum sensing and spectrum access decision. We identify the issue of optimal spectrum sensing decision for a single secondary transmission pair, and formulate it as an optimal stopping problem. A decentralized MAC protocol is then proposed for the ad hoc cognitive radio networks. Simulation results are presented to demonstrate the effectiveness of our proposed protocol.","Radio spectrum management,
Cognitive radio,
Media Access Protocol,
Hardware,
Access protocols,
Wireless communication,
GSM,
Costs,
Technological innovation,
Computer science"
An Optimized Blockwise Nonlocal Means Denoising Filter for 3-D Magnetic Resonance Images,"A critical issue in image restoration is the problem of noise removal while keeping the integrity of relevant image information. Denoising is a crucial step to increase image quality and to improve the performance of all the tasks needed for quantitative imaging analysis. The method proposed in this paper is based on a 3-D optimized blockwise version of the nonlocal (NL)-means filter (Buades, , 2005). The NL-means filter uses the redundancy of information in the image under study to remove the noise. The performance of the NL-means filter has been already demonstrated for 2-D images, but reducing the computational burden is a critical aspect to extend the method to 3-D images. To overcome this problem, we propose improvements to reduce the computational complexity. These different improvements allow to drastically divide the computational time while preserving the performances of the NL-means filter. A fully automated and optimized version of the NL-means filter is then presented. Our contributions to the NL-means filter are: 1) an automatic tuning of the smoothing parameter; 2) a selection of the most relevant voxels; 3) a blockwise implementation; and 4) a parallelized computation. Quantitative validation was carried out on synthetic datasets generated with BrainWeb (Collins, , 1998). The results show that our optimized NL-means filter outperforms the classical implementation of the NL-means filter, as well as two other classical denoising methods [anisotropic diffusion (Perona and Malik, 1990)] and total variation minimization process (Rudin, , 1992) in terms of accuracy (measured by the peak signal-to-noise ratio) with low computation time. Finally, qualitative results on real data are presented.","Noise reduction,
Magnetic separation,
Filters,
Magnetic resonance,
Optimization methods,
Image restoration,
Magnetic noise,
Image quality,
Magnetic resonance imaging,
Image analysis"
A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications,"Detection of moving objects in video streams is the first relevant step of information extraction in many computer vision applications. Aside from the intrinsic usefulness of being able to segment video streams into moving and background components, detecting moving objects provides a focus of attention for recognition, classification, and activity analysis, making these later steps more efficient. We propose an approach based on self organization through artificial neural networks, widely applied in human image processing systems and more generally in cognitive science. The proposed approach can handle scenes containing moving backgrounds, gradual illumination variations and camouflage, has no bootstrapping limitations, can include into the background model shadows cast by moving objects, and achieves robust detection for different types of videos taken with stationary cameras. We compare our method with other modeling techniques and report experimental results, both in terms of detection accuracy and in terms of processing speed, for color video sequences that represent typical situations critical for video surveillance systems.","Surveillance,
Object detection,
Application software,
Streaming media,
Data mining,
Computer vision,
Image segmentation,
Focusing,
Artificial neural networks,
Humans"
The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.","Large-scale systems,
Face recognition,
Image databases,
Research and development,
Computer science,
Object recognition,
Application software,
Computer security,
Cameras,
Access protocols"
People-tracking-by-detection and people-detection-by-tracking,"Both detection and tracking people are challenging problems, especially in complex real world scenes that commonly involve multiple people, complicated occlusions, and cluttered or even moving backgrounds. People detectors have been shown to be able to locate pedestrians even in complex street scenes, but false positives have remained frequent. The identification of particular individuals has remained challenging as well. Tracking methods are able to find a particular individual in image sequences, but are severely challenged by real-world scenarios such as crowded street scenes. In this paper, we combine the advantages of both detection and tracking in a single framework. The approximate articulation of each person is detected in every frame based on local features that model the appearance of individual body parts. Prior knowledge on possible articulations and temporal coherency within a walking cycle are modeled using a hierarchical Gaussian process latent variable model (hGPLVM). We show how the combination of these results improves hypotheses for position and articulation of each person in several subsequent frames. We present experimental results that demonstrate how this allows to detect and track multiple people in cluttered scenes with reoccurring occlusions.","Layout,
Detectors,
Hidden Markov models,
Image sequences,
Gaussian processes,
Computer science,
Legged locomotion,
Cameras,
Indexing,
Surveillance"
Benchmarking GPUs to tune dense linear algebra,"We present performance results for dense linear algebra using recent NVIDIA GPUs. Our matrix-matrix multiply routine (GEMM) runs up to 60% faster than the vendor's implementation and approaches the peak of hardware capabilities. Our LU, QR and Cholesky factorizations achieve up to 80–90% of the peak GEMM rate. Our parallel LU running on two GPUs achieves up to ∼540 Gflop/s. These results are accomplished by challenging the accepted view of the GPU architecture and programming guidelines. We argue that modern GPUs should be viewed as multithreaded multicore vector units. We exploit blocking similarly to vector computers and heterogeneity of the system by computing both on GPU and CPU. This study includes detailed benchmarking of the GPU memory system that reveals sizes and latencies of caches and TLB. We present a couple of algorithmic optimizations aimed at increasing parallelism and regularity in the problem that provide us with slightly higher performance.",
The cost of doing science on the cloud: The Montage example,"Utility grids such as the Amazon EC2 cloud and Amazon S3 offer computational and storage resources that can be used on-demand for a fee by compute and data-intensive applications. The cost of running an application on such a cloud depends on the compute, storage and communication resources it will provision and consume. Different execution plans of the same application may result in significantly different costs. Using the Amazon cloud fee structure and a real-life astronomy application, we study via simulation the cost performance tradeoffs of different execution and resource provisioning plans. We also study these trade-offs in the context of the storage and communication fees of Amazon S3 when used for long-term application data archival. Our results show that by provisioning the right amount of storage and compute resources, cost can be significantly reduced with no significant impact on application performance.","Costs,
Cloud computing,
Grid computing,
Information analysis,
Marine technology,
Optical computing,
Collaborative work,
Permission,
Computer applications,
Astronomy"
Four-Chamber Heart Modeling and Automatic Segmentation for 3-D Cardiac CT Volumes Using Marginal Space Learning and Steerable Features,"We propose an automatic four-chamber heart segmentation system for the quantitative functional analysis of the heart from cardiac computed tomography (CT) volumes. Two topics are discussed: heart modeling and automatic model fitting to an unseen volume. Heart modeling is a nontrivial task since the heart is a complex nonrigid organ. The model must be anatomically accurate, allow manual editing, and provide sufficient information to guide automatic detection and segmentation. Unlike previous work, we explicitly represent important landmarks (such as the valves and the ventricular septum cusps) among the control points of the model. The control points can be detected reliably to guide the automatic model fitting process. Using this model, we develop an efficient and robust approach for automatic heart chamber segmentation in 3D CT volumes. We formulate the segmentation as a two-step learning problem: anatomical structure localization and boundary delineation. In both steps, we exploit the recent advances in learning discriminative models. A novel algorithm, marginal space learning (MSL), is introduced to solve the 9-D similarity transformation search problem for localizing the heart chambers. After determining the pose of the heart chambers, we estimate the 3D shape through learning-based boundary delineation. The proposed method has been extensively tested on the largest dataset (with 323 volumes from 137 patients) ever reported in the literature. To the best of our knowledge, our system is the fastest with a speed of 4.0 s per volume (on a dual-core 3.2-GHz processor) for the automatic segmentation of all four chambers.","Heart,
Computed tomography,
Automatic control,
Functional analysis,
Valves,
Robustness,
Anatomical structure,
Search problems,
Shape,
Testing"
Enhanced Forward Explicit Congestion Notification (E-FECN) scheme for datacenter Ethernet networks,"Ethernet is replacing the traditional storage networking technologies like Fiber Channel and Infiniband in Datacenters. The key feature of these traditional technologies that make them suitable for datacenter is their low-loss low-delay operation. Consequently IEEE 802.1 standards committee is developing new specification for congestion management for Ethernet in datacenter networks. Backward Congestion Notification (BCN) and Forward Explicit Congestion Notification (FECN) are two initial proposals. Each of the proposals has its own advantages and disadvantages. FECN outperforms BCN in fairness and response time while BCN is able to respond to sudden increases in load in less than a round trip time. In this paper, we propose an enhanced version of FECN that takes the best of both proposals and adds BCN if there is a sudden severe congestion. It is shown that E-FECN performs better than the previous proposals.",
Cooperative spectrum sensing with transmit and relay diversity in cognitive radio networks - [transaction letters],"In the letter the problem of cooperative spectrum sensing is investigated in cognitive radio (CR) networks over Rayleigh fading channels. By taking into account the error effect on the decision reporting, a general performance analysis of cooperative spectrum sensing is given. The analytical detection results show that the performance of cooperative spectrum sensing is limited by the probability of reporting errors. To deal with this limitation, we propose a transmit diversity based cooperative spectrum sensing method. By regarding multiple CRs as a virtual antenna array, space-time coding and space-frequency coding are applied into CR networks over flat-fading and frequency-selective fading channels, respectively. Moreover, we propose a relay diversity based cooperative spectrum sensing approach to increase the diversity of detection when some CRs are in heavy shadowing. It is then shown that, when combined with algebraic coding, relay diversity can further improve the cooperative spectrum sensing performance.","Relays,
Cognitive radio,
Chromium,
Fading,
Performance analysis,
FCC,
Cities and towns,
Interference,
Additive white noise,
AWGN"
Image super-resolution as sparse representation of raw image patches,"This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse and the recovered high-resolution image is competitive or even superior in quality to images produced by other SR methods.","Image resolution,
Signal resolution,
Dictionaries,
Image reconstruction,
Strontium,
Equations,
Compressed sensing,
Prototypes,
Inverse problems,
Markov random fields"
Finite-state Markov modeling of fading channels - a survey of principles and applications,"This article's goal is to provide an in-depth understanding of the principles of FSMC modeling of fading channels with its applications in wireless communication systems. While the emphasis is on frequency nonselective or flat-fading channels, this understanding will be useful for future generalizations of FSMC models for frequency-selective fading channels. The target audience of this article include both theory- and practice-oriented researchers who would like to design accurate channel models for evaluating the performance of wireless communication systems in the physical or media access control layers, or those who would like to develop more efficient and reliable transceivers that take advantage of the inherent memory in fading channels. Both FSMC models and flat-fading channels will be formally introduced. FSMC models are particulary suitable to represent and estimate the relatively fast flat-fading channel gain in each subcarrier.","Fading,
Telephony,
Circuits,
Wireless communication,
Power capacitors,
Channel capacity,
Error analysis,
Communication channels,
History,
Transmitters"
Content-Based Music Information Retrieval: Current Directions and Future Challenges,"The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.","Music information retrieval,
Content based retrieval,
Multiple signal classification,
Marketing and sales,
Organizing,
Cognition,
Computer science,
Signal processing algorithms,
Fingerprint recognition,
Digital signal processing"
A Review of Geometric Transformations for Nonrigid Body Registration,"This paper provides a comprehensive and quantitative review of spatial transformations models for nonrigid image registration. It explains the theoretical foundation of the models and classifies them according to this basis. This results in two categories, physically based models described by partial differential equations of continuum mechanics (e.g., linear elasticity and fluid flow) and basis function expansions derived from interpolation and approximation theory (e.g., radial basis functions, B-splines and wavelets). Recent work on constraining the transformation so that it preserves the topology or is diffeomorphic is also described. The final section reviews some recent evaluation studies. The paper concludes by explaining under what conditions a particular transformation model is appropriate.","Spline,
Tensile stress,
Image registration,
Partial differential equations,
Elasticity,
Lagrangian functions,
Fluid flow,
Interpolation,
Approximation methods,
Topology"
Action recognition by learning mid-level motion features,"This paper presents a method for human action recognition based on patterns of motion. Previous approaches to action recognition use either local features describing small patches or large-scale features describing the entire human figure. We develop a method constructing mid-level motion features which are built from low-level optical flow information. These features are focused on local regions of the image sequence and are created using a variant of AdaBoost. These features are tuned to discriminate between different classes of action, and are efficient to compute at run-time. A battery of classifiers based on these mid-level features is created and used to classify input sequences. State-of-the-art results are presented on a variety of standard datasets.","Shape,
Humans,
Image motion analysis,
Image sequences,
Pattern recognition,
Video sequences,
Object recognition,
Biomedical optical imaging,
Large-scale systems,
Runtime"
Adaptive Data Hiding in Edge Areas of Images With Spatial LSB Domain Systems,"This paper proposes a new adaptive least-significant- bit (LSB) steganographic method using pixel-value differencing (PVD) that provides a larger embedding capacity and imperceptible stegoimages. The method exploits the difference value of two consecutive pixels to estimate how many secret bits will be embedded into the two pixels. Pixels located in the edge areas are embedded by a k-bit LSB substitution method with a larger value of k than that of the pixels located in smooth areas. The range of difference values is adaptively divided into lower level, middle level, and higher level. For any pair of consecutive pixels, both pixels are embedded by the k-bit LSB substitution method. However, the value k is adaptive and is decided by the level which the difference value belongs to. In order to remain at the same level where the difference value of two consecutive pixels belongs, before and after embedding, a delicate readjusting phase is used. When compared to the past study of Wu et al.'s PVD and LSB replacement method, our experimental results show that our proposed approach provides both larger embedding capacity and higher image quality.","Data encapsulation,
Steganography,
Atherosclerosis,
Internet,
Sun,
Cryptography,
Councils,
Computer science,
Image quality"
Space mapping,"In this article we review state-of-the-art concepts of space mapping and place them con- textually into the history of design optimization and modeling of microwave circuits. We formulate a generic space-mapping optimization algorithm, explain it step-by-step using a simple microstrip filter example, and then demonstrate its robustness through the fast design of an interdigital filter. Selected topics of space mapping are discussed, including implicit space mapping, gradient-based space mapping, the optimal choice of surrogate model, and tuning space mapping. We consider the application of space mapping to the modeling of microwave structures. We also discuss a software package for automated space-mapping optimization that involves both electromagnetic (EM) and circuit simulators.","Optimization,
Integrated circuit modeling,
Microwave circuits,
Computational modeling,
Solid modeling,
Algorithm design and analysis"
On the Levy-Walk Nature of Human Mobility,"We report that human walks performed in outdoor settings of tens of kilometers resemble a truncated form of Levy walks commonly observed in animals such as monkeys, birds and jackals. Our study is based on about one thousand hours of GPS traces involving 44 volunteers in various outdoor settings including two different college campuses, a metropolitan area, a theme park and a state fair. This paper shows that many statistical features of human walks follow truncated power-law, showing evidence of scale-freedom and do not conform to the central limit theorem. These traits are similar to those of Levy walks. It is conjectured that the truncation, which makes the mobility deviate from pure Levy walks, comes from geographical constraints including walk boundary, physical obstructions and traffic. None of commonly used mobility models for mobile networks captures these properties. Based on these findings, we construct a simple Levy walk mobility model which is versatile enough in emulating diverse statistical patterns of human walks observed in our traces. The model is also used to recreate similar power-law inter-contact time distributions observed in previous human mobility studies. Our network simulation indicates that the Levy walk features are important in characterizing the performance of mobile network routing performance.","Humans,
Biological system modeling,
Global Positioning System,
Gaussian distribution,
Animals,
Urban areas,
Communications Society,
Computer science,
Birds,
Educational institutions"
A Surface-Based Approach to Quantify Local Cortical Gyrification,"The high complexity of cortical convolutions in humans is very challenging both for engineers to measure and compare it, and for biologists and physicians to understand it. In this paper, we propose a surface-based method for the quantification of cortical gyrification. Our method uses accurate 3-D cortical reconstruction and computes local measurements of gyrification at thousands of points over the whole cortical surface. The potential of our method to identify and localize precisely gyral abnormalities is illustrated by a clinical study on a group of children affected by 22q11 Deletion Syndrome, compared to control individuals.","Biomedical imaging,
Educational institutions,
Biomedical signal processing,
Surface reconstruction,
Neuroimaging,
Psychiatry,
Humans,
Image reconstruction,
Biology computing,
Biomedical measurements"
Evolutionary many-objective optimization: A short review,"Whereas evolutionary multiobjective optimization (EMO) algorithms have successfully been used in a wide range of real-world application tasks, difficulties in their scalability to many-objective problems have also been reported. In this paper, first we demonstrate those difficulties through computational experiments. Then we review some approaches proposed in the literature for the scalability improvement of EMO algorithms. Finally we suggest future research directions in evolutionary many-objective optimization.",Evolutionary computation
Preserving Privacy in Social Networks Against Neighborhood Attacks,"Recently, as more and more social network data has been published in one way or another, preserving privacy in publishing social network data becomes an important concern. With some local knowledge about individuals in a social network, an adversary may attack the privacy of some victims easily. Unfortunately, most of the previous studies on privacy preservation can deal with relational data only, and cannot be applied to social network data. In this paper, we take an initiative towards preserving privacy in social network data. We identify an essential type of privacy attacks: neighborhood attacks. If an adversary has some knowledge about the neighbors of a target victim and the relationship among the neighbors, the victim may be re-identified from a social network even if the victim's identity is preserved using the conventional anonymization techniques. We show that the problem is challenging, and present a practical solution to battle neighborhood attacks. The empirical study indicates that anonymized social networks generated by our method can still be used to answer aggregate network queries with high accuracy.","Social network services,
Privacy,
Loss measurement,
Data privacy,
Knowledge engineering,
Publishing,
Data models"
Optic Disc Detection From Normalized Digital Fundus Images by Means of a Vessels' Direction Matched Filter,"Optic disc (OD) detection is a main step while developing automated screening systems for diabetic retinopathy. We present in this paper a method to automatically detect the position of the OD in digital retinal fundus images. The method starts by normalizing luminosity and contrast through out the image using illumination equalization and adaptive histogram equalization methods respectively. The OD detection algorithm is based on matching the expected directional pattern of the retinal blood vessels. Hence, a simple matched filter is proposed to roughly match the direction of the vessels at the OD vicinity. The retinal vessels are segmented using a simple and standard 2-D Gaussian matched filter. Consequently, a vessels direction map of the segmented retinal vessels is obtained using the same segmentation algorithm. The segmented vessels are then thinned, and filtered using local intensity, to represent finally the OD-center candidates. The difference between the proposed matched filter resized into four different sizes, and the vessels' directions at the surrounding area of each of the OD-center candidates is measured. The minimum difference provides an estimate of the OD-center coordinates. The proposed method was evaluated using a subset of the STARE project's dataset, containing 81 fundus images of both normal and diseased retinas, and initially used by literature OD detection methods. The OD-center was detected correctly in 80 out of the 81 images (98.77%). In addition, the OD-center was detected correctly in all of the 40 images (100%) using the publicly available DRIVE dataset.","Optical filters,
Optical detectors,
Matched filters,
Retina,
Adaptive equalizers,
Retinal vessels,
Diabetes,
Retinopathy,
Lighting,
Histograms"
Automatic Model-Based Segmentation of the Heart in CT Images,"Automatic image processing methods are a pre-requisite to efficiently analyze the large amount of image data produced by computed tomography (CT) scanners during cardiac exams. This paper introduces a model-based approach for the fully automatic segmentation of the whole heart (four chambers, myocardium, and great vessels) from 3-D CT images. Model adaptation is done by progressively increasing the degrees-of-freedom of the allowed deformations. This improves convergence as well as segmentation accuracy. The heart is first localized in the image using a 3-D implementation of the generalized Hough transform. Pose misalignment is corrected by matching the model to the image making use of a global similarity transformation. The complex initialization of the multicompartment mesh is then addressed by assigning an affine transformation to each anatomical region of the model. Finally, a deformable adaptation is performed to accurately match the boundaries of the patient's anatomy. A mean surface-to-surface error of 0.82 mm was measured in a leave-one-out quantitative validation carried out on 28 images. Moreover, the piecewise affine transformation introduced for mesh initialization and adaptation shows better interphase and interpatient shape variability characterization than commonly used principal component analysis.","Image segmentation,
Heart,
Computed tomography,
Image processing,
Image analysis,
Myocardium,
Adaptation model,
Deformable models,
Convergence,
Anatomy"
The variational approximation for Bayesian inference,"The influence of this Thomas Bayes' work was immense. It was from here that ""Bayesian"" ideas first spread through the mathematical world, as Bayes's own article was ignored until 1780 and played no important role in scientific debate until the 20th century. It was also this article of Laplace's that introduced the mathematical techniques for the asymptotic analysis of posterior distributions that are still employed today. And it was here that the earliest example of optimum estimation can be found, the derivation and characterization of an estimator that minimized a particular measure of posterior expected loss. After more than two centuries, we mathematicians, statisticians cannot only recognize our roots in this masterpiece of our science, we can still learn from it.","Bayesian methods,
Signal processing algorithms,
Inference algorithms,
Iterative algorithms,
Maximum likelihood estimation,
Autobiographies,
Loss measurement,
Particle measurements,
Approximation algorithms,
Life estimation"
Summarizing visual data using bidirectional similarity,"We propose a principled approach to summarization of visual data (images or video) based on optimization of a well-defined similarity measure. The problem we consider is re-targeting (or summarization) of image/video data into smaller sizes. A good “visual summary” should satisfy two properties: (1) it should contain as much as possible visual information from the input data; (2) it should introduce as few as possible new visual artifacts that were not in the input data (i.e., preserve visual coherence). We propose a bi-directional similarity measure which quantitatively captures these two requirements: Two signals S and T are considered visually similar if all patches of S (at multiple scales) are contained in T, and vice versa. The problem of summarization/re-targeting is posed as an optimization problem of this bi-directional similarity measure. We show summarization results for image and video data. We further show that the same approach can be used to address a variety of other problems, including automatic cropping, completion and synthesis of visual data, image collage, object removal, photo reshuffling and more.",
Floodgate: A Micropayment Incentivized P2P Content Delivery Network,"As the sale of digital content is moving more and more online, the content providers are beginning to realize that bandwidth infrastructures are not easily scalable. The emergence of peer-to-peer content delivery networks present these providers with a way to overcome this limitation. However, such networks have so far been ad-hoc in nature. One of the main reason for this has been the lack of incentives for end users to contribute their bandwidth to the network. In this paper we present the design and implementation of a peer-to-peer protocol named Floodgate that provides a micropayment based incentive for peers to contribute their bandwidth. Floodgate implements an optimistic fair exchange protocol and is designed to be resilient against targeted attacks. Performance measurements, including those conducted over the PlanetLab infrastructure, show that Floodgate's security and cryptographic overheads are low when compared against the standard BitTorrent implementation.",
SybilGuard: Defending Against Sybil Attacks via Social Networks,"Peer-to-peer and other decentralized, distributed systems are known to be particularly vulnerable to sybil attacks. In a sybil attack, a malicious user obtains multiple fake identities and pretends to be multiple, distinct nodes in the system. By controlling a large fraction of the nodes in the system, the malicious user is able to ldquoout voterdquo the honest users in collaborative tasks such as Byzantine failure defenses. This paper presents SybilGuard, a novel protocol for limiting the corruptive influences of sybil attacks. Our protocol is based on the ldquosocial networkrdquo among user identities, where an edge between two identities indicates a human-established trust relationship. Malicious users can create many identities but few trust relationships. Thus, there is a disproportionately small ldquocutrdquo in the graph between the sybil nodes and the honest nodes. SybilGuard exploits this property to bound the number of identities a malicious user can create. We show the effectiveness of SybilGuard both analytically and experimentally.",
1-Bit compressive sensing,"Compressive sensing is a new signal acquisition technology with the potential to reduce the number of measurements required to acquire signals that are sparse or compressible in some basis. Rather than uniformly sampling the signal, compressive sensing computes inner products with a randomized dictionary of test functions. The signal is then recovered by a convex optimization that ensures the recovered signal is both consistent with the measurements and sparse. Compressive sensing reconstruction has been shown to be robust to multi-level quantization of the measurements, in which the reconstruction algorithm is modified to recover a sparse signal consistent to the quantization measurements. In this paper we consider the limiting case of 1-bit measurements, which preserve only the sign information of the random measurements. Although it is possible to reconstruct using the classical compressive sensing approach by treating the 1-bit measurements as ± 1 measurement values, in this paper we reformulate the problem by treating the 1-bit measurements as sign constraints and further constraining the optimization to recover a signal on the unit sphere. Thus the sparse signal is recovered within a scaling factor. We demonstrate that this approach performs significantly better compared to the classical compressive sensing reconstruction methods, even as the signal becomes less sparse and as the number of measurements increases.",
Reciprocal Velocity Obstacles for real-time multi-agent navigation,"In this paper, we propose a new concept — the ‘Reciprocal Velocity Obstacle’— for real-time multi-agent navigation. We consider the case in which each agent navigates independently without explicit communication with other agents. Our formulation is an extension of the Velocity Obstacle concept [3], which was introduced for navigation among (passively) moving obstacles. Our approach takes into account the reactive behavior of the other agents by implicitly assuming that the other agents make a similar collision-avoidance reasoning. We show that this method guarantees safe and oscillation-free motions for each of the agents. We apply our concept to navigation of hundreds of agents in densely populated environments containing both static and moving obstacles, and we show that real-time and scalable performance is achieved in such challenging scenarios.",
Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures,"Understanding the most efficient design and utilization of emerging multicore systems is one of the most challenging questions faced by the mainstream and scientific computing industries in several decades. Our work explores multicore stencil (nearest-neighbor) computations — a class of algorithms at the heart of many structured grid codes, including PDE solvers. We develop a number of effective optimization strategies, and build an auto-tuning environment that searches over our optimizations and their parameters to minimize runtime, while maximizing performance portability. To evaluate the effectiveness of these strategies we explore the broadest set of multicore architectures in the current HPC literature, including the Intel Clovertown, AMD Barcelona, Sun Victoria Falls, IBM QS22 PowerXCell 8i, and NVIDIA GTX280. Overall, our auto-tuning optimization methodology results in the fastest multicore stencil performance to date. Finally, we present several key insights into the architectural tradeoffs of emerging multicore designs and their implications on scientific algorithm development.","Multicore processing,
Computer architecture,
Scientific computing,
Computer industry,
Grid computing,
Heart,
Runtime environment,
Sun,
Optimization methods,
Algorithm design and analysis"
Communication patterns in VANETs,"Vehicular networks are a very promising technology to increase traffic safety and efficiency, and to enable numerous other applications in the domain of vehicular communication. Proposed applications for VANETs have very diverse properties and often require nonstandard communication protocols. Moreover, the dynamics of the network due to vehicle movement further complicates the design of an appropriate comprehensive communication system. In this article we collect and categorize envisioned applications from various sources and classify the unique network characteristics of vehicular networks. Based on this analysis, we propose five distinct communication patterns that form the basis of almost all VANET applications. Both the analysis and the communication patterns shall deepen the understanding of VANETs and simplify further development of VANET communication systems.","Telecommunication traffic,
Wireless communication,
Vehicle safety,
Automotive engineering,
Vehicle dynamics,
Communication system traffic,
Pattern analysis,
Algorithm design and analysis,
Wireless application protocol,
System testing"
Combining geometry and combinatorics: A unified approach to sparse signal recovery,"There are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. The geometric approach utilizes geometric properties of the measurement matrix Φ. A notable example is the Restricted Isometry Property, which states that the mapping Φ preserves the Euclidean norm of sparse signals; it is known that random dense matrices satisfy this constraint with high probability. On the other hand, the combinatorial approach utilizes sparse matrices, interpreted as adjacency matrices of sparse (possibly random) graphs, and uses combinatorial techniques to recover an approximation to the signal.",
"Object categorization using co-occurrence, location and appearance","In this work we introduce a novel approach to object categorization that incorporates two types of context — co-occurrence and relative location — with local appearance-based features. Our approach, named CoLA (for Co-occurrence, Location and Appearance), uses a conditional random field (CRF) to maximize object label agreement according to both semantic and spatial relevance. We model relative location between objects using simple pairwise features. By vector quantizing this feature space, we learn a small set of prototypical spatial relationships directly from the data. We evaluate our results on two challenging datasets: PASCAL 2007 and MSRC. The results show that combining co-occurrence and spatial context improves accuracy in as many as half of the categories compared to using co-occurrence alone.","Layout,
Object recognition,
Image segmentation,
Context modeling,
Computer science,
Prototypes,
Face detection,
Lighting,
Psychology,
Computer vision"
Nonchronological Video Synopsis and Indexing,"The amount of captured video is growing with the increased numbers of video cameras, especially the increase of millions of surveillance cameras that operate 24 hours a day. Since video browsing and retrieval is time consuming, most captured video is never watched or examined. Video synopsis is an effective tool for browsing and indexing of such a video. It provides a short video representation, while preserving the essential activities of the original video. The activity in the video is condensed into a shorter period by simultaneously showing multiple activities, even when they originally occurred at different times. The synopsis video is also an index into the original video by pointing to the original time of each activity. Video synopsis can be applied to create a synopsis of an endless video streams, as generated by Webcams and by surveillance cameras. It can address queries like ""show in one minute the synopsis of this camera broadcast during the past day''. This process includes two major phases: (i) an online conversion of the endless video stream into a database of objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user's query.","Indexing,
Cameras,
Surveillance,
Streaming media,
Sorting,
Spatiotemporal phenomena,
Birds,
Watches,
Multimedia communication,
Broadcasting"
Provenance for Computational Tasks: A Survey,"The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.",
Efficient Multilevel Brain Tumor Segmentation With Integrated Bayesian Model Classification,"We present a new method for automatic segmentation of heterogeneous image data that takes a step toward bridging the gap between bottom-up affinity-based segmentation methods and top-down generative model based approaches. The main contribution of the paper is a Bayesian formulation for incorporating soft model assignments into the calculation of affinities, which are conventionally model free. We integrate the resulting model-aware affinities into the multilevel segmentation by weighted aggregation algorithm, and apply the technique to the task of detecting and segmenting brain tumor and edema in multichannel magnetic resonance (MR) volumes. The computationally efficient method runs orders of magnitude faster than current state-of-the-art techniques giving comparable or improved results. Our quantitative results indicate the benefit of incorporating model-aware affinities into the segmentation process for the difficult case of glioblastoma multiforme brain tumor.","Neoplasms,
Bayesian methods,
Brain modeling,
Image segmentation,
Biomedical imaging,
Magnetic resonance,
Biomedical engineering,
Medical diagnostic imaging,
Image analysis,
Libraries"
Brain-Computer Interfaces Based on Visual Evoked Potentials,"Recently, electroencephalogram (EEG)-based brain- computer interfaces (BCIs) have become a hot spot in the study of neural engineering, rehabilitation, and brain science. In this article, we review BCI systems based on visual evoked potentials (VEPs). Although the performance of this type of BCI has already been evaluated by many research groups through a variety of laboratory demonstrations, researchers are still facing many difficulties in changing the demonstrations to practically applicable systems. On the basis of the literature, we describe the challenges in developing practical BCI systems. Also, our recent work in the designs and implementations of the BCI systems based on steady-state VEPs (SSVEPs) is described in detail. The results show that by adequately considering the problems encountered in system design, signal processing, and parameter optimization, SSVEPs can provide the most useful information about brain activities using the least number of electrodes. At the same time, system cost could be greatly decreased and usability could be readily improved, thus benefiting the implementation of a practical BCI.","Brain computer interfaces,
Computer interfaces,
Neural engineering,
Laboratories,
Steady-state,
Signal processing,
Design optimization,
Electrodes,
Costs,
Usability"
IEEE 802.16J relay-based wireless access networks: an overview,"Multihop wireless systems have the potential to offer improved coverage and capacity over single-hop radio access systems. Standards development organizations are considering how to incorporate such techniques into new standards. One such initiative is the IEEE 802.16j standardization activity, adding relay capabilities to IEEE 802.16 systems. This article provides an overview of this relay-based technology, focusing on some of the most pertinent aspects. In particular, the different modes of operation (transparent and non-transparent), framing structures, and network entry procedures are described. Some consideration of the issues in designing such systems is then given, which highlights when different features within the standard are most appropriate. As these systems are very new, many open issues remain to be resolved.","Relays,
Wireless networks,
Spread spectrum communication,
Standardization,
Wireless mesh networks,
Wireless sensor networks,
Power system relaying,
Costs,
Standards development,
IEEE activities"
A Region Ensemble for 3-D Face Recognition,"In this paper, we introduce a new system for 3D face recognition based on the fusion of results from a committee of regions that have been independently matched. Experimental results demonstrate that using 28 small regions on the face allow for the highest level of 3D face recognition. Score-based fusion is performed on the individual region match scores and experimental results show that the Borda count and consensus voting methods yield higher performance than the standard sum, product, and min fusion rules. In addition, results are reported that demonstrate the robustness of our algorithm by simulating large holes and artifacts in images. To our knowledge, no other work has been published that uses a large number of 3D face regions for high-performance face matching. Rank one recognition rates of 97.2% and verification rates of 93.2% at a 0.1% false accept rate are reported and compared to other methods published on the face recognition grand challenge v2 data set.","Face recognition,
Voting,
Principal component analysis,
Robustness,
Image recognition,
Iterative closest point algorithm,
Iterative algorithms,
Computer science,
Shape,
Testing"
Security in Cognitive Radio Networks: Threats and Mitigation,"This paper describes a new class of attacks specific to cognitive radio networks. Wireless devices that can learn from their environment can also be taught things by malicious elements of their environment. By putting artificial intelligence in charge of wireless network devices, we are allowing unanticipated, emergent behavior, fitting a perhaps distorted or manipulated level of optimality. The state space for a cognitive radio is made up of a variety of learned beliefs and current sensor inputs. By manipulating radio sensor inputs, an adversary can affect the beliefs of a radio, and consequently its behavior. In this paper we focus primarily on PHY-layer issues, describing several classes of attacks and giving specific examples for dynamic spectrum access and adaptive radio scenarios. These attacks demonstrate the capabilities of an attacker who can manipulate the spectral environment when a radio is learning. The most powerful of which is a self-propagating AI virus that could interactively teach radios to become malicious. We then describe some approaches for mitigating the effectiveness of these attacks by instilling some level of ""common sense"" into radio systems, and requiring learned beliefs to expire and be relearned. Lastly we provide a road-map for extending these ideas to higher layers in the network stack.","Cognitive radio,
Artificial intelligence,
Data security,
Cryptography,
Engines,
Physical layer,
Computer networks,
Communication system security,
Intelligent sensors,
Statistics"
Cognitive MIMO radio,"Radio regulatory bodies are recognizing that the rigid spectrum assignment granting exclusive use to licensed services is highly inefficient, due to the high variability of the traffic statistics across time, space, and frequency. Recent Federal Communications Commission (FCC) measurements show that, in fact, the spectrum usage is typically concentrated over certain portions of the spectrum, while a significant amount of the licensed bands (or idle slots in static time division multiple access (TDMA) systems with bursty traffic) remains unused or underutilized for 90% of time [1]. It is not surprising then that this inefficiency is motivating a flurry of research activities in the engineering, economics, and regulation communities in the effort of finding more efficient spectrum management policies.","MIMO,
Interference constraints,
FCC,
Time division multiple access,
Radio spectrum management,
Cognitive radio,
Chromium,
Quality of service,
Radio transmitters,
Statistics"
A high-resolution 3D dynamic facial expression database,"Face information processing relies on the quality of data resource. From the data modality point of view, a face database can be 2D or 3D, and static or dynamic. From the task point of view, the data can be used for research of computer based automatic face recognition, face expression recognition, face detection, or cognitive and psychological investigation. With the advancement of 3D imaging technologies, 3D dynamic facial sequences (called 4D data) have been used for face information analysis. In this paper, we focus on the modality of 3D dynamic data for the task of facial expression recognition. We present a newly created high-resolution 3D dynamic facial expression database, which is made available to the scientific research community. The database contains 606 3D facial expression sequences captured from 101 subjects of various ethnic backgrounds. The database has been validated through our facial expression recognition experiment using an HMM based 3D spatio-temporal facial descriptor. It is expected that such a database shall be used to facilitate the facial expression analysis from a static 3D space to a dynamic 3D space, with a goal of scrutinizing facial behavior at a higher level of detail in a real 3D spatio-temporal domain.",
ICT4D 2.0: The Next Phase of Applying ICT for International Development,"Use of information and communication technologies for international development is moving to its next phase. This will require new technologies, new approaches to innovation, new intellectual integration, and, above all, a new view of the world's poor. The phase change from information and communication technologies for international development (ICT4D) 1.0 to ICT4D 2.0 presents opportunities for informatics professionals and offers new markets for ICT vendors. It also brings new challenges to our established methods of working and emphasizes the need for new expertise and new world views. The paper have shown that ICT4D 2.0 focuses on reframing the poor. Where ICT4D 1.0 marginalized them, allowing a supply-driven focus, ICT4D 2.0 centralizes them, creating a demand-driven focus. Where ICT4D 1.0 - fortified by the ""bottom of the pyramid"" concept - characterized the poor largely as passive consumers, ICT4D 2.0 sees them as active producers and innovators.","Informatics,
Diseases,
Internet,
Ethics,
Productivity,
Terrorism,
Standards development,
Environmental economics,
Statistics,
Application software"
Proactive process-level live migration in HPC environments,"As the number of nodes in high-performance computing environments keeps increasing, faults are becoming common place. Reactive fault tolerance (FT) often does not scale due to massive I/O requirements and relies on manual job resubmission. This work complements reactive with proactive FT at the process level. Through health monitoring, a subset of node failures can be anticipated when one's health deteriorates. A novel process-level live migration mechanism supports continued execution of applications during much of processes migration. This scheme is integrated into an MPI execution environment to transparently sustain health-inflicted node failures, which eradicates the need to restart and requeue MPI jobs. Experiments indicate that 1–6.5 seconds of prior warning are required to successfully trigger live process migration while similar operating system virtualization mechanisms require 13–24 seconds. This self-healing approach complements reactive FT by nearly cutting the number of checkpoints in half when 70% of the faults are handled proactively.","Condition monitoring,
Computer science,
Fault tolerant systems,
Middleware,
Temperature sensors,
US Department of Energy,
Chaos,
Mathematics,
Fault tolerance,
Operating systems"
An approach to detecting duplicate bug reports using natural language and execution information,"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as DUPLICATE and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67%-93% of duplicate bug reports in the Firefox bug repository, compared to 43%-72% using natural language information alone.","Natural languages,
Costs,
Software maintenance,
Software quality,
Testing,
Computer bugs,
Laboratories,
Educational technology,
Computer science education,
Open source software"
Realistic and Efficient Multi-Channel Communications in Wireless Sensor Networks,"This paper demonstrates how to use multiple channels to improve communication performance in Wireless Sensor Networks (WSNs). We first investigate multi-channel realities in WSNs through intensive empirical experiments with Micaz motes. Our study shows that current multi-channel protocols are not suitable for WSNs, because of the small number of available channels and unavoidable time errors found in real networks. With these observations, we propose a novel tree-based multichannel scheme for data collection applications, which allocates channels to disjoint trees and exploits parallel transmissions among trees. In order to minimize interference within trees, we define a new channel assignment problem which is proven NP- complete. Then we propose a greedy channel allocation algorithm which outperforms other schemes in dense networks with a small number of channels.We implement our protocol, called TMCP, in a real testbed. Through both simulation and real experiments, we show that TMCP can significantly improve network throughput and reduce packet losses. More importantly, evaluation results show that TMCP better accommodates multi-channel realities found in WSNs than other multi-channel protocols.",
Probabilistic graph and hypergraph matching,"We consider the problem of finding a matching between two sets of features, given complex relations among them, going beyond pairwise. Each feature set is modeled by a hypergraph where the complex relations are represented by hyper-edges. A match between the feature sets is then modeled as a hypergraph matching problem. We derive the hyper-graph matching problem in a probabilistic setting represented by a convex optimization. First, we formalize a soft matching criterion that emerges from a probabilistic interpretation of the problem input and output, as opposed to previous methods that treat soft matching as a mere relaxation of the hard matching problem. Second, the model induces an algebraic relation between the hyper-edge weight matrix and the desired vertex-to-vertex probabilistic matching. Third, the model explains some of the graph matching normalization proposed in the past on a heuristic basis such as doubly stochastic normalizations of the edge weights. A key benefit of the model is that the global optimum of the matching criteria can be found via an iterative successive projection algorithm. The algorithm reduces to the well known Sinkhorn [15] row/column matrix normalization procedure in the special case when the two graphs have the same number of vertices and a complete matching is desired. Another benefit of our model is the straight-forward scalability from graphs to hyper-graphs.",
System-Level Performance Metrics for Multiprogram Workloads,"Assessing the performance of multiprogram workloads running on multithreaded hardware is difficult because it involves a balance between single-program performance and overall system performance. This article argues for developing multiprogram performance metrics in a top-down fashion starting from system-level objectives. The authors propose two performance metrics: average normalized turnaround time, a user-oriented metric, and system throughput, a system-oriented metric.",
Optimal Wavelet Transform for the Detection of Microaneurysms in Retina Photographs,"In this paper, we propose an automatic method to detect microaneurysms in retina photographs. Microaneurysms are the most frequent and usually the first lesions to appear as a consequence of diabetic retinopathy. So, their detection is necessary for both screening the pathology and follow up (progression measurement). Automating this task, which is currently performed manually, would bring more objectivity and reproducibility. We propose to detect them by locally matching a lesion template in sub- bands of wavelet transformed images. To improve the method performance, we have searched for the best adapted wavelet within the lifting scheme framework. The optimization process is based on a genetic algorithm followed by Powell's direction set descent. Results are evaluated on 120 retinal images analyzed by an expert and the optimal wavelet is compared to different conventional mother wavelets. These images are of three different modalities: there are color photographs, green filtered photographs, and angiographs. Depending on the imaging modality, microaneurysms were detected with a sensitivity of respectively 89.62%, 90.24%, and 93.74% and a positive predictive value of respectively 89.50%, 89.75%, and 91.67%, which is better than previously published methods.","Wavelet transforms,
Retina,
Lesions,
Wavelet analysis,
Diabetes,
Retinopathy,
Pathology,
Reproducibility of results,
Genetic algorithms,
Image analysis"
Resilient Network Coding in the Presence of Byzantine Adversaries,"Network coding substantially increases network throughput. But since it involves mixing of information inside the network, a single corrupted packet generated by a malicious node can end up contaminating all the information reaching a destination, preventing decoding. This paper introduces distributed polynomial-time rate-optimal network codes that work in the presence of Byzantine nodes. We present algorithms that target adversaries with different attacking capabilities. When the adversary can eavesdrop on all links and jam links, our first algorithm achieves a rate of , where is the network capacity. In contrast, when the adversary has limited eavesdropping capabilities, we provide algorithms that achieve the higher rate of . Our algorithms attain the optimal rate given the strength of the adversary. They are information-theoretically secure. They operate in a distributed manner, assume no knowledge of the topology, and can be designed and implemented in polynomial time. Furthermore, only the source and destination need to be modified; nonmalicious nodes inside the network are oblivious to the presence of adversaries and implement a classical distributed network code. Finally, our algorithms work over wired and wireless networks.",
Jamming-resistant Key Establishment using Uncoordinated Frequency Hopping,"We consider the following problem: how can two devices that do not share any secrets establish a shared secret key over a wireless radio channel in the presence of a communication jammer? An inherent challenge in solving this problem is that known anti-jamming techniques (e.g., frequency hopping or direct-sequence spread spectrum) which should support device communication during the key establishment require that the devices share a secret spreading key (or code) prior to the start of their communication. This requirement creates a circular dependency between antijamming spread-spectrum communication and key establishment, which has so far not been addressed. In this work, we propose an uncoordinated frequency hopping (UFH) scheme that breaks this dependency and enables key establishment in the presence of a communication jammer. We perform a detailed analysis of our UFH scheme and show its feasibility, both in terms of execution time and resource requirements.",
Online Electromyographic Control of a Robotic Prosthesis,"This paper presents a two-part study investigating the use of forearm surface electromyographic (EMG) signals for real-time control of a robotic arm. In the first part of the study, we explore and extend current classification-based paradigms for myoelectric control to obtain high accuracy (92-98%) on an eight-class offline classification problem, with up to 16 classifications/s. This offline study suggested that a high degree of control could be achieved with very little training time (under 10 min). The second part of this paper describes the design of an online control system for a robotic arm with 4 degrees of freedom. We evaluated the performance of the EMG-based real-time control system by comparing it with a keyboard-control baseline in a three-subject study for a variety of complex tasks.",
Interference Alignment and Spatial Degrees of Freedom for the K User Interference Channel,We show that the sum capacity of the K user frequency selective (or time-varying) interference channel is C(SNR) = (K/2) log(SNR) +o(log(SNR)) meaning that the channel has a total of K/2 degrees of freedom per orthogonal time and frequency dimension. Linear schemes of interference alignment and zero forcing suffice to achieve all the degrees of freedom and multi-user detection is not required.,"Interference channels,
Frequency,
Wireless networks,
Signal to noise ratio,
MIMO,
Communications Society,
Transmitters,
Computer science,
USA Councils,
Multiuser detection"
Evolutionary many-objective optimization,"In this paper, we first explain why many-objective problems are difficult for Pareto dominance-based evolutionary multiobjective optimization algorithms such as NSGA-II and SPEA. Then we explain recent proposals for the handling of many-objective problems by evolutionary algorithms. Some proposals are examined through computational experiments on multiobjective knapsack problems with two, four and six objectives. Finally we discuss the viability of many-objective genetic fuzzy systems (i.e., the use of many-objective genetic algorithms for the design of fuzzy rule-based systems).","Fuzzy systems,
Knowledge based systems,
Proposals,
Genetic algorithms,
Fuzzy sets,
Pareto optimization,
Algorithm design and analysis,
Evolutionary computation,
Stress,
Humans"
Agreeing Asynchronously,"This paper formulates and solves a version of the widely studied Vicsek consensus problem in which each member of a group of n > 1 agents independently updates its heading at times determined by its own clock. It is not assumed that the agents' clocks are synchronized or that the ldquoeventrdquo times between which any one agent updates its heading are evenly spaced. Nor is it assumed that heading updates must occur instantaneously. Using the concept of ldquoanalytic synchronizationrdquo together with several key results concerned with properties of ldquocompositionsrdquo of directed graphs, it is shown that the conditions under which a consensus is achieved are essentially the same as those applicable in the synchronous case provided the notion of an agent's neighbor between its event times is appropriately defined. However, in sharp contrast with the synchronous case where for analysis an n dimensional state space model is adequate, for the asynchronous version of the problem a 2n-dimensional state space model is required. It is explained how to analyze this model despite the fact that, unlike the synchronous case, the stochastic matrices involved do not have all positive diagonal entries.","Australia,
Clocks,
Synchronization,
State-space methods,
Nearest neighbor searches,
Computer science,
Stochastic processes,
Control systems,
Multiagent systems,
Switched systems"
Compensation of Scanner Creep and Hysteresis for AFM Nanomanipulation,"Nanomanipulation with atomic force microscopes (AFMs) for nanoparticles with overall sizes on the order of 10 nm has been hampered in the past by the large spatial uncertainties encountered in tip positioning. This paper addresses the compensation of nonlinear effects of creep and hysteresis on the piezo scanners which drive most AFMs. Creep and hysteresis are modeled as the superposition of fundamental operators, and their inverse model is obtained by using the inversion properties of the Prandtl-Ishlinskii operator. Identification of the parameters in the forward model is achieved by a novel method that uses the topography of the sample and does not require position sensors. The identified parameters are used to compute the inverse model, which in turn serves to drive the AFM in an open-loop, feedforward scheme. Experimental results show that this approach effectively reduces the spatial uncertainties associated with creep and hysteresis, and supports automated, computer-controlled manipulation operations that otherwise would fail.","Creep,
Hysteresis,
Atomic force microscopy,
Uncertainty,
Laboratories,
Inverse problems,
Robot sensing systems,
Robotics and automation,
Prototypes,
Sensor arrays"
Scientific Cloud Computing: Early Definition and Experience,"Cloud computing emerges as a new computing paradigm which aims to provide reliable, customized and QoS guaranteed computing dynamic environments for end-users. This paper reviews recent advances of Cloud computing, identifies the concepts and characters of scientific Clouds, and finally presents an example of scientific Cloud for data centers","Meteorology,
Cloud computing,
Software,
Hardware,
Web services,
File systems,
Internet"
Regional congestion awareness for load balance in networks-on-chip,"Interconnection networks-on-chip (NOCs) are rapidly replacing other forms of interconnect in chip multiprocessors and system-on-chip designs. Existing interconnection networks use either oblivious or adaptive routing algorithms to determine the route taken by a packet to its destination. Despite somewhat higher implementation complexity, adaptive routing enjoys better fault tolerance characteristics, increases network throughput, and decreases latency compared to oblivious policies when faced with non-uniform or bursty traffic. However, adaptive routing can hurt performance by disturbing any inherent global load balance through greedy local decisions. To improve load balance in adapting routing, we propose Regional Congestion Awareness (RCA), a lightweight technique to improve global network balance. Instead of relying solely on local congestion information, RCA informs the routing policy of congestion in parts of the network beyond adjacent routers. Our experiments show that RCA matches or exceeds the performance of conventional adaptive routing across all workloads examined, with a 16% average and 71% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. Compared to a baseline adaptive router, RCA incurs a negligible logic and modest wiring overhead.","Routing,
Resource management,
Adaptive systems,
Switches,
System-on-a-chip,
Pipelines,
Microarchitecture"
An improved adi-fdtd method with lower splitting error,"Based on the Crank-Nicolson (CN) scheme, an improved alternating direction implicit finite-difference time-domain (ADI-FDTD) method is proposed. By introducing local correction, the proposed method reduces the splitting error while the unconditional stability and computational efficiency are maintained. Theoretical analyses are given.","Finite difference methods,
Stability analysis,
Dispersion,
Accuracy,
Artificial neural networks,
Time domain analysis,
Electromagnetics"
Learning object motion patterns for anomaly detection and improved object detection,"We present a novel framework for learning patterns of motion and sizes of objects in static camera surveillance. The proposed method provides a new higher-level layer to the traditional surveillance pipeline for anomalous event detection and scene model feedback. Pixel level probability density functions (pdfs) of appearance have been used for background modelling in the past, but modelling pixel level pdfs of object speed and size from the tracks is novel. Each pdf is modelled as a multivariate Gaussian Mixture Model (GMM) of the motion (destination location & transition time) and the size (width & height) parameters of the objects at that location. Output of the tracking module is used to perform unsupervised EM-based learning of every GMM. We have successfully used the proposed scene model to detect local as well as global anomalies in object tracks. We also show the use of this scene model to improve object detection through pixel-level parameter feedback of the minimum object size and background learning rate. Most object path modelling approaches first cluster the tracks into major paths in the scene, which can be a source of error. We avoid this by building local pdfs that capture a variety of tracks which are passing through them. Qualitative and quantitative analysis of actual surveillance videos proved the effectiveness of the proposed approach.","Object detection,
Motion detection,
Layout,
Surveillance,
Feedback,
Cameras,
Pipelines,
Event detection,
Probability density function,
Videos"
Intraretinal Layer Segmentation of Macular Optical Coherence Tomography Images Using Optimal 3-D Graph Search,"Current techniques for segmenting macular optical coherence tomography (OCT) images have been 2-D in nature. Furthermore, commercially available OCT systems have only focused on segmenting a single layer of the retina, even though each intraretinal layer may be affected differently by disease. We report an automated approach for segmenting (anisotropic) 3-D macular OCT scans into five layers. Each macular OCT dataset consisted of six linear radial scans centered at the fovea. The six surfaces defining the five layers were identified on each 3-D composite image by transforming the segmentation task into that of finding a minimum-cost closed set in a geometric graph constructed from edge/regional information and a priori determined surface smoothness and interaction constraints. The method was applied to the macular OCT scans of 12 patients (24 3-D composite image datasets) with unilateral anterior ischemic optic neuropathy (AION). Using the average of three experts' tracings as a reference standard resulted in an overall mean unsigned border positioning error of 6.1 plusmn 2.9 mum, a result comparable to the interobserver variability (6.9 plusmn 3.3 mum).Our quantitative analysis of the automated segmentation results from AION subject data revealed that the inner retinal layer thickness for the affected eye was 24.1 mum (21%) smaller on average than for the unaffected eye (p < 0.001), supporting the need for segmenting the layers separately.","Image segmentation,
Tomography,
Cities and towns,
Biomedical optical imaging,
Retina,
Biomedical imaging,
Geometrical optics,
Biomedical engineering,
Medical diagnostic imaging,
Oncology"
Pose primitive based human action recognition in videos or still images,"This paper presents a method for recognizing human actions based on pose primitives. In learning mode, the parameters representing poses and activities are estimated from videos. In run mode, the method can be used both for videos or still images. For recognizing pose primitives, we extend a Histogram of Oriented Gradient (HOG) based descriptor to better cope with articulated poses and cluttered background. Action classes are represented by histograms of poses primitives. For sequences, we incorporate the local temporal context by means of n-gram expressions. Action recognition is based on a simple histogram comparison. Unlike the mainstream video surveillance approaches, the proposed method does not rely on background subtraction or dynamic features and thus allows for action recognition in still images.","Humans,
Image recognition,
Videos,
Histograms,
Image sequences,
Target recognition,
Legged locomotion,
Shape,
Detectors,
Principal component analysis"
"System level analysis of fast, per-core DVFS using on-chip switching regulators","Portable, embedded systems place ever-increasing demands on high-performance, low-power microprocessor design. Dynamic voltage and frequency scaling (DVFS) is a well-known technique to reduce energy in digital systems, but the effectiveness of DVFS is hampered by slow voltage transitions that occur on the order of tens of microseconds. In addition, the recent trend towards chip-multiprocessors (CMP) executing multi-threaded workloads with heterogeneous behavior motivates the need for per-core DVFS control mechanisms. Voltage regulators that are integrated onto the same chip as the microprocessor core provide the benefit of both nanosecond-scale voltage switching and per-core voltage control. We show that these characteristics provide significant energy-saving opportunities compared to traditional off-chip regulators. However, the implementation of on-chip regulators presents many challenges including regulator efficiency and output voltage transient characteristics, which are significantly impacted by the system-level application of the regulator. In this paper, we describe and model these costs, and perform a comprehensive analysis of a CMP system with on-chip integrated regulators. We conclude that on-chip regulators can significantly improve DVFS effectiveness and lead to overall system energy savings in a CMP, but architects must carefully account for overheads and costs when designing next-generation DVFS systems and algorithms.","Regulators,
System-on-a-chip,
Voltage control,
Program processors,
Load modeling,
Switches,
Inductors"
Reliability Gain of Network Coding in Lossy Wireless Networks,"The capacity gain of network coding has been extensively studied in wired and wireless networks. Recently, it has been shown that network coding improves network reliability by reducing the number of packet retransmissions in lossy networks. However, the extent of the reliability benefit of network coding is not known. This paper quantifies the reliability gain of network coding for reliable multicasting in wireless networks, where network coding is most promising. We define the expected number of transmissions per packet as the performance metric for reliability and derive analytical expressions characterizing the performance of network coding. We also analyze the performance of reliability mechanisms based on rateless codes and automatic repeat request (ARQ), and compare them with network coding. We first study network coding performance in an access point model, where an access point broadcasts packets to a group of K receivers over lossy wireless channels. We show that the expected number of transmissions using ARQ, compared to network coding, scales as ominus (log K) as the number of receivers becomes large. We then use the access point model as a building block to study reliable multicast in a tree topology. In addition to scaling results, we derive expressions for the expected number of transmissions for finite multicast groups as well. Our results show that network coding significantly reduces the number of retransmissions in lossy networks compared to an ARQ scheme. However, rateless coding achieves asymptotic performance results similar to that of network coding.","Network coding,
Wireless networks,
Automatic repeat request,
Computer network reliability,
Performance analysis,
Network topology,
Telecommunication network reliability,
Computer science,
Broadcasting,
Error correction"
Recursively separated and weighted histogram equalization for brightness preservation and contrast enhancement,"This paper proposes a new histogram equalization method, called RSWHE (recursively separated and weighted histogram equalization), for brightness preservation and image contrast enhancement. The essential idea of RSWHE is to segment an input histogram into two or more sub-histograms recursively, to modify the sub-histograms by means of a weighting process based on a normalized power law function, and to perform histogram equalization on the weighted sub-histograms independently. RSIHE (recursive sub-image histogram equalization) and RMSHE (recursive mean separate histogram equalization) are some methods similar to RSWHE, but they do not carry out the above weighting process. We show that compared to other existent methods, RSWHE preserves the image brightness more accurately and produces images with better contrast enhancement.","Histograms,
Brightness,
Image segmentation,
Consumer electronics,
Computer science,
Dynamic range,
Distribution functions,
Computer errors,
Image converters"
Facelift: Hiding and slowing down aging in multicores,"Processors progressively age during their service life due to normal workload activity. Such aging results in gradually slower circuits. Anticipating this fact, designers add timing guardbands to processors, so that processors last for a number of years. As a result, aging has important design and cost implications. To address this problem, this paper shows how to hide the effects of aging and how to slow it down. Our framework is called Facelift. It hides aging through aging-driven application scheduling. It slows down aging by applying voltage changes at key times — it uses a non-linear optimization algorithm to carefully balance the impact of voltage changes on the aging rate and on the critical path delays. Moreover, Facelift can gainfully configure the chip for a short service life. Simulation results indicate that Facelift leads to more cost-effective multicores. We can take a multicore designed for a 7-year service life and, by hiding and slowing down aging, enable it to run, on average, at a 14–15% higher frequency during its whole service life. Alternatively, we can design the multicore for a 5 to 7-month service life and still use it for 7 years.",
Accurate Event-Driven Motion Compensation in High-Resolution PET Incorporating Scattered and Random Events,"With continuing improvements in spatial resolution of positron emission tomography (PET) scanners, small patient movements during PET imaging become a significant source of resolution degradation. This work develops and investigates a comprehensive formalism for accurate motion-compensated reconstruction which at the same time is very feasible in the context of high-resolution PET. In particular, this paper proposes an effective method to incorporate presence of scattered and random coincidences in the context of motion (which is similarly applicable to various other motion correction schemes). The overall reconstruction framework takes into consideration missing projection data which are not detected due to motion, and additionally, incorporates information from all detected events, including those which fall outside the field-of-view following motion correction. The proposed approach has been extensively validated using phantom experiments as well as realistic simulations of a new mathematical brain phantom developed in this work, and the results for a dynamic patient study are also presented.",
Intelligent Scheduling of Hybrid and Electric Vehicle Storage Capacity in a Parking Lot for Profit Maximization in Grid Power Transactions,"This paper proposes an intelligent method for scheduling usage of available energy storage capacity from plug-in hybrid electric vehicles (PHEV) and electric vehicles (EV). The batteries on these vehicles can either provide power to the grid when parked, known as vehicle-to-grid (V2G) concept or take power from the grid to charge the batteries on the vehicles. A scalable parking lot model is developed with different parameters assigned to fleets of vehicles. The size of the parking lot is assumed to be large enough to accommodate the number of vehicles performing grid transactions. In order to figure out the appropriate charge and discharge times throughout the day, binary particle swarm optimization is applied. Price curves from the California ISO database are used in this study to have realistic price fluctuations. Finding optimal solutions that maximize profits to vehicle owners while satisfying system and vehicle owners constraints is the objective of this study. Different fleets of vehicles are used to approximate varying customer base and demonstrate the scalability of parking lots for V2G. The results are compared for consistency and scalability. Discussions on how this technique can be applied to other grid issues such as peaking power are included at the end.","Intelligent vehicles,
Hybrid electric vehicles,
Battery powered vehicles,
Particle swarm optimization,
Iterative algorithms,
USA Councils,
Processor scheduling,
Energy storage,
Scalability,
Wind energy"
Broadcasting with Side Information,"A sender holds a word x
consisting of n
blocks x_i
, each of t
bits, and wishes to broadcast a codeword to m
receivers, R_1,...,R_m
. Each receiver R_i
is interested in one block, and has prior side information consisting of some subset of the other blocks. Let \beta_t
be the minimum number of bits that has to be transmitted when each block is of length t
, and let \beta
be the limit \beta = \lim_{t \rightarrow \infty} \beta_t/t
. Informally, \beta
is the average communication cost per bit in each block (for long blocks). Finding the coding rate \beta
, for such an informed broadcast setting, generalizes several coding theoretic parameters related to Informed Source Coding on Demand, Index Coding and Network Coding.In this work we show that usage of large data blocks may strictly improve upon the trivial encoding which treats each bit in the block independently. To this end, we provide general bounds on \beta_t
, and prove that for any constant C
there is an explicit broadcast setting in which \beta = 2
but \beta_1 ≫ C
. One of these examples answers a question of \cite{LS}.In addition, we provide examples with the following counterintuitive direct-sum phenomena. Consider a union of several mutually independent broadcast settings. The optimal code for the combined setting may yield a significant saving in communication over concatenating optimal encodings for the individual settings. This result also provides new non-linear coding schemes which improve upon the largest known gap between linear and non-linear Network Coding, thus improving the results of \cite{DFZ}.The proofs are based on a relation between this problem and results in the study of Witsenhausen's rate, OR graph products, colorings of Cayley graphs, and the chromatic numbers of Kneser graphs.","Source coding,
Computer science,
USA Councils,
Network coding,
Satellite broadcasting,
Mathematics,
Costs,
Binary codes,
Video on demand,
Multimedia communication"
Segmentation of the Left Ventricle of the Heart in 3-D+t MRI Data Using an Optimized Nonrigid Temporal Model,"Modern medical imaging modalities provide large amounts of information in both the spatial and temporal domains and the incorporation of this information in a coherent algorithmic framework is a significant challenge. In this paper, we present a novel and intuitive approach to combine 3-D spatial and temporal (3-D + time) magnetic resonance imaging (MRI) data in an integrated segmentation algorithm to extract the myocardium of the left ventricle. A novel level-set segmentation process is developed that simultaneously delineates and tracks the boundaries of the left ventricle muscle. By encoding prior knowledge about cardiac temporal evolution in a parametric framework, an expectation-maximization algorithm optimally tracks the myocardial deformation over the cardiac cycle. The expectation step deforms the level-set function while the maximization step updates the prior temporal model parameters to perform the segmentation in a nonrigid sense.","Heart,
Magnetic resonance imaging,
Image segmentation,
Myocardium,
Biomedical imaging,
Data mining,
Muscles,
Encoding,
Expectation-maximization algorithms,
Deformable models"
Tracking the Time-Varying Cortical Connectivity Patterns by Adaptive Multivariate Estimators,"The directed transfer function (DTF) and the partial directed coherence (PDC) are frequency-domain estimators that are able to describe interactions between cortical areas in terms of the concept of Granger causality. However, the classical estimation of these methods is based on the multivariate autoregressive modelling (MVAR) of time series, which requires the stationarity of the signals. In this way, transient pathways of information transfer remains hidden. The objective of this study is to test a time-varying multivariate method for the estimation of rapidly changing connectivity relationships between cortical areas of the human brain, based on DTF/PDC and on the use of adaptive MVAR modelling (AMVAR) and to apply it to a set of real high resolution EEG data. This approach will allow the observation of rapidly changing influences between the cortical areas during the execution of a task. The simulation results indicated that time-varying DTF and PDC are able to estimate correctly the imposed connectivity patterns under reasonable operative conditions of signal-to-noise ratio (SNR) ad number of trials. An SNR of Ave and a number of trials of at least 20 provide a good accuracy in the estimation. After testing the method by the simulation study, we provide an application to the cortical estimations obtained from high resolution EEG data recorded from a group of healthy subject during a combined foot-lips movement and present the time-varying connectivity patterns resulting from the application of both DTF and PDC. Two different cortical networks were detected with the proposed methods, one constant across the task and the other evolving during the preparation of the joint movement.","Electroencephalography,
Brain modeling,
Frequency estimation,
Transfer functions,
Testing,
Signal resolution,
Magnetic resonance imaging,
Image resolution,
Coherence,
Humans"
Cardinality Estimation for Large-scale RFID Systems,"Counting or estimating the number of tags is crucial for large-scale RFID systems. The use of multiple readers was recently proposed to improve the efficiency and effectiveness in reading RFID tags. Due to the long processing time, tag identification based counting schemes are often impractical, especially when tags are attached to moving objects. The existing estimation based schemes, on the other hand, suffer from the multiple-reading problem. To address this issue, we propose the Lottery Frame (LoF) scheme, a replicate-insensitive estimation protocol, that is able to eliminate multiple-readings. We show the high accuracy, short processing time and low overhead of the proposed LoF scheme through analysis and simulations.","Large-scale systems,
Radiofrequency identification,
RFID tags,
Protocols,
Delay,
Pervasive computing,
Computer science,
Analytical models,
Application software,
Monitoring"
An embedded artificial skin for humanoid robots,"A novel artificial skin for covering the whole body of a humanoid robot is presented. It provides pressure measurements and shape information about the contact surfaces between the robot and the environment. The system is based on a mesh of sensors interconnected in order to form a networked structure. Each sensor has 12 capacitive taxels, has a triangular shape and is supported by a flexible substrate in order to conform to smooth curved surfaces. Three communications ports placed along the sides of each sensor sides allow communications with adjacent sensors. The tactile measurements are sent to embed microcontroller boards using serial bus communication links. The system can adaptively reduce its spatial resolution, improving the response time. This feature is very useful for detecting the first contact very rapidly, at a lower spatial resolution, and then increase the spatial resolution in the region of contact for accurate reconstruction of the contact pressure distribution.",
Brain Anatomical Structure Segmentation by Hybrid Discriminative/Generative Models,"In this paper, a hybrid discriminative/generative model for brain anatomical structure segmentation is proposed. The learning aspect of the approach is emphasized. In the discriminative appearance models, various cues such as intensity and curvatures are combined to locally capture the complex appearances of different anatomical structures. A probabilistic boosting tree (PBT) framework is adopted to learn multiclass discriminative models that combine hundreds of features across different scales. On the generative model side, both global and local shape models are used to capture the shape information about each anatomical structure. The parameters to combine the discriminative appearance and generative shape models are also automatically learned. Thus, low-level and high-level information is learned and integrated in a hybrid model. Segmentations are obtained by minimizing an energy function associated with the proposed hybrid model. Finally, a grid-face structure is designed to explicitly represent the 3-D region topology. This representation handles an arbitrary number of regions and facilitates fast surface evolution. Our system was trained and tested on a set of 3-D magnetic resonance imaging (MRI) volumes and the results obtained are encouraging.","Anatomical structure,
Hybrid power systems,
Brain modeling,
Shape,
Magnetic resonance imaging,
Image segmentation,
Protocols,
Biomedical imaging,
Boosting,
Neuroimaging"
Robust Radiometric Calibration and Vignetting Correction,"In many computer vision systems, it is assumed that the image brightness of a point directly reflects the scene radiance of the point. However, the assumption does not hold in most cases due to nonlinear camera response function, exposure changes, and vignetting. The effects of these factors are most visible in image mosaics and textures of 3D models where colors look inconsistent and notable boundaries exist. In this paper, we propose a full radiometric calibration algorithm that includes robust estimation of the radiometric response function, exposures, and vignetting. By decoupling the effect of vignetting from the response function estimation, we approach each process in a manner that is robust to noise and outliers. We verify our algorithm with both synthetic and real data, which shows significant improvement compared to existing methods. We apply our estimation results to radiometrically align images for seamless mosaics and 3D model textures. We also use our method to create high dynamic range (HDR) mosaics that are more representative of the scene than normal mosaics.","Robustness,
Radiometry,
Calibration,
Brightness,
Layout,
Lenses,
Dynamic range,
Shape measurement,
Apertures,
Computer vision"
Real-Time Communication Analysis for On-Chip Networks with Wormhole Switching,"In this paper, we discuss a real-time on-chip communication service with a priority-based wormhole switching policy. A novel off-line schedulability analysis approach is presented. By evaluating diverse inter-relationships among the traffic-flows, this approach can predict the packet network latency based on two quantifiable different delays: direct interference from higher priority traffic-flows and indirect interference from other higher priority traffic-flows. Due to the inevitable existence of parallel interference, we prove that the general problem of determining the exact schedulability of real-time traffic-flow over the onchip network is NP-hard. However the results presented do form an upper bound. In addition, an error in a previous published scheduling approach is illustrated and remedied. Utilizing this analysis scheme, we can flexibly evaluate at design time the schedulability of a set of traffic-flows with different QoS requirements on a real-time SoC/NoC communication platform.",
Data mining techniques for a functional verification of SoC,"A typical System-on-Chip integrates many blocks including peripheral IPs, buses, complex interconnects, multiple processors, memory, clock and power distribution, test structures, and buses. There are always several master blocks on the bus, resulting in complex bus architectures. That is why new system level approach in design and verification has been recently suggested in EDA industry. Despite the rising the abstraction level it is still hard to manage and analyze simulation data. This data can be archived for later use or it can be mined to look for different kind of violations or to get statistical information about the specified design. The paper describes the powerful data mining techniques within transaction level modeling for a functional verification of System-on-a-Chip models. As the result of applying new data mining techniques it is possible to gain engineer's understanding level of model and provide transactional data analysis. Moreover, the frequency patterns can be mined from simulation data to provide the transactional debugging feature.",
Trajectory Outlier Detection: A Partition-and-Detect Framework,"Outlier detection has been a popular data mining task. However, there is a lack of serious study on outlier detection for trajectory data. Even worse, an existing trajectory outlier detection algorithm has limited capability to detect outlying sub-trajectories. In this paper, we propose a novel partition-and-detect framework for trajectory outlier detection, which partitions a trajectory into a set of line segments, and then, detects outlying line segments for trajectory outliers. The primary advantage of this framework is to detect outlying sub-trajectories from a trajectory database. Based on this partition-and-detect framework, we develop a trajectory outlier detection algorithm TRAOD. Our algorithm consists of two phases: partitioning and detection. For the first phase, we propose a two-level trajectory partitioning strategy that ensures both high quality and high efficiency. For the second phase, we present a hybrid of the distance-based and density-based approaches. Experimental results demonstrate that TRAOD correctly detects outlying sub-trajectories from real trajectory data.",
MRI-Based Automated Computer Classification of Probable AD Versus Normal Controls,"Automated computer classification (ACC) techniques are needed to facilitate physician's diagnosis of complex diseases in individual patients. We provide an example of ACC using computational techniques within the context of cross-sectional analysis of magnetic resonance images (MRI) in neurodegenerative diseases, namely Alzheimer's dementia (AD). In this paper, the accuracy of our ACC methodology is assessed when presented with real life, imperfect data, i.e., cohorts of MRI with varying acquisition parameters and imaging quality. The comparative methodology uses the Jacobian determinants derived from dense deformation fields and scaled grey-level intensity from a selected volume of interest centered on the medial temporal lobe. The ACC performance is assessed in a series of leave-one-out experiments aimed at separating 75 probable AD and 75 age-matched normal controls. The resulting accuracy is 92% using a support vector machine classifier based on least squares optimization. Finally, it is shown in the Appendix that determinants and scaled grey-level intensity are appreciably more robust to varying parameters in validation studies using simulated data, when compared to raw intensities or grey/white matter volumes. The ability of cross-sectional MRI at detecting probable AD with high accuracy could have profound implications in the management of suspected AD candidates.",
"Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories","We examine the problem of segmenting tracked feature point trajectories of multiple moving objects in an image sequence. Using the affine camera model, this motion segmentation problem can be cast as the problem of segmenting samples drawn from a union of linear subspaces. Due to limitations of the tracker, occlusions and the presence of nonrigid objects in the scene, the obtained motion trajectories may contain grossly mistracked features, missing entries, or not correspond to any valid motion model. In this paper, we develop a robust subspace separation scheme that can deal with all of these practical issues in a unified framework. Our methods draw strong connections between lossy compression, rank minimization, and sparse representation. We test our methods extensively and compare their performance to several extant methods with experiments on the Hopkins 155 database. Our results are on par with state-of-the-art results, and in many cases exceed them. All MATLAB code and segmentation results are publicly available for peer evaluation at http://perception.csl.uiuc.edu/coding/motion/.","Motion segmentation,
Computer vision,
Robustness,
Image segmentation,
Trajectory,
Mathematical model,
Image sequences,
Cameras,
Tracking,
Layout"
Six-DoF Haptic Rendering of Contact Between Geometrically Complex Reduced Deformable Models,"Real-time evaluation of distributed contact forces between rigid or deformable 3D objects is a key ingredient of 6-DoF force-feedback rendering. Unfortunately, at very high temporal rates, there is often insufficient time to resolve contact between geometrically complex objects. We propose a spatially and temporally adaptive approach to approximate distributed contact forces under hard real-time constraints. Our method is CPU based, and supports contact between rigid or reduced deformable models with complex geometry. We propose a contact model that uses a point-based representation for one object, and a signed-distance field for the other. This model is related to the voxmap pointshell method (VPS), but gives continuous contact forces and torques, enabling stable rendering of stiff penalty-based distributed contacts. We demonstrate that stable haptic interactions can be achieved by point-sampling offset surfaces to input ""polygon soup'' geometry using particle repulsion. We introduce a multi-resolution nested pointshell construction which permits level-of-detail contact force computation, and enables contact graceful degradation in close-proximity scenarios. Parametrically deformed distance fields are proposed to support contact between reduced deformable objects. We present several examples of 6-DoF haptic rendering of geometrically complex rigid and deformable objects in distributed contact at real-time kilohertz rates.","Haptic interfaces,
Deformable models,
Solid modeling,
Geometry,
Computational modeling,
Spatial resolution,
Computer science,
Airplanes,
Degradation,
Time factors"
GerAmi: Improving Healthcare Delivery in Geriatric Residences,"Many countries face an ever-growing need to supply constant care and support for their disabled and elderly populations. In this paper, we've developed geriatric ambient intelligence, an intelligent environment that integrates multiagent systems, mobile devices, RFID, and Wi-Fi technologies to facilitate management and control of geriatric residences. At GerAmi's core is the geriatric agent (GerAg), a deliberative agent that incorporates a case-based planning (CBP) mechanism to optimize work schedules and provide up-to-date patient and facility data. We've successfully implemented a system prototype at a care facility for Alzheimer patients.",
Toward Social Learning Environments,"We are teaching a new generation of students, cradled in technologies, communication and abundance of information. The implications are that we need to focus the design of learning technologies to support social learning in context. Instead of designing technologies that ldquoteachrdquo the learner, the new social learning technologies will perform three main roles: 1) support the learner in finding the right content (right for the context, for the particular learner, for the specific purpose of the learner, right pedagogically); 2) support learners to connect with the right people (again right for the context, learner, purpose, educational goal etc.), and 3) motivate/incentivize people to learn. In the pursuit of such environments, new areas of sciences become relevant as a source of methods and techniques: social psychology, economic/game theory, multi-agent systems. The paper illustrates how social learning technologies can be designed using some existing and emerging technologies: ontologies vs. social tagging, exploratory search, collaborative vs. self-managed social recommendations, trust and reputation mechanisms, mechanism design and social visualization.",
Intelligent heart disease prediction system using data mining techniques,"The healthcare industry collects huge amounts of healthcare data which, unfortunately, are not “mined” to discover hidden information for effective decision making. Discovery of hidden patterns and relationships often goes unexploited. Advanced data mining techniques can help remedy this situation. This research has developed a prototype Intelligent Heart Disease Prediction System (IHDPS) using data mining techniques, namely, Decision Trees, Naïve Bayes and Neural Network. Results show that each technique has its unique strength in realizing the objectives of the defined mining goals. IHDPS can answer complex “what if” queries which traditional decision support systems cannot. Using medical profiles such as age, sex, blood pressure and blood sugar it can predict the likelihood of patients getting a heart disease. It enables significant knowledge, e.g. patterns, relationships between medical factors related to heart disease, to be established. IHDPS is Web-based, user-friendly, scalable, reliable and expandable. It is implemented on the .NET platform.",
RevLib: An Online Resource for Reversible Functions and Reversible Circuits,"Synthesis of reversible logic has become an active research area in the last years. But many proposed algorithms are evaluated with a small set of benchmarks only. Furthermore, results are often documented only in terms of gate counts or quantum costs, rather than presenting the specific circuit. In this paper RevLib (www.revlib.org) is introduced, an online resource for reversible functions and reversible circuits. RevLib provides a large database of functions with respective circuit realizations. RevLib is designed to ease the evaluation of new methods and facilitate the comparison of results. In addition, tools are introduced to support researchers in evaluating their algorithms and documenting their results.","Circuit synthesis,
Databases,
Costs,
Computer science,
Quantum computing,
Libraries,
Multivalued logic,
Niobium,
Logic circuits,
Digital circuits"
High-Speed Nonlinear Finite Element Analysis for Surgical Simulation Using Graphics Processing Units,"The use of biomechanical modelling, especially in conjunction with finite element analysis, has become common in many areas of medical image analysis and surgical simulation. Clinical employment of such techniques is hindered by conflicting requirements for high fidelity in the modelling approach, and fast solution speeds. We report the development of techniques for high-speed nonlinear finite element analysis for surgical simulation. We use a fully nonlinear total Lagrangian explicit finite element formulation which offers significant computational advantages for soft tissue simulation. However, the key contribution of the work is the presentation of a fast graphics processing unit (GPU) solution scheme for the finite element equations. To the best of our knowledge, this represents the first GPU implementation of a nonlinear finite element solver. We show that the present explicit finite element scheme is well suited to solution via highly parallel graphics hardware, and that even a midrange GPU allows significant solution speed gains (up to 16.8 times) compared with equivalent CPU implementations. For the models tested the scheme allows real-time solution of models with up to 16 000 tetrahedral elements. The use of GPUs for such purposes offers a cost-effective high-performance alternative to expensive multi-CPU machines, and may have important applications in medical image analysis and surgical simulation.","Finite element methods,
Surgery,
Analytical models,
Graphics,
Medical simulation,
Image analysis,
Biomedical imaging,
Computational modeling,
Employment,
Lagrangian functions"
Energy Efficient Scheduling of Real-Time Tasks on Multicore Processors,"Multicore processors deliver a higher throughput at lower power consumption than unicore processors. In the near future, they will thus be widely used in mobile real-time systems. There have been many research on energy-efficient scheduling of real-time tasks using DVS. These approaches must be modified for multicore processors, however, since normally all the cores in a chip must run at the same performance level. Thus, blindly adopting existing DVS algorithms that do not consider the restriction will result in a waste of energy. This article suggests Dynamic Repartitioning algorithm based on existing partitioning approaches of multiprocessor systems. The algorithm dynamically balances the task loads of multiple cores to optimize power consumption during execution. We also suggest Dynamic Core Scaling algorithm, which adjusts the number of active cores to reduce leakage power consumption under low load conditions. Simulation results show that Dynamic Repartitioning can produce energy savings of about 8 percent even with the best energy-efficient partitioning algorithm. The results also show that Dynamic Core Scaling can reduce energy consumption by about 26 percent under low load conditions.",
SpiNNaker: Mapping neural networks onto a massively-parallel chip multiprocessor,"SpiNNaker is a novel chip - based on the ARM processor - which is designed to support large scale spiking neural networks simulations. In this paper we describe some of the features that permit SpiNNaker chips to be connected together to form scalable massively-parallel systems. Our eventual goal is to be able to simulate neural networks consisting of 109 neurons running in ‘real time’, by which we mean that a similarly sized collection of biological neurons would run at the same speed. In this paper we describe the methods by which neural networks are mapped onto the system, and how features designed into the chip are to be exploited in practice. We will also describe the modelling and verification activities by which we hope to ensure that, when the chip is delivered, it will work as anticipated.","Neurons,
Routing,
Computational modeling,
Magnetic cores,
Biological system modeling,
Delay,
Hardware"
Wireless Capsule Endoscopy Color Video Segmentation,"This paper describes the use of color image analysis to automatically discriminate between oesophagus, stomach, small intestine, and colon tissue in wireless capsule endoscopy (WCE). WCE uses ldquopill-camrdquo technology to recover color video imagery from the entire gastrointestinal tract. Accurately reviewing and reporting this data is a vital part of the examination, but it is tedious and time consuming. Automatic image analysis tools play an important role in supporting the clinician and speeding up this process. Our approach first divides the WCE image into subimages and rejects all subimages in which tissue is not clearly visible. We then create a feature vector combining color, texture, and motion information of the entire image and valid subimages. Color features are derived from hue saturation histograms, compressed using a hybrid transform, incorporating the discrete cosine transform and principal component analysis. A second feature combining color and texture information is derived using local binary patterns. The video is segmented into meaningful parts using support vector or multivariate Gaussian classifiers built within the framework of a hidden Markov model. We present experimental results that demonstrate the effectiveness of this method.","Endoscopes,
Image color analysis,
Image segmentation,
Image analysis,
Stomach,
Intestines,
Colon,
Gastrointestinal tract,
Image motion analysis,
Image texture analysis"
A Novel Vessel Segmentation Algorithm for Pathological Retina Images Based on the Divergence of Vector Fields,"In this paper, a method is proposed for detecting blood vessels in pathological retina images. In the proposed method, blood vessel-like objects are extracted using the Laplacian operator and noisy objects are pruned according to the centerlines, which are detected using the normalized gradient vector field. The method has been tested with all the pathological retina images in the publicly available STARE database. Experiment results show that the method can avoid detecting false vessels in pathological regions and can produce reliable results for healthy regions.",
Unsupervised Pattern Discovery in Speech,"We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.","Sequences,
Glass,
Speech processing,
Speech recognition,
Streaming media,
Pattern matching,
Automatic speech recognition,
Computer science,
Artificial intelligence,
Laboratories"
Wireless Medical Sensor Networks in Emergency Response: Implementation and Pilot Results,"This project demonstrates the feasibility of using cost-effective, flexible, and scalable sensor networks to address critical bottlenecks of the emergency response process. For years, emergency medical service providers conducted patient care by manually measuring vital signs, documenting assessments on paper, and communicating over handheld radios. When disasters occurred, the large numbers of casualties quickly and easily overwhelmed the responders. Collaboration with EMS and hospitals in the Baltimore Washington Metropolitan region prompted us to develop miTag (medical information tag), a cost-effective wireless sensor platform that automatically track patients throughout each step of the disaster response process, from disaster scenes, to ambulances, to hospitals. The miTag is a highly extensible platform that supports a variety of sensor add-ons - GPS, pulse oximetry, blood pressure, temperature, ECG - and relays data over a self-organizing wireless mesh network. Scalability is the distinguishing characteristic of miTag: its wireless network scales across a wide range of network densities, from sparse hospital network deployments to very densely populated mass casualty sites. The miTag system is out-of-the-box operational and includes the following key technologies: 1) cost-effective sensor hardware, 2) self-organizing wireless network and 3) scalable server software that analyzes sensor data and delivers real-time updates to handheld devices and web portals. The system has evolved through multiple iterations of development and pilot deployments to become an effective patient monitoring solution. A pilot conducted with the Department of Homeland Security indicates miTags can increase the patient care capacity of responders in the field.","Wireless sensor networks,
Hospitals,
Wireless mesh networks,
Medical services,
Sensor phenomena and characterization,
Temperature sensors,
Sensor systems,
Collaboration,
Layout,
Global Positioning System"
Low-Complexity Block Turbo Equalization for OFDM Systems in Time-Varying Channels,"We propose low-complexity block turbo equalizers for orthogonal frequency-division multiplexing (OFDM) systems in time-varying channels. The presented work is based on a soft minimum mean-squared error (MMSE) block linear equalizer (BLE) that exploits the banded structure of the frequency-domain channel matrix, as well as a receiver window that enforces this banded structure. This equalization approach allows us to implement the proposed designs with a complexity that is only linear in the number of subcarriers. Three block turbo equalizers are discussed: two are based on a biased MMSE criterion, while the third is based on the unbiased MMSE criterion. Simulation results show that the proposed iterative MMSE BLE achieves a better bit error rate (BER) performance than a previously proposed iterative MMSE serial linear equalizer (SLE). The proposed equalization algorithms are also tested in the presence of channel estimation errors.","OFDM,
Time-varying channels,
Equalizers,
Frequency division multiplexing,
Intersymbol interference,
Bit error rate,
Digital video broadcasting,
Degradation,
Mathematics,
Computer science"
Phonovibrography: Mapping High-Speed Movies of Vocal Fold Vibrations Into 2-D Diagrams for Visualizing and Analyzing the Underlying Laryngeal Dynamics,"Endoscopic high-speed laryngoscopy in combination with image analysis strategies is the most promising approach to investigate the interrelation between vocal fold vibrations and voice disorders. So far, due to the lack of an objective and standardized analysis procedure a unique characterization of vocal fold vibrations has not been achieved yet. We present a visualization and analysis strategy which transforms the segmented edges of vibrating vocal folds into a single 2-D image, denoted Phonovibrogram (PVG). Within a PVG the individual type of vocal fold vibration becomes uniquely characterized by specific geometric patterns. The PVG geometries give an intuitive access on the type and degree of the laryngeal asymmetry and can be quantified using an image segmentation approach. The PVG analysis was applied to 14 representative recordings derived from a high-speed database comprising normal and pathological voices. We demonstrate that PVGs are capable to differentiate and quantify different types of normal and pathological vocal fold vibrations. The objective and precise quantification of the PVG geometry may have the potential to realize a novel classification of vocal fold vibrations.","Motion pictures,
Visualization,
Image segmentation,
Geometry,
Pathology,
Image edge detection,
Image analysis,
Speech analysis,
Image databases,
Spatial databases"
Detection and Measurement of Fetal Anatomies from Ultrasound Images using a Constrained Probabilistic Boosting Tree,"We propose a novel method for the automatic detection and measurement of fetal anatomical structures in ultrasound images. This problem offers a myriad of challenges, including: difficulty of modeling the appearance variations of the visual object of interest, robustness to speckle noise and signal dropout, and large search space of the detection procedure. Previous solutions typically rely on the explicit encoding of prior knowledge and formulation of the problem as a perceptual grouping task solved through clustering or variational approaches. These methods are constrained by the validity of the underlying assumptions and usually are not enough to capture the complex appearances of fetal anatomies. We propose a novel system for fast automatic detection and measurement of fetal anatomies that directly exploits a large database of expert annotated fetal anatomical structures in ultrasound images. Our method learns automatically to distinguish between the appearance of the object of interest and background by training a constrained probabilistic boosting tree classifier. This system is able to produce the automatic segmentation of several fetal anatomies using the same basic detection algorithm. We show results on fully automatic measurement of biparietal diameter (BPD), head circumference (HC), abdominal circumference (AC), femur length (FL), humerus length (HL), and crown rump length (CRL). Notice that our approach is the first in the literature to deal with the HL and CRL measurements. Extensive experiments (with clinical validation) show that our system is, on average, close to the accuracy of experts in terms of segmentation and obstetric measurements. Finally, this system runs under half second on a standard dual-core PC computer.","Ultrasonic variables measurement,
Anatomy,
Ultrasonic imaging,
Boosting,
Anatomical structure,
Noise robustness,
Speckle,
Object detection,
Encoding,
Image databases"
Contention window optimization for ieee 802.11 DCF access control,"According to the latest version of the IEEE 802.11 standard, the backoff parameters of its collision avoidance mechanism are far from optimal, especially in a heavy load or error-prone WLAN environment. This strategy has a high collision probability and channel utilization is degraded in bursty arrivals or congested scenarios. Besides, the standard backoff mechanism may treat noise corruption as packet collisions. In this paper, we identify the relationship between backoff parameters, contention level, and channel BER in order to propose a simple, but yet well-performing distributed algorithm that allows a station to dynamically adjust its contention window size based on turn-around-time measurement of channel status. In addition to theoretical analysis, simulations are conducted to evaluate its performance. The proposed scheme works very well in providing a substantial performance improvement in heavy loaded and error-prone WLAN environments.",
Restoration of DWI Data Using a Rician LMMSE Estimator,"This paper introduces and analyzes a linear minimum mean square error (LMMSE) estimator using a Rician noise model and its recursive version (RLMMSE) for the restoration of diffusion weighted images. A method to estimate the noise level based on local estimations of mean or variance is used to automatically parametrize the estimator. The restoration performance is evaluated using quality indexes and compared to alternative estimation schemes. The overall scheme is simple, robust, fast, and improves estimations. Filtering diffusion weighted magnetic resonance imaging (DW-MRI) with the proposed methodology leads to more accurate tensor estimations. Real and synthetic datasets are analyzed.","Rician channels,
Image restoration,
Recursive estimation,
Image analysis,
Mean square error methods,
Noise level,
Noise robustness,
Magnetic separation,
Filtering,
Magnetic resonance imaging"
Privacy Preservation in the Publication of Trajectories,"We study the problem of protecting privacy in the publication of location sequences. Consider a database of trajectories, corresponding to movements of people, captured by their transactions when they use credit or RFID debit cards. We show that, if such trajectories are published exactly (by only hiding the identities of persons that followed them),there is a high risk of privacy breach by adversaries who hold partial information about them (e.g., shop owners). In particular, we show that one can use partial trajectory knowledge as a quasi-identifier for the remaining locations in the sequence. We device a data suppression technique, which prevents this type of breach, while keeping the posted data as accurate as possible.",
TSVC: timed efficient and secure vehicular communications with privacy preserving,"In this paper, we propose a timed efficient and secure vehicular communication (TSVC) scheme with privacy preservation, which aims at minimizing the packet overhead in terms of signature overhead and signature verification latency without compromising the security and privacy requirements. Compared with currently existing public key based packet authentication schemes for security and privacy, the communication and computation overhead of TSVC can be significantly reduced due to the short message authentication code (MAC) tag attached in each packet for the packet authentication, by which only a fast hash operation is required to verify each packet. Simulation results demonstrate that TSVC maintains acceptable packet latency with much less packet overhead, while significantly reducing the packet loss ratio compared with that of the existing public key infrastructure (PKI) based schemes, especially when the road traffic is heavy.",
A dependency-aware task-based programming environment for multi-core architectures,"Parallel programming on SMP and multi-core architectures is hard. In this paper we present a programming model for those environments based on automatic function level parallelism that strives to be easy, flexible, portable, and performant. Its main trait is its ability to exploit task level parallelism by analyzing task dependencies at run time. We present the programming environment in the context of algorithms from several domains and pinpoint its benefits compared to other approaches. We discuss its execution model and its scheduler. Finally we analyze its performance and demonstrate that it offers reasonable performance without tuning, and that it can rival highly tuned libraries with minimal tuning effort.","Arrays,
Programming,
Runtime,
Parallel processing,
Computer architecture,
Matrix decomposition,
Runtime library"
A Neural Network Degradation Model for Computing and Updating Residual Life Distributions,The ability to accurately estimate the residual life of partially degraded components is arguably the most challenging problem in prognostic condition monitoring. This paper focuses on the development of a neural network-based degradation model that utilizes condition-based sensory signals to compute and continuously update residual life distributions of partially degraded components. Initial predicted failure times are estimated through trained neural networks using real-time sensory signals. These estimates are used to derive a prior failure time distribution for the component that is being monitored. Subsequent failure time estimates are then utilized to update the prior distributions using a Bayesian approach. The proposed methodology is tested using real world vibration-based degradation signals from rolling contact thrust bearings. The proposed methodology performed favorably when compared to other reliability-based and statistical-based benchmarks.,
Classification and evaluation of cost aggregation methods for stereo correspondence,"In the last decades several cost aggregation methods aimed at improving the robustness of stereo correspondence within local and global algorithms have been proposed. Given the recent developments and the lack of an appropriate comparison, in this paper we survey, classify and compare experimentally on a standard data set the main cost aggregation approaches proposed in literature. The experimental evaluation addresses both accuracy and computational requirements, so as to outline the best performing methods under these two criteria.","Cost function,
Computational efficiency,
Robustness,
Performance evaluation,
Computational complexity,
Computer science,
Standards development,
Pixel,
Taxonomy,
Belief propagation"
Pattern generators with sensory feedback for the control of quadruped locomotion,"Central pattern generators (CPGs) are becoming a popular model for the control of locomotion of legged robots. Biological CPGs are neural networks responsible for the generation of rhythmic movements, especially locomotion. In robotics, a systematic way of designing such CPGs as artificial neural networks or systems of coupled oscillators with sensory feedback inclusion is still missing. In this contribution, we present a way of designing CPGs with coupled oscillators in which we can independently control the ascending and descending phases of the oscillations (i.e. the swing and stance phases of the limbs). Using insights from dynamical system theory, we construct generic networks of oscillators able to generate several gaits under simple parameter changes. Then we introduce a systematic way of adding sensory feedback from touch sensors in the CPG such that the controller is strongly coupled with the mechanical system it controls. Finally we control three different simulated robots (iCub, Aibo and Ghostdog) using the same controller to show the effectiveness of the approach. Our simulations prove the importance of independent control of swing and stance duration. The strong mutual coupling between the CPG and the robot allows for more robust locomotion, even under non precise parameters and non-flat environment.","Feedback,
Robot sensing systems,
Oscillators,
Neurofeedback,
Artificial neural networks,
Control systems,
Biological system modeling,
Centralized control,
Legged locomotion,
Systematics"
Multi-label Classification Using Ensembles of Pruned Sets,"This paper presents a Pruned Sets method (PS) for multi-label classification. It is centred on the concept of treating sets of labels as single labels. This allows the classification process to inherently take into account correlations between labels. By pruning these sets, PS focuses only on the most important correlations, which reduces complexity and improves accuracy. By combining pruned sets in an ensemble scheme (EPS), new label sets can be formed to adapt to irregular or complex data. The results from experimental evaluation on a variety of multi-label datasets show that [E]PS can achieve better performance and train much faster than other multi-label methods.",
Secure and Serverless RFID Authentication and Search Protocols,"With the increased popularity of RFID applications, different authentication schemes have been proposed to provide security and privacy protection for users. Most recent RFID protocols use a central database to store the RFID tag data. The RFID reader first queries the RFID tag and returns the reply to the database. After authentication, the database returns the tag data to the reader. In this paper, we propose a more flexible authentication protocol that provides comparable protection without the need for a central database. We also suggest a protocol for secure search for RFID tags. We believe that as RFID applications become widespread, the ability to securely search for RFID tags will be increasingly useful.",
Evolving software product lines with aspects,"Software product lines (SPLs) enable modular, large-scale reuse through a software architecture addressing multiple core and varying features. To reap the benefits of SPLs, their designs need to be stable. Design stability encompasses the sustenance of the product line's modularity properties in the presence of changes to both the core and varying features. It is usually assumed that aspect-oriented programming promotes better modularity and changeability of product lines than conventional variability mechanisms, such as conditional compilation. However, there is no empirical evidence on its efficacy to prolong design stability of SPLs through realistic development scenarios. This paper reports a quantitative study that evolves two SPLs to assess various design stability facets of their aspect-oriented implementations. Our investigation focused upon a multi-perspective analysis of the evolving product lines in terms of modularity, change propagation, and feature dependency. We have identified a number of scenarios which positively or negatively affect the architecture stability of aspectual SPLs.","Stability,
Computer science,
Large-scale systems,
Software architecture,
Computer architecture,
Permission,
Encapsulation,
Product design,
Software engineering,
Computer languages"
Behavioral Patterns of Older Adults in Assisted Living,"In this paper, we examine at-home activity rhythms and present a dozen of behavioral patterns obtained from an activity monitoring pilot study of 22 residents in an assisted living setting with four case studies. Established behavioral patterns have been captured using custom software based on a statistical predictive algorithm that models circadian activity rhythms (CARs) and their deviations. The CAR was statistically estimated based on the average amount of time a resident spent in each room within their assisted living apartment, and also on the activity level given by the average n.umber of motion events per room. A validated in-home monitoring system (IMS) recorded the monitored resident's movement data and established the occupancy period and activity level for each room. Using these data, residents' circadian behaviors were extracted, deviations indicating anomalies were detected, and the latter were correlated to activity reports generated by the IMS as well as notes of the facility's professional caregivers on the monitored residents. The system could be used to detect deviations in activity patterns and to warn caregivers of such deviations, which could reflect changes in health status, thus providing caregivers with the opportunity to apply standard of care diagnostics and to intervene in a timely manner.","Rhythm,
Biomedical monitoring,
Chronobiology,
Medical diagnostic imaging,
Biosensors,
Sensor systems,
Prediction algorithms,
Software algorithms,
Predictive models,
Motion estimation"
SAMER: Spectrum Aware Mesh Routing in Cognitive Radio Networks,"Cognitive radio technology holds great promises in enabling unlicensed operation in licensed bands, to meet the increasing demand for radio spectrum. The new open spectrum operation necessitates novel routing protocols to exploit the available spectrum opportunistically. In this paper we present SAMER, a routing solution for cognitive radio mesh networks. SAMER opportunistically routes traffic across paths with higher spectrum availability and quality via a new routing metric. It balances between long-term route stability and short-term opportunistic performance. SAMER builds a runtime forwarding mesh that is updated periodically and offers a set of candidate routes to the destination. The actual forwarding path opportunistically adapts to the dynamic spectrum conditions and exploits the link with the highest spectrum availability at the time. We evaluate SAMER through simulations, and show that it effectively exploits the available network spectrum and results in higher end-to-end performance.","Routing,
Availability,
Routing protocols,
Cognitive radio,
Throughput,
Bandwidth,
Interference"
Integrating Concept Ontology and Multitask Learning to Achieve More Effective Classifier Training for Multilevel Image Annotation,"In this paper, we have developed a new scheme for achieving multilevel annotations of large-scale images automatically. To achieve more sufficient representation of various visual properties of the images, both the global visual features and the local visual features are extracted for image content representation. To tackle the problem of huge intraconcept visual diversity, multiple types of kernels are integrated to characterize the diverse visual similarity relationships between the images more precisely, and a multiple kernel learning algorithm is developed for SVM image classifier training. To address the problem of huge interconcept visual similarity, a novel multitask learning algorithm is developed to learn the correlated classifiers for the sibling image concepts under the same parent concept and enhance their discrimination and adaptation power significantly. To tackle the problem of huge intraconcept visual diversity for the image concepts at the higher levels of the concept ontology, a novel hierarchical boosting algorithm is developed to learn their ensemble classifiers hierarchically. In order to assist users on selecting more effective hypotheses for image classifier training, we have developed a novel hyperbolic framework for large-scale image visualization and interactive hypotheses assessment. Our experiments on large-scale image collections have also obtained very positive results.","Ontologies,
Large-scale systems,
Kernel,
Computer science,
Boosting,
Image classification,
Computational efficiency,
Large scale integration,
Feature extraction,
Support vector machines"
Byzantine Modification Detection in Multicast Networks With Random Network Coding,"An information-theoretic approach for detecting Byzantine or adversarial modifications in networks employing random linear network coding is described. Each exogenous source packet is augmented with a flexible number of hash symbols that are obtained as a polynomial function of the data symbols. This approach depends only on the adversary not knowing the random coding coefficients of all other packets received by the sink nodes when designing its adversarial packets. We show how the detection probability varies with the overhead (ratio of hash to data symbols), coding field size, and the amount of information unknown to the adversary about the random code.","Network coding,
Peer to peer computing,
Polynomials,
Laboratories,
Associate members,
Robustness,
Ad hoc networks,
Computer networks,
Information theory,
Computer science"
Visually Augmented Navigation for Autonomous Underwater Vehicles,"As autonomous underwater vehicles (AUVs) are becoming routinely used in an exploratory context for ocean science, the goal of visually augmented navigation (VAN) is to improve the near-seafloor navigation precision of such vehicles without imposing the burden of having to deploy additional infrastructure. This is in contrast to traditional acoustic long baseline navigation techniques, which require the deployment, calibration, and eventual recovery of a transponder network. To achieve this goal, VAN is formulated within a vision-based simultaneous localization and mapping (SLAM) framework that exploits the systems-level complementary aspects of a camera and strap-down sensor suite. The result is an environmentally based navigation technique robust to the peculiarities of low-overlap underwater imagery. The method employs a view-based representation where camera-derived relative-pose measurements provide spatial constraints, which enforce trajectory consistency and also serve as a mechanism for loop closure, allowing for error growth to be independent of time for revisited imagery. This article outlines the multisensor VAN framework and demonstrates it to have compelling advantages over a purely vision-only approach by: 1) improving the robustness of low-overlap underwater image registration; 2) setting the free gauge scale; and 3) allowing for a disconnected camera-constraint topology.","Navigation,
Underwater vehicles,
Simultaneous localization and mapping,
Robustness,
Oceans,
Marine vehicles,
Remotely operated vehicles,
Mobile robots,
Underwater acoustics,
Calibration"
Double synchronized switch harvesting (DSSH): a new energy harvesting scheme for efficient energy extraction,"This paper presents a new technique for optimized energy harvesting using piezoelectric microgenerators called double synchronized switch harvesting (DSSH). This technique consists of a nonlinear treatment of the output voltage of the piezoelectric element. It also integrates an intermediate switching stage that ensures an optimal harvested power whatever the load connected to the microgenerator. Theoretical developments are presented considering either constant vibration magnitude, constant driving force, or independent extraction. Then experimental measurements are carried out to validate the theoretical predictions. This technique exhibits a constant output power for a wide range of load connected to the microgenerator. In addition, the extracted power obtained using such a technique allows a gain up to 500% in terms of maximal power output compared with the standard energy harvesting method. It is also shown that such a technique allows a fine-tuning of the trade-off between vibration damping and energy harvesting.","Switches,
Decision support systems,
Inductors,
Energy exchange,
Capacitors,
Voltage,
Diodes,
Resistors,
Bridge circuits,
Rectifiers"
Leakage-Resilient Cryptography,"We construct a  stream-cipher \SC whose \emph{implementation} is secure even if a  bounded amount of arbitrary (adversarially chosen) information on  the internal state of \SC is leaked during computation. This  captures \emph{all} possible side-channel attacks on \SC where the  amount of information leaked in a given period is bounded, but  overall can be arbitrary large.The only other assumption we make on the  \emph{implementation} of \SC is that only data that is accessed  during computation leaks information.  The stream-cipher \SC generates its output in chunks  K_1,K_2,\ldots and arbitrary but bounded information leakage is  modeled by allowing the adversary to adaptively chose a function  f_\ell:\bin^*\rightarrow\bin^\lambda before K_\ell is computed,  she then gets f_\ell(\tau_\ell) where \tau_\ell is the internal  state of \SC that is accessed during the computation of  K_\ell.One notion of security we prove for \SC is that K_\ell  is indistinguishable from random when given K_1,\ldots,K_{\ell-1},  f_1(\tau_1),\ldots, f_{\ell-1}(\tau_{\ell-1}) and also the complete  internal state of \SC after K_{\ell} has been computed  (i.e. \SC is forward-secure).  The construction is based on alternating extraction  (used in the intrusion-resilient secret-sharing scheme from  FOCS'07). We move this concept to the computational setting by  proving a lemma that states that the output of any PRG has high HILL  pseudoentropy (i.e. is indistinguishable from some distribution with  high min-entropy) even if arbitrary information about the seed is  leaked.  The amount of leakage \leak that we can tolerate in each  step depends on the strength of the underlying PRG, it is at least  logarithmic, but can be as large as a constant fraction of the  internal state of \SC if the PRG is exponentially hard.","Cryptography,
Information security,
Electromagnetic radiation,
Protection,
Computer science,
Data mining,
Distributed computing,
Mathematical model,
Energy consumption,
Fault detection"
Noninvasive BCIs: Multiway Signal-Processing Array Decompositions,"In addition to helping better understand how the human brain works, the brain-computer interface neuroscience paradigm allows researchers to develop a new class of bioengineering control devices and robots, offering promise for rehabilitation and other medical applications as well as exploring possibilities for advanced human-computer interfaces.",
Distributed function calculation and consensus using linear iterative strategies,"Given an arbitrary network of interconnected nodes, we develop and analyze a distributed strategy that enables a subset of the nodes to calculate any given function of the node values. Our scheme utilizes a linear iteration where, at each time-step, each node updates its value to be a weighted average of its own previous value and those of its neighbors. We show that this approach can be viewed as a linear dynamical system, with dynamics that are given by the weight matrix of the linear iteration, and with outputs for each node that are captured by the set of values that are available to that node at each time-step. In connected networks with time-invariant topologies, we use observability theory to show that after running the linear iteration for a finite number of time-steps with almost any choice of weight matrix, each node obtains enough information to calculate any arbitrary function of the initial node values. The problem of distributed consensus via linear iterations, where all nodes in the network calculate the same function, is treated as a special case of our approach. In particular, our scheme allows nodes in connected networks with time-invariant topologies to reach consensus on any arbitrary function of the initial node values in a finite number of steps for almost any choice of weight matrix.","Network topology,
Observability,
Multiagent systems,
Control systems,
Computer science,
Protocols,
Engineering profession,
Information processing,
Convergence"
TeraGrid Science Gateways and Their Impact on Science,"Funded by the National Science Foundation (NSF), TeraGrid is one of the world's largest distributed cyberinfrastructures for open scientific research. The project began in 2001 as the Distributed Tera-scale Facility, which linked computers, visualization systems, and data at four sites through a dedicated 40-gigabit optical network. Today TeraGrid includes 25 platforms at 11 sites and provides access to more than a petaflop of computing power and petabytes of storage. TeraGrid has three primary focus areas. Its deep goal is to support the most challenging computational science activities those that cannot be achieved without TeraGrid facilities. TeraGrid's wide mission is to broaden its user base. The project's open goal is to achieve compatibility with peer grids and information services that allow development of programmatic interfaces to TeraGrid. The Science Gateways program seeks to provide researchers with easy access to TeraGrid's high-performance computing resources. A look at four successful gateways illustrates the program's goals, challenges, and opportunities.","Computer networks,
Distributed computing,
Optical computing,
Data visualization,
Optical fiber networks,
Biomedical computing,
Grid computing,
Cancer,
Biomedical informatics,
User interfaces"
Real-time face pose estimation from single range images,"We present a real-time algorithm to estimate the 3D pose of a previously unseen face from a single range image. Based on a novel shape signature to identify noses in range images, we generate candidates for their positions, and then generate and evaluate many pose hypotheses in parallel using modern graphics processing units (GPUs). We developed a novel error function that compares the input range image to precomputed pose images of an average face model. The algorithm is robust to large pose variations of ±90 ° yaw, ±45 ° pitch and ±30 ° roll rotation, facial expression, partial occlusion, and works for multiple faces in the field of view. It correctly estimates 97.8% of the poses within yaw and pitch error of 15 ° at 55.8 fps. To evaluate the algorithm, we built a database of range images with large pose variations and developed a method for automatic ground truth annotation.","Face detection,
Robustness,
Nose,
Graphics,
Runtime,
Computer vision,
Laboratories,
Shape,
Image generation,
Error correction"
PARSEC vs. SPLASH-2: A quantitative comparison of two multithreaded benchmark suites on Chip-Multiprocessors,"The PARSEC benchmark suite was recently released and has been adopted by a significant number of users within a short amount of time. This new collection of workloads is not yet fully understood by researchers. In this study we compare the SPLASH-2 and PARSEC benchmark suites with each other to gain insights into differences and similarities between the two program collections. We use standard statistical methods and machine learning to analyze the suites for redundancy and overlap on Chip-Multiprocessors (CMPs). Our analysis shows that PARSEC workloads are fundamentally different from SPLASH-2 benchmarks. The observed differences can be explained with two technology trends, the proliferation of CMPs and the accelerating growth of world data.","Benchmark testing,
Distance measurement,
Principal component analysis,
Oceans,
Redundancy,
Data mining,
Sea measurements"
A Novel Method for the Automatic Grading of Retinal Vessel Tortuosity,"Tortuosity is among the first alterations in the retinal vessel network to appear in many retinopathies, such as those due to hypertension. An automatic evaluation of retinal vessel tortuosity would help the early detection of such retinopathies. Quite a few techniques for tortuosity measurement and classification have been proposed, but they do not always match the clinical concept of tortuosity. This justifies the need for a new definition, able to express in mathematical terms the tortuosity as perceived by ophthalmologists. We propose here a new algorithm for the evaluation of tortuosity in vessels recognized in digital fundus images. It is based on partitioning each vessel in segments of constant-sign curvature and then combining together each evaluation of such segments and their number. The algorithm has been compared with other available tortuosity measures on a set of 30 arteries and one of 30 veins from 60 different images. These vessels had been preliminarily ordered by a retina specialist by increasing perceived tortuosity. The proposed algorithm proved to be the best one in matching the clinically perceived vessel tortuosity.","Retinal vessels,
Retinopathy,
Retina,
Hypertension,
Partitioning algorithms,
Image segmentation,
Pathology,
Image recognition,
Arteries,
Veins"
Sets of Optimal Frequency-Hopping Sequences,"In communication systems, frequency-hopping spread spectrum and direct-sequence spread spectrum are two main spread coding technologies. Frequency-hopping sequences are needed in FH-CDMA systems. In this correspondence, three algebraic constructions of sets of optimal frequency-hopping sequences are presented. The parameters of these sets of frequency-hopping sequences are new and flexible.",
Cyber-Physical Systems: A New Frontier,"The report of the President's Council of Advisors on Science and Technology (PCAST) has placed CPS on the top of the priority list for federal research investment [6]. This article first reviews some of the challenges and promises of CPS, followed by an articulation of some specific challenges and promises that are more closely related to the Sensor Networks, Ubiquitous and Trustworthy Computing Conference.","Diseases,
Costs,
Computer networks,
Pervasive computing,
Transportation,
Physics computing,
Councils,
Aging,
Sensor systems,
Energy consumption"
Secure wireless communications via cooperation,"The feasibility of physical-layer-based security approaches for wireless communications in the presence of one or more eavesdroppers is hampered by channel conditions. In this paper, cooperation is investigated as an approach to overcome this problem and improve the performance of secure communications. In particular, a decode-and-forward (DF) based cooperative protocol is considered, and the objective is to design the system for secrecy capacity maximization or transmit power minimization. System design for the DF-based cooperative protocol is first studied by assuming the availability of global channel state information (CSI). For the case of one eavesdropper, an iterative scheme is proposed to obtain the optimal solution for the problem of transmit power minimization. For the case of multiple eavesdroppers, the problem of secrecy capacity maximization or transmit power minimization is in general intractable. Suboptimal system design is proposed by adding an additional constraint, i.e., the complete nulling of signals at all eavesdroppers, which yields simple closed-form solutions for the aforementioned two problems. Then, the impact of imperfect CSI of eavesdroppers on system design is studied, in which the ergodic secrecy capacity is of interest.","Wireless communication,
Communication system security,
Protocols,
Broadcasting,
Information security,
National security,
Physics computing,
Channel state information,
Physical layer,
Fading"
Capacity of Symmetric K-User Gaussian Very Strong Interference Channels,"This paper studies a symmetric K user Gaussian interference channel with K transmitters and K receivers. A ""very strong"" interference regime is derived for this channel setup. A ""very strong"" interference regime is one where the capacity region of the interference channel is the same as the capacity region of the channel with no interference. In this regime, the interference can be perfectly canceled by all the receivers without incurring any rate penalties. A ""very strong"" interference condition for an example symmetric K user deterministic interference channel is also presented.","Interference channels,
Lattices,
Transmitters,
Interference cancellation,
Frequency,
Wireless communication,
Computer science"
Multi-robot perimeter patrol in adversarial settings,"This paper considers the problem of multi-robot patrol around a closed area with the existence of an adversary attempting to penetrate into the area. In case the adversary knows the patrol scheme of the robots and the robots use a deterministic patrol algorithm, then in many cases it is possible to penetrate with probability 1. Therefore this paper considers a non-deterministic patrol scheme for the robots, such that their movement is characterized by a probability p. This patrol scheme allows reducing the probability of penetration, even under an assumption of a strong opponent that knows the patrol scheme. We offer an optimal polynomial-time algorithm for finding the probability p such that the minimal probability of penetration detection throughout the perimeter is maximized. We describe three robotic motion models, defined by the movement characteristics of the robots. The algorithm described herein is suitable for all three models.","Frequency,
Polynomials,
Monitoring,
Robotics and automation,
USA Councils,
Computer science,
Robot motion,
Change detection algorithms,
Robot sensing systems,
Security"
Computational Analysis and Improvement of SIRT,"Iterative X-ray computed tomography (CT) algorithms have the potential for producing high-quality images but are computationally very demanding, especially when applied to high-resolution problems. Focusing on simultaneous iterative reconstruction technique (SIRT), we provide an eigenvalue based scheme for automatically determining a near-optimal value of the relaxation parameter. This accelerates the convergence rate of SIRT to the point where only half the number of iterations normally required is needed. We also modify the way SIRT uses preconditioning to solve a weighted least squares problem. The resulting algorithm, which we call PSIRT, is associated with a smaller memory footprint and calls for less data to be communicated in a distributed-memory implementation. Experimental residual norm and timing results are provided based on cone-beam micro-CT mouse data, including for an ordered subsets study.","X-ray imaging,
Computed tomography,
Iterative algorithms,
Image reconstruction,
Eigenvalues and eigenfunctions,
Acceleration,
Convergence,
Least squares methods,
Timing,
Mice"
Physical Collaboration of Human-Human and Human-Robot Teams,"Human partners working on a target acquisition task perform faster than do individuals on the same task, even though the partners consider each other to be an impediment. We recorded the force profile of each partner during the task, revealing an emergent specialization of roles that could only have been negotiated through a haptic channel. With this understanding of human haptic communication we attempted a ""haptic Turing test,"" replicating human behaviors in a robot partner. Human participants consciously and incorrectly believed their partner was human. However, force profiles did not show specialization of roles in the human partner, nor enhanced dyadic performance, suggesting that haptic interaction holds a greater subconscious subtlety. We further report observations of a non-zero dyadic steady state force perhaps analogous to co-contraction within the limb of an individual, where it contributes to limb stiffness and disturbance rejection. We present results on disturbance rejection in a dyad, showing lack of an effective dyadic strategy for brief events.",
Video Semantic Event/Concept Detection Using a Subspace-Based Multimedia Data Mining Framework,"In this paper, a subspace-based multimedia data mining framework is proposed for video semantic analysis, specifically video event/concept detection, by addressing two basic issues, i.e., semantic gap and rare event/concept detection. The proposed framework achieves full automation via multimodal content analysis and intelligent integration of distance-based and rule-based data mining techniques. The content analysis process facilitates the comprehensive video analysis by extracting low-level and middle-level features from audio/visual channels. The integrated data mining techniques effectively address these two basic issues by alleviating the class imbalance issue along the process and by reconstructing and refining the feature dimension automatically. The promising experimental performance on goal/corner event detection and sports/commercials/building concepts extraction from soccer videos and TRECVID news collections demonstrates the effectiveness of the proposed framework. Furthermore, its unique domain-free characteristic indicates the great potential of extending the proposed multimedia data mining framework to a wide range of different application domains.",
Global stereo reconstruction under second order smoothness priors,"Second-order priors on the smoothness of 3D surfaces are a better model of typical scenes than first-order priors. However, stereo reconstruction using global inference algorithms, such as graph-cuts, has not been able to incorporate second-order priors because the triple cliques needed to express them yield intractable (non-submodular) optimization problems.","Stereo image processing,
Image reconstruction,
Inference algorithms,
Layout,
Surface reconstruction,
Proposals,
Minimization methods,
Optimization methods,
Testing,
History"
Characterizing application sensitivity to OS interference using kernel-level noise injection,"Operating system noise has been shown to be a key limiter of application scalability in high-end systems. While several studies have attempted to quantify the sources and effects of system interference using userlevel mechanisms, there are few published studies on the effect of different kinds of kernel-generated noise on application performance at scale. In this paper, we examine the sensitivity of real-world, large-scale applications to a range of OS noise patterns using a kernel-based noise injection mechanism implemented in the Catamount lightweight kernel. Our results demonstrate the importance of how noise is generated, in terms of frequency and duration, and how this impact changes with application scale. For example, our results show that 2.5% net processor noise at 10,000 nodes can have no impact or can result in over a factor of 20 slowdown for the same application, depending solely on how the noise is generated. We also discuss how the characteristics of the applications we studied, for example computation/communication ratios, collective communication sizes, and other characteristics, related to their tendency to amplify or absorb noise. Finally, we discuss the implications of our findings on the design of new operating systems, middleware, and other system services for high-end parallel systems.","Interference,
Operating systems,
Kernel,
Large-scale systems,
Application software,
Noise generators,
Noise reduction,
System software,
Laboratories,
Scalability"
Accelerating the Nonequispaced Fast Fourier Transform on Commodity Graphics Hardware,"We present a fast parallel algorithm to compute the nonequispaced fast Fourier transform on commodity graphics hardware (the GPU). We focus particularly on a novel implementation of the convolution step in the transform as it was previously its most time consuming part. We describe the performance for two common sample distributions in medical imaging (radial and spiral trajectories), and for different convolution kernels as these parameters all influence the speed of the algorithm. The GPU-accelerated convolution is up to 85 times faster as our reference, the open source NFFT library on a state-of-the-art 64 bit CPU. The accuracy of the proposed GPU implementation was quantitatively evaluated at the various settings. To illustrate the applicability of the transform in medical imaging, in which it is also known as gridding, we look specifically at non-Cartesian magnetic resonance imaging and reconstruct both a numerical phantom and an in vivo cardiac image.","Acceleration,
Fast Fourier transforms,
Graphics,
Hardware,
Convolution,
Biomedical imaging,
Parallel algorithms,
Concurrent computing,
Spirals,
Kernel"
A Survey of Opportunistic Networks,"We define an opportunistic network as one type of challenged networks where network contacts are intermittent or where link performance is highly variable or extreme. In such a network, there does not exist a complete path from source to destination for most of the time. In addition, the path can be highly unstable and may change or break quickly. Therefore, in order to make communication possible in an opportunistic network, the intermediate nodes may take custody of data during the blackout and forward it when the connectivity resumes. In this paper, we discuss some research challenges in an opportunistic network.","Protocols,
Physical layer,
IP networks,
Application software,
Computer science,
Electronic mail,
Resumes,
TCPIP,
Propagation delay,
Disruption tolerant networking"
Underwater Localization in Sparse 3D Acoustic Sensor Networks,"We study the localization problem in sparse 3D underwater sensor networks. Considering the fact that depth information is typically available for underwater sensors, we transform the 3D underwater positioning problem into its two- dimensional counterpart via a projection technique and prove that a non-degenerative projection preserves network localizability. We further prove that given a network and a constant k, all of the geometric k-lateration localization methods are equivalent. Based on these results, we design a purely distributed localization framework termed USP. This framework can be applied with any ranging method proposed for 2D terrestrial sensor networks. Through theoretical analysis and extensive simulation, we show that USP preserves the localizability of the original 3D network via a simple projection and improves localization capabilities when bilateration is employed. USP has low storage and computation requirements, and predictable and balanced communication overhead.",
Wavelet Based Noise Reduction in CT-Images Using Correlation Analysis,"The projection data measured in computed tomography (CT) and, consequently, the slices reconstructed from these data are noisy. We present a new wavelet based structure-preserving method for noise reduction in CT-images that can be used in combination with different reconstruction methods. The approach is based on the assumption that data can be decomposed into information and temporally uncorrelated noise. In CT two spatially identical images can be generated by reconstructions from disjoint subsets of projections: using the latest generation dual source CT-scanners one image can be reconstructed from the projections acquired at the first, the other image from the projections acquired at the second detector. For standard CT-scanners the two images can be generated by splitting up the set of projections into even and odd numbered projections. The resulting images show the same information but differ with respect to image noise. The analysis of correlations between the wavelet representations of the input images allows separating information from noise down to a certain signal-to-noise level. Wavelet coefficients with small correlation are suppressed, while those with high correlations are assumed to represent structures and are preserved. The final noise-suppressed image is reconstructed from the averaged and weighted wavelet coefficients of the input images. The proposed method is robust, of low complexity and adapts itself to the noise in the images. The quantitative and qualitative evaluation based on phantom as well as real clinical data showed, that high noise reduction rates of around 40% can be achieved without noticeable loss of image resolution.","Wavelet analysis,
Noise reduction,
Image reconstruction,
Computed tomography,
Image generation,
Wavelet coefficients,
Reconstruction algorithms,
Detectors,
Image analysis,
Information analysis"
Importance-Driven Time-Varying Data Visualization,"The ability to identify and present the most essential aspects of time-varying data is critically important in many areas of science and engineering. This paper introduces an importance-driven approach to time-varying volume data visualization for enhancing that ability. By conducting a block-wise analysis of the data in the joint feature-temporal space, we derive an importance curve for each data block based on the formulation of conditional entropy from information theory. Each curve characterizes the local temporal behavior of the respective block, and clustering the importance curves of all the volume blocks effectively classifies the underlying data. Based on different temporal trends exhibited by importance curves and their clustering results, we suggest several interesting and effective visualization techniques to reveal the important aspects of time-varying data.","Data visualization,
Entropy,
Transfer functions,
Information theory,
Time measurement,
Chaos,
Data engineering,
Data analysis,
Information analysis,
Earthquakes"
Spatial-Temporal correlatons for unsupervised action classification,"Spatial-temporal local motion features have shown promising results in complex human action classification. Most of the previous works [6],[16],[21] treat these spatial-temporal features as a bag of video words, omitting any long range, global information in either the spatial or temporal domain. Other ways of learning temporal signature of motion tend to impose a fixed trajectory of the features or parts of human body returned by tracking algorithms. This leaves little flexibility for the algorithm to learn the optimal temporal pattern describing these motions. In this paper, we propose the usage of spatial-temporal correlograms to encode flexible long range temporal information into the spatial-temporal motion features. This results into a much richer description of human actions. We then apply an unsupervised generative model to learn different classes of human actions from these ST-correlograms. KTH dataset, one of the most challenging and popular human action dataset, is used for experimental evaluation. Our algorithm achieves the highest classification accuracy reported for this dataset under an unsupervised learning scheme.","Humans,
Cameras,
Robustness,
Video sequences,
Computer science,
Intelligent robots,
Legged locomotion,
Feature extraction,
Intelligent systems,
Trajectory"
Exploring Historical Location Data for Anonymity Preservation in Location-Based Services,"We present a new approach for if-anonymity protection in Location-Based Services (LBSs). Specifically, we depersonalize location information by ensuring that each location reported for LBSs is a cloaking area that contains K different footprints-historical locations of different mobile nodes. Therefore, the exact identity and location of the service requestor remain anonymous from LBS service providers. Existing techniques, on the other hand, compute the cloaking area using current locations of K neighboring hosts of the service requestor. Because of this difference, our approach significantly reduces the cloaking area, which in turn decreases query processing and communication overhead for returning query results to the requesting host. In addition, existing techniques also require frequent location updates from all nodes, regardless of whether or not these nodes are requesting LBSs. Most importantly, our approach is the first practical solution that provides K-anonymity trajectory protection needed to ensure anonymity when a mobile host requests LBSs continuously as it moves. Our solution depersonalizes a user's trajectory (a time-series of the user's locations) based on the historical trajectories of other users. We evaluate our techniques under various conditions using location data synthetically generated based on real road maps. The results show that our techniques can provide K-anonymity trajectory protection using a minimized cloaking area.",
Tensor-Based Cortical Surface Morphometry via Weighted Spherical Harmonic Representation,"We present a new tensor-based morphometric framework that quantifies cortical shape variations using a local area element. The local area element is computed from the Riemannian metric tensors, which are obtained from the smooth functional parametrization of a cortical mesh. For the smooth parametrization, we have developed a novel weighted spherical harmonic (SPHARM) representation, which generalizes the traditional SPHARM as a special case. For a specific choice of weights, the weighted-SPHARM is shown to be the least squares approximation to the solution of an isotropic heat diffusion on a unit sphere. The main aims of this paper are to present the weighted-SPHARM and to show how it can be used in the tensor-based morphometry. As an illustration, the methodology has been applied in the problem of detecting abnormal cortical regions in the group of high functioning autistic subjects.",
"Fuzzy Interpolative Reasoning for Sparse Fuzzy Rule-Based Systems Based on {\bm \alpha}
-Cuts and Transformations Techniques","In sparse fuzzy rule-based systems, the fuzzy rule bases are usually incomplete. In this situation, the system may not properly perform fuzzy reasoning to get reasonable consequences. In order to overcome the drawback of sparse fuzzy rule-based systems, there is an increasing demand to develop fuzzy interpolative reasoning techniques in sparse fuzzy rule-based systems. In this paper, we present a new fuzzy interpolative reasoning method via cutting and transformation techniques for sparse fuzzy rule-based systems. It can produce more reasonable results than the existing methods. The proposed method provides a useful way to deal with fuzzy interpolative reasoning in sparse fuzzy rule-based systems.","Fuzzy reasoning,
Fuzzy systems,
Knowledge based systems,
Interpolation,
Fuzzy sets,
Extrapolation,
Computer science,
Councils,
Linear approximation,
Equations"
A layered lattice coding scheme for a class of three user Gaussian interference channels,The paper studies a class of three user Gaussian interference channels. A new layered lattice coding scheme is introduced as a transmission strategy. The use of lattice codes allows for an “alignment” of the interference observed at each receiver. The layered lattice coding is shown to achieve more than one degree of freedom for a class of interference channels and also achieves rates which are better than the rates obtained using the Han-Kobayashi coding scheme.,"Lattices,
Interference channels,
AWGN channels,
Integrated circuit modeling,
Frequency,
Transmitters,
Equations,
Region 3,
Wireless communication,
Computer science"
Anti-Swinging Input Shaping Control of an Automatic Construction Crane,"This paper describes the control of an automatic overhead crane for assembly of modular building elements. The automatic system was developed using a commercial full-size crane, which was modified by adding adequate sensors, servomotors, and control strategy. The crane, which maintained its original cables, was converted to a robotic system, and is controlled via a computer-based multiaxes control board. The implemented algorithms solve two main problems of module assembly: 1) precision positioning of large and heavy modules in the order of a few centimeters, and 2) anti-swinging transportation of modules, even in difficult weather conditions (not extreme). The implementation of the anti-swinging control is achieved using the on-line two-dimensional inclinometer measurements and an on-line calculation of input impulse trains. The developed input-shaping control is divided into two phases: straight line motion and external perturbances cancellation. Experimental results of the developed algorithms which demonstrate the effectiveness of the new process are presented.","Automatic control,
Shape control,
Cranes,
Control systems,
Robotic assembly,
Modular construction,
Buildings,
Sensor systems,
Servomotors,
Cables"
"A Factorization-Based Approach for Articulated Nonrigid Shape, Motion and Kinematic Chain Recovery From Video","Recovering articulated shape and motion, especially human body motion, from video is a challenging problem with a wide range of applications in medical study, sport analysis, animation, and so forth. Previous work on articulated motion recovery generally requires prior knowledge of the kinematic chain and usually does not concern the recovery of the articulated shape. The nonrigidity of some articulated part, for example, human body motion with nonrigid facial motion, is completely ignored. We propose a factorization-based approach to recover the shape, motion, and kinematic chain of an articulated object with nonrigid parts altogether directly from video sequences under a unified framework. The proposed approach is based on our modeling of the articulated nonrigid motion as a set of intersecting motion subspaces. A motion subspace is the linear subspace of the trajectories of an object. It can model a rigid or nonrigid motion. The intersection of two motion subspaces of linked parts models the motion of an articulated joint or axis. Our approach consists of algorithms for motion segmentation, kinematic chain building, and shape recovery. It handles outliers and can be automated. We test our approach through synthetic and real experiments and demonstrate how to recover an articulated structure with nonrigid parts via a single-view camera without prior knowledge of its kinematic chain.","Shape,
Kinematics,
Motion analysis,
Humans,
Animation,
Video sequences,
Biological system modeling,
Motion segmentation,
Computer vision,
Buildings"
Intraoperative Laparoscope Augmentation for Port Placement and Resection Planning in Minimally Invasive Liver Resection,"In recent years, an increasing number of liver tumor indications were treated by minimally invasive laparoscopic resection. Besides the restricted view, two major intraoperative issues in laparoscopic liver resection are the optimal planning of ports as well as the enhanced visualization of (hidden) vessels, which supply the tumorous liver segment and thus need to be divided (e.g., clipped) prior to the resection. We propose an intuitive and precise method to plan the placement of ports. Pre operatively, self-adhesive fiducials are affixed to the patient's skin and a computed tomography (CT) data set is acquired while contrasting the liver vessels. Immediately prior to the intervention, the laparoscope is moved around these fiducials, which are automatically reconstructed to register the patient to its preoperative imaging data set. This enables the simulation of a camera flight through the patient's interior along the laparoscope's or instruments' axes to easily validate potential ports. Intraoperatively, surgeons need to update their surgical planning based on actual patient data after organ deformations mainly caused by application of carbon dioxide pneumoperitoneum. Therefore, preoperative imaging data can hardly be used. Instead, we propose to use an optically tracked mobile C-arm providing cone-beam CT imaging capability intraoperatively. After patient positioning, port placement, and carbon dioxide insufflation, the liver vessels are contrasted and a 3-D volume is reconstructed during patient exhalation. Without any further need for patient registration, the reconstructed volume can be directly augmented on the live laparoscope video, since prior calibration enables both the volume and the laparoscope to be positioned and oriented in the tracking coordinate frame. The augmentation provides the surgeon with advanced visual aid for the localization of veins, arteries, and bile ducts to be divided or sealed.","Laparoscopes,
Minimally invasive surgery,
Computed tomography,
Image reconstruction,
Optical imaging,
Carbon dioxide,
Liver neoplasms,
Data visualization,
Skin,
Registers"
Transmission techniques for relay-interference networks,"In this paper we study the relay-interference wireless network, in which relay (helper) nodes are to facilitate competing information flows over a wireless network. We examine this in the context of a deterministic wireless interaction model, which eliminates the channel noise and focuses on the signal interactions. Using this model, we show that almost all the known schemes such as interference suppression, interference alignment and interference separation are necessary for relay-interference networks. In addition, we discover a new interference management technique, which we call interference neutralization, which allows for over-the-air interference removal, without the transmitters having complete access the interfering signals. We show that interference separation, suppression, and neutralization arise in a fundamental manner, since we show complete characterizations for special configurations of the relay-interference network.",
Cluster-level feedback power control for performance optimization,"Power control is becoming a key challenge for effectively operating a modern data center. In addition to reducing operation costs, precisely controlling power consumption is an essential way to avoid system failures caused by power capacity overload or overheating due to increasing high-density. Control-theoretic techniques have recently shown a lot of promise on power management thanks to their better control performance and theoretical guarantees on control accuracy and system stability. However, existing work over-simplifies the problem by controlling a single server independently from others. As a result, at the cluster level where multiple servers are correlated by common workloads and share common power supplies, power cannot be shared to improve application performance. In this paper, we propose a cluster-level power controller that shifts power among servers based on their performance needs, while controlling the total power of the cluster to be lower than a constraint. Our controller features a rigorous design based on an optimal multi-input-multi-output control theory. Empirical results demonstrate that our controller outperforms two state-of-the-art controllers, by having better application performance and more accurate power control.",
Fast plane detection and polygonalization in noisy 3D range images,"A fast but nevertheless accurate approach for surface extraction from noisy 3D point clouds is presented. It consists of two parts, namely a plane fitting and a polygonalization step. Both exploit the sequential nature of 3D data acquisition on mobile robots in form of range images. For the plane fitting, this is used to revise the standard mathematical formulation to an incremental version, which allows a linear computation. For the polygonalization, the neighborhood relation in range images is exploited. Experiments are presented using a time-of-flight range camera in form of a Swissranger SR-3000. Results include lab scenes as well as data from two runs of the rescue robot league at the RoboCup German Open 2007 with 1,414, respectively 2,343 sensor snapshots. The 36⋅106, respectively 59⋅106 points from the two point clouds are reduced to about 14⋅103, respectively 23⋅103 planes with only about 0.2 sec of total computation time per snapshot while the robot moves along. Uncertainty analysis of the computed plane parameters is presented as well.","Three dimensional displays,
Distance measurement,
Meteorology,
Robot sensing systems,
Fitting,
Cameras,
Robots"
On-line LDA: Adaptive Topic Models for Mining Text Streams with Applications to Topic Detection and Tracking,"This paper presents Online Topic Model (OLDA), a topic model that automatically captures the thematic patterns and identifies emerging topics of text streams and their changes over time. Our approach allows the topic modeling framework, specifically the Latent Dirichlet Allocation (LDA) model, to work in an online fashion such that it incrementally builds an up-to-date model (mixture of topics per document and mixture of words per topic) when a new document (or a set of documents) appears. A solution based on the Empirical Bayes method is proposed. The idea is to incrementally update the current model according to the information inferred from the new stream of data with no need to access previous data. The dynamics of the proposed approach also provide an efficient mean to track the topics over time and detect the emerging topics in real time. Our method is evaluated both qualitatively and quantitatively using benchmark datasets. In our experiments, the OLDA has discovered interesting patterns by just analyzing a fraction of data at a time. Our tests also prove the ability of OLDA to align the topics across the epochs with which the evolution of the topics over time is captured. The OLDA is also comparable to, and sometimes better than, the original LDA in predicting the likelihood of unseen documents.","Linear discriminant analysis,
Data mining,
Application software,
Computer science,
USA Councils,
Benchmark testing,
Pattern analysis,
Organizing,
Yarn,
Software libraries"
Video-rate localization in multiple maps for wearable augmented reality,"We show how a system for video-rate parallel camera tracking and 3D map-building can be readily extended to allow one or more cameras to work in several maps, separately or simultaneously. The ability to handle several thousand features per map at video-rate, and for the cameras to switch automatically between maps, allows spatially localized AR workcells to be constructed and used with very little intervention from the user of a wearable vision system. The user can explore an environment in a natural way, acquiring local maps in real-time. When revisiting those areas the camera will select the correct local map from store and continue tracking and structural acquisition, while the user views relevant AR constructs registered to that map.",
Approximate capacity of Gaussian relay networks,"We present an achievable rate for general Gaussian relay networks.We show that the achievable rate is within a constant number of bits from the information-theoretic cut-set upper bound on the capacity of these networks. This constant depends on the topology of the network, but not the values of the channel gains. Therefore, we uniformly characterize the capacity of Gaussian relay networks within a constant number of bits, for all channel parameters.","Relays,
Upper bound,
Gain,
Wireless communication,
Encoding,
Approximation methods,
Electronic mail"
Evaluating the Significance of the Desktop Area in Everyday Computer Use,"Computers have become part of our homes and day-to-day lives. This paper presents selected results of an interview-based user study focused on information management on the personal computer. We focus on the Desktop, confirming results of previous studies as well as revealing new issues and ensuing design suggestions. While even basic competence users inventively appropriated the desktop, some features, in particular user-defined shortcuts, appeared counter-intuitive, and were underused. Users are still dissatisfied with their information organization and the challenge is to provide tools that support rather than replace the users 'flexible and creative use of the current desktop.","Telecommunication computing,
Information management,
Microcomputers,
TV,
Computer aided instruction,
Informatics,
Computer science,
Home computing,
Operating systems,
Laboratories"
Tractography Gone Wild: Probabilistic Fibre Tracking Using the Wild Bootstrap With Diffusion Tensor MRI,"Diffusion tensor magnetic resonance imaging (DT-MRI) permits the noninvasive assessment of tissue microstructure and, with fibre-tracking algorithms, allows for the 3-D trajectories of white matter fasciculi to be reconstructed noninvasively. Probabilistic algorithms allow one to assign a ldquoconfidencerdquo to a given reconstructed pathway - but often rely on a priori assumptions about sources of uncertainty in the data. Bootstrap methods have been proposed as a way of circumventing this problem, deriving the uncertainty from the data themselves - but acquisition times for data amenable to precise and robust bootstrapping are clinically prohibitive. By combining the wild bootstrap, recently introduced to the DT-MRI literature, with tractography, we show how confidence can be assigned to reconstructed trajectories using data collected in a fraction of the time required for regular bootstrapping. We compare in vivo wild bootstrap tracking results with regular tracking results and show that results are comparable. This approach therefore allows users who have collected data sets for use with deterministic tracking algorithms, rather than those specifically designed for bootstrapping, to be able to apply bootstrap analyses and retrospectively assign confidence to their reconstructed trajectories with minimum additional effort.","Tensile stress,
Magnetic resonance imaging,
Image reconstruction,
Diffusion tensor imaging,
Trajectory,
Uncertainty,
Algorithm design and analysis,
Microstructure,
Robustness,
In vivo"
A nonlocal-means approach to exemplar-based inpainting,"This paper introduces a novel approach to the problem of image inpainting through the use of nonlocal-means. In traditional inpainting techniques, only local information around the target regions are used to fill in the missing information, which is insufficient in many cases. More recent inpainting techniques based on the concept of exemplar-based synthesis utilize nonlocal information but in a very limited way. In the proposed algorithm, we use nonlocal image information from multiple samples within the image. The contribution of each sample to the reconstruction of a target pixel is determined using an weighted similarity function and aggregated to form the missing information. Experimental results show that the proposed method yields quantitative and qualitative improvements compared to the current exemplar-based approach. The proposed approach can also be integrated into existing exemplar-based inpainting techniques to provide improved visual quality.","Image reconstruction,
Filling,
Image processing,
Design engineering,
Systems engineering and theory,
Computer science,
Robustness"
Constrained Relay Node Placement in Wireless Sensor Networks to Meet Connectivity and Survivability Requirements,"The relay node placement problem for wireless sensor networks is concerned with placing a minimum number of relay nodes into a wireless sensor network to meet certain connectivity and survivability requirements. In this paper, we study constrained versions of the relay node placement problem, where relay nodes can only be placed at a subset of candidate locations. In the connected relay node placement problem, we want to place a minimum number of relay nodes to ensure the connectivity of the sensor nodes and the base stations. In the survivable relay node placement problem, we want to place a minimum number of relay nodes to ensure the biconnectivity of the sensor nodes and the base stations. For each of the two problems, we discuss its computational complexity, and present a framework of polynomial time O(1) -approximation algorithms with small approximation ratios.",
Color Image Discriminant Models and Algorithms for Face Recognition,"This paper presents a basic color image discriminant (CID) model and its general version for color image recognition. The CID models seek to unify the color image representation and recognition tasks into one framework. The proposed models, therefore, involve two sets of variables: a set of color component combination coefficients for color image representation and one or multiple projection basis vectors for color image discrimination. An iterative basic CID algorithm and its general version are designed to find the optimal solution of the proposed models. The general CID (GCID) algorithm is further extended to generate three color components (such as the three color components of the RGB color images) for further improvement of the recognition performance. Experiments using the face recognition grand challenge (FRGC) database and the biometric experimentation environment (BEE) system show the effectiveness of the proposed models and algorithms. In particular, for the most challenging FRGC version 2 Experiment 4, which contains 12 776 training images, 16 028 controlled target images, and 8014 uncontrolled query images, the proposed method achieves the face verification rate (ROC III) of 78.26% at the false accept rate (FAR) of 0.1%.","Color,
Face recognition,
Image recognition,
Computer science,
Iterative algorithms,
Image databases,
Biometrics,
Linear discriminant analysis,
Image segmentation,
Indexing"
An Over-60 dB True Rail-to-Rail Performance Using Correlated Level Shifting and an Opamp With Only 30 dB Loop Gain,"Correlated level shifting (CLS) is introduced as a new switched-capacitor technique to provide true rail-to-rail performance while reducing errors from finite opamp gain. There is negligible kT/C noise increase and in many cases a speed advantage compared to using a high gain opamp. The gain enhancement is quantified with formulas and the general technique is compared to correlated double sampling (CDS). Results are presented from a 0.18 mum CMOS testchip of a 20 MHz, 12-bit pipelined A/D converter using CLS to reduce errors from finite opamp dc gain and limited opamp swing. It achieves 10.5 ENOB operating beyond the supply rails using an opamp circuit with 30 dB loop gain and 0.9 V supply.","Performance gain,
Sampling methods,
Moon,
Voltage,
Computer errors,
Rail to rail amplifiers,
Circuit testing,
Analog-digital conversion,
Analog circuits,
Computer science"
Homography based multiple camera detection and tracking of people in a dense crowd,"Tracking people in a dense crowd is a challenging problem for a single camera tracker due to occlusions and extensive motion that make human segmentation difficult. In this paper we suggest a method for simultaneously tracking all the people in a densely crowded scene using a set of cameras with overlapping fields of view. To overcome occlusions, the cameras are placed at a high elevation and only people’s heads are tracked. Head detection is still difficult since each foreground region may consist of multiple subjects. By combining data from several views, height information is extracted and used for head segmentation. The head tops, which are regarded as 2D patches at various heights, are detected by applying intensity correlation to aligned frames from the different cameras. The detected head tops are then tracked using common assumptions on motion direction and velocity. The method was tested on sequences in indoor and outdoor environments under challenging illumination conditions. It was successful in tracking up to 21 people walking in a small area (2.5 people per m2), in spite of severe and persistent occlusions.","Cameras,
Head,
Tracking,
Motion detection,
Layout,
Trajectory,
Humans,
Radio access networks,
Computer science,
Data mining"
Testing Techniques for Hardware Security,"System security has emerged as a premier design requirement. While there has been an enormous body of impressive work on testing integrated circuits (ICs) desiderata such as manufacturing correctness, delay, and power, there is no reported effort to systematically test IC security in hardware. Our goal is to provide an impetus for this line of research and development by introducing techniques and methodology for rigorous testing of physically unclonable functions (PUFs). Recently, PUFs received a great deal of attention as security mechanisms due to their flexibility to form numerous security protocols and intrinsic resiliency against physical and side channels attacks. We study three classes of PUFs properties to design pertinent test methods: (i) predictability, (ii) sensitivity to component accuracy, and (iii) susceptibility to reverse engineering. As our case studies, we analyze two popular PUF structures, linear and feed-forward, and show that their security is not adequate from several points of view. The technical highlights of the paper are the first non-destructive technique for PUF reverse engineering and a new PUF structure that is capable of passing our security tests.","Hardware,
Circuit testing,
Power system security,
Integrated circuit testing,
System testing,
Reverse engineering,
Integrated circuit manufacture,
Delay,
Research and development,
Protocols"
Advantages of Inset PM Machines for Zero-Speed Sensorless Position Detection,"The aim of this paper is to analyze the behavior of a sensorless control system based on a high-frequency signal injection combined with an inset permanent-magnet (PM) motor. The rotor position of a synchronous PM motor with anisotropic rotor can be detected by the control system which superimposes a high-frequency signal to the fundamental stator voltage. The corresponding high-frequency current is modulated by the rotor anisotropy and used to determine the rotor position. These techniques are effective at zero and at low motor speed. Both saturation and cross-coupling have a heavy influence on the correct rotor position detection. In order to highlight the effectiveness of the sensorless technique, the tests are carried out at various operating conditions.","Rotors,
Synchronous motors,
Stators,
Sensorless control,
Anisotropic magnetoresistance,
Industry Applications Society,
Computer science,
Signal analysis,
Voltage control,
Control systems"
Parallel I/O prefetching using MPI file caching and I/O signatures,"Parallel I/O prefetching is considered to be effective in improving I/O performance. However, the effectiveness depends on determining patterns among future I/O accesses swiftly and fetching data in time, which is difficult to achieve in general. In this study, we propose an I/O signature-based prefetching strategy. The idea is to use a predetermined I/O signature of an application to guide prefetching. To put this idea to work, we first derived a classification of patterns and introduced a simple and effective signature notation to represent patterns. We then developed a toolkit to trace and generate I/O signatures automatically. Finally, we designed and implemented a thread-based client-side collective prefetching cache layer for MPI-IO library to support prefetching. A prefetching thread reads I/O signatures of an application and adjusts them by observing I/O accesses at runtime. Experimental results show that the proposed prefetching method improves I/O performance significantly for applications with complex patterns.","Prefetching,
Yarn,
Runtime,
Computer science,
Libraries,
Sun,
Mathematics,
Laboratories,
Pattern analysis,
Search methods"
Necessary and Sufficient Conditions for Dynamical Structure Reconstruction of LTI Networks,"This paper formulates and solves the network reconstruction problem for linear time-invariant systems. The problem is motivated from a variety of disciplines, but it has recently received considerable attention from the systems biology community in the study of chemical reaction networks. Here, we demonstrate that even when a transfer function can be identified perfectly from input-output data, not even Boolean reconstruction is possible, in general, without more information about the system. We then completely characterize this additional information that is essential for dynamical reconstruction without appeal to ad-hoc assumptions about the network, such as sparsity or minimality.","Sufficient conditions,
Transfer functions,
Systems biology,
Chemicals,
Evolution (biology),
Laboratories,
Computer science"
Fluid Registration of Diffusion Tensor Images Using Information Theory,"We apply an information-theoretic cost metric, the symmetrized Kullback-Leibler (sKL) divergence, or J-divergence, to fluid registration of diffusion tensor images. The difference between diffusion tensors is quantified based on the sKL-divergence of their associated probability density functions (PDFs). Three-dimensional DTI data from 34 subjects were fluidly registered to an optimized target image. To allow large image deformations but preserve image topology, we regularized the flow with a large-deformation diffeomorphic mapping based on the kinematics of a Navier-Stokes fluid. A driving force was developed to minimize the J-divergence between the deforming source and target diffusion functions, while reorienting the flowing tensors to preserve fiber topography. In initial experiments, we showed that the sKL-divergence based on full diffusion PDFs is adaptable to higher-order diffusion models, such as high angular resolution diffusion imaging (HARDI). The sKL-divergence was sensitive to subtle differences between two diffusivity profiles, showing promise for nonlinear registration applications and multisubject statistical analysis of HARDI data.",
VisGets: Coordinated Visualizations for Web-based Information Exploration and Discovery,"In common Web-based search interfaces, it can be difficult to formulate queries that simultaneously combine temporal, spatial, and topical data filters. We investigate how coordinated visualizations can enhance search and exploration of information on the World Wide Web by easing the formulation of these types of queries. Drawing from visual information seeking and exploratory search, we introduce VisGets - interactive query visualizations of Web-based information that operate with online information within a Web browser. VisGets provide the information seeker with visual overviews of Web resources and offer a way to visually filter the data. Our goal is to facilitate the construction of dynamic search queries that combine filters from more than one data dimension. We present a prototype information exploration system featuring three linked VisGets (temporal, spatial, and topical), and used it to visually explore news items from online RSS feeds.",
A novel co-evolutionary approach to automatic software bug fixing,"Many tasks in Software Engineering are very expensive, and that has led the investigation to how to automate them. In particular, Software Testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. In this paper we propose an evolutionary approach to automate the task of fixing bugs. This novel evolutionary approach is based on Co-evolution, in which programs and test cases co-evolve, influencing each other with the aim of fixing the bugs of the programs. This competitive co-evolution is similar to what happens in nature for predators and prey. The user needs only to provide a buggy program and a formal specification of it. No other information is required. Hence, the approach may work for any implementable software. We show some preliminary experiments in which bugs in an implementation of a sorting algorithm are automatically fixed.","Computer bugs,
Software,
Software testing,
Formal specifications,
Distance measurement,
Training,
Sorting"
Self-adaptive differential evolution with neighborhood search,"In athis paper we investigate several self-adaptive mechanisms to improve our previous work on NSDE [1], which is a recent DE variant for numerical optimization. The self-adaptive methods originate from another DE variant, SaDE [2], but are remarkably modified and extended to fit our NSDE. And thus a Self-adaptive NSDE (SaNSDE) is proposed to improve NSDE’s performance. Three self-adaptive mechanisms are utilized in SaNSDE: self-adaptation for two candidate mutation strategies, self-adaptations for controlling scale factor F and crossover rate CR, respectively. Experimental studies are carried out on a broad range of different benchmark functions, and the proposed SaNSDE has shown significant superiority over NSDE.","Chromium,
Evolution (biology),
Benchmark testing,
Next generation networking,
Arrays,
Indexes,
Adaptation model"
Manifold-Manifold Distance with application to face recognition based on image set,"In this paper, we address the problem of classifying image sets, each of which contains images belonging to the same class but covering large variations in, for instance, viewpoint and illumination. We innovatively formulate the problem as the computation of Manifold-Manifold Distance (MMD), i.e., calculating the distance between nonlinear manifolds each representing one image set. To compute MMD, we also propose a novel manifold learning approach, which expresses a manifold by a collection of local linear models, each depicted by a subspace. MMD is then converted to integrating the distances between pair of subspaces respectively from one of the involved manifolds. The proposed MMD method is evaluated on the task of Face Recognition based on Image Set (FRIS). In FRIS, each known subject is enrolled with a set of facial images and modeled as a gallery manifold, while a testing subject is modeled as a probe manifold, which is then matched against all the gallery manifolds by MMD. Identification is achieved by seeking the minimum MMD. Experimental results on two public face databases, Honda/UCSD and CMU MoBo, demonstrate that the proposed MMD method outperforms the competing methods.",
Cognitive Radio Network setup without a Common Control Channel,"The concept of Cognitive Radio Networks has introduced a new way of sharing the open spectrum flexibly and efficiently. However, there are several issues that hinder the deployment of such dynamic networks. The common control channel problem is one of such issue. Cognitive radio networks are designed by assuming the availability of a dedicated control channel. In this paper, we identify and discuss the Network Setup Problem as a part of the Common Control Channel Problem. Probabilistic and deterministic ways to start the initial communication and setup a Cognitive Radio network without the need of having a common control channel in both centralized and multi-hop scenarios are suggested. Extensive MATLAB simulations validate the effectiveness of the algorithms.",
An approach to a Cloud Computing network,"“Cloud Computing” is becoming increasingly relevant, as it will enable companies involved in spreading this technology to open the doors to Web 3.0. In this work the basic features of cloud computing are presented and compared with those of the original technology: Grid Computing. The new categories of services introduced will slowly replace many types of computational resources currently used. In this perspective, grid computing, the basic element for the large scale supply of cloud services, will play a fundamental role in defining how those services will be provided. The paper describes the concept of computational resources outsourcing, referred to computational grids and a real application. This work utilises the results by the Cybersar Project managed by the COSMOLAB Consortium (Italy).","grid computing,
cosmology"
Assessing link quality in IEEE 802.11 Wireless Networks: Which is the right metric?,"The accurate determination of the link quality is critical for ensuring that functionalities such as intelligent routing, load-balancing, power control and frequency selection operate efficiently. There are 4 primary metrics for capturing the quality of a wireless link: RSSI (Received Signal Strength Indication), SINR (Signal-to-Interference-plus-Noise Ratio), PDR (Packet-Delivery Ratio), and BER (Bit-Error Rate). In this paper, we perform a measurement-based study in order to answer the question: which is the appropriate metric to use, and under what conditions? We evaluate the relative accuracy of each metric by conducting experiments with multiple transmission rates and varying levels of interference on a large set of links. We observe that each metric has advantages and projects one or more limitations. Our study suggests that a careful consideration of these limitations is essential, and provides guidelines on the applicability of each metric.",
Granularity in software product lines,"Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.","Software tools,
Design engineering,
Computer science,
Permission,
Informatics,
Mathematics,
Feature extraction,
Software design,
Spatial databases,
Transaction databases"
Coordinated multi-robot exploration using a segmentation of the environment,"This paper addresses the problem of exploring an unknown environment with a team of mobile robots. The key issue in coordinated multi-robot exploration is how to assign target locations to the individual robots such that the overall mission time is minimized. In this paper, we propose a novel approach to distribute the robots over the environment that takes into account the structure of the environment. To achieve this, it partitions the space into segments, for example, corresponding to individual rooms. Instead of only considering frontiers between unknown and explored areas as target locations, we send the robots to the individual segments with the task to explore the corresponding area. Our approach has been implemented and tested in simulation as well as in real world experiments. The experiments demonstrate that the overall exploration time can be significantly reduced by considering our segmentation method.","Robots,
Robot kinematics,
Robot sensing systems,
Distance measurement,
Gain,
Runtime,
Buildings"
Robots at home: Understanding long-term human-robot interaction,"Human-robot interaction (HRI) is now well enough understood to allow us to build useful systems that can function outside of the laboratory. We are studying long-term interaction in natural user environments and describe the implementation of a robot designed to help individuals effect behavior change while dieting. Our robotic weight loss coach is compared to a standalone computer and a paper log in a controlled study. We describe the software model used to create successful long-term HRI. We summarize the experimental design, analysis, and results of our study, the first where a sociable robot interacts with a user to achieve behavior change. Results show that participants track their calorie consumption and exercise for nearly twice as long when using the robot than with the other methods and develop a closer relationship with the robot. Both are indicators of longer-term success at weight loss and maintenance and show the effectiveness of sociable robots for long-term HRI.","Robots,
Robot kinematics,
Computers,
Software,
Maintenance engineering,
Software systems,
Laboratories"
Catch Me (If You Can): Data Survival in Unattended Sensor Networks,"Unattended sensor networks operating in hostile environments might collect data that represents a high-value target for the adversary.The unattended sensor's inability to off-load -- in real time -- sensitive data to a safe external entity makes it easy for the adversary to mount a focused attack aimed at eliminating certain target data. In order to facilitate survival of this data, sensors can collectively attempt to confuse the adversary by changing its location and content, i.e., by periodically moving the data around the network and encrypting it.In this paper, we focus on data survival in unattended sensor networks faced with an adversary intent on surgically destroying data which it considers to be of high value. After motivating the problem and considering several attack flavors, we propose several simple techniques and provide their detailed evaluation.","Wireless sensor networks,
Peer to peer computing,
Acoustic sensors,
Data security,
Monitoring,
Pervasive computing,
Computer science,
Cryptography,
Surgery,
Routing"
Year,,
Parallel Image Processing Based on CUDA,"CUDA (Compute Unified Device Architecture) is a novel technology of general-purpose computing on the GPU, which makes users develop general GPU (Graphics Processing Unit) programs easily. This paper analyzes the distinct features of CUDA GPU, summarizes the general program mode of CUDA. Furthermore, we implement several classical image processing algorithms by CUDA, such as histogram equalization, removing clouds, edge detection and DCT encode and decode etc., especially introduce the first two algorithms. If we don’t take the data transfer time in experiment between host memory and device memory into account, as the image size increase, histogram computation can get a more than 40x speedup, removing clouds can get an about 79x speedup, DCT can gain around 8x and edge detection more than 200x.","Image processing,
Graphics,
Concurrent computing,
Central Processing Unit,
Read-write memory,
Parallel processing,
Samarium,
Computer architecture,
Histograms,
Clouds"
Efficient people tracking in laser range data using a multi-hypothesis leg-tracker with adaptive occlusion probabilities,"We present an approach to laser-based people tracking using a multi-hypothesis tracker that detects and tracks legs separately with Kalman filters, constant velocity motion models, and a multi-hypothesis data association strategy. People are defined as high-level tracks consisting of two legs that are found with little model knowledge. We extend the data association so that it explicitly handles track occlusions in addition to detections and deletions. Additionally, we adapt the corresponding probabilities in a situation-dependent fashion so as to reflect the fact that legs frequently occlude each other. Experimental results carried out with a mobile robot illustrate that our approach can robustly and efficiently track multiple people even in situations of high levels of occlusion.",
Wirelessly-Charged UHF Tags for Sensor Data Collection,"We present the WISP Passive Data Logger (PDL), an RFID sensor data logging platform that relies on a new, wirelessly-charged power model. A PDL has no battery yet (unlike a passive sensor tag) is able to collect data while away from an RFID reader. A PDL senses and logs data using energy stored in a capacitor; the capacitor can be wirelessly recharged (unlike active tags), and data can be uploaded whenever the PDL is near a reader. Standard EPC Generation 2 readers are used for WISP-PDL charging, ID-reading, and sensor data transfer. This allows WISP-PDLs to operate using commercial RFID readers as the only support infrastructure (for both data and power), and allows WISP-PDLs to co-exist with standard RFID tags. We describe the design and implementation of a prototype WISP-PDL, and report results from a short demonstration study that shows it can monitor the temperature and fullness of a milk carton as it is used over the course of a day.","Radiofrequency identification,
Page description languages,
Batteries,
Temperature sensors,
Biomedical monitoring,
Wireless sensor networks,
Costs,
Capacitors,
Supply chains,
Passive RFID tags"
Stereoscopic inpainting: Joint color and depth completion from stereo images,"We present a novel algorithm for simultaneous color and depth inpainting. The algorithm takes stereo images and estimated disparity maps as input and fills in missing color and depth information introduced by occlusions or object removal. We first complete the disparities for the occlusion regions using a segmentation-based approach. The completed disparities can be used to facilitate the user in labeling objects to be removed. Since part of the removed regions in one image is visible in the other, we mutually complete the two images through 3D warping. Finally, we complete the remaining unknown regions using a depth-assisted texture synthesis technique, which simultaneously fills in both color and depth. We demonstrate the effectiveness of the proposed algorithm on several challenging data sets.","Layout,
Filling,
Image segmentation,
Algorithm design and analysis,
Image sampling,
Visualization,
Virtual environment,
Computer science,
Labeling,
Digital images"
On Gradient-Based Search for Multivariable System Estimates,"This paper addresses the design of gradient-based search algorithms for multivariable system estimation. In particular, the paper here considers so-called ldquofull parametrizationrdquo approaches, and establishes that the recently developed ldquodata-driven local coordinaterdquo methods can be seen as a special case within a broader class of techniques that are designed to deal with rank-deficient Jacobians. This informs the design of a new algorithm that, via a strategy of dynamic Jacobian rank determination, is illustrated to offer enhanced performance.",
Recommending adaptive changes for framework evolution,"In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework adapts to its own changes. In a study of the evolution of the Eclipse JDT framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision, and detected non-trivial changes typically undiscovered by current refactoring detection techniques.","Software maintenance,
Computer science,
Humans,
Manuals,
Inspection,
Java,
Software engineering,
Documentation,
Application software,
Large-scale systems"
Toward a Revolution in Transportation Operations: AI for Complex Systems,"This article presents an parallel traffic management system for integrated control and management of urban transportation systems. This system's construction and operation are based on the ACP (artificial, computational, parallel) approach, which consists of modeling with artificial systems, analysis with computational experiments, and operation through parallel execution for control and management of complex systems with social and behavioral dimensions.","Transportation,
Artificial intelligence,
Traffic control,
Object oriented modeling,
Computational modeling,
Computer architecture,
Environmental economics,
Conferences,
Testing,
Management training"
Revisiting the Core Ontology and Problem in Requirements Engineering,"In their seminal paper in the ACM Transactions on Software Engineering and Methodology, Zave and Jackson established a core ontology for Requirements Engineering (RE) and used it to formulate the ""requirements problem"", thereby defining what it means to successfully complete RE. Given that stakeholders of the system-to-be communicate the information needed to perform RE, we show that Zave and Jackson's ontology is incomplete. It does not cover all types of basic concerns that the stakeholders communicate. These include beliefs, desires, intentions, and attitudes. In response, we propose a core ontology that covers these concerns and is grounded in sound conceptual foundations resting on a foundational ontology. The new core ontology for RE leads to a new formulation of the requirements problem that extends Zave and Jackson's formulation. We thereby establish new standards for what minimum information should be represented in RE languages and new criteria for determining whether RE has been successfully completed.",
"Variational Bayesian Image Restoration Based on a Product of
t
-Distributions Image Prior","Image priors based on products have been recognized to offer many advantages because they allow simultaneous enforcement of multiple constraints. However, they are inconvenient for Bayesian inference because it is hard to find their normalization constant in closed form. In this paper, a new Bayesian algorithm is proposed for the image restoration problem that bypasses this difficulty. An image prior is defined by imposing Student-t densities on the outputs of local convolutional filters. A variational methodology, with a constrained expectation step, is used to infer the restored image. Numerical experiments are shown that compare this methodology to previous ones and demonstrate its advantages.",
Generalized Features for Electrocorticographic BCIs,"This paper studies classifiability of electrocorticographic signals (ECoG) for use in a human brain-computer interface (BCI). The results show that certain spectral features can be reliably used across several subjects to accurately classify different types of movements. Sparse and nonsparse versions of the support vector machine and regularized linear discriminant analysis linear classifiers are assessed and contrasted for the classification problem. In conjunction with a careful choice of features, the classification process automatically and consistently identifies neurophysiological areas known to be involved in the movements. An average two-class classification accuracy of 95% for real movement and around 80% for imagined movement is shown. The high accuracy and generalizability of these results, obtained with as few as 30 data samples per class, support the use of classification methods for ECoG-based BCIs.",
Efficient Merging and Filtering Algorithms for Approximate String Searches,"We study the following problem: how to efficiently find in a collection of strings those similar to a given query string? Various similarity functions can be used, such as edit distance, Jaccard similarity, and cosine similarity. This problem is of great interests to a variety of applications that need a high real-time performance, such as data cleaning, query relaxation, and spellchecking. Several algorithms have been proposed based on the idea of merging inverted lists of grams generated from the strings. In this paper we make two contributions. First, we develop several algorithms that can greatly improve the performance of existing algorithms. Second, we study how to integrate existing filtering techniques with these algorithms, and show that they should be used together judiciously, since the way to do the integration can greatly affect the performance. We have conducted experiments on several real data sets to evaluate the proposed techniques.","Merging,
Filtering algorithms,
Cleaning,
Dictionaries,
Computer science,
Databases,
Management information systems,
Postal services"
In Vivo MR-Tracking Based on Magnetic Signature Selective Excitation,"A novel magnetic resonance (MR)-tracking method specifically developed to locate the ferromagnetic core of an untethered microdevice, microrobot, or nanorobot for navigation or closed-loop control purpose is described. The tracking method relies on the application of radio-frequency (RF) excitation signals tuned to the equipotential magnetic curves generated by the magnetic signature of the object being tracked. Positive contrast projections are obtained with reference to the position of the magnetic source. A correlation function performed on only one k-space line for each of the three axes and corresponding to three projections, is necessary to obtain a 3-D location of the device. In this study, the effects of the sphere size and the RF frequency offset were investigated in order to find the best contrast noise ratio (CNR) for tracking. Resolution and precision were also investigated by proper measurement of the position of a ferromagnetic sphere by magnetic resonance imaging (MRI) acquisition and by comparing them with the real position. This method is also tested for a moving marker where the positions found by MRI projections were compared with the ones taken with a camera. In vitro and in vivo experiments show the operation of the technique in tortuous phantom and in animal models. Although the method was developed in the prospect of new interventional MR-guided endovascular operations based on miniature untethered devices, it could also be used as a passive tracking method using tools such as catheters or guide wires.","In vivo,
Radio frequency,
Magnetic resonance imaging,
Magnetic resonance,
Magnetic cores,
Radio navigation,
RF signals,
Signal generators,
Magnetic noise,
Signal to noise ratio"
Distributed Cognition as a Theoretical Framework for Information Visualization,"Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.",
On the Class Imbalance Problem,"The class imbalance problem has been recognized in many practical domains and a hot topic of machine learning in recent years. In such a problem, almost all the examples are labeled as one class, while far fewer examples are labeled as the other class, usually the more important class. In this case, standard machine learning algorithms tend to be overwhelmed by the majority class and ignore the minority class since traditional classifiers seeking an accurate performance over a full range of instances. This paper reviewed academic activities special for the class imbalance problem firstly. Then investigated various remedies in four different levels according to learning phases. Following surveying evaluation metrics and some other related factors, this paper showed some future directions at last.","Intrusion detection,
Machine learning algorithms,
Machine learning,
Computer science,
Radar detection,
Fault detection,
Diseases,
Biomedical monitoring,
Condition monitoring,
Data mining"
fuzzyDL: An expressive fuzzy description logic reasoner,"In this paper we present fuzzyDL, an expressive fuzzy description logic reasoner.We present its salient features, including some novel concept constructs and queries, and examples of use cases: matchmaking and fuzzy control.","OWL,
Computational modeling,
Humans,
Reactive power,
Fuzzy logic,
Fuzzy set theory,
Moment methods"
Efficient nonlocal-means denoising using the SVD,"Nonlocal-means (NL-means) is an image denoising method that replaces each pixel by a weighted average of all the pixels in the image. Unfortunately, the method requires the computation of the weighting terms for all possible pairs of pixels, making it computationally expensive. Some short-cuts assign a weight of zero to any pixel pairs whose neighbourhood averages are too dissimliar. In this paper, we propose an alternative strategy that uses the SVD to more efficiently eliminate pixel pairs that are dissimilar. Experiments comparing this method against other NL-means speed-up strategies show that its refined discrimination between similar and dissimilar pixel neighbourhoods significantly improves the denoising effect.","Noise reduction,
Pixel,
Additive white noise,
Filters,
Image denoising,
Computer science,
Mathematics,
Design engineering,
Systems engineering and theory,
Gaussian noise"
Test-access mechanism optimization for core-based three-dimensional SOCs,"Test-access mechanisms (TAMs) and test wrappers (e.g., the IEEE Standard 1500 wrapper) facilitate the modular testing of embedded cores in a core-based system-on-chip (SOC). Such a modular testing approach can also be used for emerging three-dimensional integrated circuits based on through-silicon vias (TSVs). Core-based SOCs based on 3D IC technology are being advocated as a means to continue technology scaling and overcome interconnect-related bottlenecks. We present an optimization technique for minimizing the test time for 3D core-based SOCs under constraints on the number of TSVs and the TAM bitwidth. The proposed optimization method is based on a combination of integer linear programming, LP-relaxation, and randomized rounding. Simulation results are presented for the ITC 02 SOC Test Benchmarks and the test times are compared to that obtained when methods developed earlier for two-dimensional ICs are applied to 3D ICs.",
A region based stereo matching algorithm using cooperative optimization,"This paper presents a new stereo matching algorithm based on inter-regional cooperative optimization. The proposed algorithm uses regions as matching primitives and defines the corresponding region energy functional for matching by utilizing the color Statistics of regions and the constraints on smoothness and occlusion between adjacent regions. In order to obtain a more reasonable disparity map, a cooperative optimization procedure has been employed to minimize the matching costs of all regions by introducing the cooperative and competitive mechanism between regions. Firstly, a color based segmentation method is used to segment the reference image into regions with homogeneous color. Secondly, a local window-based matching method is used to determine the initial disparity estimate of each image pixel. And then, a voting based plane fitting technique is applied to obtain the parameters of disparity plane corresponding to each image region. Finally, the disparity plane parameters of all regions are iteratively optimized by an inter-regional cooperative optimization procedure until a reasonable disparity map is obtained. The experimental Results on Middlebury test set and real stereo images indicate that the performance of our method is competitive with the best stereo matching algorithms and the disparity maps recovered are close to the ground truth data.","Stereo vision,
Image segmentation,
Clustering algorithms,
Statistics,
Cost function,
Layout,
Labeling,
Pixel,
Voting,
Testing"
When HART goes wireless: Understanding and implementing the WirelessHART standard,"As a newly released industrial communication standard, WirelessHART complements the ever so successful HART field devices by providing the possible means for communicating via wireless channels. The WirelessHART standard is designed to offer simple configuration, flexible installation and easy access of instrument data, and at the same time, ensure robust and reliable communications. In this paper, we first look closely into the specifications and present a comprehensive overview of the standard by summarizing the main functions of the various protocol layers. We then survey the literature and identify amongst the existing methods and algorithms, which ones can be effectively adopted in implementing the standard. More specifically, we set our focus on issues relating to realization of the medium access layer and the network manager, which are essential in creating a successful WirelessHART network for specific applications.","Wireless communication,
Logic gates,
Routing,
Zigbee,
Synchronization,
Protocols,
Security"
Understanding the High-Performance-Computing Community: A Software Engineer's Perspective,Computational scientists developing software for HPC systems face unique software engineering issues. Attempts to transfer SE technologies to this domain must take these issues into account.,"Educational institutions,
Management training,
World Wide Web,
Programming profession,
Productivity,
Software systems,
Software engineering,
Writing,
Software development management,
Supercomputers"
Adaptive SPECT,"Adaptive imaging systems alter their data-acquisition configuration or protocol in response to the image information received. An adaptive pinhole single-photon emission computed tomography (SPECT) system might acquire an initial scout image to obtain preliminary information about the radiotracer distribution and then adjust the configuration or sizes of the pinholes, the magnifications, or the projection angles in order to improve performance. This paper briefly describes two small-animal SPECT systems that allow this flexibility and then presents a framework for evaluating adaptive systems in general, and adaptive SPECT systems in particular. The evaluation is in terms of the performance of linear observers on detection or estimation tasks. Expressions are derived for the ideal linear (Hotelling) observer and the ideal linear (Wiener) estimator with adaptive imaging. Detailed expressions for the performance figures of merit are given, and possible adaptation rules are discussed.","Optical imaging,
Adaptive optics,
Computed tomography,
Optical distortion,
Nuclear imaging,
Radiology,
Biomedical optical imaging,
Optical sensors,
Adaptive systems,
High-resolution imaging"
EKG-based key agreement in Body Sensor Networks,"Preserving a person’s privacy in an efficient manner is very important for critical, life-saving infrastructures like Body Sensor Networks (BSN). This paper presents a novel key agreement scheme which allows two sensors in a BSN to agree to a common key generated using electrocardiogram (EKG) signals. This EKG-based Key Agreement (EKA) scheme aims to bring the “plug-n-play” paradigm to BSN security whereby simply deploying sensors on the subject can enable secure communication, without requiring any form of initialization such as pre-deployment. Analysis of the scheme based on real EKG data (obtained from MIT PhysioBank database) shows that keys resulting from EKA are: random, time variant, can be generated based on short-duration EKG measurements, identical for a given subject and different for separate individuals.","Body sensor networks,
Data security,
Signal generators,
Patient monitoring,
Cryptography,
Protection,
Communication system security,
Delay,
Computer science,
Privacy"
Breaking the simulation barrier: SRAM evaluation through norm minimization,"With process variation becoming a growing concern in deep submicron technologies, the ability to efficiently obtain an accurate estimate of failure probability of SRAM components is becoming a central issue. In this paper we present a general methodology for a fast and accurate evaluation of the failure probability of memory designs. The proposed statistical method, which we call importance sampling through norm minimization principle, reduces the variance of the estimator to produce quick estimates. It builds upon the importance sampling, while using a novel norm minimization principle inspired by the classical theory of Large Deviations. Our method can be applied for a wide class of problems, and our illustrative examples are the data retention voltage and the read/write failure tradeoff for 6T SRAM in 32 nm technology. The method yields computational savings on the order of 10000x over the standard Monte Carlo approach in the context of failure probability estimation for SRAM considered in this paper.","Random access memory,
Probability,
Monte Carlo methods,
Statistical analysis,
Discrete event simulation,
Failure analysis,
Computational modeling,
Computer simulation,
Minimization methods,
Voltage"
Effect of Overlapping Projections on Reconstruction Image Quality in Multipinhole SPECT,"Multipinhole single photon emission computed tomography (SPECT) imaging has several advantages over single pinhole SPECT imaging, including an increased sensitivity and an improved sampling. However, the quest for a good design is challenging, due to the large number of design parameters. The effect of one of these, the amount of overlap in the projection images, on the reconstruction image quality, is examined in this paper. The evaluation of the quality is based on efficient approximations for the linearized local impulse response and the covariance in a voxel, and on the bias of the reconstruction of the noiseless projection data. Two methods are proposed that remove the overlap in the projection image by blocking certain projection rays with the use of extra shielding between the pinhole plate and the detector. Also two measures to quantify the amount of overlap are suggested. First, the approximate method, predicting the contrast-to-noise ratio (CNR), is validated using postsmoothed maximum likelihood expectation maximization (MLEM) reconstructions with an imposed target resolution. Second, designs with different amounts of overlap are evaluated to study the effect of multiplexing. In addition, the CNR of each pinhole design is also compared with that of the same design where overlap is removed. Third, the results are interpreted with the overlap quantification measures. Fourth, the two proposed overlap removal methods are compared. From the results we can conclude that, once the complete detector area has been used, the extra sensitivity due to multiplexing is only able to compensate for the loss of information, not to improve the CNR. Removing the overlap, however, improves the CNR. The gain is most prominent in the central field of view, though often at the cost of the CNR of some voxels at the edges, since after overlap removal very little information is left for their reconstruction. The reconstruction images provide insight in the multiplexing and truncation artifacts.","Image reconstruction,
Image quality,
Detectors,
Apertures,
Collimators,
Single photon emission computed tomography,
Nuclear medicine,
Pixel,
Image sampling,
Linear approximation"
Using likely program invariants to detect hardware errors,"In the near future, hardware is expected to become increasingly vulnerable to faults due to continuously decreasing feature size. Software-level symptoms have previously been used to detect permanent hardware faults. However, they can not detect a small fraction of faults, which may lead to Silent Data Corruptions(SDCs). In this paper, we present a system that uses invariants to improve the coverage and latency of existing detection techniques for permanent faults. The basic idea is to use training inputs to create likely invariants based on value ranges of selected program variables and then use them to identify faults at runtime. Likely invariants, however, can have false positives which makes them challenging to use for permanent faults. We use our on-line diagnosis framework for detecting false positives at runtime and limit the number of false positives to keep the associated overhead minimal. Experimental results using microarchitecture level fault injections in full-system simulation show 28.6% reduction in the number of undetected faults and 74.2% reduction in the number of SDCs over existing techniques, with reasonable overhead for checking code.",
Provably Secure Constant Round Contributory Group Key Agreement in Dynamic Setting,"In this paper, we present and analyze a variant of Burmester-Desmedt group key agreement protocol (BD) and enhance it to dynamic setting where a set of users can leave or join the group at any time during protocol execution with updated keys. In contrast to BD protocol, let us refer to our protocol as DB protocol. Although the DB protocol is similar to BD protocol, there are subtle differences between them: 1) Key computation in DB protocol is different and simpler than in BD protocol with same complexity of BD protocol; 2) Number of rounds required in our authenticated DB protocol is one less than that in authenticated BD protocol introduced by Katz-Yung; 3) DB protocol is more flexible than BD protocol in the sense that DB protocol is dynamic. The reusability of user's precomputed data in previous session enables the join and leave algorithms of our DB protocol to reduce most user's computation complexities which can be useful in real life applications; and 4) DB protocol has the ability to detect the presence of corrupted group members, although one can not detect who among the group members are behaving improperly.","Cryptographic protocols,
Cryptography,
Fasteners,
Information security,
Computer science,
Communication standards,
Authentication,
Collaboration,
Tree graphs"
Using bandwidth data to make computation offloading decisions,"We present a framework for making computation offloading decisions in computational grid settings in which schedulers determine when to move parts of a computation to more capable resources to improve performance. Such schedulers must predict when an offloaded computation will outperform one that is local by forecasting the local cost (execution time for computing locally) and remote cost (execution time for computing remotely and transmission time for the input/output of the computation to/from the remote system). Typically, this decision amounts to predicting the bandwidth between the local and remote systems to estimate these costs. Our framework unifies such decision models by formulating the problem as a statistical decision problem that can either be treated “classically” or using a Bayesian approach. Using an implementation of this framework, we evaluate the efficacy of a number of different decision strategies (several of which have been employed by previous systems). Our results indicate that a Bayesian approach employing automatic change-point detection when estimating the prior distribution is the best-performing approach.","Bandwidth,
Grid computing,
Processor scheduling,
Costs,
Bayesian methods,
Time measurement,
Distributed computing,
Computer science,
Humans,
Performance gain"
DynaCAS: Computational Experiments and Decision Support for ITS,"Accurate, reliable, and timely traffic information is critical for deployment and operation of intelligent transportation systems (ITSs). Traffic forecasting for travelers and traffic operators should become at least as useful and convenient as weather reports. In the US, the Federal Highway Administration (FHWA) has envisioned a real-time traffic estimation and prediction system (TrEPS) as an ITS support platform that resides at traffic management centers (TMCs) for dynamic route assignment (DRA) and other transportation operations.",
Ultrasound Elastography: A Dynamic Programming Approach,"This paper introduces a 2D strain imaging technique based on minimizing a cost function using dynamic programming (DP). The cost function incorporates similarity of echo amplitudes and displacement continuity. Since tissue deformations are smooth, the incorporation of the smoothness into the cost function results in reduced decorrelation noise. As a result, the method generates high-quality strain images of freehand palpation elastography with up to 10% compression, showing that the method is more robust to signal decorrelation (caused by scatterer motion in high axial compression and nonaxial motions of the probe) in comparison to the standard correlation techniques. The method operates in less than 1 s and is thus also potentially suitable for real time elastography.","Ultrasonic imaging,
Dynamic programming,
Cost function,
Capacitive sensors,
Decorrelation,
Image coding,
Noise reduction,
Image generation,
Signal generators,
Noise robustness"
Deploying Four-Connectivity and Full-Coverage Wireless Sensor Networks,"We study the issue of optimal deployment to achieve four connectivity and full coverage for wireless sensor networks (WSNs) under different ratios of sensors' communication range (denoted by rc) to their sensing range (denoted by rs). We propose a ""Diamond"" pattern, which can be viewed as a series of different evolving patterns. When rc/rs ges radic3, the Diamond pattern coincides with the well-known triangle lattice pattern; when rc/rs ges radic2, it degenerates to a ""Square"" pattern. We prove the Diamond pattern to be asymptotically optimal when rc/rs ges radic2- Our work is the first to propose an asymptotically optimal deployment pattern to achieve four connectivity and full coverage for WSNs. We hope our work will provide some insights on how optimal patterns evolve and how to search for them.","Wireless sensor networks,
Computer science,
Lattices,
Acoustic sensors,
Communications Society,
USA Councils,
Mathematics,
Costs,
Heuristic algorithms,
Topology"
Simple calibration of non-overlapping cameras with a mirror,"Calibrating a network of cameras with non-overlapping views is an important and challenging problem in computer vision. In this paper, we present a novel technique for camera calibration using a planar mirror. We overcome the need for all cameras to see a common calibration object directly by allowing them to see it through a mirror. We use the fact that the mirrored views generate a family of mirrored camera poses that uniquely describe the real camera pose. Our method consists of the following two steps: (1) using standard calibration methods to find the internal and external parameters of a set of mirrored camera poses, (2) estimating the external parameters of the real cameras from their mirrored poses by formulating constraints between them. We demonstrate our method on real and synthetic data for camera clusters with small overlap between the views and non-overlapping views.","Calibration,
Cameras,
Mirrors,
Computer vision,
Computer science,
Application software,
Nonlinear distortion,
Parameter estimation,
Image reconstruction,
Motion estimation"
Mechanical Imaging of the Breast,"In this paper, we analyze the physical basis for elasticity imaging of the breast by measuring breast skin stress patterns that result from a force sensor array pressed against the breast tissue. Temporal and spatial changes in the stress pattern allow detection of internal structures with different elastic properties and assessment of geometrical and mechanical parameters of these structures. The method entitled mechanical imaging is implemented in the breast mechanical imager (BMI), a compact device consisting of a hand held probe equipped with a pressure sensor array, a compact electronic unit, and a touchscreen laptop computer. Data acquired by the BMI allows calculation of size, shape, consistency/hardness, and mobility of detected lesions. The BMI prototype has been validated in laboratory experiments on tissue models and in an ongoing clinical study. The obtained results prove that the BMI has potential to become a screening and diagnostic tool that could largely supplant clinical breast examination through its higher sensitivity, quantitative record storage, ease-of-use, and inherent low cost.","Breast,
Sensor arrays,
Image analysis,
Pattern analysis,
Elasticity,
Mechanical variables measurement,
Force measurement,
Stress measurement,
Skin,
Force sensors"
Image Stitching Using Structure Deformation,"The aim of this paper is to achieve seamless image stitching without producing visual artifact caused by severe intensity discrepancy and structure misalignment, given that the input images are roughly aligned or globally registered. Our new approach is based on structure deformation and propagation for achieving the overall consistency in image structure and intensity. The new stitching algorithm, which has found applications in image compositing, image blending, and intensity correction, consists of the following main processes. Depending on the compatibility and distinctiveness of the 2D features detected in the image plane, single or double optimal partitions are computed subject to the constraints of intensity coherence and structure continuity. Afterwards, specific 1D features are detected along the computed optimal partitions from which a set of sparse deformation vectors is derived to encode 1D feature matching between the partitions. These sparse deformation cues are robustly propagated into the input images by solving the associated minimization problem in gradient domain, thus providing a uniform framework for the simultaneous alignment of image structure and intensity. We present results in general image compositing and blending in order to show the effectiveness of our method in producing seamless stitching results from complex input images.",
Enhancement of Student Learning in Experimental Design Using a Virtual Laboratory,"This paper describes the instructional design, implementation, and assessment of a virtual laboratory based on a numerical simulation of a chemical vapor deposition (CVD) process, the virtual CVD laboratory. The virtual CVD laboratory provides a capstone experience in which students synthesize engineering science and statistics principles and have the opportunity to apply experimental design in the context similar to that of a practicing engineer in industry with a wider design space than is typically seen in the undergraduate laboratory. The simulation of the reactor is based on fundamental principles of mass transfer and chemical reaction, obscured by added ldquonoise.rdquo The software application contains a 3-D student client that simulates a cleanroom environment, an instructor Web interface with integrated assessment tools, and a database server. As opposed to being constructed as a direct one-to-one replacement, this virtual laboratory is intended to complement the physical laboratories in the curriculum so that certain specific elements of student learning can be enhanced. Implementation in four classes is described. Assessment demonstrates students are using an iterative experimental design process reflective of practicing engineers and correlates success in this project to higher order thinking skills. Student surveys indicate that students perceived the virtual CVD laboratory as the most effective learning medium used, even above physical laboratories.","Laboratories,
Inductors,
Mathematical model,
Industries,
Chemical vapor deposition,
Semiconductor process modeling,
Materials,
Chemicals"
Classification of Dynamic Contrast-Enhanced Magnetic Resonance Breast Lesions by Support Vector Machines,"Early detection of breast cancer is one of the most important factors in determining prognosis for women with malignant tumors. Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has been shown to be the most sensitive modality for screening high-risk women. Computer-aided diagnosis (CAD) systems have the potential to assist radiologists in the early detection of cancer. A key component of the development of such a CAD system will be the selection of an appropriate classification function responsible for separating malignant and benign lesions. The purpose of this study is to evaluate the effects of variations in temporal feature vectors and kernel functions on the separation of malignant and benign DCE-MRI breast lesions by support vector machines (SVMs). We also propose and demonstrate a classifier visualization and evaluation technique. We show that SVMs provide an effective and flexible framework from which to base CAD techniques for breast MRI, and that the proposed classifier visualization technique has potential as a mechanism for the evaluation of classification solutions.",
SwisTrack - a flexible open source tracking software for multi-agent systems,"Vision-based tracking is used in nearly all robotic laboratories for monitoring and extracting of agent positions, orientations, and trajectories. However, there is currently no accepted standard software solution available, so many research groups resort to developing and using their own custom software. In this paper, we present version 4 of SwisTrack, an open source project for simultaneous tracking of multiple agents. While its broad range of pre-implemented algorithmic components allows it to be used in a variety of experimental applications, its novelty stands in its highly modular architecture. Advanced users can therefore also implement additional customized modules which extend the functionality of the existing components within the provided interface. This paper introduces SwisTrack and shows experiments with both marked and marker-less agents.","Robots,
Cameras,
Robot vision systems,
Gray-scale,
Pixel,
Pipelines,
Calibration"
Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface,"In motor imagery-based Brain Computer Interfaces (BCI), discriminative patterns can be extracted from the electroencephalogram (EEG) using the Common Spatial Pattern (CSP) algorithm. However, the performance of this spatial filter depends on the operational frequency band of the EEG. Thus, setting a broad frequency range, or manually selecting a subject-specific frequency range, are commonly used with the CSP algorithm. To address this problem, this paper proposes a novel Filter Bank Common Spatial Pattern (FBCSP) to perform autonomous selection of key temporal-spatial discriminative EEG characteristics. After the EEG measurements have been bandpass-filtered into multiple frequency bands, CSP features are extracted from each of these bands. A feature selection algorithm is then used to automatically select discriminative pairs of frequency bands and corresponding CSP features. A classification algorithm is subsequently used to classify the CSP features. A study is conducted to assess the performance of a selection of feature selection and classification algorithms for use with the FBCSP. Extensive experimental results are presented on a publicly available dataset as well as data collected from healthy subjects and unilaterally paralyzed stroke patients. The results show that FBCSP, using a particular combination feature selection and classification algorithm, yields relatively higher cross-validation accuracies compared to prevailing approaches.","Classification algorithms,
Filter bank,
Filtering algorithms,
Electroencephalography,
Band pass filters,
Gabor filters,
Accuracy"
Image Segmentation Based on 2D Otsu Method with Histogram Analysis,"Image segmentation plays an important role in image analysis and computer vision system. Among all segmentation techniques, the automatic thresholding methods are widely used because of their advantages of simple implement and time saving. Otsu method is one of thresholding methods and frequently used in various fields. Two-dimensional (2D) Otsu method behaves well in segmenting images of low signal-to-noise ratio than one-dimensional (1D). But it gives satisfactory results only when the numbers of pixels in each class are close to each other. Otherwise, it gives the improper results. In this paper, 2D histogram projection is used to correct the Otsu threshold. The 1D histograms are acquired by 2D histogram projection in x and y axes and a fast algorithm for searching the extrema of the projected histogram is proposed based on the wavelet transform in this paper. Experimental results show that the proposed method performs better than the traditional Otsu method for our renal biopsy samples.","Image segmentation,
Histograms,
Image analysis,
Wavelet transforms,
Computer vision,
Biopsy,
Entropy,
Computer science,
Software engineering,
Information analysis"
Multilevel cooperative coevolution for large scale optimization,"In this paper, we propose a multilevel cooperative coevolution (MLCC) framework for large scale optimization problems. The motivation is to improve our previous work on grouping based cooperative coevolution (EACC-G) [1], which has a hard-to-determine parameter, group size, in tackling problem decomposition. The problem decomposer takes group size as parameter to divide the objective vector into low dimensional subcomponents with a random grouping strategy. In the MLCC, a set of problem decomposers is constructed based on the random grouping strategy with different group sizes. The evolution process is divided into a number of cycles, and at the start of each cycle MLCC uses a self-adapted mechanism to select a decomposer according to its historical performance. Since different group sizes capture different interaction levels between the original objective variables, MLCC is able to self-adapt among different levels. The efficacy of the proposed MLCC is evaluated on the set of benchmark functions provided by CEC’2008 special session [2].","Optimization,
Evolution (biology),
Evolutionary computation,
Scalability,
Computer science,
Benchmark testing,
Algorithms"
Wavelet-domain compressive signal reconstruction using a Hidden Markov Tree model,"Compressive sensing aims to recover a sparse or compressible signal froma small set of projections onto random vectors; conventional solutions involve linear programming or greedy algorithms that can be computationally expensive. Moreover, these recovery techniques are generic and assume no particular structure in the signal aside from sparsity. In this paper, we propose a new algorithm that enables fast recovery of piecewise smooth signals, a large and useful class of signals whose sparse wavelet expansions feature a distinct “connected tree” structure. Our algorithm fuses recent results on iterative reweighted ℓ1-norm minimization with the wavelet Hidden Markov Tree model. The resulting optimization-based solver outperforms the standard compressive recovery algorithms as well as previously proposed wavelet-based recovery algorithms. As a bonus, the algorithm reduces the number of measurements necessary to achieve low-distortion reconstruction.","Hidden Markov models,
Signal reconstruction,
Iterative algorithms,
Wavelet coefficients,
Vectors,
Greedy algorithms,
Minimization methods,
Reconstruction algorithms,
Wavelet domain,
Linear programming"
A Fast Nonrigid Image Registration With Constraints on the Jacobian Using Large Scale Constrained Optimization,"This paper presents a new nonrigid monomodality image registration algorithm based on B-splines. The deformation is described by a cubic B-spline field and found by minimizing the energy between a reference image and a deformed version of a floating image. To penalize noninvertible transformation, we propose two different constraints on the Jacobian of the transformation and its derivatives. The problem is modeled by an inequality constrained optimization problem which is efficiently solved by a combination of the multipliers method and the L-BFGS algorithm to handle the large number of variables and constraints of the registration of 3-D images. Numerical experiments are presented on magnetic resonance images using synthetic deformations and atlas based segmentation.","Image registration,
Jacobian matrices,
Large-scale systems,
Constraint optimization,
Image segmentation,
Deformable models,
Image resolution,
Cost function,
Magnetic resonance,
Energy resolution"
"Warm or Cool, Large or Small? The Challenge of Thermal Displays","Thermal displays have been developed to present thermal cues to the hand to facilitate object recognition in virtual environments or in teleoperated robotic systems. This review focuses on this application domain of thermal displays and considers the models developed to simulate the thermal interaction between an object and the hand as they make contact. An overview of thermal perception and the mechanisms underlying the processing of thermal information is provided to give a framework for analyzing the design of thermal displays. The models developed to simulate thermal feedback are examined together with a description of the implementation of these models in thermal displays. The domains in which thermal displays have been used are described; this includes the simulation of material properties, the recreation of large-scale thermal effects in virtual environments, the encoding of abstract concepts and the use of thermal feedback in interactive art. The review concludes by considering the advantages and challenges associated with using thermal displays in these diverse areas.","Displays,
Virtual environment,
Feedback,
Object recognition,
Robots,
Information analysis,
Material properties,
Large-scale systems,
Encoding,
Art"
The effect of presence on human-robot interaction,"This study explores how a robot’s physical or virtual presence affects unconscious human perception of the robot as a social partner. Subjects collaborated on simple book-moving tasks with either a physically present humanoid robot or a video-displayed robot. Each task examined a single aspect of interaction: greetings, cooperation, trust, and personal space. Subjects readily greeted and cooperated with the robot in both conditions. However, subjects were more likely to fulfill an unusual instruction and to afford greater personal space to the robot in the physical condition than in the video-displayed condition. The same tendencies occurred when the virtual robot was supplemented by disambiguating 3-D information.",Robots
"Performance Comparison of VxWorks, Linux, RTAI, and Xenomai in a Hard Real-Time Application","We report on a set of performance measurements executed on VMEbus MVME5500 boards equipped with MPC7455 PowerPC processor, running four different operating systems: Wind River VxWorks, Linux, RTAI, and Xenomai. Some components of RTAI and Xenomai have been ported to the target architecture. Interrupt latency, rescheduling and inter-process communication times are compared in the framework of a sample real-time application. Performance measurements on Gigabit Ethernet network communication have also been carried out on the target boards. To this purpose, we have considered the Linux IP stack and RTnet, an open-source hard real-time network protocol stack for Xenomai and RTAI, which was ported to the considered architecture. Performance measurements show that the tested open-source software is suitable for hard real-time applications.","Linux,
Measurement,
Open source software,
Operating systems,
Rivers,
Delay,
Ethernet networks,
Protocols,
Computer architecture,
Software testing"
Texture-based Transfer Functions for Direct Volume Rendering,"Visualization of volumetric data faces the difficult task of finding effective parameters for the transfer functions. Those parameters can determine the effectiveness and accuracy of the visualization. Frequently, volumetric data includes multiple structures and features that need to be differentiated. However, if those features have the same intensity and gradient values, existing transfer functions are limited at effectively illustrating those similar features with different rendering properties. We introduce texture-based transfer functions for direct volume rendering. In our approach, the voxelpsilas resulting opacity and color are based on local textural properties rather than individual intensity values. For example, if the intensity values of the vessels are similar to those on the boundary of the lungs, our texture-based transfer function will analyze the textural properties in those regions and color them differently even though they have the same intensity values in the volume. The use of texture-based transfer functions has several benefits. First, structures and features with the same intensity and gradient values can be automatically visualized with different rendering properties. Second, segmentation or prior knowledge of the specific features within the volume is not required for classifying these features differently. Third, textural metrics can be combined and/or maximized to capture and better differentiate similar structures. We demonstrate our texture-based transfer function for direct volume rendering with synthetic and real-world medical data to show the strength of our technique.","Transfer functions,
Data visualization,
Biomedical imaging,
Image color analysis,
Computer science,
Lungs,
Statistical analysis,
Probes,
Image segmentation,
Visual system"
"Algorithm for X-ray Scatter, Beam-Hardening, and Beam Profile Correction in Diagnostic (Kilovoltage) and Treatment (Megavoltage) Cone Beam CT","Quantitative reconstruction of cone beam X-ray computed tomography (CT) datasets requires accurate modeling of scatter, beam-hardening, beam profile, and detector response. Typically, commercial imaging systems use fast empirical corrections that are designed to reduce visible artifacts due to incomplete modeling of the image formation process. In contrast, Monte Carlo (MC) methods are much more accurate but are relatively slow. Scatter kernel superposition (SKS) methods offer a balance between accuracy and computational practicality. We show how a single SKS algorithm can be employed to correct both kilovoltage (kV) energy (diagnostic) and megavoltage (MV) energy (treatment) X-ray images. Using MC models of kV and MV imaging systems, we map intensities recorded on an amorphous silicon flat panel detector to water-equivalent thicknesses (WETs). Scattergrams are derived from acquired projection images using scatter kernels indexed by the local WET values and are then iteratively refined using a scatter magnitude bounding scheme that allows the algorithm to accommodate the very high scatter-to-primary ratios encountered in kV imaging. The algorithm recovers radiological thicknesses to within 9% of the true value at both kV and megavolt energies. Nonuniformity in CT reconstructions of homogeneous phantoms is reduced by an average of 76% over a wide range of beam energies and phantom geometries.",
Bayesian Co-clustering,"In recent years, co-clustering has emerged as a powerful data mining tool that can analyze dyadic data connecting two entities. However, almost all existing co-clustering techniques are partitional, and allow individual rows and columns of a data matrix to belong to only one cluster. Several current applications, such as recommendation systems and market basket analysis, can substantially benefit from a mixed membership of rows and columns. In this paper, we present Bayesian co-clustering (BCC) models, that allow a mixed membership in row and column clusters. BCC maintains separate Dirichlet priors for rows and columns over the mixed membership and assumes each observation to be generated by an exponential family distribution corresponding to its row and column clusters. We propose a fast variational algorithm for inference and parameter estimation. The model is designed to naturally handle sparse matrices as the inference is done only based on the non-missing entries. In addition to finding a co-cluster structure in observations, the model outputs a low dimensional co-embedding, and accurately predicts missing values in the original matrix. We demonstrate the efficacy of the model through experiments on both simulated and real data.","Bayesian methods,
Clustering algorithms,
Data mining,
Sparse matrices,
Motion pictures,
Computer science,
Data engineering,
Power engineering and energy,
Cities and towns,
Inference algorithms"
DT-MRI Fiber Tracking: A Shortest Paths Approach,"We derive a new fiber tracking algorithm for DT-MRI that parts with the locally ldquogreedyrdquo paradigm intrinsic to conventional tracking algorithms. We demonstrate the ability to precisely reconstruct a diverse range of fiber trajectories in authentic and computer-generated DT-MRI data, for which well-known conventional tracking algorithms are shown to fail. Our approach is to pose fiber tracking as a problem in computing shortest paths in a weighted digraph. Voxels serve as vertices, and edges are included between neighboring voxels. We assign probabilities (weights) to edges using a Bayesian framework. Higher probabilities are assigned to edges that are aligned with fiber trajectories in their close proximity. We compute optimal paths of maximum probability using computationally scalable shortest path algorithms. The salient features of our approach are: global optimality-unlike conventional tracking algorithms, local errors do not accumulate and one ldquowrong-turnrdquo does not spell disaster; a target point is specified a priori; precise reconstruction is demonstrated for extremely low signal-to-noise ratio; impartiality to which of two endpoints is used as a seed; and, faster computation times than conventional all-paths tracking. We can use our new tracking algorithm in either a single-path tracking mode (deterministic tracking) or an all-paths tracking mode (probabilistic tracking).","Diffusion tensor imaging,
Target tracking,
Trajectory,
Magnetic resonance imaging,
Image reconstruction,
Bayesian methods,
Australia,
Eigenvalues and eigenfunctions,
Additive noise,
Signal to noise ratio"
Flexible Decoupled Transactional Memory Support,"A high-concurrency transactional memory (TM) implementation needs to track concurrent accesses, buffer speculative updates, and manage conflicts. We present a system, FlexTM (FLEXible Transactional Memory), that coordinates four decoupled hardware mechanisms: read and write signatures, which summarize per-thread access sets; per-thread conflict summary tables (CSTs), which identify the threads with which conflicts have occurred; Programmable Data Isolation, which maintains speculative updates in the local cache and employs a thread-private buffer (in virtual memory) in the rare event of overflow; and Alert-On-Update, which selectively notifies threads about coherence events. All mechanisms are software-accessible, to enable virtualization and to support transactions of arbitrary length. FlexTM allows software to determine when to manage conflicts (either eagerly or lazily), and to employ a variety of conflict management and commit protocols. We describe an STM-inspired protocol thatuses CSTs to manage conflicts in a distributed manner (no global arbitration) and allows parallel commits. In experiments with a prototype on Simics/GEMS, FlexTM exhibits 5x speedup over high-quality software TM, with no loss in policy flexibility. Its distributed commit protocol is also more efficient than a central hardware manager. Our results highlight the importance of flexibility in determining when to manage conflicts: lazy maximizes concurrency and helps to ensure forward progress while eager provides better overall utilization in a multi-programmed system.","Hardware,
Memory management,
Yarn,
Protocols,
Computer architecture,
Read-write memory,
Concurrent computing,
Computer science,
Software prototyping,
Delay systems"
A Sophisticated Parallel MLFMA for Scattering by Extremely Large Targets [EM Programmer's Notebook],"The development of the MLFMA for realistic targets is currently at the stage where clever implementation, using a series of tricks based on the characteristics of the MLFMA and current computer technology, is required. A sophisticated parallel MLFMA for use on distributed-memory computers is presented as a whole picture of developing the program using a style of multilevel development in this paper. A series of implementation tricks for the parallel MLFMA are analyzed and compared. Particularly, a novel trick for reducing the truncation numbers is presented for extremely large targets, in the paper. These tricks are integrated into a sophisticated parallel MLFMA. The memory requirement and the CPU time for each part in each level are analyzed numerically by typical numerical experiments. The capability of this sophisticated parallel MLFMA is demonstrated by computing scattering by a sphere with a diameter of 480 wavelengths, containing around 130 million unknowns, and for a plane model with a fuselage of more than 1000 wavelengths, containing more than 72 million unknowns. These are the largest scattering problems ever solved by full-wave numerical methods, to our knowledge.",
Real-time recognition of U.S. speed signs,"In this paper a camera-based system for detection, tracking, and classification of U.S. speed signs is presented. The implemented application uses multiple connected stages and iteratively reduces the number of pixels to process for recognition. Possible sign locations are detected using a fast, shape-based interest operator. Remaining objects other than speed signs are discarded using a classifier similar to the Viola-Jones detector. Classification results from tracked candidates are utilized to improve recognition accuracy. On a standard PC the system reached a detection speed of 27fps with an accuracy of 98.8%. Including classification, speed sign recognition rates of 96.3% were achieved with a frame rate of approximately 11fps and one false alarm every 42s.",Vehicles
A Charge-Based Model for Long-Channel Cylindrical Surrounding-Gate MOSFETs From Intrinsic Channel to Heavily Doped Body,"A charge-based model is presented for long-channel cylindrical surrounding-gate (SRG) MOSFETs from an intrinsic channel to a heavily doped body. The model derivation is based on an accurate inversion charge solution of Poisson's equation in a cylindrical coordinate system. The general drain-current equation is obtained from Pao-Sah's dual integral, which is expressed as a function of inversion charge at the source and drain terminals. The model is valid for all regions of operation without employing any smoothing function. The model has been extensively verified by numerical simulations with a wide range of SRG MOSFET geometry parameters and channel doping concentrations, including the undoped channel.",
ORBIT: A Multiresolution Framework for Deformable Registration of Brain Tumor Images,"A deformable registration method is proposed for registering a normal brain atlas with images of brain tumor patients. The registration is facilitated by first simulating the tumor mass effect in the normal atlas in order to create an atlas image that is as similar as possible to the patient's image. An optimization framework is used to optimize the location of tumor seed as well as other parameters of the tumor growth model, based on the pattern of deformation around the tumor region. In particular, the optimization is implemented in a multiresolution and hierarchical scheme, and it is accelerated by using a principal component analysis (PCA)-based model of tumor growth and mass effect, trained on a computationally more expensive biomechanical model. Validation on simulated and real images shows that the proposed registration framework, referred to as ORBIT (optimization of tumor parameters and registration of brain images with tumors), outperforms other available registration methods particularly for the regions close to the tumor, and it has the potential to assist in constructing statistical atlases from tumor-diseased brain images.","Image resolution,
Neoplasms,
Brain modeling,
Diseases,
Biomedical imaging,
Deformable models,
Predictive models,
Magnetic resonance imaging,
Image analysis,
Radiology"
On Reducing Mesh Delay for Peer-to-Peer Live Streaming,"Peer-to-peer (P2P) technology has emerged as a promising scalable solution for live streaming to large group. In this paper, we address the design of overlay which achieves low source-to-peer delay, is robust to user churn, accommodates of asymmetric and diverse uplink bandwidth, and continuously improves based on existing user pool. A natural choice is the use of mesh, where each peer is served by multiple parents. Since the peer delay in a mesh depends on its longest path through its parents, we study how to optimize such delay while meeting a certain streaming rate requirement. We first formulate the minimum delay mesh problem and show that it is NP-hard. Then we propose a centralized heuristic based on complete knowledge which serves as our benchmark and optimal solution for all the other schemes under comparison. Our heuristic makes use of the concept of power in network given by the ratio of throughput and delay. By maximizing the network power, our heuristic achieves very low delay. We then propose a simple distributed algorithm where peers select their parents based on the power concept. The algorithm makes continuous improvement on delay until some minimum delay is reached. Simulation results show that our distributed protocol performs close to the centralized one, and substantially outperforms traditional and state-of-the-art approaches.",
A fast method for designing time-optimal gradient waveforms for arbitrary k-space trajectories,"A fast and simple algorithm for designing time-optimal waveforms is presented. The algorithm accepts a given arbitrary multidimensional k-space trajectory as the input and outputs the time-optimal gradient waveform that traverses k-space along that path in minimum time. The algorithm is noniterative, and its run time is independent of the complexity of the curve, i.e., the number of switches between slew-rate limited acceleration, slew-rate limited deceleration, and gradient amplitude limited regions. The key in the method is that the gradient amplitude is designed as a function of arc length along the k-space trajectory, rather than as a function of time. Several trajectory design examples are presented.","Design methodology,
Magnetic resonance imaging,
Optimal control,
Magnetic resonance,
Hardware,
Acceleration,
Spirals,
Algorithm design and analysis,
Multidimensional systems,
Switches"
Electrically small supergain end-fire arrays,"The theory, computer simulations, and experimental measurements are presented for electrically small, two-element supergain arrays with near-optimal end-fire gains of 7 dB. We show how the difficulties of narrow tolerances, large mismatches, low radiation efficiencies, and reduced scattering of electrically small parasitic elements are overcome by using electrically small resonant antennas as the elements in both separately driven and singly driven (parasitic), two-element, electrically small supergain end-fire arrays. Although rapidly increasing narrow tolerances prevent the practical realization of the maximum theoretically possible end-fire gain of electrically small arrays with many elements, the theory and preliminary numerical simulations indicate that near-maximum supergains are also achievable in practice for electrically small arrays with three (and possibly more) resonant elements if the decreasing bandwidth with increasing number of elements can be tolerated.","Antenna arrays,
Gain,
Antenna measurements,
Dipole antennas,
Gain measurement,
Bandwidth"
Differential evolution particle swarm optimization for digital filter design,"In this paper, swarm and evolutionary algorithms have been applied for the design of digital filters. Particle swarm optimization (PSO) and differential evolution particle swarm optimization (DEPSO) have been used here for the design of linear phase finite impulse response (FIR) filters. Two different fitness functions have been studied and experimented, each having its own significance. The first study considers a fitness function based on the passband and stopband ripple, while the second study considers a fitness function based on the mean squared error between the actual and the ideal filter response. DEPSO seems to be promising tool for FIR filter design especially in a dynamic environment where filter coefficients have to be adapted and fast convergence is of importance.",
Ultrasonic monitoring of bone fracture healing,"Quantitative ultrasound has attracted significant interest in the evaluation of bone fracture healing. Animal and clinical studies have demonstrated that the propagation velocity across fractured bones can be used as an indicator of healing. Researchers have recently employed computational methods for modeling wave propagation in bones, aiming to gain insight into the underlying mechanisms of wave propagation and to further enhance the monitoring capabilities of ultrasound. In this paper, we review the relevant literature and present the current status of knowledge.","Monitoring,
Bones,
Ultrasonic imaging,
Medical diagnostic imaging,
Materials science and technology,
Delay,
Intelligent systems,
Information systems,
Computer science,
Ultrasonography"
Coordinated beamforming for the multi-cell multi-antenna wireless system,"In a conventional wireless cellular system, signal processing is performed on a per-cell basis; out-of-cell interference is treated as background noise. This paper considers the benefit of coordinating base-stations across multiple cells in a multi-antenna beamforming system, where multiple base-stations may jointly optimize their respective beamformers to improve the overall system performance. This paper focuses on a downlink scenario where each remote user is equipped with a single antenna, but where multiple remote users may be active simultaneously in each cell. The design criterion is the minimization of the total weighted transmitted power across the base-stations subject to signal-to-interference-and-noise-ratio (SINR) constraints at the remote users. The main contribution is a practical algorithm that is capable of finding the joint optimal beamformers for all base-stations globally and efficiently. The proposed algorithm is based on a generalization of uplink-downlink duality to the multi-cell setting using the Lagrangian duality theory. The algorithm also naturally leads to a distributed implementation. Simulation results show that a coordinated beamforming system can significantly outperform a conventional system with per-cell signal processing.",
Segmenting Lung Fields in Serial Chest Radiographs Using Both Population-Based and Patient-Specific Shape Statistics,"This paper presents a new deformable model using both population-based and patient-specific shape statistics to segment lung fields from serial chest radiographs. There are two novelties in the proposed deformable model. First, a modified scale invariant feature transform (SIFT) local descriptor, which is more distinctive than the general intensity and gradient features, is used to characterize the image features in the vicinity of each pixel. Second, the deformable contour is constrained by both population-based and patient-specific shape statistics, and it yields more robust and accurate segmentation of lung fields for serial chest radiographs. In particular, for segmenting the initial time-point images, the population-based shape statistics is used to constrain the deformable contour; as more subsequent images of the same patient are acquired, the patient-specific shape statistics online collected from the previous segmentation results gradually takes more roles. Thus, this patient-specific shape statistics is updated each time when a new segmentation result is obtained, and it is further used to refine the segmentation results of all the available time-point images. Experimental results show that the proposed method is more robust and accurate than other active shape models in segmenting the lung fields from serial chest radiographs.",
Automated Topographic Segmentation and Transit Time Estimation in Endoscopic Capsule Exams,"Endoscopic capsule is a recent medical technology with important clinical benefits but suffering from a practical handicap: long exam annotation times. This paper proposes and compares two approaches (Bayesian and support vector machines) that can be used to segment the gastrointestinal tract into its four major topographic areas, allowing the automatic estimation of the clinically relevant gastric and intestinal sections and corresponding transit times. According to medical specialists, this can reduce exam annotation times by up to 12% (15 min). This automatic tool has been integrated into our CapView annotation software that is currently being used by three medical institutions.","Endoscopes,
Gastrointestinal tract,
Intestines,
Optical transmitters,
Biomedical imaging,
Lenses,
Batteries,
Support vector machines,
Optical imaging,
Frequency"
Bag-of-visual-words models for adult image classification and filtering,"We present a method to classify images into different categories of pornographic content to create a system for filtering pornographic images from network traffic. Although different systems for this application were presented in the past, most of these systems are based on simple skin colour features and have rather poor performance. Recent advances in the image recognition field in particular for the classification of objects have shown that bag-of-visual-words-approaches are a good method for many image classification problems. The system we present here, is based on this approach, uses a task-specific visual vocabulary and is trained and evaluated on an image database of 8500 images from different categories. It is shown that it clearly outperforms earlier systems on this dataset and further evaluation on two novel web-traffic collections shows the good performance of the proposed system.","Image classification,
Filters,
Vocabulary,
Filtering,
Skin,
Telecommunication traffic,
Image databases,
Histograms,
Feature extraction,
Iterative algorithms"
"Shape priors in variational image segmentation: Convexity, Lipschitz continuity and globally optimal solutions","In this work, we introduce a novel implicit representation of shape which is based on assigning to each pixel a probability that this pixel is inside the shape. This probabilistic representation of shape resolves two important drawbacks of alternative implicit shape representations such as the level set method: Firstly, the space of shapes is convex in the sense that arbitrary convex combinations of a set of shapes again correspond to a valid shape. Secondly, we prove that the introduction of shape priors into variational image segmentation leads to functionals which are convex with respect to shape deformations. For a large class of commonly considered (spatially continuous) functionals, we prove that — under mild regularity assumptions — segmentation and tracking with statistical shape priors can be performed in a globally optimal manner. In experiments on tracking a walking person through a cluttered scene we demonstrate the advantage of global versus local optimality.","Shape,
Image segmentation,
Level set,
Pixel,
Legged locomotion,
Probability,
Cost function,
Computer science,
Layout,
Background noise"
Estimating Location Using Wi-Fi,This department presents the results of the first Data Mining Contest held at the 2007 International Conference on Data Mining.,
Multifunctional Remote Laboratory for Education in Automatic Control: The CrAutoLab Experience,"This paper describes the experience gathered in the last years due to the creation and development of an automatic control laboratory suitable for remote use via the Web. First, the principles that guide the laboratory design and evolution are stated and motivated. Then, an overview of the laboratory is given, describing its architecture and the most important technological solutions adopted. The available experiments are described, and some pedagogical considerations are reported. Finally, some general ideas are presented that reflect the authors' experience and opinion on the use of remote laboratories in control education.","Remote laboratories,
Automatic control,
Control engineering education,
Computer science education,
Engineering education,
Educational technology,
Computer industry,
Industrial control,
Informatics,
Application software"
Modifiable Walking Pattern of a Humanoid Robot by Using Allowable ZMP Variation,"In order to handle complex navigational commands, this paper proposes a novel algorithm that can modify a walking period and a step length in both sagittal and lateral planes. By allowing a variation of zero moment point (ZMP) over the convex hull of foot polygon, it is possible to change the center of mass (CM) position and velocity independently throughout the single support phase. This permits a range of dynamic walking motion, which is not achievable using the 3-D linear inverted pendulum mode (3D-LIPM). In addition, the proposed algorithm enables to determine the dynamic feasibility of desired motion via the construction of feasible region, which is explicitly computed from the current CM state with simple ZMP functions. Moreover, adopting the closed-form functions makes it possible to calculate the algorithm in real time. The effectiveness of the proposed algorithm is demonstrated through both computer simulation and experiment on the humanoid robot, HanSaRam-VII, developed at the Robot Intelligence Technology (RIT) laboratory, Korea Advanced Institute of Science and Technology (KAIST).","Legged locomotion,
Humanoid robots,
Navigation,
Intelligent robots,
Trajectory,
Foot,
Dynamic range,
Computer simulation,
Laboratories,
Fast Fourier transforms"
Real-time crowd motion analysis,"Video-surveillance systems are becoming more and more autonomous in the detection and the reporting of abnormal events. In this context, this paper presents an approach to detect abnormal situations in crowded scenes by analyzing the motion aspect instead of tracking subjects one by one. The proposed approach estimates sudden changes and abnormal motion variations of a set of points of interest (POI). The number of tracked POIs is reduced using a mask that corresponds to hot areas of the built motion heat map. The approach detects events where local motion variation is important compared to previous events. Optical flow techniques are used to extract information such as density, direction and velocity. To demonstrate the interest of the approach, we present the results on the detection of collapsing events in real videos of airport escalator exits.",
Explicit Codes Achieving List Decoding Capacity: Error-Correction With Optimal Redundancy,"In this paper, we present error-correcting codes that achieve the information-theoretically best possible tradeoff between the rate and error-correction radius. Specifically, for every 0 < R < 1 and epsiv < 0, we present an explicit construction of error-correcting codes of rate that can be list decoded in polynomial time up to a fraction (1- R - epsiv) of worst-case errors. At least theoretically, this meets one of the central challenges in algorithmic coding theory. Our codes are simple to describe: they are folded Reed-Solomon codes, which are in fact exactly Reed-Solomon (RS) codes, but viewed as a code over a larger alphabet by careful bundling of codeword symbols. Given the ubiquity of RS codes, this is an appealing feature of our result, and in fact our methods directly yield better decoding algorithms for RS codes when errors occur in phased bursts. The alphabet size of these folded RS codes is polynomial in the block length. We are able to reduce this to a constant (depending on epsiv) using existing ideas concerning ldquolist recoveryrdquo and expander-based codes. Concatenating the folded RS codes with suitable inner codes, we get binary codes that can be efficiently decoded up to twice the radius achieved by the standard GMD decoding.","Decoding,
Redundancy,
Error correction codes,
Computer science,
Binary codes,
Code standards,
Background noise,
Upper bound,
Error correction,
Engineering profession"
A Statistical Model for Point-Based Target Registration Error With Anisotropic Fiducial Localizer Error,"Error models associated with point-based medical image registration problems were first introduced in the late 1990s. The concepts of fiducial localizer error, fiducial registration error, and target registration error are commonly used in the literature. The model for estimating the target registration error at a position r in a coordinate frame defined by a set of fiducial markers rigidly fixed relative to one another is ubiquitous in the medical imaging literature. The model has also been extended to simulate the target registration error at the point of interest in optically tracked tools. However, the model is limited to describing the error in situations where the fiducial localizer error is assumed to have an isotropic normal distribution in R3. In this work, the model is generalized to include a fiducial localizer error that has an anisotropic normal distribution. Similar to the previous models, the root mean square statistic rmstre is provided along with an extension that provides the covariance matrix Sigmatre. The new model is verified using a Monte Carlo simulation and a set of statistical hypothesis tests. Finally, the differences between the two assumptions, isotropic and anisotropic, are discussed within the context of their use in 1) optical tool tracking simulation and 2) image registration.","Anisotropic magnetoresistance,
Biomedical imaging,
Image registration,
Biomedical optical imaging,
Gaussian distribution,
Medical simulation,
Target tracking,
Root mean square,
Statistical distributions,
Covariance matrix"
Efficient Processing of Laser Speckle Contrast Images,"Though laser speckle contrast imaging enables the measurement of scattering particle dynamics with high temporal resolution, the subsequent processing has previously been much slower. In prior studies, generating a laser speckle contrast image required about 1 s to process a raw image potentially collected in 10 ms or less. In this paper, novel algorithms are described which are demonstrated to convert 291 raw images per second to laser speckle contrast images and as many as 410 laser speckle contrast images per second to relative correlation time images. As long as image processing occurs during image acquisition, these algorithms render processing time irrelevant in most circumstances and enable real-time imaging of blood flow dynamics.","Speckle,
Optical imaging,
High-resolution imaging,
Image resolution,
Image processing,
Blood flow,
Optical scattering,
Particle scattering,
Biomedical imaging,
Spatial resolution"
Dynamic 3-D Virtual Fixtures for Minimally Invasive Beating Heart Procedures,"Two-dimensional or 3-D visual guidance is often used for minimally invasive cardiac surgery and diagnosis. This visual guidance suffers from several drawbacks such as limited field of view, loss of signal from time to time, and in some cases, difficulty of interpretation. These limitations become more evident in beating-heart procedures when the surgeon has to perform a surgical procedure in the presence of heart motion. In this paper, we propose dynamic 3-D virtual fixtures (DVFs) to augment the visual guidance system with haptic feedback, to provide the surgeon with more helpful guidance by constraining the surgeon's hand motions thereby protecting sensitive structures. DVFs can be generated from preoperative dynamic magnetic resonance (MR) or computed tomograph (CT) images and then mapped to the patient during surgery. We have validated the feasibility of the proposed method on several simulated surgical tasks using a volunteer's cardiac image dataset. Validation results show that the integration of visual and haptic guidance can permit a user to perform surgical tasks more easily and with reduced error rate. We believe this is the first work presented in the field of virtual fixtures that explicitly considers heart motion.","Fixtures,
Minimally invasive surgery,
Heart,
Surge protection,
Haptic interfaces,
Feedback,
Magnetic resonance,
Computed tomography,
Computational modeling,
Error analysis"
An Efficient Syntactic Web Service Composition Algorithm Based on the Planning Graph Model,"In this paper, we have studied a common Web service composition problem, the syntactic matching problem, where the output parameters of a Web service can be used as the input parameters of another Web service. Many automatic Web service composition algorithms based on AI planning techniques have been proposed. However, most of them do not scale well when the number of Web services increases, or may miss finding a solution even if one exists. The planning graph, another AI planning technique, provides a unique search space. We have found that when we model the Web service composition problem as a planning graph, it actually provides a trivial solution to the problem. Instead of following the usual way to find a solution by a backward search, we put our efforts into removing the redundant Web services contained in the planning graph. Our approach can find a solution in polynomial time, but with possible redundant Web services. We have tested our algorithms on the data set used in ICEBE’05 and compared our results with existing methods.",
Degrees of freedom of the K user MIMO interference channel,"We provide innerbound and outerbound for the total number of degrees of freedom of the K user multiple input multiple output (MIMO) Gaussian interference channel with M antennas at each transmitter and N antennas at each receiver if the channel coefficients are time-varying and drawn from a continuous distribution. The bounds are tight when the ratio equation is equal to an integer. For this case, we show that the total number of degrees of freedom is equal to min(M, N)K if K ≤ R and equation if K ≫ R. Achievability is based on interference alignment.","MIMO,
Interference channels,
Receiving antennas,
Transmitting antennas,
Transmitters,
Computer science,
Wireless communication,
Cognition,
Tin"
Towards automated application signature generation for traffic identification,"Traditionally, Internet applications have been identified by using predefined well-known ports with questionable accuracy. An alternative approach, application-layer signature mapping, involves the exhaustive search of reliable signatures but with more promising accuracy. With a prior protocol knowledge, the signature generation can guarantee a high accuracy. As more applications use proprietary protocols, it becomes increasingly difficult to obtain an accurate signature while avoiding time-consuming and manual signature generation process. This paper proposes an automated approach for generating application-level signature, the LASER algorithm, that does not need to be preceded by an analysis of application protocols. We show that our approach is as accurate and efficient as the approach that uses preceding application protocol analysis.","Internet,
Protocols,
Telecommunication traffic,
Payloads,
Peer to peer computing,
Application software,
Computer science,
Spine,
Information science,
Reliability engineering"
FLoD: A Framework for Peer-to-Peer 3D Streaming,"Interactive 3D content on Internet has yet become popular due to its typically large volume and the limited network bandwidth. Progressive content transmission, or 3D streaming, thus is necessary to enable real-time content interactions. However, the heavy data and processing requirements of 3D streaming challenge the scalability of client-server delivery methods. We propose the use of peer-to-peer (P2P) networks for 3D streaming, and argue that due to the non-linear access patterns of 3D content, P2P 3D streaming is a new class of applications apart from existing media streaming and requires new investigations. We also present FLoD, the first P2P 3D streaming framework that allows clients of 3D virtual globe or virtual environment (VE) applications to obtain relevant data from other clients while minimizing server resource usage. To demonstrate how FLoD applies to real-world scenarios, we build a prototype system that adapts JPEG 2000-based 3D mesh streaming for P2P delivery. Experiments show that server-side bandwidth usage can thus be reduced, while simulations indicate that P2P 3D streaming is fundamentally more scalable than client-server approaches.","Peer to peer computing,
Streaming media,
Layout,
Bandwidth,
Computer science,
Virtual environment,
Navigation,
Earth,
Communications Society,
Information management"
System Calibration and Statistical Image Reconstruction for Ultra-High Resolution Stationary Pinhole SPECT,"For multipinhole single-photon emission computed tomography (SPECT), iterative reconstruction algorithms are preferred over analytical methods, because of the often complex multipinhole geometries and the ability of iterative algorithms to compensate for effects like spatially variant sensitivity and resolution. Ideally, such compensation methods are based on accurate knowledge of the position-dependent point spread functions (PSFs) specifying the response of the detectors to a point source at every position in the instrument. This paper describes a method for model-based generation of complete PSF lookup tables from a limited number of point-source measurements for stationary SPECT systems and its application to a submillimeter resolution stationary small-animal SPECT system containing 75 pinholes (U-SPECT-I). The method is based on the generalization over the entire object to be reconstructed, of a small number of properties of point-source responses which are obtained at a limited number of measurement positions. The full shape of measured point-source responses can almost be preserved in newly created PSF tables. We show that these PSFs can be used to obtain high-resolution SPECT reconstructions: the reconstructed resolutions judged by rod visibility in a micro-Derenzo phantom are 0.45 mm with 0.6-mm pinholes and below 0.35 mm with 0.3-mm pinholes. In addition, we show that different approximations, such as truncating the PSF kernel, with significant reduction of reconstruction time, can still lead to acceptable reconstructions.","Calibration,
Image reconstruction,
Image resolution,
Spatial resolution,
Shape measurement,
Computed tomography,
Iterative methods,
Reconstruction algorithms,
Algorithm design and analysis,
Computational geometry"
DCAR: Distributed Coding-Aware Routing in Wireless Networks,"The practical network coding system proposed in [1] has two fundamental limitations: 1) the coding opportunity is crucially dependent on the established routes; 2) the coding structure is limited within a two-hop region. To overcome these limitations, we propose DCAR, the first distributed coding-aware routing mechanism which combines (a) the discovery for available paths between a given source and destination, and (b) the detection for potential network coding opportunities. DCAR has the potential to find high throughput paths with coding opportunities while conventional routing fails to do so. In addition, DCAR can detect coding opportunities on the entire path, thus eliminating the ""two-hop"" coding limitation in [1]. We also propose a novel routing metric called ""CRM"" (Coding-aware Routing Metric) which facilitates the comparison between coding-possible and coding-impossible paths. We implement the DCAR system in NS-2 and conduct extensive evaluation, which shows that DCAR achieves 7\% to 20\% throughput gain over the coding system in [1].","Encoding,
Routing,
Throughput,
Wireless networks,
Bandwidth,
Decoding,
Gain"
Scalable Localization with Mobility Prediction for Underwater Sensor Networks,"Due to adverse aqueous environments, non-negligible node mobility and large network scale, localization for large-scale mobile underwater sensor networks is very challenging. In this paper, by utilizing the predictable mobility patterns of underwater objects, we propose a scheme, called Scalable Localization scheme with mobility prediction (SLMP), for underwater sensor networks. In SLMP, localization is performed in a hierarchical way, and the whole localization process is divided into two parts: anchor node localization and ordinary node localization. During the localization process, every node predicts its future mobility pattern according to its past known location information, and it can estimate its future location based on its predicted mobility pattern. Anchor nodes with known locations in the network will control the whole localization process in order to balance the tradeoff between localization accuracy, localization coverage and communication cost. We conduct extensive simulations, and our results show that SLMP can greatly reduce localization communication cost while maintaining relatively high localization coverage and localization accuracy.",
Approximation Algorithms for Computing Capacity of Wireless Networks with SINR Constraints,"A fundamental problem in wireless networks is to estimate its throughput capacity - given a set of wireless nodes, and a set of connections, what is the maximum rate at which data can be sent on these connections. Most of the research in this direction has focused on either random distributions of points, or has assumed simple graph-based models for wireless interference. In this paper, we study capacity estimation problem using the more general Signal to Interference Plus Noise Ratio (SINR) model for interference, on arbitrary wireless networks. The problem becomes much harder in this setting, because of the non-locality of the SINR model. Recent work by Moscibroda et al. (2006) has shown that the throughput in this model can differ from graph based models significantly. We develop polynomial time algorithms to provably approximate the total throughput in this setting.","Approximation algorithms,
Computer networks,
Wireless networks,
Signal to noise ratio,
Throughput,
Computer science,
Protocols,
Interference constraints,
Algorithm design and analysis,
Propagation losses"
The LiteOS Operating System: Towards Unix-Like Abstractions for Wireless Sensor Networks,"This paper presents LiteOS, a multi-threaded operating system that provides Unix-like abstractions for wireless sensor networks. Aiming to be an easy-to-use platform, LiteOS offers a number of novel features, including: (1) a hierarchical file system and a wireless shell interface for user interaction using UNIX-like commands; (2) kernel support for dynamic loading and native execution of multithreaded applications; and (3) online debugging, dynamic memory, and file system assisted communication stacks. LiteOS also supports software updates through a separation between the kernel and user applications, which are bridged through a suite of system calls. Besides the features that have been implemented, we also describe our perspective on LiteOS as an enabling platform. We evaluate the platform experimentally by measuring the performance of common tasks, and demonstrate its programmability through twenty-one example applications.","Operating systems,
Wireless sensor networks,
Sensor systems,
File systems,
Computer science,
Application software,
Linux,
Kernel,
Debugging,
Network operating systems"
Spectrum-blind sampling and compressive sensing for continuous-index signals,"Spectrum-blind sampling (SBS), proposed in the mid-90’s, is a sensing technique enabling minimum-rate sampling and reconstruction of signals with unknown but sparse spectra. SBS is applicable to continuous or discrete-index signals, finite or infinite length, in one or more dimensions. We revisit SBS and explore its relationship to compressive sensing (CS). On the one hand, recent results in CS provide efficient reconstruction techniques for SBS. On the other hand, SBS provides efficient structured designs for blind, non-adaptive sensing of spectrum-sparse signals with minimal sampling requirements, and formulation leading to reconstruction cost only linear in the amount of data, and robustness against noise.","Sensors,
Scattering,
Discrete Fourier transforms,
Time frequency analysis,
Filtering,
Eigenvalues and eigenfunctions,
Vectors"
Personalized Concept-Based Clustering of Search Engine Queries,"The exponential growth of information on the Web has introduced new challenges for building effective search engines. A major problem of Web search is that search queries are usually short and ambiguous, and thus are insufficient for specifying the precise user needs. To alleviate this problem, some search engines suggest terms that are semantically related to the submitted queries so that users can choose from the suggestions the ones that reflect their information needs. In this paper, we introduce an effective approach that captures the user's conceptual preferences in order to provide personalized query suggestions. We achieve this goal with two new strategies. First, we develop online techniques that extract concepts from the Web-snippets of the search result returned from a query and use the concepts to identify related queries for that query. Second, we propose a new two-phase personalized agglomerative clustering algorithm that is able to generate personalized query clusters. To the best of the authors' knowledge, no previous work has addressed personalization for query suggestions. To evaluate the effectiveness of our technique, a Google middleware was developed for collecting clickthrough data to conduct experimental evaluation. Experimental results show that our approach has better precision and recall than the existing query clustering methods.","Search engines,
Data mining,
Web search,
Clustering algorithms,
Middleware,
Clustering methods"
Locally Adjusted Robust Regression for Human Age Estimation,"Automatic human age estimation has considerable potential applications in human computer interaction and multimedia communication. However, the age estimation problem is challenging. We design a locally adjusted robust regressor (LARR) for learning and prediction of human ages. The novel approach reduces the age estimation errors significantly over all previous methods. Experiments on two aging databases show the success of the proposed method for human aging estimation.","Robustness,
Aging,
Human computer interaction,
Face,
Application software,
Multimedia communication,
Internet,
Image retrieval,
Shape measurement,
Training data"
"Model-based hand tracking with texture, shading and self-occlusions","A novel model-based approach to 3D hand tracking from monocular video is presented. The 3D hand pose, the hand texture and the illuminant are dynamically estimated through minimization of an objective function. Derived from an inverse problem formulation, the objective function enables explicit use of texture temporal continuity and shading information, while handling important self-occlusions and time-varying illumination. The minimization is done efficiently using a quasi-Newton method, for which we propose a rigorous derivation of the objective function gradient. Particular attention is given to terms related to the change of visibility near self-occlusion boundaries that are neglected in existing formulations. In doing so we introduce new occlusion forces and show that using all gradient terms greatly improves the performance of the method. Experimental results demonstrate the potential of the formulation.","Image motion analysis,
Optical sensors,
Lighting,
Tracking,
Surface texture,
Shape,
Computer science,
Inverse problems,
Minimization methods,
Man machine systems"
A study of finger vein biometric for personal identification,"Finger vein authentication can be a leading biometric technology nowadays in terms of security and convenience, since it introduces the features inside the human body. An image of a finger captured by the web camera under the IR light transmission contains not only the vein pattern itself, but also shade produced by various thickness of the finger muscles, bones, and tissue networks surrounding the vein. In this paper, we introduce preliminary process to enhance the image quality worsened by light effect and noise produced by the web camera, then segment the vein pattern by using adaptive threshold method and matched them using improved template matching. The experimental result shows that even the image quality is not good, as long as our veins are clear and also with some appropriate process it still can be used as the means of personal identification. Hence it still can achieve up to 100% identification accuracy.","Fingers,
Veins,
Biometrics,
Cameras,
Image quality,
Pattern matching,
Authentication,
Humans,
Muscles,
Bones"
"Real-Time Synchronization on Multiprocessors: To Block or Not to Block, to Suspend or Spin?","In the domain of multiprocessor real-time systems, there has been a wealth of recent work on scheduling, but relatively little work on the equally important topic of synchronization. When synchronizing accesses to shared resources, four basic options exist: lock-free execution, wait-free execution, spin-based locking, and suspension-based locking. To our knowledge, no empirical multiprocessor-based evaluation of these basic techniques that focuses on real-time systems has ever been conducted before. In this paper, we present such an evaluation and report on our efforts to incorporate synchronization support in the testbed used in this effort.","Real time systems,
Resource management,
Processor scheduling,
Scheduling algorithm,
Linux,
Multicore processing,
System testing,
Application software,
Computer science,
Concurrent computing"
Spot Me if You Can: Uncovering Spoken Phrases in Encrypted VoIP Conversations,"Despite the rapid adoption of Voice over IP (VoIP), its security implications are not yet fully understood. Since VoIP calls may traverse untrusted networks, packets should be encrypted to ensure confidentiality. However, we show that when the audio is encoded using variable bit rate codecs, the lengths of encrypted VoIP packets can be used to identify the phrases spoken within a call. Our results indicate that a passive observer can identify phrases from a standard speech corpus within encrypted calls with an average accuracy of 50%, and with accuracy greater than 90% for some phrases. Clearly, such an attack calls into question the efficacy of current VoIP encryption standards.  In addition, we examine the impact of various features of the underlying audio on our performance and discuss methods for mitigation.","Cryptography,
Codecs,
Natural languages,
Hidden Markov models,
Privacy,
Internet telephony,
Speech,
Computer security,
Bit rate,
Computer science"
Analysis and improvement of the consistency of extended Kalman filter based SLAM,"In this work, we study the inconsistency of EKF-based SLAM from the perspective of observability. We analytically prove that when the Jacobians of the state and measurement models are evaluated at the latest state estimates during every time step, the linearized error-state system model of the EKF-based SLAM has observable subspace of dimension higher than that of the actual, nonlinear, SLAM system. As a result, the covariance estimates of the EKF undergo reduction in directions of the state space where no information is available, which is a primary cause of inconsistency. To address this issue, a new “First Estimates Jacobian” (FEJ) EKF is proposed, which is shown to perform better in terms of consistency. In the FEJ-EKF, the filter Jacobians are calculated using the first-ever available estimates for each state variable, which insures that the observable subspace of the error-state system model is of the same dimension as that of the underlying nonlinear SLAM system. The theoretical analysis is validated through extensive simulations.","Simultaneous localization and mapping,
State estimation,
Observability,
Jacobian matrices,
Robots,
Filters,
Robotics and automation,
USA Councils,
Computer science,
Time measurement"
Dynamic Thermal Management through Task Scheduling,"The evolution of microprocessors has been hindered by their increasing power consumption and the heat generation speed on-die. High temperature impairs the processor's reliability and reduces its lifetime. While hardware level dynamic thermal management (DTM) techniques, such as voltage and frequency scaling, can effectively lower the chip temperature when it surpasses the thermal threshold, they inevitably come at the cost of performance degradation. We propose an OS level technique that performs thermal-aware job scheduling to reduce the number of thermal trespasses. Our scheduler reduces the amount of hardware DTMs and achieves higher performance while keeping the temperature low. Our methods leverage the natural discrepancies in thermal behavior among different workloads, and schedule them to keep the chip temperature below a given budget. We develop a heuristic algorithm based on the observation that there is a difference in the resulting temperature when a hot and a cool job are executed in a different order To evaluate our scheduling algorithms, we developed a lightweight runtime temperature monitor to enable informed scheduling decisions. We have implemented our scheduling algorithm and the entire temperature monitoring framework in the Linux kernel. Our proposed scheduler can remove 10.5-73.6% of the hardware DTMs in various combinations of workloads in a medium thermal environment. As a result, the CPU throughput was improved by up to 7.6% (4.1% on average) even under a severe thermal environment.","Thermal management,
Dynamic scheduling,
Hardware,
Scheduling algorithm,
Temperature measurement,
Temperature sensors,
Processor scheduling,
Microprocessors,
Energy consumption,
Power generation"
Debugging reinvented,"When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn't questions derived from the program's code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.",
Multibiometric Template Security Using Fuzzy Vault,"Template security is a critical issue in biometric systems because biometric templates cannot be easily revoked and reissued. While multibiometric systems overcome limitations such as non-universality and high error rates that affect unibiometric systems, they require storage of multiple templates for the same user. Securing the different templates of a user separately is not optimal in terms of security. Hence, we propose a scheme for securing multiple templates of a user as a single entity. We derive a single multibiometric template from the individual templates and secure it using the fuzzy vault framework. We demonstrate that a multibiometric vault provides better recognition performance and higher security compared to a unibiometric vault. For example, our multibiometric vault based on fingerprint and iris achieves a GAR of 98.2% at a FAR of ap 0.01%, while the corresponding GAR values of the individual iris and fingerprint vaults are 88% and 78.8%, respectively. Further, we also show that the security of the system is only 41 bits when the iris and fingerprint vaults are stored separately. On the other hand, the multibiometric vault based on fingerprint and iris provides 49 bits of security.","Biometrics,
Fingerprint recognition,
Iris,
Data security,
Protection,
Cryptography,
Fuzzy systems,
Information security,
Polynomials,
Error analysis"
Efficient Processing of Top-k Queries in Uncertain Databases with x-Relations,"This work introduces novel polynomial algorithms for processing top-k queries in uncertain databases under the generally adopted model of x-relations. An x-relation consists of a number of x-tuples, and each x-tuple randomly instantiates into one tuple from one or more alternatives. Our results significantly improve the best known algorithms for top-k query processing in uncertain databases, in terms of both runtime and memory usage. In the single-alternative case, the new algorithms are 2 to 3 orders of magnitude faster than the previous algorithms. In the multialternative case, we introduce the first-known polynomial algorithms, while the current best algorithms have exponential complexity in both time and space. Our algorithms run in near linear or low polynomial time and cover both types of top-k queries in uncertain databases. We provide both the theoretical analysis and an extensive experimental evaluation to demonstrate the superiority of the new approaches over existing solutions.","Databases,
Web server,
Power system modeling,
Polynomials,
Query processing,
Probability distribution,
Uncertainty,
Data models,
Runtime,
Cleaning"
Stand-alone wind system with Vanadium Redox Battery energy storage,"Energy storage devices are required for power balance and power quality in stand alone wind energy systems. A Vanadium Redox Flow Battery (VRB) system has many features which make its integration with a stand-alone wind energy system attractive. This paper proposes the integration of a VRB system with a typical stand-alone wind energy system during wind speed variation as well as transient performance under variable load. The investigated system consists of a variable speed wind turbine with permanent magnet synchronous generator (PMSG), diode rectifier bridge, buck-boost converter, bidirectional charge controller, transformer, inverter, ac loads and VRB (to store a surplus of wind energy and to supply power during a wind power shortage). The main purpose is to supply domestic appliances through a single phase 230V, 50Hz inverter. Simulations are accomplished in order to validate the stability of the supply.","Batteries,
Energy storage,
Wind speed,
Wind energy,
Wind turbines,
Discharges,
Load modeling"
Robust cooperative routing protocol in mobile wireless sensor networks,"In wireless sensor networks, path breakage occurs frequently due to node mobility, node failure, and channel impairments. It is challenging to combat path breakage with minimal control overhead, while adapting to rapid topological changes. Due to the Wireless Broadcast Advantage (WBA), all nodes inside the transmission range of a single transmitting node may receive the packet, hence naturally they can serve as cooperative caching and backup nodes if the intended receiver fails to receive the packet. In this paper, we present a distributed robust routing protocol in which nodes work cooperatively to enhance the robustness of routing against path breakage. We compare the energy efficiency of cooperative routing with noncooperative routing and show that our robust routing protocol can significantly improve robustness while achieving considerable energy efficiency.",
Cognitive radio modulation techniques,"The growing demand on wireless communication systems to provide high data rates has brought with it the need for a flexible and efficient use of the spectrum resource, which is a scarce commodity. The regional spectrum allocation policy counteracts the free mobility of radio communication equipment. The vast majority of the available spectral resources have already been licensed, so it appears that there is little or no room to add any new services, unless some of the existing licenses are discontinued. Furthermore, recent studies and measurements have shown that vast portions of the licensed spectra are rarely used due to the inflexible spectrum regulations. The whole idea behind cognitive radio (CR) use is that it should prompt effective spectrum use, since intelligence and learning processes aid the radio system to access the spectrum effectively. The CR system has learning and understanding capabilities so that the stated goals may be achieved. In this article, we shall limit the scope of cognition to reduce mutual interference between CR-based rental (unlicensed) users (RUs) and licensed users (LUs) and in providing coexistence between them. It is expected that the rental users will be allowed to transmit and receive data over portions of spectra when primary (i.e., licensed) users are inactive. In this article, we will introduce the modulation strategies employed to realize a coexistence between the CR-based rental system and the licensed system. This is done in such a way that the RUs are invisible to the LUs. We consider the rental user accesses the unoccupied LU band in overlay fashion.","Cognitive radio,
Chromium,
Wireless communication,
Interference,
Radio frequency,
Software radio,
Radio communication equipment,
Licenses,
Cognition,
Digital modulation"
Geometric Variability of the Scoliotic Spine Using Statistics on Articulated Shape Models,"This paper introduces a method to analyze the variability of the spine shape and of the spine shape deformations using articulated shape models. The spine shape was expressed as a vector of relative poses between local coordinate systems of neighboring vertebrae. Spine shape deformations were then modeled by a vector of rigid transformations that transforms one spine shape into another. Because rigid transformations do not naturally belong to a vector space, conventional mean and covariance could not be applied. The Frechet mean and a generalized covariance were used instead. The spine shapes of a group of 295 scoliotic patients were quantitatively analyzed as well as the spine shape deformations associated with the Cotrel-Dubousset corrective surgery (33 patients), the Boston brace (39 patients), and the scoliosis progression without treatment (26 patients). The variability of intervertebral poses was found to be inhomogeneous (lumbar vertebrae were more variable than the thoracic ones) and anisotropic (with maximal rotational variability around the coronal axis and maximal translational variability along the axial direction). Finally, brace and surgery were found to have a significant effect on the Frechet mean and on the generalized covariance in specific spine regions where treatments modified the spine shape.",
A workload for evaluating deep packet inspection architectures,"High-speed content inspection of network traffic is an important new application area for programmable networking systems, and has recently led to several proposals for high-performance regular expression matching. At the same time, the number and complexity of the patterns present in well-known network intrusion detection systems has been rapidly increasing. This increase is important since both the practicality and the performance of specific pattern matching designs are strictly dependent upon characteristics of the underlying regular expression set. However, a commonly agreed upon workload for the evaluation of deep packet inspection architectures is still missing, leading to frequent unfair comparisons, and to designs lacking in generality or scalability. In this paper, we propose a workload for the evaluation of regular expression matching architectures. The workload includes a regular expression model and a traffic generator, with the former characterizing different levels of expressiveness within rule-sets and the latter characterizing varying degrees of malicious network activity. The proposed workload is used here to evaluate designs (e.g., different memory layouts and hardware organizations) where the matching algorithm is based on compressed deterministic and non deterministic finite automata (DFAs and NFAs).",
Greenhouse Monitoring with Wireless Sensor Network,"In modern greenhouses, several measurement points are required to trace down the local climate parameters in different parts of the big greenhouse to make the greenhouse automation system work properly. Cabling would make the measurement system expensive and vulnerable. Moreover, the cabled measurement points are difficult to relocate once they are installed. Thus, a Wireless Sensor Network (WSN) consisting of small-size wireless sensor nodes equipped with radio and one or several sensors, is an attractive and cost-efficient option to build the required measurement system. In this work, we developed a wireless sensor node for greenhouse monitoring by integrating a sensor platform provided by Sensinode Ltd. [1] with three commercial sensors capable to measure four climate variables. The feasibility of the developed node was tested by deploying a simple sensor network into Martens Greenhouse Research Foundation's greenhouse in Närpiö town in Western Finland. During a one day experiment, we collected data to evaluate the network reliability and its ability to detect the microclimate layers, which typically exist in the greenhouse between lower and upper flora. We were also able to show that the network can detect the local differences in the greenhouse climate caused by various disturbances, such as direct sunshine near the greenhouse walls. This article is our first step in the area of greenhouse monitoring and control, and it is all about the developed sensor network feasibility and reliability.","Monitoring,
Wireless sensor networks,
Automation,
Control systems,
Automatic control,
Batteries,
Sensor systems,
Heating,
Ventilation,
Productivity"
Keywords to visual categories: Multiple-instance learning forweakly supervised object categorization,"Conventional supervised methods for image categorization rely on manually annotated (labeled) examples to learn good object models, which means their generality and scalability depends heavily on the amount of human effort available to help train them. We propose an unsupervised approach to construct discriminative models for categories specified simply by their names. We show that multiple-instance learning enables the recovery of robust category models from images returned by keyword-based search engines. By incorporating constraints that reflect the expected sparsity of true positive examples into a large-margin objective function, our approach remains accurate even when the available text annotations are imperfect and ambiguous. In addition, we show how to iteratively improve the learned classifier by automatically refining the representation of the ambiguously labeled examples. We demonstrate our method with benchmark datasets, and show that it performs well relative to both state-of-the-art unsupervised approaches and traditional fully supervised techniques.",
Fast disparity and motion estimation based on correlations for multiview video coding,"Recently, multiview video coding has attracted great attention from industries and research institutes. However, the heavy computational complexity limits its practical applications. In this paper, a fast disparity and motion estimation for multiview video coding is presented, based on the correlations between the neighboring cameras and between the motion and the disparity. In the proposed approach, first, a search region estimation is proposed to reduce the disparity estimation complexity according to that the camera set is usually fixed and therefore the disparity between the two neighboring views can be limited to an estimable range. Second, a motion vector derivation is given based on the geometric relationship between the motion and the disparity. In addition, an early termination scheme is provided to further reduce the number of reference frames. The experimental results show that roughly 50% time saving for disparity and motion computing can be reached when compared to the anchor in multiview video coding test model JSVM only with negligible coding efficiency loss.",
Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems,"Cache partitioning and sharing is critical to the effective utilization of multicore processors. However, almost all existing studies have been evaluated by simulation that often has several limitations, such as excessive simulation time, absence of OS activities and proneness to simulation inaccuracy. To address these issues, we have taken an efficient software approach to supporting both static and dynamic cache partitioning in OS through memory address mapping. We have comprehensively evaluated several representative cache partitioning schemes with different optimization objectives, including performance, fairness, and quality of service (QoS). Our software approach makes it possible to run the SPEC CPU2006 benchmark suite to completion. Besides confirming important conclusions from previous work, we are able to gain several insights from whole-program executions, which are infeasible from simulation. For example, giving up some cache space in one program to help another one may improve the performance of both programs for certain workloads due to reduced contention for memory bandwidth. Our evaluation of previously proposed fairness metrics is also significantly different from a simulation-based study. The contributions of this study are threefold. (1) To the best of our knowledge, this is a highly comprehensive execution- and measurement-based study on multicore cache partitioning. This paper not only confirms important conclusions from simulation-based studies, but also provides new insights into dynamic behaviors and interaction effects. (2) Our approach provides a unique and efficient option for evaluating multicore cache partitioning. The implemented software layer can be used as a tool in multicore performance evaluation and hardware design. (3) The proposed schemes can be further refined for OS kernels to improve performance.",
Globally Optimal Channel Assignment for Non-Cooperative Wireless Networks,"Channel assignment is a very important topic in wireless networks. In this paper, we study FDMA channel assignment in a non-cooperative wireless network, where devices are selfish. Existing work on this problem has considered Nash equilibrium (NE), which is not a very strong solution concept and may not guarantee a good system-wide performance. In contrast, in this work we introduce a payment formula to ensure the existence of a strongly dominant strategy equilibrium (SDSE), a much stronger solution concept. We show that, when the system converges to a SDSE, it also achieves global optimality in terms of effective system-wide throughput. Furthermore, we extend our work to the case in which some radios have limited tunability. We show that, in this case, it is generally impossible to have a similar SDSE solution; but, with additional assumptions on the numbers of radios and the types of channels, etc., we can again achieve a SDSE solution that guarantees globally optimal effective system throughput in the entire system. Besides this extension, we also consider another extension of our strategic game, which is a repeated game that provides fairness. Finally, we evaluate our design in experiments. Our evaluations verify that the system does converge to the globally optimal channel assignment with our designed payment formula, and that the effective system- wide throughput is significantly higher than that of anarchy and Nash equilibrium (NE).",
A Contention-Aware Routing Metric for Multi-Rate Multi-Radio Mesh Networks,"We present a new routing metric for multi-rate multi-radio mesh networks, which takes into account both contention for the shared wireless channel and rate diversity in multi-radio multi-channel mesh networks. A key property of the proposed contention-aware transmission time (CATT) metric is that it is isotonic, hence can be applied to link-state routing protocols. We have implemented the CATT metric in the OLSR routing protocol, and evaluate it in a test-bed with mesh nodes each equipped with four radio interfaces. Our experiments show that the proposed routing metric significantly outperforms other metrics that have appeared in the literature, in a number of scenarios that correspond to different mesh network topologies.","Mesh networks,
Routing protocols,
Interference,
Throughput,
Wireless mesh networks,
Network topology,
Testing,
Computer science,
Attenuation,
Proposals"
Analysis of Tumor Vascularity Using Three-Dimensional Power Doppler Ultrasound Images,"Tumor vascularity is an important factor that has been shown to correlate with tumor malignancy and was demonstrated as a prognostic indicator for a wide range of cancers. Three-dimensional (3-D) power Doppler ultrasound (PDUS) offers a convenient tool for investigators to inspect the signals of blood flow and vascular structures in breast cancer. In this paper, a new computer-aided diagnosis (CAD) system for quantifying Doppler ultrasound images based on 3-D thinning algorithm and neural network is proposed. We extracted the skeleton of blood vessels from 3-D PDUS data to facilitate the capturing of morphological changes. Nine features including vessel-to-volume ratio, number of vascular trees, length of vessels, number of branching, mean of radius, number of cycles, and three tortuosity measures, were extracted from the thinning result. Benign and malignant tumors can therefore be differentiated by a score computed by a multilayered perceptron (MLP) neural network using these features as parameters. The proposed system was tested on 221 breast tumors, including 110 benign and 111 malignant lesions. The accuracy, sensitivity, specificity, and positive and negative predictive values were 88.69% (196/221), 91.89% (102/111), 85.45% (94/110), 86.44% (102/118), and 91.26% (94/103), respectively. The value of the ROC curve was 0.94. The results demonstrate a correlation between the morphology of blood vessels and tumor malignancy, indicating that the newly proposed method can retrieves a high accuracy in the classification of benign and malignant breast tumors.","Image analysis,
Ultrasonic imaging,
Cancer,
Breast neoplasms,
Neural networks,
Data mining,
Blood vessels,
Biomedical imaging,
Breast tumors,
Blood flow"
Multiple trajectory search for Large Scale Global Optimization,"In this paper, the multiple trajectory search (MTS) is presented for large scale global optimization. The MTS uses multiple agents to search the solution space concurrently. Each agent does an iterated local search using one of three candidate local searchmethods. By choosing a local search method that best fits the landscape of a solution’s neighborhood, an agent may find its way to a local optimumor the global optimum. We applied the MTS to the seven benchmark problems designed for the CEC 2008 Special Session and Competition on Large Scale Global Optimization.","Optimization,
Search methods,
Arrays,
Trajectory,
Search problems,
Strontium,
Genetic algorithms"
Automatic Classification of Bird Species From Their Sounds Using Two-Dimensional Cepstral Coefficients,"This paper presents a method for automatic classification of birds into different species based on the audio recordings of their sounds. Each individual syllable segmented from continuous recordings is regarded as the basic recognition unit. To represent the temporal variations as well as sharp transitions within a syllable, a feature set derived from static and dynamic two-dimensional Mel-frequency cepstral coefficients are calculated for the classification of each syllable. Since a bird might generate several types of sounds with variant characteristics, a number of representative prototype vectors are used to model different syllables of identical bird species. For each bird species, a model selection method is developed to determine the optimal mode between Gaussian mixture models (GMM) and vector quantization (VQ) when the amount of training data is different for each species. In addition, a component number selection algorithm is employed to find the most appropriate number of components of GMM or the cluster number of VQ for each species. The mean vectors of GMM or the cluster centroids of VQ will form the prototype vectors of a certain bird species. In the experiments, the best classification accuracy is 84.06% for the classification of 28 bird species.",
Intraoperative Magnetic Tracker Calibration Using a Magneto-Optic Hybrid Tracker for 3-D Ultrasound-Based Navigation in Laparoscopic Surgery,"This paper describes a ultrasound (3D US) system that aims to achieve augmented reality (AR) visualization during laparoscopic surgery, especially for the liver. To acquire 3D US data of the liver, the tip of a laparoscopic ultrasound probe is tracked inside the abdominal cavity using a magnetic tracker. The accuracy of magnetic trackers, however, is greatly affected by magnetic field distortion that results from the close proximity of metal objects and electronic equipment, which is usually unavoidable in the operating room. In this paper, we describe a calibration method for intraoperative magnetic distortion that can be applied to laparoscopic 3D US data acquisition; we evaluate the accuracy and feasibility of the method by in vitro and in vivo experiments. Although calibration data can be acquired freehand using a magneto-optic hybrid tracker, there are two problems associated with this method - error caused by the time delay between measurements of the optical and magnetic trackers, and instability of the calibration accuracy that results from the uniformity and density of calibration data. A temporal calibration procedure is developed to estimate the time delay, which is then integrated into the calibration, and a distortion model is formulated by zeroth-degree to fourth-degree polynomial fitting to the calibration data. In the in vivo experiment using a pig, the positional error caused by magnetic distortion was reduced from 44.1 to 2.9 mm. The standard deviation of corrected target positions was less than 1.0 mm. Freehand acquisition of calibration data was performed smoothly using a magneto-optic hybrid sampling tool through a trocar under guidance by realtime 3-D monitoring of the tool trajectory; data acquisition time was less than 2 min. The present study suggests that our proposed method could correct for magnetic field distortion inside the patient's abdomen during a laparoscopic procedure within a clinically permissible period of time, as well as enabling an accurate 3D US reconstruction to be obtained that can be superimposed onto live endoscopic images.","Calibration,
Ultrasonic imaging,
Navigation,
Minimally invasive surgery,
Laparoscopes,
Liver,
Abdomen,
Magnetic fields,
Data acquisition,
In vivo"
A Practical Approach to Classify Evolving Data Streams: Training with Limited Amount of Labeled Data,"Recent approaches in classifying evolving data streams are based on supervised learning algorithms, which can be trained with labeled data only. Manual labeling of data is both costly and time consuming. Therefore, in a real streaming environment, where huge volumes of data appear at a high speed, labeled data may be very scarce. Thus, only a limited amount of training data may be available for building the classification models, leading to poorly trained classifiers. We apply a novel technique to overcome this problem by building a classification model from a training set having both unlabeled and a small amount of labeled instances. This model is built as micro-clusters using semisupervised clustering technique and classification is performed with κ-nearest neighbor algorithm. An ensemble of these models is used to classify the unlabeled data. Empirical evaluation on both synthetic data and real botnet traffic reveals that our approach, using only a small amount of labeled data for training, outperforms state-of-the-art stream classification algorithms that use twenty times more labeled data than our approach.","Training data,
Clustering algorithms,
Supervised learning,
Computer science,
Labeling,
Classification algorithms,
Testing,
Buffer storage,
Probability distribution,
Data mining"
Modeling of Tool-Tissue Interactions for Computer-Based Surgical Simulation: A Literature Review,"Surgical simulators present a safe and potentially effective method for surgical training, and can also be used in robot-assisted surgery for pre- and intra-operative planning. Accurate modeling of the interaction between surgical instruments and organs has been recognized as a key requirement in the development of high-fidelity surgical simulators. Researchers have attempted to model tool-tissue interactions in a wide variety of ways, which can be broadly classified as (1) linear elasticity-based, (2) nonlinear (hyperelastic) elasticity-based finite element (FE) methods, and (3) other techniques not based on FE methods or continuum mechanics. Realistic modeling of organ deformation requires populating the model with real tissue data (which are difficult to acquire in vivo) and simulating organ response in real time (which is computationally expensive). Further, it is challenging to account for connective tissue supporting the organ, friction, and topological changes resulting from tool-tissue interactions during invasive surgical procedures. Overcoming such obstacles will not only help us to model tool-tissue interactions in real time, but also enable realistic force feedback to the user during surgical simulation. This review paper classifies the existing research on tool-tissue interactions for surgical simulators specifically based on the modeling techniques employed and the kind of surgical operation being simulated, in order to inform and motivate future research on improved tool-tissue interaction models.",
Multistage switches are not crossbars: Effects of static routing in high-performance networks,"Multistage interconnection networks based on central switches are ubiquitous in high-performance computing. Applications and communication libraries typically make use of such networks without consideration of the actual internal characteristics of the switch. However, application performance of these networks, particularly with respect to bisection bandwidth, does depend on communication paths through the switch. In this paper we discuss the limitations of the hardware definition of bisection bandwidth (capacity-based) and introduce a new metric: effective bisection bandwidth. We assess the effective bisection bandwidth of several large-scale production clusters by simulating artificial communication patterns on them. Networks with full bisection bandwidth typically provided effective bisection bandwidth in the range of 55–60%. Simulations with application-based patterns showed that the difference between effective and rated bisection bandwidth could impact overall application performance by up to 12%.",
Game theoretical mechanism design methods,"Dynamic spectrum access with cognitive radios has become a promising approach to improve spectrum efficiency by adaptively coordinating different users' access according to spectrum dynamics. However, selfish users competing with each other for spectrum may exchange false private information or collude with others to get more access to the spectrum and achieve higher profits. In this article, we investigate two game-theoretical mechanism design methods to suppress cheating and collusion behavior of selfish users: a self-enforcing truth-telling mechanism for unlicensed spectrum sharing and a collusion-resistant multistage dynamic spectrum pricing game for licensed spectrum sharing.",
Level-set based automatic cup-to-disc ratio determination using retinal fundus images in ARGALI,"Glaucoma is a leading cause of permanent blindness. However, disease progression can be limited if detected early. The optic cup-to-disc ratio (CDR) is one of the main clinical indicators of glaucoma, and is currently determined manually, limiting its potential in mass screening. In this paper, we propose an automatic CDR determination method using a variational level-set approach to segment the optic disc and cup from retinal fundus images. The method is a core component of ARGALI, a system for automated glaucoma risk assessment. Threshold analysis is used in pre-processing to estimate the initial contour. Due to the presence of retinal vasculature traversing the disc and cup boundaries which can cause inaccuracies in the detected contours, an ellipse-fitting post-processing step is also introduced. The method was tested on 104 images from the Singapore Malay Eye Study, and it was found the results produced a clinically acceptable variation of up to 0.2 CDR units from the manually graded samples, with potential use in mass screening.","automated level-set segmentation,
optic cup-to-disc ratio,
medical image processing"
Comparison of bi-directional relaying protocols,"In a bi-directional relay channel, two nodes wish to exchange independent messages over a shared wireless channel with the help of a relay. In this paper, we derive achievable rate regions for four new half-duplex protocols and compare these to four existing half-duplex protocols and outer bounds. In time, our protocols consist of either two or three phases. In the two phase protocols, both users simultaneously transmit during the first phase and the relay alone transmits during the second phase, while in the three phase protocol the two users sequentially transmit followed by a transmission from the relay. The relay may forward information in one of four manners; we outline existing Amplify and Forward (AF) and Decode and Forward (DF) relaying schemes and introduce novel Compress and Forward (CF), and Mixed Forward schemes. We derive achievable rate regions for the CF and Mixed relaying schemes for the two and three phase protocols. Finally, we provide a comprehensive treatment of 8 possible half-duplex bi-directional relaying protocols in Gaussian noise, obtaining their respective achievable rate regions, outer bounds, and their relative performance under different SNR and relay geometries.",
The Nearest Neighbor Algorithm of Local Probability Centers,"When classes are nonseparable or overlapping, training samples in a local neighborhood may come from different classes. In this situation, the samples with different class labels may be comparable in the neighborhood of query. As a consequence, the conventional nearest neighbor classifier, such as -nearest neighbor scheme, may produce a wrong prediction. To address this issue, in this paper, we propose a new classification method, which performs a classification task based on the local probabilistic centers of each class. This method works by reducing the number of negative contributing points, which are the known samples falling on the wrong side of the ideal decision boundary, in a training set and by restricting their influence regions. In classification, this method classifies the query sample by using two measures of which one is the distance between the query and the local categorical probability centers, and the other is the computed posterior probability of the query. Although both measures are effective, the experiments show that the second one achieves the smaller classification error. Meanwhile, the theoretical analyses of the suggested methods are investigated, and some experiments are conducted on the basis of both constructed and real datasets. The investigation results show that this method substantially improves the classification performance of the nearest neighbor algorithm.","Nearest neighbor searches,
Pattern classification,
Probability distribution,
Bayesian methods,
Sufficient conditions,
Computer science education,
Educational programs,
Computer science,
Information science"
Mining With Noise Knowledge: Error-Aware Data Mining,"Real-world data mining deals with noisy information sources where data collection inaccuracy, device limitations, data transmission and discretization errors, or man-made perturbations frequently result in imprecise or vague data. Two common practices are to adopt either data cleansing approaches to enhance the data consistency or simply take noisy data as quality sources and feed them into the data mining algorithms. Either way may substantially sacrifice the mining performance. In this paper, we consider an error-aware (EA) data mining design, which takes advantage of statistical error information (such as noise level and noise distribution) to improve data mining results. We assume that such noise knowledge is available in advance, and we propose a solution to incorporate it into the mining process. More specifically, we use noise knowledge to restore original data distributions, which are further used to rectify the model built from noise- corrupted data. We materialize this concept by the proposed EA naive Bayes classification algorithm. Experimental comparisons on real-world datasets will demonstrate the effectiveness of this design.","Data mining,
Computer science,
Noise level,
Decision theory,
Data communication,
Feeds,
Classification algorithms,
Niobium,
Mining industry,
Costs"
The Haemodynamics of Endovascular Aneurysm Treatment: A Computational Modelling Approach for Estimating the Influence of Multiple Coil Deployment,"This paper proposes a novel computational methodology for modelling the haemodynamic effects of endovascular coil embolization for cerebral aneurysms. We employ high-resolution 3-D angiographic data to reconstruct the intracranial geometry and we model the coiled part of the aneurysm as a porous medium, with porosity decreasing as coils are inserted. The actual dimensions of the coils employed are used to determine the characteristics of the porous medium. Simulation results for saccular aneurysms from the anterior communicating and middle cerebral arteries show that insertion of coils rapidly changes intraaneurysmal blood flow and causes reduction in mural pressure and blood velocity up to stagnation, providing favorable conditions for thrombus formation and obliteration of the aneurysm.",
Tactile and Multisensory Spatial Warning Signals for Drivers,"The last few years have seen many exciting developments in the area of tactile and multisensory interface design. One of the most rapidly-moving practical application areas for these findings is in the development of warning signals and information displays for drivers. For instance, tactile displays can be used to awaken sleepy drivers, to capture the attention of distracted drivers, and even to present more complex information to drivers who may be visually-overloaded. This review highlights the most important potential costs and benefits associated with the use of tactile and multisensory information displays in a vehicular setting. Multisensory displays that are based on the latest cognitive neuroscience research findings can capture driver attention significantly more effective than their unimodal (i.e., tactile) counterparts. Multisensory displays can also be used to transmit information more efficiently, as well as to reduce driver workload. Finally, we highlight the key research questions currently awaiting further research, including questions such as: Are tactile warning signals really intuitive? Are there certain regions of the body (or in the space surrounding the body) where tactile/multisensory warning signals are particularly effective? To what extent is the spatial coincidence and temporal synchrony of the individual sensory signals critical to determining the effectiveness of multisensory displays? And, finally, how does the issue of compliance vs. reliance (or the 'cry wolf' phenomenon associated with the presentation of signals that are perceived as false alarms) influence the effectiveness of tactile and/or multisensory warning signals?","Psychology,
Auditory displays,
Signal design,
Skin,
Neuroscience,
Haptic interfaces,
Vehicle safety,
Vehicle driving,
Ergonomics,
Torque"
Optimal Camera Network Configurations for Visual Tagging,"Proper placement of cameras in a distributed smart camera network is an important design problem. Not only does it determine the coverage of the surveillance, but it also has a direct impact on the appearance of objects in the cameras which dictates the performance of all subsequent computer vision tasks. In this paper, we propose a generic camera placement model based on the visibility of objects at different cameras. Our motivation stems from the need of identifying and locating objects with distinctive visual features or ldquotags.rdquo This is a very common goal in computer vision with applications ranging from identifying soccer players by their jersey numbers to locating and recognizing faces of individuals. Our proposed framework places no restriction on the visual classification tasks. It incorporates realistic camera models, self occlusion of tags, and occlusion by other moving objects. It is also flexible enough to handle arbitrary-shaped three-dimensional environments. Using this framework, two novel binary integer programming (BIP) algorithms are proposed to find the optimal camera placement for ldquovisual taggingrdquo and a greedy implementation is developed to cope with the complexity of BIP. Extensive performance analysis is performed using Monte Carlo simulations, virtual environment simulations, and real-world experiments. We also demonstrate the usefulness of visual tagging through robust individual identification and obfuscation across multiple camera views for privacy protection.",
A comprehensive approach to DRAM power management,"This paper describes a comprehensive approach for using the memory controller to improve DRAM energy efficiency and manage DRAM power. We make three contributions: (1) we describe a simple power-down policy for exploiting low power modes of modern DRAMs; (2) we show how the idea of adaptive history-based memory schedulers can be naturally extended to manage power and energy; and (3) for situations in which additional DRAM power reduction is needed, we present a throttling approach that arbitrarily reduces DRAM activity by delaying the issuance of memory commands. Using detailed microarchitectural simulators of the IBM Power5+ and a DDR2-533 SDRAM, we show that our first two techniques combine to increase DRAM energy efficiency by an average of 18.2%, 21.7%, 46.1%, and 37.1% for the Stream, NAS, SPEC2006fp, and commercial benchmarks, respectively. We also show that our throttling approach provides performance that is within 4.4% of an idealized oracular approach.","Random access memory,
Delay,
Power demand,
Benchmark testing,
Memory management,
Computational modeling,
Mathematical model"
Probabilistic Verifiers: Evaluating Constrained Nearest-Neighbor Queries over Uncertain Data,"In applications like location-based services, sensor monitoring and biological databases, the values of the database items are inherently uncertain in nature. An important query for uncertain objects is the Probabilistic Nearest-Neighbor Query (PNN), which computes the probability of each object for being the nearest neighbor of a query point. Evaluating this query is computationally expensive, since it needs to consider the relationship among uncertain objects, and requires the use of numerical integration or Monte-Carlo methods. Sometimes, a query user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the Constrained Nearest-Neighbor Query (C-PNN), which returns the IDs of objects whose probabilities are higher than some threshold, with a given error bound in the answers. The C-PNN can be answered efficiently with probabilistic verifiers. These are methods that derive the lower and upper bounds of answer probabilities, so that an object can be quickly decided on whether it should be included in the answer. We have developed three probabilistic verifiers, which can be used on uncertain data with arbitrary probability density functions. Extensive experiments were performed to examine the effectiveness of these approaches.","Nearest neighbor searches,
Uncertainty,
Biology computing,
Histograms,
Spatial databases,
Qualifications,
Biosensors,
Probability density function,
Monitoring,
Temperature sensors"
"Autonomy for Mars Rovers: Past, Present, and Future","The vehicles used to explore the Martian surface require a high degree of autonomy to navigate challenging and unknown terrain, investigate targets, and detect scientific events. Increased autonomy will be critical to the success of future missions. In July 1997, as part of NASA's Mars Pathfinder mission, the Sojourner rover became the first spacecraft to autonomously drive on another planet. The twin Mars Exploration Rovers (MER) vehicles landed in January 2004, and after four years Spirit had driven more than four miles and Opportunity more than seven miles-lasting well past their projected three-month lifetime and expected distances traveled. The newest member of the Mars rover family will have the ability to autonomously approach and inspect a target and automatically detect interesting scientific events. In fall 2009, NASA plans to launch the Mars Science Laboratory (MSL) rover, with a primary mission of two years of surface exploration and the ability to acquire and process rock samples. In the near future, the Mars Sample Return (MSR) mission, a cooperative project of NASA and the European Space Agency, will likely use a lightweight rover to drive out and collect samples and bring them back to an Earth return vehicle. This rover will use an unprecedented level of autonomy because of the limited lifetime of a return rocket on the Martian surface and the desire to obtain samples from distant crater walls.","Mars,
Event detection,
Space vehicles,
Vehicle driving,
NASA,
Navigation,
Vehicle detection,
Planets,
Remotely operated vehicles,
Laboratories"
Congestion-Aware Rate Adaptation in Wireless Networks: A Measurement-Driven Approach,"Traditional rate adaptation solutions for IEEE 802.11 wireless networks perform poorly in congested networks. Measurement studies show that congestion in a wireless network leads to the use of lower transmission data rates and thus reduces overall network throughput and capacity. The lack of techniques to reliably identify and characterize congestion in wireless networks has prevented development of rate adaptation solutions that incorporate congestion information in their decision framework. To this end, our main contributions in this paper are two-fold. First, we present a technique that identifies and measures congestion in an 802.11 network in real time. Second, we design Wireless congestion Optimized Fallback (WOOF), a measurement-driven rate adaptation scheme for 802.11 devices that uses the congestion measurement to identify congestion related packet losses. Through experimental evaluation, we show that WOOF achieves up to 300% higher throughput in congested networks, compared to other well-known adaptation algorithms.",
A Memory Efficient Multiple Pattern Matching Architecture for Network Security,"Pattern matching is one of the most important components for the content inspection based applications of network security, and it requires well designed algorithms and architectures to keep up with the increasing network speed. For most of the solutions, AC and its derivative algorithms are widely used. They are based on the DFA model but utilize large amount of memory because of so many transition rules. An algorithm, called ACC, is presented in this paper for multiple pattern matching. It uses a novel model, namely cached deterministic finite automate (CDFA). In ACC, by using CDFA, only 4.1% transition rules for ClamAV (20.8% for Snort) are needed to represent the same function using DFA built by AC. This paper also proposes a new scheme named next-state addressing (NSA) to store and access transition rules of DFA in memory. Using this method, transition rules can be efficiently stored and directly accessed. Finally the architecture for multiple pattern matching is optimized by several approaches. Experiments show our architecture can achieve matching speed faster than 10 Gbps with very efficient memory utilization, i.e., 81KB memory for 1.8 K Snort rules with total 29 K characters, and 9.5 MB memory for 50 K ClamAV rules with total 4.44 M characters. A single architecture is memory efficient for large pattern set, and it is possible to support more than 10 M patterns with at most half amount of the memory utilization compared to the state-of-the-art architectures.","Pattern matching,
Doped fiber amplifiers,
Inspection,
Intrusion detection,
Computer architecture,
Computer security,
Communications Society,
Computer science,
Application software,
Algorithm design and analysis"
Scheduling Arbitrary-Deadline Sporadic Task Systems on Multiprocessors,"A new algorithm is proposed for scheduling preemptiblearbitrary-deadline sporadic task systems upon multiprocessorplatforms, with interprocessor migration permitted. This algorithmis based on a task-splitting approach --- while most tasks areentirely assigned to specific processors, a few tasks (fewer thanthe number of processors) may be split across two processors.  Thisalgorithm can be used for two distinct purposes: for actuallyscheduling specific sporadic task systems, and for feasibilityanalysis.Simulation-based evaluation indicates that this algorithm offers asignificant improvement on the ability to schedulearbitrary-deadline sporadic task systems as compared to thecontemporary state-of-art.With regard to feasibility analysis, the new algorithm is proved tooffer superior performance guarantees in comparison to priorfeasibility tests.",
Physical-layer security in stochastic wireless networks,"Motivated by recent developments in physical-layer security and stochastic geometry, we aim to characterize the fundamental limits of secure communication in wireless networks. Based on a general model in which legitimate nodes and potential eavesdroppers are randomly scattered in space, we define the secure communication graph (s-graph) from the point of view of information-theoretic security. For the Poisson s-graph, we provide conclusive results for: (a) the in-degree and out-degree of a node; (b) the isolation probability; and (c) the secrecy capacity between a node and each of its neighbours. Our analysis reveals the innate connections between information-theoretic security and the spatial densities of legitimate and eavesdropper nodes.",
Parameter Estimation of Quantum Channels,"The efficiency of parameter estimation of quantum channels is studied in this paper. We introduce the concept of programmable parameters to the theory of estimation. It is found that programmable parameters obey the standard quantum limit strictly; hence, no speedup is possible in its estimation. We also construct a class of nonunitary quantum channels whose parameter can be estimated in a way that the standard quantum limit is broken. The study of estimation of general quantum channels also enables an investigation of the effect of noises on quantum estimation.","Parameter estimation,
Estimation theory,
Information theory,
Quantum mechanics,
Statistics,
Laboratories,
Computer science,
Intelligent systems,
Maximum likelihood estimation,
Communication channels"
A 0.9 V 92 dB Double-Sampled Switched-RC Delta-Sigma Audio ADC,"A 0.9 V third-order double-sampled delta-sigma audio ADC is presented. A new method using a combination of a switched-RC technique and a floating switched-capacitor double-sampling configuration enabled low-voltage operation without clock boosting or bootstrapping. A three-level quantizer with simple dynamic element matching was used to improve linearity. The prototype IC implemented in a 0.13 CMOS process achieves 92 dB DR, 91 dB SNR and 89 dB SNDR in a 24 kHz audio signal bandwidth, while consuming 1.5 mW from a 0.9 V supply. The prototype operates from 0.65 V to 1.5 V supply with minimal performance degradation.",
A New Method for Registration-Based Medical Image Interpolation,"A new technique is presented for interpolating between grey-scale images in a medical data set. Registration between neighboring slices is achieved with a modified control grid interpolation algorithm that selectively accepts displacement field updates in a manner optimized for performance. A cubic interpolator is then applied to pixel intensities correlated by the displacement fields. Special considerations are made for efficiency, interpolation quality, and compression in the implementation of the algorithm. Experimental results show that the new method achieves good quality, while offering dramatic improvement in efficiency relative to the best competing method.","Biomedical imaging,
Interpolation,
Biomedical engineering,
Image reconstruction,
Picture archiving and communication systems,
Displacement control,
Anisotropic magnetoresistance,
Surface reconstruction,
Rendering (computer graphics),
Image resolution"
Alpha-Cut Implemented Fuzzy Clustering Algorithms and Switching Regressions,"In the fuzzy c-means (FCM) clustering algorithm, almost none of the data points have a membership value of 1. Moreover, noise and outliers may cause difficulties in obtaining appropriate clustering results from the FCM algorithm. The embedding of FCM into switching regressions, called the fuzzy c-regressions (FCRs), still has the same drawbacks as FCM. In this paper, we propose the -cut implemented fuzzy clustering algorithms, referred to as , which allow the data points being able to completely belong to one cluster. The proposed algorithms can form a cluster core for each cluster, where data points inside a cluster core will have a membership value of 1 so that it can resolve the drawbacks of FCM. On the other hand, the fuzziness index plays different roles for FCM and . We find that the clustering results obtained by are more robust to noise and outliers than FCM when a larger is used. Moreover, the cluster cores generated by are workable for various data shape clusters, so that is very suitable for embedding into switching regressions. The embedding of into switching regressions is called . The proposed provides better results than FCR for environments with noise or outliers. Numerical examples show the robustness and the superiority of our proposed methods.","Clustering algorithms,
Noise robustness,
Noise shaping,
Educational programs,
Partitioning algorithms,
Shape,
Working environment noise,
Mathematics,
Information management,
Computer science"
Sparsity-Enforced Slice-Selective MRI RF Excitation Pulse Design,"We introduce a novel algorithm for the design of fast slice-selective spatially-tailored magnetic resonance imaging (MRI) excitation pulses. This method, based on sparse approximation theory, uses a second-order cone optimization to place and modulate a small number of slice-selective sine-like radio-frequency (RF) pulse segments (""spokes"") in excitation fc-space, enforcing sparsity on the number of spokes allowed while si multaneously encouraging those that remain to be placed and modulated in a way that best forms a user-defined in-plane target magnetization. Pulses are designed to mitigate B1 inhomogeneity in a water phantom at 7 T and to produce highly-structured excitations in an oil phantom on an eight-channel parallel excitation system at 3 T. In each experiment, pulses generated by the spar- sity-enfoldquoced method outperform those created via conventional Fourier-based techniques, e.g., when attempting to produce a uniform magnetization in the presence of severe B1 inhomogeneity, a 5.7-ms 15-spoke pulse generated by the sparsity-enforced method produces an excitation with 1.28 times lower root mean square error than conventionally-designed 15-spoke pulses. To achieve this same level of uniformity, the conventional methods need to use 29-spoke pulses that are 7.8 ms long.","Magnetic resonance imaging,
Radio frequency,
Pulse modulation,
Algorithm design and analysis,
Magnetic modulators,
Magnetization,
Imaging phantoms,
Pulse generation,
Approximation methods,
Optimization methods"
Mashup Advisor: A Recommendation Tool for Mashup Development,"Mashup editors, like Yahoo Pipes and IBM Lotus Mashup Maker, allow non-programmer end-users to “mash-up” information sources and services to meet their information needs. However, with the increasing number of services, information sources and complex operations like filtering and joining, even an easy to use editor is not sufficient. MashupAdvisor aims to assist mashup creators to build higher quality mashups in less time. Based on the current state of a mashup, the MashupAdvisor quietly suggests outputs (goals) that the user might want to include in the final mashup. MashupAdvisor exploits a repository of mashups to estimate the popularity of specific outputs, and makes suggestions using the conditional probability thatan output will be included, given the current state of the mashup. When a suggestion is accepted, MashupAdvisor uses a semantic matching algorithm and a metric planner to modify the mashup to produce the suggested output. Our prototype was implemented on top of IBM Lotus MashupMaker and our initial results show that it is effective.","Mashups,
Information filtering,
Information filters,
Joining processes,
Companies,
Artificial intelligence,
Web services,
Computer science,
Informatics,
State estimation"
Sparse reconstruction by separable approximation,"Finding sparse approximate solutions to large underdetermined linear systems of equations is a common problem in signal/image processing and statistics. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution and reconstruction, and compressed sensing (CS) are a few well-known areas in which problems of this type appear. One standard approach is to minimize an objective function that includes a quadratic (pound 2) error term added to a sparsity-inducing (usually pound 1) regularizer. We present an algorithmic framework for the more general problem of minimizing the sum of a smooth convex function and a nonsmooth, possibly nonconvex, sparsity-inducing function. We propose iterative methods in which each step is an optimization subproblem involving a separable quadratic term (diagonal Hessian) plus the original sparsity-inducing term. Our approach is suitable for cases in which this subproblem can be solved much more rapidly than the original problem. In addition to solving the standard pound 2 - pound 1 case, our approach handles other problems, e.g., pound p regularizers with p ne 1, or group-separable (GS) regularizers. Experiments with CS problems show that our approach provides state-of-the-art speed for the standard pound 2 - pound 1 problem, and is also efficient on problems with GS regularizers.","Image reconstruction,
Linear systems,
Equations,
Signal processing,
Image processing,
Statistics,
Deconvolution,
Compressed sensing,
Iterative algorithms,
Iterative methods"
A Crosstab-based Statistical Method for Effective Fault Localization,"Fault localization is the most expensive activity in program debugging. Traditional ad-hoc methods can be time-consuming and ineffective because they rely on programmers’ intuitive guesswork, which may neither be accurate nor reliable. A better solution is to utilize a systematic and statistically well-defined method to automatically identify suspicious code that should be examined for possible fault locations. We present a crosstab-based statistical method using the coverage information of each executable statement and the execution result (success or failure) with respect to each test case. A crosstab is constructed for each executable statement and a statistic is computed to determine the suspiciousness of the corresponding statement. Statements with a higher suspiciousness are more likely to contain bugs and should be examined before those with a lower suspiciousness. Three case studies using the Siemens suite, the Space program, and the Unix suite, respectively, are conducted. Our results suggest that the crosstab-based method is effective in fault localization and performs better (in terms of a smaller percentage of executable statements that have to be examined until the first statement containing the fault is reached) than other methods such as Tarantula. The difference in efficiency (computational time) between these two methods is very small.","Statistical analysis,
Computer bugs,
Programming profession,
Debugging,
Statistics,
Vehicle crash testing,
Runtime,
Software testing,
Computer science,
Fault diagnosis"
Segmentation of Rodent Whole-Body Dynamic PET Images: An Unsupervised Method Based on Voxel Dynamics,"Positron emission tomography (PET) is a useful tool for pharmacokinetics studies in rodents during the preclinical phase of drug and tracer development. However, rodent organs are small as compared to the scanner's intrinsic resolution and are affected by physiological movements. We present a new method for the segmentation of rodent whole-body PET images that takes these two difficulties into account by estimating the pharmacokinetics far from organ borders. The segmentation method proved efficient on whole-body numerical rat phantom simulations, including 3-14 organs, together with physiological movements (heart beating, breathing, and bladder filling). The method was resistant to spillover and physiological movements, while other methods failed to obtain a correct segmentation. The radioactivity concentrations calculated with this method also showed an excellent correlation with the manual delineation of organs in a large set of preclinical images. In addition, it was faster, detected more organs, and extracted organs' mean time activity curves with a better confidence on the measure than manual delineation.","Image segmentation,
Rodents,
Whole-body PET,
Positron emission tomography,
Drugs,
Imaging phantoms,
Numerical simulation,
Bladder,
Filling,
Time measurement"
Segmentation of Whole Cells and Cell Nuclei From 3-D Optical Microscope Images Using Dynamic Programming,"Communications between cells in large part drive tissue development and function, as well as disease-related processes such as tumorigenesis. Understanding the mechanistic bases of these processes necessitates quantifying specific molecules in adjacent cells or cell nuclei of intact tissue. However, a major restriction on such analyses is the lack of an efficient method that correctly segments each object (cell or nucleus) from 3-D images of an intact tissue specimen. We report a highly reliable and accurate semi-automatic algorithmic method for segmenting fluorescence-labeled cells or nuclei from 3-D tissue images. Segmentation begins with semi-automatic, 2-D object delineation in a user-selected plane, using dynamic programming (DP) to locate the border with an accumulated intensity per unit length greater that any other possible border around the same object. Then the two surfaces of the object in planes above and below the selected plane are found using an algorithm that combines DP and combinatorial searching. Following segmentation, any perceived errors can be interactively corrected. Segmentation accuracy is not significantly affected by intermittent labeling of object surfaces, diffuse surfaces, or spurious signals away from surfaces. The unique strength of the segmentation method was demonstrated on a variety of biological tissue samples where all cells, including irregularly shaped cells, were accurately segmented based on visual inspection.","Optical microscopy,
Image segmentation,
Dynamic programming,
Cells (biology),
Biomedical optical imaging,
Drives,
Image analysis,
Fluorescence,
Error correction,
Labeling"
CARS: Context-Aware Rate Selection for vehicular networks,"Traffic querying, road sensing and mobile content delivery are emerging application domains for vehicular networks whose performance depends on the throughput these networks can sustain. Rate adaptation is one of the key mechanisms at the link layer that determine this performance. Rate adaptation in vehicular networks faces the following key challenges: (1) due to the rapid variations of the link quality caused by fading and mobility at vehicular speeds, the transmission rate must adapt fast in order to be effective, (2) during infrequent and bursty transmission, the rate adaptation scheme must be able to estimate the link quality with few or no packets transmitted in the estimation window, (3) the rate adaptation scheme must distinguish losses due to environment from those due to hidden-station induced collision. Our extensive outdoor experiments show that the existing rate adaptation schemes for 802.11 wireless networks underutilize the link capacity in vehicular environments. In this paper, we design, implement and evaluate CARS, a novel Context-Aware Rate Selection algorithm that makes use of context information (e.g. vehicle speed and distance from neighbor) to systematically address the above challenges, while maximizing the link throughput. Our experimental evaluation in real outdoor vehicular environments with different mobility scenarios shows that CARS adapts to changing link conditions at high vehicular speeds faster than existing rate-adaptation algorithms. Our scheme achieves significantly higher throughput, up to 79%, in all the tested scenarios, and is robust to packet loss due to collisions, improving the throughput by up to 256% in the presence of hidden stations.","Throughput,
Telecommunication traffic,
Roads,
Fading,
Propagation losses,
Wireless networks,
Algorithm design and analysis,
Vehicles,
Testing,
Robustness"
Overlay Networks with Linear Capacity Constraints,"Overlay networks are virtual networks residing over the IP network; consequently, overlay links may share hidden lower level bottlenecks. Previous work has assumed an independent overlay model: a graph with independent link capacities. We introduce a model of overlays that incorporates correlated link capacities and linear capacity constraints (LCC) to formulate hidden shared bottlenecks; we refer to these as LCC-overlays. We define metrics to qualitatively measure overlay quality in terms of its accuracy (in representing the true network topology) and efficiency (that is, performance). Through analysis and simulations, we show that LCC-overlay is perfectly accurate and, hence, enjoys much higher efficiency than the inaccurate independent overlay. We discover that even a highly restricted LCC class-node-based LCC-yields near-optimal accuracy and significantly higher efficiency. We study two network flow problems in the context of LCC-graphs: widest path and maximum flow. We prove that Widest Path with LCC is NP-complete. We formulate Maximum Flow with LCC as a linear program and propose an efficient distributed algorithm to solve it. Based on the LCC model, we further study the problem of optimizing delay while still maintaining optimal or near-optimal bandwidth. We also outline a distributed algorithm to efficiently construct an overlay with node-based LCC.","Unicast,
Network topology,
Distributed algorithms,
Delay,
IP networks,
Analytical models,
Bandwidth,
Algorithm design and analysis,
Streaming media,
Multicast algorithms"
A new descriptive clustering algorithm based on Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization (NMF) provides a way for finding a part-based representation of nonnegative data. An important property of NMF is that it can produce a sparse representation of the data; however, in some applications, especially in text clustering, the sparse representation always consists of separated words, which cannot explicitly express the meaning of the basis vector. This paper presents a new descriptive clustering algorithm based on NMF, called DC-NMF that can avoid this separated word problem. In our proposed method, we embrace the phrase-by-document matrix in addition to the commonly used term-by-document matrix. Then, we use conjunct gradient descent to minimize the mean squared error objective function. Finally, we describe each cluster with the highest weighted element corresponding to one particular phrase. Our experimental results indicate that our method can obtain more “pure” clusters than other methods.","Clustering algorithms,
Sparse matrices,
Entropy,
XML,
Computer science,
HTML,
Dentistry"
Temporal Description Logics: A Survey,"We survey temporal description logics that are based on standard  temporal logics such as LTL and CTL. In particular, we concentrate  on the computational complexity of the satisfiability problem and  algorithms for deciding it.","Ontologies,
Computer science,
Automatic logic units,
Databases,
Information systems,
Educational institutions,
Computational complexity,
Knowledge representation,
Standardization,
OWL"
Impact of Cache Partitioning on Multi-tasking Real Time Embedded Systems,"Cache partitioning techniques have been proposed in the past as a solution for the cache interference problem. Due to qualitative differences with general purpose platforms, real-time embedded systems need to minimize task real-time utilization (function of execution time and period) instead of only minimizing the number of cache misses. In this work, the partitioning problem is presented as an optimization problem whose solution sets the size of each cache partition and assigns tasks to partitions such that system worst-case utilization is minimized thus increasing real-time schedulability. Since the problem is NP-Hard, a genetic algorithm is presented to find a near optimal solution. A case study and experiments show that in a typical real-time embedded system, the proposed algorithm is able to reduce the worst-case utilization by 15% (on average) if compared to the case when the system uses a shared cache or a proportional cache partitioned environment.","Real time systems,
Partitioning algorithms,
Algorithm design and analysis,
Interference,
Genetic algorithms,
Optimization,
Cathode ray tubes"
Linear Level Lasserre Lower Bounds for Certain k-CSPs,"We show that for
k≥3
even the
Ω(n)
level of the Lasserre hierarchycannot disprove a random {\sc
k
-CSP} instance over any predicate type implied by
k
-XOR constraints, for example {\sc
k
-SAT} or {\sc
k
-XOR}).(One constant is said to imply another if the latter is true whenever the former is.For example
k
-XOR constraints imply
k
-CNF constraints.)  As a result the
Ω(n)
level Lasserre relaxation fails to approximate such CSPs betterthan the trivial, random algorithm.As corollaries, we obtain
Ω(n)
level integrality gapsfor the Lasserre hierarchy of  
7
6
−ϵ
for {\sc VertexCover},  
2−ϵ
for {\sc
k
-UniformHypergraphVertexCover}, and any constant for{\sc
k
-UniformHypergraphIndependentSet}. This is the first construction of a Lasserre integralitygap.","Computer science,
Polynomials,
Approximation algorithms,
Constraint optimization,
Computational modeling"
Dynamic PET Reconstruction Using Wavelet Regularization With Adapted Basis Functions,"Tomographic reconstruction from positron emission tomography (PET) data is an ill-posed problem that requires regularization. An attractive approach is to impose an lscr1-regularization constraint, which favors sparse solutions in the wavelet domain. This can be achieved quite efficiently thanks to the iterative algorithm developed by Daubechies et al., 2004. In this paper, we apply this technique and extend it for the reconstruction of dynamic (spatio-temporal) PET data. Moreover, instead of using classical wavelets in the temporal dimension, we introduce exponential-spline wavelets (E-spline wavelets) that are specially tailored to model time activity curves (TACs) in PET. We show that the exponential-spline wavelets naturally arise from the compartmental description of the dynamics of the tracer distribution. We address the issue of the selection of the ldquooptimalrdquo E-spline parameters (poles and zeros) and we investigate their effect on reconstruction quality. We demonstrate the usefulness of spatio-temporal regularization and the superior performance of E-spline wavelets over conventional Battle-Lemarie wavelets in a series of experiments: the 1-D fitting of TACs, and the tomographic reconstruction of both simulated and clinical data. We find that the E-spline wavelets outperform the conventional wavelets in terms of the reconstructed signal-to-noise ratio (SNR) and the sparsity of the wavelet coefficients. Based on our simulations, we conclude that replacing the conventional wavelets with E-spline wavelets leads to equal reconstruction quality for a 40% reduction of detected coincidences, meaning an improved image quality for the same number of counts or equivalently a reduced exposure to the patient for the same image quality.","Positron emission tomography,
Image reconstruction,
Signal to noise ratio,
Image quality,
Biomedical imaging,
Nuclear electronics,
Information systems,
Inverse problems,
Wavelet domain,
Iterative algorithms"
Multi-view facial expression recognition,"The ability to handle multi-view facial expressions is important for computers to understand affective behavior under less constrained environment. However, most of existing methods for facial expression recognition are based on the near-frontal view face data, which are likely to fail in the non-frontal facial expression analysis. In this paper, we conduct an investigation on analyzing multi-view facial expressions. Three local patch descriptors (HoG, LBP, and SIFT) are used to extract facial features, which are the inputs to a nearest-neighbor indexing method that identifies facial expressions. We also investigate the influence of feature dimension reductions (PCA, LDA, and LPP) and classifier fusion on the recognition performance. We test our approaches on multi-view data generated from BU-3DFE 3D facial expression database that includes 100 subjects with 6 emotions and 4 intensity levels. Our extensive person-independent experiments suggest that the SIFT descriptor outperforms HoG and LBP, and LPP outperforms PCA and LDA in this application. But the classifier fusion does not show a significant advantage over SIFT-only classifier.","Face recognition,
Principal component analysis,
Linear discriminant analysis,
Failure analysis,
Data mining,
Facial features,
Indexing,
Testing,
Fusion power generation,
Spatial databases"
Online network coding for optimal throughput and delay - the three-receiver case,"For a packet erasure broadcast channel with three receivers, we propose a new coding algorithm that makes use of feedback to dynamically adapt the code. Our algorithm is throughput optimal, and we conjecture that it also achieves an asymptotically optimal average decoding delay at the receivers. We consider heavy traffic asymptotics, where the load factor ρ approaches 1 from below with either the arrival rate (λ) or the channel parameter (μ) being fixed at a number less than 1. We verify through simulations that our algorithm achieves an asymptotically optimal decoding delay of O (1/1−ρ).","Network coding,
Throughput,
Decoding,
Delay,
Broadcasting,
Feedback,
Automatic repeat request,
Space technology,
Subcontracting,
Information theory"
An FPGA-specific approach to floating-point accumulation and sum-of-products,"This article studies two common situations where the flexibility of FPGAs allows one to design application-specific floating-point operators which are more efficient and more accurate than those offered by processors and GPUs. First, for applications involving the addition of a large number of floating-point values, an ad-hoc accumulator is proposed. By tailoring its parameters to the numerical requirements of the application, it can be made arbitrarily accurate, at an area cost comparable to that of a standard floating-point adder, and at a higher frequency. The second example is the sum-of-product operation, which is the building block of matrix computations. A novel architecture is proposed that feeds the previous accumulator out of a floating-point multiplier whose rounding logic has been removed, again improving the area/accuracy tradeoff. These architectures are implemented within the FloPoCo generator, freely available under the LGPL.","Field programmable gate arrays,
Computer architecture,
Application software,
Frequency,
Costs,
Acceleration,
Delay,
Computer science,
Feeds,
Logic"
Verifiable Privacy-Preserving Range Query in Two-Tiered Sensor Networks,"We consider a sensor network that is not fully trusted and ask the question how we preserve privacy for the collected data and how we verify the data reply from the network. We explore the problem in the context of a network augmented with storage nodes and target at range query. We use bucketing scheme to mix the data for a range, use message encryption for data integrity, and employ encoding numbers to prevent the storage nodes from dropping data.","Data privacy,
Data security,
Secure storage,
Peer to peer computing,
Encoding,
Information security,
Base stations,
Communications Society,
Computer science,
Educational institutions"
CASCADE: Cluster-Based Accurate Syntactic Compression of Aggregated Data in VANETs,"We present a method for accurate aggregation of highway traffic information in vehicular ad hoc networks (VANETs). Highway congestion notification applications need to disseminate information about traffic conditions to distant vehicles. In dense traffic, aggregation is needed to allow a single frame to carry information about a large number of vehicles. Our technique, CASCADE, uses compression to provide aggregation without losing accuracy. We show that CASCADE makes efficient use of the wireless channel while providing each vehicle with data that is highly accurate, represents a large area in front of the vehicle, and can be combined with aggregated data from other vehicles to further extend the covered area.","Telecommunication traffic,
Road transportation,
Broadcasting,
Ad hoc networks,
Road vehicles,
Computer science,
Application software,
Safety,
Road accidents,
Analytical models"
The WCCI 2008 simulated car racing competition,"This paper describes the simulated car racing competition held in association with the IEEE WCCI 2008 conference. The organization of the competition is described, along with the rules, the software used, and the submitted car racing controllers. The results of the competition are presented, followed by a discussion about what can be learned from this competition, both about learning controllers with evolutionary methods and about competition organization. The paper is co-authored by the organizers and participants of the competition.","Computational intelligence,
Computational modeling,
Benchmark testing,
Open source software,
Software algorithms,
Computer science,
USA Councils,
Evolutionary computation,
Computer industry,
Industrial control"
A new anthropomorphic robotic hand,"This paper presents the new robotic FRH-4 hand. The FRH-4 hand constitutes a new hybrid concept of an anthropomorphic five fingered hand and a three jaw robotic gripper. The hand has a humanoid appearance while maintaining the precision of a robotic gripper. Since it is actuated with flexible fluidic actuators, it exhibits an excellent power to weight ratio. These elastic actuators also ensure that the hand is safe for interacting with humans. In order to fully control the joints, it is equipped with position sensors on all of the 11 joints. The hand is also fitted with tactile sensors based on cursor navigation sensor elements, which allows it to have grasping feedback and the ability for exploration.","Anthropomorphism,
Grippers,
Actuators,
Tactile sensors,
Robot sensing systems,
Humanoid robots,
Humans,
Navigation,
Grasping,
Feedback"
Fast Deformable Registration on the GPU: A CUDA Implementation of Demons,"In the medical imaging field, we need fast deformable registration methods especially in intra-operative settings characterized by their time-critical applications. Image registration studies which are based on Graphics Processing Units (GPUs) provide fast implementations. However, only a small number of these GPU-based studies concentrate on deformable registration. We implemented Demons, a widely used deformable image registration algorithm, on NVIDIA’s Quadro FX 5600 GPU with the Compute Unified Device Architecture (CUDA) programming environment. Using our code, we registered 3D CT lung images of patients. Our results show that we achieved the fastest runtime among the available GPU-based Demons implementations. Additionally, regardless of the given dataset size, we provided a factor of 55 speedup over an optimized CPU-based implementation. Hence, this study addresses the need for on-line deformable registration methods in intra-operative settings by providing the fastest and most scalable Demons implementation available to date. In addition, it provides an implementation of a deformable registration algorithm on a GPU, an understudied type of registration in the general-purpose computation on graphics processors (GPGPU) community.","Biomedical imaging,
Image registration,
Graphics,
Runtime,
Programming environments,
Surgery,
Time factors,
Computer architecture,
Lungs,
Computed tomography"
Supervised Enhancement Filters: Application to Fissure Detection in Chest CT Scans,"In medical image processing, many filters have been developed to enhance certain structures in 3-D data. In this paper, we propose to use pattern recognition techniques to design more optimal filters. The essential difference with previous approaches is that we provide a system with examples of what it should enhance and suppress. This training data is used to construct a classifier that determines the probability that a voxel in an unseen image belongs to the target structure(s). The output of a rich set of basis filters serves as input to the classifier. In a feature selection process, this set is reduced to a compact, efficient subset. We show that the output of the system can be reused to extract new features, using the same filters, that can be processed by a new classifier. Such a multistage approach further improves performance. While the approach is generally applicable, in this work the focus is on enhancing pulmonary fissures in 3-D computed tomography (CT) chest scans. A supervised fissure enhancement filter is evaluated on two data sets, one of scans with a normal clinical dose and one of ultra-low dose scans. Results are compared with those of a recently proposed conventional fissure enhancement filter. It is demonstrated that both methods are able to enhance fissures, but the supervised approach shows better performance; the areas under the receiver operating characteristic (ROC) curve are 0.98 versus 0.90, for the normal dose data and 0.97 versus 0.87 for the ultra low dose data, respectively.","Filters,
Computed tomography,
Eigenvalues and eigenfunctions,
Tensile stress,
Pattern recognition,
Biomedical imaging,
Filtering theory,
Biomedical image processing,
Training data,
Feature extraction"
System Architecture of the LabPET Small Animal PET Scanner,"To address modern molecular imaging requirements, a digital positron emission tomography (PET) scanner for small animals has been developed at Universite de Sherbrooke. Based on individual readout of avalanche photodiodes (APD) coupled to LYSO/LGSO phoswich detectors, the scanner supports up to 4608 channels in a 16.2 cm diameter, 11.25 cm axial field of view with an isotropic ~ 1.2 mm FWHM intrinsic spatial resolution at the center of the field of view. Custom data acquisition boards preprocess and sample APD signals at 45 MHz and compute in real time crystal identification, energy and timing information of detected events at an average sustained rate of up to 1250 raw counts per second per mm2 (10 000 cps/channel). Real time digital signal analysis also filters out events outside the pre-selected energy window with crystal granularity to eliminate Compton events and electronic noise. Retained events are then merged into a single stream through a real-time sorting tree, at which end prompt and delayed coincidences are extracted. A single Firewire link handles both control and data transfers with a host computer. The LabPET features four data recording modes, giving the user the choice to retain data for research or to minimize file size for high coincidence count rate and imaging purposes. The electronic system also supports time synchronized data insertion for flags such as vital signs used in gated image reconstruction. Aside from data acquisition, hardware can generate live energy and discrimination spectra suitable for fast, automatic channel calibration.","Animals,
Positron emission tomography,
Data acquisition,
Firewire,
Molecular imaging,
Avalanche photodiodes,
Detectors,
Spatial resolution,
Signal processing,
Timing"
Goal-Based Modeling of Dynamically Adaptive System Requirements,"Self-adaptation is emerging as an increasingly important capability for many applications, particularly those deployed in dynamically changing environments, such as ecosystem monitoring and disaster management. One key challenge posed by Dynamically Adaptive Systems (DASs) is the need to handle changes to the requirements and corresponding behavior of a DAS in response to varying environmental conditions. Berry et al. previously identified four levels of RE that should be performed for a DAS. In this paper, we propose the Levels of RE for Modeling that reify the original levels to describe RE modeling work done by DAS developers. Specifically, we identify four types of developers: the system developer, the adaptation scenario developer, the adaptation infrastructure developer, and the DAS research community. Each level corresponds to the work of a different type of developer to construct goal model(s) specifying their requirements. We then leverage the Levels of RE for Modeling to propose two complementary processes for performing RE for a DAS. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed to monitor the River Ribble in Yorkshire, England.",
Power reduction of CMP communication networks via RF-interconnects,"As chip multiprocessors scale to a greater number of processing cores, on-chip interconnection networks will experience dramatic increases in both bandwidth demand and power dissipation. Fortunately, promising gains can be realized via integration of Radio Frequency Interconnect (RF-I) through on-chip transmission lines with traditional interconnects implemented with RC wires. While prior work has considered the latency advantage of RF-I, we demonstrate three further advantages of RF-I: (1) RF-I bandwidth can be flexibly allocated to provide an adaptive NoC, (2) RF-I can enable a dramatic power and area reduction by simplification of NoC topology, and (3) RF-I provides natural and efficient support for multicast. In this paper, we propose a novel interconnect design, exploiting dynamic RF-I bandwidth allocation to realize a reconfigurable network-on-chip architecture. We find that our adaptive RF-I architecture on top of a mesh with 4B links can even outperform the baseline with 16B mesh links by about 1%, and reduces NoC power by approximately 65% including the overhead incurred for supporting RF-I.","Communication networks,
Network-on-a-chip,
Bandwidth,
Multiprocessor interconnection networks,
Power dissipation,
Radio frequency,
Power transmission lines,
Wires,
Delay,
Network topology"
Early evaluation of IBM BlueGene/P,"BlueGene/P (BG/P) is the second generation BlueGene architecture from IBM, succeeding BlueGene/L (BG/L). BG/P is a system-on-a-chip (SoC) design that uses four PowerPC 450 cores operating at 850 MHz with a double precision, dual pipe floating point unit per core. These chips are connected with multiple interconnection networks including a 3-D torus, a global collective network, and a global barrier network. The design is intended to provide a highly scalable, physically dense system with relatively low power requirements per flop. In this paper, we report on our examination of BG/P, presented in the context of a set of important scientific applications, and as compared to other major large scale supercomputers in use today. Our investigation confirms that BG/P has good scalability with an expected lower performance per processor when compared to the Cray XT4's Opteron. We also find that BG/P uses very low power per floating point operation for certain kernels, yet it has less of a power advantage when considering science-driven metrics for mission applications.","System-on-a-chip,
Computer networks,
Bandwidth,
Laboratories,
Multiprocessor interconnection networks,
Hardware,
Ethernet networks,
Power generation,
Large-scale systems,
Supercomputers"
Nonstationary Fuzzy Sets,"In this paper, the notion termed a ldquononstationary fuzzy setrdquo is introduced, and the concept of a perturbation function that is used for generating nonstationary fuzzy sets is presented. Definitions of the basic set operators (the union, the intersection, and the complement) for nonstationary fuzzy sets are given, together with proofs of selected properties of these operators. Two case studies were carried out in order to illustrate the use of nonstationary fuzzy sets in a nonstationary fuzzy inference, and to provide an initial insight into the relationships between nonstationary and interval type-2 fuzzy sets.","Fuzzy sets,
Uncertainty,
Decision making,
Councils,
Government,
Computer science,
Information technology,
Computational efficiency,
Inference algorithms"
Nesting One-Against-One Algorithm Based on SVMs for Pattern Classification,"Support vector machines (SVMs), which were originally designed for binary classifications, are an excellent tool for machine learning. For the multiclass classifications, they are usually converted into binary ones before they can be used to classify the examples. In the one-against-one algorithm with SVMs, there exists an unclassifiable region where the data samples cannot be classified by its decision function. This paper extends the one-against-one algorithm to handle this problem. We also give the convergence and computational complexity analysis of the proposed method. Finally, one-against-one, fuzzy decision function (FDF), and decision-directed acyclic graph (DDAG) algorithms and our proposed method are compared using five University of California at Irvine (UCI) data sets. The results report that the proposed method can handle the unclassifiable region better than others.","Pattern classification,
Classification algorithms,
Machine learning algorithms,
Support vector machines,
Support vector machine classification,
Computer science,
Algorithm design and analysis,
Machine learning,
Convergence,
Computational complexity"
SIFT implementation and optimization for multi-core systems,"Scale Invariant Feature Transform (SIFT) is an approach for extracting distinctive invariant features from images, and it has been successfully applied to many computer vision problems (e.g. face recognition and object detection). However, the SIFT feature extraction is compute-intensive, and a real-time or even super-real-time processing capability is required in many emerging scenarios. Nowadays, with the multi-core processor becoming mainstream, SIFT can be accelerated by fully utilizing the computing power of available multi-core processors. In this paper, we propose two parallel SIFT algorithms and present some optimization techniques to improve the implementation’s performance on multi-core systems. The result shows our improved parallel SIFT implementation can process general video images in super-real-time on a dual-socket, quad-core system, and the speed is much faster than the implementation on GPUs. We also conduct a detailed scalability and memory performance analysis on the 8-core system and on a 32-core Chip Multiprocessor (CMP) simulator. The analysis helps us identify possible causes of bottlenecks, and we suggest avenues for scalability improvement to make this application more powerful on future large-scale multi-core systems.","Feature extraction,
Scalability,
Computer vision,
Face recognition,
Object detection,
Acceleration,
Multicore processing,
Performance analysis,
Analytical models,
Large-scale systems"
Distributed pose averaging in camera networks via consensus on SE(3),"In this paper, we propose distributed algorithms for estimating the average pose of an object viewed by a localized network of camera motes. To this effect, we propose distributed averaging consensus algorithms on the group of 3-D rigid-body transformations, SE(3). We rigorously analyze the convergence of the proposed algorithms, and show that naive generalizations of Euclidean consensus algorithms fail to converge to the correct solution. We also provide synthetic experiments that confirm our analysis and validate our approach.","Manifolds,
Protocols,
Cameras,
Distance measurement,
Optimization,
Convergence,
Algorithm design and analysis"
On-demand transparency for improving hardware Trojan detectability,"Malevolent Trojan circuits inserted by layout modifications in an IC at untrustworthy fabrication facilities are difficult to detect by traditional post-manufacturing testing. In this paper, we develop a novel low-overhead design methodology that facilitates the detection of inserted Trojan hardware in an IC through logic testing. As a byproduct, it also increases the security of the design by design obfuscation. Application of the proposed design methodology to an 8-bit RISC processor and a JPEG encoder resulted in improvement in Trojan detection probability significantly. It also obfuscated the design with verification mismatch for 90% of the verification points, while incurring moderate area, power and delay overheads.",
Online constraint network optimization for efficient maximum likelihood map learning,"In this paper, we address the problem of incrementally optimizing constraint networks for maximum likelihood map learning. Our approach allows a robot to efficiently compute configurations of the network with small errors while the robot moves through the environment. We apply a variant of stochastic gradient descent and use a tree-based parameterization of the nodes in the network. By integrating adaptive learning rates in the parameterization of the network, our algorithm can use previously computed solutions to determine the result of the next optimization run. Additionally, our approach updates only the parts of the network which are affected by the newly incorporated measurements and starts the optimization approach only if the new data reveals inconsistencies with the network constructed so far. These improvements yield an efficient solution for this class of online optimization problems. Our approach has been implemented and tested on simulated and on real data. We present comparisons to recently proposed online and offline methods that address the problem of optimizing constraint network. Experiments illustrate that our approach converges faster to a network configuration with small errors than the previous approaches.","Constraint optimization,
Robotics and automation,
Simultaneous localization and mapping,
Computer networks,
Robots,
Stochastic processes,
Information filters,
USA Councils,
Adaptive systems,
Testing"
Enhancing AODV routing protocol using mobility parameters in VANET,"VANET is new generation of ad hoc networks that implement between vehicles on a road. Because of high mobility, routing in VANET has more problems than MANET. Thereby, in this paper we propose a modification on AODV as MANET routing protocol to make it adaptive for VANET. When a node is mobile, it has three mobility parameters: position, direction and speed. In our method, we have used direction as most important parameter to select next hop during a route discovery phase. With respect to mobility model, if nodes has same direction with source and/or destination nodes, our solution might selects them as a next hop. Position is another parameter that we used for next hop selection.",
On the Connection-Level Stability of Congestion-Controlled Communication Networks,"In this paper, we are interested in the connection-level stability of a network employing congestion control. In particular, we study how the stability region of the network (i.e., the set of offered loads for which the number of active users in the network remains finite) is affected by congestion control. Previous works in the literature typically adopt a time-scale separation assumption, which assumes that, whenever the number of users in the system changes, the data rates of the users are adjusted instantaneously to the optimal and fair rate allocation. Under this assumption, it has been shown that such rate assignment policies can achieve the largest possible stability region. In this paper, this time-scale separation assumption is removed and it is shown that the largest possible stability region can still be achieved by a large class of control algorithms. A second assumption often made in prior work is that the packets of a source (or user) are offered to each link along its path instantaneously, rather than passing through one queue at a time. We show that connection-level stability is again maintained when this assumption is removed, provided that a back-pressure scheduling algorithm is used jointly with the appropriate congestion controller.","Communication networks,
Communication system control,
Iterative algorithms,
Control systems,
Scheduling algorithm,
Conferences,
Computer science,
Asymptotic stability,
Feedback,
Delay"
Formal Models for Expert Finding on DBLP Bibliography Data,"Finding relevant experts in a specific field is often crucial for consulting, both in industry and in academia. The aim of this paper is to address the expert-finding task in a real world academic field. We present three models for expert finding based on the large-scale DBLP bibliography and Google Scholar for data supplementation. The first, a novel weighted language model, models an expert candidate based on the relevance and importance of associated documents by introducing a document prior probability, and achieves much better results than the basic language model. The second, a topic-based model, represents each candidate as a weighted sum of multiple topics, whilst the third, a hybrid model, combines the language model and the topic-based model. We evaluate our system using a benchmark dataset based on human relevance judgments of how well the expertise of proposed experts matches a query topic. Evaluation results show that our hybrid model outperforms other models in nearly all metrics.","Bibliographies,
Data mining,
Computer science,
Data engineering,
Computer industry,
Mining industry,
Large-scale systems,
Humans,
Application software,
Information retrieval"
Multiobjective Optimization of SLA-Aware Service Composition,"In Service Oriented Architecture, each application is often designed as a set of abstract services, which defines its functions. A concrete service(s) is selected at runtime for each abstract service to fulfill its function. Since different concrete services may operate at different Quality of Service measures, application developers are required to select an appropriate set of concrete services that satisfies a given Service Level Agreement when a number of concrete services are available for each abstract service. This problem, the QoS-aware service composition problem, is known NP-hard, which takes a significant amount of time and costs to find optimal solutions (optimal combinations of concrete services) from a huge number of possible solutions. This paper proposes an optimization framework, called
E
3
, to address the issue. By leveraging a multiobjective genetic algorithm, E3 heuristically solves the QoS-aware service composition problem in a reasonably short time. The algorithm E3 proposes can consider multiple SLAs simultaneously and produce a set of Pareto solutions, which have the equivalent quality to satisfy multiple SLAs.",
Distributed Recovery of Actor Failures in Wireless Sensor and Actor Networks,"Wireless sensor and actor networks (WSANs) additionally employ actor nodes within the wireless sensor network (WSN) which can process the sensed data and perform certain actions based on this collected data. In most applications, inter-actor coordination is required to provide the best response. This suggests that the employed actors should form and maintain a connected inter-actor network at all times. However, WSANs often operate unattended in harsh environments where actors can easily fail or get damaged. Such failures can partition the inter-actor network and thus eventually make the network useless. In order to handle such failures, we present a connected dominating set (CDS) based partition detection and recovery algorithm. The idea is to identify whether the failure of a node causes partitioning or not in advance. If a partitioning is to occur, the algorithm designates one of the neighboring nodes to initiate the connectivity restoration process. This process involves repositioning of a set of actors in order to restore the connectivity. The overall goal in this restoration process is to localize the scope of the recovery and minimize the movement overhead imposed on the involved actors. The effectiveness of the approach is validated through simulation experiments.","Wireless sensor networks,
Peer to peer computing,
Partitioning algorithms,
Computer science,
Fires,
Collaboration,
Sea measurements,
Landmine detection,
Radiation detectors,
Communications Society"
Distributed source coding using short to moderate length rate-compatible LDPC codes: the entire Slepian-Wolf rate region,"In this paper, we propose a scheme for distributed source coding of correlated sources using a single systematic LDPC code. In particular, since we are interested in wireless sensor network applications, we consider LDPC codes with short to moderate lengths that achieve every arbitrary coding rate on the Slepian-Wolf rate region. We simplify the distributed source coding problem to the rate-compatible LDPC code design with an unequal error protection property. The decoders communicate to each other to exchange information bits prior to decoding. However, thereafter, each performs the decoding independently. Therefore, errors in one decoder do not affect the other one. The simulation results confirm that the gap from the theoretical limit remains almost the same for different rates on the Slepian-Wolf rate region. First, we consider two correlated sources. We show that our proposed scheme improves the performance of distributed source coding of two sources considerably. This benefit is more stressed for application with short to moderate length sequences. Then, we study distributed source coding of three sources. As a special case, we investigate three sources that are pairwise correlated with the same correlation probability. We show that the gap from the theoretical limit is smaller than that of previous work. We also investigate the distributed source coding of correlated sources when there is no prior knowledge of the correlation parameter at the time of code design. We note that although the proposed distributed source coding is well suited for sensor networks (where sequences with less than 10000 bits are used), the method can be generalized to other distributed source coding applications.","Source coding,
Parity check codes,
Wireless sensor networks,
Decoding,
Error correction codes,
Scattering,
Base stations,
Channel coding,
Communications Society,
Computer science"
Time-Domain Optimized Near-Field Estimator for Ultrasound Imaging: Initial Development and Results,"For nearly four decades, adaptive beamforming (ABF) algorithms have been applied in RADAR and SONAR signal processing. These algorithms reduce the contribution of undesired off-axis signals while maintaining a desired response along a specific look direction. Typically, higher resolution and contrast is attainable using adaptive beamforming at the price of an increased computational load. In this paper, we describe a novel ABF designed for medical ultrasound, named the time-domain optimized near-field estimator (TONE). We performed a series of simulations using synthetic ultrasound data to test the performance of this algorithm and compared it to conventional, data independent, delay and sum beamforming (CBF) method. We also performed experiments using a Philips SONOS 5500 phased array imaging system. CBF was applied using the default parameters of the Philips scanner, whereas TONE was applied on per channel, unfocused data using an unfocused transmit beam. TONE images were reconstructed at a sampling of 67 m laterally and 19 m axially. The results obtained for a series of five 20-m wires in a water tank show a significant improvement in spatial resolution when compared to CBF. We also analyzed the performance of TONE as a function of speed of sound errors and array sparsity, finding it robust to both.","Ultrasonic imaging,
Time domain analysis,
Signal processing algorithms,
Array signal processing,
Radar signal processing,
Phased arrays,
High-resolution imaging,
Radar imaging,
Sonar applications,
Adaptive signal processing"
Cluster filtered KNN: A WLAN-based indoor positioning scheme,"Location Based Service (LBS) is one kind of ubiquitous applications whose functions are based on the locations of clients. The core of LBS is an effective positioning system. As wireless LAN (WLAN) costs less and is easy to access, using WLAN for indoor positioning has been widely studied recently. K Nearest Neighbors (KNN) is one of the basic deterministic fingerprint based algorithms and widely used for WLAN-based indoor positioning. However, KNN takes all the nearest K neighbors for calculating the estimated result, which could be improved if some selective work could be done to those neighbors beforehand. In this paper we propose a new scheme called ‘Cluster Filtered KNN’ (CFK). CFK utilizes clustering technique to partition those neighbors into different clusters and chooses one cluster as the delegate. In the end, the final estimate can be calculated only based on the elements of the delegate. With experiments, we found that CFK does outperform KNN.","Distance measurement,
Wireless LAN,
Fingerprint recognition,
Nearest neighbor searches,
Databases,
Wireless communication,
Buildings"
Survey of Non-facial/Non-verbal Affective Expressions for Appearance-Constrained Robots,"Non-facial and non-verbal methods of affective expression are essential for naturalistic social interaction in robots that are designed to be functional and lack expressive faces (appearance-constrained) such as those used in search and rescue, law enforcement, and military applications. This correspondence identifies five main methods of non-facial and non-verbal affective expression (body movement, posture, orientation, color, and sound), and ranks their effectiveness for appearance-constrained robots operating within the intimate, personal, and social proximity zones of a human corresponding to interagent distances of approximately 3 m or less. This distance is significant because it encompasses the most common human social interaction distances, the exception being the public distance zone used for formal presentations. The correspondence complements prior, broad surveys of affective expression by reviewing the psychology, computer science, and robotics literature specifically relating the impact of social interaction in non-anthropomorphic and appearance-constrained robots, and summarizing robotic implementations that utilize non-facial and non-verbal methods of affective expression as their primary means of expression. The literature is distilled into a set of prescriptive recommendations of the appropriate affective expression methods for each of the three proximity zones of interest. These recommendations serve as design guidelines for retroactively adding affective expression through software to a robot without physical modifications or designing a new robot.","Human robot interaction,
Anthropomorphism,
Law enforcement,
Guidelines,
Power generation economics,
Environmental economics,
Psychology,
Computer science,
Mobile communication,
Animals"
Robust Gradient-Based 3-D/2-D Registration of CT and MR to X-Ray Images,"One of the most important technical challenges in image-guided intervention is to obtain a precise transformation between the intrainterventional patient's anatomy and corresponding preinterventional 3-D image on which the intervention was planned. This goal can be achieved by acquiring intrainterventional 2-D images and matching them to the preinterventional 3-D image via 3-D/2-D image registration. A novel 3-D/2-D registration method is proposed in this paper. The method is based on robustly matching 3-D preinterventional image gradients and coarsely reconstructed 3-D gradients from the intrainterventional 2-D images. To improve the robustness of finding the correspondences between the two sets of gradients, hypothetical correspondences are searched for along normals to anatomical structures in 3-D images, while the final correspondences are established in an iterative process, combining the robust random sample consensus algorithm (RANSAC) and a special gradient matching criterion function. The proposed method was evaluated using the publicly available standardized evaluation methodology for 3-D/2-D registration, consisting of 3-D rotational X-ray, computed tomography, magnetic resonance (MR), and 2-D X-ray images of two spine segments, and standardized evaluation criteria. In this way, the proposed method could be objectively compared to the intensity, gradient, and reconstruction-based registration methods. The obtained results indicate that the proposed method performs favorably both in terms of registration accuracy and robustness. The method is especially superior when just a few X-ray images and when MR preinterventional images are used for registration, which are important advantages for many clinical applications.","Robustness,
Computed tomography,
X-ray imaging,
Image reconstruction,
Anatomy,
Image registration,
Anatomical structure,
Iterative algorithms,
Magnetic resonance,
Image segmentation"
A Distributed Replication Strategy Evaluation and Selection Framework for Fault Tolerant Web Services,"Redundancy-based fault tolerance strategies are proposed for building reliable Service-Oriented Architectures/Applications (SOA), which are usually developed on the unpredictable remote Web services. This paper proposes and implements a distributed replication strategy evaluation and selection framework for fault tolerant Web services. Based on this framework, we provide a systematic comparison of various replication strategies by theoretical formula and real-world experiments. Moreover, a userparticipated strategy selection algorithm is designed and verified. Experiments are conducted to illustrate the advantage of this framework. In these experiments, users from six different locations all over the world perform evaluation of Web services distributed in six countries. Over 1,000,000 test cases are executed in a collaborative manner and detailed results are also provided.","Fault tolerance,
Web services,
Service oriented architecture,
Algorithm design and analysis,
Web and internet services,
Delay,
Telecommunication network reliability,
Computer science,
Reliability engineering,
Buildings"
Capacity theorems for quantum multiple-access channels: classical-quantum and quantum-quantum capacity regions,"In this paper, we consider quantum channels with two senders and one receiver. For an arbitrary such channel, we give multiletter characterizations of two different two-dimensional capacity regions. The first region comprises the rates at which it is possible for one sender to send classical information, while the other sends quantum information. The second region consists of the rates at which each sender can send quantum information. For each region, we give an example of a channel for which the corresponding region has a single-letter description. One of our examples relies on a new result proved here, perhaps of independent interest, stating that the coherent information over any degradable channel is concave in the input density operator. We conclude with connections to other work and a discussion on generalizations where each user simultaneously sends classical and quantum information.","Quantum mechanics,
Quantum computing,
Channel capacity,
Computer science,
Degradation,
Information systems,
Information theory,
Source coding"
VAST: Virtualization-Assisted Concurrent Autonomous Self-Test,"Virtualization-Assisted concurrent, autonomous Self-Test, or VAST, enables a multi-/many-core system to test itself, concurrently during normal operation, without any user-visible downtime. Such on-line self-test is required for large-scale robust systems with built-in support for circuit failure prediction, failure detection, diagnosis, and self-healing. The main idea behind VAST is hardware and software co-design of on-line self-test features in a multi-/many-core system through integration of: 1. multi-/many-core architecture, 2. virtualization software, and, 3. special self-test techniques such as BIST (Built-In Self-Test) or CASP (Concurrent Autonomous chip self-test using Stored Patterns). As a result, optimized trade-offs in system design complexity, system performance and power impact, and test thoroughness are possible. Experimental results from an actual multi-core system demonstrate that: 1. VAST is practical and effective; and, 2. Special VAST-supported self-test policies enable extremely thorough on-line self-test with very small performance impact.","Built-in self-test,
System testing,
Circuit testing,
Automatic testing,
Large-scale systems,
Robustness,
Hardware,
Computer architecture,
Design optimization,
System performance"
Extracting and Modeling Product Line Functional Requirements,"We introduce an extractive approach to building a product line's requirements assets. We define the functional requirements profiles (FRPs) according to the linguistic characterization of a domain's action-oriented concerns, and show that FRPs can be extracted from a document based on domain-aware lexical affinities that bear a 'verb -  direct object' relation. The validated FRPs are then amenable to semantic case analysis so as to uncover the variation structures. Finally, merging FRPs helps discover the requirements interdependencies. We use orthogonal variability modeling to represent the product line's external variability and constraints. We apply our approach to an auto-marker product line. The study shows our approach complements domain analysis by quickly offering insights into system functionalities and product line variabilities.",
Power-supply-variation-aware timing analysis of synchronous systems,"With state of the art technology scaling, the problem of delay variability due to power supply variations is becoming more and more critical. This paper addresses the problem of analyzing the speed degradation in synchronous systems caused by power supply IR-drop in deep submicron CMOS devices. Considering the impact of power supply variation on the clock skew value, violations of the timing constraints equations are presented. To satisfy the timing constraints over a range of 20% of VDD variation, a 42% increase in the operational clock period has to be met with circuits operating at 2GHz and implemented on 65nm CMOS technology.","Timing,
Power supplies,
Clocks,
Circuits,
Voltage,
CMOS technology,
Variable structure systems,
Delay,
Performance loss,
Degradation"
Semantic Provenance for eScience: Managing the Deluge of Scientific Data,"Provenance information in eScience is metadata that's critical to effectively manage the exponentially increasing volumes of scientific data from industrial-scale experiment protocols. Semantic provenance, based on domain-specific provenance ontologies, lets software applications unambiguously interpret data in the correct context. The semantic provenance framework for eScience data comprises expressive provenance information and domain-specific provenance ontologies and applies this information to data management. The authors' ""two degrees of separation"" approach advocates the creation of high-quality provenance information using specialized services. In contrast to workflow engines generating provenance information as a core functionality, the specialized provenance services are integrated into a scientific workflow on demand. This article describes an implementation of the semantic provenance framework for glycoproteomics.",
Dynamic scene shape reconstruction using a single structured light pattern,"3D acquisition techniques to measure dynamic scenes and deformable objects with little texture are extensively researched for applications like the motion capturing of human facial expression. To allow such measurement, several techniques using structured light have been proposed. These techniques can be largely categorized into two types. The first involves techniques to temporally encode positional information of a projector’s pixels using multiple projected patterns, and the second involves techniques to spatially encode positional information into areas or color spaces. Although the former allows dense reconstruction with a sufficient number of patterns, it has difficulty in scanning objects in rapid motion. The latter technique uses only a single pattern, so this problem can be resolved, however, it often uses complex patterns or color intensities, which are weak to noise, shape distortions, or textures. Thus, it remains an open problem to achieve dense and stable 3D acquisition in real cases. In this paper, we propose a technique to achieve dense shape reconstruction that requires only a single-frame image of a grid pattern. The proposed technique also has the advantage of being robust in terms of image processing.","Layout,
Image reconstruction,
Colored noise,
Shape measurement,
Motion measurement,
Humans,
Spatial resolution,
Noise shaping,
Robustness,
Image processing"
"Computation of Diffusion Function Measures in q
-Space Using Magnetic Resonance Hybrid Diffusion Imaging","The distribution of water diffusion in biological tissues may be estimated by a 3-D Fourier transform (FT) of diffusion-weighted measurements in q-space. In this study, methods for estimating diffusion spectrum measures (the zero-displacement probability, the mean-squared displacement, and the orientation distribution function) directly from the q-space signals are described. These methods were evaluated using both computer simulations and hybrid diffusion imaging (HYDI) measurements on a human brain. The HYDI method obtains diffusion-weighted measurements on concentric spheres in q-space. Monte Carlo computer simulations were performed to investigate effects of noise, q-space truncation, and sampling interval on the measures. This new direct computation approach reduces HYDI data processing time and image artifacts arising from 3-D FT and regridding interpolation. In addition, it is less sensitive to the noise and q-space truncation effects than conventional approach. Although this study focused on data using the HYDI scheme, this computation approach may be applied to other diffusion sampling schemes including Cartesian diffusion spectrum imaging.","Q measurement,
Computer simulation,
Image sampling,
Biology computing,
Biological tissues,
Fourier transforms,
Displacement measurement,
Distribution functions,
Anthropometry,
Humans"
"Granular computing: Past, present and future","Granular computing is gradually changing from a label to a new field of study. The driving forces, the major schools of thought, and the future research directions on granular computing are examined. A triarchic theory of granular computing is outlined. Granular computing is viewed as an interdisciplinary study of human-inspired computing, characterized by structured thinking, structured problem solving, and structured information processing.","Problem-solving,
Humans,
Artificial intelligence,
Computational modeling,
Brain modeling,
Search problems,
Fuzzy sets"
Genetic Algorithms with Memory- and Elitism-Based Immigrants in Dynamic Environments,"In recent years the genetic algorithm community has shown a growing interest in studying dynamic optimization problems. Several approaches have been devised. The random immigrants and memory schemes are two major ones. The random immigrants scheme addresses dynamic environments by maintaining the population diversity while the memory scheme aims to adapt genetic algorithms quickly to new environments by reusing historical information. This paper investigates a hybrid memory and random immigrants scheme, called memory-based immigrants, and a hybrid elitism and random immigrants scheme, called elitism-based immigrants, for genetic algorithms in dynamic environments. In these schemes, the best individual from memory or the elite from the previous generation is retrieved as the base to create immigrants into the population by mutation. This way, not only can diversity be maintained but it is done more efficiently to adapt genetic algorithms to the current environment. Based on a series of systematically constructed dynamic problems, experiments are carried out to compare genetic algorithms with the memory-based and elitism-based immigrants schemes against genetic algorithms with traditional memory and random immigrants schemes and a hybrid memory and multi-population scheme. The sensitivity analysis regarding some key parameters is also carried out. Experimental results show that the memory-based and elitism-based immigrants schemes efficiently improve the performance of genetic algorithms in dynamic environments.","elitism-based immigrants,
Genetic algorithms,
dynamic optimization problems,
memory,
random immigrants,
memory-based immigrants"
Arithmetic Circuits: A Chasm at Depth Four,"We show that proving exponential lower bounds on depth four arithmeticcircuits imply exponential lower bounds for unrestricted depth arithmeticcircuits. In other words, for exponential sized circuits additional depthbeyond four does not help.We then show that a complete black-box derandomization of Identity Testing problem for depth four circuits with multiplicationgates of small fanin implies a nearly complete derandomization of general Identity Testing.","Circuit testing,
Polynomials,
Digital arithmetic,
Computer science,
Information systems,
Galois fields,
Size measurement,
Costs"
The cache-and-forward network architecture for efficient mobile content delivery services in the future internet,"This paper presents a novel “cache-and-forward” (CNF) protocol architecture for mobile content delivery services in the future Internet. The CNF architecture can be implemented as an overlay on top of the Internet Protocol (IP), or as a clean slate protocol for next-generation networks. CNF is based on the concept of store-and-forward routers with large storage, providing for opportunistic delivery to occasionally disconnected mobile users and for in-network caching of content. The proposed CNF protocol uses reliable hop-by-hop transfer of large data files between CNF routers in place of an end-to-end transport protocol like TCP. This approach makes it possible to serve mobile users with intermittent connectivity, while also mitigating self-interference problems which arise in multi-hop wireless scenarios. Hop-by-hop transport is similarly useful in wired networks where router storage can help to smooth out link congestion bottlenecks which arise in TCP/IP networks. A second key feature of the CNF protocol is the integration of address-based and content-based routing to support various content delivery modes that take advantage of in-network storage. An overview of the CNF architecture and major protocol components is given, and preliminary performance evaluation results are summarized to validate the main design principles.","IP networks,
Web and internet services,
TCPIP,
Wireless application protocol,
Computer architecture,
Spread spectrum communication,
Mobile computing,
Transport protocols,
Costs,
Wireless sensor networks"
Firewall Compressor: An Algorithm for Minimizing Firewall Policies,"A firewall is a security guard placed between a private network and the outside Internet that monitors all incoming and outgoing packets. The function of a firewall is to examine every packet and decide whether to accept or discard it based upon the firewall's policy. This policy is specified as a sequence of (possibly conflicting) rules. When a packet comes to a firewall, the firewall searches for the first rule that the packet matches, and executes the decision of that rule. With the explosive growth of Internet-based applications and malicious attacks, the number of rules in firewalls have been increasing rapidly, which consequently degrades network performance and throughput. In this paper, we propose Firewall Compressor, a framework that can significantly reduce the number of rules in a firewall while keeping the semantics of the firewall unchanged. We make three major contributions in this paper. First, we propose an optimal solution using dynamic programming techniques for compressing one-dimensional firewalls. Second, we present a systematic approach to compressing multi-dimensional firewalls. Last, we conducted extensive experiments to evaluate Firewall Compressor. In terms of effectiveness, Firewall Compressor achieves an average compression ratio of 52.3% on real- life rule sets. In terms of efficiency, Firewall Compressor runs in seconds even for a large firewall with thousands of rules. Moreover, the algorithms and techniques proposed in this paper are not limited to firewalls. Rather, they can be applied to other rule-based systems such as packet filters on Internet routers.","IP networks,
Communications Society,
Computer science,
Computer security,
Computer displays,
Explosives,
Degradation,
Throughput,
Dynamic programming,
Knowledge based systems"
Novel Silicon-Controlled Rectifier (SCR) for High-Voltage Electrostatic Discharge (ESD) Applications,"Electrostatic discharge (ESD) protection for high-voltage integrated circuits is challenging due to the requirement of high holding voltage to minimize the risk of ESD-induced latchup and electrical overstress. In this letter, a new silicon-controlled rectifier (SCR) is developed for this particular application. The SCR is designed based on the concept that the holding voltage can be increased by reducing the emitter injection efficiency in the SCR. This is accomplished by using a segmented emitter topology. Experimental data show that the new SCR can possess a holding voltage that is larger than 40 V and a failure current It2 that is higher than 28 mA/mum.","Electrostatic discharge,
Rectifiers,
Thyristors,
Voltage,
Protection,
Resistors,
Application specific integrated circuits,
Topology,
Clamps,
Computer science"
Toward loosely coupled programming on petascale systems,"We have extended the Falkon lightweight task execution framework to make loosely coupled programming on petascale systems a practical and useful programming model. This work studies and measures the performance factors involved in applying this approach to enable the use of petascale systems by a broader user community, and with greater ease. Our work enables the execution of highly parallel computations composed of loosely coupled serial jobs with no modifications to the respective applications. This approach allows a new—and potentially far larger—class of applications to leverage petascale systems, such as the IBM Blue Gene/P supercomputer. We present the challenges of I/O performance encountered in making this model practical, and show results using both microbenchmarks and real applications from two domains: economic energy modeling and molecular dynamics. Our benchmarks show that we can scale up to 160K processor-cores with high efficiency, and can achieve sustained execution rates of thousands of tasks per second.","Petascale computing,
Optical coupling,
Concurrent computing,
Supercomputers,
Power generation economics"
"Generic Carrier-Based Core Model for Undoped Four-Terminal Double-Gate MOSFETs Valid for Symmetric, Asymmetric, and Independent-Gate-Operation Modes","A generic carrier-based core model for undoped four-terminal double-gate (DG) MOSFETs has been developed and is presented in this paper. The model is valid for symmetric, asymmetric, and independent-gate-operation modes. Based on the exact solution of the 1-D Poisson's equation in a general DG MOSFET configuration, a rigorous derivation of the drain-current equations from the Pao-Sah's double integral has been performed. By using the channel carriers as the intermediate variable, a very compact analytical drain-current expression can be obtained. The model is extensively verified by comparisons with a 2-D numerical simulator under a large number of biasing conditions. The concise mathematical formulation allows the unification of various DG models into a carrier-based core model for a compact DG MOSFET model development.","MOSFETs,
Mathematical model,
Silicon,
Integrated circuit modeling,
Equations,
Numerical models,
Electric potential"
Improving Structural Testing of Object-Oriented Programs via Integrating Evolutionary Testing and Symbolic Execution,"Achieving high structural coverage such as branch coverage in object-oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the symbolic execution technique and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and symbolic execution (used to generate desirable method arguments). We have implemented our framework and applied it to test 13 classes previously used in evaluating white-box test generation tools. The experimental results show that the tests generated using our framework can achieve higher branch coverage than the ones generated by evolutionary testing, symbolic execution, or random testing within the same amount of time.","Testing,
Biological cells,
Encoding,
Construction industry,
Receivers,
Genetic algorithms,
Skeleton"
Nonthreshold-Based Event Detection for 3D Environment Monitoring in Sensor Networks,"Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.","Event detection,
Monitoring,
Wireless sensor networks,
Sensor phenomena and characterization,
Spatiotemporal phenomena,
Vehicle detection,
Energy efficiency,
Pattern matching,
Safety,
Robustness"
C-Oracle: Predictive thermal management for data centers,"Designing thermal management policies for today’s power-dense server clusters is currently a challenge, since it is difficult to predict the exact temperature and performance that would result from trying to react to a thermal emergency. To address this challenge, in this paper we propose C-Oracle, a software infrastructure for Internet services that dynamically predicts the temperature and performance impact of different thermal management reactions into the future, allowing the thermal management policy to select the best reaction at each point in time. We experimentally evaluate C-Oracle for thermal management policies based on load redistribution and dynamic voltage/frequency scaling in both single-tier and multi-tier services. Our results show that, regardless of management policy or service organization, C-Oracle enables non-trivial decisions that effectively manage thermal emergencies, while avoiding unnecessary performance degradation.","Servers,
Thermal management,
Degradation,
Temperature control,
Feedback control,
Turning,
Thermal loading"
Validation and Application of a Computational Model for Wrist and Hand Movements Using Surface Markers,"A kinematic model is presented based on surface marker placement generating wrist, metacarpal arch, fingers and thumb movements. Standard calculations are used throughout the model and then applied to the specified marker placement. A static trial involving eight unimpaired participants was carried out to assess inter-rater reliability. The standard deviations across the data were comparable to manual goniometers. In addition, a test-retest trial of ten unimpaired participants is also reported to illustrate the variability of movement at the wrist joint, metacarpal arch, and index finger as an example of model output when repeating the same task many times. Light and heavyweight versions of the tasks are assessed and characteristics of individual movement strategies presented. The participant trial showed moderate correlation in radial/ulnar deviation of the wrist (r = 0.65), and strong correlation in both metacarpal arch joints (r = 0.75 and r = 0.85), the MCP (r = 0.79), and PIP (r = 0.87) joints of the index finger. The results indicate that individuals use repeated strategies of movement when lifting light and heavyweight versions of the same object, but showed no obvious repeated pattern of movement across the population.",
NTHU-Route 2.0: A fast and stable global router,"We present in this paper a fast and stable global router called NTHU-Route 2.0 that improves the solution quality and runtime of a state-of-the-art router, NTHU-Route, by the following enhancements: (1) a new history based cost function, (2) new ordering methods for congested region identification and rip-up and reroute, and (3) two implementation techniques. The experimental results show that NTHU-Router 2.0 solves all ISPD98 benchmarks with very good quality. Moreover, it routes 7 of 8 ISPD07 benchmarks without any overflow. In particular, for one of the ISPD07 benchmarks which are thought to be difficult cases previously, NTHU-Route 2.0 can completely eliminate its total overflow. NTHU-Route 2.0 also successfully solves 12 of 16 ISPD08 benchmarks without causing any overflow.","Routing,
Delay,
Computer science,
Runtime,
History,
Cost function,
Pins,
Very large scale integration,
Integrated circuit interconnections,
Joining processes"
Whole body humanoid control from human motion descriptors,"Many advanced motion control strategies developed in robotics use captured human motion data as valuable source of examples to simplify the process of programming or learning complex robot motions. Direct and online control of robots from observed human motion has several inherent challenges. The most important may be the representation of the large number of mechanical degrees of freedom involved in the execution of movement tasks. Attempting to map all such degrees of freedom from a human to a humanoid is a formidable task from an instrumentation and sensing point of view. More importantly, such an approach is incompatible with mechanisms in the central nervous system which are believed to organize or simplify the control of these degrees of freedom during motion execution and motor learning phase. Rather than specifying the desired motion of every degree of freedom for the purpose of motion control, it is important to describe motion by low dimensional motion primitives that are defined in Cartesian (or task) space. In this paper, we formulate the human to humanoid retargeting problem as a task space control problem. The control objective is to track desired task descriptors while satisfying constraints such as joint limits, velocity limits, collision avoidance, and balance. The retargeting algorithm generates the joint space trajectories that are commanded to the robot. We present experimental and simulation results of the retargeting control algorithm on the Honda humanoid robot ASIMO.","Motion control,
Humans,
Robot programming,
Robot motion,
Robot control,
Instruments,
Central nervous system,
Centralized control,
Control systems,
Velocity control"
Automatic Inference and Enforcement of Kernel Data Structure Invariants,"Kernel-level rootkits affect system security by modifying key kernel data structures to achieve a variety of malicious goals. While early rootkits modified control data structures, such as the system call table and values of function pointers, recent work has demonstrated rootkits that maliciously modify non-control data. Prior techniques for rootkit detection fail to identify such rootkits either because they focus solely on detecting control data modifications or because they require elaborate, manually-supplied specifications to detect modifications of non-control data. This paper presents a novel rootkit detection technique that automatically detects rootkits that modify both control and non-control data. The key idea is to externally observe the execution of the kernel during a training period and hypothesize invariants on kernel data structures. These invariants are used as specifications of data structure integrity during an enforcement phase; violation of these invariants indicates the presence of a rootkit. We present the design and implementation of Gibraltar, a tool that uses the above approach to infer and enforce invariants. In our experiments, we found that Gibraltar can detect rootkits that modify both control and non-control data structures, and that its false positive rate and monitoring overheads are negligible.","Kernel,
Data structures,
Automatic control,
Control systems,
Computer security,
Condition monitoring,
Application software,
Computer science,
Data security,
Degradation"
2-D Locally Regularized Tissue Strain Estimation From Radio-Frequency Ultrasound Images: Theoretical Developments and Results on Experimental Data,"In this paper, a 2-D locally regularized strain estimation method for imaging deformation of soft biological tissues from radio-frequency (RF) ultrasound (US) data is introduced. Contrary to most 2-D techniques that model the compression-induced local displacement as a 2-D shift, our algorithm also considers a local scaling factor in the axial direction. This direction-dependent model of tissue motion and deformation is induced by the highly anisotropic resolution of RF US images. Optimal parameters are computed through the constrained maximization of a similarity criterion defined as the normalized correlation coefficient. Its value at the solution is then used as an indicator of estimation reliability, the probability of correct estimation increasing with the correlation value. In case of correlation loss, the estimation integrates an additional constraint, imposing local continuity within displacement and strain fields. Using local scaling factors and regularization increase the method's robustness with regard to decorrelation noise, resulting in a wider range of precise measurements. Results on simulated US data from a mechanically homogeneous medium subjected to successive uniaxial loadings demonstrate that our method is theoretically able to accurately estimate strains up to 17%. Experimental strain images of phantom and cut specimens of bovine liver clearly show the harder inclusions.","Capacitive sensors,
Radio frequency,
Ultrasonic imaging,
Biological system modeling,
Biological tissues,
Image coding,
Deformable models,
Anisotropic magnetoresistance,
Image resolution,
Noise robustness"
Perturbative Refinement of the Geometric Calibration in Pinhole SPECT,"The paper investigates the geometric calibration of a rotating gamma camera for pinhole (PH) single photon emission computed tomography (SPECT) imaging. Most calibration methods previously applied in PH-SPECT assume that the motion of the camera around the object belongs to a well-defined class described by a small number of geometric parameters, for instance seven parameters for a circular acquisition with a single pinhole camera. The proposed new method refines an initial parametric calibration by applying to each position of the camera a rigid body transformation that is determined to improve the fit between the measured and calculated projections of the calibration sources. A stable estimate of this transformation can be obtained with only three calibration sources by linearizing the equations around the position estimated by the initial parametric calibration. The performance of the method is illustrated using simulated and measured micro-SPECT data.","Calibration,
Cameras,
Optical imaging,
Collimators,
Single photon emission computed tomography,
Nuclear medicine,
Solid modeling,
Position measurement,
Spatial resolution,
Detectors"
Intrinsically Multivariate Predictive Genes,"Canalizing genes possess such broad regulatory power, and their action sweeps across a such a wide swath of processes that the full set of affected genes are not highly correlated under normal conditions. When not active, the controlling gene will not be predictable to any significant degree by its subject genes, either alone or in groups, since their behavior will be highly varied relative to the inactive controlling gene. When the controlling gene is active, its behavior is not well predicted by any one of its targets, but can be very well predicted by groups of genes under its control. To investigate this question, we introduce in this paper the concept of intrinsically multivariate predictive (IMP) genes, and present a mathematical study of IMP in the context of binary genes with respect to the coefficient of determination (CoD), which measures the predictive power of a set of genes with respect to a target gene. A set of predictor genes is said to be IMP for a target gene if all properly contained subsets of the predictor set are bad predictors of the target but the full predictor set predicts the target with great accuracy. We show that logic of prediction, predictive power, covariance between predictors, and the entropy of the joint probability distribution of the predictors jointly affect the appearance of IMP genes. In particular, we show that high-predictive power, small covariance among predictors, a large entropy of the joint probability distribution of predictors, and certain logics, such as XOR in the 2-predictor case, are factors that favor the appearance of IMP. The IMP concept is applied to characterize the behavior of the gene DUSP1, which exhibits control over a central, process-integrating signaling pathway, thereby providing preliminary evidence that IMP can be used as a criterion for discovery of canalizing genes.","Irrigation,
Genomics,
Bioinformatics,
Cancer,
Logic,
Entropy,
Probability distribution,
Computer science,
Stress,
Power measurement"
Generalized Semantics-Based Service Composition,"Service-oriented computing (SOC) has emerged as the eminent market environment for sharing and reusing service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services.  Although industry approaches to composition have a strong notion of business processes, these approaches largely use syntactic descriptions. As such composition is limited since the true functionality of ambiguous service operations cannot be inferred. Alternatively, academia uses semantic approaches to disambiguate services, but, at the same time, most of these approaches neglect the process rigor needed for complex compositions. In this paper we present a generalized semantics-based technique for automatic service composition that combines the rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG) where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented. Furthermore, the composition can be represented semantically as OWL-S documents. Our contributions are applied for automatic workflow generation in context of the currently important bioinformatics domain.","Web services,
Bioinformatics,
Computer science,
Automatic control,
Engines,
Process planning,
Application software,
Satellites,
Automation,
Humans"
Overhead Analysis of Scientific Workflows in Grid Environments,"Scientific workflows are a topic of great interest in the grid community that sees in the workflow model an attractive paradigm for programming distributed wide-area grid infrastructures. Traditionally, the grid workflow execution is approached as a pure best effort scheduling problem that maps the activities onto the grid processors based on appropriate optimization or local matchmaking heuristics such that the overall execution time is minimized. Even though such heuristics often deliver effective results, the execution in dynamic and unpredictable grid environments is prone to severe performance losses that must be understood for minimizing the completion time or for the efficient use of high-performance resources. In this paper, we propose a new systematic approach to help the scientists and middleware developers understand the most severe sources of performance losses that occur when executing scientific workflows in dynamic grid environments. We introduce an ideal model for the lowest execution time that can be achieved by a workflow and explain the difference to the real measured grid execution time based on a hierarchy of performance overheads for grid computing. We describe how to systematically measure and compute the overheads from individual activities to larger workflow regions and adjust well-known parallel processing metrics to the scope of grid computing, including speedup and efficiency. We present a distributed online tool for computing and analyzing the performance overheads in real time based on event correlation techniques and introduce several performance contracts as quality-of-service parameters to be enforced during the workflow execution beyond traditional best effort practices. We illustrate our method through postmortem and online performance analysis of two real-world workflow applications executed in the Austrian grid environment.",
DiSTM: A Software Transactional Memory Framework for Clusters,"While Transactional Memory (TM) research on shared-memory chip multiprocessors has been flourishing over the last years,limited research has been conducted in the cluster domain. In this paper,we introduce a research platform for exploiting software TMon clusters. The Distributed Software Transactional Memory (DiSTM) system has been designed for easy prototyping of TM coherence protocols and it does not rely on a software or hardware implementation of distributed shared memory. Three TM coherence protocols have been implemented and evaluated with established TM benchmarks. The decentralized  TransactionalCoherence and Consistency protocol has been compared against two centralized protocols that utilize leases. Results indicate thatdepending on network congestion and amount of contention different protocols perform better.","Protocols,
Coherence,
Software,
Engines,
Benchmark testing,
Java,
Synchronization"
Hybrid Arc/Glow Microdischarges at Atmospheric Pressure and Their Use in Portable Systems for Liquid and Gas Sensing,"This paper reports on DC pulse-powered microdischarges in air at atmospheric pressure and their potential utility in chemical sensing. For electrode gaps of 50-100 mum, microdischarges take the form of a glow discharge, an arc discharge, or a hybrid of the two. Arc microdischarges have high optical intensity but suffer from high background emission. Glow microdischarges have low background emission, but the prominent emission is confined in the UV-blue region of the spectrum. The arc-glow hybrid has characteristics that are intermediate between the two and can be tuned by circuitry to suit the chemical sensing application. A handheld system for chemical analysis using synchronized emission spectroscopy of these pulsed microdischarges is demonstrated. The system employs an exchangeable sensor chip (different for gas and liquid samples), a control circuit, and a commercially available portable spectrum analyzer coupled to a handheld computer. A pump and inert carrier gases are not utilized. The system can generate one or a series of single-shot microdischarges per chemical analysis. The gas discharge microchip, which utilizes electroplated copper electrodes on a glass substrate, has an electrode separation of 75 mum and an active area of 300times300 mum2. The handheld system has been used to detect 17 ppm of acetone vapor in air. The liquid discharge microchip also has an electrode gap of 75 mum and an active area of 1times1 mm2. It uses a porous cathode fabricated by micromolding and sintering glass frit slurry in a microchannel. When a microdischarge is initiated between the metal anode and the wet cathode, the liquid is sputtered into the microdischarge and emits characteristic line spectra. In this configuration, the system can detect 2 ppm of aqueous Cr without preconcentration.","Electrodes,
Chemical analysis,
Glass,
Cathodes,
Optical pulses,
Glow discharges,
Arc discharges,
Stimulated emission,
Optical pumping,
Optical sensors"
Crime and Punishment for Cognitive Radios,"Frequency-agile radios hold the potential for improving spectrum utilization by allowing wireless systems to dynamically adapt their spectral footprint based on local conditions. Whether this is done using market mechanisms or opportunistic approaches, the gains result from shifting some responsibility for avoiding harmful interference from the static “regulatory layer„ to layers that can adapt at runtime. However, this leaves open the major problem of how to enforce/incentivize compliance and what the structure of “light-handed„ regulation should be. This paper addresses this question by developing a model for the incentives associated with cheating and for the tradeoffs between different elements of an enforcement structure which will effectively deter cheating. It then investigates a code-based scheme for detecting and assigning liability to culprits.","Cognitive radio,
Interference,
Space technology,
Frequency,
Certification,
Computer crime,
Runtime,
FCC,
Radio broadcasting,
TV broadcasting"
Secure Authentication using Image Processing and Visual Cryptography for Banking Applications,"Core banking is a set of services provided by a group of networked bank branches. Bank customers may access their funds and perform other simple transactions from any of the member branch offices. The major issue in core banking is the authenticity of the customer. Due to unavoidable hacking of the databases on the internet, it is always quite difficult to trust the information on the internet. To solve this problem of authentication, we are proposing an algorithm based on image processing and visual cryptography. This paper proposes a technique of processing the signature of a customer and then dividing it into shares. Total number of shares to be created is depending on the scheme chosen by the bank. When two shares are created, one is stored in the Bank database and the other is kept by the customer. The customer has to present the share during all of his transactions. This share is stacked with the first share to get the original signature. The Correlation method is used to take the decision on acceptance or rejection of the output and authenticate the customer.","Authentication,
Image processing,
Cryptography,
Banking,
Image databases,
Transaction databases,
Visual databases,
Internet,
Computer crime,
Correlation"
Human swarm interaction for radiation source search and localization,This study1shows that appropriate human interaction can benefit a swarm of robots to achieve goals more efficiently. A set of desirable features for human swarm interaction is identified based on the principles of swarm robotics. Human swarm interaction architecture is then proposed that has all of the desirable features. A swarm simulation environment is created that allows simulating a swarm behavior in an indoor environment. The swarm behavior and the results of user interaction are studied by considering radiation source search and localization application of the swarm. Particle swarm optimization algorithm is slightly modified to enable the swarm to autonomously explore the indoor environment for radiation source search and localization. The emergence of intelligence is observed that enables the swarm to locate the radiation source completely on its own. Proposed human swarm interaction is then integrated in a simulation environment and user evaluation experiments are conducted. Participants are introduced to the interaction tool and asked to deploy the swarm to complete the missions. The performance comparison of the user guided swarm to that of the autonomous swarm shows that the interaction interface is fairly easy to learn and that user guided swarm is more efficient in achieving the goals. The results clearly indicate that the proposed interaction helped the swarm achieve emergence.,"Human robot interaction,
Particle swarm optimization,
Indoor environments,
Insects,
USA Councils,
Intelligent robots,
Laboratories,
Collaboration,
Chirp,
Educational robots"
Decentralized Diagnosis of Stochastic Discrete Event Systems,"We investigate the decentralized diagnosis of stochastic discrete event systems (SDESs) by using multiple local stochastic diagnosers, each possessing its own sensors to deal with different information. We formalize the notions of decentralized diagnosis for SDESs by defining the concept of codiagnosability for stochastic automata, in which any communication among the local stochastic diagnosers or to any coordinators is not involved. These notions are weaker than the corresponding notions of decentralized diagnosis of classical DESs. A stochastic system being codiagnosable means that a fault can be detected by at least one local stochastic diagnoser within a finite delay. We construct a codiagnoser from a given stochastic system with a finite number of projections whose each diagnosis component uses the complete model of the system. We also deal with a number of basic properties of the codiagnoser. In particular, a necessary and sufficient condition of the codiagnosability for SDESs is presented, which generalizes the corresponding results of centralized diagnosis for SDESs. Also, we give a computing method in detail to check the codiagnosability of SDESs. As an application of our results, some examples are described.","Stochastic systems,
Discrete event systems,
Fault diagnosis,
Stochastic processes,
Automata,
Delay,
Fault detection,
Automatic control,
Computer science,
Sensor systems"
Solving Box-Constrained Integer Least Squares Problems,"A box-constrained integer least squares problem (BILS) arises from several wireless communications applications. Solving a BILS problem usually has two stages: reduction (or preprocessing) and search. This paper presents a reduction algorithm and a search algorithm. Unlike the typical reduction algorithms, which use only the information of the lattice generator matrix, the new reduction algorithm also uses the information of the given input vector and the box constraint and is very effective for search. The new search algorithm overcomes some shortcomings of the existing search algorithms and gives some other improvement. Simulation results indicate the combination of the new reduction algorithm and the new search algorithm can be much more efficient than the existing algorithms, in particular when the least squares residual is large.",
A New Model of Information Content for Semantic Similarity in WordNet,"Information Content(IC) is an important dimension of assessing the semantic similarity between two terms or word senses in word knowledge. The conventional method of obtaining IC of word senses is to combine knowledge of their hierarchical structure from an ontology like WordNet with actual usage in text as derived from a large corpus. In this paper, a new model of IC is presented, which relies on hierarchical structure alone. The model considers not only the hyponyms of each word sense but also its depth in the structure. The IC value is easier to calculate based on our model, and when used as the basis of a similarity approach it yields judgments that correlate more closely with human assessments than others, which using IC value obtained only considering the hyponyms and IC value got by employing corpus analysis.",
Adaptive Routing in Underwater Delay/Disruption Tolerant Sensor Networks,"As an emerging technique, underwater sensor network (UWSN) will enable a wide range of aquatic applications. However, due to the adverse underwater environmental conditions as well as some system constraints, an underwater sensor network is usually viewed as an intermittently connected network (ICN) (or delay/disruption tolerant network (DTN)), which requires specialized routing protocols. Moreover, applications may have different requirements for different types of messages, as demands a smart protocol to handle packets adaptively. In this paper, we propose a novel routing protocol where routing is performed adaptively based on the types of messages and application requirements. This is obtained by exploiting message redundancy and resource reallocation in order to achieve different performance requirements. We demonstrate through simulations that our protocol can satisfy different application requirements and achieve a good trade-off among delivery ratio, end-to-end delay and energy consumption.","Disruption tolerant networking,
Routing protocols,
Delay,
Acoustic sensors,
Underwater communication,
Energy consumption,
Intelligent sensors,
Redundancy,
Pollution,
Surveillance"
Bitmask-Based Code Compression for Embedded Systems,"Embedded systems are constrained by the available memory. Code-compression techniques address this issue by reducing the code size of application programs. It is a major challenge to develop an efficient code-compression technique that can generate substantial reduction in code size without affecting the overall system performance. We present a novel code-compression technique using bitmasks, which significantly improves the compression efficiency without introducing any decompression penalty. This paper makes three important contributions. 1) It develops an efficient bitmask-selection technique that can create a large set of matching patterns. 2) It develops an efficient dictionary-selection technique based on bitmasks. 3) It proposes a dictionary-based code-compression algorithm using the bitmask- and dictionary-selection techniques that can significantly reduce the memory requirement. To demonstrate the usefulness of our approach, we have performed code compression using applications from various domains and compiled for a wide variety of architectures. Our approach outperforms the existing dictionary-based techniques by an average of 20%, giving a compression ratio of 55%-65%.","Embedded system,
Chromium,
System performance,
Pattern matching,
Costs,
Dictionaries,
Power dissipation,
Memory management,
Size measurement,
Information science"
First Human Brain Imaging by the jPET-D4 Prototype With a Pre-Computed System Matrix,"The jPET-D4 is a novel brain PET scanner which aims to achieve not only high spatial resolution but also high scanner sensitivity by using 4-layer depth-of-interaction (DOI) information. The dimensions of a system matrix for the jPET-D4 are 3.3 billion (lines-of-response) times 5 million (image elements) when a standard field-of-view (FOV) of 25 cm diameter is sampled with a (1.5 mm)3 voxel . The size of the system matrix is estimated as 117 petabytes (PB) with the accuracy of 8 bytes per element. An on-the-fly calculation is usually used to deal with such a huge system matrix. However we cannot avoid extension of the calculation time when we improve the accuracy of system modeling. In this work, we implemented an alternative approach based on pre-calculation of the system matrix. A histogram-based 3D OS-EM algorithm was implemented on a desktop workstation with 32 GB memory installed. The 117 PB system matrix was compressed under the limited amount of computer memory by (1) eliminating zero elements, (2) applying the DOI compression (DOIC) method and (3) applying rotational symmetry and an axial shift property of the crystal arrangement. Spanning, which degrades axial resolution, was not applied. The system modeling and the DOIC method, which had been validated in 2D image reconstruction, were expanded into 3D implementation. In particular, a new system model including the DOIC transformation was introduced to suppress resolution loss caused by the DOIC method. Experimental results showed that the jPET-D4 has almost uniform spatial resolution of better than 3 mm over the FOV. Finally the first human brain images were obtained with the jPET-D4.","Humans,
Brain,
Prototypes,
Spatial resolution,
Modeling,
High-resolution imaging,
Positron emission tomography,
Workstations,
Image coding,
Degradation"
ADL recognition based on the combination of RFID and accelerometer sensing,"The manual assessment of Activities of Daily Living (ADLs) is a fundamental problem in elderly care. The use of miniature sensors placed in the environment or worn by a person has great potential in effective and unobtrusive long term monitoring and recognition of ADLs. This paper presents an effective and unobtrusive activity recognition system based on the combination of the data from two different types of sensors: RFID tag readers and accelerometers. We evaluate our algorithms on non-scripted datasets of 10 housekeeping activities performed by 12 subjects. The experimental results show that recognition accuracy can be significantly improved by fusing the two different types of sensors. We analyze different acceleration features and algorithms, and based on tag detections we suggest the best tags’ placements and the key objects to be tagged for each activity.","Radiofrequency identification,
Accelerometers,
Senior citizens,
Wearable sensors,
Monitoring,
Sensor systems,
RFID tags,
Performance evaluation,
Algorithm design and analysis,
Acceleration"
Design and control of an active electrical knee and ankle prosthesis,"This paper presents an overview of the design and control of an electrically powered knee and ankle prosthesis. The prosthesis design incorporates two motor-driven ball screw units to drive the knee and ankle joints. A spring in parallel with the ankle motor unit is employed to decrease the power consumption and increase the torque output for a given motor size. The device’s sensor package includes a custom load cell to measure the sagittal socket interface moment above the knee joint, a custom sensorized foot to measure the ground reaction force at the heel and ball of the foot, and commercial potentiometers and load cells to measure joint positions and torques. A finite-state based impedance control approach, previously developed by the authors, is used and experimental results on level treadmill walking are presented that demonstrate the potential of the device to restore normal gait. The experimental power consumption of the device projects a walking distance of 5.0 km at a speed of 2.8 km/hr with a lithium polymer battery pack.","Knee,
Prosthetics,
Force measurement,
Position measurement,
Torque measurement,
Energy consumption,
Foot,
Legged locomotion,
Fasteners,
Springs"
Bad Words: Finding Faults in Spirit's Syslogs,"Accurate fault detection is a key element of resilient computing. Syslogs provide key information regarding faults, and are found on nearly all computing systems. Discovering new fault types requires expert human effort, however, as no previous algorithm has been shown to localize faults in time and space with an operationally acceptable false positive rate. We present experiments on three weeks of syslogs from Sandia's 512-node ""Spirit""' Linux cluster, showing one algorithm that localizes 50% of faults with 75% precision, corresponding to an excellent false positive rate of 0.05%. The salient characteristics of this algorithm are (1) calculation of nodewise information entropy, and (2) encoding of word position. The key observation is that similar computers correctly executing similar work should produce similar logs.","Programming profession,
Fault detection,
Humans,
Clustering algorithms,
Grid computing,
Laboratories,
USA Councils,
Computer science,
Supercomputers,
Monitoring"
On the hardness of approximating the network coding capacity,"This work addresses the computational complexity of achieving the capacity of a general network coding instance. We focus on the linear capacity, namely the capacity of the given instance when restricted to linear encoding functions. It has been shown [Lehman and Lehman, SODA 2005] that determining the (scalar) linear capacity of a general network coding instance is NP-hard. In this work we initiate the study of approximation in this context. Namely, we show that given an instance to the general network coding problem of linear capacity C, constructing a linear code of rate αC for any universal (i.e., independent of the size of the instance) constant α ≤ 1 is “hard”. Specifically, finding such network codes would solve a long standing open problem in the field of graph coloring. In addition, we consider the problem of determining the (scalar) linear capacity of a planar network coding instance (i.e., a general instance in which the underlying graph is planar). We show that even for planar networks this problem remains NP-hard.","Encoding,
Color,
Optimized production technology,
Approximation methods,
Servers,
Linear code,
Vectors"
An Opportunistic Cognitive MAC Protocol for Coexistence with WLAN,"In last decades, the demand of wireless spectrum has increased rapidly with the development of mobile communication services. Recent studies recognize that traditional fixed spectrum assignment does not use spectrum efficiently. Such a wasting phenomenon could be amended after the present of cognitive radio. Cognitive radio is a new type of technology that enables secondary usage to unlicensed user. This paper presents an opportunistic cognitive MAC protocol (OC-MAC) for cognitive radios to access unoccupied resource opportunistically and coexist with wireless local area network (WLAN). By a primary traffic predication model and transmission etiquette, OC-MAC avoids producing fatal damage to licensed users. Then a ns2 simulation model is developed to evaluate its performance in scenarios with coexisting WLAN and cognitive network.","Media Access Protocol,
Wireless LAN,
Chromium,
Cognitive radio,
Computer science,
Wireless application protocol,
Access protocols,
Telecommunication traffic,
White spaces,
Bandwidth"
The University of Southampton Multi-Biometric Tunnel and introducing a novel 3D gait dataset,"This paper presents the University of Southampton Multi-Biometric Tunnel, a constrained environment that is designed with airports and other high throughput environments in mind. It is able to acquire a variety of non-contact biometrics in a non-intrusive manner. The system uses eight synchronised IEEE1394 cameras to capture gait and additional cameras to capture images from the face and one ear, as an individual walks through the tunnel. We demonstrate that it is possible to achieve a 99.6% correct classification rate and a 4.3% equal error rate without feature selection using the gait data collected from the system; comparing well with state-of-art approaches. The tunnel acquires data automatically as a subject walks through it and is designed for the collection of very large gait datasets.","Biometrics,
Cameras,
Application software,
Automatic control,
Medical diagnostic imaging,
Airports,
Throughput,
Data security,
Computer vision,
Lighting control"
An Information Hiding Scheme Using Sudoku,"Steganography is the science of secret message delivery using cover media. A digital image is a flexible medium used to carry a secret message because the slight modification of a cover image is hard to distinguish by human eyes. The proposed method is inspired from Zhang and Wang's method and Sudoku solutions. A selected Sudoku solution is used to guide cover pixels' modification in order to imply secret data. Because the number of possible Sudoku solutions is very large, the proposed method is more secure than Mielikainen's method and Zhang and Wang's method. From the experimental results, the visual quality of stego images produced by the proposed method is higher than 44 dB in average, which is slightly less than that of related works; however, the embedding capacity of the proposed method is 1.5 bit per pixel, which is greater than that of the related works.","Pixel,
Visualization,
PSNR,
Data mining,
Matrix converters,
Gray-scale,
Streaming media"
Visual servoing for humanoid grasping and manipulation tasks,"Using visual feedback to control the movement of the end-effector is a common approach for robust execution of robot movements in real-world scenarios. Over the years several visual servoing algorithms have been developed and implemented for various types of robot hardware. In this paper, we present a hybrid approach which combines visual estimations with kinematically determined orientations to control the movement of a humanoid arm. The approach has been evaluated with the humanoid robot ARMAR III using the stereo system of the active head for perception as well as the torso and arms equipped with five finger hands for actuation. We show how a robust visual perception is used to control complex robots without any hand-eye calibration. Furthermore, the robustness of the system is improved by estimating the hand position in case of failed visual hand tracking due to lightning artifacts or occlusions. The proposed control scheme is based on the fusion of the sensor channels for visual perception, force measurement and motor encoder data. The combination of these different data sources results in a reactive, visually guided control that allows the robot ARMAR-III to execute grasping tasks in a real-world scenario.","Visual servoing,
Robust control,
Visual perception,
Robot sensing systems,
Feedback,
Hardware,
Humanoid robots,
Torso,
Manipulators,
Fingers"
Following Controller for Autonomous Mobile Robots Using Behavioral Cues,"This paper proposes an autonomous-robot following controller that can integrate information provided from behavioral cues of the leader to increase the reliability and the performance of following. The controller continuously estimates the future predicted position of the leader as it moves, and then directs the follower robot to this position. A Kalman filter is employed for an estimation that uses vision-based measurements of leader position, a dynamic model of the leader, and a behavioral-cue model of the leader. The behavioral-cue model serves to either tune the dynamic model and/or create pseudomeasurements to further help the Kalman filter estimate the leader's future position. Once the leader's future position is estimated, a trajectory planner plans a path to the future position, and a motor controller implements the required control signals to the robot wheels. It is suggested that this controller may have particular importance for human following by autonomous robots in future human-robot interaction environments.",
Robust dual motion deblurring,"This paper presents a robust algorithm to deblur two consecutively captured blurred photos from camera shaking. Previous dual motion deblurring algorithms succeeded in small and simple motion blur and are very sensitive to noise. We develop a robust feedback algorithm to perform iteratively kernel estimation and image deblurring. In kernel estimation, the stability and capability of the algorithm is greatly improved by incorporating a robust cost function and a set of kernel priors. The robust cost function serves to reject outliers and noise, while kernel priors, including sparseness and continuity, remove ambiguity and maintain kernel shape. In deblurring, we propose a novel and robust approach which takes two blurred images as input to infer the clear image. The deblurred image is then used as feedback to refine kernel estimation. Our method can successfully estimate large and complex motion blurs which cannot be handled by previous dual or single image motion deblurring algorithms. The results are shown to be significantly better than those of previous approaches.","Noise robustness,
Kernel,
Iterative algorithms,
Feedback,
Robust stability,
Cost function,
Cameras,
Image restoration,
Noise shaping,
Shape"
Optimal Constant Composition Codes From Zero-Difference Balanced Functions,"Constant composition codes are a special class of constant weight codes, and include permutation codes as a subclass. They have applications in communications engineering. In this correspondence, a generic construction of optimal constant composition codes using zero-difference balanced functions is introduced. It generalizes the earlier construction of optimal constant composition codes employing perfect nonlinear functions. In addition, two classes of optimal constant composition codes with new parameters are reported.","Hamming weight,
Hamming distance,
History,
Computer science,
Codes,
Upper bound"
Building detection from aerial images using invariant color features and shadow information,"Robust detection of buildings is an important part of the automated aerial image interpretation problem. Automatic detection of buildings enables creation of maps, detecting changes, and monitoring urbanization. Due to the complexity and uncontrolled appearance of the scene, an intelligent fusion of different methods gives better results. In this study, we present a novel approach for building detection using multiple cues. We benefit from segmentation of aerial images using invariant color features. Besides, we use the edge and shadow information for building detection. We also determine the shape of the building by a novel method.","Image edge detection,
Buildings,
Image segmentation,
Shape,
Data mining,
Computer vision,
Gray-scale,
Testing,
Lighting,
Laboratories"
SpotWeb: Detecting Framework Hotspots and Coldspots via Mining Open Source Code on the Web,"Software developers often face challenges in reusing open source frameworks due to several factors such as the framework complexity and lack of proper documentation. In this paper, we propose a code-search-engine-based approach that detects hotspots in a given framework by mining code examples gathered from open source repositories available on the Web; these hotspots are API classes and methods that are frequently reused. Hotspots can serve as starting points for developers in understanding and reusing the given framework. Our approach also detects coldspots, which are API classes and methods that are rarely used. Coldspots serve as caveats for developers as there can be difficulties in finding relevant code examples and are generally less exercised compared to hotspots. We developed a tool, called SpotWeb, for frameworks or libraries written in Java and used our tool to detect hotspots and coldspots of eight widely used open source frameworks. We show the utility of our detected hotspots by comparing these hotspots with the API classes reused by a real application and compare our results with the results of a previous related approach.","Classification algorithms,
Documentation,
Java,
Search engines,
Data mining,
Computer bugs,
USA Councils"
A generalization of the nonlinear small-gain theorem for large-scale complex systems,"This paper presents a generalization of the nonlinear small-gain theorem for large-scale complex systems consisting of multiple input-to-output stable systems. It includes as a special case the previous nonlinear small-gain theorems with two interconnected systems, and recent small-gain theorems for networks of input-to-state stable subsystems. It is expected that this new small-gain theorem will find wide applications in the analysis and control synthesis of large-scale complex systems.","Gain,
Interconnected systems,
Lyapunov method,
Stability analysis,
Stability criteria,
Large-scale systems,
Nonlinear systems"
Loper: A quadruped-hybrid stair climbing robot,"The purpose of this paper is to describe the Loper, a multi-purpose robotic platform under development at the University of Minnesota's Center for Distributed Robotics. Leper's unique Tri-lobe wheel design and highly compliant chassis make the platform especially suited for overcoming many of the challenges associated with search operations in urban settings. The mechanically simple design and use of commercially available components make Loper easily maintainable. The platform also features long operational time, onboard sensor processing, dedicated motion control, and four reconfigurable sensor bays.","Climbing robots,
Mobile robots,
Wheels,
Humanoid robots,
Legged locomotion,
Control systems,
Motion control,
Robotics and automation,
Sensor phenomena and characterization,
Couplings"
Exploratory factor analysis of gait recognition,"Many studies have now shown that it is possible to recognize people by the way they walk. As yet there has been little formal study of the effects of covariates on the recognition process. We show how these factors can separately affect the walking pattern. Further we assess the contribution and discriminatory significance of the gait dynamics used for recognition. Based on a covariate-based probe dataset of 440 samples, a high recognition rate of 73.4% is achieved using the KNN classifier. This is to confirm that people identification using dynamic gait features is still perceivable with better recognition rate even under the different covariate factors.","Legged locomotion,
Biometrics,
Surveillance,
Footwear,
Clothing,
Video sequences,
Probes,
Cameras,
Face recognition,
Fingerprint recognition"
CasJobs and MyDB: A Batch Query Workbench,"Catalog archive server jobs (CasJobs) is an asynchronous query workbench service that lets users run unrestricted SQL queries against scientific catalog archives. After running queries in batch mode, users can save their results to a personal database called MyDB before downloading them, letting users manage their query workloads, results, and histories without causing network overloads.","Content addressable storage,
Object oriented databases,
History,
Rendering (computer graphics),
Web server,
Load management,
Pattern analysis,
Radio access networks,
Degradation,
Transaction databases"
Plethysmogram-based secure inter-sensor communication in Body Area Networks,"Body Area Networks (BAN) can play a major role in monitoring the health of soldiers in a battlefield. Securing BANs is essential to ensure safety of the soldiers. This paper presents a novel key agreement protocol called Photoplethysmogram PPGbased based Key Agreement (PKA) which allows sensors in a BAN to agree to a common key using PPG values obtained from the subject (soldier) they are deployed on. Using the stimuli which the sensors are designed to monitor directly for cryptographic purposes, enables administrators to provide security for BANs with minimal initial setup. The principal contributions of this paper are: 1) demonstration of the viability of the PPG signals for agreeing upon common symmetric cryptographic keys between two nodes in BAN, and 2) analysis of the security, performance and quality of the keys produced by PKA.","Body area networks,
Body sensor networks,
Cryptography,
Biomedical monitoring,
Wearable sensors,
Sensor phenomena and characterization,
Cryptographic protocols,
Security,
Wireless communication,
Wireless sensor networks"
A Novel Method of Combined Feature Extraction for Recognition,"Multimodal recognition is an emerging technique to overcome the non-robustness of the unimodal recognition in real applications. Canonical correlation analysis (CCA) has been employed as a powerful tool for feature fusion in the realization of such multimodal system. However, CCA is the unsupervised feature extraction and it does not utilize the class information of the samples, resulting in the constraint of the recognition performance. In this paper, the class information is incorporated into the framework of CCA for combined feature extraction, and a novel method of combined feature extraction for multimodal recognition, called discriminative canonical correlation analysis (DCCA), is proposed. The experiments show that DCCA outperforms some related methods of both unimodal recognition and multimodal recognition.","Feature extraction,
Space technology,
Electronic mail,
Computer science,
Signal mapping,
Pattern recognition,
Speech recognition,
Data mining,
Data engineering,
Application software"
Dynamically adapting file domain partitioning methods for collective I/O based on underlying parallel file system locking protocols,"Collective I/O, such as that provided in MPI-IO, enables process collaboration among a group of processes for greater I/O parallelism. Its implementation involves file domain partitioning, and having the right partitioning is a key to achieving high-performance I/O. As modern parallel file systems maintain data consistency by adopting a distributed file locking mechanism to avoid centralized lock management, different locking protocols can have significant impact to the degree of parallelism of a given file domain partitioning method. In this paper, we propose dynamic file partitioning methods that adapt according to the underlying locking protocols in the parallel file systems and evaluate the performance of four partitioning methods under two locking protocols. By running multiple I/O benchmarks, our experiments demonstrate that no single partitioning guarantees the best performance. Using MPI-IO as an implementation platform, we provide guidelines to select the most appropriate partitioning methods for various I/O patterns and file systems.","File systems,
Protocols,
Collaboration,
Parallel processing,
File servers,
Network servers,
Data analysis,
Guidelines,
Production,
Telecommunication traffic"
EVAL: Utilizing processors with variation-induced timing errors,"Parameter variation in integrated circuits causes sections of a chip to be slower than others. If, to prevent any resulting timing errors, we design processors for worst-case parameter values, we may lose substantial performance. An alternate approach explored in this paper is to design for closer to nominal values, and provide some transistor budget to tolerate unavoidable variation-induced errors. To assess this approach, this paper first presents a novel framework that shows how microarchitecture techniques can trade off variation-induced errors for power and processor frequency. Then, the paper introduces an effective technique to maximize performance and minimize power in the presence of variation-induced errors, namely High-Dimensional dynamic adaptation. For efficiency, the technique is implemented using a machine-learning algorithm. The results show that our best configuration increases processor frequency by 56% on average, allowing the processor to cycle 21% faster than without variation. Processor performance increases by 40% on average, resulting in a performance that is 14% higher than without variation — at only a 10.6% area cost.","Timing,
Frequency,
Process design,
Microarchitecture,
Voltage,
Temperature,
Computer errors,
Logic,
Computer science,
Costs"
On the Use of Domain Terms in Source Code,"Information about the problem domain of the software and the solution it implements is often embedded by developers in comments and identifiers. When using software developed by others or when are new to a project, programmers know little about how domain information is reflected in the source code. Programmers often learn about the domain from external sources such as books, articles, etc. Hence, it is important to use in comments and identifiers terms that are commonly known in the domain literature, as it is likely that programmers will use such terms when searching the source code. The paper presents a case study that investigated how domain terms are used in comments and identifiers. The study focused on three research questions: (1) to what degree are domain terms found in the source code of software from a particular problem domain?; (2) which is the preponderant source of domain terms: identifiers or comments?; and (3) to what degree are domain terms shared between several systems from the same problem domain? Within the studied software, we found that in average: 42% of the domain terms were used in the source code; 23% of the domain terms used in the source code are present in comments only, whereas only 11% in the identifiers alone, and there is a 63% agreement in the use of domain terms between any two software systems.","Programming profession,
Software libraries,
Software systems,
Books,
Open source software,
Computer science,
Vocabulary,
Graphics,
Graph theory,
Embedded software"
Inhabitants Tracking System in a Cluttered Home Environment Via Floor Load Sensors,"Home automation systems should evolve to the next phase in which they are aware of contexts, because providing services based on contexts will upgrade the service quality, thus making people more comfortable and home living more convenient. In particular, location is a piece of important and useful context information for seeking appropriate services, as well as providing them to people living in a home. So far, there have been several studies focused on tracking inhabitants in smart home environments. However, these approaches are often intrusive or require inhabitants to wear some sort of devices, which may make the inhabitants uncomfortable or even inconvenient. This problem could devastate the ultimate goal, which is to provide convenient services, and hence cause such approaches somewhat controversial. In this study, we utilize a number of load sensors together to construct a sensory floor on which exerted pressure can be detected and cover its surface with wooden flooring as in a normal home environment. Although the wooden flooring provides a flat surface for inhabitants to walk on, it also causes clutter in the sensor readings, which lead to difficulty in clearly identifying the location(s) of pressure source(s). Thus, we apply probabilistic data association technique and LeZi-Update approach to analyze the cluttered pressure readings collected by the load sensors so as to determine the positions of the inhabitants, as well as to track their movements. With our nonintrusive approach, there is no need for inhabitants to wear any devices, and this also solves the cross-walking problem in the home environment. Note to Practitioners-This system aims for detecting the location of multiple inhabitants in the home environment. We adopt the sensory floor as our tracking sensor, which consists of many blocks, each with a load cell to collect the human body weight. These blocks are covered with conventional wooden flooring to provide a flat surface on which inhabitants can walk freely. After collecting data, we apply mathematical techniques to analyze them, thus determining inhabitants' locations and tracking their movements. The limitation is that the system cannot differentiate different persons if they have close weight readings, which is the natural limitation of the load cell.","Sensor systems,
Home automation,
Context-aware services,
Context awareness,
Tracking,
Humans,
Computer science,
Actuators,
Chaos,
Smart homes"
The Kneed Walker for human pose tracking,"The Kneed Walker is a physics-based model derived from a planar biomechanical characterization of human locomotion. By controlling torques at the knees, hips and torso, the model captures a full range of walking motions with foot contact and balance. Constraints are used to properly handle ground collisions and joint limits. A prior density over walking motions is based on dynamics that are optimized for efficient cyclic gaits over a wide range of natural human walking speeds and step lengths, on different slopes. The generative model used for monocular tracking comprises the Kneed Walker prior, a 3D kinematic model constrained to be consistent with the underlying dynamics, and a simple measurement model in terms of appearance and optical flow. The tracker is applied to people walking with varying speeds, on hills, and with occlusion.","Humans,
Legged locomotion,
Motion control,
Torque control,
Knee,
Hip,
Torso,
Foot,
Kinematics,
Fluid flow measurement"
Roadmap to adopting OPC UA,"This paper highlights a roadmap, consisting of three alternative paths, to adopting OPC UA, a new generation OPC specification. The OPC UA specification aims to improve and extend the scope of OPC in system integration. It introduces SOA paradigm and platform independence into OPC world, enabling a wide variety of new applications. It also provides integration from an embedded device up into the EPP level. This paper details ways to adapt existing systems to utilize OPC UA in order to timely take advantage of the new possibilities. Novel key features and application possibilities offered by OPC UA are also discussed. At last a small sample OPC UA application is presented as a proof of concept.","management information systems,
computer architecture,
enterprise resource planning"
Computational Approaches for Automatic Structural Analysis of Large Biomolecular Complexes,"We present computational solutions to two problems of macromolecular structure interpretation from reconstructed three-dimensional electron microscopy (3D-EM) maps of large bio-molecular complexes at intermediate resolution (5A-15A). The two problems addressed are: (a) 3D structural alignment (matching) between identified and segmented 3D maps of structure units (e.g. trimeric configuration of proteins), and (b) the secondary structure identification of a segmented protein 3D map (i.e. locations of alpha-helices, beta-sheets). For problem (a), we present an efficient algorithm to correlate spatially (and structurally) two 3D maps of structure units. Besides providing a similarity score between structure units, the algorithm yields an effective technique for resolution refinement of repeated structure units,by 3D alignment and averaging. For problem (b), we present an efficient algorithm to compute eigenvalues and link eigenvectors of a Gaussian convoluted structure tensor derived from the protein 3D Map, thereby identifying and locating secondary structural motifs of proteins. The efficiency and performance of our approach is demonstrated on several experimentally reconstructed 3D maps of virus capsid shells from single-particle cryo-EM, as well as computationally simulated protein structure density 3D maps generated from protein model entries in the Protein Data Bank.",
Topology Control for Maintaining Network Connectivity and Maximizing Network Capacity under the Physical Model,"In this paper we study the issue of topology control under the physical signal-to-interference-noise-ratio (SINR) model, with the objective of maximizing network capacity. We show that existing graph-model-based topology control captures interference inadequately under the physical SINR model, and as a result, the interference in the topology thus induced is high and the network capacity attained is low. Towards bridging this gap, we propose a centralized approach, called spatial reuse maximizer (MaxSR), that combines a power control algorithm T2P with a topology control algorithm P2T. T2P optimizes the assignment of transmit power given a fixed topology, where by optimality we mean that the transmit power is so assigned that it minimizes the average interference degree (defined as the number of interfering nodes that may interfere with the ongoing transmission on a link) in the topology. P2T, on the other hand, constructs, based on the power assignment made in T2P, a new topology by deriving a spanning tree that gives the minimal interference degree. By alternately invoking the two algorithms, the power assignment quickly converges to an operational point that maximizes the network capacity. We formally prove the convergence of MaxSR. We also show via simulation that the topology induced by MaxSR outperforms that derived from existing topology control algorithms by 50%-110% in terms of maximizing the network capacity.","Network topology,
Interference,
Communication system control,
Signal to noise ratio,
Transmitters,
Communications Society,
Computer science,
Power control,
Centralized control,
Peer to peer computing"
Voronoi State Management for Peer-to-Peer Massively Multiplayer Online Games,"State management is a basic requirement for multiuser virtual environments (VEs) such as massively multiplayer online games (MMOGs). Current MMOGs rely on centralized server-clusters that possess inherent scalability bottlenecks and are expensive to adopt and deploy. In this paper, we propose Voronoi state management (VSM) to maintain object states for peer-to-peer-based virtual worlds. By dynamically partitioning the VE with Voronoi diagrams and aggregating game states of overloaded nodes onto superpeers, VSM supports existing consistency control to enable scalable, load balanced, and fault tolerant VE state management. As both client and server-side resources are utilized collaboratively, VSM also integrates both client-server and peer-to-peer VE designs in a unified approach.","Peer to peer computing,
Avatars,
Network servers,
Virtual environment,
Scalability,
Fault tolerance,
Proposals,
Computer architecture,
Engineering management,
Computer science"
Instant Places: Using Bluetooth for Situated Interaction in Public Displays,"Public digital displays are increasingly pervasive and an important enabling technology for many types of ubiquitous computing scenarios. Not only do they provide a simple and effective way of bringing digital information into our physical world, but their presence could also be a catalyst for situated interaction and the emergence of local user-generated content. This paper explores the role of presence, particularly as enabled by Bluetooth device discovery, as the driver for the system's behavior and its situational awareness.","Bluetooth,
Visualization,
Pervasive computing,
Computer displays,
User-generated content,
Bridges,
Local activities,
Cultural differences,
Convergence,
Mediation"
"Decomposition, discovery and detection of visual categories using topic models",We present a novel method for the discovery and detection of visual object categories based on decompositions using topic models. The approach is capable of learning a compact and low dimensional representation for multiple visual categories from multiple view points without labeling of the training instances. The learnt object components range from local structures over line segments to global silhouette-like descriptions. This representation can be used to discover object categories in a totally unsupervised fashion. Furthermore we employ the representation as the basis for building a supervised multi-category detection system making efficient use of training examples and out-performing pure features-based representations. The proposed speed-ups make the system scale to large databases. Experiments on three databases show that the approach improves the state-of-the-art in unsupervised learning as well as supervised detection. In particular we improve the state-of-the-art on the challenging PASCAL’06 multi-class detection tasks for several categories.,
Automatic age classification with LBP,"Estimating the age exactly and then producing the younger and older images of the person is important in security systems design. In this paper local binary patterns are used to classify the age from facial images. The local binary patterns (LBP) are fundamental properties of local image texture and the occurrence histogram of these patterns is an effective texture feature for face description. In the study we classify the FERET images according to their ages with 10 years intervals. The faces are divided into small regions from which the LBP histograms are extracted and concatenated into a feature vector to be used as an efficient face descriptor. For every new face presented to the system, spatial LBP histograms are produced and used to classify the image into one of the age classes. In the classification phase, minimum distance, nearest neighbor and k-nearest neighbor classifiers are used. The experimental results have shown that system performance is 80% for age estimation.","Histograms,
Aging,
Face detection,
Feature extraction,
Image texture,
Image databases,
Pixel,
System performance,
Pediatrics,
Calendars"
Association and Temporal Rule Mining for Post-Filtering of Semantic Concept Detection in Video,"Automatic semantic concept detection in video is important for effective content-based video retrieval and mining and has gained great attention recently. In this paper, we propose a general post-filtering framework to enhance robustness and accuracy of semantic concept detection using association and temporal analysis for concept knowledge discovery. Co-occurrence of several semantic concepts could imply the presence of other concepts. We use association mining techniques to discover such inter-concept association relationships from annotations. With discovered concept association rules, we propose a strategy to combine associated concept classifiers to improve detection accuracy. In addition, because video is often visually smooth and semantically coherent, detection results from temporally adjacent shots could be used for the detection of the current shot. We propose temporal filter designs for inter-shot temporal dependency mining to further improve detection accuracy. Experiments on the TRECVID 2005 dataset show our post-filtering framework is both efficient and effective in improving the accuracy of semantic concept detection in video. Furthermore, it is easy to integrate our framework with existing classifiers to boost their performance.","Gunshot detection systems,
Content based retrieval,
Face detection,
Robustness,
Data mining,
Association rules,
Filters,
Bridges,
Terrorism,
Computer science"
Sum capacity of the Gaussian interference channel in the low interference regime,"New upper bounds on the sum capacity of the two-user Gaussian interference channel are derived. Using these bounds, it is shown that treating interference as noise achieves the sum capacity if the interference levels are below certain thresholds.","Interference,
Interference channels,
Upper bound,
Noise,
Receivers,
Random variables,
Mutual information"
Detouring: Translating software to circumvent hard faults in simple cores,"CMOS technology trends are leading to an increasing incidence of hard (permanent) faults in processors. These faults may be introduced at fabrication or occur in the field. Whereas high-performance processor cores have enough redundancy to tolerate many of these faults, the simple, low-power cores that are attractive for multicore chips do not. We propose Detouring, a software-based scheme for tolerating hard faults in simple cores. The key idea is to automatically modify software such that its functionality is unchanged but it does not use any of the faulty hardware. Our initial implementation of Detouring tolerates hard faults in several hardware components, including the instruction cache, registers, functional units, and the operand bypass network. Detouring has no hardware cost and no performance overhead for fault-free cores.","microprocessor chips,
electronic engineering computing,
fault tolerant computing"
Efficient modelling of spiking neural networks on a scalable chip multiprocessor,"we propose a system based on the Izhikevich model running on a scalable chip multiprocessor - SpiNNaker - for large-scale spiking neural network simulation. The design takes into account the requirements for processing, storage, and communication which are essential to the efficient modelling of spiking neural networks. To gain a speedup of the processing as well as saving storage space, the Izhikevich model is implemented in 16-bit fixed-point arithmetic. An approach based on using two scaling factors is developed, making the precision comparable to the original. With the two scaling factors scheme, all of the firing patterns by the original model can be reproduced with a much faster execution speed. To reduce the communication overhead, rather than sending synaptic weights on communicating, we only send out event packets to indicate the neuron firings while holding the synaptic weights in the memory of the post-synaptic neurons, which is so-called event-driven algorithm. The communication based on event packets can be handled efficiently by the multicast system supported by the SpiNNaker machine. We also describe a system level model for spiking neural network simulation based on the schemes above. The model has been functionally verified and experimental results are included. An analysis of the performance of the whole system is presented at the end of the paper.","Artificial neural networks,
Joints,
Conferences"
Mobility Metric based LEACH-Mobile Protocol,"Cluster based protocols like LEACH were found best suited for routing in wireless sensor networks. In mobility centric environments some improvements were suggested in the basic scheme. LEACH-Mobile is one such protocol. The basic LEACH protocol is improved in the mobile scenario by ensuring whether a sensor node is able to communicate with its cluster head. Since all the nodes, including cluster head is moving it will be better to elect a node as cluster head which is having less mobility related to its neighbours. In this paper, LEACH-Mobile protocol has been enhanced based on a mobility metric ""remoteness"" for cluster head election. This ensures high success rate in data transfer between the cluster head and the collector nodes even though nodes are moving. We have simulated and compared our LEACH-mobile-enhanced protocol with LEACH-mobile. Results show that inclusion of neighbouring node information improves the routing protocol.","Sensor phenomena and characterization,
Wireless sensor networks,
Routing protocols,
Intelligent sensors,
Wireless application protocol,
Monitoring,
Vehicles,
Mobile robots,
Robot sensing systems,
Nominations and elections"
SYNAPSE: A Network Reprogramming Protocol for Wireless Sensor Networks Using Fountain Codes,"Wireless reprogramming is a key functionality in wireless sensor networks (WSNs). In fact, the requirements for the network may change in time, or new parameters might have to be loaded to change the behavior of a given protocol. In large scale WSNs it makes economical as well as practical sense to upload the code with the needed functionalities without human intervention, i.e., by means of efficient over the air reprogramming. This poses several challenges as wireless links are affected by errors, data dissemination has to be 100% reliable, and data transmission and recovery schemes are often called to work with a large number of receivers. State-of-the-art protocols, such as Deluge, implement error recovery through the adaptation of standard automatic repeat request (ARQ) techniques. These, however, do not scale well in the presence of channel errors and multiple receivers. In this paper, we present an original reprogramming system for WSNs called SYNAPSE, which we designed to improve the efficiency of the error recovery phase. SYNAPSE features a hybrid ARQ (HARQ) solution where data are encoded prior to transmission and incremental redundancy is used to recover from losses, thus considerably reducing the transmission overhead. For the coding, digital fountain codes were selected as they are rateless and allow for lightweight implementations. In this paper, we design special fountain codes and use them at the heart of SYNAPSE to provide high performance while meeting the requirements of WSNs. Moreover, we present our implementation of SYNAPSE for the Tmote Sky sensor platform and show experimental results, where we compare the performance of SYNAPSE with that of state of the art protocols.","Wireless application protocol,
Wireless sensor networks,
Automatic repeat request,
Energy efficiency,
Topology,
Read-write memory,
Feedback,
Computer science,
Large-scale systems,
Environmental economics"
What Can We Learn Privately?,"Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in the contexts where aggregate information is released about a database containing sensitive information about individuals.",
Semiautomated Segmentation of Myocardial Contours for Fast Strain Analysis in Cine Displacement-Encoded MRI,"The purposes of this study were to develop a semiautomated cardiac contour segmentation method for use with cine displacement-encoded MRI and evaluate its accuracy against manual segmentation. This segmentation model was designed with two distinct phases: preparation and evolution. During the model preparation phase, after manual image cropping and then image intensity standardization, the myocardium is separated from the background based on the difference in their intensity distributions, and the endo- and epi-cardial contours are initialized automatically as zeros of an underlying level set function. During the model evolution phase, the model deformation is driven by the minimization of an energy function consisting of five terms: model intensity, edge attraction, shape prior, contours interaction, and contour smoothness. The energy function is minimized iteratively by adaptively weighting the five terms in the energy function using an annealing algorithm. The validation experiments were performed on a pool of cine data sets of five volunteers. The difference between the semiautomated segmentation and manual segmentation was sufficiently small as to be considered clinically irrelevant. This relatively accurate semiautomated segmentation method can be used to significantly increase the throughput of strain analysis of cine displacement-encoded MR images for clinical applications.","Myocardium,
Capacitive sensors,
Magnetic resonance imaging,
Image segmentation,
Deformable models,
Standardization,
Level set,
Shape,
Annealing,
Iterative algorithms"
Learning the viewpoint manifold for action recognition,"Researchers are increasingly interested in providing video-based, view-invariant action recognition for human motion. Addressing this problem will lead to more accurate modeling and analysis of the type of unconstrained video commonly collected in the areas of athletics and medicine. Previous viewpoint-invariant methods use multiple cameras in both the training and testing phases of action recognition or require storing many examples of a single action from multiple viewpoints. In this paper, we present a framework for learning a compact representation of primitive actions (e.g., walk, punch, kick, sit) that can be used for video obtained from a single camera for simultaneous action recognition and viewpoint estimation. Using our method, which models the low-dimensional structure of these actions relative to viewpoint, we show recognition rates on a publicly available data set previously only acheieved using multiple simultaneous views.","Cameras,
Humans,
Testing,
Motion analysis,
Robustness,
Computer science,
Biomedical imaging,
Image motion analysis,
Computer vision,
Application software"
A Study of Uncertainty in Software Cost and Its Impact on Optimal Software Release Time,"For a software development project, management often faces the dilemma of when to stop testing the software and release it for operation, which requires careful decision making as it has great impact on both software reliability and project cost. In most existing research on the optimal software release problem, the cost considered was the Expected Cost (EC) of the project. However, what concerns management is the Actual Cost (AC) of the project rather than the EC. Treatment (such as minimization) of the EC may not ensure the desired low level of the AC due to the uncertainty (variability) involved in the AC. In this paper, we study the uncertainty in software cost and its impact on optimal software release time in detail. The uncertainty is quantified by the variance of the AC and several risk functions. A risk-control approach to the optimal software release problem is proposed. New formulations of the problem which are extensions of current formulations are developed and solution procedures are established. Several examples are presented. Results reveal that it seems crucial to take into account the uncertainty in software cost in the optimal software release problem; otherwise, unsafe decisions may be reached which could be a false dawn to management.","Uncertainty,
Cost function,
Software testing,
Programming,
Project management,
Decision making,
Software reliability,
Software safety,
Software development management,
Software quality"
Multipurpose Watermarking Based on Multiscale Curvelet Transform,"Multipurpose watermarking for content authentication and copyright verification are accomplished by using the multiscale curvelet transform. A curvelet transform gains better and sparser representation than most traditional multiscale transforms. In this paper, an image is decomposed into multiscale coefficients with a dyadic number of wedges constructed from a variety of neighboring scales. Image hash is designed to extract image features from an approximate scale. The image features represented in the form of bit sequences are then embedded onto the wedges by a quantization based on human visual system behavior. The implementation strategy achieves content authentications for fatigue watermarking and copyright verifications for robust watermarking. The experiments demonstrate good results to support the feasibility of using this method in multipurpose applications.","Watermarking,
Authentication,
Robustness,
Casting,
Application software,
Computer networks,
Copyright protection,
Information science,
Feature extraction,
Quantization"
Improved novel view synthesis from depth image with large baseline,"In this paper, a new algorithm is developed for recovering the large disocclusion regions in depth image based rendering (DIBR) systems on 3DTV. For the DIBR systems, undesirable artifacts occur in the disocclusion regions by using the conventional view synthesis techniques especially with large baseline. Three techniques are proposed to improve the view synthesis results. The first is the preprocessing of the depth image by using the bilateral filter, which helps to sharpen the discontinuous depth changes as well as to smooth the neighboring depth of similar color, thus restraining noises from appearing on the warped images. Secondly, on the warped image of a new viewpoint, we fill the disocclusion regions on the depth image with the background depth levels to preserve the depth structure. For the color image, we propose the depth-guided exemplar-based image inpainting that combines the structural strengths of the color gradient to preserve the image structure in the restored regions. Finally, a trilateral filter, which simultaneous combines the spatial location, the color intensity, and the depth information to determine the weighting, is applied to enhance the image synthesis results. Experimental results are shown to demonstrate the superior performance of the proposed novel view synthesis algorithm compared to the traditional methods.","Filters,
Streaming media,
Filtering,
Colored noise,
Image restoration,
Displays,
Filling,
Computer science,
Computer industry,
Rendering (computer graphics)"
Courteous Cars,"In this article, we have demonstrated, through the parking and leaving example, how high-level specifications containing multiple temporally dependent goals can be given to a team of realistic robots, which in turn automatically satisfy them. By switching between low-level feedback control policies and moving in a well-behaved environment, the correctness of each robot's behavior is guaranteed by the automaton. The system satisfies the high-level specification without needing to plan the low-level motions in configuration space. Sensor inputs play a crucial role in this framework. A hazard input becoming true at the wrong time may lead to deadlock. Deciding when and how long to stop is a hard problem even for humans, as sometimes demonstrated at four-way stops, let alone robots.","Robotics and automation,
Robot kinematics,
Robot sensing systems,
Feedback control,
Automata,
Orbital robotics,
Logic,
Automatic control,
Computer science,
Shape"
Sampling bounds for sparse support recovery in the presence of noise,"It is well known that the support of a sparse signal can be recovered from a small number of random projections. However, in the presence of noise all known sufficient conditions require that the per-sample signal-to-noise ratio (SNR) grows without bound with the dimension of the signal. If the noise is due to quantization of the samples, this means that an unbounded rate per sample is needed. In this paper, it is shown that an unbounded SNR is also a necessary condition for perfect recovery, but any fraction (less than one) of the support can be recovered with bounded SNP. This means that a finite rate per sample is sufficient for partial support recovery. Necessary and sufficient conditions are given for both stochastic and non-stochastic signal models. This problem arises in settings such as compressive sensing, model selection, and signal denoising.","Signal to noise ratio,
Stochastic processes,
Sufficient conditions,
Noise,
Noise measurement,
Distortion measurement,
Sensors"
A new priority based congestion control protocol for Wireless Multimedia Sensor Networks,"New applications made possible by the rapid improvements and miniaturization in hardware has motivated recent developments in Wireless Multimedia Sensor Networks (WMSNs). As multimedia applications produce high volumes of data which require high transmission rates, multimedia traffic is usually high speed. This may cause congestion in the sensor nodes, leading to impairments in the quality of service (QoS) of multimedia applications. Thus, to meet the QoS requirements of multimedia applications, a reliable and fair transport protocol is mandatory. An important function of the transport layer in WMSNs is congestion control. In this paper, we present a new Queue based Congestion Control Protocol with Priority Support (QCCP-PS), using the queue length as an indication of congestion degree. The rate assignment to each traffic source is based on its priority index as well as its current congestion degree. Simulation results show that the proposed QCCP-PS protocol can detect congestion better than previous mechanisms. Furthermore it has a good achieved priority close to the ideal and near-zero packet loss probability, which make it an efficient congestion control protocol for multimedia traffic in WMSNs. As congestion wastes the scarce energy due to a large number of retransmissions and packet drops, the proposed QCCP-PS protocol can save energy at each node, given the reduced number of retransmissions and packet losses.","Protocols,
Pediatrics,
Multimedia communication,
Indexes,
Wireless sensor networks,
Streaming media,
Quality of service"
A Network Connection Proxy to Enable Hosts to Sleep and Save Energy,"Billions of dollars of electricity are being used to keep idle or unused network hosts fully powered-on only to maintain their network presence. We investigate how a Network Connectivity Proxy (NCP) could enable significant energy savings by allowing idle hosts to enter a low-power sleep state and still maintain full network presence. An NCP must handle ARP, ICMP, DHCP, and other low-level network presence tasks for a network host. An NCP must also be able to maintain TCP connections and UDP data flows and to respond to application messages. The focus of this paper is on how TCP connections can be kept alive during periods of host sleep by using a SOCKS-based approach called green SOCKS (gSOCKS) as part of an NCP. The gSOCKS includes awareness of the power state of a host. A prototype implementation of gSOCKS in a Linksys router shows that TCP connections can be preserved.","Sleep,
Energy consumption,
Personal communication networks,
Prototypes,
Power engineering and energy,
Computer science,
Maintenance engineering,
Internet,
Energy management,
Protocols"
Enhancing the Data Collection Rate of Tree-Based Aggregation in Wireless Sensor Networks,"What is the fastest rate at which we can collect a stream of aggregated data from a set of wireless sensors organized as a tree? We explore a hierarchy of techniques using realistic simulation models to address this question. We begin by considering TDMA scheduling on a single channel, reducing the original problem to minimizing the number of time slots needed to schedule each link of the aggregation tree. The second technique is to combine the scheduling with transmission power control to reduce the effects of interference. To better cope with interference, we then study the impact of utilizing multiple frequency channels by introducing a simple receiver-based frequency and time scheduling approach. We find that for networks of about a hundred nodes, the use of multi-frequency scheduling can suffice to eliminate most of the interference. The data collection rate then becomes limited not by interference, but by the maximum degree of the routing tree. Therefore we consider finally how the data collection rate can be further enhanced by the use of degree-constrained routing trees. Considering deployments at different densities, we show that these enhancements can improve the streaming aggregated data collection by as much as 10 times compared to the baseline of single-channel data collection over non-degree constrained routing trees. Addition to our primary conclusion, in the frequency scheduling domain we evaluate the impact of different interference models on the scheduling performance and give topology-specific bounds on time slot and frequency channel requirements.","Wireless sensor networks,
Interference,
Scheduling,
Power control,
Time division multiple access,
Frequency,
Routing,
Computer science,
Sensor systems,
Power system modeling"
Gaussian belief propagation solver for systems of linear equations,"The canonical problem of solving a system of linear equations arises in numerous contexts in information theory, communication theory, and related fields. In this contribution, we develop a solution based upon Gaussian belief propagation (GaBP) that does not involve direct matrix inversion. The iterative nature of our approach allows for a distributed message-passing implementation of the solution algorithm. We also address some properties of the GaBP solver, including convergence, exactness, its max-product version and relation to classical solution methods. The application example of decorrelation in CDMA is used to demonstrate the faster convergence rate of the proposed solver in comparison to conventional linear-algebraic iterative solution methods.","Symmetric matrices,
Mathematical model,
Vectors,
Equations,
Iterative methods,
Linear systems,
Probabilistic logic"
Avoiding communication in sparse matrix computations,"The performance of sparse iterative solvers is typically limited by sparse matrix-vector multiplication, which is itself limited by memory system and network performance. As the gap between computation and communication speed continues to widen, these traditional sparse methods will suffer. In this paper we focus on an alternative building block for sparse iterative solvers, the “matrix powers kernel” [x, Ax, A2x, …, Akx], and show that by organizing computations around this kernel, we can achieve near-minimal communication costs. We consider communication very broadly as both network communication in parallel code and memory hierarchy access in sequential code. In particular, we introduce a parallel algorithm for which the number of messages (total latency cost) is independent of the power k, and a sequential algorithm, that reduces both the number and volume of accesses, so that it is independent of k in both latency and bandwidth costs. This is part of a larger project to develop “communication-avoiding Krylov subspace methods,” which also addresses the numerical issues associated with these methods. Our algorithms work for general sparse matrices that “partition well”. We introduce parallel performance models of matrices arising from 2D and 3D problems and show predicted speedups over a conventional algorithm of up to 7x on a Petaflop-scale machine and up to 22x on computation across the Grid. Analogous sequential performance models of the same problems predict speedups over a conventional algorithm of up to 10x on an out-of-core implementation, and up to 2.5x when we use our ideas to reduce off-chip latency and bandwidth to DRAM. Finally, we validate the model on an out-of-core sequential implementation and measured a speedup of over 3x, which is close to the predicted speedup.","Sparse matrices,
Costs,
Delay,
Predictive models,
Kernel,
Bandwidth,
Partitioning algorithms,
Organizing,
Parallel algorithms,
Concurrent computing"
Practical near-optimal sparse recovery in the L1 norm,"We consider the approximate sparse recovery problem, where the goal is to (approximately) recover a high-dimensional vector x ∈ Rn from its lower-dimensional sketch Ax ∈ Rm. Specifically, we focus on the sparse recovery problem in the l1 norm: for a parameter k, given the sketch Ax, compute an approximation x̂ of x such that the l1 approximation error ‖x − x̂‖1 is close to minx′ ‖x − x′‖1, where x′ ranges over all vectors with at most k terms. The sparse recovery problem has been subject to extensive research over the last few years. Many solutions to this problem have been discovered, achieving different trade-offs between various attributes, such as the sketch length, encoding and recovery times.","Encoding,
Sparse matrices,
Approximation error,
Matching pursuit algorithms,
Vectors,
Computer science,
Signal processing algorithms,
Computer errors,
EMP radiation effects,
Pursuit algorithms"
Sufficient mutation operators for measuring test effectiveness,"Mutants are automatically-generated, possibly faulty variants of programs. The mutation adequacy ratio of a test suite is the ratio of non-equivalent mutants it is able to identify to the total number of non-equivalent mutants. This ratio can be used as a measure of test effectiveness. However, it can be expensive to calculate, due to the large number of different mutation operators that have been proposed for generating the mutants. In this paper, we address the problem of finding a small set of mutation operators which is still sufficient for measuring test effectiveness. We do this by defining a statistical analysis procedure that allows us to identify such a set, together with an associated linear model that predicts mutation adequacy with high accuracy. We confirm the validity of our procedure through cross-validation and the application of other, alternative statistical analyses.","Genetic mutations,
Software testing,
Statistical analysis,
Costs,
Computer science,
Predictive models,
Software measurement,
Time measurement,
Automatic testing,
Application software"
Scaling parallel I/O performance through I/O delegate and caching system,"Increasingly complex scientific applications require massive parallelism to achieve the goals of fidelity and high computational performance. Such applications periodically offload checkpointing data to file system for post-processing and program resumption. As a side effect of high degree of parallelism, I/O contention at servers doesn't allow overall performance to scale with increasing number of processors. To bridge the gap between parallel computational and I/O performance, we propose a portable MPI-IO layer where certain tasks, such as file caching, consistency control, and collective I/O optimization are delegated to a small set of compute nodes, collectively termed as I/O Delegate nodes. A collective cache design is incorporated to resolve cache coherence and hence alleviates the lock contention at I/O servers. By using popular parallel I/O benchmark and application I/O kernels, our experimental evaluation indicates considerable performance improvement with a small percentage of compute resources reserved for I/O.","Concurrent computing,
High performance computing,
File systems,
Application software,
Parallel processing,
Checkpointing,
Portable computers,
Computer applications,
Computer architecture,
Bandwidth"
DeLorean: Recording and Deterministically Replaying Shared-Memory Multiprocessor Execution Ef?ciently,"Support for deterministic replay of multithreaded execution can greatly help in finding concurrency bugs. For highest effectiveness, replay schemes should (i) record at production-run speed, (ii) keep their logging requirements minute, and (iii) replay at a speed similar to that of the initial execution. In this paper, we propose a new substrate for deterministic replay that provides substantial advances along these axes. In our proposal, processors execute blocks of instructions atomically, as in transactional memory or speculative multithreading, and the system only needs to record the commit order of these blocks. We call our scheme DeLorean. Our results show that DeLorean records execution at a speed similar to that of Release Consistency (RC) execution and replays at about 82% of its speed. In contrast, most current schemes only record at the speed of Sequential Consistency (SC) execution. Moreover, DeLorean only needs 7.5% of the log size needed by a state-of-the-art scheme. Finally, DeLorean can be configured to need only 0.6% of the log size of the state-of-the-art scheme at the cost of recording at 86% of RC’s execution speed — still faster than SC. In this configuration, the log of an 8-processor 5-GHz machine is estimated to be only about 20GB per day.","Computer bugs,
Debugging,
Timing,
Computer science,
Concurrent computing,
Hardware,
Computer architecture,
Proposals,
Multithreading,
Costs"
NCTUns 5.0: A Network Simulator for IEEE 802.11(p) and 1609 Wireless Vehicular Network Researches,"NCTUns is a novel network simulator and emulator that has many unique features over traditional network simulators and emulators. It is an open-source software running on Linux and is being used by many researchers in the world. According to the NCTUns official Web site (http://NSL.csie.nctu.edu.tw/nctuns.html), as of June 1, 2008, more than 11,546 people from 124 countries have registered and downloaded this software and these numbers are still fast increasing. In its 5.0 release, NCTUns provides a complete implementation of the IEEE 802.11(p)/1609 standards defined for wireless vehicular networks. In this paper, we present the capabilities of NCTUns 5.0 focusing on its uses for IEEE 802.11(p)/1609 wireless vehicular network researches.","Computational modeling,
Linux,
Protocols,
Wireless communication,
Standards,
Roads,
IP networks"
A Localized Self-Healing Algorithm for Networks of Moveable Sensor Nodes,"The effectiveness of wireless sensor networks (WSNs) deployed in search and rescue, battlefield reconnaissance, surveillance, and other applications depends on inter-node interaction and maintaining network connectivity. While connectivity can be provisioned at startup, then sustained through careful coordination when nodes move, the network can be partitioned if a node suddenly fails. This paper presents recovery through inward motion (RIM), a distributed algorithm to efficiently restore network connectivity after a node failure. Instead of performing a network-wide analysis to assess the impact of the node failure and set a course of action, RIM triggers a local recovery process by relocating the neighbors of the lost node. RIM minimizes messaging overhead and reduces the distance that individual nodes travel during the recovery. Simulations validate RIM's performance.",
DEC ECC design to improve memory reliability in Sub-100nm technologies,"Exacerbated SRAM reliability issues, due to soft errors and increased process variations in sub-100nm technologies, limit the efficacy of conventionally used error correcting codes (ECC). The double error correcting (DEC) BCH codes have not found favorable application in SRAMs due to non-alignment of their block sizes to typical memory word widths and particularly due to the large multi-cycle latency of traditional iterative decoding algorithms. This work presents DEC code design that is aligned to typical memory word widths and a parallel decoding implementation approach that operates on complete memory words in a single cycle. The practicality of this approach is demonstrated through ASIC implementations, in which it incurs only 1.4ns and 2.2ns decoding latencies for 16-and 64-bit words, respectively, using 90nm ASIC technology. A comparative analysis between conventionally used ECC and DEC ECC for reliability gains and costs incurred has also been performed.",
AC vs. DC distribution: Maximum transfer capability,"Many studies comparing AC and DC systems have focused on efficiency, stability, and controllability, but have not compared the maximum transfer capability. In this paper, the maximum transfer capability of an AC system and two DC systems, one with two lines and another with three, is determined through the continuation power flow method and compared. The results reveal that significant gains can be achieved by moving to a DC system with three lines.","Load flow,
Mathematical model,
Equations,
Jacobian matrices,
Power systems,
Converters,
Power system stability"
Cooperative Routing in Multi-Source Multi-Destination Multi-Hop Wireless Networks,"In a network supporting cooperative communication, the sender of a transmission is no longer a single node, which causes the concept of a traditional link to be reinvestigated. Thus, the routing scheme basing on the link concept should also be reconsidered to "";truly""; exploit the potential performance gain introduced by cooperative communication. In this paper, we investigate the joint problem of routing selection in network layer and contention avoidance among multiple links in MAC layer for multi-hop wireless networks in a cooperative communication aware network. To the best of our knowledge, it is the first work to investigate the problem of cooperative communication aware routing in multi-source multi-destination multi-hop wireless networks. Several important concepts, including virtual node, virtual link and virtual link based contention graph are introduced. Basing on those concepts, an optimal cooperative routing is achieved and a distributed routing scheme is proposed after some practical approximations. The simulation results show that our scheme reduces the total transmission power comparing with non-cooperative routing and greatly increases the network throughput comparing with single flow cooperative routings.","Routing,
Spread spectrum communication,
Wireless networks,
Physical layer,
Broadcasting,
Peer to peer computing,
Performance gain,
Throughput,
Communications Society,
Computer science"
High performance control for graceful motion of an intelligent wheelchair,"To be acceptable to human drivers, the motion of an intelligent robotic wheelchair must be more than just collision-free: it must be graceful. We define graceful motion as being safe, comfortable, fast and intuitive.","Motion control,
Wheelchairs,
Intelligent robots,
Humans,
Mobile robots,
Spline,
Velocity control,
Vehicle driving,
Acceleration,
Motion measurement"
Handover Latency Analysis of a Network-Based Localized Mobility Management Protocol,"Recently, the IETF NETLMM working group is standardizing a network-based localized mobility management (NETLMM) protocol called proxy mobile IPv6 (PMIPv6), yet the research on NETLMM is still in its early stage while it has attracted a fair amount of critical attention both in the telecommunication and the Internet communities. Unlike previous host-based mobility management protocols such as mobile IPv6 (MIPv6), hierarchical mobile IPv6 (HMIPv6), and fast handover for mobile IPv6 (FMIPv6), PMIPv6 has salient features and is expected to expedite the real deployment of IP mobility support protocol by using only collaborative operations between the network entities without mobile node (MN) being involved. In this paper, we analyze and compare the handover latency of PMIPv6 with those of the various existing host-based IP mobility management protocols. In addition, we show good efficacy of the desirable features and key strengths of PMIPv6. Numerical results demonstrate that (1) the handover latency of PMIPv6 is much lower than those of MlPv6 and HMIPv6, and (2) the handover latency of PMIPv6 becomes lower than that of FMIPv6 in case the wireless link delay is greater than the delay between mobile access gateway (MAG) and local mobility anchor (LMA).",
All-pairs: An abstraction for data-intensive cloud computing,"Although modern parallel and distributed computing systems provide easy access to large amounts of computing power, it is not always easy for non-expert users to harness these large systems effectively. A large workload composed in what seems to be the obvious way by a naive user may accidentally abuse shared resources and achieve very poor performance. To address this problem, we propose that production systems should provide end users with high-level abstractions that allow for the easy expression and efficient execution of data intensive workloads. We present one example of an abstraction - All-Pairs - that fits the needs of several data-intensive scientific applications. We demonstrate that an optimized All-Pairs abstraction is both easier to use than the underlying system, and achieves performance orders of magnitude better than the obvious but naive approach, and twice as fast as a hand-optimized conventional approach.","Cloud computing,
Distributed computing,
Biometrics,
Data mining,
Face recognition,
Data engineering,
Clustering algorithms,
Computer science,
Power engineering and energy,
Concurrent computing"
Identifying Botnets Using Anomaly Detection Techniques Applied to DNS Traffic,"Bots are compromised computers that communicate with a botnet command and control (C& C) server. Bots typically employ dynamic DNS (DDNS) to locate the respective C&C server. By injecting commands into such servers, botmasters can reuse bots for a variety of attacks. We evaluate two approaches for identifying botnet C&C servers based on anomalous DDNS traffic. The first approach consists in looking for domain names whose query rates are abnormally high or temporally concentrated. High DDNS query rates may be expected because botmasters frequently move C&C servers, and botnets with as many as 1.5 million bots have been discovered. The second approach consists in looking for abnormally recurring DDNS replies indicating that the query is for an inexistent name (NXDOMAIN). Such queries may correspond to bots trying to locate C&C servers that have been taken down. In our experiments, the second approach automatically identified several domain names that were independently reported by others as being suspicious, while the first approach was not as effective.","Network servers,
Chebyshev approximation,
Communication system traffic control,
Object detection,
Computer science,
Command and control systems,
Automatic control,
Aggregates,
Superluminescent diodes,
Proposals"
An Integrated UAV Navigation System Based on Aerial Image Matching,"The aim of this paper is to explore the possibility of using geo-referenced satellite or aerial images to augment an Unmanned Aerial Vehicle (UAV) navigation system in case of GPS failure. A vision based navigation system which combines inertial sensors, visual odometer and registration of a UAV on-board video to a given geo-referenced aerial image has been developed and tested on real flight-test data. The experimental results show that it is possible to extract useful position information from aerial imagery even when the UAV is flying at low altitude. It is shown that such information can be used in an automated way to compensate the drift of the UAV state estimation which occurs when only inertial sensors and visual odometer are used.",
ALEACH: Advanced LEACH routing protocol for wireless microsensor networks,"A wireless network consisting of hundreds or thousands of cheap microsensor nodes allow users accurately monitor the characteristics of the remote environment or detect an event. As the sensor nodes have limited energy resources, so the routing protocol designed for the wireless sensor networks should be energy efficient and provide low latency. For this reason, we propose advanced low-energy adaptive clustering hierarchy (ALEACH), a clustering-based protocol architecture where nodes make autonomous decision without any central intervention. ALEACH proposes a new cluster head selection algorithms that enables selecting best suited node for cluster head, algorithms for adaptive clusters and rotating cluster head positions to evenly distribute the energy load among all the nodes. Simulation results show that ALEACH can improve system life time and energy efficiency in terms of different simulation performance metrics.","Routing protocols,
Microsensors,
Wireless sensor networks,
Energy efficiency,
Clustering algorithms,
Remote monitoring,
Event detection,
Sensor phenomena and characterization,
Energy resources,
Delay"
Re-thinking non-rigid structure from motion,"We present a novel approach to non-rigid structure from motion (NRSFM) from an orthographic video sequence, based on a new interpretation of the problem. Existing approaches assume the object shape space is well-modeled by a linear subspace. Our approach only assumes that small neighborhoods of shapes are well-modeled with a linear subspace. This constrains the shapes to belong to a manifold of dimensionality equal to the number of degrees of freedom of the object. After showing that the problem is still overconstrained, we present a solution composed of a novel initialization algorithm, followed by a robust extension of the Locally Smooth Manifold Learning algorithm tailored to the NRSFM problem. We finally present some test cases where the linear basis method fails (and is actually not meant to work) while the proposed approach is successful.","Shape,
Cameras,
Video sequences,
Computer science,
Robustness,
Testing,
Springs,
Deformable models,
Constraint optimization,
Government"
Beyond the Gamma Band: The Role of High-Frequency Features in Movement Classification,"Electrocorticographic spectral changes during movement show a behavioral inflection in the classic gamma band (30-70 Hz). We quantify this inflection and demonstrate that it limits classification accuracy. We call for the designation of a functionally defined band above it, which we denote the chi-band.","Frequency,
Electroencephalography,
Physics,
Tongue,
Computer science,
Electrodes"
Video Annotation Based on Kernel Linear Neighborhood Propagation,"The insufficiency of labeled training data for representing the distribution of the entire dataset is a major obstacle in automatic semantic annotation of large-scale video database. Semi-supervised learning algorithms, which attempt to learn from both labeled and unlabeled data, are promising to solve this problem. In this paper, a novel graph-based semi-supervised learning method named kernel linear neighborhood propagation (KLNP) is proposed and applied to video annotation. This approach combines the consistency assumption, which is the basic assumption in semi-supervised learning, and the local linear embedding (LLE) method in a nonlinear kernel-mapped space. KLNP improves a recently proposed method linear neighborhood propagation (LNP) by tackling the limitation of its local linear assumption on the distribution of semantics. Experiments conducted on the TRECVID data set demonstrate that this approach outperforms other popular graph-based semi-supervised learning methods for video semantic annotation.","Kernel,
Semisupervised learning,
Training data,
Large-scale systems,
Databases,
Humans,
Information science,
Asia,
Automation,
Machine learning"
A Topic Modeling Approach and Its Integration into the Random Walk Framework for Academic Search,"In this paper, we propose a unified topic modeling approach and its integration into the random walk framework for academic search. Specifically, we present a topic model for simultaneously modeling papers, authors, and publication venues. We combine the proposed topic model into the random walk framework. Experimental results show that our proposed approach for academic search significantly outperforms the baseline methods of using BM25 and language model, and those of using the existing topic models (including pLSI, LDA, and the AT model).","Neodymium,
Data mining,
Computer science,
Linear discriminant analysis,
Frequency,
Search engines,
Information retrieval,
Smoothing methods"
Physical Interference Driven Dynamic Spectrum Management,"Dynamic spectrum management can drastically improve the performance of wireless networks struggling under increasing user demands. However, performing efficient spectrum allocation is a complex and difficult process. Current proposals make the problem tractable by simplifying interference constraints as conflict graphs, but they face potential performance degradation from inaccurate interference estimation. In this paper, we show that conflict graphs, if optimized properly, can produce spectrum allocations that closely match those derived from the physical interference model. Thus we propose PLAN, a systematic framework to produce conflict graphs based on physical interference characteristics. PLAN first applies an analytical framework to derive the criterion for identifying conflicting neighbors, capturing the cumulative effect of interference. PLAN then applies a local conflict adjustment algorithm to address heterogeneous interference conditions and improve spectrum allocation efficiency. Through detailed analysis and experimental evaluations, we show that PLAN builds a conflict graph to effectively represent the complex interference conditions and allow the reuse of efficient graph-based spectrum allocation solutions. PLAN also significantly outperforms the conventional graph model based solutions.","Interference,
Resource management,
Distance measurement,
Radio spectrum management,
Throughput,
Signal to noise ratio,
Optimization"
Large Scale Global Optimization using Differential Evolution with self-adaptation and cooperative co-evolution,"In this paper, an optimization algorithm is formulated and its performance assessment for large scale global optimization is presented. The proposed algorithm is named DEwSAcc and is based on Differential Evolution (DE) algorithm, which is a floating-point encoding evolutionary algorithm for global optimization over continuous spaces. The original DE is extended by log-normal self-adaptation of its control parameters and combined with cooperative co-evolution as a dimension decomposition mechanism. Experimental results are given for seven high-dimensional test functions proposed for the Special Session on Large Scale Global Optimization at 2008 IEEE World Congress on Computational Intelligence.","Optimization,
Evolution (biology),
Chromium,
Algorithm design and analysis,
Encoding,
Process control,
Evolutionary computation"
QoS Performance Analysis of Cognitive Radio-Based Virtual Wireless Networks,"Cognitive radio presents a new approach to wireless spectrum utilization and management. In this work, the potential performance improvement gained by applying cognitive radio to multiple-provider wireless systems is investigated. It is shown that virtual wireless networks can be created, utilizing only the residual wasted bandwidth of the primary service providers. These virtual networks are able to support large volumes of users, while still ensuring that QoS reliability requirements, such as blocking and dropping guarantees, are achieved. A Markov chain-based analysis of classic and cognitive systems is complemented by simulations in order to present a quantified perspective of the potential benefits of cognitive radio techniques.","Performance analysis,
Wireless networks,
Cognitive radio,
Chromium,
Radio spectrum management,
Bandwidth,
Analytical models,
Frequency,
Resource management,
Computer science"
Methods of Artificial Enlargement of the Training Set for Statistical Shape Models,"Due to the small size of training sets, statistical shape models often over-constrain the deformation in medical image segmentation. Hence, artificial enlargement of the training set has been proposed as a solution for the problem to increase the flexibility of the models. In this paper, different methods were evaluated to artificially enlarge a training set. Furthermore, the objectives were to study the effects of the size of the training set, to estimate the optimal number of deformation modes, to study the effects of different error sources, and to compare different deformation methods. The study was performed for a cardiac shape model consisting of ventricles, atria, and epicardium, and built from magnetic resonance (MR) volume images of 25 subjects. Both shape modeling and image segmentation accuracies were studied. The objectives were reached by utilizing different training sets and datasets, and two deformation methods. The evaluation proved that artificial enlargement of the training set improves both the modeling and segmentation accuracy. All but one enlargement techniques gave statistically significantly (p < 0.05) better segmentation results than the standard method without enlargement. The two best enlargement techniques were the nonrigid movement technique and the technique that combines principal component analysis (PCA) and finite element model (FEM). The optimal number of deformation modes was found to be near 100 modes in our application. The active shape model segmentation gave better segmentation accuracy than the one based on the simulated annealing optimization of the model weights.","Image segmentation,
Biomedical imaging,
Principal component analysis,
Deformable models,
Active shape model,
Magnetic resonance imaging,
Magnetic resonance,
Finite element methods,
Medical simulation,
Simulated annealing"
On the Maintainability of Aspect-Oriented Software: A Concern-Oriented Measurement Framework,"Aspect-oriented design needs to be systematically assessed with respect to modularity flaws caused by the realization of driving system concerns, such as tangling, scattering, and excessive concern dependencies. As a result, innovative concern metrics have been defined to support quantitative analyses of concern's properties. However, the vast majority of these measures have not yet being theoretically validated and managed to get accepted in the academic or industrial settings. The core reason for this problem is the fact that they have not been built by using a clearly-defined terminology and criteria. This paper defines a concern-oriented framework that supports the instantiation and comparison of concern measures. The framework subsumes the definition of a core terminology and criteria in order to lay down a rigorous process to foster the definition of meaningful and well-founded concern measures. In order to evaluate the framework generality, we demonstrate the framework instantiation and extension to a number of concern measures suites previously used in empirical studies of aspect-oriented software maintenance.","Software maintenance,
Software measurement,
Terminology,
Programming,
Scattering,
Particle measurements,
Area measurement,
Computer science,
Software design"
Apprenticeship learning for motion planning with application to parking lot navigation,"Motion and path-planning algorithms often use complex cost functions for both global navigation and local smoothing of trajectories. Obtaining good results typically requires carefully hand-engineering the trade-offs between different terms in the cost function. In practice, it is often much easier to demonstrate a few good trajectories. In this paper, we describe an efficient algorithm which—when given access to a few trajectory demonstrations—can automatically infer good trade-offs between the different costs. In our experiments, we apply our algorithm to the problem of navigating a robotic car in a parking lot.","Trajectory,
Navigation,
Distance measurement,
Optimization,
Driver circuits,
Robots,
Algorithm design and analysis"
Identifying Tampered Regions Using Singular Value Decomposition in Digital Image Forensics,"Detecting tampered regions and proving the authenticity and integrity of a digital image becomes increasingly important in digital forensics and multimedia security. In this paper we propose a novel framework for identifying the location of copy-move image tampering by applying the singular value decomposition(SVD). In the proposed passive techniques, SVD served to produce algebraic and geometric invariant and feature vectors. Experimental results demonstrate the validity of the proposed approach to tampered images undergone some attacks like Gaussian blur filtering, Gaussian white noise contamination, lossy JPEG compression, etc.","Singular value decomposition,
Digital images,
Forensics,
Transform coding,
Image coding,
Watermarking,
Digital filters,
Contamination,
Military computing,
Colored noise"
"Profiling, Prediction, and Capping of Power Consumption in Consolidated Environments","Consolidation of workloads has emerged as a key mechanism to dampen the rapidly growing energy expenditure within enterprise-scale data centers. To gainfully utilize consolidation-based techniques, we must be able to characterize the power consumption of groups of co-located applications. Such characterization is crucial for effective prediction and enforcement of appropriate limits on power consumption-power budgets-within the data center. We identify two kinds of power budgets (i) an average budget to capture an upper bound on long-term energy consumption within that level and (ii) a sustained budget to capture any restrictions on sustained draw of current above a certain threshold. Using a simple measurement infrastructure, we derive power profiles-statistical descriptions of the power consumption of applications. Based on insights gained from detailed profiling of several applications both individual and consolidated-we develop models for predicting average and sustained power consumption of consolidated applications. We conduct an experimental evaluation of our techniques on a Xen-based server that consolidates applications drawn from a diverse pool. For a variety of consolidation scenarios, We are able to predict average power consumption within 5% error margin and sustained power within 10% error margin. Our sustained power prediction techniques allow us to predict close yet safe upper bounds on the sustained power consumption of consolidated applications.","Energy consumption,
Energy capture,
Upper bound,
Predictive models,
Power engineering and energy,
Cooling,
Power measurement,
Large-scale systems,
Costs,
Power system modeling"
Hyperproperties,"Properties, which have long been used for reasoning about systems, are sets of traces. Hyperproperties, introduced here, are sets of properties. Hyperproperties can express security policies, such as secure information flow, that properties cannot. Safety and liveness are generalized to hyperproperties, and every hyperproperty is shown to be the intersection of a safety hyperproperty and a liveness hyperproperty. A verification technique for safety hyperproperties is given and is shown to generalize prior techniques for verifying secure information flow. Refinement is shown to be valid for safety hyperproperties. A topological characterization of hyperproperties is given.","Safety,
Information security,
Topology,
Delay effects,
Computer security,
Computer science,
Writing"
Training of artificial neural networks using differential evolution algorithm,"In the paper an application of differential evolution algorithm to training of artificial neural networks is presented. The adaptive selection of control parameters has been introduced in the algorithm; due to this property only one parameter is set at the start of proposed algorithm. The artificial neural networks to classification of parity-p problem have been trained using proposed algorithm. Results obtained using proposed algorithm have been compared to the results obtained using other evolutionary method, and gradient training methods such as: error back-propagation, and Levenberg-Marquardt method. It has been shown in this paper that application of differential evolution algorithm to artificial neural networks training can be an alternative to other training methods.","Training,
Artificial neural networks,
Classification algorithms,
Evolution (biology),
Nickel,
Neurons,
Chromium"
Direct computation of sound and microphone locations from time-difference-of-arrival data,In this paper we present a novel approach to directly recover the location of both microphones and sound sources from time-difference-of-arrival measurements only. No approximation solution is required for initialization and in the absence of noise our approach is guaranteed to always recover the exact solution. Our approach only requires solving linear equations and matrix factorization. We demonstrate the feasibility of our approach with synthetic data.,"Time difference of arrival,
Microphone arrays,
Sensor arrays,
Iterative algorithms,
Acoustic noise,
Adaptive arrays,
Phased arrays,
Extraterrestrial measurements,
Intelligent sensors,
Acoustic propagation"
Using sink mobility to increase wireless sensor networks lifetime,"A critical issue for data gathering in wireless sensor networks is the formation of energy holes near the sinks. Sensors near the sinks have to participate in relaying data on behalf of other sensors and thus will deplete their energy very quickly, resulting in network partitioning and limitation of the network lifetime. The solution that we propose in this paper is to use mobile sinks that change their location when the nearby sensors’ energy becomes low. In this way the sensors located near sinks change over time. In deciding a new location, a sink searches for zones with richer sensor energy. First, we study the improvement in network lifetime when sinks move on a predetermined path, along the perimeter of a hexagonal tiling. Two cases are considered for data gathering when sinks stop in the hexagon’s corners and when the sinks stop on multiple locations on the hexagon perimeter. This study shows an improvement of up to 4.86 times in network lifetime. Second, we design a distributed and localized algorithm used by the sinks to decide their next movement location such that the virtual backbone formed by the sinks remains interconnected at all times. Simulation results are presented to verify our approaches.","Corona,
Robot sensing systems,
Wireless sensor networks,
Energy consumption,
Mobile communication,
Trajectory,
Energy resources"
Modeling and Predicting the Helpfulness of Online Reviews,"Online reviews provide a valuable resource for potential customers to make purchase decisions. However, the sheer volume of available reviews as well as the large variations in the review quality present a big impediment to the effective use of the reviews, as the most helpful reviews may be buried in the large amount of low quality reviews. The goal of this paper is to develop models and algorithms for predicting the helpfulness of reviews, which provides the basis for discovering the most helpful reviews for given products. We first show that the helpfulness of a review depends on three important factors: the reviewer’s expertise, the writing style of the review, and the timeliness of the review. Based on the analysis of those factors, we present a nonlinear regression model for helpfulness prediction. Our empirical study on the IMDB movie reviews dataset demonstrates that the proposed approach is highly effective.","Predictive models,
Voting,
Writing,
Motion pictures,
Impedance,
Data mining,
Computer science,
Data engineering,
Information technology,
Prediction algorithms"
Distributed interference pricing with MISO channels,"We study a distributed algorithm for adapting transmit beamforming vectors in a multi-antenna peer-to-peer wireless network. The algorithm attempts to maximize a sum of per-user utility functions, where each user's utility is a function of his transmission rate, or equivalently the received signal-to-interference plus noise ratio (SINR). This is accomplished by exchanging interference prices, each of which represents the marginal cost of interference to a particular user. Given the interference prices, users update their beamforming vectors to maximize their utility minus the cost of interference. For a two-user system, we show that this algorithm converges for a suitable class of utility functions. Convergence of the algorithm with more than two users is illustrated numerically.","Pricing,
Array signal processing,
Signal to noise ratio,
Interference channels,
Iterative algorithms,
Costs,
Distributed algorithms,
Peer to peer computing,
Wireless networks,
Convergence"
Parallel Distributed Processing of Constrained Skyline Queries by Filtering,"Skyline queries are capable of retrieving interesting points from a large data set according to multiple criteria. Most work on skyline queries so far has assumed a centralized storage, whereas in practice relevant data are often distributed among geographically scattered sites. In this work, we tackle constrained skyline queries in large-scale distributed environments without the assumption of any overlay structures, and propose a novel algorithm named PaDSkyline (Parallel Distributed Skyline query processing). PaDSkyline significantly shortens the response time by performing parallel processing over site groups produced by a partition algorithm. Within each group, it locally optimizes the query processing over distributed sites. It also drastically enhances the network transmission efficiency by performing early reduction of skyline candidates with deliberately selected multiple filtering points. Results of extensive experiments demonstrate the efficiency and robustness of our proposals.","Distributed processing,
Filtering,
Partitioning algorithms,
Query processing,
Information retrieval,
Scattering,
Large-scale systems,
Delay,
Parallel processing,
Robustness"
Peacock Hashing: Deterministic and Updatable Hashing for High Performance Networking,"Hash tables are extensively used in networking to implement data-structures that associate a set of keys to a set of values, as they provide O(1), query, insert and delete operations. However, at moderate or high loads collisions are quite frequent which not only increases the access time, but also induces non- determinism in the performance. Due to this non-determinism, the performance of these hash tables degrades sharply in the multi-threaded network processor based environments, where a collection of threads perform the hashing operations in a loosely synchronized manner. In such systems, it is critical to keep the hash operations more deterministic. A recent series of papers have been proposed, which employs a compact on-chip memory to enable deterministic and fast hash queries. While effective, these schemes require substantial on- chip memory, roughly 10-bits for every entry in the hash table. This limits their general usability; specifically in the network processor context, where on-chip resources are scarce. In this paper, we propose a novel hash table construction called Peacock hash, which reduces the on-chip memory by more than 10-folds while keeping a high degree of determinism in performance. This significantly reduced on-chip memory not only makes Peacock hashing much more appealing for the general use but also makes it an attractive choice for the implementation of a hash hardware accelerator on a network processor.","Yarn,
Network-on-a-chip,
Filters,
Degradation,
Costs,
System performance,
Communications Society,
Computer science,
Data engineering,
Usability"
A solar energy harvesting circuit for low power applications,"In this paper we present a solar energy harvesting circuit for low-power applications describing circuit architecture and guidelines for an optimal design. We evaluate the performance of two implemented prototypes intended to power a wireless embedded system under different light intensities and different switching frequencies. Measurements show that higher switching frequencies allow reaching the maximum efficiency (∼90%) at higher light intensities, whereas lower operating frequencies perform better under lower irradiance. Experimental results show that circuit optimization depends on light conditions and the proposed solar energy harvester can autonomously supply the nodes of a wireless sensor network WSN.","Solar energy,
Circuits"
MILSA: A Mobility and Multihoming Supporting Identifier Locator Split Architecture for Naming in the Next Generation Internet,"Naming and addressing are important issues for next generation Internet (NGI). In this paper, we discuss a new mobility and multihoming supporting identifier locator split architecture (MILSA). There are three main contributions of our solution. First, we separate trust relationships (realms) from connectivity (zones). A hierarchical identifier system for the realms and a Realm Zone Bridging Server (RZBS) infrastructure that performs the bridging function is introduced. Second, we separate the signaling and data plane functions to improve the performance and support mobility. Third, to provide transparency to the upper layer applications, identifier locator split happens in network layer. A Hierarchical URI-like Identifier (HUI) is used by the upper layers and is mapped to a locators set by HUI Mapping Sublayer (HMS) through interaction with RZBS infrastructure. Further scenarios description and analysis show the benefits of this scheme for routing scalability, mobility and multihoming.",
"Scars, marks and tattoos (SMT): Soft biometric for suspect and victim identification","Scars, marks and tattoos (SMT) are being increasingly used for suspect and victim identification in forensics and law enforcement agencies. Tattoos, in particular, are getting serious attention because of their visual and demographic characteristics as well as their increasing prevalence. However, current tattoo matching procedure requires human-assigned class labels in the ANSI/NIST ITL 1–2000 standard which makes it time consuming and subjective with limited retrieval performance. Further, tattoo images are complex and often contain multiple objects with large intra-class variability, making it very difficult to assign a single category in the ANSI/NIST standard. We describe a content-based image retrieval (CBIR) system for matching and retrieving tattoo images. Based on Scale Invariant Feature Transform (SIFT) features extracted from tattoo images and optional accompanying demographical information, our system computes feature-based similarity between the query tattoo image and tattoos in the criminal database. Experimental results on two different tattoo databases show encouraging results.","Databases,
Image retrieval,
Feature extraction,
Fingerprint recognition,
NIST,
Law enforcement,
Image color analysis"
Mobile Device Profiling and Intrusion Detection Using Smart Batteries,"This paper introduces capabilities developed for a battery-sensing intrusion protection system (B-SIPS) for mobile computers, which alerts when abnormal current changes are detected. The intrusion detection system's (IDS's) IEEE 802.15.1 (Bluetooth) and 802.11 (Wi-Fi) capabilities are enhanced with iterative safe process checking, wireless connection determination, and an automated intrusion protection disconnect ability. The correlation intrusion detection engine (CIDE) provides power profiling for mobile devices and a correlated view of B-SIPS and snort alerts. An examination of smart battery drain times was conducted to ascertain the optimal transmission rate for the B-SIPS client. A 10 second reporting rate was used to assess 9 device types, which were then compared with their corresponding baseline battery lifetime. Lastly, an extensive usability study was conducted to improve the B-SIPS client and CIDE features. The 31 expert participants provided feedback and data useful for validating the system's viability as a complementary IDS for mobile devices.","Intrusion detection,
Batteries,
Power system management,
Energy management,
Usability,
Portable computers,
Sleep,
Bluetooth,
Engines,
Power system protection"
WBest: A bandwidth estimation tool for IEEE 802.11 wireless networks,"Bandwidth estimation techniques seek to provide an accurate estimation of available bandwidth such that network applications can adjust their behavior accordingly. However, most current techniques were designed for wired networks and produce relatively inaccurate results and long convergence times on wireless networks where capacity can vary dramatically. This paper presents a new Wireless Bandwidth estimation tool, WBest, designed for fast, non-intrusive, accurate estimation of available bandwidth in IEEE 802.11 networks. WBest is a two-stage algorithm: 1) a packet pair technique estimates the effective capacity over a flow path where the last hop is a wireless LAN (WLAN); and 2) a packet train technique estimates achievable throughput to infer the available bandwidth. WBest parameters are optimized given the tradeoffs of accuracy, intrusiveness and convergence time. The advantage of WBest stems from avoiding a search algorithm to detect the available bandwidth by statistically detecting the available fraction of the effective capacity to mitigate estimation delay and the impact of random wireless channel errors. WBest is implemented and evaluated on an 802.11 wireless testbed. Comparisons with other available bandwidth estimation tools shows WBest to have higher accuracy, lower intrusiveness and faster convergence times. Thus, WBest demonstrates the potential for improving the performance of applications that need bandwidth estimation, such as multimedia streaming, on wireless networks.","Bandwidth,
Wireless communication,
Estimation,
Dispersion,
Wireless networks,
Convergence,
Wireless LAN"
Can the subthreshold swing in a classical FET be lowered below 60 mV/decade?,"We show that by biasing a ferroelectric insulator in its negative capacitance region and using it on top of the gate of a classical FET, it should be possible to achieve a subthreshold swing significantly lower than 60 mV/decade at room temperature. We further show that this, in principle, does not change the transport properties of the FET so that the ON current of the FET remains unaffected. A simple analysis is used to describe the basic principle of device operation","FETs,
Capacitance,
Ferroelectric materials,
Voltage,
Insulation,
Temperature,
Circuits,
Capacitors,
Computer networks,
Tunneling"
A Laboratory Setup and Teaching Methodology for Wireless and Mobile Embedded Systems,"Increasingly, electrical and computer engineers are making their careers in designing wireless embedded systems. This paper presents a teaching methodology and the associated laboratory setup designed to meet the needs in teaching wireless embedded systems. The courses allow the students not only to apply their previous knowledge of digital system design, computer architecture, electronic circuits, wireless networking, and software engineering, but experience actual systems engineering by designing and implementing a large-scale team project within a semester. A flexible hardware platform was developed and was accompanied by teaching methodologies that allow quick completion of ambitious course projects in this area.","Education,
Wireless communication,
Laboratories,
Hardware,
Software,
Microprocessors,
Computers"
Image equalization based on singular value decomposition,"In this paper, a novel image equalization technique which is based on singular value decomposition (SVD) is proposed. The singular value matrix represents the intensity information of the given image and any change on the singular values change the intensity of the input image. The proposed technique converts the image into the SVD domain and after normalizing the singular value matrix it reconstructs the image in the spatial domain by using the updated singular value matrix. The technique is called the singular value equalization (SVE) and compared with the standard grayscale histogram equalization (GHE) method. The visual and quantitative results suggest that the proposed SVE method clearly outperforms the GHE method.","Singular value decomposition,
Histograms,
Matrix converters,
Matrix decomposition,
Gray-scale,
Shape,
Electronic mail,
Image converters,
Image reconstruction,
Visual perception"
On the Hardness of Being Truthful,"The central problem in computational mechanism design is the tension between incentive compatibility and computational efficiency. We establish the first significant approximability gap between algorithms that are both truthful and computationally-efficient, and algorithms that only achieve one of these two desiderata. This is shown in the context of a novel mechanism design problem which we call the combinatorial public project problem (cppp). cpppis an abstraction of many common mechanism design situations, ranging from elections of kibbutz committees to network design.Our result is actually made up of two complementary results -- one in the communication-complexity model and one in the computational-complexity model. Both these hardness results heavily rely on a combinatorial characterization of truthful algorithms for our problem. Our computational-complexity result is one of the first impossibility results connecting mechanism design to complexity theory; its novel proof technique involves an application of the Sauer-Shelah Lemma and may be of wider applicability, both within and without mechanism design.","Computer science,
Cost accounting,
Routing,
USA Councils,
Nominations and elections,
Design engineering,
Computational modeling,
Computer networks,
Joining processes,
Complexity theory"
Temperature aware task sequencing and voltage scaling,"On-chip power density and temperature are rising exponentially with decreasing feature sizes. This alarming trend calls for temperature management at every level of system design. In this paper, we propose task sequencing as a powerful and complimentary mechanism to voltage scaling in improving the thermal profile of an embedded system executing a set of periodic heterogenous tasks under timing constraints. We first derive the peak temperature of a repeating task sequence analytically and develop a heuristic to construct the task sequence that minimizes the peak temperature. Experimental evaluation shows that our task sequencing heuristic achieves peak temperature within 0.5°C of the optimal solution and 7.47°C lower, on an average, compared to the worst sequence for a large range of embedded task sets. We also propose an iterative algorithm that combines task sequencing with voltage scaling to further lower the peak temperature while satisfying the timing constraints. For embedded task sets, our combined task sequencing and voltage scaling approach achieves, on an average, 2.1°C – 6.94°C reduction in peak temperature compared to voltage scaling alone.","Temperature,
Voltage,
Cooling,
Packaging,
Thermal management,
Costs,
Embedded system,
Timing,
Energy consumption,
Power system management"
Image similarity measurement by Kullback-Leibler divergences between complex wavelet subband statistics for texture retrieval,"In this work, we present a texture-image retrieval approach, which is based on the idea of measuring the Kullback-Leibler divergence between the marginal distributions of complex wavelet coefficient magnitudes. We employ Kingsbury’s Dual-Tree Complex Wavelet Transform for image decomposition and propose to model the detail subband coefficient magnitudes by either two-parameter Weibull or Gamma distributions for which we provide closed-form solutions to the Kullback-Leibler divergence. The experimental results indicate that our approach can achieve higher retrieval rates than the classical approach of using the pyramidal Discrete Wavelet Transform together with the Generalized Gaussian model for detail subband coefficients.","Statistics,
Image retrieval,
Discrete wavelet transforms,
Image databases,
Distributed computing,
Closed-form solution,
Spatial databases,
Feature extraction,
Statistical distributions,
Wavelet coefficients"
ECG to Individual Identification,"This paper presents an individual identification system using single lead electrocardiogram (ECG). The proposed techniques for P and T wave delineation are based on time derivative and adaptive thresholding. The performance of proposed delineators is evaluated on manually annotated Physionet QT database. The accuracy of delineators are quantified on mean error and standard deviation of differences between manually annotations and automated results. Especially, lower values of error in standard deviation for onset and offset of P wave fiducials are obtained as 8.1 and 6.29 while for T wave fiducials are 9.4 and 11.2 (where units are in ms). It shows the performance of P and T wave delineators is optimum and also stable in comparison to other published results. Found fiducials are processed for the extraction of heartbeat features. From each heartbeat, 19 stable features related to interval, amplitude and angle are computed. The feasibility of ECG as a new biometric is tested on proposed identification system designed on template matching and adaptive thresholding. The accuracy of identification system is achieved to 99% on the datasize of 125 recordings prepared from 25 individual ECG of Physionet.",
Road sign detection using eigen colour,"A novel colour-based method to detect road signs directly from videos is presented. A road sign is usually painted with different colours to show its functionalities. To detect it, different detectors should be designed to deal with its colour changes. A statistic linear model of colour change space that makes road sign colours be more compact and thus sufficiently concentrated on a smaller area is presented. On this model, only one detector is needed to detect different road signs even though their colours are different. The model is global and can be used to detect any new road signs. The colour model is invariant to different perspective effects and occlusions. After that, a radial basis function (RBF) network is then used to train a classifier to find all possible road sign candidates from road scenes. Furthermore, a verification process is applied to verify each candidate using its contour feature. After verification, a rectification process is used for rectifying each skewed road sign so that its embedded texts can be well segmented and recognised. Due to the filtering effect of the proposed colour model, different road signs can be very efficiently and effectively detected from videos. Experimental results have proved that the proposed method is robust, accurate and powerful in road sign detection.",
Frequency regulation with wind power plants,A new frequency regulation scheme is developed for the wind turbine/generator/converter trio that will provide the capability to participate in restoring frequency in a way similar to the droop response of conventional generators. Output active power adjustment can be realized by both converter and pitch angle control in addition to inertial response of the wind turbine. This helps in maintaining instantaneous power balance as well as in longer term frequency regulation.,"Frequency control,
Wind turbines,
Rotors,
Generators,
Synchronous machines,
Frequency synchronization,
Synchronous generators"
Beacon Placement for Indoor Localization using Bluetooth,"We describe a method for determining the location of a mobile device, such as a handheld computer or mobile phone, in an indoor environment using Bluetooth beacons. Since it uses inexpensive commodity devices, this method is inexpensive to deploy. The limited range of Bluetooth reception is used to advantage. Another important advantage of this method is that it allows the mobile device to determine its location while remaining anonymous, unidentified to the beacons or other nearby devices. In such a deployment, an important design task is the placement of beacons. Signal propagation in indoor environments is complex, affected by factors such as floor-plans and duct-work, varying transmission and reflection properties of building materials and furniture, and interference from other devices. Therefore, the area from which a beacon is visible is very irregular and not well approximated by simple models such as ellipsoids. Our solution permits complex reception characteristics to be accurately modeled and provides a simple method for choosing beacon locations.","Bluetooth,
Indoor environments,
Global Positioning System,
Uncertainty,
Intelligent transportation systems,
Satellites,
Signal processing,
Costing,
Airports,
Rail transportation"
Local Dynamic Stability Assessment of Motion Impaired Elderly Using Electronic Textile Pants,"A clear association has been demonstrated between gait stability and falls in the elderly. Integration of wearable computing and human dynamic stability measures into home automation systems may help differentiate fall-prone individuals in a residential environment. The objective of the current study was to evaluate the capability of a pair of electronic textile (e-textile) pants system to assess local dynamic stability and to differentiate motion-impaired elderly from their healthy counterparts. A pair of e-textile pants comprised of numerous e-TAGs at locations corresponding to lower extremity joints was developed to collect acceleration, angular velocity and piezoelectric data. Four motion-impaired elderly together with nine healthy individuals (both young and old) participated in treadmill walking with a motion capture system simultaneously collecting kinematic data. Local dynamic stability, characterized by maximum Lyapunov exponent, was computed based on vertical acceleration and angular velocity at lower extremity joints for the measurements from both e-textile and motion capture systems. Results indicated that the motion-impaired elderly had significantly higher maximum Lyapunov exponents (computed from vertical acceleration data) than healthy individuals at the right ankle and hip joints. In addition, maximum Lyapunov exponents assessed by the motion capture system were found to be significantly higher than those assessed by the e-textile system. Despite the difference between these measurement techniques, attaching accelerometers at the ankle and hip joints was shown to be an effective sensor configuration. It was concluded that the e-textile pants system, via dynamic stability assessment, has the potential to identify motion-impaired elderly.",
Electrical Impedance Tomography Problem With Inaccurately Known Boundary and Contact Impedances,"In electrical impedance tomography (EIT) electric currents are injected into a body with unknown electromagnetic properties through a set of contact electrodes at the boundary of the body. The resulting voltages are measured on the same electrodes and the objective is to reconstruct the unknown conductivity function inside the body based on these data. All the traditional approaches to the reconstruction problem assume that the boundary of the body and the electrode-skin contact impedances are known a priori. However, in clinical experiments one usually lacks the exact knowledge of the boundary and contact impedances, and therefore, approximate model domain and contact impedances have to be used in the image reconstruction. However, it has been noticed that even small errors in the shape of the computation domain or contact impedances can cause large systematic artefacts in the reconstructed images, leading to loss of diagnostically relevant information. In a recent paper (Kolehmainen , 2006), we showed how in the 2-D case the errors induced by the inaccurately known boundary can be eliminated as part of the image reconstruction and introduced a novel method for finding a deformed image of the original isotropic conductivity using the theory of Teichmuller mappings. In this paper, the theory and reconstruction method are extended to include the estimation of unknown contact impedances. The method is implemented numerically and tested with experimental EIT data. The results show that the systematic errors caused by inaccurately known boundary and contact impedances can efficiently be eliminated by the reconstruction method.",
Programming modular robots with locally distributed predicates,"We present a high-level language for programming modular robotic systems, based on locally distributed predicates (LDP), which are distributed conditions that hold for a connected subensemble of the robotic system. An LDP program is a collection of LDPs with associated actions which are triggered on any subensemble that matches the predicate. The result is a reactive programming language which efficiently and concisely supports ensemble-level programming. We demonstrate the utility of LDP by implementing three common, but diverse, modular robotic tasks.","Robot programming,
Robot kinematics,
Programming profession,
Robot sensing systems,
Robotics and automation,
Computer languages,
Logic programming,
Automatic programming,
Functional programming,
Shape"
A Survey of the Stable Marriage Problem and Its Variants,"The stable marriage problem is to find a matching between men and women, considering preference lists in which each person expresses his/her preference over the members of the opposite gender. The output matching must be stable, which intuitively means that there is no man- woman pair both of which have incentive to elope. This problem was introduced in 1962 in the seminal paper of Gale and Shapley, and has attracted researchers in several areas, including mathematics, economics, game theory, computer science, etc. This paper introduces old and recent results on the stable marriage problem and some other related problems.",
Visual analytics for complex concepts using a human cognition model,"As the information being visualized and the process of understanding that information both become increasingly complex, it is necessary to develop new visualization approaches that facilitate the flow of human reasoning. In this paper, we endeavor to push visualization design a step beyond current user models by discussing a modeling framework of human “higher cognition.” Based on this cognition model, we present design guidelines for the development of visual interfaces designed to maximize the complementary cognitive strengths of both human and computer. Some of these principles are already being reflected in the better visual analytics designs, while others have not yet been applied or fully applied. But none of the guidelines have explained the deeper rationale that the model provides. Lastly, we discuss and assess these visual analytics guidelines through the evaluation of several visualization examples.",
Global pose estimation using non-tree models,"We propose a novel global pose estimation method to detect body parts of articulated objects in images based on non-tree graph models. There are two kinds of edges defined in the body part relation graph: Strong (tree) edges corresponding to the body plan that can enforce any type of constraint, and weak (non-tree) edges that express exclusion constraints arising from inter-part occlusion and symmetry conditions. We express optimal part localization as a multiple shortest path problem in a set of correlated trellises constructed from the graph model. Strong model edges generate the trellises, while weak model edges prohibit implausible poses by generating exclusion constraints among trellis nodes and edges. The optimization may be expressed as an integer linear program and solved using a novel two-stage relaxation scheme. Experiments show that the proposed method has a high chance of obtaining the globally optimal pose at low computational cost.",
Ontology-Based Test Modeling and Partition Testing of Web Services,"Testing is useful to establish trust between service providers and clients. To test the service-oriented applications, automated and specification-based test generation and test collaboration are necessary. The paper proposes an ontology-based approach for Web Services (WS) testing. A Test Ontology Model (TOM) is defined to specify the test concepts, relationships, and semantics from two aspects: test design (such as test data, test behavior, and test cases) and test execution (such as test plan, schedule and configuration). The TOM specification using OWL (Web Ontology Language) can serve as test contracts among test components. Based on the WS semantic specification in OWL-S, the paper discusses the techniques to generate the sub-domains for input partition testing. Data pools are established for each parameter of the specified service. Data partitions are derived by class property and relationship analysis. Completeness and consistency (C&C) checking can be performed on the data partitions and data values, both within the TOM and against the OWL-S, by ontology class computation and reasoning. A prototype tool is implemented to support OWL-S analysis, test ontology generation and C&C checking.","Ontologies,
Web services,
Automatic testing,
Collaboration,
Contracts,
OWL,
Service oriented architecture,
Software testing,
Collaborative work,
Computer science"
PlayStation 3-based tele-rehabilitation for children with hemiplegia,"The convergence of game technology (software and hardware), the Internet, and rehabilitation science forms the second-generation virtual rehabilitation framework. This reduced-cost and patient/therapist familiarity facilitate adoption in clinical practice. This paper presents a PlayStation 3-based hand physical rehabilitation system for children with hemiplegia due to perinatal brain injury (hemiplegic cerebral palsy) or later childhood stroke. Unlike precursor systems aimed at providing hand training for post-stroke adults in a clinical setting, the experimental system described here was developed for in-home tele-rehabilitation on a game console for children and adults with chronic hemiplegia after stroke or other focal brain injury. Significant improvements in Activities of Daily Living function followed three months of training at home on the system. Clinical trials are ongoing at this time.","Games,
Fingers,
Pediatrics,
Training,
Software,
Medical treatment,
Java"
Overcoming scaling challenges in biomolecular simulations across multiple platforms,"NAMD is a portable parallel application for biomolecular simulations. NAMD pioneered the use of hybrid spatial and force decomposition, a technique now used by most scalable programs for biomolecular simulations, including Blue Matter and Desmond developed by IBM and D. E. Shaw respectively. NAMD has been developed using Charm++ and benefits from its adaptive communication-computation overlap and dynamic load balancing. This paper focuses on new scalability challenges in biomolecular simulations: using much larger machines and simulating molecular systems with millions of atoms. We describe new techniques developed to overcome these challenges. Since our approach involves automatic adaptive runtime optimizations, one interesting issue involves dealing with harmful interaction between multiple adaptive strategies. NAMD runs on a wide variety of platforms, ranging from commodity clusters to supercomputers. It also scales to large machines: we present results for up to 65,536 processors on IBM’s Blue Gene/L and 8,192 processors on Cray XT3/XT4. In addition, we present performance results on NCSA’s Abe, SDSC’s DataStar and TACC’s LoneStar cluster, to demonstrate efficient portability. We also compare NAMD with Desmond and Blue Matter.",
Sequential particle swarm optimization for visual tracking,"Visual tracking usually involves an optimization process for estimating the motion of an object from measured images in a video sequence. In this paper, a new evolutionary approach, PSO (particle swarm optimization), is adopted for visual tracking. Since the tracking process is a dynamic optimization problem which is simultaneously influenced by the object state and the time, we propose a sequential particle swarm optimization framework by incorporating the temporal continuity information into the traditional PSO algorithm. In addition, the parameters in PSO are changed adaptively according to the fitness values of particles and the predicted motion of the tracked object, leading to a favourable performance in tracking applications. Furthermore, we show theoretically that, in a Bayesian inference view, the sequential PSO framework is in essence a multilayer importance sampling based particle filter. Experimental results demonstrate that, compared with the state-of-the-art particle filter and its variation-the unscented particle filter, the proposed tracking algorithm is more robust and effective, especially when the object has an arbitrary motion or undergoes large appearance changes.",
Gap Filling of 3-D Microvascular Networks by Tensor Voting,"We present a new algorithm which merges discontinuities in 3-D images of tubular structures presenting undesirable gaps. The application of the proposed method is mainly associated to large 3-D images of microvascular networks. In order to recover the real network topology, we need to fill the gaps between the closest discontinuous vessels. The algorithm presented in this paper aims at achieving this goal. This algorithm is based on the skeletonization of the segmented network followed by a tensor voting method. It permits to merge the most common kinds of discontinuities found in microvascular networks. It is robust, easy to use, and relatively fast. The microvascular network images were obtained using synchrotron tomography imaging at the European Synchrotron Radiation Facility. These images exhibit samples of intracortical networks. Representative results are illustrated.","Filling,
Tensile stress,
Voting,
Image segmentation,
Biomedical imaging,
Robustness,
X-ray imaging,
Uninterruptible power systems,
Network topology,
Synchrotrons"
Distributed subgradient methods and quantization effects,"We consider a convex unconstrained optimization problem that arises in a network of agents whose goal is to cooperatively optimize the sum of the individual agent objective functions through local computations and communications. For this problem, we use averaging algorithms to develop distributed subgradient methods that can operate over a time-varying topology. Our focus is on the convergence rate of these methods and the degradation in performance when only quantized information is available. Based on our recent results on the convergence time of distributed averaging algorithms, we derive improved upper bounds on the convergence rate of the unquantized subgradient method. We then propose a distributed subgradient method under the additional constraint that agents can only store and communicate quantized information, and we provide bounds on its convergence rate that highlight the dependence on the number of quantization levels.","Quantization,
Convergence,
Optimization methods,
Network topology,
Upper bound,
Resource management,
Control systems,
Communication industry,
Computer industry,
Electrical equipment industry"
Unsupervised modeling of object categories using link analysis techniques,"We propose an approach for learning visual models of object categories in an unsupervised manner in which we first build a large-scale complex network which captures the interactions of all unit visual features across the entire training set and we infer information, such as which features are in which categories, directly from the graph by using link analysis techniques. The link analysis techniques are based on well-established graphmining techniques used in diverse applications such as WWW, bioinformatics, and social networks. The techniques operate directly on the patterns of connections between features in the graph rather than on statistical properties, e.g., from clustering in feature space. We argue that the resulting techniques are simpler, and we show that they perform similarly or better compared to state of the art techniques on common data sets. We also show results on more challenging data sets than those that have been used in prior work on unsupervised modeling.","Information analysis,
Complex networks,
World Wide Web,
Social network services,
Data mining,
Computer science,
Large-scale systems,
Bioinformatics,
Visual perception,
Web search"
Modeling and Discovery of Data Providing Services,"Abstract Web Services providing access to datasources with structured data have an important place in the SOA. In this paper we focus on modeling and discovery of generic data providing services (DPS), with the goal of making data providing services available for interactions with service requesters in contexts such as service composition and mediation. In our model RDF Views are used to represent the content provided by the DPS. A characterization of match between description of DPS as RDF Views and the OWL-S service request is specified, based on which we developed a flexible matchmaking algorithm for discovery of data providing services. Finally, we propose a realization of the DPS using a SOAP version of the SPARQL protocol and a dynamic configuration interface allowing easy interactions of service requesters with data providing services.","Resource description framework,
Web services,
Ontologies,
Relational databases,
Computer science,
Context-aware services,
OWL,
Educational institutions,
Robots,
Service oriented architecture"
Verification and validation of simulation models,"In this paper we discuss verification and validation of simulation models. Four different approaches to deciding model validity are described; two different paradigms that relate verification and validation to the model development process are presented; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are discussed; a way to document results is given; a recommended procedure for model validation is presented; and model accreditation is briefly discussed.","Accreditation,
Computational modeling,
Computer simulation,
Random variables,
Testing,
Educational institutions,
Computer science,
Decision making,
Application software,
Costs"
Bounds on the Degree of Impropriety of Complex Random Vectors,"A complex random vector is called improper if it is correlated with its complex conjugate. We introduce a measure for the degree of impropriety, which is a function of the canonical correlations between the vector and its complex conjugate (sometimes called the circularity spectrum). This measure is invariant under linear transformation, and it relates the entropy of an improper Gaussian random vector to its corresponding proper version. For vectors with given spectrum, we present upper and lower bounds on the attainable degree of impropriety, in terms of the eigenvalues of the augmented covariance matrix.","Covariance matrix,
Vectors,
Eigenvalues and eigenfunctions,
Entropy,
Independent component analysis,
Upper bound,
Australia Council,
Computer science"
3D facial expression recognition with geometrically localized facial features,"This paper describes a pose invariant three-dimensional (3D) facial expression recognition method using distance vectors retrieved from 3D distributions of facial feature points to classify universal facial expressions. Probabilistic Neural Network architecture is employed as a classifier to recognize the facial expressions from a distance vector obtained from 3D facial feature locations. Facial expressions such as anger, sadness, surprise, joy, disgust, fear and neutral are successfully recognized with an average recognition rate of 87.8%.",
High resolution matting via interactive trimap segmentation,"We present a new approach to the matting problem which splits the task into two steps: interactive trimap extraction followed by trimap-based alpha matting. By doing so we gain considerably in terms of speed and quality and are able to deal with high resolution images. This paper has three contributions: (i) a new trimap segmentation method using parametric max-flow; (ii) an alpha matting technique for high resolution images with a new gradient preserving prior on alpha; (iii) a database of 27 ground truth alpha mattes of still objects, which is considerably larger than previous databases and also of higher quality. The data-base is used to train our system and to validate that both our trimap extraction and our matting method improve on state-of-the-art techniques.",
RSS-Based Localization in Environments with Different Path Loss Exponent for Each Link,"The path loss exponent is very important parameter for localization using receive signal strength (RSS). In actual environments, path loss exponent for each link (target to each receive node) differs. However, the conventional localization methods use the same path loss exponent for all links. Hence, there are some mismatches between the real path loss exponent and the one used to estimate. We proposed the localization method that considers all the combinations of path loss exponents for each link and estimates the target location by averaging the target locations derived with all the combinations. However, the amount of calculation is huge. In this paper we propose RSS-based localization in environments with different path loss exponent for each link. The proposed method is a grid-based centralized localization using RSS. First the proposed method sets the minimum distance di,min and maximum distance di,max for each node i by using the RSS of each receive node i and the minimum and maximum path loss exponents set before estimation. Next, it calculates the distance di,(k,l) between the candidate target position (k, I) and each receive node i. If di,min les di,(k,l) les di,max, vote the grid (k,l). These processes are performed for all the receive nodes over the search area. Finally, the grid point with most voting is estimated to be the target location. According to the simulation results, we show that the proposed method achieves the higher localization accuracy than the conventional localization method using the same path loss exponent for all the links when the distribution of the path loss exponents over the field is uniform distribution.",
Mitigating power-supply induced delay variations using self adjusting clock buffers,"Aggressive technology scaling tends to reduce integrated circuits resilience against environmental variations. In this paper, we present an adaptive clock buffer circuit design and an adaptive clock distribution network (CDN) to improve chip performance and reliability in the presence of on-chip power-supply variations. The adaptive buffer provides a supply insensitive propagation delay to minimize the supply variation induced clock skew in clock distribution networks. Experimental results show that our technique reduces supply variation induced clock skew by at least 85% in a typical seven level clock tree architecture as compared to a nonadaptive worst case CDN, which represents up to 40% reduction in cycle time in state of the art processors.",
Stable and Efficient Spectrum Access in Next Generation Dynamic Spectrum Networks,"Future wireless infrastructure networks will dynamically access spectrum for maximum utilization. However, the fundamental challenge is how to provide stable spectrum access required for most applications. Using dynamic spectrum access, each node's spectrum usage is inherently unpredictable and unstable. We propose to address this challenge by integrating interference-aware statistical admission control with stability- driven spectrum allocation. Specifically, we propose to proactively regulate nodes' spectrum demand to allow efficient statistical multiplexing while minimizing outages. Admitted nodes coordinate to adapt instantaneous spectrum allocation to match time- varying demand. While the optimization problem is NP-hard, we develop computational-efficient algorithms with strong analytical guarantees. Experimental results show that the proposed approach can provide stable spectrum usage while improving its utilization by 80-100% compared to conventional solutions.","Next generation networking,
Peer to peer computing,
Admission control,
Stability,
Algorithm design and analysis,
Interference constraints,
Communications Society,
Computer science,
Application software,
Technological innovation"
An Availability-Aware Task Scheduling Strategy for Heterogeneous Systems,"High availability is a key requirement in the design and development of heterogeneous systems where processors operate at different speeds and are not continuously available for computation. Most existing scheduling algorithms designed for heterogeneous systems do not factor in availability requirements imposed by multiclass applications. To remedy this shortcoming, we investigate in this paper the scheduling problem for multiclass applications running in heterogeneous systems with availability constraints. In an effort to explore this issue, we model each node in a heterogeneous system using the node's computing capability and availability. Multiple classes of tasks are characterized by their execution times and availability requirements. To incorporate availability and heterogeneity into scheduling, we define new metrics to quantify system availability and heterogeneity for multiclass tasks. We then propose a scheduling algorithm to improve the availability of heterogeneous systems while maintaining good performance in the response time of tasks. Experimental results show that our algorithm achieves a good trade-off between availability and responsiveness.","Availability,
Scheduling algorithm,
Processor scheduling,
Single machine scheduling,
Scheduling,
Computational modeling,
Time factors,
Resource management"
Two-Phased Approximation Algorithms for Minimum CDS in Wireless Ad Hoc Networks,"Connected dominating set (CDS) has a wide range of applications in wireless ad hoc networks. A number of distributed algorithms for constructing a small CDS in wireless ad hoc networks have been proposed in the literature. The majority of these distributed algorithms follow a general two-phased approach. The first phase constructs a dominating set, and the second phase selects additional nodes to interconnect the nodes in the dominating set. In this paper, we prove that the approximation ratio of the two-phased algorithm in [10] is at most 7 1\3, improving upon the previous best-known approximation ratio of 7.6 due to [12]. We also propose a new two-phased approximation algorithm and prove that its approximation ratio is at most     6 7\18. Our analyses exploit an improved upper bound on the number independent points that can be packed in the neighborhood of a connected finite planar set.",
Vision-Based Online Process Control in Manufacturing Applications,"Applications such as layered manufacturing, or in general, solid free-form fabrication, pose a major challenge on online process control. For these parts to be functional, it is imperative that their mechanical and structural properties are strictly kept within respective tolerances. To that end, no internal or external defects, especially voids, can be tolerated. Since these parts are made layer by layer, it is necessary to inspect top surface and boundary of each layer before the next layer is deposited. Two issues are of major concern here: 1) inspection must be nondestructive, that is, layer surface and boundary must not be touched by the inspecting instruments and 2) the time window for inspection and any corrective measures should not exceed the maximum time limits necessary for two adjacent layers to properly bind together. Here, we present a closed-loop online process control model, where the process feedbacks are obtained from a 3-D imaging system, and where the process dynamics model takes into account the correlation and dependency between adjacent layers. To ensure that the feedback is performed within the tolerable time windows, our 3-D image processing parametric model takes advantage of physical characteristic of layer surface, and uses Gaussian function as a shape descriptor of image units. We obtain 2-D profiles from representative signature(s), and then sweep along road path defined in the CAD model. The idea of reconstructing representative signature comprises some accuracy, but compared with classical shape-from-shading method, the proposed approach is computationally more efficient. The 3-D quality measures (such as volume or depth of voids) are then fed to process dynamics model, which computes the necessary compensation on the deposition flow rate for the next layer. We examine three process dynamics models to And out that a fuzzy model which takes into account correlation between adjacent layers and includes locally linear submodels for underfills and overfills is the most appropriate.","Process control,
Manufacturing processes,
Inspection,
Feedback,
Layered manufacturing,
Solids,
Fabrication,
Mechanical factors,
Instruments,
Time measurement"
Calysto,"Automatically detecting bugs in programs has been a long-held goal in software engineering. Many techniques exist, trading-off varying levels of automation, thoroughness of coverage of program behavior, precision of analysis, and scalability to large code bases. This paper presents the Calysto static checker, which achieves an unprecedented combination of precision and scalability in a completely automatic extended static checker. Calysto is interprocedurally path-sensitive, fully context-sensitive, and bit-accurate in modeling data operations --- comparable coverage and precision to very expensive formal analyses --- yet scales comparably to the leading, less precise, static-analysis-based tool for similar properties. Using Calysto, we have discovered dozens of bugs, completely automatically, in hundreds of thousands of lines of production, open-source applications, with a very low rate of false error reports. This paper presents the design decisions, algorithms, and optimizations behind Calysto's performance.","Scalability,
Computer bugs,
Automation,
Software testing,
Formal verification,
Java,
Computer science,
Permission,
Software engineering,
Context modeling"
On the representation and multiplication of hypersparse matrices,"Multicore processors are marking the beginning of a new era of computing where massive parallelism is available and necessary. Slightly slower but easy to parallelize kernels are becoming more valuable than sequentially faster kernels that are unscalable when parallelized. In this paper, we focus on the multiplication of sparse matrices (SpGEMM). We first present the issues with existing sparse matrix representations and multiplication algorithms that make them unscalable to thousands of processors. Then, we develop and analyze two new algorithms that overcome these limitations. We consider our algorithms first as the sequential kernel of a scalable parallel sparse matrix multiplication algorithm and second as part of a polyalgorithm for SpGEMM that would execute different kernels depending on the sparsity of the input matrices. Such a sequential kernel requires a new data structure that exploits the hypersparsity of the individual submatrices owned by a single processor after the 2D partitioning. We experimentally evaluate the performance and characteristics of our algorithms and show that they scale significantly better than existing kernels.","Sparse matrices,
Transmission line matrix methods,
Kernel,
Arithmetic,
Computer science,
Concurrent computing,
Algorithm design and analysis,
Partitioning algorithms,
Numerical analysis,
Multicore processing"
Interference alignment on the deterministic channel and application to fully connected AWGN interference networks,"An interference alignment example is constructed for the deterministic channel model of the K user interference channel. The deterministic channel example is then translated into the Gaussian setting, creating the first known example of a fully connected Gaussian K user interference network with single antenna nodes, real, non-zero and contant channel coefficients, and no propagation delays where the degrees of freedom outerbound is achieved. An analogy is drawn between the propagation delay based interference alignment examples and the deterministic channel model which also allows similar constructions for the 2 user X channel as well.","Interference,
AWGN,
Wireless networks,
Transmitters,
Vectors,
Propagation delay,
Signal to noise ratio,
Application software,
Computer science,
Antennas and propagation"
Efficient symbolic multi-objective design space exploration,"Nowadays many design space exploration tools are based on Multi-Objective Evolutionary Algorithms (MOEAs). Beside the advantages of MOEAs, there is one important drawback as MOEAs might fail in design spaces containing only a few feasible solutions or as they are often afflicted with premature convergence, i.e., the same design points are revisited again and again. Exact methods, especially Pseudo Boolean solvers (PB solvers) seem to be a solution. However, as typical design spaces are multi-objective, there is a need for multi-objective PB solvers. In this paper, we will formalize the problem of design space exploration as multi-objective 0–1 ILP. We will propose (1) a heuristic approach based on PB solvers and (2) a complete multi-objective PB solver based on a backtracking algorithm that incorporates the non-dominance relation from multi-objective optimization and is restricted to linear objective functions. First results from applying our novel multi-objective PB solver to synthetic problems will show its effectiveness in small sized design spaces as well as in large design spaces only containing a few feasible solutions. For non-linear and large problems, the proposed heuristic approach is outperforming common MOEA approaches. Finally, a real world example from the automotive area will emphasize the efficiency of the proposed algorithms.","Space exploration,
Algorithm design and analysis,
Evolutionary computation,
Runtime,
Hardware,
Software tools,
Computer science,
Glass,
Automotive engineering,
Constraint optimization"
Focus+Context Visualization with Distortion Minimization,"The need to examine and manipulate large surface models is commonly found in many science, engineering, and medical applications. On a desktop monitor, however, seeing the whole model in detail is not possible. In this paper, we present a new, interactive Focus+Context method for visualizing large surface models. Our method, based on an energy optimization model, allows the user to magnify an area of interest to see it in detail while deforming the rest of the area without perceivable distortion. The rest of the surface area is essentially shrunk to use as little of the screen space as possible in order to keep the entire model displayed on screen. We demonstrate the efficacy and robustness of our method with a variety of models.","Visualization,
Deformable models,
Lenses,
Displays,
Optical distortion,
Computer science,
Space technology,
Biomedical engineering,
Power engineering and energy,
Medical services"
Faster matrix-vector multiplication on GeForce 8800GTX,"Recently a GPU has acquired programmability to perform general purpose computation fast by running ten thousands of threads concurrently. This paper presents a new algorithm for dense matrix-vector multiplication on NVIDIA CUDA architecture. The experimental results on GeForce 8800GTX show that the proposed algorithm runs maximum 15.69 (resp., 32.88) times faster than the sgemv routine in NVIDIA’s BLAS library CUBLAS 1.1 (resp., Intel Math Kernel Library 9.1 on one-core of 2.0 GHz Intel Xeon E5335 CPU with SSE3 SIMD instructions) for matrices with order 16 to 12800. The performance, including the data transfer between CPU and GPU, of Jacobi’s iterative method for solving linear equations shows that the proposed algorithm is practical for some real applications.",
Recognizing primitive interactions by exploring actor-object states,"In this paper, we present a solution to the novel problem of recognizing primitive actor-object interactions from videos. Here, we introduce the concept of actor-object states. Our method is based on the observation that at the moment of physical contact, both the motion and the appearance of actors are constrained by the target object. We propose a probabilistic framework that automatically learns models in such constrained states. We use joint probability distributions to represent both actor and object appearances as well as their intrinsic spatio-temporal configurations. Finally, we demonstrate the applicability of our approach on series of human-object interaction classification experiments.",
Utility-Based Opportunistic Routing in Multi-Hop Wireless Networks,"Recently, opportunistic routing (OR) has been widely used to compensate for the low packet delivery ratio of multi-hop wireless networks. Previous works either provide heuristic solutions without optimality analysis, or assume that unlimited retransmission is available for delivering a data packet. In this paper, we apply OR to a utility-based routing where the successful delivery of a datapacket generates benefit. The objective is to maximize utility, defined as a function of benefit and cost of transmission. As the link reliability of each relay determines eventual packet delivery and hence utility, OR offers the ability to increase reliability through opportunistic relays. We explore the optimality of utility-based routing through OR without allowing retransmission, and observe that the optimal scheme requires exhaustive searching of all paths from source to destination. We then propose a heuristic solution to select relays and determine priorities among them. Finally, we provide distributed implementations for both schemes. Simulations on NS-2 and our customized simulator are conducted to verify the effectiveness of the heuristic compared with the optimal.","Relays,
Reliability,
Computational modeling,
Complexity theory,
Network topology,
Routing,
Heuristic algorithms"
Spatial Outsourcing for Location-based Services,"The embedding of positioning capabilities in mobile devices and the emergence of location-based applications have created novel opportunities for utilizing several types of multi-dimensional data through spatial outsourcing. In this setting, a data owner (DO) delegates its data management tasks to a location-based service (LBS) that processes queries originating from several clients/ subscribers. Because the LBS is not the real owner of the data, it must prove (to each client) the correctness of query output using an authenticated structure signed by the DO. Currently there is very narrow selection of multi-dimensional authenticated structures, among which the VR-tree is the best choice. Our first contribution is the MR-tree, a novel index suitable for spatial outsourcing. We show, analytically and experimentally, that the MR-tree outperforms the VR-tree, usually by orders of magnitude, on all performance metrics, including construction cost, index size, query and verification overhead. Motivated by the fact that successive queries by the same mobile client exhibit locality, we also propose a synchronized caching technique that utilizes the results of previous queries to reduce the size of the additional information sent to the client for verification purposes.","Outsourcing,
Relational databases,
Computer science,
Mobile computing,
Multidimensional systems,
Costs,
Public key,
Spatial databases,
Data engineering,
Application software"
Integrated boost-sepic converter for high step-up applications,"A general boost converter has limited voltage step-up ratio because of its parasitic resistances. Thus, it is not applicable for high step-up applications. As a solution, combining a boost converter with a series output module can be considered to supplement the insufficient step-up ratio. By applying this concept, a new integrated boost-sepic (IBS) converter, which provides additional step-up ratio with the help of an isolated sepic converter, is proposed in this paper. Since the boost converter and the sepic converter share a boost inductor and a switch, its structure is simple. Moreover, the proposed IBS converter needs no current-snubber for the diodes, since the transformer leakage inductor alleviates the reverse recovery. The operational principle and characteristics of proposed converter are presented, and verified experimentally with a 200-W, 42-Vdc input, 400-Vdc output converter prototype.",
Distributed Spatial Anomaly Detection,"Detection of traffic anomalies is an important problem that has been the focus of considerable research. Recent work has shown the utility of spatial detection of anomalies via crosslink traffic comparisons. In this paper we identify three advances that are needed to make such methods more useful and practical for network operators. First, anomaly detection methods should avoid global communication and centralized decision making. Second, nonparametric anomaly detection methods are needed to augment current parametric approaches. And finally, such methods should not just identify possible anomalies, but should also annotate each detection with some probabilistic qualifier of its importance. We propose a framework that simultaneously advances the current state of the art on all three fronts. We show that routers can effectively identify volume anomalies through crosslink comparison of traffic observed only on the router's own links. Second, we show that generalized quantile estimators are an effective way to identify high-dimensional sets of local traffic patterns that are potentially anomalous; such methods can be either parametric or nonparametric, and we evaluate both. Third, through the use of false discovery rate as a detection metric, we show that candidate anomalous patterns can be equipped with an estimate of a probability that they truly are anomalous. Overall, our framework provides network operators with an anomaly detection methodology that is distributed, effective, and easily interpretable. Part of the underlying statistical framework, which merges aspects of nonparametric set estimation and multiple hypothesis testing, is novel in itself, although the derivation of that framework is necessarily given elsewhere.","Telecommunication traffic,
Communications Society,
Global communication,
Decision making,
Probability,
Testing,
Computer crime,
Computer worms,
Equipment failure,
Computer science"
A Geometry-Driven Optical Flow Warping for Spatial Normalization of Cortical Surfaces,"Spatial normalization is frequently used to map data to a standard coordinate system by removing intersubject morphological differences, thereby allowing for group analysis to be carried out. The work presented in this paper is motivated by the need for an automated cortical surface normalization technique that will automatically identify homologous cortical landmarks and map them to the same coordinates on a standard manifold. The geometry of a cortical surface is analyzed using two shape measures that distinguish the sulcal and gyral regions in a multiscale framework. A multichannel optical flow warping procedure aligns these shape measures between a reference brain and a subject brain, creating the desired normalization. The partial differential equation that carries out the warping is implemented in a Euclidean framework in order to facilitate a multiresolution strategy, thereby permitting large deformations between the two surfaces. The technique is demonstrated by aligning 33 normal cortical surfaces and showing both improved structural alignment in manually labeled sulci and improved functional alignment in positron emission tomography data mapped to the surfaces. A quantitative comparison between our proposed surface-based spatial normalization method and a leading volumetric spatial normalization method is included to show that the surface-based spatial normalization performs better in matching homologous cortical anatomies.",
Method for Determining Kinematic Parameters of the In Vivo Thumb Carpometacarpal Joint,"The mobility of the thumb carpometacarpal (CMC) joint is critical for functional grasping and manipulation tasks. We present an optimization technique for determining from surface marker measurements a subject-specific kinematic model of the in vivo CMC joint that is suitable for measuring mobility. Our anatomy-based cost metric scores a candidate joint model by the plausibility of the corresponding joint angle values and kinematic parameters rather than only the marker trajectory reconstruction error. The proposed method repeatably determines CMC joint models with anatomically-plausible directions for the two dominant rotational axes and a lesser range of motion (RoM) for the third rotational axis. We formulate a low-dimensional parameterization of the optimization domain by first solving for joint axis orientation variables that then constrain the search for the joint axis location variables. Individual CMC joint models were determined for 24 subjects. The directions of the flexion-extension (FE) axis and adduction-abduction (AA) axis deviated on average by 9deg and 22deg, respectively, from the mean axis direction. The average RoM for FE, AA, and pronation-supination (PS) joint angles were 76deg, 43deg, and 23deg for active CMC movement. The mean separation distance between the FE and AA axes was 4.6 mm, and the mean skew angle was 87deg from the positive flexion axis to the positive abduction axis.","Kinematics,
In vivo,
Joints,
Thumb,
Iron,
Read only memory,
Bones,
Computer science,
Biomedical measurements,
Robots"
Polymorphic On-Chip Networks,"As the number of cores per die increases, be they processors, memory blocks, or custom accelerators, the on-chip interconnect the cores use to communicate gains importance. We begin this study with an area-performance analysis of the interconnect design space. We find that there is no single network design that yields optimal performance across a range of traffic patterns. This indicates that there is an opportunity to gain performance by customizing the interconnect to a particular application or workload. We propose polymorphic on-chip networks to enable per-application network customization. This network can be configured prior to application runtime, to have the topology and buffering of arbitrary network designs. This paper proposes one such polymorphic network architecture. We demonstrate its modes of configurability, and evaluate the polymorphic network architecture design space, producing polymorphic fabrics that minimize the network area overhead. Finally, we expand the network on chip design space to include a polymorphic network design, showing that a single polymorphic network is capable of implementing all of the pareto optimal fixed-network designs.","Network-on-a-chip,
Network topology,
Telecommunication traffic,
Traffic control,
Computer science,
Runtime,
Hardware,
Bandwidth,
Space exploration,
Pareto analysis"
Vision-Based Real-Time Lane Marking Detection and Tracking,"Detection and tracking of lane marking is essential for driving safety and intelligent vehicle. In this paper, an algorithm is presented which allows detection and tracking of multiple lane markings. Edge points cue is used to detect the lane marking and a road orientation estimation method is used to delete the edge lines which are impossible attribute to lane markings. In order to select the candidate lane marking, a Confidence Measures method is proposed. Then a finite-state machine decides whether or not a lane marking is really detected by fusion multi-frame detection results. Specifically, a particle filter is used to predict the future values of the lane marking model parameters, based on past observations. With particle filtering and Confidence Measures method, lane markings on various road scenes are detected and tracked. Experimental in different conditions, including illumination, weather and road, demonstrates its effectiveness and robustness. The algorithm runs in real-time at rates of about 30 Hz.",
Maximum a Posteriori Adaptation of the Centroid Model for Speaker Verification,"Maximum a posteriori adapted Gaussian mixture model (GMM-MAP) is widely used in speaker verification. GMMs have three sets of parameters to be adapted: means, covariances, and weights. However, practice has shown that it is sufficient to adapt the means only. Motivated by this, we formulate maximum a posteriori vector quantization (VQ-MAP) procedure which stores and adapts the mean vectors (centroids) only. Experiments on the NIST 2001 and NIST 2006 corpora indicate that VQ-MAP gives comparable accuracy with GMM-MAP with simpler implementation and faster adaptation.",
A CFD-Based Tool for Studying Temperature in Rack-Mounted Servers,"Temperature-aware computing is becoming more important in design of computer systems as power densities are increasing and the implications of high operating temperatures result in higher failure rates of components and increased demand for cooling capability. Computer architects and system software designers need to understand the thermal consequences of their proposals, and develop techniques to lower operating temperatures to reduce both transient and permanent component failures. Recognizing the need for thermal modeling tools to support those researches, there has been work on modeling temperatures of processors at the micro-architectural level which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor - a server box, rack or even a machine room. In this paper we presents a detailed 3-dimensional computational fluid dynamics based thermal modeling tool, called ThermoStat, for rack-mounted server systems. We conduct several experiments with this tool to show how different load conditions affect the thermal profile, and also illustrate how this tool can help design dynamic thermal management techniques. We propose reactive and proactive thermal management for rack mounted server and isothermal workload distribution for rack.","thermal management (packaging),
computational fluid dynamics,
cooling,
integrated circuit design,
microprocessor chips"
A GPU-Based Multiple-Pattern Matching Algorithm for Network Intrusion Detection Systems,"By the development of network applications, network security issues are getting more and more important. This paper proposes a multiple-pattern matching algorithm for the network intrusion detection systems based on the GPU (Graphics Processing Units). The highly parallelism of the GPU computation power is used to inspect the packet content in parallel. The performance of the proposed approach is analyzed through evaluations such as using various texture formats and different implementations. Experimental results indicate that the performance of the proposed approach is twice of that of the modified Wu-Manber algorithm used in Snort. The proposed approach makes a commodity and cheap GPU card as a high performance pattern matching co-processor.",
Map-reduce as a Programming Model for Custom Computing Machines,"The map-reduce model requires users to express their problem in terms of a map function that processes single records in a stream, and a reduce function that merges all mapped outputs to produce a final result. By exposing structural similarity in this way, a number of key issues associated with the design of custom computing machines including parallelisation; design complexity; software-hardware partitioning; hardware-dependency, portability and scalability can be easily addressed. We present an implementation of a map-reduce library supporting parallel field programmable gate arrays (FPGAs) and graphics processing units (GPUs). Parallelisation due to pipelining, multiple data paths and concurrent execution of FPGA/GPU hardware is automatically achieved. Users first specify the map and reduce steps for the problem in ANSI Cand no knowledge of the underlying hardware or parallelisation is needed. The source code is then manually translated into a pipelined data path which, along with the map-reduce library, is compiled into appropriate binary configurations for the processing units. We describe our experience in developing a number of benchmark problems in signal processing, Monte Carlo simulation and scientific computing as well as report on the performance of FPGA, GPU and hetereogeneous systems.",
On the Need for Mixed Media in Distributed Requirements Negotiations,"Achieving agreement with respect to software requirements is a collaborative process that traditionally relies on same-time, same-place interactions. As the trend toward geographically distributed software development continues, colocated meetings are becoming increasingly problematic. Our research investigates the impact of computer-mediated communication on the performance of distributed client/developer teams involved in the collaborative development of a requirements specification. Drawing on media-selection theories, we posit that a combination of lean and rich media is needed for an effective process of requirements negotiations when stakeholders are geographically dispersed. In this paper, we present an empirical study that investigates the performance of six educational global project teams involved in a negotiation process using both asynchronous text-based and synchronous videoconferencing-based communication modes. The findings indicate that requirement negotiations were more effective when the groups conducted asynchronous structured discussions of requirement issues prior to the synchronous negotiation meeting. Asynchronous discussions were useful in resolving issues related to uncertainty in requirements, thus allowing synchronous negotiations to focus more on removing ambiguities in the requirements.","Collaboration,
Programming,
Collaborative software,
Computer mediated communication,
Grounding,
Computer science,
Dispersion,
Uncertainty,
Globalization,
Computer industry"
Analysis of Resolution and Noise Properties of Nonquadratically Regularized Image Reconstruction Methods for PET,"We present accurate and efficient methods for estimating the spatial resolution and noise properties of nonquadratically regularized image reconstruction for positron emission tomography (PET). It is well known that quadratic regularization tends to over-smooth sharp edges. Many types of edge-preserving nonquadratic penalties have been proposed to overcome this problem. However, there has been little research on the quantitative analysis of nonquadratic regularization due to its nonlinearity. In contrast, quadratically regularized estimators are approximately linear and are well understood in terms of resolution and variance properties. We derive new approximate expressions for the linearized local perturbation response (LLPR) and variance using the Taylor expansion with the remainder term. Although the expressions are implicit, we can use them to accurately predict resolution and variance for nonquadratic regularization where the conventional expressions based on the first-order Taylor truncation fail. They also motivate us to extend the use of a certainty-based modified penalty to nonquadratic regularization cases in order to achieve spatially uniform perturbation responses, analogous to uniform spatial resolution in quadratic regularization. Finally, we develop computationally efficient methods for predicting resolution and variance of nonquadratically regularized reconstruction and present simulations that illustrate the validity of these methods.","Image analysis,
Image resolution,
Image reconstruction,
Positron emission tomography,
Spatial resolution,
Signal resolution,
Linear approximation,
Signal processing,
Image processing,
Signal to noise ratio"
Dependence-aware transactional memory for increased concurrency,"Transactional memory (TM) is a promising paradigm for helping programmers take advantage of emerging multi-core platforms. Though they perform well under low contention, hardware TM systems have a reputation of not performing well under high contention, as compared to locks. This paper presents a model and implementation of dependence-aware transactional memory (DATM), a novel solution to the problem of scaling under contention. Unlike many proposals to deal with write-shared data (which arise in common data structures like counters and linked lists), DATM operates transparently to the programmer. The main idea in DATM is to accept any transaction execution interleaving that is conflict serializable, including interleavings that contain simple conflicts. Current TM systems reduce useful concurrency by restarting conflicting transactions, even if the execution interleaving is conflict serializable. DATM manages dependences between uncommitted transactions, sometimes forwarding data between them to safely commit conflicting transactions. The evaluation of our prototype shows that DATM increases concurrency, for example by reducing the runtime of STAMP benchmarks by up to 39% and reducing transaction restarts by up to 94%.",
Noninvasive Three-Dimensional Cardiac Activation Imaging From Body Surface Potential Maps: A Computational and Experimental Study on a Rabbit Model,"Three-dimensional (3-D) cardiac activation imaging (3-DCAI) is a recently developed technique that aims at imaging the activation sequence throughout the the ventricular myocardium. 3-DCAI entails the modeling and estimation of the cardiac equivalent current density (ECD) distribution from which the activation time at any myocardial site is determined as the time point with the peak amplitude of local ECD estimates. In this paper, we report, for the first time, an in vivo validation study assessing the feasibility of 3-DCAI in comparison with the 3-D intracardiac mapping, for a group of four healthy rabbits undergoing the ventricular pacing from various locations. During the experiments, the body surface potentials and the intramural bipolar electrical recordings were simultaneously measured in a closed-chest condition. The ventricular activation sequence noninvasively imaged from the body surface measurements by using 3-DCAI was generally in agreement with that obtained from the invasive intramural recordings. The quantitative comparison between them showed a root mean square (rms) error of 7.42 plusmn0.61 ms, a relative error (RE) of 0.24 plusmn0.03, and a localization error (LE) of 5.47 plusmn1.57 mm. The experimental results were also consistent with our computer simulations conducted in well-controlled and realistic conditions. The present study suggest that 3-DCAI can noninvasively capture some important features of ventricular excitation (e.g., the activation origin and the activation sequence), and has the potential of becoming a useful imaging tool aiding cardiovascular research and clinical diagnosis of cardiac diseases.",
Congestion-Constrained Layer Assignment for Via Minimization in Global Routing,"In this paper, we study the problem of layer assignment for via minimization, which arises during multilayer global routing. In addressing this problem, we take the total overflow and the maximum overflow as the congestion constraints from a given one-layer global routing solution and aim to find a layer assignment result for each net such that the via cost is minimized while the given congestion constraints are satisfied. To solve the problem, we propose a polynomial-time algorithm which first generates a net order and then performs layer assignment one net at a time according to the order using dynamic programming. Our algorithm is guaranteed to generate a layer assignment solution satisfying the given congestion constraints. We used the six-layer benchmarks released from the ISPD'07 global routing contest to test our algorithm. The experimental results show that our algorithm was able to improve the contest results of the top three winners MaizeRouter, BoxRouter, and FGR on each benchmark. As compared to BoxRouter 2.0 and FGR 1.1, which are newer versions of BoxRouter and FGR, our algorithm respectively produced smaller via costs on all benchmarks and half the benchmarks. Our algorithm can also be adapted to refine a given multilayer global routing solution in a net-by-net manner, and the experimental results show that this refinement approach improved the via costs on all benchmarks for FGR 1.1.",
Digitally enhanced analog circuits: System aspects,"An overview of digital enhancement techniques for analog circuits is presented. Recent research suggests that the high density and low energy of digital circuits can be leveraged to enable a new generation of interface electronics that is based on minimal precision, low complexity analog blocks. Today, examples of enhancement schemes can be found in diverse applications and include nonlinearity compensation of ADCs, predistortion of power amplifiers and mismatch calibration in radio receivers. Since it is often difficult to identify commonalities among these different, but conceptually related schemes, this tutorial paper aims to provide a unified and system-oriented perspective of the field.","Analog circuits,
Calibration,
Signal processing,
Microprocessors,
Circuit noise,
Digital circuits,
Linearity,
Power dissipation,
Signal resolution,
Digital signal processing"
Mixed Deterministic/Randomized Methods for Fixed Order Controller Design,"In this paper, we propose a general methodology for designing fixed order controllers for single-input single-output plants. The controller parameters are classified into two classes: randomized and deterministically designed. For the first class, we study randomized algorithms. In particular, we present two low-complexity algorithms based on the Chernoff bound and on a related bound (often called ldquolog-over-logrdquo bound) which is generally used for optimization problems. Secondly, for the deterministically designed parameters, we reformulate the original problem as a set of linear equations. Then, we develop a technique which efficiently solves it using a combination of matrix inversions and sensitivity methods. A detailed complexity analysis of this technique is carried on, showing its superiority (from the computational point of view) to existing algorithms based on linear programming. In the second part of the paper, these results are extended to H infin performance. One of the contributions is to prove that the deterministically designed parameters enjoy a special convex characterization. This characterization is then exploited in order to design fixed order controllers efficiently. We then show further extensions of these methods for stabilization of interval plants. In particular, we derive a simple one-parameter formula for computing the so-called critical frequencies which are required by the algorithms.","Algorithm design and analysis,
Output feedback,
Design methodology,
Linear programming,
Control systems,
Three-term control,
Automatic control,
Equations,
Frequency,
Computer science"
A point-based POMDP planner for target tracking,"Target tracking has two variants that are often studied independently with different approaches: target searching requires a robot to find a target initially not visible, and target following requires a robot to maintain visibility on a target initially visible. In this work, we use a partially observable Markov decision process (POMDP) to build a single model that unifies target searching and target following. The POMDP solution exhibits interesting tracking behaviors, such as anticipatory moves that exploit target dynamics, informationgathering moves that reduce target position uncertainty, and energy-conserving actions that allow the target to get out of sight, but do not compromise long-term tracking performance. To overcome the high computational complexity of solving POMDPs, we have developed SARSOP, a new point-based POMDP algorithm based on successively approximating the space reachable under optimal policies. Experimental results show that SARSOP is competitive with the fastest existing pointbased algorithm on many standard test problems and faster by many times on some.",
Test Data Compression Based on Variable-to-Variable Huffman Encoding With Codeword Reusability,"A new statistical test data compression method that is suitable for IP cores of an unknown structure with multiple scan chains is proposed in this paper. Huffman, which is a well-known fixed-to-variable code, is used in this paper as a variable-to-variable code. The precomputed test set of a core is partitioned into variable-length blocks, which are, then, compressed by an efficient Huffman-based encoding procedure with a limited number of codewords. To increase the compression ratio, the same codeword can be reused for encoding compatible blocks of different sizes. Further compression improvements can be achieved by using two very simple test set transformations. A simple and low-overhead decompression architecture is also proposed.",
Effects of cooperation on the secrecy of multiple access channels with generalized feedback,"We investigate the effects of user cooperation on the secrecy of multiple access channels with generalized feedback (MAC-GF). We show that cooperation can increase the achievable secrecy region. We propose achievable schemes which use compress-and-forward (CAF) based transmission strategies. CAF based strategies allow users to increase their rates up to levels which are not decodable by the cooperating partners, consequently improving the secrecy of the users. We also provide outer bounds on the achievable equivocation rates. The outer bounds we derive depend only on the channel inputs and outputs, and hence, are easily computable. Finally, we specialize our results to a Gaussian MAC-GF, and present numerical results which demonstrate the beneficial effects of cooperation on secrecy.",
Improving the coverage of randomized scheduling in wireless sensor networks,"Randomized scheduling is a well-known scheduling algorithm for wireless sensor networks to prolong network lifetime. In this algorithm, sensor nodes are randomly assigned to multiple working subsets. These subsets alternatively perform the sensing tasks for monitoring a sensor filed. In each subset, only a portion of sensor nodes are active. It is difficult to maintain the full sensing coverage and network connectivity in a sensor field. For the network connectivity problem of the randomized scheduling, it has been discussed. In this paper, we propose a distributed approach for assisting the randomized scheduling to provide the full coverage. To efficiently achieve this goal, the proposed approach is divided into two main stages: field partition and coverage improvement. The handling problems of these two stages can be transferred to two geometry problems: Voronoi polygon construction and circle covering. Simulation results show that the proposed approach can significantly improve the coverage quality of the randomized scheduling.","Wireless sensor networks,
Monitoring,
Geometry,
Scheduling algorithm,
Medical services,
Event detection,
Councils,
Computer science,
Polynomials,
Sections"
Technique for automatic emotion recognition by body gesture analysis,"This paper illustrates our recent work on the analysis of expressive gesture related to the motion of the upper body (the head and the hands) in the context of emotional portrayals performed by professional actors. An experiment is presented which is the result of a multidisciplinary joint work. The experiment aims at (i) developing models and algorithms for analysis of such expressive content (ii) individuating which motion cues are involved in conveying the actor’s expressive intentions to portray four emotions (anger, joy, relief, sadness) via a scenario approach. The paper discusses the experiment in detail with reference to related conceptual issues, developed techniques, and the obtained results.","Emotion recognition,
Humans,
Motion analysis,
Algorithm design and analysis,
Performance analysis,
Psychology,
Information analysis,
Data mining,
Auditory displays,
Computer science"
Social Networking,"In the context of today's electronic media, social networking has come to mean individuals using the Internet and Web applications to communicate in previously impossible ways. This is largely the result of a culture-wide paradigm shift in the uses and possibilities of the Internet itself. The current Web is a much different entity than the Web of a decade ago. This new focus creates a riper breeding ground for social networking and collaboration. In an abstract sense, social networking is about everyone. The mass adoption of social-networking Websites points to an evolution in human social interaction.","Social network services,
IP networks,
Internet,
Collaboration,
Humans,
Facebook,
Educational institutions,
Explosions,
Web sites,
Context"
Scaling Laws for Overlaid Wireless Networks: A Cognitive Radio Network vs. a Primary Network,"We study the scaling laws for the throughputs of two coexisting wireless networks that operate in the same geographic region. The primary network consists of Poisson distributed legacy users of density n, and the secondary network consists of Poisson distributed cognitive users of density m, with m > n. The primary users have a higher priority to access the spectrum without particular considerations for the secondary users, while the secondary users have to act conservatively in order to limit interference to the primary users. With a practical assumption that the secondary users only know the locations of the primary transmitters, we show that both networks can achieve the same throughput scaling law as a stand-alone wireless network if proper transmission schemes are deployed, where a finite throughput is achievable for each individual secondary user (i.e., zero outage) with high probability.","Wireless networks,
Cognitive radio,
Throughput,
Wireless communication,
Interference,
Radio transmitters,
Spread spectrum communication,
FCC,
Laboratories,
Computer science education"
Efficient and accurate eye diagram prediction for high speed signaling,"This paper introduces an accumulative prediction method to predict the eye diagram for high speed signaling systems. We use the step responses of pull-up and pull-down to extract the worst-case eye diagram, including the eye height and jitter. Furthermore, the method produces the input patterns of the worst-case intersymbol interference. The algorithm handles signals of either symmetric or asymmetric rise/fall time. Experimental results demonstrate the accuracy and efficiency of the proposed method.",
Real-time 3D segmentation of the left ventricle using deformable subdivision surfaces,"In this paper, we extend a computationally efficient framework for real-time 3D tracking and segmentation to support deformable subdivision surfaces. Segmentation is performed in a sequential state-estimation fashion, using an extended Kalman filter to estimate shape and pose parameters for the subdivision surface. As an example, we have integrated Doo-Sabin subdivision surfaces into the framework. Furthermore, we provide a method for evaluating basis functions for Doo-Sabin surfaces at arbitrary parameter values. These basis functions are precomputed during initialization, and later used during segmentation to quickly evaluate surface points used for edge detection. Fully automatic tracking and segmentation of the left ventricle is demonstrated in a dataset of 21 3D echocardiography recordings. Successful segmentation was achieved in all cases, with limits of agreement (mean±1.96SD) for point to surface distance of 2.2±0.8 mm compared to manually verified segmentations. Real-time segmentation at a rate of 25 frames per second consumed a CPU load of 8%.",
Future delivery of health care: Cybercare,"Health-care system reforms can change the structure of the current U.S. health-care system, from centralized large hospitals to a distributed, networked healthcare system. In our model, medical care is delivered locally in neighborhoods and individual homes, using computer technologies like telemedicine, to link patients and primary care providers to tertiary medical providers. This decentralization could reduce costs enough to provide all citizens with medical insurance coverage; it would benefit patients and providers; and as a dual-use system, it would better protect the country's resources and citizens in an event of biological terror or natural disasters.",
A3: A Topology Construction Algorithm for Wireless Sensor Networks,"Topology control is a well-known strategy to save energy and extend the lifetime of wireless sensor networks. This paper introduces the A3 (a tree) algorithm, a simple, distributed, and energy-efficient topology construction mechanism that finds a sub-optimal Connected Dominating Set (CDS) to turn unnecessary nodes off while keeping the network connected and providing complete communication coverage. A3 utilizes a weighted distance-energy-based metric that permits the network operator to trade off the lengths of the branches (distance) for the robustness and durability of the tree (energy). Comparisons with other well-known topology construction mechanisms show the superiority of the proposed scheme in terms of the number of active nodes and energy efficiency. Simulation experiments show that to achieve complete communication coverage, A3 needs only 6% and 41% of the nodes active in dense and sparse scenarios, versus 8% and 43% and 5% and 43% of the EECDS and CDS- Rule-K algorithms, respectively. More importantly, the proposed protocol presents a low linearly bounded worst-case amount of messages per node that limits the overhead and the energy usage compared to a non-linear increase of the EECDS and CDS-Rule- K algorithms.",
A new operation method for grid-connected PV system considering voltage regulation in distribution system,"This paper presents a grid-connected photovoltaic (PV) system with the new operation method considering voltage regulation in distribution system. In proposed system, PV system adjusts real power and reactive power injection at connection point, in order to maintain the overall system voltage within acceptable range and improve power quality. It consists of modeling of PV array, inverter and inverter controller using PSCAD/EMTDC, and development of operation method of PV generation for voltage regulation in distribution system. PV array modeling is accomplished using 2-diode model which is generally used, and inverter with control system is implemented using proposed operation method based on instantaneous power theory and hysteresis current control. Proposed operation method consists of 3 stages control and using that operation method, overall system effectiveness is improved. To validate the effectiveness of the proposed system, it is compared with system with conventional operation under variable case such that connection, disconnection, load changes and voltage sag using PSCAD/EMTDC.","Voltage control,
Reactive power,
Inverters,
Control systems,
Voltage measurement,
Hysteresis,
Power generation"
PESIC: An Integrated Front-End for PET Applications,"An ASIC front-end has been developed for multianode photomultiplier based nuclear imaging devices. Its architecture has been designed to improve resolution and decrease pile-up probability in positron emission tomography systems which employ continuous scintillator crystals. Analog computation elements are isolated from the photomultiplier by means of a current sensitive preamplifier stage. This allows digitally programmable adjustment of every anode gain, also providing better resolution in gamma event position calculation and a shorter front-end deadtime. The preamplifier stage also offers the possibility of using other types of photomultiplier devices such as SiPM. The ASIC architecture includes measurement of the depth of interaction of the gamma event based on the width of the light distribution in order to reduce parallax error and increase spatial resolution during image reconstruction stage. An output stage of transresistance amplifiers offer voltage output signals which may be introduced in the A/D conversion stage with no further processing.","Positron emission tomography,
Photomultipliers,
Application specific integrated circuits,
Preamplifiers,
Nuclear imaging,
Computer architecture,
Crystals,
Analog computers,
Anodes,
Spatial resolution"
Trajectory analysis and semantic region modeling using a nonparametric Bayesian model,"We propose a novel nonparametric Bayesian model, Dual Hierarchical Dirichlet Processes (Dual-HDP), for trajectory analysis and semantic region modeling in surveillance settings, in an unsupervised way. In our approach, trajectories are treated as documents and observations of an object on a trajectory are treated as words in a document. Trajectories are clustered into different activities. Abnormal trajectories are detected as samples with low likelihoods. The semantic regions, which are intersections of paths commonly taken by objects, related to activities in the scene are also modeled. Dual-HDP advances the existing Hierarchical Dirichlet Processes (HDP) language model. HDP only clusters co-occurring words from documents into topics and automatically decides the number of topics. Dual-HDP co-clusters both words and documents. It learns both the numbers of word topics and document clusters from data. Under our problem settings, HDP only clusters observations of objects, while Dual-HDP clusters both observations and trajectories. Experiments are evaluated on two data sets, radar tracks collected from a maritime port and visual tracks collected from a parking lot.","Bayesian methods,
Radar tracking,
Layout,
Surveillance,
Videos,
Radar imaging,
Trajectory,
Signal analysis,
Artificial intelligence,
Laboratories"
Automatic pose estimation of 3D facial models,"Pose estimation plays an essential role in many computer vision applications, such as human computer interaction (HCI), driver attentiveness monitoring, face recognition, automatic face model editing, etc. In this paper, we propose a geometric feature based pose estimation approach based on 3D facial models. By identifying two clusters of inner eye corners of a 3D facial model, we find the nose tip with the aid of a facial reference plane. Then, a symmetry plane is generated and the pose orientation can be estimated. Our proposed algorithm is robust with respect to different persons, large pose variations, different expressions, partial facial data missing, and non-facial outliers. All computation is based on the pure 3D mesh model of a face without texture information. We tested our approach using 1,200 3D raw facial models and 1,200 corresponding clean facial models, and achieved 92.1% and 96.4% correct pose estimation rate, respectively.",
Massively parallel cosmological simulations with ChaNGa,"Cosmological simulators are an important component in the study of the formation of galaxies and large scale structures, and can help answer many important questions about the universe. Despite their utility, existing parallel simulators do not scale effectively on modern machines containing thousands of processors. In this paper we present ChaNGa, a recently released production simulator based on the CHARM++ infrastructure. To achieve scalable performance, ChaNGa employs various optimizations that maximize the overlap between computation and communication. We present experimental results of ChaNGa simulations on machines with thousands of processors, including the IBM Blue Gene/L and the Cray XT3. The paper goes on to highlight efforts toward even more efficient and scalable cosmological simulations. In particular, novel load balancing schemes that base decisions on certain characteristics of tree-based particle codes are discussed. Further, the multistepping capabilities of ChaNGa are presented, as are solutions to the load imbalance that such multiphase simulations face. We outline key requirements for an effective practical implementation and conclude by discussing preliminary results from simulations run with our multiphase load balancer.","Computational modeling,
Load management,
Computer simulation,
Computer science,
Astronomy,
Large-scale systems,
Production,
Dark energy,
Numerical simulation,
Planets"
Data gathering capacity of large scale multihop wireless networks,"This paper studies the scaling laws of the data gathering capacity of large scale multihop wireless networks. Unlike the data communication paradigms studied in previous research, for example, the many-to-many, many-to-one, broadcast, and multicast paradigms, the data gathering capacity concerns the per source node throughput in a network where a subset of nodes send data to some designated destinations while other nodes serve as relays. This some-to-some communication paradigm is commonplace in many wireless networks, for example, wireless mesh networks and wireless sensor networks, and in some cases perhaps more prevalent than the other paradigms. We first derive the upper and constructive lower bounds for the data gathering capacity, and then examine their design and performance implications. Our results show that the data gathering capacity is constrained by different factors in several different scaling regimes of the number of source and destination nodes, exhibiting distinct scaling laws in those regimes. This work fills a gap in our understanding of the capacity of various communication paradigms, and can lead to better network planning and performance for data gathering wireless network applications.","Upper bound,
Wireless networks,
Wireless sensor networks,
Wireless communication,
Relays,
Receivers,
Protocols"
Browsing within Lecture Videos Based on the Chain Index of Speech Transcription,"The number of digital lecture video recordings has increased dramatically since recording technology became easier to use. The accessibility and ability to search within this large archive are limited and difficult. Additionally, detailed browsing in videos is not supported due to the lack of an explicit annotation. Manual annotation and segmentation is time-consuming and therefore useless. A promising approach is based on using the audio layer of a lecture recording to obtain information about the lecture's contents. In this paper we're going to present an indexing method for computer science courses based on their existing recorded videos. The transcriptions from a speech-recognition engine (SRE) are sufficient to create a chain index for detailed browsing inside a lecture video. The index structure and the evaluation of the supplied keywords are presented. The user interface for dynamic browsing of the e-learning contents concludes this paper.",
Scalable Dynamic Load Balancing Using UPC,"An asynchronous work-stealing implementation of dynamic load balance is implemented using Unified Parallel C (UPC) and evaluated using the Unbalanced Tree Search(UTS) benchmark. The UTS benchmark presents a synthetic tree-structured search space that is highly imbalanced. Parallel implementation of the search requires continuous dynamic load balancing to keep all processors engaged in the search. Our implementation achieves better scaling and parallel efficiency in both shared memory and distributed memory settings than previous efforts using UPC and MPI. We observe parallel efficiency of 80% using 1024 processors performing over 85,000 total load balancing operations per second continuously. The UPC programming model provides substantial simplifications in the expression of the asynchronous work stealing protocol compared with MPI. However, to obtain performance portability with UPC in both shared memory and distributed memory settings requires the careful use of onesided reads and writes to minimize the impact of high latency communication. Additional protocol improvements are made to improve dissemination of available work and to decrease the cost of termination detection.","Program processors,
Search problems,
Load management,
Algorithm design and analysis,
Pediatrics,
Probes,
Benchmark testing"
Multihop Routing Protocol with Unequal Clustering for Wireless Sensor Networks,"In order to prolong the lifetime of wireless sensor networks, this paper presents a multihop routing protocol with unequal clustering (MRPUC). On the one hand, cluster heads deliver the data to the base station with relay to reduce energy consumption. On the other hand, MRPUC uses many measures to balance the energy of nodes. First, it selects the nodes with more residual energy as cluster heads, and clusters closer to the base station have smaller sizes to preserve some energy during intra-cluster communication for inter-cluster packets forwarding. Second, when regular nodes join clusters, they consider not only the distance to cluster heads but also the residual energy of cluster heads. Third, cluster heads choose those nodes as relay nodes, which have minimum energy consumption for forwarding and maximum residual energy to avoid dying earlier. Simulation results show that MRPUC performs much better than similar protocols.","Distance measurement,
Wireless sensor networks,
Energy consumption,
Head,
Protocols,
Sensors,
Routing"
Error-correcting codes in projective space,"The projective space of order n over the finite field Fq, denoted Pq(n), is the set of all subspaces of the vector space Fn q. The distance function d(U,V) = dim U + dim V - 2 dim(UcapV) turns Pq(n) into a metric space. With this, an (n, M, d) code C in projective space is a subset of Pq(n) of size M such that the distance between any two codewords (subspaces) is at least d. Koetter and Kschischang recently showed that codes in projective space are precisely what is needed for error-correction in networks: an (n, M, d) code can correct t packet errors and rho packet erasures introduced (adversarially) anywhere in the network as long as 2t + 2p < d. This motivates our interest in such codes. In this paper, we investigate certain basic aspects of ""coding theory in projective space"". First, we present several new bounds on the size of codes in Pq(n), which may be thought of as counterparts of the classical bounds in coding theory due to Johnson, Delsarte, and Gilbert-Varshamov. Some of these are stronger than all the previously known bounds, at least for certain code parameters. Next, we examine the fundamental concepts of ""linear codes"" and ""complements"" in the context of Pq(n). These turn out to be considerably more involved than their classical counterparts. In particular, we construct linear codes of size 2n and conjecture that larger linear codes do not exist. We also present several specific constructions of codes and code families in Pq(n). Finally, we prove that nontrivial perfect codes in Pq(n) do not exist.","Distance measurement,
Indium phosphide,
Binary codes,
Ear,
Extraterrestrial measurements,
Q measurement,
Linear code"
iPDA: An integrity-protecting private data aggregation scheme for wireless sensor networks,"Data aggregation is an efficient mechanism widely used in wireless sensor networks (WSN) to collect statistics about data of interests. However, the shared-medium nature of communication makes the WSNs are vulnerable to eavesdropping and packet tampering/injection by adversaries. Hence, how to protect data privacy and data integrity are two major challenges for data aggregation in wireless sensor networks. In this paper, we present iPDA- an integrity-protecting private data aggregation scheme. In iPDA, data privacy is achieved through data slicing and assembling technique; and data integrity is achieved through redundancy by constructing disjoint aggregation paths/trees to collect data of interests. In iPDA, the data integrity-protection and data privacy-preservation mechanisms work synergistically. We evaluate the performance of iPDA scheme in terms of communication overhead and data aggregation accuracy, comparing with a typical data aggregation scheme - TAG, where no integrity protection and privacy preservation is provided. Simulation results show that iPDA achieves the design goals while still maintains the efficiency of data aggregation.",
Efficient and effective grasping of novel objects through learning and adapting a knowledge base,"This paper introduces a new approach to establish a good grasp for a novel object quickly. A comprehensive knowledge base for grasping is learned that takes into account the geometrical and physical knowledge of grasping. To automate the learning process as much as possible, learning happens in a virtual environment. We used the GraspIt! simulation environment with the Barrett hand for this work. As only approximate features of objects are used for training the grasping knowledge base (GKB), the knowledge gained is rather robust to object uncertainty. Based on the guidance of the GKB, a suitable grasp for a novel object can be found quickly. The newly gained grasping information of the new object can also be feedback to the GKB so that the knowledge base continues to improve as it is exposed to more grasping cases. The GKB serves as the ldquoexperiencerdquo of the robotic gripper to make grasping more and more skillful. We implemented the approach and tested it on This paper introduces a new approach to establish a good grasp for a novel object quickly. A comprehensive knowledge base for grasping is learned that takes into account the geometrical and physical knowledge of grasping. To automate the learning process as much as possible, learning happens in a virtual environment. We used the GraspIt! [T. Miller and P.K. Allen, 2000] simulation environment with the Barrett hand for this work. As only approximate features of objects are used for training the grasping knowledge base (GKB), the knowledge gained is rather robust to object uncertainty. Based on the guidance of the GKB, a suitable grasp for a novel object can be found quickly. The newly gained grasping information of the new object can also be feedback to the GKB so that the knowledge base continues to improve as it is exposed to more grasping cases. The GKB serves as the ldquoexperiencerdquo of the robotic gripper to make grasping more and more skillful. We implemented the approach and tested it on a wide variety of objects. The results show the effectiveness of this approach to achieve quick and good grasps of novel objects.a wide variety of objects. The results show the effectiveness of this approach to achieve quick and good grasps of novel objects.","Grasping,
Training,
Shape,
Materials,
Knowledge based systems,
Fingers,
Glass"
Edge preserving spatially varying mixtures for image segmentation,"A new hierarchical Bayesian model is proposed for image segmentation based on Gaussian mixture models (GMM) with a prior enforcing spatial smoothness. According to this prior, the local differences of the contextual mixing proportions (i.e. the probabilities of class labels) are Student’s t-distributed. The generative properties of the Student’s t-pdf allow this prior to impose smoothness and simultaneously model the edges between the segments of the image. A maximum a posteriori (MAP) expectation-maximization (EM) based algorithm is used for Bayesian inference. An important feature of this algorithm is that all the parameters are automatically estimated from the data in closed form. Numerical experiments are presented that demonstrate the superiority of the proposed model for image segmentation as compared to standard GMM-based approaches and to GMM segmentation techniques with “standard” spatial smoothness constraints.",
EMI analysis methods for synchronous buck converter EMI root cause analysis,"DC/DC synchronous buck converters cause broadband emissions. A variety of methods are applied to analyze the root cause of the EMI. Time-domain voltage measurement and joint-time-frequency analysis allows to determine the location of the noise source. The near field scan reveals the current paths, and impedance measurement and 3D modeling can be used for further analysis of the noise source. A dual port TEM cell allows to distinguish E from H field coupling, This paper shows the application of those methods to a synchronous buck converter and reveals the sources of EMI leading to advice on the optimal PCB design. Finally, an innovative method of using a TEM cell measurement to predict the maximum possible radiated emissions is introduced.",
Vision based 3-D shape sensing of flexible manipulators,"Rigid robotic manipulators employ traditional sensors such as encoders or potentiometers to measure joint angles and determine end-effector position. Manipulators that are flexible, however, introduce motions that are much more difficult to measure. This is especially true for continuum manipulators that articulate by means of material compliance. In this paper, we present a vision based system for quantifying the 3-D shape of a flexible manipulator in real-time. The sensor system is validated for accuracy with known point measurements and for precision by estimating a known 3-D shape. We present two applications of the validated system relating to the open-loop control of a tendon driven continuum manipulator. In the first application, we present a new continuum manipulator model and use the sensor to quantify 3-D performance. In the second application, we use the shape sensor system for model parameter estimation in the absence of tendon tension information.",
Cache-Aware Real-Time Scheduling on Multicore Platforms: Heuristics and a Case Study,"Multicore architectures, which have multiple processing units on a single chip, have been adopted by most chip manufacturers. Most such chips contain on-chip caches that are shared by some or all of the cores on the chip. To effectively use the available processing resources on such platforms,scheduling methods must be aware of these caches. In this paper, we explore various heuristics that attempt to improve cache performance when scheduling real-time workloads. Such heuristics are applicable when multiple multithreaded applications exist with large working sets. In addition, we present a case study that shows how our best-performing heuristics can improve the end-user performance of video encoding applications.","processor scheduling,
microprocessor chips"
Requirements Prioritization Based on Benefit and Cost Prediction: An Agenda for Future Research,"In early phases of the software cycle, requirements prioritization necessarily relies on the specified requirements and on predictions of benefit and cost of individual requirements. This paper presents results of a systematic review of literature, which investigates how existing methods approach the problem of requirements prioritization based on benefit and cost. From this review, it derives a set of under-researched issues which warrant future efforts and sketches an agenda for future research in this area.","Strontium,
Computer science,
Software engineering,
Phase estimation,
Costs,
Mathematics,
Economic forecasting,
Proposals,
Embedded software"
Network Inference From Co-Occurrences,"The discovery of networks is a fundamental problem arising in numerous fields of science and technology, including communication systems, biology, sociology, and neuroscience. Unfortunately, it is often difficult, or impossible, to obtain data that directly reveal network structure, and so one must infer a network from incomplete data. This paper considers inferring network structure from ""co-occurrence"" data: observations that identify which network components (e.g., switches, routers, genes) carry each transmission but do not indicate the order in which they handle the transmission. Without order information, the number of networks that are consistent with the data grows exponentially with the size of the network (i.e., the number of nodes). Yet, the basic engineering/evolutionary principles underlying most networks strongly suggest that not all data-consistent networks are equally likely. In particular, nodes that co-occur in many observations are probably closely connected. With this in mind, we model the co-occurrence observations as independent realizations of a random walk on the network, subjected to a random permutation to account for the lack of order information. Treating permutations as missing data, we derive an expectation-maximization (EM) algorithm for estimating the random walk parameters. The model and EM algorithm significantly simplify the problem, but the computational complexity of the reconstruction process does grow exponentially in the length of each transmission path. For networks with long paths, the exact e-step may be computationally intractable. We propose a polynomial-time Monte Carlo EM algorithm based on importance sampling and derive conditions that ensure convergence of the algorithm with high probability. Simulations and experiments with Internet measurements demonstrate the promise of this approach.",
Identifying tractable decentralized control problems on the basis of information structure,"Sequential decomposition of two general models of decentralized systems with non-classical information structures is presented. In model A, all agents have two observations at each step: a common observation that all agents observe and a private observation of their own. The control actions of each agent is based on all past common observations, the current private observation and the contents of its memory. At each step, each agent also updates the contents of its memory. A cost function, which depends on the state of the plant and the control actions of all agents, is given. The objective is to choose control and memory update functions for all agents to either minimize a total expected cost over a finite horizon or to minimize a discounted cost over an infinite horizon. In model B, the agents do not have any common observation, the rest is same as in model A. The key idea of our solution methodology is the following. From the point of view of a fictitious agent that observes all common observations, the system can be viewed as a centralized system with partial observations. This allows us to identify information states and obtain a sequential decomposition. When the system variables take values in finite sets, the optimality equations of the sequential decomposition are similar to those of partially observable Markov decision processes (POMDP) with finite state and action spaces. For such systems, we can use algorithms for POMDPs to compute optimal designs for models A and B.","Distributed control,
Infinite horizon,
Cost function,
Sensor systems,
Surveillance,
Unmanned aerial vehicles,
Equations,
Telecommunication computing,
Algorithm design and analysis,
Automotive engineering"
Address Assignment and Routing Schemes for ZigBee-Based Long-Thin Wireless Sensor Networks,"Wireless sensor networks (WSNs) have been extensively researched recently. This paper makes two contributions to this field. First, we promote a new concept of long-thin (LT) topology for WSNs, where a network may have a number of linear paths of nodes as backbones connecting to each other. These backbones are to extend the network to the intended coverage areas. At the first glance, a LT WSN only seems to be a special case of numerous WSN topologies. However, we observe, from real deployment experiments, that such a topology is quite general in many applications and deployments. The second contribution is that we show that the address assignment and thus the tree routing scheme defined in the original ZigBee specification may work poorly, if not fail, in a LT topology. We thus propose simple, yet efficient, address assignment and routing schemes for a LT WSN. Simulation results and prototyping experiences are also reported.",
Casting out Demons: Sanitizing Training Data for Anomaly Sensors,"The efficacy of Anomaly Detection (AD) sensors depends heavily on the quality of the data used to train them. Artificial or contrived training data may not provide a realistic view of the deployment environment. Most realistic data sets are dirty; that is, they contain a number of attacks or anomalous events. The size of these high-quality training data sets makes manual removal or labeling of attack data infeasible. As a result, sensors trained on this data can miss attacks and their variations. We propose extending the training phase of AD sensors (in a manner agnostic to the underlying AD algorithm) to include a sanitization phase. This phase generates multiple models conditioned on small slices of the training data. We use these “micro-models” to produce provisional labels for each training input, and we combine the micro-models in a voting scheme to determine which parts of the training data may represent attacks. Our results suggest that this phase automatically and significantly improves the quality of unlabeled training data by making it as “attack-free” and “regular” as possible in the absence of absolute ground truth. We also show how a collaborative approach that combines models from different networks or domains can further refine the sanitization process to thwart targeted training or mimicry attacks against a single site.",
Mobility Aware Routing for the Airborne Network backbone,"The Airborne Network (AN) will form an essential part of the Global Information Grid in the future, thus providing information and decision superiority to US armed forces. AN is an enabling technology for Network Centric Warfare. AN is different from the terrestrial mobile ad-hoc networks (MANETs) and the wire-line internet, both in terms of network capability and underlying assumptions. The backbone nodes of the AN are envisioned to fly in pre-planned orbits whose knowledge can be exploited for efficient routing. In this paper we propose a dynamic adaptive routing protocol that uses known trajectories of the AN nodes to enhance performance. Our routing protocol has two components: (1) a Mobility Aware Routing Protocol (MARP), that routes traffic based on the knowledge of network topology with respect to time and makes preemptive decisions to minimize packet losses due to link failure and discover better routes, and (2) a Mobility Dissemination protocol (MDP) that informs all network nodes of any deviation from the preplanned behavior. We analyze MARP/MDP protocol suite using the QualNet network simulator for representative AN deployment scenarios and compare performance with proactive and reactive MANET routing protocols. We use packet delivery ratio, end-to-end latency and control overhead as performance metrics. We also analyze the performance of the MARP/MDP routing protocol for varying degrees of prediction accuracy. This work is part of an ongoing Phase II Small Business Innovation Research program administered by the Air Force Research Laboratory/Information Directorate in Rome, New York.",
On Supporting Kleene Closure over Event Streams,"Complex event patterns involving Kleene closure are finding application in a variety of stream environments for tracking and monitoring purposes. In this paper, we propose a compact language, SASE+, that can be used to define a wide variety of Kleene closure patterns, analyze the expressive power of the language, and outline an automata-based implementation for efficient Kleene closure evaluation over event streams.",
Degrees of freedom of wireless X networks,"We study the degrees of freedom characterization of wireless X networks, i.e. networks of M distributed single antenna transmitters and N distributed single antenna receivers where every transmitter has an independent message to every receiver. We provide an outerbound on the capacity region of X networks within o(log(SNR)). If the channel co-efficients are time-varying/frequency selective, we show that the total number of degrees of freedom is equal to MN/M+N−1 using a coding scheme based on the idea of interference alignment.",
Database Support for Probabilistic Attributes and Tuples,"The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.",
Joint multi-label multi-instance learning for image classification,"In real world, an image is usually associated with multiple labels which are characterized by different regions in the image. Thus image classification is naturally posed as both a multi-label learning and multi-instance learning problem. Different from existing research which has considered these two problems separately, we propose an integrated multi-label multi-instance learning (MLMIL) approach based on hidden conditional random fields (HCRFs), which simultaneously captures both the connections between semantic labels and regions, and the correlations among the labels in a single formulation. We apply this MLMIL framework to image classification and report superior performance compared to key existing approaches over the MSR Cambridge (MSRC) and Corel data sets.","Image classification,
Asia,
Automation,
Internet,
Digital photography,
Noise reduction"
Emotion Classification of Online News Articles from the Reader's Perspective,"Past studies on emotion classification focus on the writer’s emotional state. This research addresses the reader aspect instead. The classification of documents into reader-emotion categories has several applications. One of them is to integrate reader-emotion classification into a web search engine to allow users to retrieve documents that contain relevant contents and at the same time instill proper emotions. In this paper, we automatically classify documents into reader-emotion categories, and examine classification performance under different feature settings. Experiments show that certain feature combinations achieve good accuracy. We also compare the best classifier’s classification results with the emotional distributions of documents to determine how closely the classifier models the underlying reader behavior. Finally, we investigate the feasibility of emotion ranking.",
Quality of service assessment of opportunistic spectrum access: a medium access control approach,"Opportunistic spectrum access (OSA) is a promising new spectrum management approach that will allow coexistence of both licensed and opportunistic users in each spectrum band, potentially decreasing the spectrum licensing costs for both classes of users. However, this has significant implications on the QoS experienced by the licensed and opportunistic spectrum users. In this article we investigate how tolerant to secondary user activity a licensed user should be so as to provide dependable communication with sufficient QoS to an opportunistic user. We also look at key multichannel MAC features for such OSA networks proposed in the literature, and discuss how the design of control channel management affects the QoS of opportunistic users as a function of the tolerance of licensed users. We quantify the trade-off between dependability of the OSA network and the dependability of licensed users. The main conclusion is that opportunistic users can indeed achieve good QoS, as long as the licensed users are not highly active. For example, in one of the scenarios we studied, opportunistic users can achieve a delay below 100 ms if licensed user activity stays below 30 percent.","Quality of service,
Media Access Protocol,
Radio spectrum management,
Licenses,
Costs,
Delay,
Access protocols,
Ubiquitous computing,
Communication system control,
Throughput"
Self-Localization with RFID snapshots in densely tagged environments,"In this paper we show that, despite some disadvantageous properties of radio frequency identification (RFID), it is possible to localize a mobile robot quite accurately in environments which are densely tagged. We therefore employ a recently presented probabilistic fingerprinting technique called RFID snapshots. This method interprets short series of RFID measurements as feature vectors and is able to position a mobile robot after a training phase. It requires no explicit sensor model and is capable of exploiting given tag infrastructures, e.g., provided by supermarket shelves containing labeled products.",
Efficient visual hull computation for real-time 3D reconstruction using CUDA,"In this paper we present two efficient GPU-based visual hull computation algorithms. We compare them in terms of performance using image sets of varying size and different voxel resolutions. In addition, we present a real-time 3D reconstruction system which uses the proposed GPU-based reconstruction method to achieve real-time performance (30 fps) using 16 cameras and 4 PCs.",
Experimental analysis of RSSI-based location estimation in wireless sensor networks,"With a widespread increase in the number of mobile wireless systems and applications, the need for location aware services has risen at a very high pace in the last few years. Much research has been done for the development of new models for location aware systems, but most of it has primarily used the support of 802.11 wireless networks. Less work has been done towards an exhaustive error analysis of the underlying theories and models, especially in an indoor environment using a wireless sensor network. We present a thorough analysis of the Radio Signal Strength (RSS) model for distance estimation in wireless sensor networks through an empirical quantification of error metrics. Further on the basis of this experimental analysis, we implement a k - nearest signal space neighbor match algorithm for location estimation, and evaluate some crucial control parameters using which this technique can be adapted to different cases and scenarios, to achieve finer and more precise location estimates.",
DySy,"Dynamically discovering likely program invariants from concrete test executions has emerged as a highly promising software engineering technique. Dynamic invariant inference has the advantage of succinctly summarizing both ""expected"" program inputs and the subset of program behaviors that is normal under those inputs. In this paper, we introduce a technique that can drastically increase the relevance of inferred invariants, or reduce the size of the test suite required to obtain good invariants. Instead of falsifying invariants produced by pre-set patterns, we determine likely program invariants by combining the concrete execution of actual test cases with a simultaneous symbolic execution of the same tests. The symbolic execution produces abstract conditions over program variables that the concrete tests satisfy during their execution. In this way, we obtain the benefits of dynamic inference tools like Daikon: the inferred invariants correspond to the observed program behaviors. At the same time, however, our inferred invariants are much more suited to the program at hand than Daikon's hard-coded invariant patterns. The symbolic invariants are literally derived from the program text itself, with appropriate value substitutions as dictated by symbolic execution. We implemented our technique in the DySy tool, which utilizes a powerful symbolic execution and simplification engine. The results confirm the benefits of our approach. In Daikon's prime example benchmark, we infer the majority of the interesting Daikon invariants, while eliminating invariants that a human user is likely to consider irrelevant.",
3D model matching with Viewpoint-Invariant Patches (VIP),The robust alignment of images and scenes seen from widely different viewpoints is an important challenge for camera and scene reconstruction. This paper introduces a novel class of viewpoint independent local features for robust registration and novel algorithms to use the rich information of the new features for 3D scene alignment and large scale scene reconstruction. The key point of our approach consists of leveraging local shape information for the extraction of an invariant feature descriptor. The advantages of the novel viewpoint invariant patch (VIP) are: that the novel features are invariant to 3D camera motion and that a single VIP correspondence uniquely defines the 3D similarity transformation between two scenes. In the paper we demonstrate how to use the properties of the VIPs in an efficient matching scheme for 3D scene alignment. The algorithm is based on a hierarchical matching method which tests the components of the similarity transformation sequentially to allow efficient matching and 3D scene alignment. We evaluate the novel features on real data with known ground truth information and show that the features can be used to reconstruct large scale urban scenes.,"Layout,
Image reconstruction,
Robustness,
Cameras,
Large-scale systems,
Geometry,
Computer science,
Shape,
Data mining,
Sequential analysis"
Clustering Uncertain Data Using Voronoi Diagrams,"We study the problem of clustering uncertain objects whose locations are described by probability density functions (pdf). We show that the UK-means algorithm, which generalises the k-means algorithm to handle uncertain objects, is very inefficient. The inefficiency comes from the fact that UK-means computes expected distances (ED) between objects and cluster representatives. For arbitrary pdf's, expected distances are computed by numerical integrations, which are costly operations. We propose pruning techniques that are based on Voronoi diagrams to reduce the number of expected distance calculation. These techniques are analytically proven to be more effective than the basic bounding-box-based technique previous known in the literature. We conduct experiments to evaluate the effectiveness of our pruning techniques and to show that our techniques significantly outperform previous methods.",
The Elliptical Cone of Uncertainty and Its Normalized Measures in Diffusion Tensor Imaging,"Diffusion tensor magnetic resonance imaging (DT-MRI) is capable of providing quantitative insights into tissue microstructure in the brain. An important piece of information offered by DT-MRI is the directional preference of diffusing water molecules within a voxel. Building upon this local directional information, DT-MRI tractography attempts to construct global connectivity of white matter tracts. The interplay between local directional information and global structural information is crucial in understanding changes in tissue microstructure as well as in white matter tracts. To this end, the right circular cone of uncertainty was proposed by Basser as a local measure of tract dispersion. Recent experimental observations by Jeong et al. and Lazar et al. that the cones of uncertainty in the brain are mostly elliptical motivate the present study to investigate analytical approaches to quantify their findings. Two analytical approaches for constructing the elliptical cone of uncertainty, based on the first-order matrix perturbation and the error propagation method via diffusion tensor representations, are presented and their theoretical equivalence is established. We propose two normalized measures, circumferential and areal, to quantify the uncertainty of the major eigenvector of the diffusion tensor. We also describe a new technique of visualizing the cone of uncertainty in 3-D.","Diffusion tensor imaging,
Tensile stress,
Dispersion,
Humans,
Magnetic resonance imaging,
Pediatrics,
Microstructure,
Area measurement,
Shape measurement,
Buildings"
Efficient and Scalable Consistency Maintenance for Heterogeneous Peer-to-Peer Systems,"Consistency maintenance mechanism is necessary for the emerging peer-to-peer applications due to their frequent data updates. Centralized approaches suffer single point of failure, while previous decentralized approaches incur too many duplicate update messages because of locality-ignorant structures. To address this issue, we propose a scalable and efficient consistency maintenance scheme for heterogeneous P2P systems. Our scheme takes the heterogeneity nature into account and forms the replica nodes of a key into a locality-aware hierarchical structure, in which the upper layer is DHT-based and consists of powerful and stable replica nodes, while a replica node at the lower layer attaches to a physically close upper layer node. A d-ary update message propagation tree (UMPT) is dynamically built upon the upper layer for propagating the updated contents. As a result, the tree structure does not need to be maintained all the time, saving a lot of cost. Through theoretical analyses and comprehensive simulations, we examine the efficiency and scalability of this design. The results show that, compared with previous designs, especially locality-ignorant ones, our approach is able to reduce the cost by about 25-67 percent.",
On the Predictability of Random Tests for Object-Oriented Software,"Intuition suggests that random testing of object-oriented programs should exhibit a significant difference in the number of faults detected by two different runs of equal duration. As a consequence, random testing would be rather unpredictable. We evaluate the variance of the number of faults detected by random testing over time. We present the results of an empirical study that is based on 1215 hours of randomly testing 27 Eiffel classes, each with 30 seeds ofthe random number generator. Analyzing over 6 million failures triggered during the experiments, the study provides evidence that the relative number of faults detected by random testing over time is predictable but that different runs of the random test case generator detect different faults. The study also shows that random testing quickly finds faults: the first failure is likely to be triggered within 30 seconds.","Software testing,
Fault detection,
Random number generation,
Contracts,
Computer science,
Failure analysis,
Electronic equipment testing,
Concrete,
Performance evaluation,
Resource management"
Design and development of a multi-die embedded micro wafer level package,"The primary trend in electronics industry is product miniaturization. Both design and manufacturing engineers are looking for ways to make products lighter, smaller, less expensive, and at the same time faster, more powerful, reliable, user-friendly, and functional. A partial list of today’s “shrinking” products would include cellular phones, personal and sub-notebook computers, pagers, PCMCIA cards, camcorders, palmtop organizers, telecommunications equipment, and automotive components. With silicon chips continue integrating more functionality as per Moore’s Law, the packaging is challenged to integrate and shrink. Chips First or Embedded Chip packaging is a revolutionary way to overcome these recent packaging integration challenges. Packaging researchers have worked on embedded packaging and developed newer way of embedding the chip. The PBGA replaced the lead frame based peripheral array packages, in which the die is electrically connected to circuit board (PCB) substrate by wire bonding or flip chip technology, before covering with molding compound. Embedded Wafer level packaging takes the next step, eliminating the PCB, as well as the need to use wire bonding or flip-chip bumps to establish electrical connection. This paper deals with the development embedding multiple dies at wafer level. A detailed mechanical and structural analysis of the package in terms of the die thickness, wafer size and warpage is presented. The package format is suitable for stacking multiple die in 3D format and 2D format. The paper also deals with characterization of the materials and the process integration of the multidie wafer level packaging. Initial reliability results of the package are also presented.","Wafer scale integration,
Packaging machines,
Personal digital assistants,
Wire,
Wafer bonding,
Electronics packaging,
Electronics industry,
Electronic equipment manufacture,
Automotive engineering,
Reliability engineering"
Fast Indexes and Algorithms for Set Similarity Selection Queries,"Data collections often have inconsistencies that arise due to a variety of reasons, and it is desirable to be able to identify and resolve them efficiently. Set similarity queries are commonly used in data cleaning for matching similar data. In this work we concentrate on set similarity selection queries: Given a query set, retrieve all sets in a collection with similarity greater than some threshold. Various set similarity measures have been proposed in the past for data cleaning purposes. In this work we concentrate on weighted similarity functions like TF/IDF, and introduce variants that are well suited for set similarity selections in a relational database context. These variants have special semantic properties that can be exploited to design very efficient index structures and algorithms for answering queries efficiently. We present modifications of existing technologies to work for set similarity selection queries. We also introduce three novel algorithms based on the Threshold Algorithm, that exploit the semantic properties of the new similarity measures to achieve the best performance in theory and practice.","Algorithm design and analysis,
Relational databases,
Cleaning,
Weight measurement,
Space technology,
Costs,
Computer science,
Indexes,
Performance evaluation,
Information retrieval"
On the semantics and evaluation of top-k queries in probabilistic databases,"We formulate three intuitive semantic properties for top-k queries in probabilistic databases, and propose Global-Topk query semantics which satisfies all of them. We provide a dynamic programming algorithm to evaluate top-k queries under Global-Topk semantics in simple probabilistic relations. For general probabilistic relations, we show a polynomial reduction to the simple case. Our analysis shows that the complexity of query evaluation is linear in k and at most quadratic in database size.",
A Hybrid Approach to QoS-Aware Service Composition,"QoS-aware service composition intends to maximize the QoS of a composite service when selecting service providers. This paper proposes a service composition scheme that uses a combination of Integer Programming, case-based reasoning, and, genetic algorithms techniques. The scheme reduces the service composition costs by reusing existing compositions. Experiments show that, compared with solutions purely based on Integer Programming, the proposed scheme is effective in reducing the time for carrying out service composition.",
Confidentiality Protection for Distributed Sensor Data Aggregation,"Efficiency and security are two basic requirements for sensor network design. However, these requirements could be sharply contrary to each other in some scenarios. For example, in- network data aggregation can significantly reduce communication overhead and thus has been adopted widely as a means to improve network efficiency; however, the adoption of in-network data aggregation may prevent data from being encrypted since it is a prerequisite for aggregation that data be accessible during forwarding. In this paper, we address this dilemma by proposing a family of secret perturbation-based schemes that can protect sensor data confidentiality without disrupting additive data aggregation. Extensive simulations are also conducted to evaluate the proposed schemes. The results show that our schemes provide confidentiality protection for both raw and aggregated data items with an overhead lower than that of existing related schemes.","Protection,
Intelligent sensors,
Wireless sensor networks,
Sensor phenomena and characterization,
Data security,
Cryptography,
Temperature sensors,
Communications Society,
Computer science,
Computer security"
Designing a 3-D FPGA: Switch Box Architecture and Thermal Issues,"Three-dimensional (3-D) integration is an attractive technology to reduce wirelengths in a field-programmable gate array (FPGA). However, it suffers from two problems: one, the inter-layer vias are limited in number, and second, the increased power density leads to high junction temperatures. In this paper, we tackle the first problem by designing switch boxes that maximize the use of the vias. Compared to the previously used subset switch box, our best switch box reduces the number of vias by about 49% and area-delay product by about 9%. For the second problem, we utilize the difference in power densities between CLBs and some of the hard blocks in modern FPGAs to distribute the power more uniformly across the FPGA. The peak temperature in a two-layer FPGA reduces by about 16degC after our change.","Switches,
Field programmable gate arrays,
Temperature,
Stacking,
Thermal management,
Computer science,
Fabrication,
Delay,
Silicon,
Logic"
Behavioral Inference across Cultures: Using Telephones as a Cultural Lens,"Most people carry mobile telephones, which automatically capture behavioral data and store it in service provider databases around the world. The different types of captured data can provide insight into human cultures. Examples from various cultures and hundreds of millions of individuals illustrate how phones can serve as a cultural lens, improving our understanding of social networks, outlier events, and a culture's pace of life.","Telephony,
Cultural differences,
Lenses,
Cities and towns,
Humans,
Poles and towers,
Social network services,
Nominations and elections,
Radio access networks,
Data privacy"
Boosting adaptive linear weak classifiers for online learning and tracking,"Online boosting methods have recently been used successfully for tracking, background subtraction etc. Conventional online boosting algorithms emphasize on interchanging new weak classifiers/features to adapt with the change over time. We are proposing a new online boosting algorithm where the form of the weak classifiers themselves are modified to cope with scene changes. Instead of replacement, the parameters of the weak classifiers are altered in accordance with the new data subset presented to the online boosting process at each time step. Thus we may avoid altogether the issue of how many weak classifiers to be replaced to capture the change in the data or which efficient search algorithm to use for a fast retrieval of weak classifiers. A computationally efficient method has been used in this paper for the adaptation of linear weak classifiers. The proposed algorithm has been implemented to be used both as an online learning and a tracking method. We show quantitative and qualitative results on both UCI datasets and several video sequences to demonstrate improved performance of our algorithm.","Boosting,
Target tracking,
Machine learning algorithms,
Layout,
Computer science,
Approximation algorithms,
Filtering,
Least squares methods,
Information retrieval,
Video sequences"
Obstacle-Avoiding Rectilinear Steiner Tree Construction Based on Spanning Graphs,"Given a set of pins and a set of obstacles on a plane, an obstacle-avoiding rectilinear Steiner minimal tree (OARSMT) connects these pins, possibly through some additional points (called the Steiner points), and avoids running through any obstacle to construct a tree with a minimal total wirelength. The OARSMT problem becomes more important than ever for modern nanometer IC designs which need to consider numerous routing obstacles incurred from power networks, prerouted nets, IP blocks, feature patterns for manufacturability improvement, antenna jumpers for reliability enhancement, etc. Consequently, the OARSMT problem has received dramatically increasing attention recently. Nevertheless, considering obstacles significantly increases the problem complexity, and thus, most previous works suffer from either poor quality or expensive running time. Based on the obstacle-avoiding spanning graph, this paper presents an efficient algorithm with some theoretical optimality guarantees for the OARSMT construction. Unlike previous heuristics, our algorithm guarantees to find an optimal OARSMT for any two-pin net and many higher pin nets. Extensive experiments show that our algorithm results in significantly shorter wirelengths than all state-of-the-art works.","Routing,
Pins,
Steiner trees,
Manufacturing,
Tree graphs,
Heuristic algorithms,
Large-scale systems,
Computer science,
NP-complete problem,
Timing"
Reachability calculations for automated aerial refueling,This paper describes Hamilton-Jacobi (HJ) reachability calculations for a hybrid systems formalism governing unmanned aerial vehicles (UAVs) interacting with another vehicle in a safety-critical situation. We use this problem to lay the foundations toward the goal of refining or designing protocols for multi-UAV and/or manned vehicle interaction. We describe here what mathematical foundations are necessary to formulate verification problems on reachability and safety of flight maneuvers. We finally show how this formalism can be used in the chosen application to inform UAV decisions on avoiding unsafe scenarios while achieving mission objectives.,
Using a Game Engine for VR Simulations in Evacuation Planning,"Researchers who want to use virtual reality-based simulations in their work must address some important requirements. Simulations require a good 3D graphical rendering capability. For realistic results, they must also consider the effects of physical laws, such as gravity and collision forces. A software platform that meets these requirements can serve a broad range of science and technology applications, but developing an entire platform is hard work itself. In this article, we present our use of a game engine for virtual simulations of building evacuations in emergency situations. We have modeled a real IEN building in 3D to perform preliminary evacuation tests, prior to real ones, and thus support evacuation planning. Nuclear plants represent just one of many environments where virtual simulations might be the best or only means of evaluating situations that are too dangerous to simulate in real environments-for example, in the presence of fire and smoke or radioactive or chemical contamination.",
Fuzzifying Allen's Temporal Interval Relations,"When the time span of an event is imprecise, it can be represented by a fuzzy set, called a fuzzy time interval. In this paper, we propose a framework to represent, compute, and reason about temporal relationships between such events. Since our model is based on fuzzy orderings of time points, it is not only suitable to express precise relationships between imprecise events (ldquoRoosevelt died before the beginning of the Cold Warrdquo) but also imprecise relationships (ldquoRoosevelt died just before the beginning of the Cold Warrdquo). We show that, unlike previous models, our model is a generalization that preserves many of the properties of the 13 relations Allen introduced for crisp time intervals. Furthermore, we show how our model can be used for efficient fuzzy temporal reasoning by means of a transitivity table. Finally, we illustrate its use in the context of question answering systems.","Fuzzy sets,
Fuzzy reasoning,
Algebra,
Natural languages,
Mathematics,
Computer science,
Intelligent systems,
Data mining,
Information analysis,
Time measurement"
Maximum Confidence Hidden Markov Modeling for Face Recognition,"This paper presents a hybrid framework of feature extraction and hidden Markov modeling (HMM) for two-dimensional pattern recognition. Importantly, we explore a new discriminative training criterion to assure model compactness and discriminability. This criterion is derived from the hypothesis test theory via maximizing the confidence of accepting the hypothesis that observations are from target HMM states rather than competing HMM states. Accordingly, we develop the maximum confidence hidden Markov modeling (MC-HMM) for face recognition. Under this framework, we merge a transformation matrix to extract discriminative facial features. The closed-form solutions to continuous-density HMM parameters are formulated. Attractively, the hybrid MC-HMM parameters are estimated under the same criterion and converged through the expectation-maximization procedure. From the experiments on the FERET database and GTFD, we find that the proposed method obtains robust segmentation in the presence of different facial expressions, orientations, and so forth. In comparison with the maximum likelihood and minimum classification error HMMs, the proposed MC-HMM achieves higher recognition accuracies with lower feature dimensions.",
Robust Linearized Image Reconstruction for Multifrequency EIT of the Breast,"Electrical impedance tomography (EIT) is a developing imaging modality that is beginning to show promise for detecting and characterizing tumors in the breast. At Rensselaer Polytechnic Institute, we have developed a combined EIT-tomosynthesis system that allows for the coregistered and simultaneous analysis of the breast using EIT and X-ray imaging. A significant challenge in EIT is the design of computationally efficient image reconstruction algorithms which are robust to various forms of model mismatch. Specifically, we have implemented a scaling procedure that is robust to the presence of a thin highly-resistive layer of skin at the boundary of the breast and we have developed an algorithm to detect and exclude from the image reconstruction electrodes that are in poor contact with the breast. In our initial clinical studies, it has been difficult to ensure that all electrodes make adequate contact with the breast, and thus procedures for the use of data sets containing poorly contacting electrodes are particularly important. We also present a novel, efficient method to compute the Jacobian matrix for our linearized image reconstruction algorithm by reducing the computation of the sensitivity for each voxel to a quadratic form. Initial clinical results are presented, showing the potential of our algorithms to detect and localize breast tumors.",
Variable Latency Speculative Addition: A New Paradigm for Arithmetic Circuit Design,"Adders are one of the key components in arithmetic circuits. Enhancing their performance can significantly improve the quality of arithmetic designs. This is the reason why the theoretical lower bounds on the delay and area of an adder have been analysed, and circuits with performance close to these bounds have been designed. In this paper, we present a novel adder design that is exponentially faster than traditional adders; however, it produces incorrect results, deterministically, for a very small fraction of input combinations. We have also constructed a reliable version of this adder that can detect and correct mistakes when they occur. This creates the possibility of a variable-latency adder that produces a correct result very fast with extremely high probability; however, in some rare cases when an error is detected, the correction term must be applied and the correct result is produced after some time. Since errors occur with extremely low probability, this new type of adder is significantly faster than state-of-the-art adders when the overall latency is averaged over many additions.","Circuit synthesis,
Adders,
Cryptography,
Frequency,
Error correction,
Digital arithmetic,
Added delay,
Circuit analysis,
Performance analysis,
Natural languages"
Application of system models in regression test suite prioritization,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.","Fault detection,
Testing,
Heuristic algorithms,
Modeling,
Life estimation,
Software systems,
USA Councils"
Usability challenges for enterprise service-oriented architecture APIs,"An important part of many programming tasks is the use of libraries and other forms of Application Programming Interfaces (APIs). Programming via web services using a Service-Oriented Architecture (SOA) is an emerging form of API usage. Web services in a business context (called enterprise SOA or E-SOA) add additional complexity in terms of the number of the services, the variety of internal data structures, and service interdependencies. After altering existing Human-Computer Interaction (HC[) methodologies to address the unique context of software development for SOA, we evaluated a large E-SOA API and identified many usability challenges. Prominent results include difficulties developers encountered while assembling data structures in web service parameters, cycles of errors due to unclear control parameters within data structures, and difficulties with understanding long identifier names. We recommend a tolerance for unspecified objects in automatically-generated web service proxy code, consistent data structures in parame ters across services, and encoding optional namespace schemes into WSDL files.",Visualization
The Z-properties chart,"Dynamic friction modeling has received considerable attention, and diverse models have been put forward. Some of the most widely applied models are state-variable models, which incorporate one or more internal states governed by nonlinear differential equations. As empirically motivated nonlinear differential equations, state-variable friction models exhibit a surprising variety of behavior. Most of these models are dissipative for motions on all scales, while others are nondissipative for very small motions; some show true sliding for very small applied forces, while others show purely elastic displacement under the same conditions; and some show hysteresis with nonlocal memory, while others do not. In this article the Z-properties chart is introduced. This chart provides a means for comparing and contrasting the properties of the state-variable friction models under the condition referred to as presliding, where the dynamics of the state variables play their greatest role. The friction state is often designated z(t) and thus the name ""Z-properties chart"".",
Paired Learners for Concept Drift,"To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas are active learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the stable learner performs worse than does there active learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the stable learner to predict, since the stable learner performs better than does the reactive learner when acquiring  target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dynamic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm (SEA) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: meeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.","Algorithm design and analysis,
Stability,
Data mining,
Computer science,
USA Councils,
Scheduling algorithm,
Dynamic scheduling,
Heuristic algorithms"
A Combination of Rough-Based Feature Selection and RBF Neural Network for Classification Using Gene Expression Data,"This paper presents a novel rough-based feature selection method for gene expression data analysis. It can find the relevant features without requiring the number of clusters to be known a priori and identify the centers that approximate to the correct ones. In this paper, we attempt to introduce a prediction scheme that combines the rough-based feature selection method with radial basis function neural network. For further consider the effect of different feature selection methods and classifiers on this prediction process, we use the Naive Bayes and linear support vector machine as classifiers, and compare the performance with other feature selection methods, including information gain and principle component analysis. We demonstrate the performance by several published datasets and the results show that our proposed method can achieve high classification accuracy rate.",
DICOM Image Communication in Globus-Based Medical Grids,"Grid computing, the collaboration of distributed resources across institutional borders, is an emerging technology to meet the rising demand on computing power and storage capacity in fields such as high-energy physics, climate modeling, or more recently, life sciences. A secure, reliable, and highly efficient data transport plays an integral role in such grid environments and even more so in medical grids. Unfortunately, many grid middleware distributions, such as the well-known Globus Toolkit, lack the integration of the world-wide medical image communication standard Digital Imaging and Communication in Medicine (DICOM). Currently, the DICOM protocol first needs to be converted to the file transfer protocol (FTP) that is offered by the grid middleware. This effectively reduces most of the advantages and security an integrated network of DICOM devices offers. In this paper, a solution is proposed that adapts the DICOM protocol to the Globus grid security infrastructure and utilizes routers to transparently route traffic to and from DICOM systems. Thus, all legacy DICOM devices can be seamlessly integrated into the grid without modifications. A prototype of the grid routers with the most important DICOM functionality has been developed and successfully tested in the MediGRID test bed, the German grid project for life sciences.","DICOM,
Image communication,
Biomedical imaging,
Protocols,
Grid computing,
Middleware,
Life testing,
Collaboration,
Distributed computing,
Physics computing"
Hardware Runtime Monitoring for Dependable COTS-Based Real-Time Embedded Systems,"COTS peripherals are heavily used in the embedded market, but their unpredictability is a threat for high-criticality real-time systems: it is hard or impossible to formally verify COTS components. Instead, we propose to monitor the runtime behavior of COTS peripherals against their assumed specifications. If violations are detected, then an appropriate recovery measure can be taken. Ourmonitoring solution is decentralized: a monitoring device is plugged in on a peripheral bus and monitors the peripheral behavior by examining read and write transactions on the bus. Provably correct (w.r.t. given specifications) hardware monitors are synthesized from high level specifications, and executed on FPGAs, resultingin zero runtime overhead on the system CPU. The proposed technique, called BusMOP, has been implemented as an instance of a generic runtime verification framework, called MOP, which until now has only been used for software monitoring.  We experimented with our techniqueusing a COTS data acquisition board.","Hardware,
Runtime,
Real time systems,
Embedded system,
Computerized monitoring,
Aerospace electronics,
Computer science,
Field programmable gate arrays,
Data acquisition,
Systems engineering and theory"
An Asynchronous Neighbor Discovery Algorithm for Cognitive Radio Networks,"Cognitive radio (CR) technology is a promising emerging technology that enables a more efficient usage of already assigned spectrum. However, before CR networks can be deployed, new algorithms and protocols need to be developed. When forming an ad-hoc cognitive radio network (CRN), one of the fundamental tasks is to determine the neighbors of each node and channels that can be used to communicate among neighbors. The nature of the CRN makes this a challenging problem. Specifically in CRN, not only the network is multi-channel, but the channels available at different nodes may be different. Furthermore no time synchronization may be available to nodes. In this paper, we propose an asynchronous distributed algorithm that allows nodes to discover their neighbors and channels that can be used to communicate with them in a single-hop CRN.","Lead,
Chromium,
Synchronization,
Cognitive radio,
Nominations and elections,
Upper bound,
Algorithm design and analysis"
Standards-Based Computing Capabilities for Distributed Geospatial Applications,"Researchers face increasingly large repositories of geospatial data stored in different locations and in various formats. To address this problem, the Open Geospatial Consortium and the Open Grid Forum are collaborating to develop standards for distributed geospatial computing.","Distributed computing,
Standards development,
Grid computing,
Best practices,
Collaboration,
Resource management,
Software standards,
Software tools,
Pervasive computing,
Internet"
Coscheduling of CPU and I/O Transactions in COTS-Based Embedded Systems,"Integrating COTS components in critical real-time systems ischallenging. In particular, we show that the interference between cache activity and I/O traffic generated by COTS peripherals can unpredictably slow down a real-time task by up to 44%. To solve this issue, we propose a framework comprised of three main components: 1) a COTS-compatible device, the peripheral gate, that controls peripheral access to the system; 2) an analytical technique that computes safe bounds on the I/O-induced task delay; 3) a coscheduling algorithm that maximizes the amount of allowed peripheral traffic while guaranteeing all real-time task constraints. We implemented the complete framework on a COTS-based system using PCI peripherals, and we performed extensive experiments to show its feasibility.","Embedded system,
Real time systems,
Central Processing Unit,
Control systems,
Computer peripherals,
Aerospace electronics,
Costs,
Computer science,
Interference,
Algorithm design and analysis"
Time-Dependent Network Pricing and Bandwidth Trading,"The usage of a network usually differs significantly at different times of a day, due to users' time-preference. This phenomenon is also prominent in the market of ""bandwidth- on-demand"", since the demand is typically higher during large events. Thus, an unselfish ""social planner"" should deploy a proper pricing scheme to reduce congestions and achieve efficient use of the network (i.e., maximize the ""social welfare""); whereas a selfish service provider (SP) can exploit the time-preference to increase its revenue. In this paper, we present a model to study the important role of time-preference in network pricing. In this model, each user chooses his access time based on his preference, the congestion level, and the price he would be charged. Without pricing, the ""price of anarchy"" (POA) can be arbitrarily bad. We then derive a simple pricing scheme to maximize the social welfare. Next, from the SP's viewpoint, we consider the revenue- maximizing pricing strategy and its effect on the social welfare. We show that if the SP can differentiate its prices over different users and times, the maximal revenue can be achieved, as well as the maximal social welfare. However, if the SP has insufficient information about the users and can only differentiate its prices over the access times, then the resulting social welfare can be much less than the optimum, especially when there are many low-utility users. Otherwise, the difference is bounded and less significant.","Pricing,
Bandwidth,
Traffic control,
Routing,
Telecommunication traffic,
Cost function,
Delay,
Computer science,
Communication networks,
Cultural differences"
The LilyPad Arduino: Toward Wearable Engineering for Everyone,"Electronic textiles, or e-textiles, are an increasingly important part of wearable computing, helping to make pervasive devices truly wearable. These soft, fabric-based computers can function as lovely embodiments of Mark Weiser's vision of ubiquitous computing: providing useful functionality while disappearing discreetly into the fabric of our clothing. E-textiles also give new, expressive materials to fashion designers, textile designers, and artists, and garments stemming from these disciplines usually employ technology in visible and dramatic style. Integrating computer science, electrical engineering, textile design, and fashion design, e-textiles cross unusual boundaries, appeal to a broad spectrum of people, and provide novel opportunities for creative experimentation both in engineering and design. Moreover, e-textiles are cutting- edge technologies that capture people's imagination in unusual ways. (What other emerging pervasive technology has Vogue magazine featured?) Our work aims to capitalize on these unique features by providing a toolkit that empowers novices to design, engineer, and build their own e-textiles.","Textile technology,
Clothing,
Design engineering,
Wearable computers,
Computer vision,
Pervasive computing,
Ubiquitous computing,
Fabrics,
Computer science,
Electrical engineering"
Beam-Scanning Performance of Leaky-Wave Slot-Array Antenna on Variable Stub-Loaded Left-Handed Waveguide,"Metamaterial is an artificial material in which the effective phase constant can be controlled by the internal physical periodic structure of the transmission line. Effective phase constant is designed to be of any value, which may be small, large, approximately zero, or negative, attained by changing the dimensions of the periodic structure. Miniaturization of the antenna and wide design flexibility of radiation patterns can be expected from the use of metamaterials. In this work, we developed a leaky-wave slot-array antenna on a variable stub-loaded left-handed waveguide. The radiation pattern of the fabricated antenna was measured and the characteristics of the left-handed transmission line were observed. Moreover, performance of beam-scanning by changing the length of the stubs on the waveguide was confirmed experimentally.","Slot antennas,
Metamaterials,
Periodic structures,
Transmission line antennas,
Dielectric materials,
Antenna radiation patterns,
Permittivity,
Computer science,
Transmission lines,
Distributed parameter circuits"
Formal Verification of Firewall Policies,"Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. The quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration). Due to the lack of tools for verifying firewall policies, most firewalls on the Internet have been plagued with policy errors. A firewall policy error either creates security holes that will allow malicious traffic to sneak into a private network or blocks legitimate traffic and disrupts normal business processes, which in turn could lead to irreparable, if not tragic, consequences. We propose a firewall verification tool in this paper. Our tool takes as input a firewall policy and a given property, then outputs whether the policy satisfies the property. Despite of the importance of verifying firewall policies, this problem has not been explored in previous work. Due to the complex nature of firewall policies, designing algorithms for such a verification tool is challenging. In this paper, we designed and implemented a verification algorithm using decision diagrams, and tested it on both real-life firewall policies and synthetic firewall policies of large sizes. The experimental results show that our algorithm is very efficient. In practice, our firewal verification algorithm can be used in the iterative process of firewall policy design, verification, and maintenance. Note that the firewall policy verification algorithm proposed in this paper is not limited to firewalls. Rather, they can be potentially applied to other rule- based systems as well.","Formal verification,
Iterative algorithms,
Internet,
Algorithm design and analysis,
Protection,
Telecommunication traffic,
Communications Society,
Computer science,
Computer security,
Testing"
Scaling up Classifiers to Cloud Computers,"As the size of available datasets has grown from Megabytes to Gigabytes and now into Terabytes, machine learning algorithms and computing infrastructures have continuously evolved in an effort to keep pace. But at large scales, mining for useful patterns still presents challenges in terms of data management as well as computation. These issues can be addressed by dividing both data and computation to build ensembles of classifiers in a distributed fashion, but trade-offs in cost, performance, and accuracy must be considered when designing or selecting an appropriate architecture. In this paper, we present an abstraction for scalable data mining that allows us to explore these trade-offs. Data and computation are distributed to a computing cloud with minimal effort from the user, and multiple models for data management are available depending on the workload and system configuration. We demonstrate the performance and scalability characteristics of our ensembles using a wide variety of datasets and algorithms on a Condor-based pool with Chirp to handle the storage.","Cloud computing,
Data mining,
Distributed computing,
Scalability,
Partitioning algorithms,
Large-scale systems,
Computer science,
Data engineering,
USA Councils,
Machine learning algorithms"
A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data,"How many independent samples N do we need from a distribution p to decide that p is epsiv-distant from uniform in an L1 sense, Sigmai=1 m |p(i) - 1/m| > epsiv? (Here m is the number of bins on which the distribution is supported, and is assumed known a priori.) Somewhat surprisingly, we only need N epsiv2 Gt m 1/2 to make this decision reliably (this condition is both sufficient and necessary). The test for uniformity introduced here is based on the number of observed ldquocoincidencesrdquo (samples that fall into the same bin), the mean and variance of which may be computed explicitly for the uniform distribution and bounded nonparametrically for any distribution that is known to be epsiv-distant from uniform. Some connections to the classical birthday problem are noted.",
Reflections on Teaching and Learning in an Advanced Undergraduate Course in Embedded Systems,"An integrated series of courses on embedded systems has been developed at Iowa State University, Ames, spanning early undergraduate to graduate levels. The newest course in the series is CPRE 488: Embedded Systems Design, an advanced undergraduate course that fills a gap in the curriculum by providing system-level design experiences and incorporating new technology advancements. CPRE 488 development focused on lecture-lab integration and laboratory learning. Course and lab activities were designed using a learning model that captures lower-order and higher-order cognition levels of Bloom's taxonomy. The learning experience in the laboratory is characterized using a technique to assess cognitive behavior. Results of applying the Florida Taxonomy of Cognitive Behavior are presented to summarize the depth of student learning and the opportunities for students to progress to higher-order thinking in the laboratory. After two years of experience with the new course, the authors reflect on the course design and outcomes, from both disciplinary and pedagogical viewpoints.","Laboratories,
Education,
Hardware,
Embedded system,
Computers,
Software,
Computational modeling"
Synthesis of Optimal Interfaces for Hierarchical Scheduling with Resources,"This paper presents algorithms that (1) facilitatesystem-independent synthesis of timing-interfaces for subsystems and(2) system-level selection of interfaces to minimize CPU load. Theresults presented are developed for hierarchical fixed-priorityscheduling of subsystems that may share logical recourses (i.e.,semaphores). We show that the use of shared resources results in atradeoff problem, where resource locking times can be traded for CPUallocation, complicating the problem of finding the optimalinterface configuration subject to scheduability.This paper presents a methodology where such a tradeoff can beeffectively explored. It first synthesizes a bounded set ofinterface-candidates for each subsystem, independently of the finalsystem, such that the set contains the interface that minimizessystem load for any given system. Then, integrating subsystems intoa system, it finds the optimal selection of interfaces. Ouralgorithms have linear complexity to the number of tasks involved.Thus, our approach is also suitable for adaptable andreconfigurable systems.","Resource management,
Real time systems,
Processor scheduling,
Computer science,
Scheduling algorithm,
Vehicles,
Software systems,
Application software,
Fellows,
Access protocols"
Lightweight and Compromise-Resilient Message Authentication in Sensor Networks,"Numerous authentication schemes have been proposed in the past for protecting communication authenticity and integrity in wireless sensor networks. Most of them however have following limitations: high computation or communication overhead, no resilience to a large number of node compromises, delayed authentication, lack of scalability, etc. To address these issues, we propose in this paper a novel message authentication approach which adopts a perturbed polynomial-based technique to simultaneously accomplish the goals of lightweight, resilience to a large number of node compromises, immediate authentication, scalability, and non-repudiation. Extensive analysis and experiments have also been conducted to evaluate the scheme in terms of security properties and system overhead.","Message authentication,
Peer to peer computing,
Polynomials,
Resilience,
Scalability,
Wireless sensor networks,
Computer science,
Delay,
Public key,
Digital signatures"
Performance Analysis and Evaluation of PCIe 2.0 and Quad-Data Rate InfiniBand,"High-performance systems are undergoing a major shift as commodity multi-core systems become increasingly prevalent. As the number of processes per compute node increase, the other parts of the system must also scale appropriately to maintain a balanced system. In the area of high-performance computing, one very important element of the overall system is the network interconnect that connects compute nodes in the system. InfiniBand is a popular interconnect for high-performance clusters. Unfortunately, due to limited bandwidth of the PCI-Express fabric, InfiniBand performance has remained limited. PCI-Express (PCIe) 2.0 has recently become available and has doubled the transfer rates available. This additional I/O bandwidth balances the system and makes higher data rates for external interconnects such as InfiniBand feasible. As a result, InfiniBand Quad-Data Rate (QDR) mode has become available on the newest Mellanox InfiniBand Host Channel Adapter (HCA) with a 40 Gb/sec signaling rate. In this paper we perform an in-depth performance analysis of PCIe 2.0 and the effect of increased InfiniBand signaling rates. We show that even using the Double DataRate (DDR) interface, PCIe 2.0 enables a 25% improvement in NAS Parallel Benchmark IS performance. Furthermore, we show that when using QDR on PCIe 2.0, network loopback can outperform a shared memory message passing implementation. We show that increased interconnection bandwidth significantly improves the overall system balance by lowering latency and increasing bandwidth.","Bandwidth,
Benchmark testing,
Throughput,
Computer architecture,
Performance evaluation,
Switches,
Physical layer"
Grading Method of Leaf Spot Disease Based on Image Processing,"Since current grading of plant diseases is mainly based on eyeballing, a new method is developed based on computer image processing. All influencingfactors existed in the process of image segmentation was analyzed and leaf region was segmented by using Otsu method. In the HSI color system, H component was chosen to segment disease spot to reduce the disturbance of illumination changes and the vein. Then, disease spot regions were segmented by using Sobel operator to examine disease spot edges. Finally, plant diseases are graded by calculating the quotient of disease spot and leaf areas. Researches indicate that this method to grade plant leaf spot diseases is fast and accurate.","Diseases,
Image processing,
Image segmentation,
Plants (biology),
Lesions,
Agricultural engineering,
Pixel,
Educational institutions,
Image analysis,
Computer science"
"LLCM-WIP: Low-Latency, Continuous-Motion Walking-in-Place","Walking-in-place techniques for locomotion in virtual environments typically have two problems that impact their usability: system latency (particularly troublesome when starting and stopping locomotion), and the fact that the change in the user's viewpoint may not be smooth and continuous. This paper describes a new WIP interface that improves both latency and the continuity of synthesized locomotion in the virtual environment. By basing the virtual avatar motion on the speed of the user's heel motion while walking in place, we create a direct mapping from foot-motion to locomotion that is responsive, intuitive, and easy to implement. In this paper, we describe the technique, analyze its starting and stopping latency, and provide experimental results on the suppression of false steps and general usability of the system.","Delay,
Legged locomotion,
Virtual environment,
Usability,
Avatars,
User interfaces,
Computer graphics,
Layout,
Control systems,
Turning"
Succincter,"We can represent an array of n values from {0,1,2} using ceil(n log_2 3) bits (arithmetic coding), but then we cannot retrieve a single element efficiently. Instead, we can encode every block of t elements using ceil(t log_2 3)  bits, and bound the retrieval time by t. This gives a linear trade-off between the redundancy of the representation and the query time.In fact, this type of linear trade-off is ubiquitous in known succinct data structures, and in data compression. The folk wisdom is that if we want to waste one bit per block, the encoding is so constrained that it cannot help the query in any way. Thus, the only thing a query can do is to read the entire block and unpack it.We break this limitation and show how to use recursion to improve redundancy. It turns out that if a block is encoded with two (!) bits of redundancy, we can decode a single element, and answer many other interesting queries, in time logarithmic in the block size.Our technique allows us to revisit classic problems in succinct data structures, and give surprising new upper bounds. We also construct a locally-decodable version of arithmetic coding.",
Automatic Mood-Transferring between Color Images,"This paper presents a new color-conversion method that offers users an intuitive, one-click interface for style conversion. So, rather than having to supply a reference image, users simply select a color mood with a mouse click. To address the color-transfer quality problem, we associate each color mood with a set of up to 10 images from our image database. After selecting their color mood, users choose one associated image. Our histogram matching algorithm then uses the selected image to determine the input image's color distribution. We thereby achieve more accurate and justifiable color conversion results, while also preserving spatial coherence. Here, we further describe our solutions and their results and compare them to existing approaches.","Color,
Mood,
Image converters,
Image databases,
Histograms,
Pixel,
Spatial coherence,
Digital cameras,
Robustness,
Impedance matching"
Choosing splitting parameters and summation limits in the numerical evaluation of 1-D and 2-D periodic Green's functions using the Ewald method,"Accurate and efficient computation of periodic free-space Green's functions using the Ewald method is considered for three cases: a 1-D array of line sources, a 1-D array of point sources, and a 2-D array of point sources. A limitation on the numerical accuracy when using the “optimum” E parameter (which gives optimum asymptotic convergence) at high frequency is discussed. A “best” E parameter is then derived to overcome these limitations. This choice allows for the fastest convergence while maintaining a specific level of accuracy (loss of significant figures) in the final result. Formulas for the number of terms needed for convergence are also derived for both the spectral and the spatial series that appear in the Ewald method, and these are found to be accurate in almost all cases.","Green's function methods,
Convergence,
Lattices,
Manganese,
Method of moments,
Limiting,
Phased arrays"
Color Based Hand and Finger Detection Technology for User Interaction,The aim of this paper is to present the methodology for hand detection and propose the finger detection method. The detected hand and finger can be used to implement the non-contact mouse. This technology can be used to control the home devices such as curtain and television. Skin color is used to segment the hand region from background and counter is extracted from the segmented hand. Analysis of counter gives us the location of finger tip in the hand. We have performed extensive experiment and achieve very encouraging result.,"Computational modeling,
Fingers,
Solid modeling,
Image color analysis,
Feature extraction,
Image segmentation,
Joints"
Efficient online computation of core speeds to maximize the throughput of thermally constrained multi-core processors,"We address the problem of efficient online computation of the speeds of different cores of a multi-core processor to maximize the throughput (which is expressed as a weighted sum of the speeds), subject to an upper bound on the core temperatures. We first compute the solution for steady-state thermal conditions by solving a linear program. We then present two approaches to computing the transient speed curves for each core: (i) a local solution, which involves solving a linear program every time step (of about 10 ms), and (ii) a global solution, which computes the optimal speed curve over a large time window (of about 100 s) by solving a non-linear program. We showed that the local solution is insensitive to the weights assigned in the performance objective (hence the need for the global solution). This is because a reduction in the speed of a core can only reduce the temperature of the other cores over much larger time periods (of the order of several seconds). The local solution is then completely determined by the temperature constraint equations. We show that the constraint matrix exhibits a special property - it can be expressed as the sum of a diagonal matrix and a matrix with identical rows. This allows us to solve the multi-core thermal constraint equations analytically to determine the (temporally) local optimum speeds. Further, we showed that due to this property, the steady-state speed solution selects a set of threads to operate at maximum temperature, and turns off all unused cores. Hence, to ensure that all available threads are scheduled, we impose a “fairness” constraint. Finally, we show how the open-loop speed control methods proposed above could be used together with a feedback controller to achieve robustness to model uncertainty.",
Oscillation monitoring from ambient PMU measurements by Frequency Domain Decomposition,"This paper extends the method of Frequency Domain Decomposition (FDD) towards real-time analysis of ambient PMU measurements in power systems for the purpose of oscillation monitoring. The main idea of FDD is to apply Singular Value Decomposition (SVD) to the power density spectrum matrix. The resulting singular values correspond to individual modes under specific conditions. The damping ratio, modal frequency and the mode shape of poorly damped (with less than 5% damping ratios) oscillatory modes can be directly determined from ambient PMU measurements. The theoretical background is presented in the paper. The technique is tested on small linear systems to show its effectiveness and weaknesses. Finally, the application of FDD in power system is shown.",
Toeplitz block matrices in compressed sensing and their applications in imaging,"Recent work in compressed sensing theory shows that sensing matrices whose entries are drawn independently from certain probability distributions guarantee exact recovery of a sparse signal from a small number of measurements with high probability. In most medical imaging systems, the encoding matrices cannot take that form. Instead, they are Toeplitz block matrix. Motivated by this fact, we consider Toeplitz block matrices as the sensing matrices. We show that the probability of perfect reconstruction from a smaller number of filter outputs is also high if the filter coefficients are independently and identically-distributed random variable. Their applications in medical imaging is discussed. Simulation results are also shown to validate the theorem.","Compressed sensing,
Sparse matrices,
Biomedical imaging,
Probability distribution,
Biomedical measurements,
Encoding,
Image reconstruction,
Filters,
Vectors,
Information technology"
A Hybrid Approach to Private Record Linkage,"Real-world entities are not always represented by the same set of features in different data sets. Therefore matching and linking records corresponding to the same real-world entity distributed across these data sets is a challenging task. If the data sets contain private information, the problem becomes even harder due to privacy concerns. Existing solutions of this problem mostly follow two approaches: sanitization techniques and cryptographic techniques. The former achieves privacy by perturbing sensitive data at the expense of degrading matching accuracy. The later, on the other hand, attains both privacy and high accuracy under heavy communication and computation costs. In this paper, we propose a method that combines these two approaches and enables users to trade off between privacy, accuracy and cost. Experiments conducted on real data sets show that our method has significantly lower costs than cryptographic techniques and yields much more accurate matching results compared to sanitization techniques, even when the data sets are perturbed extensively.","Couplings,
Cryptography,
Costs,
Sliding mode control,
Data privacy,
Cryptographic protocols,
Joining processes,
Hospitals,
Protection,
Computer science"
Adversarial models and resilient schemes for network coding,"In a recent paper, Jaggi et al. [10], presented a distributed polynomial-time rate-optimal network-coding scheme that works in the presence of Byzantine faults.We revisit their adversarial models and augment them with three, arguably realistic, models. In each of the models, we present a distributed scheme that demonstrates the usefulness of the model. In particular, all of the schemes obtain optimal rate C-z, where C is the network capacity and z is a bound on the number of links controlled by the adversary.",
Understanding human intentions via Hidden Markov Models in autonomous mobile robots,"Understanding intent is an important aspect of communication among people and is an essential component of the human cognitive system. This capability is particularly relevant for situations that involve collaboration among agents or detection of situations that can pose a threat. In this paper, we propose an approach that allows a robot to detect intentions of others based on experience acquired through its own sensory-motor capabilities, then using this experience while taking the perspective of the agent whose intent should be recognized. Our method uses a novel formulation of Hidden Markov Models designed to model a robot's experience and interaction with the world. The robot's capability to observe and analyze the current scene employs a novel vision-based technique for target detection and tracking, using a non-parametric recursive modeling approach. We validate this architecture with a physically embedded robot, detecting the intent of several people performing various activities.","Hidden Markov models,
Robot sensing systems,
Computational modeling,
Humans,
Training,
Robot kinematics"
A sensor selection approach for target tracking in sensor networks with quantized measurements,"This paper extends our earlier work on sensor selection [1]. We are now focusing on a more challenging problem of how to effectively utilize quantized sensor data for target tracking in sensor networks by considering sensor selection problems with quantized data. A subset of sensors are dynamically selected to optimize the tracking performance. The one-step- look-ahead posterior Cramer-Rao Lower Bound (CRLB) on the state estimation error is proposed as the sensor selection criterion. Particle filtering method is employed to compute the posterior CRLB, as well as to estimate the target state. Simulation results show that the proposed posterior CRLB based method outperforms the one based on information theoretic measures.",
Joint-action for humans and industrial robots for assembly tasks,"This paper presents a concept of a smart working environment designed to allow true joint-actions of humans and industrial robots. The proposed system perceives its environment with multiple sensor modalities and acts in it with an industrial robot manipulator to assemble capital goods together with a human worker. In combination with the reactive behavior of the robot, safe collaboration between the human and the robot is possible. Furthermore, the system anticipates human behavior, based on knowledge databases and decision processes, ensuring an effective collaboration between the human and robot. As a proof of concept, we introduce a use case where an arm is assembled and mounted on a robotpsilas body.",
Derivation and Validation of a Sensitivity Formula for Slit-Slat Collimation,"An analytic formula is derived for the sensitivity of collimators achieving transverse collimation with a slit and axial collimation with a slat assembly whose septa may be parallel or focus on a line. The formula predicts sin3 phi dependence on the incidence angle and, in the particular case of parallel slats, 1/h dependence on the distance from the slit. More complex expressions for sensitivity that do not diverge at points near the slit or the focal line of the slat assembly are also derived. The predictions of the formulas are checked against simple cases for which solutions are available from direct calculation as well as against Monte Carlo simulation and published experimental data. Agreement is good in all cases analyzed. An approximate penetration model is also introduced: it involves the use of a sensitivity-effective slit width and septal length. Its predictions are compared to simulation results. Agreement was found to be compatible with statistical fluctuation (plusmn0.3%) for geometric sensitivity and better than 3 % of total sensitivity in the worst case of septa designed for high-energy (364.5 keV) photons.","Single photon emission computed tomography,
Apertures,
Assembly,
Radiology,
Predictive models,
Fluctuations,
Optical collimators,
Nuclear imaging,
Animals,
Geometry"
Multiscale Vascular Surface Model Generation From Medical Imaging Data Using Hierarchical Features,"Computational fluid dynamics (CFD) modeling of blood flow from image-based patient specific models can provide useful physiologic information for guiding clinical decision making. A novel method for the generation of image-based, 3-D, multiscale vascular surface models for CFD is presented. The method generates multiscale surfaces based on either a linear triangulated or a globally smooth nonuniform rational B-spline (NURB) representation. A robust local curvature analysis is combined with a novel global feature analysis to set mesh element size. The method is particularly useful for CFD modeling of complex vascular geometries that have a wide range of vasculature size scales, in conditions where 1) initial surface mesh density is an important consideration for balancing surface accuracy with manageable size volumetric meshes, 2) adaptive mesh refinement based on flow features makes an underlying explicit smooth surface representation desirable, and 3) semi-automated detection and trimming of a large number of inlet and outlet vessels expedites model construction.","Biomedical imaging,
Computational fluid dynamics,
Spline,
Solid modeling,
Blood flow,
Decision making,
Image generation,
Surface topography,
Surface reconstruction,
Robustness"
Multi-Agent Based Adaptive Consensus Control for Multiple Manipulators with Kinematic Uncertainties,"An adaptive control approach is proposed to deal with the multiple manipulators consensus problem based on the multi-agent theory. In the current multi-agent literature, agents were assumed to have determined models. However, the practical manipulator's kinematics contains uncertain parameters. By using the projection method, the adaptive updating law for uncertain kinematic parameters is derived. Then, the estimated manipulator Jacobian matrix can be obtained to design the decentralized controller. By the proposed controller, all the manipulators' end-effectors move towards the same configuration to achieve certain coordination tasks. In addition, performance of the control system is analyzed by the Lyapunov method, and the consensus error is proved to approach zero. Finally, the effectiveness of the proposed scheme is illustrated by simulations on a multiple PUMA 560 robots system.","Intelligent control,
Control systems,
USA Councils"
Informed visual search: Combining attention and object recognition,"This paper studies the sequential object recognition problem faced by a mobile robot searching for specific objects within a cluttered environment. In contrast to current state-of-the-art object recognition solutions which are evaluated on databases of static images, the system described in this paper employs an active strategy based on identifying potential objects using an attention mechanism and planning to obtain images of these objects from numerous viewpoints. We demonstrate the use of a bag-of-features technique for ranking potential objects, and show that this measure outperforms geometric matching for invariance across viewpoints. Our system implements informed visual search by prioritising map locations and re-examining promising locations first. Experimental results demonstrate that our system is a highly competent object recognition system that is capable of locating numerous challenging objects amongst distractors.",
Simultaneous Ultrasound and MRI System for Breast Biopsy: Compatibility Assessment and Demonstration in a Dual Modality Phantom,"Simultaneous capturing of ultrasound (US) and magnetic resonance (MR) images allows fusion of information obtained from both modalities. We propose an MR-compatible US system where MR images are acquired in a known orientation with respect to the US imaging plane and concurrent real-time imaging can be achieved. Compatibility of the two imaging devices is a major issue in the physical setup. Tests were performed to quantify the radio frequency (RF) noise introduced in MR and US images, with the US system used in conjunction with MRI scanner of different field strengths (0.5 T and 3 T). Furthermore, simultaneous imaging was performed on a dual modality breast phantom in the 0.5 T open bore and 3 T close bore MRI systems to aid needle-guided breast biopsy. Fiducial based passive tracking and electromagnetic based active tracking were used in 3 T and 0.5 T, respectively, to establish the location and orientation of the US probe inside the magnet bore. Our results indicate that simultaneous US and MR imaging are feasible with properly-designed shielding, resulting in negligible broadband noise and minimal periodic RF noise in both modalities. US can be used for real time display of the needle trajectory, while MRI can be used to confirm needle placement.","Ultrasonic imaging,
Magnetic resonance imaging,
Breast biopsy,
Imaging phantoms,
Radio frequency,
Boring,
Performance evaluation,
Needles,
Magnetic resonance,
Real time systems"
"Local Harmonic
B
z
Algorithm With Domain Decomposition in MREIT: Computer Simulation Study","Magnetic resonance electrical impedance tomography (MREIT) attempts to provide conductivity images of an electrically conducting object with a high spatial resolution. When we inject current into the object, it produces internal distributions of current density J and magnetic flux density B=(Bx,By,Bz). By using a magnetic resonance imaging (MRI) scanner, we can measure Bz data where z is the direction of the main magnetic field of the scanner. Conductivity images are reconstructed based on the relation between the injection current and Bz data. The harmonic Bz algorithm was the first constructive MREIT imaging method and it has been quite successful in previous numerical and experimental studies. Its performance is, however, degraded when the imaging object contains low-conductivity regions such as bones and lungs. To overcome this difficulty, we carefully analyzed the structure of a current density distribution near such problematic regions and proposed a new technique, called the local harmonic Bz algorithm. We first reconstruct conductivity values in local regions with a low conductivity contrast, separated from those problematic regions. Then, the method of characteristics is employed to find conductivity values in the problematic regions. One of the most interesting observations of the new algorithm is that it can provide a scaled conductivity image in a local region without knowing conductivity values outside the region. We present the performance of the new algorithm by using computer simulation methods.","Conductivity,
Current density,
Magnetic resonance imaging,
Magnetic field measurement,
Image reconstruction,
Magnetic resonance,
Impedance,
Tomography,
Spatial resolution,
Magnetic flux density"
Multiscale competitive code for efficient palmprint recognition,"Coding-based method, which encodes the responses of a bank of filters into bitwise features, has been very successful in palmprint representation and matching. Palmprints, however, are typically multiscale features, where the palm lines can be represented at a higher scale while the wrinkles at a lower scale. In this work, we present a mutliscale competitive code method for efficient palmprint representation and matching. In filterbank design, we adopt the log-Gabor wavelets since of its less overlapping in the frequency domain. In palmprint representation, competitive code is used to encoding the dominant orientation of the filter responses in each scale. In palmprint matching, a fusion rule is proposed to combine the distances obtained using different scales. Experimental results indicate that the proposed method achieves better recognition accuracy and faster matching speed while compared with several state-of-the-art methods.","Gabor filters,
Filter bank,
Matched filters,
Biometrics,
Computer science,
Wavelet domain,
Frequency domain analysis,
Encoding,
Data mining,
Aging"
Resolving Anonymous Routers in Internet Topology Measurement Studies,"Internet measurement studies utilize traceroute to collect path traces from the Internet. A router that does not respond to a traceroute query is referred to as an anonymous router and is represented by a '*' in the traceroute output. Anonymous router resolution refers to the task of identifying the occurrences of '*'s that belong to the same router in the underlying network. This task is an important step in building traceroute-based topology maps and obtaining an optimum solution is shown to be NP-complete. In this paper, we use a novel technique from graph data mining field to build an efficient solution. The results of our experiments on both synthetic and genuine topologies show a significant improvement in accuracy and effectiveness over the existing approaches.","Internet,
Network topology,
Computational complexity,
IP networks,
Communications Society,
Computer science,
Probes,
Spine,
Data mining,
Character generation"
On the Visualization of Social and other Scale-Free Networks,"This paper proposes novel methods for visualizing specifically the large power-law graphs that arise in sociology and the sciences. In such cases a large portion of edges can be shown to be less important and removed while preserving component connectedness and other features (e.g. cliques) to more clearly reveal the networkpsilas underlying connection pathways. This simplification approach deterministically filters (instead of clustering) the graph to retain important node and edge semantics, and works both automatically and interactively. The improved graph filtering and layout is combined with a novel computer graphics anisotropic shading of the dense crisscrossing array of edges to yield a full social network and scale-free graph visualization system. Both quantitative analysis and visual results demonstrate the effectiveness of this approach.",
Combating the Insider Cyber Threat,"The penetration of US national security by foreign agents as well as American citizens is a historical and current reality that's a persistent and increasing phenomenon. Surveys, such as the e-crime watch survey, reveal that current or former employees and contractors are the second greatest cybersecurity threat, exceeded only by hackers, and that the number of security incidents has increased geometrically in recent years. The insider threat is manifested when human behavior departs from compliance with established policies, regardless of whether it results from malice or a disregard for security policies. In this article, we focus on the need for effective training to raise staff awareness about insider threats and the need for organizations to adopt a more effective approach to identifying potential risks and then taking proactive steps to mitigate them.",
Constant-Time Approximation Algorithms via Local Improvements,"We present a technique for transforming classical approximation algorithms into constant-time algorithms that approximate the size of the optimal solution. Our technique is applicable to a certain subclass of algorithms that compute a solution in a constant number of phases. The technique is based on greedily considering local improvements in random order.The problems amenable to our technique include Vertex Cover, Maximum Matching, Maximum Weight Matching, Set Cover, and Minimum Dominating Set. For example, for Maximum Matching, we give the first constant-time algorithm that for the class of graphs of degree bounded by d, computes the maximum matching size to within \eps n, for any \eps ≫ 0, where n is the number of nodes in the graph. The running time of the algorithm is independent of n, and only depends on d and \eps.","Approximation algorithms,
Distributed algorithms,
Computer science,
Random number generation,
Computational modeling"
G2Way A Backtracking Strategy for Pairwise Test Data Generation,"Our continuous dependencies on software (i.e. to assist as well as facilitate our daily chores) often raise dependability issue particularly when software is being employed harsh and life threatening or (safety) critical applications. Here, rigorous software testing becomes immensely important. Many combinations of possible input parameters, hardware/software environments, and system conditions need to be tested and verified against for conformance. Due to resource constraints as well as time and costing factors, considering all exhaustive test possibilities would be impossible (i.e. due to combinatorial explosion problem). Earlier work suggests that pairwise sampling strategy (i.e. based on two-way parameter interaction) can be effective. Building and complementing earlier work, this paper discusses an efficient pairwise test data generation strategy, called G2Way. In doing so, this paper demonstrates the correctness of G2Way as well as compares its effectiveness against existing strategies including AETG and its variations, IPO, SA, GA, ACA, and All Pairs. Empirical evidences demonstrate that G2Way, in some cases, outperformed other strategies in terms of the number of generated test data within reasonable execution time.","Software testing,
System testing,
Software safety,
Hardware,
Explosions,
Application software,
Electronic equipment testing,
Computer science,
Software systems,
Control systems"
Reputation Systems for Self-Organized Networks,"Self-organized networks such as mobile ad-hoc, Internet-based peer-to-peer, wireless mesh and Fourth generation (4G) wireless networks depend on cooperation of nodes. Reputation systems help nodes decide with whom to cooperate and which nodes to avoid. They have been studied and applied almost separately in diverse disciplines such as economics, computer science, and social science, resulting in effort duplication and inconsistent terminology. We aim to bring together these efforts by outlining features and fundamental questions common to reputation systems in general. We derive methodologies to address these questions for both reputation system design and research from our own experiences and evaluations by simulation and analytical modeling. We argue for using deviation tests, discounting, passing on only first-hand information, introducing secondary response, and stressing the importance of identity.","Peer to peer computing,
IP networks,
Computer science,
Terminology,
Wireless mesh networks,
Springs,
Analytical models,
Testing,
Quality of service,
Pricing"
R-OSGi-based architecture of distributed smart home system,The conventional architecture of smart home is usually server-centric and this may cause many problems. The distributed architecture presented in this paper is based on multiple OSGi (Open Service Gateway Initiative) platforms. Relative home appliance services which are compliant with R-OSGi middleware for the home network system are also developed. The important aspect of R-OSGi is that it offers distribution transparency and uses the natural boundaries between software modules to establish the potential boundaries for distribution of the application. Experimental results obtained from the demo room show that the architecture is practical and reliable.,
CIlib: A collaborative framework for Computational Intelligence algorithms - Part I,"Research in Computational Intelligence (CI) has produced a huge collection of algorithms, grouped into the main CI paradigms. Development of a new CI algorithm requires such algorithm to be thoroughly benchmarked against existing algorithms, which requires researchers to implement already published algorithms. This re-implementation of existing algorithms unnecessarily wastes valuable time, and may be the cause of incorrect results due to unexpected bugs in the code. It is also the case that more, new CI algorithms are hybrids of algorithms from different paradigms. This illustrates a demand for a comprehensive library of CI algorithms, to minimize development time and the occurrence of programming errors, and to facilitate combination of components to form hybrid models. This paper presents such a library, called CIlib.",
On matching latent fingerprints,"Latent fingerprint identification is of critical importance to law enforcement agencies in forensics application. While tremendous progress has been made in the field of automatic fingerprint matching, latent fingerprint matching continues to be a difficult problem because the challenges involved in latent print matching are quite different from plain or rolled fingerprint matching. Poor quality of friction ridge impressions, small finger area and large non-linear distortion are some of the main difficulties in latent fingerprint matching. We propose a system for matching latent images to rolled fingerprints that takes into account the specific characteristics of the latent matching problem. In addition to minutiae, additional features like orientation field and quality map are also used in our system. Experimental results on the NIST SD27 latent database indicate that the introduction of orientation field and quality map to minutiae-based matching leads to good recognition performance despite the inherently difficult nature of the problem. We achieve the rank-20 accuracy of 93.4% in retrieving 258 latents from a background database of 2,258 rolled fingerprints.","Fingerprint recognition,
Law enforcement,
Forensics,
Friction,
Fingers,
Nonlinear distortion,
NIST,
Image databases,
Spatial databases,
Information retrieval"
Scaling Up the Formal Verification of Lustre Programs with SMT-Based Techniques,"We present a general approach for verifying safety properties of Lustre programs automatically. Key aspects of the approach are the choice of an expressive first-order logic in which Lustre's semantics is modeled very naturally, the tailoring to this logic of SAT-based k-induction and abstraction techniques, and the use of SMT solvers to reason efficiently in this logic. We discuss initial experimental results showing that our implementation of the approach is highly competitive with existing verification solutions for Lustre.",
Clonetracker,"Code clones are generally considered to be an obstacle to software maintenance. Research has provided evidence that it may not always be practical, feasible, or cost-effective to eliminate certain clone groups through refactoring. This paper describes CloneTracker, an Eclipse plug-in that provides support for tracking code clones in evolving software. With CloneTracker, developers can specify clone groups they wish to track, and the tool will automatically generate a clone model that is robust to changes to the source code, and can be shared with other collaborators of the project. When future modifications intersect with tracked clones, CloneTracker will notify the developer, provide support to consistently apply changes to a corresponding clone region, and provide support for updating the clone model. CloneTracker complements existing techniques by providing support for reusing knowledge about the location of clones in source code, and support for keeping track of clones when refactoring is not desirable.","Cloning,
Software maintenance,
Robustness,
Collaborative tools,
Logic programming,
Software systems,
Computer science,
Computer languages,
Writing,
History"
Topic Detection and Extraction in Chat,"Internet-based chat environments such as Internet Relay Chat and instant messaging pose a challenge for datamining and information retrieval systems due to the multi-threaded, overlapping nature of the dialog and the nonstandard usage of language. In this paper we present preliminary methods of topic detection and topic thread extraction that augment a typical TF-IDF-based vector space model approach with temporal relationship information between posts of the chat dialog combined with WordNet hypernym augmentation. We show results that promise better performance than using only a TF-IDF bag-of-words vector space model.","Distance measurement,
Social network services,
Data mining,
Computational modeling,
Africa,
Feature extraction,
Internet"
The ASC-Alliance Projects: A Case Study of Large-Scale Parallel Scientific Code Development,"Computational scientists face many challenges when developing software that runs on large-scale parallel machines. However, software-engineering researchers haven't studied their software development processes in much detail. To better understand the nature of software development in this context, the authors examined five large-scale computational science software projects operated at the five ASC-Alliance centers.","Large-scale systems,
Solid modeling,
Electric shock,
Concurrent computing,
Programming,
Computational modeling,
Fires,
Containers,
Rockets,
Parallel machines"
DiffGen: Automated Regression Unit-Test Generation,"Software programs continue to evolve throughout their lifetime. Maintenance of such evolving programs, including regression testing, is one of the most expensive activities in software development. We present an approach and its implementation called DiffGen for automated regression unit-test generation and checking for Java programs. Given two versions of a Java class, our approach instruments the code by adding new branches such that if these branches can be covered by a test generation tool, behavioral differences between the two class versions are exposed. DiffGen then uses a coverage-based test generation tool to generate test inputs for covering the added branches to expose behavioral differences. We have evaluated DiffGen on finding behavioral differences between 21 classes and their versions. Experimental results show that our approach can effectively expose many behavioral differences that cannot be exposed by state-of-the-art techniques.","Production facilities,
Testing,
Java,
Driver circuits,
Instruments,
Software,
Detectors"
Solving dynamic multi-objective problems with vector evaluated particle swarm optimisation,"Many optimisation problems are multi-objective and change dynamically. Many methods use a weighted average approach to the multiple objectives. This paper introduces the usage of the vector evaluated particle swarm optimiser (VEPSO) to solve dynamic multi-objective optimisation problems. Every objective is solved by one swarm and the swarms share knowledge amongst each other about the objective that it is solving. Not much work has been done on using this approach in dynamic environments. This paper discusses this approach as well as the effect of the population size and the response methods to a detected change on the performance of the algorithm. The results showed that more non-dominated solutions, as well as more uniformly distributed solutions, are found when all swarms are re-intialised when a change is detected, instead of only the swarm(s) optimising the specific objective function(s) that has changed. Furthermore, an increase in population size results in a higher number of non-dominated solutions found, but can lead to solutions that are less uniformly distributed.","Optical fibers,
Measurement,
Optimization,
Benchmark testing,
Equations,
Heuristic algorithms,
Algorithm design and analysis"
Live and incremental whole-system migration of virtual machines using block-bitmap,"In this paper, we describe a whole-system live migration scheme, which transfers the whole system run-time state, including CPU state, memory data, and local disk storage, of the virtual machine (VM). To minimize the downtime caused by migrating large disk storage data and keep data integrity and consistency, we propose a three-phase migration (TPM) algorithm. To facilitate the migration back to initial source machine, we use an incremental migration (IM) algorithm to reduce the amount of the data to be migrated. Block-bitmap is used to track all the write accesses to the local disk storage during the migration. Synchronization of the local disk storage in the migration is performed according to the block-bitmap. Experiments show that our algorithms work well even when I/O-intensive workloads are running in the migrated VM. The downtime of the migration is around 100 milliseconds, close to shared-storage migration. Total migration time is greatly reduced using IM. The block-bitmap based synchronization mechanism is simple and effective. Performance overhead of recording all the writes on migrated VM is very low.","Synchronization,
Driver circuits,
Resumes,
Redundancy,
Availability,
Bandwidth,
Degradation"
Optimal technology selection for minimizing energy and variability in low voltage applications,"Ultra Low voltage operation has recently drawn significant attention due to its large potential energy savings. However, typical design practices used for super-threshold operation are not necessarily compatible with the low voltage regime. Here, radically different guidelines may be needed since existing process technologies have been optimized for super-threshold operation. We therefore study the selection of the optimal technology in ultra low voltage designs to achieve minimum energy and minimum variability which are among foremost concerns. We investigate five industrial technologies, from 250nm to 65nm. We demonstrate that mature technologies are often the best choice in very low voltage applications, saving as much as ~1800X in total energy consumption compared to a poorly selected technology. In parallel, the effect of technology choice on variability is investigated, when operating at the energy optimal design point. The results show up to a 4X improvement in delay variation due to global process shift and mismatch when using the most advanced technologies despite their large variability at nominal Vdd.",
Enhancing Web Service Selection by User Preferences of Non-functional Features,"Selection of an appropriate Web service for a particular task has become a difficult challenge due to the increasing number of Web services offering similar functionalities. The functional properties describe what the service can do and the non-functional properties depict how the service can do it. Non-functional properties involving qualitative or quantitative features have become essential criteria to enhance the selection process of services making the selection process more complicated. Chaari et al. [8] proposed an ontological framework for modeling and exploiting non-functional properties. In this paper, we propose a simple Web services selection scheme based on user’s requirement of the various non-functional properties and interaction with the system. The proposed framework utilizes user preferences as an additional input to the selection engine and the system ranks the available services based on the requirement. The method is validated using a popular Web services bench mark and the experiment results indicate that the scheme is useful and warrants further research.","Web services,
Ontologies,
Availability,
Service oriented architecture,
Time factors,
Computer science,
Noise measurement,
Search engines,
Software systems,
Software standards"
Middleware for Distributed Video Surveillance,"Graduates of computer science (CS) and software engineering (SE) programs are typically employed to develop industry-strength software. Computer engineering (CE) programs focus primarily on computing-system design, often with significant software components. These three programs have different emphases: development of new algorithms versus development of large, complex software systems versus development of small embedded software and device drivers. All three areas require good SE practices.",
Multimodal News Story Clustering With Pairwise Visual Near-Duplicate Constraint,"Story clustering is a critical step for news retrieval, topic mining, and summarization. Nonetheless, the task remains highly challenging owing to the fact that news topics exhibit clusters of varying densities, shapes, and sizes. Traditional algorithms are found to be ineffective in mining these types of clusters. This paper offers a new perspective by exploring the pairwise visual cues deriving from near-duplicate keyframes (NDK) for constraint-based clustering. We propose a constraint-driven co-clustering algorithm (CCC), which utilizes the near-duplicate constraints built on top of text, to mine topic-related stories and the outliers. With CCC, the duality between stories and their underlying multimodal features is exploited to transform features in low-dimensional space with normalized cut. The visual constraints are added directly to this new space, while the traditional DBSCAN is revisited to capitalize on the availability of constraints and the reduced dimensional space. We modify DBSCAN with two new characteristics for story clustering: 1) constraint-based centroid selection and 2) adaptive radius. Experiments on TRECVID-2004 corpus demonstrate that CCC with visual constraints is more capable of mining news topics of varying densities, shapes and sizes, compared with traditional k-means, DBSCAN, and spectral co-clustering algorithms.",
What's a Typical Commit? A Characterization of Open Source Software Repositories,"The research examines the version histories of nine open source software systems to uncover trends and characteristics of how developers commit source code to version control systems (e.g., subversion). The goal is to characterize what a typical or normal commit looks like with respect to the number of files, number of lines, and number of hunks committed together. The results of these three characteristics are presented and the commits are categorized from extra small to extra large. The findings show that approximately 75% of commits are quite small for the systems examined along all three characteristics. Additionally, the commit messages are examined along with the characteristics. The most common words are extracted from the commit messages and correlated with the size categories of the commits. It is observed that sized categories can be indicative of the types of maintenance activities being performed.",
An Information Model for Geographic Greedy Forwarding in Wireless Ad-Hoc Sensor Networks,"In wireless ad-hoc sensor networks, an important issue often faced in geographic greedy forwarding routing is the ""local minimum phenomenon"" which is caused by deployment holes and blocks the forwarding process. In this paper, we provide a new information model for the geographic greedy forwarding routing that only forwards the packet within the so-called request zone. Under this new information model, the hole and its affected area are identified easily and quickly in an unsafe area with a labeling process. The greedy forwarding will be blocked if and only if a node inside the unsafe area is used. Due to the shape of the request zone, an unsafe area can be estimated as a rectangular region in the local view of unsafe nodes. With such estimate information, the new routing method proposed in this paper will avoid blocking by holes and achieve better performance in routing time while the cost of information construction is greatly reduced compared with the best results known to date.","Wireless sensor networks,
Routing,
Shape,
Costs,
USA Councils,
Computer networks,
Peer to peer computing,
Communications Society,
Computer science,
Sensor phenomena and characterization"
Error-correcting codes for rank modulation,"We investigate error-correcting codes for a novel storage technology for flash memories, the rank-modulation scheme. In this scheme, a set of n cells stores information in the permutation induced by the different charge levels of the individual cells. The resulting scheme eliminates the need for discrete cell levels, overcomes overshoot errors when programming cells (a serious problem that reduces the writing speed), and mitigates the problem of asymmetric errors. In this paper, we study the properties of error correction in rank modulation codes. We show that the adjacency graph of permutations is a subgraph of a multi-dimensional array of a special size, a property that enables code designs based on Lee-metric codes. We present a one-error-correcting code whose size is at least half of the optimal size. We also present additional error-correcting codes and some related bounds.","Distance measurement,
Switches,
Arrays,
Error correction codes,
Flash memory,
Programming,
Modulation"
Accurate EM-TV algorithm in PET with low SNR,"PET measurements of tracers with a lower dose rate or short radioactive half life suffer from extremely low SNRs. In these cases standard reconstruction methods (OSEM, EM, filtered backprojection) deliver unsatisfactory and noisy results. Here, we propose to introduce nonlinear variational methods into the reconstruction process to make an efficient use of a-priori information and to attain improved imaging results. We illustrate our technique by evaluating cardiac H215O measurements. The general approach can also be used for other specific goals allowing to incorporate a-priori information about the solution with Poisson distributed data.","Positron emission tomography,
Image reconstruction,
TV,
Bayesian methods,
Nuclear and plasma sciences,
Signal to noise ratio,
Additive noise,
Gaussian noise,
Noise measurement,
Minimization methods"
Power Grid Analysis and Optimization Using Algebraic Multigrid,"This paper presents a class of power grid analysis and optimization techniques, all of which are based on the algebraic-multigrid (AMG) method. First, a new AMG-based reduction scheme is proposed to improve the efficiency of reducing the problem size for power grid analysis and optimization. Next, with the proposed reduction technique, a fast transient-analysis method is developed and extended to an accurate solver with error control mechanism. After that, the scope of this method is further broadened for handling the analysis of the modified grid. Finally, a fast decap-allocation (DA) scheme based on AMG is suggested. Experimental results show that these techniques not only achieve a significant speedup over reported industrial methods but also enhance the quality of solutions. By using the proposed techniques, transient analysis with 200 time steps on a 1.6-M-node power grid can be completed in less than 5 min; dc analysis on the same circuit can reach an accuracy of in about 141 s. Our DA can process a circuit with up to one million nodes in about 11 min.","Power grids,
Noise robustness,
Optimization methods,
Circuits,
Very large scale integration,
Computer science education,
Educational programs,
Information science,
Student members,
Error correction"
Metropolis Algorithms for Representative Subgraph Sampling,"While data mining in chemoinformatics studied graph data with dozens of nodes, systems biology and the Internet are now generating graph data with thousands and millions of nodes. Hence data mining faces the algorithmic challenge of coping with this significant increase in graph size: Classic algorithms for data analysis are often too expensive and too slow on large graphs. While one strategy to overcome this problem is to design novel efficient algorithms, the other is to 'reduce' the size of the large graph by sampling. This is the scope of this paper: We will present novel Metropolis algorithms for sampling a 'representative' small subgraph from the original large graph, with 'representative' describing the requirement that the sample shall preserve crucial graph properties of the original graph. In our experiments, we improve over the pioneering work of Leskovec and Faloutsos (KDD 2006), by producing representative subgraph samples that are both smaller and of higher quality than those produced by other methods from the literature.","Sampling methods,
Data mining,
Runtime,
Application software,
Systems biology,
Internet,
Data analysis,
Algorithm design and analysis,
Bioinformatics,
Computer science"
Image de-fencing,"We introduce a novel image segmentation algorithm that uses translational symmetry as the primary foreground/background separation cue. We investigate the process of identifying and analyzing image regions that present approximate translational symmetry for the purpose of image fourground/background separation. In conjunction with texture-based inpainting, understanding the different see-through layers allows us to perform powerful image manipulations such as recovering a mesh-occluded background (as much as 53% occluded area) to achieve the effect of image and photo de-fencing. Our algorithm consists of three distinct phases- (1) automatically finding the skeleton structure of a potential frontal layer (fence) in the form of a deformed lattice, (2) separating foreground/background layers using appearance regularity, and (3) occluded foreground inpainting to reveal a complete, non-occluded image. Each of these three tasks presents its own special computational challenges that are not encountered in previous, general image de-layering or texture inpainting applications.",
Taser Blunt Probe Dart-To-Heart Distance Causing Ventricular Fibrillation in Pigs,"The maximum distance between the heart and a model Taser stimulation dart, called the dart-to-heart distance, at which the Taser can directly cause ventricular fibrillation (VF), was measured in pigs. A 9-mm-long blunt probe was advanced snugly through the surrounding tissues toward the heart. Five animals [pig mass = 61.2plusmn6.23 standard deviation (SD) kg] for ten dart-to-heart distances where the Taser caused VF were tested. The dart-to-heart distances where the Taser caused VF of the first stimulation site ranged from 4 to 8 mm with average 6.2 mmplusmn1.79 (SD) and of the second stimulation site ranged from 2 to 8 mm with average 5.4 mmplusmn2.41 (SD). The results help inform the evolving discussion of risks associated with Tasers.",
Single Photon Counting X-Ray Imaging System Using a Micro Hole and Strip Plate,"A Xe-based single photon counting X-ray imaging system, based on a micro-hole & strip plate (MHSP), using a resistive charge division method was implemented. The MHSP is a hybrid microstructure that combines in a single element the features of the gas electron multiplier (GEM) and the micro-strip gas chamber (MSGC), presenting two charge multiplication stages. For 2D-imaging, the GEM-like side (top side) is structured with strips orthogonal to the MSGC-like strips at the bottom side. Two thin orthogonal resistive lines of about 100 Omega, between the strips (top side) and between the anode strips (bottom side) allows us to obtain the actual position in both x and y directions. The four signals collected from the edges of the resistive lines are then digitized and processed with a computer. Charge gains of 104 are obtained with the MHSP. Position resolutions of sigmax = 130 mum and sigmay = 250 mum were achieved for 8-keV X-rays, making it suitable for many applications in single photon X-ray imaging. First X-ray images produced by the system are presented together with a discussion of the image quality and future prospects.",
CONVEX: Similarity-Based Algorithms for Forecasting Group Behavior,"A proposed framework for predicting a group's behavior associates two vectors with that group. The context vector tracks aspects of the environment in which the group functions; the action vector tracks the group's previous actions. Given a set of past behaviors consisting of a pair of these vectors and given a query context vector, the goal is to predict the associated action vector. To achieve this goal, two families of algorithms employ vector similarity. CONVEXk _NN algorithms use k-nearest neighbors in high-dimensional metric spaces; CONVEXMerge algorithms look at linear combinations of distances of the query vector from context vectors. Compared to past prediction algorithms, these algorithms are extremely fast. Moreover, experiments on real-world data sets show that the algorithms are highly accurate, predicting actions with well over 95-percent accuracy.","Finance,
Ontologies,
Intelligent systems,
Educational institutions,
Government,
Terrorism,
Semantic Web,
Resource description framework,
OWL,
Lead"
Continuous Content-Based Copy Detection over Streaming Videos,"Digital videos are increasingly adopted in various multimedia applications where they are usually broadcasted or transmitted as video streams. Continuously monitoring copies on the fast and long streaming videos is gaining attention due to its importance in content and rights management. The problem of video copies detection on video streams is complicated by two issues. First, original videos may be edited, with their frames being reordered, to avoid detection. Second, there are many concurrent video streams and for each stream, there could be many continuous video copy monitoring queries. Efficient data stream algorithms are therefore essential for processing a large number of continuous queries on video streams. In this paper, we first define video sequence similarity that is robust with respect to changes of videos, and a hash-based video sketch for efficient computation of sequence similarity. We then present a novel bit vector signature of the sketch to achieve two optimization objectives: CPU cost and memory requirement. Finally, in order to handle multiple continuous queries simultaneously, we design an index structure for the query sequences. We implemented the system and use real videos for the experimental study. Experimental results confirm the efficiency and effectiveness of our proposed techniques.","Streaming media,
Watermarking,
Digital video broadcasting,
Monitoring,
Computer science,
Video sequences,
Digital multimedia broadcasting,
Multimedia computing,
Application software,
Multimedia communication"
A Survey on MAC Protocols for Opportunistic Spectrum Access in Cognitive Radio Networks,"Opportunistic Spectrum Access (OSA) allows unlicensed users to share licensed spectrum in space and time with no or little interference to primary users, which bring new research challenges in MAC design. In this paper, we firstly introduce the differences between the conventional multi-channel MAC protocols and the MAC protocols for OSA as well as the requirements for OSA MAC protocols, and then analyze and compare the several MAC protocols proposed so far in the literature. At last we give the characters of these protocols.","Media Access Protocol,
Access protocols,
Cognitive radio,
Wireless networks,
Wireless application protocol,
Frequency,
Software engineering,
Interference,
Laboratories,
FCC"
A Method for Automated Web Service Selection,"Automated Web service selection is a key challenge in Service Oriented Architecture (SOA) research. With SOA becoming more popular in industry, and more Web services becoming available in the e-business market, the main issue changes from service discovery to service selection and ranking. In this paper, we propose a novel nonfunctional property-based service selection method by modifying the Logic Scoring Preference (LSP) method with Ordered Weighted Averaging (OWA) Operators. Moreover, the dynamic mechanism for evaluating metadata based QoS criteria of each service is presented for this method.","Web services,
meta data,
software architecture"
Kernel integral images: A framework for fast non-uniform filtering,"Integral images are commonly used in computer vision and computer graphics applications. Evaluation of box filters via integral images can be performed in constant time, regardless of the filter size. Although Heckbert [6] extended the integral image approach for more complex filters, its usage has been very limited, in practice. In this paper, we present an extension to integral images that allows for application of a wide class of non-uniform filters. Our approach is superior to Heckbert’s in terms of precision requirements and suitability for parallelization. We explain the theoretical basis of the approach and instantiate two concrete examples: filtering with bilinear interpolation, and filtering with approximated Gaussian weighting. Our experiments show the significant speedups we achieve, and the higher accuracy of our approach compared to Heckbert’s.","Kernel,
Filters,
Filtering,
Computer vision,
Histograms,
Application software,
Target tracking,
Computer science,
Computer graphics,
Interpolation"
Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality,"We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking methods. Distinctive image features of the scene are detected and tracked frame- to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a multi-threaded manner for capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce a user interaction for establishing a global coordinate system and for locating virtual objects in the AR environment. A user's bare hand is used for the user interface by estimating a camera pose relative to the user's outstretched hand. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments using hands for interaction.","Augmented reality,
Cameras,
Optical computing,
Image motion analysis,
Real time systems,
Computer architecture,
Layout,
Optical detectors,
Computer vision,
Rendering (computer graphics)"
Visualizing Incomplete and Partially Ranked Data,"Ranking data, which result from m raters ranking n items, are difficult to visualize due to their discrete algebraic structure, and the computational difficulties associated with them when n is large. This problem becomes worse when raters provide tied rankings or not all items are ranked. We develop an approach for the visualization of ranking data for large n which is intuitive, easy to use, and computationally efficient. The approach overcomes the structural and computational difficulties by utilizing a natural measure of dissimilarity for raters, and projecting the raters into a low dimensional vector space where they are viewed. The visualization techniques are demonstrated using voting data, jokes, and movie preferences.","Data visualization,
Voting,
Statistics,
Extraterrestrial measurements,
Motion pictures,
Multidimensional systems,
Search engines,
Professional societies,
Councils,
Computer science"
Connectivity-Guaranteed and Obstacle-Adaptive Deployment Schemes for Mobile Sensor Networks,"Mobile sensors can move and self-deploy into a network. While focusing on the problems of coverage, existing deployment schemes mostly over-simplify the conditions for network connectivity: they either assume that the communication range is large enough for sensors in geometric neighborhoods to obtain each other's locationby local communications, or assume a dense network that remains connected. At the same time, an obstacle-free field or full knowledge of the field layout is often assumed. We present new schemes that are not restricted by these assumptions, and thus adapt to a much wider range of application scenarios. While maximizing sensing coverage, our schemes can achieve connectivity for a network with arbitrary sensor communication/sensing ranges or node densities, at the cost of a small moving distance; the schemes do not need any knowledge of the field layout, which can be irregular and have obstacles/holes of arbitrary shape. Simulations results show that the proposed schemes achieve the targeted properties.","Sensors,
Chromium,
Floors,
Base stations,
Layout,
Distance measurement,
Pediatrics"
Nearest Neighbor Retrieval Using Distance-Based Hashing,"A method is proposed for indexing spaces with arbitrary distance measures, so as to achieve efficient approximate nearest neighbor retrieval. Hashing methods, such as Locality Sensitive Hashing (LSH), have been successfully applied for similarity indexing in vector spaces and string spaces under the Hamming distance. The key novelty of the hashing technique proposed here is that it can be applied to spaces with arbitrary distance measures, including non-metric distance measures. First, we describe a domain-independent method for constructing a family of binary hash functions. Then, we use these functions to construct multiple multibit hash tables. We show that the LSH formalism is not applicable for analyzing the behavior of these tables as index structures. We present a novel formulation, that uses statistical observations from sample data to analyze retrieval accuracy and efficiency for the proposed indexing method. Experiments on several real-world data sets demonstrate that our method produces good trade-offs between accuracy and efficiency, and significantly outperforms VP-trees, which are a well-known method for distance-based indexing.",
Managing Cancellations and No-Shows of Reservations with Overbooking to Increase Resource Revenue,"Advance reservation allows users to request available nodes in the future, whereas economy provides an incentive for resource owners to be part of the Grid, and encourages users to utilize resources optimally and effectively. In this paper, we use overbooking models from Revenue Management to manage cancellations and no-shows of reservations in a Grid system. Without overbooking, the resource owners are faced with a prospect of loss of income and lower system utilization. Thus, the models aim to find an ideal limit that exceeds the maximum capacity, without incurring greater compensation cost. Moreover, we introduce several novel strategies for selecting which bookings to deny, based on compensation cost and user class level, namely Lottery, Denied Cost First (DCF), and Lower Class DCF. The result shows that by overbooking reservations, a resource gains an extra 6-9% in the total net revenue.","Resource management,
Costs,
Grid computing,
Pricing,
Computer applications,
Processor scheduling,
Computer science,
Software,
Information science,
Computer networks"
Multivariate analysis of thalamo-cortical connectivity loss in TBI,"Diffusion tensor (DT) images quantify connectivity patterns in the brain while the T1 modality provides high-resolution images of tissue interfaces. Our objective is to use both modalities to build subject-specific, quantitative models of fiber connections in order to discover effects specific to a neural system. The health of this thalamo-cortical network is compromised by traumatic brain injury, and we hypothesize that these effects are due to a primary injury to the thalamus which results in subsequent compromise of radiating fibers. We first use a population-specific average T1 and DT template to label the thalamus and Brodmann areas (BA) 9,10 and 11 in each subject. We also build an expected connection model within this template space that is transferred to subject space in order to provide a prior restriction on probabilistic tracking performed in subject space. We evaluate the effect of traumatic brain injury on this prefrontal-thalamus network by quantifying, in 10 subjects and 8 controls, the mean diffusion and fractional anisotropy along fiber tracts, along with the mean diffusion within the thalamus and cortical regions. We contrast results gained by a template-based tract definition with those gained by performing analysis in the subject space. Both approaches reveal connectivity effects of TBI, specifically a region of reduced FA in the white matter connecting the thalamus to BA 10.","Brain modeling,
Brain injuries,
Diffusion tensor imaging,
Anisotropic magnetoresistance,
Performance analysis,
Image analysis,
Pattern analysis,
Brain computer interfaces,
Computer interfaces,
Computer networks"
Collapsible Pushdown Automata and Recursion Schemes,"Collapsible pushdown automata (CPDA) are a new kind of higher-order pushdown automata in which every symbol in the stack has a link to a stack situated somewhere below it. In addition to the higher-order push and pop operations, CPDA have an important operation called collapse, whose effect is to ""collapse"" a stack s to the prefix as indicated by the link from the topmost symbol of s. Our first result is that CPDA are equi-expressive with recursion schemes as generators of (possibly infinite) ranked trees.  In one direction, we give a simple algorithm that transforms an order-n CPDA to an order-n recursion scheme that generates the same tree, uniformly for all n ≫= 0. In the other direction, using ideas from game semantics, we give an effective transformation of order-n recursion schemes (not assumed to be homogeneously typed, and hence not necessarily safe) to order-n CPDA that compute traversals over an abstract syntax graph of the scheme, and hence paths in the tree generated by the scheme. Our equi-expressivity result is the first automata-theoretic characterization of higher-order recursion schemes. Thus CPDA are also a characterization of the simply-typed lambda calculus with recursion (generated from uninterpreted 1st-order symbols) and of (pure) innocent strategies.  An important consequence of the equi-expressivity result is that it allows us to reduce decision problems on trees generated by recursion schemes to equivalent problems on CPDA and vice versa. Thus we show, as a consequence of a recent result by Ong (modal mu-calculus model-checking of trees generated by recursion schemes is n-EXPTIME complete), that the problem of solving parity games over the configuration graphs of order-n CPDA is n-EXPTIME complete, subsuming several well-known results about the solvability of games over higher-order pushdown graphs by (respectively) Walukiewicz, Cachat, and Knapik et al. Another contribution of our work is a self-contained proof of the same solvability result by generalizing standard techniques in the field. By appealing to our equi-expressivity result, we obtain a new proof of Ong's result.  In contrast to higher-order pushdown graphs, we show that the monadic second-order theories of the configuration graphs of CPDA are undecidable. It follows that -- as generators of graphs -- CPDA are strictly more expressive than higher-order pushdown automata.","Automata,
Tree graphs,
Personal digital assistants,
Character generation,
Logic,
Computer science,
Calculus,
Laboratories,
Safety"
A New Storage Scheme for Approximate Location Queries in Object-Tracking Sensor Networks,"Energy efficiency is one of the most critical issues in the design of wireless sensor networks. Observing that many sensor applications for object tracking can tolerate a certain degree of imprecision in the location data of tracked objects, this paper studies precision-constrained approximate queries that trade answer precision for energy efficiency. We develop an energy-conserving approximate storage (EASE) scheme to efficiently answer approximate location queries by keeping error-bounded imprecise location data at some designated storage node. The data impreciseness is captured by a system parameter called the approximation radius. We derive the optimal setting of the approximation radius for our storage scheme based on the mobility pattern and devise an adaptive algorithm to adjust the setting when the mobility pattern is not available a priori or is dynamically changing. Simulation experiments are conducted to validate our theoretical analysis of the optimal approximation setting. The simulation results show that the proposed EASE scheme reduces the network traffic from a conventional approach by up to 96 percent and, in most cases, prolongs the network lifetime by a factor of 2-5.",
Continuous k-Means Monitoring over Moving Objects,"Given a data set P, a k-means query returns k points in space (called centers), such that the average squared distance between each point in P and its nearest center is minimized. Since this problem is NP-hard, several approximate algorithms have been proposed and used in practice. In this paper, we study continuous k-means computation at a server that monitors a set of moving objects. Reevaluating k-means every time there is an object update imposes a heavy burden on the server (for computing the centers from scratch) and the clients (for continuously sending location updates). We overcome these problems with a novel approach that significantly reduces the computation and communication costs, while guaranteeing that the quality of the solution, with respect to the reevaluation approach, is bounded by a user-defined tolerance. The proposed method assigns each moving object a threshold (i.e., range) such that the object sends a location update only when it crosses the range boundary. First, we develop an efficient technique for maintaining the k-means. Then, we present mathematical formulas and algorithms for deriving the individual thresholds. Finally, we justify our performance claims with extensive experiments.",
Group Interaction Analysis in Dynamic Context,"Computer understanding of human actions and interactions is one of the key research issues in human computing. In this regard, context plays an essential role in semantic understanding of human behavioral and social signals from sensor data. This paper put forward an event-based dynamic context model to address the problems of context awareness in the analysis of group interaction scenarios. Event-driven multilevel dynamic Bayesian network is correspondingly proposed to detect multilevel events, which underlies the context awareness mechanism. Online analysis can be achieved, which is superior over previous works. Experiments in our smart meeting room demonstrate the effectiveness of our approach.","Humans,
Context modeling,
Intelligent sensors,
Context awareness,
Bayesian methods,
Event detection,
Pervasive computing,
Multimodal sensors,
Fluid dynamics,
Broadcasting"
Performance breakthrough in 8 nm gate length Gate-All-Around nanowire transistors using metallic nanowire contacts,"Parasitic S/D resistances in extremely scaled GAA nanowire devices can pathologically limit the device drive current performance. We demonstrate for the first time, that S/D extension dopant profile engineering together with successful integration of low resistivity metallic nanowire contacts greatly reduces parasitic resistances. This allows 8 nm gate length GAA nanowire devices in this work to attain record-high drive currents of 3740 μA/μm.","Logic gates,
Conductivity,
Nickel,
Silicides,
Silicon,
Resistance,
Nanoscale devices"
Base Station Association Game in Multi-Cell Wireless Networks (Special Paper),"We consider a multi-cell wireless network with a large number of users. Each user selfishly chooses the Base Station (BS) that gives it the best throughput (utility), and each BS allocates its resource by some simple scheduling policy. First we consider two cases: (1) BS allocates the same time to its users; (2) BS allocates the same throughput to its users. It turns out that, combined with users' selfish behavior, case (1) results in a single Nash Equilibrium (NE), which achieves system-wide Proportional Fairness. On the other hand, case (2) results in many possible Nash Equilibria, some of which are very inefficient. Next, we extend (1) to the case where the users have general concave utility functions. It is shown that the if each BS performs intra- cell optimization, the total utility of all users is maximized at NE. This suggests that under our model, the task of joining the "";correct""; BS can be left to individual users, leading to a distributed solution.",
Noise Correction on Rician Distributed Data for Fibre Orientation Estimators,"New complex tissue microstructure estimators have been presented recently in order to elucidate white matter fibre orientations. Since these algorithms are based on the diffusion- weighted signal profile, the estimations are affected by noise artefacts. The proven robustness of these methods cannot counteract distortions since the statistical Rician behavior has not been taken into account. In this study, two techniques to counteract the noise distortions are presented to improve the fibre orientation estimations. Simulations and in vivo experiments show an improvement in the angular resolution and convergence of the results. One of these strategies represents a good compromise between computational cost and result improvements.",
The Research of Conflict-Detection Algorithm of Concurrency Control Based on Rough Set,"Concurrency control is one of the key factors in computer supported cooperative work. Through researching conflict of concurrency control mechanism and CSCW, this paper introduces the rough set into the conflict-detection, and forms a new algorithm idea. Also, the paper analysis the feasibility of the new algorithm and gives an example.","Concurrency control,
Knowledge representation,
Classification algorithms,
Collaborative work,
Decision making,
Algorithm design and analysis,
Set theory"
A novel automatic seed point selection algorithm for breast ultrasound images,"Region growing is a frequently used segmentation method for medical ultrasound images processing. The first step of region growing is selecting the seed point which is inside the breast lesion. Most of the region growing methods require manually selecting the seed point which needs human interaction. To make the segmentation completely automatic, we propose a new automatic seed point selecting method for region growing algorithm. The method is validated on our database with 105 ultrasound images with breast masses and it is compared with other automatic seed point selecting method on the same database. Quantitative experiment results show that our proposed method can successfully find the proper seed points for 95.2% of the US images in the database which is much more robust than other automatic seed point selection methods.",
Virtual Training via Vibrotactile Arrays,"What is often missing from many virtual worlds and training simulations is a physical sense of the confinement and constraint of the virtual environment. We present a method for providing localized cutaneous vibratory feedback to the user's right arm. We created a sleeve of tactors linked to a real-time human model; the tactors activate to apply sensation to the corresponding body area. The hypothesis is that vibrotactile feedback to body areas provides the wearer sufficient guidance to assume correct body configurations and ascertain the existence and physical realism of access paths. We present the results of human subject experiments that study both explicit and implicit training of skills using vibrotactile arrays. Implicitly, collision awareness is achieved by activating the appropriate tactor when a body part collides with the scene; thus, the user will attempt to correct his or her body configuration. Explicitly, we use the tactors to guide the body into the proper configuration. The results of human subject experiments clearly show that the use of full arm vibrotactile feedback improves performance over purely visual feedback for navigating the virtual environment, as well as allowing easy acquisition of new skills. These results validate the empirical performance of this concept.",
Algorithms for Integrated Routing and Scheduling for Aggregating Data from Distributed Resources on a Lambda Grid,"In many e-science applications, there exists an important need to aggregate information from data repositories distributed around the world. In an effort to better link these resources in a unified manner, many lambda-grid networks, which provide end-to-end dedicated optical-circuit-switched connections, have been investigated. In this context, we consider the problem of aggregating files from distributed databases at a (grid) computing node over a lambda grid. The challenge is (1) to identify routes (that is, circuits) in the lambda-grid network, along which files should be transmitted, and (2) to schedule the transfers of these files over their respective circuits. To address this challenge, we propose a hybrid approach that combines offline and online scheduling. We define the Time-Path Scheduling Problem (TPSP) for offline scheduling. We prove that TPSP is NP-complete, develop a Mixed Integer Linear Program (MILP) formulation for TPSP, and then propose a greedy approach to solve TPSP because the MILP does not scale well. We compare the performance of the greedy approach on a few representative lambda-grid network topologies. One key input to the offline schedule is the file transfer time. Due to dynamics at the receiving end host, which is hard to model precisely, the actual file transfer time may vary. We first propose a model for estimating the file transfer time. Then, we propose online reconfiguration algorithms so that as files are transferred, the offline schedule may be modified online, depending on the amount of time that it actually took to transfer the file. This helps in reducing the total time to transfer all the files, which is an important metric. To demonstrate the effectiveness of our approach, we present results on an emulated lambda-grid network testbed.",
A potential field approach to dexterous tactile exploration of unknown objects,"Haptic exploration of unknown objects is of great importance for acquiring multi-modal object representations, which enable a humanoid robot to autonomously execute grasping and manipulation tasks. In this paper we present a tactile exploration strategy to guide an anthropomorphic five-finger hand along the surface of previously unknown objects and build a 3D object representation based on acquired tactile point clouds. The proposed strategy makes use of the dynamic potential field approach suggested in the context of mobile robot navigation. To demonstrate the capabilities of this strategy, we conduct experiments in a detailed physics simulation using a model of the five-finger hand. Exploration results of several test objects are given.","Humanoid robots,
Mobile robots,
Navigation,
Shape,
Manipulator dynamics,
Grasping,
Robot sensing systems,
Fingers,
Robot control,
Computer science"
Shape-Driven Three-Dimensional Watersnake Segmentation of Biological Membranes in Electron Tomography,"Due to the significant complexity of membrane morphology and the generally poor image quality in electron tomographic volumes, current automatic methods for segmentation of membranes perform poorly. Users must resort to manual tracing of recognized patterns on 2-D slices of the volume, a method that suffers from subjectivity and is very labor intensive, preventing quantitative analyses of tomographic data that require comparative analyses of many volumes. To overcome these limitations, we develop an automatic 3-D segmentation method that fully exploits the prior knowledge about the shape of the membranes as well as the 3-D information provided by the tomograms, and systematically combines this knowledge with the image data to improve segmentation results. The method is based on the watersnake framework. By mathematically reformulating the traditional watershed segmentation as an energy minimization problem, the watersnake inherits the many strengths of the watershed method while overcoming the limitations of the traditional energy-based segmentation methods. In our previous work (H. Nguyen et al., 2003), the original watersnake model was successfully modified by incorporating smoothness into watershed segmentation. In this work, we further extend that model to incorporate into the energy function various constraints representing our prior knowledge about the global shape of the cellular features to be segmented. Segmentation can, therefore, be accomplished via minimization of the energy function subject to the shape prior constraints. Finally, the mathematical framework is further extended from 2-D to 3-D so that segmentation can be carried out in 3-D to take advantage of the additional information provided by the tomograms. We apply this method for the automatic extraction of biological membranes of varying complexities including those of bacterial walls and mitochondrial boundaries.","Biomembranes,
Electrons,
Tomography,
Image segmentation,
Shape,
Pattern analysis,
Morphology,
Image quality,
Pattern recognition,
Data analysis"
Mathematical Analysis of Secondary User Traffic in Cognitive Radio System,"In this paper, a preemptive priority queueing model is developed to describe the traffic behavior of secondary users in cognitive radio systems. As the performance measures, the system dwelling time for non-real-time traffic and the blocking and forced termination probabilities for real-time traffic are derived.","Servers,
Mathematical model,
Real time systems,
Cognitive radio,
Time frequency analysis,
Electronic mail,
Load modeling"
Fast arbiters for on-chip network switches,"The need for efficient implementation of simple crossbar schedulers has increased in the recent years due to the advent of on-chip interconnection networks that require low latency message delivery. The core function of any crossbar scheduler is arbitration that resolves conflicting requests for the same output. Since, the delay of the arbiters directly determine the operation speed of the scheduler, the design of faster arbiters is of paramount importance. In this paper, we present a new bit-level algorithm and new circuit techniques for the design of programmable priority arbiters that offer significantly more efficient implementations compared to already-known solutions. From the experimental results it is derived that the proposed circuits are more than 15% faster than the most efficient previous implementations, which under equal delay comparisons, translates to 40% less energy.","Network-on-a-chip,
Switches,
Fabrics,
Virtual colonoscopy,
Processor scheduling,
Delay,
Circuits,
Routing,
Packet switching,
Buffer storage"
Finite-resolution digital receiver design for impulse radio ultra-wideband communication,"Receiver design for impulse radio based ultrawideband (UWB) communication is a challenge. High sampling rate high resolution digital receiver is usually difficult to implement. Some tradeoffs can be made on the digital receiver, such as limiting amplitude resolution to only one bit, which results in a previously considered monobit receiver. In this paper, we consider the design of finite-resolution digital UWB receivers. We derive the optimal post-quantization processing, and analyze the achievable bit-error rate performance using an approximation of the log-likelihood ratio. Optimal thresholds for 4- and 3-level quantization are obtained. Training-based receiver template estimation is presented. Our work discloses the incremental gain that additional quantization levels offer and the results provide useful guidelines for designing impulse radio UWB digital receivers.","Ultra wideband technology,
Matched filters,
Sampling methods,
Bit error rate,
RAKE receivers,
Quantization,
AWGN,
Performance analysis,
Signal resolution,
Signal to noise ratio"
NoCOUT : NoC topology generation with mixed packet-switched and point-to-point networks,"Networks-on-Chip (NoC) have been widely proposed as the future communication paradigm for use in next-generation System-on-Chip. In this paper, we present NoCOUT, a methodology for generating an energy optimized application specific NoC topology which supports both point-to-point and packet-switched networks. The algorithm uses a prohibitive greedy iterative improvement strategy to explore the design space efficiently. A system-level floorplanner is used to evaluate the iterative design improvements and provide feedback on the effects of the topology on wire length. The algorithm is integrated within a NoC synthesis framework with characterized NoC power and area models to allow accurate exploration for a NoC router library. We apply the topology generation algorithm to several test cases including real-world and synthetic communication graphs with both regular and irregular traffic patterns, and varying core sizes. Since the method is iterative, it is possible to start with a known design to search for improvements. Experimental results show that many different applications benefit from a mix of “on chip networks” and “point-to-point networks”. With such a hybrid network, we achieve approximately 25% lower energy consumption (with a maximum of 37%) than a state of the art min-cut partition based topology generator for a variety of benchmarks. In addition, the average hop count is reduced by 0.75 hops, which would significantly reduce the network latency.","Network-on-a-chip,
Network topology,
Iterative algorithms,
Space exploration,
Next generation networking,
System-on-a-chip,
Optimization methods,
Algorithm design and analysis,
Feedback,
Wire"
Simulating the influence of IVC on road traffic using bidirectionally coupled simulators,"We discuss the need for bidirectional coupling of network simulation and road traffic microsimulation for evaluating Vehicular Ad Hoc Networks (VANETs) in a simulation framework. As the selection of a mobility model influences the outcome of simulations to a great deal, the use of a representative model is necessary for producing meaningful evaluation results. Based on these observations, we present a hybrid simulation framework composed of the road traffic simulator SUMO and the network simulator OMNeT++. The coupled simulation environment is used for an evaluation of two protocols for incident warning in VANETs.","Roads,
Traffic control,
Telecommunication traffic,
Protocols,
Computational modeling,
Computer simulation,
Mobile ad hoc networks,
Vehicles,
Computer networks,
Ad hoc networks"
Power Monitoring and Control for Electric Home Appliances Based on Power Line Communication,"Home power consumption tends to grow in proportion to the increase in the number of large-sized electric home appliances. An embedded system without any new additional wiring has been developed for home power management. By using Power Line Communication (PLC) technology, electric home appliances can be controlled and monitored through domestic power lines. We describe a PPCOM (PLC Power-Controlled Outlet Module) which integrates the multiple AC power sockets, the power measuring module, the PLC module and a microcontroller into a power outlet to switch the power of the sockets on/off and to measure the power consumption of plugged-in electric home appliances. We have also designed an embedded home server which supports the Web page user interface, thus allowing the user to easily control and monitor the electric home appliances by means of the Internet. In addition, the field experiments reported have demonstrated that our design can be practically implemented and provides adequate results.",
An Evolutionary Particle Swarm Optimization algorithm for data clustering,"Clustering is an important data mining task and has been explored extensively by a number of researchers for different application areas such as finding similarities in images, text data and bio-informatics data. Various optimization techniques have been proposed to improve the performance of clustering algorithms. In this paper we propose a novel algorithm for clustering that we call Evolutionary Particle Swarm Optimization (EPSO)-clustering algorithm which is based on PSO. The proposed algorithm is based on the evolution of swarm generations where the particles are initially uniformly distributed in the input data space and after a specified number of iterations; a new generation of the swarm evolves. The swarm tries to dynamically adjust itself after each generation to optimal positions. The paper describes the new algorithm the initial implementation and presents tests performed on real clustering benchmark data. The proposed method is compared with k-means clustering- a benchmark clustering technique and simple particle swarm clustering algorithm. The results show that the algorithm is efficient and produces compact clusters.",
Service-Oriented Framework for Human Task Support and Automation,"Due to increasingly demanding requirements for business flexibility and agility, automation of end-to-end industrial processes has become an important topic. Business process execution needs to support automated tasks execution as well as human tasks. In this paper we show that for certain types of human tasks it is relevant to consider their further automation. We propose a service-oriented architectural framework for human task execution, which improves their execution by automating and semi-automating decision making based on ontologies and agent technology. The approach is generic and can be used for any type of industrial or industrial support business process. As a proof-of-concept we have developed a system providing the above-described support for human task intensive business processes in an electric power transmission company, which has shown considerable improvements in the efficiency of human tasks.",
DCA for bot detection,"Ensuring the security of computers is a non-trivial task, with many techniques used by malicious users to compromise these systems. In recent years a new threat has emerged in the form of networks of hijacked zombie machines used to perform complex distributed attacks such as denial of service and to obtain sensitive data such as password information. These zombie machines are said to be infected with a ‘hot’ - a malicious piece of software which is installed on a host machine and is controlled by a remote attacker, termed the ‘botmaster of a botnet’. In this work, we use the biologically inspired Dendritic Cell Algorithm (DCA) to detect the existence of a single hot on a compromised host machine. The DCA is an immune-inspired algorithm based on an abstract model of the behaviour of the dendritic cells of the human body. The basis of anomaly detection performed by the DCA is facilitated using the correlation of behavioural attributes such as keylogging and packet flooding behaviour. The results of the application of the DCA to the detection of a single hot show that the algorithm is a successful technique for the detection of such malicious software without responding to normally running programs.",
A new quality of bone ultrasound research,"Quantitative ultrasound (QUS) methods have strong power to predict osteoporotic fractures, but they are also very relevant for the assessment of bone quality. A representative sample of recent studies addressing these topics can be found in this special issue. Further pursuit of these methods will establish micro-QUS imaging methods as tools for measuring specific aspects of bone quality. Once this is achieved, we will be able to link such data to the clinical QUS methods used in vivo to determine which aspects of bone quality cause QUS to be a predictor of fracture risk that is independent of bone mineral density (BMD). Potentially this could lead to the development of a new generation of QUS devices for improved and expanded clinical assessment. Good quality of basic science work will thus lead to good quality of clinical patient examinations on the basis of a more detailed assessment of bone quality.",
A Real-Time 2-D Vector Doppler System for Clinical Experimentation,"A real-time hardware software 2-D vector Doppler system has been realized by means of the FEMMINA platform. The system operates by performing two independent 1-D Doppler estimations on the scan plane of a linear array probe along different directions; the probe is connected to a commercial scanner. The reconstructed velocity is presented in real-time as superposition on the conventional B-mode images. Two different scanning techniques have been implemented, in order to carry out the 2-D Doppler investigation in the area of interest. These techniques allow to use the system both in vivo and in vivo. An extensive set of simulations has been performed in order to establish a gold standard regarding vector Doppler 2-D techniques, and to be able to assess the performance of the 2-D Doppler system by comparing simulated and experimental results. The whole real-time 2-D vector Doppler system is fully certified as hospital equipment, and thus it can be employed to carry out an experimental characterization of the 2-D Doppler technique in the clinical environment.","Real time systems,
Probes,
In vivo,
Doppler measurements,
Ultrasonic variables measurement,
Blood flow,
Apertures,
Hardware,
Vectors,
Image reconstruction"
On distributed averaging algorithms and quantization effects,"We consider distributed iterative algorithms for the averaging problem over time-varying topologies. Our focus is on the convergence time of such algorithms when complete (unquantized) information is available, and on the degradation of performance when only quantized information is available. We study a large and natural class of averaging algorithms, which includes the vast majority of algorithms proposed to date, and provide tight polynomial bounds on their convergence time. We then propose and analyze distributed averaging algorithms under the additional constraint that agents can only store and communicate quantized information. We show that these algorithms converge to the average of the initial values of the agents within some error. We establish bounds on the error and tight bounds on the convergence time, as a function of the number of quantization levels.","Quantization,
Convergence,
Iterative algorithms,
Algorithm design and analysis,
Polynomials,
Distributed control,
Topology,
Degradation,
Information analysis,
Large-scale systems"
Safe Diagnosability of Stochastic Discrete Event Systems,"Recently, safe diagnosability of discrete event systems (DESs) was investigated by Paoli and Lafortune, which was viewed as the first necessary step of fault-tolerant supervision. In this paper, we consider the problem of safe diagnosability in the framework of stochastic discrete event systems (SDESs). We define the notion of safe diagnosability for stochastic automata, in which fault detection occurs before any given forbidden string in the failed mode of system is executed. The relationship between diagnosability and safe diagnosability for SDESs is analyzed. In particular, a necessary and sufficient condition for safe diagnosability of SDESs is presented by constructing the recognizer of illegal language and the safe diagnoser. Some examples are described to illustrate the results.",
Message Complexity Analysis of Mobile Ad Hoc Network Address Autoconfiguration Protocols,"This paper proposes a novel method to perform a quantitative analysis of message complexity and applies this method in comparing the message complexity among the mobile ad hoc network (MANET) address autoconfiguration protocols (AAPs). The original publications on the AAPs had incomplete parts, making them insufficient to use on practical MANETs. Therefore, the first objective of the research was to complete the AAPs by filling in the missing gaps to make them operational. The missing procedures that were filled in have been developed based on the most logical procedures being accurate to the original protocol publications. The research in this paper finds applications in wireless networks that apply reduced addresses to achieve less memory usage, smaller overhead, and higher throughput (for example, the IPv6 low-power wireless personal address network (6LoWPAN)), but, as a result, possess a high address duplication probability. This research consists of two cases, where the first case deals with the message complexity analysis of the single-node joining case (SJC) and the second case deals with the complexity analysis of the MANET group merging case (GMC).",
A bi-illuminant dichromatic reflection model for understanding images,"This paper presents a new model for understanding the appearance of objects that exhibit both body and surface reflection under realistic illumination. Specifically, the model represents the appearance of surfaces that interact with a dominant illuminant and a non-negligible ambient illuminant that may have different spectral power distributions. Real illumination environments usually have an ambient illuminant, and the current dynamic range of consumer cameras is sufficient to capture significant information in shadows. The bi-illuminant dichromatic reflection model explains numerous empirical findings in the literature and has implications for commonly used chromaticity spaces that claim to be illumination invariant but are not in many natural situations. One outcome of the model is the first 2-D chromaticity space for an RGB image that is robust to illumination change given dominant and ambient illuminants with different spectral power distributions.",
Sparse incremental learning for interactive robot control policy estimation,"We are interested in transferring control policies for arbitrary tasks from a human to a robot. Using interactive demonstration via teleoperation as our transfer scenario, we cast learning as statistical regression over sensor-actuator data pairs. Our desire for interactive learning necessitates algorithms that are incremental and realtime. We examine Locally Weighted Projection Regression, a popular robotic learning algorithm, and Sparse Online Gaussian Processes in this domain on one synthetic and several robot-generated data sets. We evaluate each algorithm in terms of function approximation, learned task performance, and scalability to large data sets.","Robot control,
Function approximation,
Robot sensing systems,
Gaussian processes,
Ground penetrating radar,
Robotics and automation,
Human robot interaction,
Robot programming,
Educational robots,
Machine learning algorithms"
Bipartite Modular Multiplication Method,"This paper proposes a new modular multiplication method that uses Montgomery residues defined by a modulus M and a Montgomery radix R whose value is less than the modulus M. This condition enables the operand multiplier to be split into two parts that can be processed separately in parallel - increasing the calculation speed. The upper part of the split multiplier can be processed by calculating a product modulo M of the multiplicand and this part of the split multiplier. The lower part of the split multiplier can be processed by calculating a product modulo M of the multiplicand, this part of the split multiplier, and the inverse of a constant R. Two different implementations based on this method are proposed: One uses a classical modular multiplier and a Montgomery multiplier and the other generates partial products for each part of the split multiplier separately, which are added and accumulated in a single pipelined unit. A radix-4 version of a multiplier based on a radix-4 classical modular multiplier and a radix-4 Montgomery multiplier has been designed and simulated. The proposed method is also suitable for software implementation in a multiprocessor environment.",
Practical camera auto-calibration based on object appearance and motion for traffic scene visual surveillance,"Camera calibration, as a fundamental issue in computer vision, is indispensable in many visual surveillance applications. Firstly, calibrated camera can help to deal with perspective distortion of object appearance on image plane. Secondly, calibrated camera makes it possible to recover metrics from images which are robust to scene or view an gle changes. In addition, with calibrated cameras, we can make use of prior information of 3D models to estimate 3D pose of objects and make object detection or tracking more robust to noise and occlusions. In this paper, we propose an automatic method to recover camera models from traffic scene surveillance videos. With only the camera height H measured, we can completely recover both intrinsic and extrinsic parameters of cameras based on appearance and motion of objects in videos. Experiments are conducted in different scenes and experimental results demonstrate the effectiveness and practicability of our approach, which can be adopted in many traffic scene surveillance applications.","Cameras,
Layout,
Surveillance,
Noise robustness,
Videos,
Calibration,
Computer vision,
Application software,
Object detection,
Traffic control"
A Model Driven Approach to Represent Sequence Diagrams as Free Choice Petri Nets,"Model Driven Development (MDD) aims to promote the role of modeling in Software Engineering. Enterprise systems and architectures are often modeled via multiple representations. For example UML models are widely used by the designers to capture various viewpoint of the system; while formal models using languages such as CSP, Z and Petri Nets are suitable for the analysis. Model transformation techniques developed as a part of MDD can be applied to generate one model from another model automatically. This allows benefiting from the tools and techniques developed and used in multiple languages. This paper presents a method of applying MDD model transformation from UML 2.0 Sequence Diagrams to Petri Nets. The paper shows that the model transformation results in Free Choice Petri Nets. As a result, the low complexity of analysis and the synthesis techniques can be applied to the models of enterprise systems which are captured in UML Sequence Diagrams.",
Monocular pedestrian recognition using motion parallax,"This paper presents a novel focus-of-attention strategy for monocular pedestrian recognition. It uses Bayes’ rule to estimate the posterior for the presence of a pedestrian in a certain (rectangular) image region, based on motion parallax features. This posterior is used as a parameter to control the amount of regions of interest (ROIs) that is passed to subsequent verification stages. For the latter, we use a state-of-the-art pedestrian recognition scheme which consists of multiple modules in a cascade architecture. We obtain optimized settings for the control parameters of the combined cascade system by a sequential ROC convex hull technique. Experiments are conducted on image data captured from a moving vehicle in an urban environment. We demonstrate that the proposed focus-of-attention strategy reduces the false positives of an otherwise identical monocular pedestrian recognition system by a factor of two, at equal detection rates. The overall system maintains processing rates close to real-time.",
"The past, present, and future of software evolution","Change is an essential characteristic of software development, as software systems must respond to evolving requirements, platforms, and other environmental pressures. In this paper, we discuss the concept of software evolution from several perspectives. We examine how it relates to and differs from software maintenance. We discuss insights about software evolution arising from Lehman’s laws of software evolution and the staged lifecycle model of Bennett and Rajlich. We compare software evolution to other kinds of evolution, from science and social sciences, and we examine the forces that shape change. Finally, we discuss the changing nature of software in general as it relates to evolution, and we propose open challenges and future directions for software evolution research.","Evolution (biology),
Software,
Maintenance engineering,
Biology,
Software systems,
Software maintenance,
Biological information theory"
KPC-Toolbox: Simple Yet Effective Trace Fitting Using Markovian Arrival Processes,"We present the KPC-Toolbox, a collection of MATLAB scripts for fitting workload traces into Markovian arrival processes (MAPs) in an automatic way. We first present detailed sensitivity analysis that builds intuition on which trace descriptors are most important for queueing. This sensitivity analysis stresses the importance of matching higher-order correlations (i.e., joint moments) of the process inter-arrival times rather than higher order moments of the distribution and provides guidance on the relative importance of different descriptors on queueing. Given that the MAP parameterization space can be very large, we focus on first determining the order of the smallest MAP that can fit the trace well, using the Bayesian information criterion (BIC) for determining the best order-accuracy tradeoff. Having determined the order of the target MAP, the KPC-Toolbox automatically derives a MAP that captures accurately the most essential features of the trace. Extensive experimentation illustrates the effectiveness of the KPC-Toolbox in fitting traces that are well-documented in the literature as very challenging to fit, showing that the KPC-Toolbox provides a simple and powerful solution to fitting accurately trace data into MAPs.",
Implementing an Inference Engine for RDFS/OWL Constructs and User-Defined Rules in Oracle,"This Inference Engines are an integral part of Semantic Data Stores. In this paper, we describe our experience of implementing a scalable inference engine for Oracle Semantic Data Store. This inference engine computes production rule based entailment of one or more RDFS/OWL encoded semantic data models. The inference engine capabilities include i) inferencing based on semantics of RDFS/OWL constructs and user-defined rules, ii) computing ancillary information (namely, semantic distance and proof) for inferred triples, and iii) validation of semantic data model based on RDFS/OWL semantics. A unique aspect of our approach is that the inference engine is implemented entirely as a database application on top of Oracle Database. The paper describes the inferencing requirements, challenges in supporting a sufficiently expressive set of RDFS/OWL constructs, and techniques adopted to build a scalable inference engine. A performance study conducted using both native and synthesized semantic datasets demonstrates the effectiveness of our approach.","Engines,
Resource description framework,
OWL,
Databases,
Vocabulary,
Data models,
Terminology,
Ontologies,
Scalability,
Production"
Surface Reconstruction for Free-Space 360 ^{\circ} Fluorescence Molecular Tomography and the Effects of Animal Motion,"Complete projection (360deg) free-space fluorescence tomography of opaque media is poised to enable 3-D imaging through entire small animals in vivo with improved depth resolution compared to 360deg-projection fiber-based systems or limited-view angle systems. This approach can lead to a new generation of fluorescence molecular tomography (FMT) performance since it allows high spatial sampling of photon fields propagating through tissue at any projection, employing nonconstricted animal surfaces. Herein, we employ a volume carving method to capture 3-D surfaces of diffusive objects and register the captured surface in the geometry of an FMT 360deg-projection acquisition system to obtain 3-D fluorescence image reconstructions. Using experimental measurements we evaluate the accuracy of the surface capture procedure by reconstructing the surfaces of phantoms of known dimensions. We then employ this methodology to characterize the animal movement of anaesthetized animals. We find that the effects of animal movement on the FMT reconstructed image were within system resolution limits (~0.07 cm).",
3-D gesture-based scene navigation in medical imaging applications using Time-of-Flight cameras,"For a lot of applications, and particularly for medical intra-operative applications, the exploration of and navigation through 3-D image data provided by sensors like ToF (Time-of-Flight) cameras, MUSTOF (Multisensor-Time-of-Flight) endoscopes or CT (Computed Tomography) [8], requires a user-interface which avoids physical interaction with an input device. Thus, we process a touchless user-interface based on gestures classified by the data provided by a ToF camera. Reasonable and necessary user interactions are described. For those interactions a suitable set of gestures is introduced. A user-interface is then proposed, which interprets the current gesture and performs the assigned functionality. For evaluating the quality of the developed user-interface we considered the aspects of classification rate, real-time applicability, usability, intuitiveness and training time. The results of our evaluation show that our system, which provides a classification rate of 94.3% at a framerate of 11 frames per second, satisfactorily addresses all these quality requirements.","Layout,
Navigation,
Biomedical imaging,
Cameras,
Application software,
Surges,
Computed tomography,
Speech recognition,
Data gloves,
Biomedical optical imaging"
The Bounded-Storage Model in the Presence of a Quantum Adversary,"An extractor is a function that is used to extract randomness. Given an imperfect random source X and a uniform seed Y, the output E(X,Y) is close to uniform. We study properties of such functions in the presence of prior quantum information about X, with a particular focus on cryptographic applications. We prove that certain extractors are suitable for key expansion in the bounded-storage model where the adversary has a limited amount of quantum memory. For extractors with one-bit output we show that the extracted bit is essentially equally secure as in the case where the adversary has classical resources. We prove the security of certain constructions that output multiple bits in the bounded-storage model.",
Green transistor as a solution to the IC power crisis,IC power consumption is not only a package thermal issue but also a significant and fast growing part of the world electricity consumption. A new low voltage transistor could contribute greatly to the need for a new Vdd scaling scenario. Green transistor (gFET) is based on tunneling and provides Ion and Ioff far superior to MOSFET at 0.2V if suitable low-Eg material is introduced into IC manufacturing.,
On Skylining with Flexible Dominance Relation,"Given a set of d dimensional objects, a skyline query finds the objects (""skyline"") that are not dominated by others. However, skylines do not always provide useful query results to users, and existing methods of various skyline queries have at least one of the following drawbacks: (1) the size of skyline objects can not be controlled, or can be only increased or only decreased but not both; (2) skyline objects do not have built-in ranks; (3) skylines do not reflect users' weights (preferences) at different dimensions. In this paper, we propose a unified approach, the ¿-skyline, to effectively solve all three drawbacks. We explore the properties of ¿-skylines and propose two different algorithms to compute ¿-skylines.","Size control,
Information science,
Computer science,
Control systems"
Singularity-Free Dynamic Equations of Open-Chain Mechanisms With General Holonomic and Nonholonomic Joints,"Standard methods to model multibody systems are aimed at systems with configuration spaces isomorphic to Ropfn. This limitation leads to singularities and other artifacts in case the configuration space has a different topology, for example, in the case of ball joints or a free-floating mechanism. This paper discusses an extension of classical methods that allows for a more general class of joints, including all joints with a Lie group structure as well as nonholonomic joints. The model equations are derived using the Boltzmann-Hamel equations and have very similar structure and complexity as obtained using classical methods. However, singularities are avoided through the use of global non-Euclidean configuration coordinates, together with mappings describing a local Euclidean structure around each configuration. The resulting equations are explicit (unconstrained) differential equations, both for holonomic and nonholonomic joints, which do not require a coordinate atlas and can be directly implemented in simulation software.",
Inferring spatial layout from a single image via depth-ordered grouping,"Inferring the 3D spatial layout from a single 2D image is a fundamental visual task. We formulate it as a grouping problem where edges are grouped into lines, quadrilaterals, and finally depth-ordered planes. We demonstrate that the 3D structure of planar objects in indoor scenes can be fast and accurately inferred without any learning or indexing.","Layout,
Computer science,
Visualization,
Image reconstruction,
Cameras,
Surface reconstruction,
Educational institutions,
Indexing,
Image recognition,
Navigation"
Online learning of patch perspective rectification for efficient object detection,"For a large class of applications, there is time to train the system. In this paper, we propose a learning-based approach to patch perspective rectification, and show that it is both faster and more reliable than state-of-the-art ad hoc affine region detection methods. Our method performs in three steps. First, a classifier provides for every keypoint not only its identity, but also a first estimate of its transformation. This estimate allows carrying out, in the second step, an accurate perspective rectification using linear predictors. We show that both the classifier and the linear predictors can be trained online, which makes the approach convenient. The last step is a fast verification - made possible by the accurate perspective rectification - of the patch identity and its sub-pixel precision position estimation. We test our approach on real-time 3D object detection and tracking applications. We show that we can use the estimated perspective rectifications to determine the object pose and as a result, we need much fewer correspondences to obtain a precise pose estimation.",
A Comparison Between a Time Domain and Continuous Wave Small Animal Optical Imaging System,"We present a phantom study to evaluate the performance of the eXplore Optix (Advanced Research Technologies-GE Healthcare), the first commercially available time-domain tomography system for small animal fluorescence imaging, and compare its capabilities with the widely used IVIS 200 (Xenogen Corporation-Caliper) continuous wave planar imaging system. The eXplore Optix, based on point-wise illumination and collection scheme, is found to be a log order more sensitive with significantly higher detection depth and spatial resolution as compared with the wide-area illumination IVIS 200 under the conditions tested. A time-resolved detection system allows the eXplore Optix to measure the arrival time distribution of fluorescence photons. This enables fluorescence lifetime measurement, absorption mapping, and estimation of fluorescent inclusion depth, which in turn is used by a reconstruction algorithm to calculate the volumetric distribution of the fluorophore concentration. An increased acquisition time and lack of ability to image multiple animals simultaneously are the main drawbacks of the eXplore Optix as compared with the IVIS 200.","Animals,
Optical imaging,
Fluorescence,
High-resolution imaging,
Lighting,
Lifetime estimation,
Imaging phantoms,
Medical services,
Time domain analysis,
Tomography"
Notary: Hardware techniques to enhance signatures,"Hardware signatures have been recently proposed as an efficient mechanism to detect conflicts amongst concurrently running transactions in transactional memory systems (e.g., Bulk, LogTM-SE, and SigTM). Signatures use fixed hardware to represent an unbounded number of addresses, but may lead to false conflicts (detecting a conflict when none exists). Previous work recommends that signatures be implemented with parallel Bloom filters with two or four hash functions (e.g., H3).","Hardware,
Filters,
Lead,
Privatization,
Sun,
Programming profession,
Parallel programming,
Testing,
Concurrent computing,
Entropy"
MEMS based bioelectronic neuromuscular interfaces for insect cyborg flight control,This paper reports the first direct control of insect flight by manipulating the wing motion via microprobes and electronics introduced through the Early Metamorphosis Insertion Technology (EMIT). EMIT is a novel hybrid biology pathway for autonomous centimeter-scale robots that forms intimate electronic-tissue interfaces by placing electronics in the pupal stage of insect metamorphosis. Our new technology may enable insect cyborgs by realizing a reliable control interface between inserted microsystems and insect physiology. The design rules on the flexibility of the inserted microsystem and the investigation towards tissue- microprobe biological and electrical compatibility are also presented.,"Micromechanical devices,
Neuromuscular,
Insects,
Aerospace control,
Probes,
Muscles,
Aerospace electronics,
Actuators,
Batteries,
Biosensors"
Convergence of rule-of-thumb learning rules in social networks,"We study the problem of dynamic learning by a social network of agents. Each agent receives a signal about an underlying state and communicates with a subset of agents (his neighbors) in each period. The network is connected. In contrast to the majority of existing learning models, we focus on the case where the underlying state is time-varying. We consider the following class of rule of thumb learning rules: at each period, each agent constructs his posterior as a weighted average of his prior, his signal and the information he receives from neighbors. The weights given to signals can vary over time and the weights given to neighbors can vary across agents. We distinguish between two subclasses: (1) constant weight rules; (2) diminishing weight rules. The latter reduces weights given to signals asymptotically to 0. Our main results characterize the asymptotic behavior of beliefs. We show that the general class of rules leads to unbiased estimates of the underlying state. When the underlying state has innovations with variance tending to zero asymptotically, we show that the diminishing weight rules ensure convergence in the mean-square sense. In contrast, when the underlying state has persistent innovations, constant weight rules enable us to characterize explicit bounds on the mean square error between an agent’s belief and the underlying state as a function of the type of learning rule and signal structure.","Technological innovation,
Social network services,
Convergence,
Noise,
Stochastic processes,
Random variables,
Bayesian methods"
Disseminating real-time traffic information in vehicular ad-hoc networks,"In this paper we propose an algorithm for disseminating reports about real-time traffic conditions in vehicular ad-hoc networks. Using this method, each vehicle makes local decision on when to disseminate reports, how many to disseminate, and which reports to disseminate. In order to deal with the bandwidth and memory constraints, reports are prioritized in terms of their value, as reflected by supply and demand. We compare the proposed algorithm with Grassroots, an existing VANET dissemination algorithm. The comparison is based on simulation of vehicle mobility in a real road network and of the 802.11 protocol. The simulation results show that the proposed algorithm outperforms Grassroots in the environments where VANET dissemination is challenging.",Vehicles
On-Demand Medium Access in Multihop Wireless Networks with Multiple Beam Smart Antennas,"The paper presents a detailed discussion of various issues involved in designing a medium access control (MAC) protocol for multihop wireless networks with nodes employing multiple beam smart antennas. Multiple beam smart antennas can form several beams simultaneously and can initiate concurrent transmissions or receptions .in them, thereby increasing the throughput of the bottleneck nodes. Traditional on-demand MAC protocols for omnidirectional and single beam directional antennas based on the IEEE 802.11 distributed coordination function (DCF) mechanism cannot take advantage of this unique capability of multiple beam antennas as they do not facilitate concurrent transmissions or receptions by a node. This paper introduces a novel protocol, hybrid MAC (HMAC), which enables concurrent packet reception (CPR) and concurrent packet transmission (CPT) at a node equipped with multiple beam antennas and is backward compatible with IEEE 802.11 DCF. Simulation results show the superior performance of HMAC in most ad hoc scenarios. Moreover, in some sample topologies, the throughput of HMAC is close to the theoretical maximum. The paper also presents a wireless mesh network architecture with heterogeneous antenna technologies and illustrates the advantages of employing multiple beam smart antennas and HMAC in such networks.","Spread spectrum communication,
Wireless networks,
Media Access Protocol,
Directional antennas,
Access protocols,
Throughput,
Wireless application protocol,
Transmitting antennas,
Network topology,
Wireless mesh networks"
QPACE: Quantum Chromodynamics Parallel Computing on the Cell Broadband Engine,"Application-driven computers for lattice gauge theory simulations have often been based on system-on-chip designs, but the development costs can be prohibitive for academic project budgets. An alternative approach uses compute nodes based on a commercial processor tightly coupled to a custom-designed network processor. Preliminary analysis shows that this solution offers good performance, but it also entails several challenges, including those arising from the processor's multicore structure and from implementing the network processor on a field-programmable gate array.",
Supporting Distributed Application Workflows in Heterogeneous Computing Environments,"Next-generation computation-intensive applications in various fields of science and engineering feature large-scale computing workflows with complex structures that are often modeled as directed acyclic graphs. Supporting such task graphs and optimizing their end-to-end network performances in heterogeneous computing environments are critical to the success of these distributed applications that require fast response. We construct analytical models for computing modules, network nodes, and communication links to estimate data processing and transport overhead, and formulate the task graph mapping with node reuse and resource sharing for minimum end-to-end delay as an NP-complete optimization problem. We propose a heuristic approach to this problem that recursively computes and maps the critical path to the network using a dynamic programming-based procedure. The performance superiority of the proposed approach is justified by an extensive set of experiments on simulated data sets in comparison with existing methods.","Distributed computing,
Computer networks,
Concurrent computing,
Physics computing,
Network topology,
Application software,
Analytical models,
Delay estimation,
Resource management,
Computational modeling"
Continuous and Discrete Data Rebinning in Time-of-Flight PET,"This paper investigates data compression methods for time-of-flight (TOF) positron emission tomography (PET), which rebin the 3-D TOF measurements into a set of 2-D TOF data for a stack of transaxial slices. The goal of this work is to develop re- binning algorithms that are more accurate than the TOF single- slice-rebinning (TOF-SSRB) method proposed by Mullani in 1982. Two approaches are explored. The first one is based on a partial differential equation, which expresses a consistency condition for TOF-PET data with a Gaussian TOF profile. From this equation we derive an analytical rebinning algorithm, which is unbiased in the limit of continuous sampling. The second approach is discrete: each 2-D rebinned data sample is calculated as a linear combination of the 3-D TOF samples in the same axial plane parallel to the axis of the scanner. The coefficients of the linear combination are precomputed by optimizing a cost function which enforces both accuracy and good variance reduction, models the TOF profile, the axial PSF of the LORs, and the specific sampling scheme of the scanner. Measurements of a thorax phantom on a prototype TOF-PET scanner with a resolution of 550 ps show that the proposed discrete method improves the bias-variance trade-off and is a promising alternative to TOF-SSRB when data compression is required to achieve clinically acceptable reconstruction time.","Positron emission tomography,
Data compression,
Sampling methods,
Partial differential equations,
Algorithm design and analysis,
Cost function,
Time measurement,
Thorax,
Imaging phantoms,
Prototypes"
Random Test Run Length and Effectiveness,"A poorly understood but important factor in random testing is the selection of a maximum length for test runs. Given a limited time for testing, it is seldom clear whether executing a small number of long runs or a large number of short runs maximizes utility. It is generally expected that longer runs are more likely to expose failures - which is certainly true with respect to runs shorter than the shortest failing trace. However, longer runs produce longer failing traces, requiring more effort from humans in debugging or more resources for automated minimization. In testing with feedback, increasing ranges for parameters may also cause the probability of failure to decrease in longer runs. We show that the choice of test length dramatically impacts the effectiveness of random testing, and that the patterns observed in simple models and predicted by analysis are useful in understanding effects observed in a large scale case study of a JPL flight software system.","Testing,
Protocols,
File systems,
Buffer overflow,
Minimization,
Probability,
Public key"
Content-Based Information Fusion for Semi-Supervised Music Genre Classification,"In this paper, we propose an information fusion framework for the semi-supervised distance-based music genre classification problem. We make use of the regularized least-square framework as the basic classifier, which only involves the similarity scores among different music tracks. We present a similarity score that multiplies different scores based on different distance measures. Particularly the distance measures are not restricted to the Euclidean distance. By adding a weight to each single distance based score, we propose an expectation-maximization (EM) algorithm to adaptively learn the fusion scores. Experiments on real music data set show that our approach can give promising results.","Music,
Feature extraction,
Particle measurements,
Euclidean distance,
Support vector machines,
Support vector machine classification,
Labeling,
Information science,
Semisupervised learning,
IP networks"
Efficient management of data center resources for Massively Multiplayer Online Games,"Today's Massively Multiplayer Online Games (MMOGs) can include millions of concurrent players spread across the world. To keep these highly-interactive virtual environments online, a MMOG operator may need to provision tens of thousands of computing resources from various data centers. Faced with large resource demand variability, and with misfit resource renting policies, the current industry practice is to maintain for each game tens of self-owned data centers. In this work we investigate the dynamic resource provisioning from external data centers for MMOG operation. We introduce a novel MMOG workload model that represents the dynamics of both the player population and the player interactions. We evaluate several algorithms, including a novel neural network predictor, for predicting the resource demand. Using trace-based simulation, we evaluate the impact of the data center policies on the resource provisioning efficiency; we show that dynamic provisioning can be much more efficient than its static alternative.","Resource management,
Predictive models,
Computational modeling,
Large-scale systems,
Neural networks,
Computer science,
Degradation,
Application software,
Ecosystems,
Web server"
"Designing Personal Robots for Education: Hardware, Software, and Curriculum","An exciting new initiative at Georgia Tech and Bryn Mawr College is using personal robots both to motivate students and to serve as the primary programming platform for the Computer Science 1 curriculum. Here, the authors introduce the initiative and outline plans for the future.","Educational robots,
Hardware,
Robot sensing systems,
Robot vision systems,
Parallel robots,
Robot programming,
Robot control,
Programming profession,
Computer science,
Computer science education"
Distributed MIMO radar using compressive sampling,"A distributed MIMO radar is considered, in which the transmit and receive antennas belong to nodes of a small scale wireless network. The transmit waveforms could be uncorrelated, or correlated in order to achieve a desirable beampattern. The concept of compressive sampling is employed at the receive nodes in order to perform direction of arrival (DOA) estimation. According to the theory of compressive sampling, a signal that is sparse in some domain can be recovered based on far fewer samples than required by the Nyquist sampling theorem. The DOAs of targets form a sparse vector in the angle space, and therefore, compressive sampling can be applied for DOA estimation. The proposed approach achieves the superior resolution of MIMO radar with far fewer samples than other approaches. This is particularly useful in a distributed scenario, in which the results at each receive node need to be transmitted to a fusion center.","MIMO,
Sampling methods,
Radar antennas,
Direction of arrival estimation,
Receiving antennas,
Sparse matrices,
Wireless networks,
Image coding,
Signal resolution,
Phased arrays"
On the delay and throughput of digital and analog network coding for wireless broadcast,"We address the problem of exchanging broadcast packets among multiple wireless terminals through a single relay node. The objective is to evaluate the delay and throughput gains of network coding over plain routing. We compare digital network coding at the packet level with analog network coding based on scheduled or random access of terminal transmissions that are forwarded by the relay node. For error-free channels, the performance gain of both types of network coding scales with the number of terminals, if they can overhear each other¿s transmissions. For channels with noise or packet erasures, we formulate network coding as a multiuser communication problem. The multi-dimensional performance measures involve the packet delay, the throughput rate and the probability of decoding error or decoding failure that are optimized either by plain routing, digital or analog network coding depending on the number of terminals and channel properties. Our results open up new questions regarding the use of wireless network coding and illustrate the delay, throughput and reliability trade-offs.","Throughput,
Network coding,
Decoding,
Routing,
Digital relays,
Wireless networks,
Telecommunication network reliability,
Performance loss,
Telecommunication traffic,
Traffic control"
A 3D face matching framework,"Many 3D face matching techniques have been developed to perform face recognition. Among these techniques are variants of 3D facial curve matching, which are techniques that reduce the amount of face data to one or a few 3D curves. The face’s central profile, for instance, proved to work well. However, the selection of the optimal set of 3D curves and the best way to match them is still under-exposed. We propose a 3D face matching framework that allows profile and contour based face matching. Using this framework we evaluate profile and contour types including those described in literature, and select subsets of facial curves for effective and efficient face matching. Results on the 3D face retrieval track of SHREC’07 (the 3D SHape Retrieval Contest) shows the highest mean average precision achieved so far, using only three facial curves of 45 samples each.",
Markerless motion capture of man-machine interaction,"This work deals with modeling and markerless tracking of athletes interacting with sports gear. In contrast to classical markerless tracking, the interaction with sports gear comes along with joint movement restrictions due to additional constraints: while humans can generally use all their joints, interaction with the equipment imposes a coupling between certain joints. A cyclist who performs a cycling pattern is one example: The feet are supposed to stay on the pedals, which are again restricted to move along a circular trajectory in 3D-space. In this paper, we present a markerless motion capture system that takes the lower-dimensional pose manifold into account by modeling the motion restrictions via soft constraints during pose optimization. Experiments with two different models, a cyclist and a snowboarder, demonstrate the applicability of the method. Moreover, we present motion capture results for challenging outdoor scenes including shadows and strong illumination changes.","Man machine systems,
Kinematics,
Mathematical model,
Tracking,
Humans,
Layout,
Joints,
Biological system modeling,
Computer vision,
Gears"
Non-uniform sensor deployment in mobile wireless sensor networks,"Good sensor deployment is vital for wireless sensor networks (WSNs). To improve the initial deployment and to prolong network lifetime, one approach is to relocate sensors in different densities which vary with the distance to the sink. Since sensors located closer to the sink are involved in more data forwarding, sensors in this region should have a higher density. In this paper, we address the problem of Movement-assisted Sensor Positioning (MSP) to increase network lifetime with the objective to achieve the theoretical sensor densities while minimizing sensor movement. We propose three solutions: an Integer-Programming formulation, a localized matching method, and a distributed corona-radius scanning algorithm. Simulation results are presented to evaluate the proposed solutions.","Robot sensing systems,
Corona,
Wireless sensor networks,
Mobile communication,
Monitoring,
Distance measurement,
Mobile computing"
An investigation on evolutionary gradient search for multi-objective optimization,"Evolutionary gradient search is a hybrid algorithm that exploits the complementary features of gradient search and evolutionary algorithm to achieve a level of efficiency and robustness that cannot be attained by either techniques alone. Unlike the conventional coupling of local search operators and evolutionary algorithm, this algorithm follows a trajectory based on the gradient information that is obtain via the evolutionary process. In this paper, we consider how gradient information can be obtained and used in the context of multi-objective optimization problems. The different types of gradient information are used to guide the evolutionary gradient search to solve multi-objective problems. Experimental studies are conducted to analyze and compare the effectiveness of various implementations.","Optimization,
Evolutionary computation,
Programming,
Algorithm design and analysis,
Trajectory,
Measurement,
Electronic mail"
An examination of IPv4 and IPv6 networks : Constraints and various transition mechanisms,"The concept of transiting from IPv4 network to IPv6 network is being processed vigorously. Extensive study is being done presently on this subject as transition from IPv4 to IPv6 requires a high level compatibility and clear procedure for easy and independent deployment of IPv6. The transition between IPv4 internet and IPv6 will be a long process as they are two completely separate protocols. IPv6 is not backward compatible with IPv4, and IPv4 hosts and routers will not be able to deal directly with IPv6 traffic and vice-versa. In fact that there will be extreme difficulties with address allocation and routing if the internet is to continue to run indefinitely using IPv4. Also it is impossible to switch the entire internet over to IPv6 over night. As IPv4 and IPv6 will co-exist for a long time, this requires the transition and inter-operation mechanism. Due to this reason several transitions mechanisms have been developed that can be used to make the transition to IPv6 smoothly. Most applications today support IPv4 and thus there is a need to run these applications on IPv6 access network, especially to persons who are generally on mobile and they want to securely connect to their home network so as to reach IPv4 services. This paper discusses constraints, various techniques and standards require for high level compatibility smooth transition and interoperation between IPv4 and IPv6 by removing the constraints.","Internet,
IP networks,
Switches,
Space technology,
Peer to peer computing,
Cost function,
Network servers,
Transport protocols,
Access protocols,
Computer science"
Towards a Definition of Source-Code Plagiarism,"A survey using a scenario-based questionnaire format has provided insight into the perceptions of U.K academics who teach programming on computing courses. This survey across various higher education (HE) institutions investigates what academics feel constitutes source-code plagiarism in an undergraduate context. Academics' responses on issues surrounding source-code reuse and acknowledgement are discussed. A general consensus exists among academics that a ldquozero tolerancerdquo plagiarism policy is appropriate; however, some issues concerning source-code reuse and acknowledgement raised controversial responses. This paper discusses the most important findings from the survey and proposes a definition of what can constitute source-code plagiarism from the perspective of U.K. academics who teach programming on computing courses.","source coding,
computer science education"
OntoDM: An Ontology of Data Mining,"Motivated by the need for unification of the field of data mining and the growing demand for formalized representation of outcomes of research, we address the task of constructing an ontology of data mining. The proposed ontology, named OntoDM, is based on a recent proposal of a general framework for data mining, and includes definitions of basic data mining entities, such as datatype and dataset, data mining task, data mining algorithm and components thereof (e.g., distance function), etc. It also allows for the definition of more complex entities, e.g., constraints in constraint-based data mining, sets of such constraints (inductive queries) and data mining scenarios (sequences of inductive queries). Unlike most existing approaches to constructing ontologies of data mining, OntoDM is a deep/heavy-weight ontology and follows best practices in ontology engineering, such as not allowing multiple inheritance of classes, using a predefined set of relations and usinga top level ontology.",
Investigating the TLB Behavior of High-end Scientific Applications on Commodity Microprocessors,"The floating point portion of the SPEC CPU suite and the HPC Challenge suite are widely recognized and utilized as benchmarks that represent scientific application behavior. In this work we show that while these benchmark suites may be representative of the cache behavior of production scientific applications, they do not accurately represent the TLB behavior of these applications. Furthermore, we demonstrate that the difference can have a significant impact on performance. In the first part of the paper we present results from implementation-independent trace-based simulations which demonstrate that benchmarks exhibit significantly different TLB behavior for a range of page sizes than a representative set of production applications. In the second part we validate these results on the AMD Opteron implementation of the x86 architecture, showing that false conclusions about choice of page size, drawn from benchmark performance, can result in performance degradations of up to nearly 50% for the production applications we investigated.","Microprocessors,
Production,
Degradation,
Application software,
Laboratories,
Computer science,
Costs"
Raptor codes based distributed storage algorithms for wireless sensor networks,"We consider a distributed storage problem in a large-scale wireless sensor network with n nodes among which k acquire (sense) independent data. The goal is to disseminate the acquired information throughout the network so that each of the n sensors stores one possibly coded packet and the original k data packets can be recovered later in a computationally simple way from any (1 + ∈)k of nodes for some small ∈ ≫ 0. We propose two Raptor codes based distributed storage algorithms for solving this problem. In the first algorithm, all the sensors have the knowledge of n and k. In the second one, we assume that no sensor has such global information.","Decoding,
Sensors,
Encoding,
Radiation detectors,
Wireless sensor networks,
Parity check codes,
Complexity theory"
Verifying and Mining Frequent Patterns from Large Windows over Data Streams,"Mining frequent itemsets from data streams has proved to be very difficult because of computational complexity and the need for real-time response. In this paper, we introduce a novel verification algorithm which we then use to improve the performance of monitoring and mining tasks for association rules. Thus, we propose a frequent itemset mining method for sliding windows, which is faster than the state-of-the-art methods¿in fact, its running time that is nearly constant with respect to the window size entails the mining of much larger windows than it was possible before. The performance of other frequent itemset mining methods (including those on static data) can be improved likewise, by replacing their counting methods (e.g., those using hash trees) by our verification algorithm.","Data mining,
Itemsets,
Association rules,
Partitioning algorithms,
Delay,
Frequency,
Computer science,
Computational complexity,
Computerized monitoring,
Credit cards"
A two-stage approach to saliency detection in images,"Researches in psychology, perception and related fields show that there may be a two-stage process involved in human vision. In this paper, we propose an approach by following a two-stage framework for saliency detection. In the first stage, we extend an existing spectrum residual model for better locating visual pop-outs, while in the second stage we make use of coherence based propagation for further refinement of the results from the first step. For evaluation of the proposed approach, 300 images with diverse contents were manually and accurately labeled. Experiments show that our approach achieves much better performance than that from the existing state-of-art.","Psychology,
Humans,
Visual system,
Colored noise,
Coherence,
Cognition,
Visual perception,
Computer science,
Image processing,
Image color analysis"
On Binary Probing Signals and Instrumental Variables Receivers for Radar,"The so-called merit factor approach (MFA) to radar binary sequence design has led to several theoretical contributions in fairly diverse research areas including information theory, computer science, combinatorial optimization, and analytical number theory. However, the MFA-which basically aims at minimizing the clutter effect on radar performance-implicitly assumes the use of a least squares (LS) receiver that is optimal only when there is no clutter. This problem can be eliminated by using a more general optimal instrumental-variables (IV) receiver in lieu of the LS receiver. The IV receiver can reject clutter more efficiently than the LS receiver. Additionally, the binary sequence design problem associated with the IV approach has an interesting form.","Instruments,
Radar cross section,
Pulse modulation,
Least squares methods,
Radar theory,
Binary sequences,
Radar clutter,
Signal design,
Signal resolution,
Autocorrelation"
Distributed peer-to-peer multiplexing using ad hoc relay networks,"We consider an ad hoc network consisting of d source-destination pairs and R relaying nodes. Each source wishes to transmit its data to its corresponding destination through the relay network. Each relay in the network transmits a properly scaled version of its received signal thereby cooperating with other relays to deliver each source's data to the corresponding destination. Assuming a minimal cooperation among the relaying nodes, we design a distributed beamformer such that the total relay transmit power dissipated by all relays is minimized while, at the same time, the quality of services at all destinations are guaranteed to be above certain pre-defined thresholds. We show that using a semi-definite relaxation approach, the power minimization problem can be turned into a semi-definite programming (SDP) optimization, and therefore, it can be solved efficiently using interior point methods. Our results show that the distributed relay multiplexing is possible and may be beneficial depending on the channel conditions.","Peer to peer computing,
Relays,
Array signal processing,
Transmitting antennas,
Mobile communication,
Ad hoc networks,
Minimization methods,
Optimization methods,
Transmitters,
Mobile antennas"
ORTEGA: An Efficient and Flexible Online Fault Tolerance Architecture for Real-Time Control Systems,"Fault tolerance is an important aspect in real-time computing. In real-time control systems, tasks could be faulty due to various reasons. Faulty tasks may compromise the performance and safety of the whole system and even cause disastrous consequences. In this paper, we describe On-demand real-time guard (ORTEGA), a new software fault tolerance architecture for real-time control systems. ORTEGA has high fault coverage and reliability. Compared with existing real-time fault tolerance architectures, such as Simplex, ORTEGA allows more efficient resource utilizations and enhances flexibility. These advantages are achieved through the on-demand detection and recovery of faulty tasks. ORTEGA is applicable to most industrial control applications where both efficient resource usage and high fault coverage are desired.",
Malignant melanoma detection by Bag-of-Features classification,"In this paper, we apply a Bag-of-Features approach to malignant melanoma detection based on epiluminescence microscopy imaging. Each skin lesion is represented by a histogram of codewords or clusters identified from a training data set. Classification results using Naive Bayes classification and Support Vector Machines are reported. The best performance obtained is 82.21% on a dataset of 100 skin lesion images. Furthermore, since in melanoma screening false negative errors have a much higher impact and associated cost than false positive ones, we use the Neyman-Pearson score in our model selection scheme.",
A User Driven Dynamic Circuit Network Implementation,"The requirements for network predictability are becoming increasingly critical to the DOE science community where resources are widely distributed and collaborations are world-wide. To accommodate these emerging requirements, the energy sciences network has established a science data network to provide user driven guaranteed bandwidth allocations. In this paper we outline the design, implementation, and secure coordinated use of such a network, as well as some lessons learned.","Circuits,
Collaboration,
Large Hadron Collider,
Bandwidth,
US Department of Energy,
Electronic mail,
Laboratories,
Quality of service,
Routing,
Computer architecture"
Particle Swarm Optimization Based Clustering of Web Usage Data,Web session clustering is one of the important web usage mining techniques which aims to group usage sessions on the basis of some similarity measures. In this paper we describe a new web session clustering algorithm that uses particle swarm optimization. We review the existing web usage clustering techniques and propose a swarm intelligence based PSO-clustering algorithm for the clustering of web user sessions. The proposed algorithm works independently without hybridization with any other clustering algorithm. The results show that our approach performs better than the benchmark K-means clustering algorithm for clustering web usage sessions.,
Capacity of Hybrid Cellular-Ad Hoc Data Networks,"In this paper, towards improving spatial reuse in a cellular network, we consider augmenting it with wireless ad hoc connectivity. The coverage area of each base-station is reduced and the users that are within the area relay traffic to nodes outside the area; these users further relay data to more distant users within the cell. The resulting network is referred to as a hybrid network. While this approach can result in shorter range higher-rate links and improved spatial reuse which, together favor a capacity increase, it relies on multi-hop forwarding which is detrimental to the overall capacity. Our objective in this work is to evaluate the impact of these conflicting factors on the capacity of the hybrid network and determine if this capacity is higher than that of the original cellular network. We formally define the capacity of the network as the maximum possible downlink throughput under the conditions of max-min fairness. We analytically compute the capacity of a two-dimensional hybrid network with regular placements of base-stations (BSs) and users. We validate our analytical results via simulations. Our studies demonstrate that capacity improvements are possible in certain parametric regimes in which the penalty due to multi-hop relaying does not outweigh the gains due to spatial reuse and shorter higher-rate links. Our simulations also demonstrate that if the users are placed randomly, the behavioral results are similar to that with regular placements of users.",
Image based resolution modeling for the HRRT OSEM reconstructions software,"The implementation and the measurement of an approximate image based model of the ECAT HRRT PET scanner response function, designed for its regular OSEM reconstruction software, are presented. The system matrix used in the iterative reconstruction is factorized into two terms: first a matrix modeling the blurring effects in the image space, followed by the projection matrix. The methodology used to measure the elements of the image based blurring matrix is presented and applied to three HRRT scanners. A spatially invariant resolution model was chosen; the columns of the blurring matrix are then defined as shifted copies of a stationary blurring kernel. This kernel was modeled as the sum of two isotropic 3D Gaussian functions. The results of the resolution measurement varied between the three scanners: at the center of the field-of-view, the standard deviation varied between 0.85 and 1.00 mm for the first Gaussian and between 2.0 and 2.7 mm for the second Gaussian. The ratio between the second and the first Gaussians was 0.07.","Image resolution,
Image reconstruction,
Positron emission tomography,
Spatial resolution,
Solid modeling,
Software measurement,
Kernel,
Detectors,
Software algorithms,
Nuclear and plasma sciences"
Why and How to Perform Fraud Experiments,"Fraud isn't new, but in the eyes of many experts, phishing and crimeware threaten to topple society's overall stability because they erode trust in its underlying computational infrastructure. Most people agree that phishing and crimeware must be fought, but to do so effectively, we must fully understand both types of threat; that starts by quantifying how and when people fall for deceit. In this article, we look closer at how to perform fraud experiments. Researchers typically use three approaches to quantify fraud: surveys, in-lab experiments, and naturalistic experiments.",
Future trends in distributed simulation and distributed virtual environments: Results of a peer study,"This paper reports main results of a peer study on future trends in distributed simulation and distributed virtual environments (Strassburger et al. 2008). The peer study was based on the opinions of more than 60 experts which were collected by means of a survey and personal interviews. The survey collected opinions concerning the current state-of-the-art, relevance, and research challenges that must be addressed to advance and strengthen these technologies to a level where they are ready to be applied in day-to-day business in industry. Most important result of this study is the observation that as research areas, both distributed simulation and distributed virtual environments are attributed a high future practical relevance and a high economic potential. At the same time the study shows that the current adoption of these technologies in the industrial sector is rather low. The study analyses reasons for this observation and identifies open research challenges.",
Cracking Cancelable Fingerprint Template of Ratha,"Cancelable biometrics may be a good approach to address the security and privacy concerns on biometric authentication. It uses some parameterized transforms to convert an original biometric template into a new version for authentication. The security of cancelable biometrics lies on noninvertibility of the transformed template, that is, the transforms should be noninvertible so that the original template can not be recovered. One way to achieve the noninvertibilty is through the use of many-to-one transforms. The idea of Ratha’s scheme of generating fingerprint templates just depends on this. However, it is revealed in this paper that the form of the transforms and the parameters chosen in his implementation weaken the many-to-one property. This results in the possible recovery of original minutiae from one transformed template.","Fingerprint recognition,
Biometrics,
Authentication,
Bioinformatics,
Data privacy,
Data security,
Information security,
Distortion,
Computer science,
Telecommunication computing"
Decentralized cooperative collision avoidance for acceleration constrained vehicles,"Safety must be ensured in the deployment of multi-agent vehicle systems. This paper presents decentralized collision avoidance algorithms for systems with second order dynamics and acceleration constraints, using a switching control law. The technique augments existing multi-agent control laws with the capability to switch to provably safe collision avoidance maneuvers when required. Two algorithms with low computational cost are presented, one for two vehicles and one for more vehicles. In both methods, each vehicle computes avoid sets with respect to every other vehicle. When one or more vehicles are on the boundary of their avoid sets, collision avoidance action is taken. These algorithms are applied in simulation scenarios for which existing techniques either fail or are computationally expensive, and used for information theoretic control of a mobile sensor network to reduce the computational complexity. Finally, they are demonstrated in quadrotor helicopter flight experiments.","Collision avoidance,
Acceleration,
Vehicle safety,
Switches,
Vehicle dynamics,
Control systems,
Computational efficiency,
Computational modeling,
Computer networks,
Mobile computing"
Efficient Sensor Network Reprogramming through Compression of Executable Modules,"Software in deployed sensor networks needs to be updated to introduce new functionality or to fix bugs. Reducing dissemination time is important because the dissemination disturbs the regular operation of the network. We present a method for reducing the dissemination time and energy consumption based on compression of native code modules. Code compression reduces the size of the software update, but the decompression on the sensor nodes requires processing time and energy. We quantify these trade-offs for seven different compression algorithms. Our results show that GZIP has the most favorable trade-offs, saving on average 67% of the dissemination time and 69% of the energy in a multi-hop wireless sensor network.","Energy consumption,
Decoding,
Data compression,
Compression algorithms,
Computer bugs,
Spread spectrum communication,
Computer science,
Wireless sensor networks,
Embedded software,
Application software"
PAM representation of ternary CPM,This letter considers the pulse amplitude modulation (PAM) representation of continuous phase modulation (CPM) with a ternary data alphabet. This technique is applied to the problem of constructing reduced-complexity detectors with near-optimum performance. The usefulness of this approach is demonstrated using the ternary CPM variant known as shaped-offset quadrature phase-shift keying (SOQPSK).,"Detectors,
Phase shift keying,
Pulse modulation,
Amplitude modulation,
Continuous phase modulation,
Phase modulation,
Pulse shaping methods,
Communication standards,
Correlators"
A Stairway Detection Algorithm based on Vision for UGV Stair Climbing,"In the paper, we present a vision based algorithm used to guide the Unmanned Ground Vehicles (UGV) for autonomous stairways climbing and implement it on UGV successfully. The reliability of guiding UGV to climb stairs requires evaluating two offset parameters: the position of vehicle on stairs and the orientation angle to stairs. The intention of our algorithm is to estimate these two parameters through extracting the stair edges robustly. To achieve this goal, we apply the Gabor filter to eliminate the influence of the illumination and keep edges, and propose a fast method to remove small lines. Finally we link stair edges, and estimate the offset parameters used to steer the vehicle by RANSAC algorithm. Experiments on various stairways including indoor and outdoor are given in various light conditions. The results validate our algorithm.","Detection algorithms,
Robotics and automation,
Vehicles,
Robustness,
Parameter estimation,
Gabor filters,
Robot vision systems,
Servomechanisms,
Lighting,
Mobile robots"
A Temporal Language for SystemC,"We describe a general approach for defining new temporal specification languages, and adopting existing languages, for SystemC. We define the concept of ""underlying trace"" describing the execution of a SystemC model, and then define a set of important primitive assertions about the states in the trace. Our framework not only provides additional expressive power for making atomic assertions, but also provides very fine control over the temporal resolution of the language. Using the primitives defined here as clock expression allows sampling at different levels, from transaction-level to the level of individual statements. The advantage of our approach is that it defines important SystemC properties that have been overlooked previously, and also provides a uniform mechanism for specifying the sampling rate of temporal languages.","Object oriented modeling,
Hardware,
Libraries,
Formal verification,
Computer science,
Sampling methods,
Mechanical factors,
Kernel,
System testing,
Protocols"
Discrete double integrator consensus,"A distributed double integrator discrete time consensus protocol is presented along with stability analysis. The protocol will achieve consensus when the communication topology contains at least a directed spanning tree. Average consensus is achieved when the communication topology is strongly connected and balanced, where average consensus for double integrator systems is discussed. For second order systems average consensus occurs when the information states tend toward the average of the current information states not their initial values. Lastly, perturbation to the consensus protocol is addressed. Using a designed perturbation input, an algorithm is presented that accurately tracks the center of a vehicle formation in a decentralized manner.","Protocols,
Communication system control,
Stability analysis,
Topology,
Spectral analysis,
Algorithm design and analysis,
Remotely operated vehicles,
Autonomous agents,
Vehicle dynamics,
Continuous time systems"
Flow-based identification of botnet traffic by mining multiple log files,"Botnet detection and disruption has been a major research topic in recent years. One effective technique for botnet detection is to identify Command and Control (C&C) traffic, which is sent from a C&C center to infected, hosts (bots) to control the bots. If this traffic can be detected, both the C&C center and the bots it controls can be detected, and the botnet can be disrupted. We propose a mutiple log-file based temporal correlation technique for detecting C&C traffic. Our main assumption is that bots respond much faster than humans. By temporally correlating two host-based log files, we are able to detect this property and thereby detect bot activity in a host machine. In our experiments we apply this technique to log files produced bt tcpdump and exedump, which record all incoming and outgoing network packets, and the start times of application executions at the host machine, respectively. We apply data mining to extract relevant features from these log files and detect C&C traffic. Our experimental results validate our assumption and show better overall performance when compared to other recently published techniques.","Data mining,
Command and control systems,
Humans,
Feature extraction,
Telecommunication traffic"
Automatic Construction of 3D-ASM Intensity Models by Simulating Image Acquisition: Application to Myocardial Gated SPECT Studies,"Active shape models bear a great promise for model-based medical image analysis. Their practical use, though, is undermined due to the need to train such models on large image databases. Automatic building of point distribution models (PDMs) has been successfully addressed and a number of autolandmarking techniques are currently available. However, the need for strategies to automatically build intensity models around each landmark has been largely overlooked in the literature. This work demonstrates the potential of creating intensity models automatically by simulating image generation. We show that it is possible to reuse a 3D PDM built from computed tomography (CT) to segment gated single photon emission computed tomography (gSPECT) studies. Training is performed on a realistic virtual population where image acquisition and formation have been modeled using the SIMIND Monte Carlo simulator and ASPIRE image reconstruction software, respectively. The dataset comprised 208 digital phantoms (4D-NCAT) and 20 clinical studies. The evaluation is accomplished by comparing point-to-surface and volume errors against a proper gold standard. Results show that gSPECT studies can be successfully segmented by models trained under this scheme with subvoxel accuracy. The accuracy in estimated LV function parameters, such as end diastolic volume, end systolic volume, and ejection fraction, ranged from 90.0% to 94.5% for the virtual population and from 87.0% to 89.5% for the clinical population.","Myocardium,
Active shape model,
Computational modeling,
Computed tomography,
Image segmentation,
Medical simulation,
Biomedical imaging,
Image analysis,
Image databases,
Buildings"
Virtual-Coordinate-Based Delivery-Guaranteed Routing Protocol in Wireless Sensor Networks with Unidirectional Links,"A wireless sensor network has unidirectional links because sensors can have different transmission ranges, sensors have unstable transmission ranges, and a hidden terminal problem exists. In this paper, we introduce a virtual coordinate assignment protocol (ABVCap_Uni) to assign virtual coordinates to nodes that have no geographic information in wireless sensor networks with unidirectional links, and we propose a routing protocol based on the ABVCap_Uni virtual coordinates. Our routing protocol guarantees packet delivery without computation and storage of global topology features in a discrete domain. Using simulation, we evaluate the performance of the proposed routing protocol (ABVCap_Uni routing), the greedy landmark- descent routing protocol (GLDR+VLM routing), and the greedy routing protocol based on physical coordinates (Euclidean routing). The simulations demonstrate that our routing protocol ensures moderate routing path length cost overhead.","Routing protocols,
Wireless sensor networks,
Network topology,
Computational modeling,
Euclidean distance,
Communications Society,
Computer science,
Electronic mail,
Wireless application protocol,
Peer to peer computing"
Building a National Telemedicine Network,"Motivated by the need to reduce the cost of patient transport to health centers, the authors designed a prototype national telemedicine network in Brazil. As a result of this project, we now have an agile, easy-to-use, high-quality telemedicine network that provides greater access to patient data and helps in medical decisions, cutting unnecessary costs to the state and benefiting the population as a whole.",
Basic study on indoor location estimation using Visible Light Communication platform,"A VLC (Visible Light Communication) system using fluorescent lights has been developed for indoor guidance of the visually impaired. While it is relatively straightforward to provide generalized location information for a blind user, precise location information is much more difficult to determine. We propose that the effective data reception range and the receiver's precise location can be calculated using measured sensor angles. A series of experiments have been performed in a practical platform with 22 fluorescent lights, 39 measuring points (MP). The average distance error could reach as low as 10 cm. This development will provide greater accuracy and therefore less stress for blind users.","location estimation,
Visible light Communication (VLC),
Fluorescent light,
Indoor guidance"
Client-Side Caching Strategies and On-Demand Broadcast Algorithms for Real-Time Information Dispatch Systems,"In this work, we propose a broadcast algorithm called Most Request Served (MRS) and its variants with caching strategies for on-time delivery of data in Real-Time Information Dispatch System. This family of algorithms consider request deadline, data object size and data popularity in making scheduling decisions. Although previous scheduling algorithms also base on some or all of these attributes to choose the most beneficial data to be broadcast, they did not consider the loss brought by their scheduling decisions. However, MRS considers both gain and loss in making a scheduling decision. We have performed a series of simulation experiments to compare the performance of various algorithms. Simulation results show that our proposed broadcast algorithm not only succeeds in providing good on-time delivery of data but at the same time provides 20% of improvement in response time over traditional scheduling algorithms like First-In-First-Out (FIFO) and Earliest-Deadline-First (EDF). Simulation results also show that our proposed caching strategy provides further improvement in terms of percentage of requests finished in time over traditional caching strategy like Least Recently Used (LRU).","Broadcasting,
Real time systems,
Scheduling algorithm,
Computer science,
Delay,
Cache memory,
Wireless communication,
Cellular networks,
IP networks,
Broadcast technology"
Simplifying Service Deployment with Virtual Appliances,"As IT services become more powerful and complex, service deployment gets more difficult and expensive. Service deployment, the process of making a service ready for use, often includes deploying multiple, interrelated software components into heterogeneous environments. Different technologies and tools try to address these complexities by describing the environments, abstracting the dependencies, and automating the process. Virtual Appliances, a set of virtual machines including optimized operating systems, pre-built, pre-configured, ready-to-run applications and embedded appliance specific components, are emerging as a breakthrough technology to solve the complexities of service deployment. Virtual appliances provide a simple, unified and easy to use interface for service deployment by encapsulating entire custom environments, and resolving the execution policy constraints and inter-dependencies through pre-installing the software applications. The motivation of this paper is to prove virtual appliances offer a better service deployment mechanism. We start with an easy to understand model to describe the complexity of service deployment and introduce the architecture of a virtual appliance. We then analyze the deployment process of using traditional deployment mechanisms, and quantitatively and qualitatively compare the deployment time, operations and parameters of the traditional approach with the use of virtual appliances. The results show virtual appliances offer significant advantages for service deployment by making the deployment process much simpler and easier, even for the deployment of advanced enterprise services.",virtual machines
The Use of Semantic Human Description as a Soft Biometric,"Gait as a biometric has a unique advantage that it can be used when images are acquired at a distance and other biometrics are at too low a resolution to be perceived. In such a situation, there is still information which can be readily perceived by human vision, yet is difficult to extract automatically. We examine how this information can be used to enrich the recognition process. We call these descriptions semantic annotations and investigate their use in biometric scenarios. We outline a group of visually assessable physical traits formulated as a mutually exclusive set of semantic terms; we contend that these traits are usable in soft biometric fusion. An experiment to gather semantic annotations was performed and the most reliable traits are identified using ANOVA. We rate the ability to correctly identify subjects using these semantically prescribed traits, both in isolation as well as in fusion with an automatically derived gait signature.","Humans,
Biometrics,
Image recognition,
Image resolution,
Data mining,
Analysis of variance,
Tellurium,
Tongue,
Security,
DNA"
Complex mission optimization for Multiple-UAVs using Linear Temporal Logic,"This paper discusses a class of mission planning problems in which mission objectives and relative timing constraints are specified using the Linear Temporal Logic language LTL_X . Among all mission plans that satisfy the LTL_X specifications, it is desired to find those minimizing a given cost functional. We show that such an optimization problem can be formulated as a Mixed-Integer Linear Program, and present an algorithm for this purpose. This algorithm mainly relies on a novel systematic procedure which converts a given LTL_X formula into a set of mixed-integer linear constraints. The approach presented here can be used for Multiple-UAV Mission Planning purposes, allowing the operator to specify complex mission objectives in LTL_X in a very natural manner; the proposed algorithm constructs the optimal mission plan satisfying the given LTL_X specification. Examples for practical problem sizes are presented and discussed in the paper.","Unmanned aerial vehicles,
Logic programming,
Control theory,
Timing,
Application software,
Linear programming,
Constraint optimization,
Cost function,
Computer science,
Specification languages"
Compression independent object encryption for ensuring privacy in video surveillance,"One of the main concerns of the wide use of video surveillance is the loss of individual privacy. Individuals who are not suspects need not be identified on camera recordings. Mechanisms that protect the identity while ensuring legitimate security needs are necessary. Selectively encrypting objects that reveal identity (e.g., faces or vehicle tags) is necessary to preserve individualspsila right to privacy. This paper presents a compression algorithm independent solution that provides privacy in video surveillance applications. The proposed approach is based on the use of permutation based encryption to hide identity revealing features. The permutation based encryption tolerates lossy compression and allows decryption at a later time. The use of permutation based encryption makes the proposed solution independent of the compression algorithms used. The paper presents the performance of the system when using H.264 video encoding.","Cryptography,
Bit rate,
Privacy,
Video surveillance,
Transform coding,
Surveillance,
Compression algorithms"
From Axioms to Analytic Rules in Nonclassical Logics,"We introduce a systematic procedure to transform large classesof (Hilbert) axioms into equivalent inference rules in sequent and hypersequent calculi. This allows for the automated generation of analytic calculi for a wide range of propositional nonclassical logics including intermediate, fuzzy and substructural logics. Our work encompasses many existing results, allows for the definition of new calculi and contains a uniform semantic proof of cut-elimination for hypersequent calculi.","Calculus,
Fuzzy logic,
Computer science,
Informatics,
Availability,
Explosions"
Optical flow estimation with uncertainties through dynamic MRFs,"In this paper, we propose a novel dynamic discrete framework to address image morphing with application to optical flow estimation. We reformulate the problem using a number of discrete displacements, and therefore the estimation of the morphing parameters becomes a tractable matching criteria independent combinatorial problem which is solved through the FastPD algorithm. In order to overcome the main limitation of discrete approaches (low dimensionality of the label space is unable to capture the continuous nature of the expected solution), we introduce a dynamic behavior in the model where the plausible discrete deformations (displacements) are varying in space (across the domain) and time (different states of the process - successive morphing states) according to the local uncertainty of the obtained solution.","Image motion analysis,
Uncertainty,
Biomedical optical imaging,
Optical sensors,
Optical computing,
Biomedical imaging,
Deformable models,
Constraint optimization,
Covariance matrix,
Computer science"
Behavioural Skeletons in GCM: Autonomic Management of Grid Components,"Autonomic management can be used to improve the QoS provided by parallel/distributed applications. We discuss behavioural skeletons introduced in earlier work: rather than relying on programmer ability to design ""from scratch"" efficient autonomic policies, we encapsulate general autonomic controller features into algorithmic skeletons. Then we leave to the programmer the duty of specifying the parameters needed to specialise the skeletons to the needs of the particular application at hand. This results in the programmer having the ability to fast prototype and tune distributed/parallel applications with non-trivial autonomic management capabilities. We discuss how behavioural skeletons have been implemented in the framework of GCM (the grid component model developed within the CoreGRID NoE and currently being implemented within the GridCOMP STREP project). We present results evaluating the overhead introduced by autonomic management activities as well as the overall behaviour of the skeletons. We also present results achieved with a long running application subject to autonomic management and dynamically adapting to changing features of the target architecture. Overall the results demonstrate both the feasibility of implementing autonomic control via behavioural skeletons and the effectiveness of our sample behavioural skeletons in managing the ""functional replication"" pattern(s).",
Learning for stereo vision using the structured support vector machine,"We present a random field based model for stereo vision with explicit occlusion labeling in a probabilistic framework. The model employs non-parametric cost functions that can be learnt automatically using the structured support vector machine. The learning algorithm enables the training of models that are steered towards optimizing for a particular desired loss function, such as the metric used to evaluate the quality of the stereo labeling. Experimental results demonstrate that the performance of our method surpasses that of previous learning approaches and is comparable to the state-of-the-art for pixel-based stereo. Moreover, our method achieves good results even when trained on different image sets, in contrast with the common practice of hand tuning to specific benchmark images. In addition, we investigate the impact of graph structure on model performance. Our study shows that random field models with longer-range edges generally outperform the 4-connected grid and that this advantage is especially pronounced for noisy images.","Machine learning,
Stereo vision,
Support vector machines,
Cost function,
Labeling,
Maximum likelihood estimation,
Kernel,
Character generation,
Computer science,
Computer vision"
Control Reconfiguration of Discrete Event Systems With Dynamic Control Specifications,"This paper defines a reconfiguration method for the class of discrete-event systems (DES) that is subject to linear constraints as their control specifications. Some existing methods for enforcing these constraints make use of Petri-net P-invariants for controller synthesis. These methods are quite appealing because their computational complexity is much more tractable than most other methods for controller synthesis. However, a common limitation of all existing P-invariant-based control architectures for DES plants is the assumption that the linear constraints defining the control specification of the plant do not change over time. Here, we relax this assumption and allow the control specifications to change during controller runtime. Under certain assumptions on DES behavior, we automatically reconfigure the DES controller after the control specification is changed. In addition, if the current state of the controlled DES has become infeasible under the new control specification, we automatically generate a so-called plant reconfiguration procedure whose execution leads the system back to a feasible state. This reconfiguration procedure is optimal in that it seeks to minimize the cost of reconfiguration actions through an Integer Programming (IP) model. The objective function of the IP model can be used to generate reconfiguration solutions that meet some desired properties. Depending on the cost of each reconfiguration action, a minimum cost reconfiguration solution may use only actions contained in the current plant configuration (an internal response), or ask for a change in the plant configuration, for instance, by adding new resources (an external response), or a combination of both strategies. Finally, we illustrate our method by applying it to a hospital control system example. Note to Practitioners-This paper proposes a dynamic reconfiguration framework that can revise the operations of systems whose control requirements change over time. The proposed framework can be applied to systems that satisfy the following two assumptions. First, the behavior of the system under study is described in terms of a set of discrete states and events. Events will cause the system to transition between states. Second, the control requirements must be expressed by linear equalities and inequalities on the system states. Under these circumstances, the proposed framework can identify an optimal transition to a new control policy that satisfies the new control requirements. Moreover, the system under consideration will continue operating while this transition is taking place. One application of this method is in modifying hospital control strategies when a hospital experiences unexpected events. In this case, the hospital operations-such as patient handling, resource assignment, and procedure scheduling-can be represented by discrete state models (e.g., Petri nets). Constraints on these operations can be modeled by linear inequalities on hospital and patient state. Upon a change in the constraints, the proposed reconfiguration method revises the hospital control strategies. For example, a shift in the hospital service demands (e.g., an increase in the flow of patients to the hospital due to a mass casualty situation) can be translated to changes in the constraints. In this case, the hospital operations must be revised to accommodate the new constraints without disrupting the operation of the hospital. The reconfiguration method of this paper provides a framework for modeling the reconfiguration steps and for calculating the least cost reconfiguration solution.","Control systems,
Discrete event systems,
Hospitals,
Automatic control,
Costs,
Control system synthesis,
Automatic generation control,
Optimal control,
Computational complexity,
Computer architecture"
Adjacent channel interference in 802.11a: Modeling and testbed validation,"In this work we utilize the model for calculation of the interference power by partially overlapping channels and combine it with the signal to interference plus noise (SINK) criterion for signal capture to quantify the effect of adjacent channel interference (ACI) in 802.11a. We validate the results from our theoretical model by applying it on an in-lab testbed, in which we use signal splitters/combiners and fixed attenuators to emulate the wireless channel. Our experiment setup is able to isolate the mechanisms that the neighboring channel interference affects the 802.11a: the packet capture at the receiver and the clear channel assessment (CCA) mechanism. We quantify the effect of ACI for both of these mechanisms in terms of throughput. Our results indicate that equipping a single node with multiple interfaces requires careful channel allocation and physical antenna separation, since throughput can be severely degraded. Our future work focuses on field experimenting with 802.11a multi-radio mesh nodes in order to introduce the antenna characteristics in our model.","Interchannel interference,
Testing,
Signal to noise ratio,
Antenna measurements,
Computer science,
Attenuators,
Throughput,
Propagation losses,
Channel allocation,
Degradation"
Speech-based cognitive load monitoring system,"Monitoring cognitive load is important for the prevention of faulty errors in task-critical operations, and the development of adaptive user interfaces, to maintain productivity and efficiency in work performance. Speech, as an objective and non-intrusive measure, is a suitable method for monitoring cognitive load. Existing approaches for cognitive load monitoring are limited in speaker-dependent recognition and need manually labeled data. We propose a novel automatic, speaker-independent classification approach to monitor, in real-time, the person’s cognitive load level by using speech features. In this approach, a Gaussian Mixture Model (GMM) based classifier is created with unsupervised training. Channel and speaker normalization are deployed for improving robustness. Different delta techniques are investigated for capturing temporal information. And a background model is introduced to reduce the impact of insufficient training data. The final system achieves 71.1% and 77.5% accuracy on two different tasks, each of which has three discrete cognitive load levels. This performance shows a great potential in real-world applications.","Speech,
Australia,
Computerized monitoring,
User interfaces,
Productivity,
Acceleration,
Cepstral analysis,
Computer science,
Computer errors,
Heart rate measurement"
An approach to specification-based attack detection for in-vehicle networks,"An upcoming trend for automotive manufacturers is to create seamless interaction between a vehicle and fleet management to provide remote diagnostics and firmware updates over the air. To allow this, the previously isolated in-vehicle network must be connected to an external network, and can thus be exposed to a whole new range of threats known as cyber attacks. In this paper we explore the applicability of a specification-based approach to detect cyber attacks within the in-vehicle network. We derive information to create security specifications for communication and ECU behavior from the CANopen draft standard 3.01 communication protocol and object directory sections. We also provide a set of example specifications, propose a suitable location for the attack detector, and evaluate the detection using a set of attack actions.",Vehicles
Nearly Tight Low Stretch Spanning Trees,"We prove that any graph
G
with
n
points has a distribution

over spanning trees such that for any edge
(u,v)
the expected stretch
E
T∼
[
d
T
(u,v)/
d
G
(u,v)]
is bounded by
O
̃ 
(logn)
. Our result is obtained via a new approach of building ``highways'' between portals and a new strong diameter probabilistic decomposition theorem.",
Security through redundant data diversity,"Unlike other diversity-based approaches, N-variant systems thwart attacks without requiring secrets. Instead, they use redundancy (to require an attacker to simultaneously compromise multiple variants with the same input) and tailored diversity (to make it impossible to compromise all the variants with the same input for given attack classes). In this work, we develop a method for using data diversity in N-variant systems to provide high-assurance arguments against a class of data corruption attacks. Data is transformed in the variants so identical concrete data values have different interpretations. In order to corrupt the data without detection, an attacker would need to alter the corresponding data in each variant in a different way while sending the same inputs to all variants. We demonstrate our approach with a case study using that thwarts attacks that corrupt UID values.","Security,
Monitoring,
Data models,
Cognition,
Conferences,
Software,
Tagging"
Analysis of low resolution accelerometer data for continuous human activity recognition,"The advent of wearable sensors like accelerometers has opened a plethora of opportunities to recognize human activities from other low resolution sensory streams. In this paper we formulate recognizing activities from accelerometer data as a classification problem. In addition to the statistical and spectral features extracted from the acceleration data, we propose to extract features that characterize the variations in the first order derivative of the acceleration signal. We evaluate the performance of different state of the art discriminative classifiers like, boosted decision stumps (AdaBoost), support vector machines (SVM) and Regularized Logistic Regression(RLogReg) under three different evaluation scenarios(namely Subject Independent, Subject Adaptive and Subject Dependent). We propose a novel computationally inexpensive methodology for incorporating smoothing classification temporally, that can be coupled with any classifier with minimal training for classifying continuous sequences. While a 3% increase in the classification accuracy was observed on adding the new features, the proposed technique for continuous recognition showed a 2.5 — 3% improvement in the performance.","Accelerometers,
Humans,
Feature extraction,
Data mining,
Acceleration,
Support vector machines,
Support vector machine classification,
Wearable sensors,
Signal resolution,
Logistics"
Current Density Impedance Imaging,"Current density impedance imaging (CDII) is a new impedance imaging technique that can noninvasively measure the conductivity distribution inside a medium. It utilizes current density vector measurements which can be made using a magnetic resonance imager (MRI) (Scott et al., 1991). CDII is based on a simple mathematical expression for nablasigma/sigma = nabla ln sigma, the gradient of the logarithm of the conductivity sigma, at each point in a region where two current density vectors J1 and J2 have been measured and J1 x J2 ne 0. From the calculated nabla In sigma and a priori knowledge of the conductivity at the boundary, the logarithm of the conductivity In sigma is integrated by two different methods to produce an image of the conductivity sigma in the region of interest. The CDII technique was tested on three different conductivity phantoms. Much emphasis has been placed on the experimental validation of CDII results against direct bench measurements by commercial LCR meters before and after CDII was performed.","Current density,
Magnetic resonance imaging,
Density measurement,
Current measurement,
Conductivity measurement,
Impedance measurement,
Magnetic resonance,
Testing,
Imaging phantoms,
Performance evaluation"
Prostate Cancer Spectral Multifeature Analysis Using TRUS Images,"This paper focuses on extracting and analyzing different spectral features from transrectal ultrasound (TRUS) images for prostate cancer recognition. First, the information about the images' frequency domain features and spatial domain features are combined using a Gabor filter and then integrated with the expert radiologist's information to identify the highly suspicious regions of interest (ROIs). The next stage of the proposed algorithm is to scan each identified region in order to generate the corresponding 1-D signal that represents each region. For each ROI, possible spectral feature sets are constructed using different new geometrical features extracted from the power spectrum density (PSD) of each region's signal. Next, a classifier-based algorithm for feature selection using particle swarm optimization (PSO) is adopted and used to select the optimal feature subset from the constructed feature sets. A new spectral feature set for the TRUS images using estimation of signal parameters via rotational invariance technique (ESPRIT) is also constructed, and its ability to represent tissue texture is compared to the PSD-based spectral feature sets using the support vector machines (SVMs) classifier. The accuracy obtained ranges from 72.2% to 94.4%, with the best accuracy achieved by the ESPRIT feature set.","Prostate cancer,
Image analysis,
Spectral analysis,
Data mining,
Radiofrequency identification,
Ultrasonic imaging,
Image recognition,
Frequency domain analysis,
Gabor filters,
Signal generators"
A Statistical Language Modeling Approach to Online Deception Detection,"Online deception is disrupting our daily life, organizational process, and even national security. Existing approaches to online deception detection follow a traditional paradigm by using a set of cues as antecedents for deception detection, which may be hindered by ineffective cue identification. Motivated by the strength of statistical language models (SLMs) in capturing the dependency of words in text without explicit feature extraction, we developed SLMs to detect online deception. We also addressed the data sparsity problem in building SLMs in general and in deception detection in specific using smoothing and vocabulary pruning techniques. The developed SLMs were evaluated empirically with diverse datasets. The results showed that the proposed SLM approach to deception detection outperformed a state-of-the-art text categorization method as well as traditional feature-based methods.","Text categorization,
Face detection,
Feature extraction,
Vocabulary,
Electronic mail,
National security,
Smoothing methods,
Text mining,
Knowledge management,
Data security"
Practical applications of Boolean Satisfiability,"Boolean Satisfiability (SAT) solvers have been the subject of remarkable improvements since the mid 90s. One of the main reasons for these improvements has been the wide range of practical applications of SAT. Indeed, examples of modern applications of SAT range from termination analysis in term-rewrite systems to circuit-level prediction of crosstalk noise. The success of SAT solvers motivated many practical applications, but many practical applications have also provided the examples and the challenges that allowed the development of more efficient SAT solvers. This paper provides an overview of some of the most well-known applications of SAT and outlines several other successful applications of SAT. Moreover, the improvements in SAT solvers motivated the development of new algorithms for strategic extensions of SAT. As a result, the paper also provides a brief survey of recent work on extensions of SAT, including pseudo-Boolean constraints, maximum satisfiability, model counting and quantified Boolean formulas.","Discrete event systems,
Conferences"
Wireless Social Community Networks: A Game-Theoretic Analysis,"Wireless social community networks formed by users with a WiFi access point have been created as an alternative to traditional wireless networks that operate in the licensed spectrum. By relying on access points owned by users for access, wireless community networks provide a wireless infrastructure in an inexpensive way. However, the coverage of such a network is limited by the set of users who open their access points to the social community. Currently, it is not clear to what degree this paradigm can serve as a replacement, or a complimentary service, of existing centralized networks operating in licensed bands. In this paper, we study the dynamics of wireless social community networks using, as well as the situation where a wireless social community networks co-exists with a traditional wireless network operating in the licensed spectrum.","Wireless networks,
Subscriptions,
Nash equilibrium,
Quality of service,
Seminars,
Laboratories,
Computer applications,
Computer networks,
Application software,
Computer science"
A Non-generative Approach for Face Recognition Across Aging,"Human faces undergo a lot of change in appearance as they age. Though facial aging has been studied for decades, it is only recently that attempts have been made to address the problem from a computational point of view. Most of these early efforts follow a simulation approach in which matching is performed by synthesizing face images at the target age. Given the innumerable different ways in which a face can potentially age, the synthesized aged image may not be similar to the actual aged image. In this paper, we bypass the synthesis step and directly analyze the drifts of facial features with aging from a purely matching perspective. Our analysis is based on the observation that facial appearance changes in a coherent manner as people age. We provide measures to capture this coherency in feature drifts. Illustrations and experimental results show the efficacy of such an approach for matching faces across age progression.","Face recognition,
Aging,
Humans,
Facial features,
Lighting,
Shape measurement,
Automation,
Computer science,
Educational institutions,
Computational modeling"
A Practical and Flexible Key Management Mechanism For Trusted Collaborative Computing,"Trusted collaborative computing (TCC) is a new research and application paradigm. Two important challenges in such a context are represented by secure information transmission among the collaborating parties and selective differentiated access to data among members of collaborating groups. Addressing such challenges requires, among other things, developing techniques for secure group communication (SGQ), secure dynamic conferencing (SDC), differential access control (DIF-AC), and hierarchical access control (HAC). Cryptography and key management have been intensively investigated and widely applied in order to secure information. However, there is a lack of key management mechanisms which are general and flexible enough to address all requirements arising from information transmission and data access. This paper proposes the first holistic group key management scheme which can directly support all these functions yet retain efficiency. The proposed scheme is based on the innovative concept of access control polynomial (ACP) that can efficiently and effectively support full dynamics, flexible access control with fine-tuned granularity, and anonymity. The new scheme is immune from various attacks from both external and internal malicious parties.","Collaboration,
Access control,
Computer science,
USA Councils,
Cryptography,
Context,
Communication system security,
Resource management,
Collaborative work,
Data security"
Toward a detector-based universal phone recognizer,"In recent research, we have proposed a high-accuracy bottom-up detection-based paradigm for continuous phone speech recognition. The key component of our system was a bank of articulatory detectors each of which computes a score describing an activation level of the specified speech phonetic features that the current frame exhibits. In this work, we present our first attempt at designing a universal phone recognizer using the detection-based approach. We show that our technique is intrinsically language independent since reliable articulatory detectors can be designed for diverse languages, and robust detection can be performed across languages. Moreover, a universal set of detectors is designed by sharing the training material available for several diverse languages. We further demonstrate that our approach makes it possible to decode new target languages by neither retraining nor applying acoustic adaptation techniques. We report phone recognition performance that compares favorably with the best results known by the authors on the OGI Multi-language Telephone Speech corpus.","Detectors,
Speech recognition,
Acoustic signal detection,
Decoding,
Natural languages,
Context modeling,
Robustness,
Telephony,
Automatic speech recognition,
Telecommunication computing"
Hybrid solid-state disks: Combining heterogeneous NAND flash in large SSDs,"NAND-flash-based SSDs (solid-state disks) are recently used in embedded computers to replace power-hungry disks. This paper presents a hybrid approach to large SSDs, which combines MLC flash and SLC flash. The idea is to complement the drawbacks of the two kinds of NAND flash with each other’s advantages. The technical issues of the design of a hybrid SSD pertain to data placement and wear leveling over heterogeneous NAND flash. Our experimental results show that, by adding a 256 MB SLC flash to a 20 GB MLC-flash array, the hybrid SSD improves over a conventional SSD by 4.85 times in terms of average are improved by 17 % and 14%, respectively. The hybrid SSD is only 2% more expensive than a purely MLC-flash-based SSD, for which the extra cost is limited and very rewarded.",
A comparative study on Thai word segmentation approaches,"In this paper, we analyze and compare various approaches for Thai word segmentation. The word segmentation approaches could be classified into two distinct types, dictionary based (DCB) and machine learning based (MLB). The DCB approach relies on a set of terms for parsing and segmenting input texts. Whereas the MLB approach relies on a model trained from a corpus by using machine learning techniques. We compare between two algorithms from the DCB approach: longest-matching and maximal matching, and four algorithms from the MLB approach: Naive Bayes (NB), decision tree, Support Vector Machine (SVM), and Conditional Random Field (CRF). From the experimental results, the DCB approach yielded better performance than the NB, decision tree and SVM algorithms from the MLB approach. However, the best performance was obtained from the CRF algorithm with the precision and recall of 95.79% and 94.98%, respectively.","Machine learning algorithms,
Dictionaries,
Machine learning,
Support vector machines,
Niobium,
Decision trees,
Classification algorithms"
Evaluating Visual Analytics at the 2007 VAST Symposium Contest,"In this article, we report on the contest's data set and tasks, the judging criteria, the winning tools, and the overall lessons learned in the competition. We believe that by organizing these contests, we're creating useful resources for researchers and are beginning to understand how to better evaluate VA tools. Competitions encourage the community to work on difficult problems, improve their tools, and develop baselines for others to build or improve upon. We continue to evolve a collection of data sets, scenarios, and evaluation methodologies that reflect the richness of the many VA tasks and applications.","Visual analytics,
Data visualization,
Data analysis,
Data mining,
Laboratories,
Information services,
Web sites,
Internet,
NIST,
Radio access networks"
Load recognition for different loads with the same real power and reactive power in a non-intrusive load-monitoring system,"This paper proposes the use of power signature to recognize different loads with the same real power and reactive power in a non-intrusive load-monitoring (NILM) system. To test the performance of the proposed approach, the data sets for electrical loads were analyzed and established using an electromagnetic transient program (EMTP) and onsite load measurement. Load recognition techniques were applied in a neural network. The effectiveness of load recognition and the time requirement were analyzed and compared using a back propagation classifier method. The experiments revealed that analyzing the turn-on transient energy signatures can enhance the efficiency of load recognition, particularly for different loads with the same real power and reactive power in a NILM system, and improve ability of computational speed.","Reactive power,
EMTP,
Transient analysis,
Testing,
Electromagnetic analysis,
Performance analysis,
Electric variables measurement,
Electromagnetic measurements,
Neural networks,
Electromagnetic propagation"
Control of an LCC HVDC system for connecting large offshore wind farms with special consideration of grid fault,"This paper describes the control and operation of an HVDC system comprising a line-commuted converter (LCC) HVDC and a STATCOM for connecting offshore wind farms based on DFIGs. During fault on the main grid, fast communications have previously been relied upon to make the wind farm aware of the condition and reduce its power output. Here, an alternative method is examined which enables automatic power balancing during fault. This is achieved through frequency modulation on the offshore network via the STATCOM. Several methods of fault detection using frequency threshold, rate of change of frequency (ROCOF) and rate of change of AC voltage (ROCOVac) are used to indicate when the wind farm power output should be reduced to achieve power balancing, and are compared with results using direct communications. PSCAD/EMTDC simulations show the effectiveness of the proposed control, which allows for faster fault identification. As a result the STATCOM DC over-voltage can be significantly reduced, requiring small DC capacitor, and tripping of the wind farm can be avoided.","Automatic voltage control,
Wind farms,
HVDC transmission,
Converters,
Rectifiers,
Voltage control,
Power harmonic filters"
Legendre-FLANN-based nonlinear channel equalization in wireless communication system,"In this paper, we present the result of our study on the application of artificial neural networks (ANNs) for adaptive channel equalization in a digital communication system using 4-quadrature amplitude modulation (QAM) signal constellation. We propose a novel single-layer Legendre functional-link ANN (L-FLANN) by using Legendre polynomials to expand the input space into a higher dimension. A performance comparison was carried out with extensive computer simulations between different ANN-based equalizers, such as, radial basis function (RBF), Chebyshev neural network (ChNN) and the proposed L-FLANN along with a linear least mean square (LMS) finite impulse response (FIR) adaptive filter-based equalizer. The performance indicators include the mean square error (MSE), bit error rate (BER), and computational complexities of the different architectures as well as the eye patterns of the various equalizers. It is shown that the L-FLANN exhibited excellent results in terms of the MSE, BER and the computational complexity of the networks.","Wireless communication,
Bit error rate,
Artificial neural networks,
Computational complexity,
Adaptive systems,
Adaptive equalizers,
Digital communication,
Amplitude modulation,
Quadrature amplitude modulation,
Constellation diagram"
Building a player strategy model by analyzing replays of real-time strategy games,"Developing computer-controlled groups to engage in combat, control the use of limited resources, and create units and buildings in Real-Time Strategy(RTS) Games is a novel application in game AI. However, tightly controlled online commercial game pose challenges to researchers interested in observing player activities, constructing player strategy models, and developing practical AI technology in them. Instead of setting up new programming environments or building a large amount of agent’s decision rules by player’s experience for conducting real-time AI research, the authors use replays of the commercial RTS game StarCraft to evaluate human player behaviors and to construct an intelligent system to learn human-like decisions and behaviors. A case-based reasoning approach was applied for the purpose of training our system to learn and predict player strategies. Our analysis indicates that the proposed system is capable of learning and predicting individual player strategies, and that players provide evidence of their personal characteristics through their building construction order.","Games,
Buildings,
Artificial intelligence,
Accuracy,
Real time systems,
Construction industry,
Training"
An Integrated Virtual and Remote Control Lab: The Three-Tank System as a Case Study,"Internet-based technologies can supplement traditional laboratories with remote or simulated experimentation sessions. The authors describe their virtual and remote laboratory, which uses a nonlinear system that students can run from anywhere on the Internet. The implementation integrates both open source and commercial software tools.","Control systems,
Internet,
Remote laboratories,
Automatic control,
Computational modeling,
System testing,
Strain control,
Distance learning,
Educational technology,
Knowledge engineering"
A helper thread based EDP reduction scheme for adapting application execution in CMPs,"In parallel to the changes in both the architecture domain - the move toward chip multiprocessors (CMPs) - and the application domain - the move toward increasingly data-intensive workloads - issues such as performance, energy efficiency and CPU availability are becoming increasingly critical. The CPU availability can change dynamically due to several reasons such as thermal overload, increase in transient errors, or operating system scheduling. An important question in this context is how to adapt, in a CMP, the execution of a given application to CPU availability change at runtime. Our paper studies this problem, targeting the energy-delay product (EDP) as the main metric to optimize. We first discuss that, in adapting the application execution to the varying CPU availability, one needs to consider the number of CPUs to use, the number of application threads to accommodate and the voltage/frequency levels to employ (if the CMP has this capability). We then propose to use helper threads to adapt the application execution to CPU availability change in general with the goal of minimizing the EDP. The helper thread runs parallel to the application execution threads and tries to determine the ideal number of CPUs, threads and voltage/frequency levels to employ at any given point in execution. We illustrate this idea using two applications (Fast Fourier Transform and MultiGrid) under different execution scenarios. The results collected through our experiments are very promising and indicate that significant EDP reductions are possible using helper threads. For example, we achieved up to 66.3% and 83.3% savings in EDP when adjusting all the parameters properly in applications FFT and MG, respectively.","Yarn,
Availability,
Application software,
Frequency,
Energy consumption,
Computer architecture,
Voltage,
Energy efficiency,
Runtime,
Computer science"
Response Time Upper Bounds for Fixed Priority Real-Time Systems,"This paper derives closed form upper bounds on the response times of tasks in fixed priority real-time systems. These bounds are valid for tasks with arbitrary deadlines, release jitter, and blocking. Response time upper bounds are given for tasks that are scheduled pre-emptively, co-operatively with intervals where pre-emption is deferred, and non-preemptively. The set of upper bounds for n tasks can be computed in O(n) time, providing a linear-time sufficient schedulability test, applicable to complex commercial real-time systems.","Delay,
Upper bound,
Real time systems,
Job shop scheduling,
System testing,
Processor scheduling,
Jitter,
Computer science,
Computational modeling,
Simulated annealing"
A random walker based approach to combining multiple segmentations,"In this paper we propose an algorithm for combining multiple image segmentations to achieve a final improved segmentation. In contrast to previous works we consider the most general class of segmentation combination, i.e. each input segmentation has an arbitrary number of regions. Our approach is based on a random walker segmentation algorithm which is able to provide high-quality segmentation starting from manually specified seeds. We automatically generate such seeds from an input segmentation ensemble. An information-theoretic optimality criterion is proposed to automatically determine the final number of regions. The experimental results on 300 images with manual ground truth segmentation clearly show the effectiveness of our combination approach.","Image segmentation,
Pixel,
Computer science,
Information theory,
Tin,
Biomedical imaging,
Image analysis,
Voting,
Greedy algorithms,
Clustering algorithms"
Overlapped speech detection for improved speaker diarization in multiparty meetings,"State-of-the-art speaker diarization systems for meetings are now at a point where overlapped speech contributes significantly to the errors made by the system. However, little if no work has yet been done on detecting overlapped speech. We present our initial work toward developing an overlap detection system for improved meeting diarization. We investigate various features, with a focus on high-precision performance for use in the detector, and examine performance results on a subset of the AMI Meeting Corpus. For the high-quality signal case of a single mixed-headset channel signal, we demonstrate a relative improvement of about 7.4% DER over the baseline diarization system, while for the more challenging case of the single far-field channel signal relative improvement is 3.6%. We also outline steps towards improvement and moving beyond this initial phase.","Detectors,
Ambient intelligence,
Computer science,
Merging,
Density estimation robust algorithm,
Speech processing,
Speech recognition,
Speech analysis,
Error analysis"
3D facial expression recognition based on properties of line segments connecting facial feature points,"The 3D facial geometry contains ample information about human facial expressions. Such information is invariant to pose and lighting conditions, which have imposed serious hurdles on many 2D facial analysis problems. In this paper, we perform person and gender independent facial expression recognition based on properties of the line segments connecting certain 3D facial feature points. The normalized distances and slopes of these line segments comprise a set of 96 distinguishing features for recognizing six universal facial expressions, namely anger, disgust, fear, happiness, sadness, and surprise. Using a multi-class support vector machine (SVM) classifier, an 87.1% average recognition rate is achieved on the publicly available 3D facial expression database BU-3DFE [1]. The highest average recognition rate obtained in our experiments is 99.2% for the recognition of surprise. Our result outperforms the result reported in the prior work [2], which uses elaborately extracted primitive facial surface features and an LDA classifier and which yields an average recognition rate of 83.6% on the same database.","Face recognition,
Joining processes,
Facial features,
Support vector machines,
Support vector machine classification,
Spatial databases,
Information geometry,
Humans,
Information analysis,
Linear discriminant analysis"
A Game Theoretical Attack-Defense Model Oriented to Network Security Risk Assessment,"How to quantify the threat probability in network security risk assessment is an important problem to be solved. Most of the existing methods tend to consider the attacker and defender separately. However, the decision to perform the attack is a trade-off between the gain from a successful attack and the possible consequences of detection; meanwhile, the defender’ssecurity strategy depends mostly on the knowledge of the intentions of the attacker. Therefore, ignoring the connections between the attacker and defender’s decisions does not correspond to reality. Game theory is the study of the ways in which strategic interactions among rational players produceoutcomes with respect to the utilities of those players. In this paper, a novel Game Theoretical Attack-Defense Model (GTADM) which quantifies the probability of threats is proposed in order to construct a risk assessment framework. According to the cost-benefit analysis, we define the method of formulating the payoff matrix; the equilibrium of the model is also analyzed. In the end, a simple scenario is presented to illustrate the usage of GTADM in the risk assessment framework to show its efficiency.","Game theory,
Risk management,
Computer networks,
Computer security,
Intrusion detection,
Computer science,
Application software,
Information security,
Stochastic processes,
Software engineering"
"Focus on quality, predicting FRVT 2006 performance","This paper summarizes a study carried out on data from the Face Recognition Vendor Test 2006 (FRVT 2006). The finding of greatest practical importance is the discovery of a strong connection between a relatively simple measure of image quality and performance of state-of-the-art vendor algorithms in FRVT 2006. The image quality measure quantifies edge density and likely relates to focus. This effect is part of a larger four-way interaction observed between edge density, face size and whether images are acquired indoors our outdoors. This finding illustrates the broader potential for statistical modeling of empirical data to play an important role in finding and codifying biometric quality measures.","Focusing,
Density measurement,
Face recognition,
Computer science,
Statistical analysis,
Image quality,
Biometrics,
Algorithm design and analysis,
Glass,
Testing"
A pre-whitening scheme in a MIMO-based spectrum-sharing environment,"In this letter, we propose a pre-whitening scheme for a spectrum-sharing environment where multiple antennas are used for both primary and secondary users. The proposed pre-whitening scheme is different from the conventional postwhitening scheme for mitigating multiple-input and multipleoutput (MIMO) interference because a primary receiver does not require any pre-knowledge about the interference channel which is a more proper assumption in a spectrum-sharing environment. Moreover, the proposed scheme outperforms the conventional post-whitening scheme in terms of the secondary user capacity due to its interference-reduction capability.",
Barrier coverage with airdropped wireless sensors,"Barrier coverage of a wireless sensor network aims at detecting intruders crossing the network. It provides a viable alternative for monitoring boundaries of battlefields, country borders, coastal lines, and perimeters of critical infrastructures. Early studies on barrier coverage typically assume that sensors are deployed uniformly at random in a large area. This assumption, while theoretically interesting, may be unrealistic in real applications. We take a more realistic approach in this paper. In particular, we consider that sensors are airdropped from an aircraft along its flying route. We note that wind, geographic terrain, and other factors may cause a sensor to land in a location deviating from its targeted landing point with a random offset. Thus, it is more realistic to assume that sensor nodes are distributed with a normal offset along the deployment line. Through extensive simulations, we study how variance of the normal distribution, the number of deployment lines, and the distance between adjacent lines would affect barrier coverage. We then investigate how a strong barrier can be formed efficiently from airdropped sensors and compare it with barrier coverage using uniformly distributed sensors. Our results show that the barrier coverage, with appropriately chosen deployment parameters, can be significantly improved using normally distributed sensors.","Wireless sensor networks,
Sensor phenomena and characterization,
Aircraft,
Sea measurements,
Computer science,
Computerized monitoring,
Gaussian distribution,
Weapons,
Region 2,
Environmental factors"
Power to the people: Leveraging human physiological traits to control microprocessor frequency,"Any architectural optimization aims at satisfying the end user. However, modern architectures execute with little to no knowledge about the individual user. If architectures could determine whether their users are satisfied, they could provide higher efficiency; improved reliability, reduced power consumption, increased security, and a better user experience. A major reason for this limitation is their input devices. Specifically, the traditional input devices (e.g., the mouse and keyboard) provide limited information about the user. In this paper, we make a case for the addition of new biometric input devices for providing the computer information about the user’s physiological traits. We explore three biometric devices as potential sensors: an eye tracker, a galvanic skin response (GSR) sensor, and force sensors. We first present two user studies that explore the link between the sensor readings and user satisfaction when the performance of the processor is varied as a video game is being played. In the first study, we drastically drop the processor clock frequency at a set point in the game. In the second study, we set the clock frequency to randomly-selected levels during game play. Both studies show that there are significant changes in human physiological traits as performance decreases. More importantly, we show that physiological changes correlate strongly to the satisfaction levels reported by the users. Based upon these observations, we construct a Physiological Traits-based Power-management (PTP) system that can be applied to existing dynamic voltage and frequency scaling (DVFS) schemes. We apply PTP to a typical CPU-utilization-based adaptive DVFS policy and evaluate our scheme using a third user study. An aggressive version of our PTP scheme reduces the total system power consumption of a laptop by up to 33.3% for an application averaged across users (18.1% averaged across three applications), while a conservative version reduces the total system power consumption by up to 25.6% across users (11.4% averaged across three applications).","Humans,
Microprocessors,
Frequency,
Biosensors,
Games,
Energy consumption,
Biometrics,
Clocks,
Information security,
Mice"
"Asymptotic Critical Total Power for
k
-Connectivity of Wireless Networks","An important issue in wireless ad hoc networks is to reduce the transmission power subject to certain connectivity requirement. In this paper, we study the fundamental scaling law of the minimum total power (termed as critical total power) required to ensure k -connectivity in wireless networks. Contrary to several previous results that assume all nodes use a (minimum) common power, we allow nodes to choose different levels of transmission power. We show that under the assumption that wireless nodes form a homogeneous Poisson point process with density lambda in a unit square region [0, 1]2, the critical total power required to maintain k-connectivity is Theta((Gamma(c/2 + k)/(k - 1)!) lambda1-c/2) with probability approaching one as lambda goes to infinity, where c is the path loss exponent. If k also goes to infinity, the expected critical total power is of the order of kc/2 lambda1-c/2. Compared with the results that all nodes use a common critical transmission power for maintaining k-connectivity, we show that the critical total power can be reduced by an order of (log lambda)c/2 by allowing nodes to optimally choose different levels of transmission power. This result is not subject to any specific power/topology control algorithm, but rather a fundamental property of wireless networks.","Wireless networks,
Mobile ad hoc networks,
H infinity control,
Power system modeling,
Network topology,
Power control,
Telecommunication network reliability,
Computer science,
Protocols,
Distributed algorithms"
Enhancing Multi-user Interaction with Multi-touch Tabletop Displays Using Hand Tracking,"A rear-projection multi-touch tabletop display was augmented with hand tracking utilizing computer vision techniques. Touch detection by frustrated total internal reflection is useful for achieving interaction with tabletop displays, but the technique is not always reliable when multiple users in close proximity simultaneously interact with the display. To solve this problem, we combine touch detection and hand tracking techniques in order to allow multiple users to simultaneously interact with the display without interference. Our hope is that by considering activities occurring on and above a tabletop display, multiuser interaction will become more natural and useful, which should ultimately support collaborative work.","Computer displays,
Cameras,
Optical reflection,
Computer vision,
Skin,
Human computer interaction,
Computer science,
Psychology,
Interference,
Collaborative work"
Robot social presence and gender: Do females view robots differently than males?,"Social-psychological processes in humans will play an important role in long-term human-robot interactions. This study investigates people's perceptions of social presence in robots during (relatively) short interactions. Findings indicate that males tend to think of the robot as more human-like and accordingly show some evidence of “social facilitation” on an arithmetic task as well as more socially desirable responding on a survey administered by a robot. In contrast, females saw the robot as more machine-like, exhibited less socially desirable responding to the robot's survey, and were not socially facilitated by the robot while engaged in the arithmetic tasks. Various alternative accounts of these findings are explored and the implications of these results for future work are discussed.","Humans,
Educational robots,
Robot sensing systems,
Computers,
Interviews,
Analysis of variance"
Color constancy beyond bags of pixels,"Estimating the color of a scene illuminant often plays a central role in computational color constancy. While this problem has received significant attention, the methods that exist do not maximally leverage spatial dependencies between pixels. Indeed, most methods treat the observed color (or its spatial derivative) at each pixel independently of its neighbors. We propose an alternative approach to illuminant estimation—one that employs an explicit statistical model to capture the spatial dependencies between pixels induced by the surfaces they observe. The parameters of this model are estimated from a training set of natural images captured under canonical illumination, and for a new image, an appropriate transform is found such that the corrected image best fits our model.","Layout,
Lighting,
Color,
Pixel,
Reflectivity,
Statistics,
Surface treatment,
Sensor phenomena and characterization,
Image generation,
Filters"
Energy-efficient MESI cache coherence with pro-active snoop filtering for multicore microprocessors,"We present a snoop filtering mechanism for multicore microprocessors that implement coherent caches using the MESI protocol. The relatively small filter structure at each core maintains coarse-grain sharing information about regions within a page to filter out snoops. On broadcast, the sharing status of all regions within the page is collected proactively and up to 90% of unnecessary snoops are eliminated. The energy savings resulting from snoop filtering in our scheme average about 30% across the benchmarks studied for both a quad core design in 65 nm and 8-core design in 45 nm CMOS.","Energy efficiency,
Multicore processing,
Microprocessors,
Information filtering,
Information filters,
Broadcasting,
Protocols,
Energy dissipation,
Computer science,
Switches"
Synthesis of embedded networks for building automation and control,"We present a methodology and a software framework for the automatic design exploration of the communication network among sensors, actuators and controllers in building automation systems. Given 1) a set of end-to-end latency, throughput and packet error rate constraints between nodes, 2) the building geometry, and 3) a library of communication components together with their performance and cost characterization, a synthesis algorithm produces a network implementation that satisfies all end-to-end constraints and that is optimal with respect to installation and maintenance cost. The methodology is applied to the synthesis of wireless networks for an essential step in any control algorithm in a distributed environment: the estimation of control variables such as temperature and air-flow in buildings.","Network synthesis,
Automation,
Automatic control,
Buildings,
Communication system control,
Control system synthesis,
Cost function,
Temperature control,
Communication system software,
Communication networks"
Interference mediation for coexistence of WLAN and ZigBee networks,"In emerging ubiquitous wireless environments, mobile devices can have multiple communication modules such as WLAN and ZigBee. Since IEEE802.15.4 ZigBee devices and IEEE802.11b WLAN devices share the same 2.4GHz ISM band, a ZigBee network operating in a low power environment can be severely interfered by overlapped WLAN networks with much larger bandwidth due to higher transmission power. To overcome this inter-system interference problem and guarantee the ZigBee communications, we propose an interference mediation scheme in an overlaid network environment of WLAN and ZigBee devices by using an interference mediator, and evaluate the performance of ZigBee and WLAN networks for three different resource allocation schemes in terms of throughput and channel occupancy time.","Interference,
Mediation,
Wireless LAN,
ZigBee,
Personal area networks,
Time division multiple access,
Throughput,
Data communication,
Electronic mail,
Ubiquitous computing"
Partial Domain Comprehension in Software Evolution and Maintenance,"Partial comprehension is a necessity in the evolution and maintenance of very large software systems. The programmers form not only partial comprehension of the code, but also partial comprehension of the application domain. To describe the comprehension process, we introduce ontology fragments and investigate how programmers form, use and extend them before and during concept location; concept location is a prerequisite of code changes. We conducted case studies of concept location in two large systems, Eclipse and Mozilla, that both have more than 50,000 methods. Using grep search and ontology fragments, the programmers were able to locate the concepts after inspecting on average less than 10 methods and operating with ontology fragments of around 14 concepts, a very small fraction of the total.","Software maintenance,
Programming profession,
Ontologies,
Software systems,
Application software,
Documentation,
Computer science,
Navigation,
Computer architecture,
Vocabulary"
A Basis for Formal Robustness Checking,"Correct input/output behavior of circuits in presence of internal malfunctions becomes more and more important. But reliable and efficient methods to measure this robustness are not available yet. In this paper a formal measure for the robustness of a circuit is introduced. Then, a first algorithm to determine the robustness is presented. This is done by reducing the problem either to sequential equivalence checking or to a sequence of property checking instances. The technique also identifies those parts of the circuit that are not robust from a functional point of view and therefore have to be hardened during layout.","Robustness,
Circuit faults,
Circuit simulation,
Formal verification,
Fault tolerance,
Single event upset,
Very large scale integration,
Computer science education,
Computer science,
State-space methods"
A modular bio-inspired architecture for movement generation for the infant-like robot iCub,"Movement generation in humans appears to be processed through a three-layered architecture, where each layer corresponds to a different level of abstraction in the representation of the movement. In this article, we will present an architecture reflecting this organization and based on a modular approach to human movement generation. We will show that our architecture is well suited for the online generation and modulation of motor behaviors, but also for switching between motor behaviors. This will be illustrated respectively through an interactive drumming task and through switching between reaching and crawling.",
Toward Realistic and Practical Ideal Observer (IO) Estimation for the Optimization of Medical Imaging Systems,"The ideal observer (IO) employs complete knowledge of the available data statistics and sets an upper limit on observer performance on a binary classification task. However, the IO test statistic cannot be calculated analytically, except for cases where object statistics are extremely simple. Kupinski et al. have developed a Markov chain Monte Carlo (MCMC) based technique to compute the IO test statistic for, in principle, arbitrarily complex objects and imaging systems. In this work, we applied MCMC to estimate the IO test statistic in the context of myocardial perfusion SPECT (MPS). We modeled the imaging system using an analytic SPECT projector with attenuation, distant-dependent detector-response modeling and Poisson noise statistics. The object is a family of parameterized torso phantoms with variable geometric and organ uptake parameters. To accelerate the imaging simulation process and thus enable the MCMC IO estimation, we used discretized anatomic parameters and continuous uptake parameters in defining the objects. The imaging process simulation was modeled by precomputing projections for each organ for a finite number of discretely-parameterized anatomic parameters and taking linear combinations of the organ projections based on continuous sampling of the organ uptake parameters. The proposed method greatly reduces the computational burden and allows MCMC IO estimation for a realistic MPS imaging simulation. We validated the proposed IO estimation technique by estimating IO test statistics for a large number of input objects. The properties of the first- and second-order statistics of the IO test statistics estimated using the MCMC IO estimation technique agreed well with theoretical predictions. Further, as expected, the IO had better performance, as measured by the receiver operating characteristic (ROC) curve, than the Hotelling observer. This method is developed for SPECT imaging. However, it can be adapted to any linear imaging system.","Biomedical imaging,
Statistical analysis,
Computational modeling,
Statistics,
Monte Carlo methods,
System testing,
Myocardium,
Image analysis,
Attenuation,
Torso"
Motion segmentation and scene classification from 3D LIDAR data,"We propose a hierarchical data segmentation method from a 3D high-definition LIDAR laser scanner for cognitive scene analysis in context of outdoor vehicles. The proposed system abstracts the raw information from a parallel laser system (velodyne system). It extracts essential information about drivable road segments in the vicinity of the vehicle and clusters the surrounding scene into point clouds representing static and dynamic objects which can attract the attention of the mission planning system. The system is validated on real data acquired from our experimental vehicle in urban, highway and cross-country scenarios.",Vehicles
Stochastic kriging for simulation metamodeling,"We extend the basic theory of kriging, as applied to the design and analysis of deterministic computer experiments, to the stochastic simulation setting. Our goal is to provide flexible, interpolation-based metamodels of simulation output performance measures as functions of the controllable design or decision variables. To accomplish this we characterize both the intrinsic uncertainty inherent in a stochastic simulation and the extrinsic uncertainty about the unknown response surface. We use tractable examples to demonstrate why it is critical to characterize both types of uncertainty, derive general results for experiment design and analysis, and present a numerical example that illustrates the stochastic kriging method.","Stochastic processes,
Metamodeling,
Computational modeling,
Response surface methodology,
Context modeling,
Stochastic systems,
Decision making,
Linear regression,
Queueing analysis,
Uncertainty"
Optimizing Multiple Object Tracking and Best View Video Synthesis,"We study schemes to tackle problems of optimizing multiple object tracking and best-view video synthesis. A novel linear relaxation method is proposed for the class of multiple object tracking problems where the inter-object interaction metric is convex and the intra-object term quantifying object state continuity may use any metric. This scheme models object tracking as multi-path searching. It explicitly models track interaction, such as object spatial layout consistency or mutual occlusion, and optimizes multiple object tracks simultaneously. The proposed scheme does not rely on track initialization and complex heuristics. It has much less average complexity than previous efficient exhaustive search methods such as extended dynamic programming and can find the global optimum with high probability. Given the tracking data from our method, optimizing best-view video synthesis using multiple-view videos is further studied, which is formulated as a recursive decision problem and optimized by a dynamic programming approach. The proposed object tracking and best-view synthesis methods have found successful applications in MyView - a system to enhance media content presentation of multiple-view video.","Filtering,
Dynamic programming,
Target tracking,
Optimization methods,
Linear programming,
Relaxation methods,
Search methods,
Surveillance,
Multimedia systems,
Computer science"
ToF-sensors: New dimensions for realism and interactivity,"A growing number of applications depend on accurate and fast 3D scene analysis. Examples are object recognition, collision prevention, 3D modeling, mixed reality, and gesture recognition. The estimation of a range map by image analysis or laser scan techniques is still a time-consuming and expensive part of such systems.","Human computer interaction,
Image analysis,
Application software,
Object recognition,
Virtual reality,
Laser modes,
Robustness,
Distance measurement,
Cameras,
Computer vision"
Real-Time All-in-Focus Video-Based Rendering Using A Network Camera Array,"We present a real-time video-based rendering system using a network camera array. Our system consists of 64 commodity network cameras that are connected to a single PC through a Gigabit Ethernet. To render a high-quality novel view, we estimate a view-dependent per-pixel depth map in real-time by using a layered representation. The rendering algorithm is fully implemented on a GPU, which allows our system to efficiently use CPU and GPU independently and in parallel. Using QVGA input video resolution, our system renders a free-viewpoint video at up to 30 fps depending on rendering parameters. Experimental results show high-quality images synthesized from various scenes.","Cameras,
Rendering (computer graphics),
Layout,
Real time systems,
Computer graphics,
Network synthesis,
Ethernet networks,
Interpolation,
Reconstruction algorithms,
Firewire"
Robust 2D Ear Registration and Recognition Based on SIFT Point Matching,"Significant recent progress has shown ear recognition to be a viable biometric. Good recognition rates have been demonstrated under controlled conditions, using manual registration or with specialised equipment. This paper describes a new technique which improves the robustness of ear registration and recognition, addressing issues of pose variation, background clutter and occlusion. By treating the ear as a planar surface and creating a homography transform using SIFT feature matches, ears can be registered accurately. The feature matches reduce the gallery size and enable a precise ranking using a simple 2D distance algorithm. When applied to the XM2VTS database it gives results comparable to PCA with manual registration. Further analysis on more challenging datasets demonstrates the technique to be robust to background clutter, viewing angles up to ±13 degrees and with over 20% occlusion.","Robustness,
Ear,
Probes,
Cameras,
Biometrics,
Computer science,
Surface treatment,
Spatial databases,
Principal component analysis,
Lighting control"
An Integrated Virtual Environment for Active and Collaborative e-Learning in Theory of Computation,"Active and collaborative learning provides a powerful mechanism to enhance depth of learning, increase material retention, and get students involved with the material instead of passively listening to a lecture. In this paper, a research using web-based active and collaborative learning in the theory of computation and related fields is presented. The twofold contribution of this work is a novel use of existing technology to improve learning and a longitudinal quasi-experimental evaluation of its use in context. As a first contribution, we introduce an integrated environment that is designed to meet the active learning preferences of computer engineering learners, in addition to a support for collaborative learning. For the second contribution: several classroom experiments are carried out. The analysis of the experiments' outcomes and the students feed back show that our integrated environment is useful as a learning tool, in addition to enhancing learners' motivation to seek more knowledge and information on their own.",
Mitigation techniques for single event induced charge sharing in a 90 nm bulk CMOS process,"Mitigation techniques to reduce the increased SEU cross-section associated with charge sharing in a 90 nm DICE latch are proposed. The increased error cross-section is caused by heavy ion angular strikes depending on the directionality of the ion vector, thereby exacerbating charge sharing among multiple circuit nodes. The use of nodal separation as a mitigation technique shows an order of magnitude decrease on upset cross-section compared to a conventional layout and the use of guard-rings show no noticeable effect on upset cross-section.","CMOS process,
Latches,
Rails,
Redundancy,
Isolation technology,
Circuit synthesis,
MOS devices,
MOSFETs,
Space charge,
Single event upset"
Master failures in the Precision Time Protocol,"If all clocks within a distributed system share the same notion of time, the application domain can gain several advantages. Among those is the possibility to implement real-time behavior, accurate time stamping, and event detection. However, with the wide spread application of clock synchronization another topic has to be taken into consideration: the fault tolerance. The well known clock synchronization protocol IEEE1588 (Precision Time Protocol, PTP), is based on a master/slave principle, which has one severe disadvantage. This disadvantage is the fact that the failure of a master automatically requires the re-election of a new master. The start of a master election based on timeout and thus takes a certain time span during which the clocks are not synchronized and thus running freely. Moreover the usage of a new master also requires new delay measurements, which prolong the time of uncertainty as well. This paper analyzes the results of such a master failure and proposes democratic master groups instead of hot-stand-by masters to overcome this problem by. It is shown by means of simulation that the proposed solution will not deteriorate the accuracy of the slave clocks in case of a master failure.",
A Design Quality Model for Service-Oriented Architecture,"Service-Oriented Architecture (SOA) is emerging as an effective solution to deal with rapid changes in the business environment. To handle fast-paced changes, organizations need to be able to assess the quality of its products prior to implementation. However, literature and industry has yet to explore the techniques for evaluating design quality of SOA artifacts. To address this need, this paper presents a hierarchical quality assessment model for early assessment of SOA system quality. By defining desirable quality attributes and tracing necessary metrics required to measure them, the approach establishes an assessment model for identification of metrics at different abstraction levels. Using the model, design problems can be detected and resolved before they work into the implemented system where they are more difficult to resolve. The model is validated against anempirical study on an existing SOA system to evaluate the quality impact from explicit and implicit changes to its requirements.","Service oriented architecture,
Semiconductor optical amplifiers,
Object oriented modeling,
Quality assessment,
Quality management,
Software engineering,
Computer science,
Design engineering,
Electronic mail,
Face detection"
Monolithic integration of lateral field-effect rectifier with normally-off HEMT for GaN-on-Si switch-mode power supply converters,"A lateral field-effect rectifier (L-FER) that can be fabricated with normally-off transistor on the same AlGaN/GaN HEMT with the same fabrication process has been demonstrated. The L-FER exhibits low turn-on voltage, low specific on-resistance and high reverse breakdown. A prototype of switch-mode dc-dc Boost converter that features monolithically integrated L-FER and normally-off HEMT is demonstrated for the first time using industry-standard GaNon-Si epitaxial wafers to prove the feasibility of GaN power inegrated technology.","Monolithic integrated circuits,
Rectifiers,
HEMTs,
Power supplies,
Switching converters,
Gallium nitride,
Breakdown voltage,
MODFETs,
Aluminum gallium nitride,
Fabrication"
Cheek to Chip: Dancing Robots and AI's Future,"Recent generations of humanoid robots increasingly resemble humans in shape and articulatory capacities. This progress has motivated researchers to design dancing robots that can mimic the complexity and style of human choreographic dancing. Such complicated actions are usually programmed manually and ad hoc. However, this approach is both tedious and inflexible. Researchers at the University of Tokyo have developed the learning-from-observation (LFO) training method to overcome this difficulty.1-2 LFO enables a robot to acquire knowledge of what to do and how to do it from observing human demonstrations. Direct mapping from human joint angles to robot joint angles doesn't work well because of the dynamic and kinematic differences between the observed person and the robot (for example, weight, balance, and arm and leg lengths). LFO therefore relies on predesigned task models, which represent only the actions (and features thereof) that are essential to mimicry. Then it adapts these actions to the robot's morphology and dynamics so that it can mimic the movement. This indirect, two-step mapping is crucial for robust imitation and performance.","Humans,
Service robots,
Legged locomotion,
Leg,
Intelligent robots,
Humanoid robots,
Biological system modeling,
Shape,
Kinematics,
Morphology"
Heuristics to Classify Internet Backbone Traffic based on Connection Patterns,"In this paper Internet backbone traffic is classified on transport layer according to network applications. Classification is done by a set of heuristics inspired by two previous articles and refined in order to better reflect a rough and highly aggregated backbone environment. Obvious misclassified flows by the existing two approaches are revealed and updated heuristics are presented, excluding the revealed false positives, but including missed P2P streams. The proposed set of heuristics is intended to provide researchers and network operators with a relatively simple and fast method to get insight into the type of data carried by their links. A complete application classification can be provided even for short 'snapshot' traces, including identification of attack and malicious traffic. The usefulness of the heuristics is finally shown on a large dataset of backbone traffic, where in the best case only 0.2% of the data is left unclassified.","Internet,
Spine,
Telecommunication traffic,
Payloads,
IP networks,
Fingerprint recognition,
Training data,
Law,
Legal factors,
Computer science"
Near real-time stereo based on effective cost aggregation,"Recent research activity on stereo matching has proved the efficacy of local approaches based on advanced cost aggregation strategies in accurately retrieving 3D information. However, accuracy is typically achieved at expense of computational efficiency, with best methods being far from meeting real-time requirements. On the other side, basic real-time local algorithms relying on a rectangular correlation window suffer from significant ambiguity along depth borders and untextured areas. This work proposes a novel local approach aimed at maximizing the speed-accuracy trade-off by means of an efficient segmentation-based cost aggregation strategy.","Costs,
Image segmentation,
Information retrieval,
Computational efficiency,
Real time systems,
Computer science,
Q measurement"
Computer Optimized Design of Electron Guns,"This paper considers the problem of designing electron guns using computer optimization techniques. Several different design parameters are manipulated while considering multiple design criteria, including beam and gun properties. The optimization routines are described. Examples of guns designed using these techniques are presented. Future research is also described.","Design optimization,
Electron guns,
Electron beams,
Computational modeling,
Radio frequency,
Klystrons,
Electron tubes,
Defense industry,
Design engineering,
Optical computing"
Flow-level performance of opportunistic OFDM-TDMA and OFDMA networks,"In this paper, the flow-level performance of opportunistic scheduling in orthogonal frequency division multiplexing (OFDM) networks is studied. The analysis accounts for the applications with a dynamic number of competing flows, such as continuous transfers of file transport protocol (FTP) or web browsing sessions. An analytical model is developed to extend the multi-class processor-sharing model in single-carrier networks to multi-carrier OFDM networks, where the total service rate varies with the number of flows. Based on the analytical model, the scheduling gains in both OFDM-TDMA (time division multiple access) and OFDMA (orthogonal frequency division multiple access) networks are evaluated for low and moderate signal-to-noise ratio (SNR). Different from previous works, we focus on the scheduling performance at the flow level and consider a dynamic network setting with random sized service demands. Furthermore, we use stochastic comparison techniques to examine the effects of physical-layer characteristics, such as fading speed and channel frequency selectivity, on flow-level performance. Simulations are performed to verify the analytical results.","OFDM,
Analytical models,
Processor scheduling,
Transport protocols,
Time division multiple access,
Frequency conversion,
Signal to noise ratio,
Dynamic scheduling,
Stochastic processes,
Fading"
"Scientific Workflow Systems for 21st Century, New Bottle or New Wine?","With the advances in e-Sciences and the growing complexity of scientific analyses, more and more scientists and researchers are relying on workflow systems for process coordination, derivation automation, provenance tracking, and bookkeeping. While workflow systems have been in use for decades, it is unclear whether scientific workflows can or even should build on existing workflow technologies, or they require fundamentally new approaches. In this paper, we analyze the status and challenges of scientific workflows, investigate both existing technologies and emerging languages, platforms and systems, and identify the key challenges that must be addressed by workflow systems for e-science in the 21st century.",
Near-Optimal Sparse Recovery in the L1 Norm,"We consider the *approximate sparse recovery problem*, where the goal is to (approximately) recover a high-dimensional vector x from Rn from its lower-dimensional *sketch* Ax from Rm.Specifically, we focus on the sparse recovery problem in the L1 norm: for a parameter k, given the sketch Ax, compute an approximation x' of x such that the L1 approximation error | |x-x'| | is close to minimum  of | |x-x*| |  over all vectors x* with at most k terms. The sparse recovery problem  has been subject to extensive research over the last few years.Many solutions to this problem have been discovered, achieving different trade-offs between various attributes, such as the sketch length, encoding and recovery times.In this paper we provide a sparse recovery scheme which achieves close to optimal performance on virtually all attributes.  In particular, this is the first recovery scheme that guarantees k log(n/k) sketch length, and near-linear n log (n/k) recovery time *simultaneously*. It also features low encoding and update times, and is noise-resilient.","Encoding,
Approximation error,
Vectors,
Computer science,
Computer errors,
Linearity,
Compressed sensing,
Data acquisition,
Hardware,
Analog computers"
Gaussian belief propagation based multiuser detection,"In this work, we present a novel construction for solving the linear multiuser detection problem using the Gaussian Belief Propagation algorithm. Our algorithm yields an efficient, iterative and distributed implementation of the MMSE detector. Compared to our previous formulation, the new algorithm offers a reduction in memory requirements, the number of computational steps, and the number of messages passed. We prove that a detection method recently proposed by Montanari et al. is an instance of ours, and we provide new convergence results applicable to both.","Algorithm design and analysis,
Multiaccess communication,
Detectors,
Vectors,
Convergence,
Construction industry,
Multiuser detection"
Robust L1 Principal Component Analysis and Its Bayesian Variational Inference,"We introduce a robust probabilistic L1-PCA model in which the conventional gaussian distribution for the noise in the observed data was replaced by the Laplacian distribution (or L1 distribution). Due to the heavy tail characteristics of the L1 distribution, the proposed model is supposed to be more robust against data outliers. In this letter, we demonstrate how a variational approximation scheme enables effective inference of key parameters in the probabilistic L1-PCA model. As the L1 density can be expanded as a superposition of infinite number of gaussian densities, we express the L1-PCA model as a marginalized model over the superpositions. By doing so, a tractable Bayesian inference can be achieved based on the variational expectation-maximization-type algorithm.",
Reconfigurable BDD based quantum circuits,"We propose a novel binary decision diagram (BDD) based reconfigurable logic architecture based on Split-Gate quantum nanodots using III-V compound semiconductor-based quantum wells. While BDD based quantum devices architectures have already been demonstrated to be attractive for achieving ultra-low power operation, our design provides the ability to reconfigure the functionality of the logic architecture. This work proposes device and architectural innovations to support such reconfiguration. At the device level, a unique programmability feature is incorporated in our proposed nanodot devices which can operate in 3 distinct operation modes: a) active b) open and c) short mode based on the split gate bias voltages and enable functional reconfiguration. At the architectural level, we address programmability and design fabric issues involved with mapping BDD’s into a reconfigurable architecture. By mapping a set of logic circuits, we demonstrate that our underlying device and architectural structure is flexible to support different functions.","Logic gates,
Data structures,
Boolean functions,
Fabrics,
Nanoscale devices,
Tunneling,
Junctions"
Improving the Readability of Clustered Social Networks using Node Duplication,"Exploring communities is an important task in social network analysis. Such communities are currently identified using clustering methods to group actors. This approach often leads to actors belonging to one and only one cluster, whereas in real life a person can belong to several communities. As a solution we propose duplicating actors in social networks and discuss potential impact of such a move. Several visual duplication designs are discussed and a controlled experiment comparing network visualization with and without duplication is performed, using 6 tasks that are important for graph readability and visual interpretation of social networks. We show that in our experiment, duplications significantly improve community-related tasks but sometimes interfere with other graph readability tasks. Finally, we propose a set of guidelines for deciding when to duplicate actors and choosing candidates for duplication, and alternative ways to render them in social network representations.","Social network services,
Clustering,
Layout,
Communities"
Robust motion estimation and structure recovery from endoscopic image sequences with an Adaptive Scale Kernel Consensus estimator,"To correctly estimate the camera motion parameters and reconstruct the structure of the surrounding tissues from endoscopic image sequences, we need not only to deal with outliers (e.g., mismatches), which may involve more than 50% of the data, but also to accurately distinguish inliers (correct matches) from outliers. In this paper, we propose a new robust estimator, Adaptive Scale Kernel Consensus (ASKC), which can tolerate more than 50 percent outliers while automatically estimating the scale of inliers. With ASKC, we develop a reliable feature tracking algorithm. This, in turn, allows us to develop a complete system for estimating endoscopic camera motion and reconstructing anatomical structures from endoscopic image sequences. Preliminary experiments on endoscopic sinus imagery have achieved promising results.","Motion estimation,
Robustness,
Image sequences,
Kernel,
Surgery,
Cameras,
Anatomy,
Biomedical imaging,
Image reconstruction,
Statistics"
TCP over WiMAX: A Measurement Study,"We present active measurement results from a commercial IEEE 802.16/WiMAX-based network, with primary focus on TCP performance. We compare four TCP variants, namely New Reno, Cubic, Vegas and Veno, using throughput, round-trip time (RTT), and retransmission rate metrics. While all TCP variants achieve similar throughput, they do so in different ways, with different impacts on the network performance. We identify adverse effects of TCP window auto-tuning in this environment and demonstrate that on the downlink, congestion losses dominate wireless transmission errors. We reveal several issues for this WiMAX-based network, including limited bandwidth for TCP, high RTT and jitter, and unfairness during bidirectional transfers. Such a network environment may be challenging for many wireless Internet applications, such as remote login, VoIP, and video streaming.","WiMAX,
Internet,
Downlink,
Throughput,
Streaming media,
Bandwidth,
Portable media players,
Quality of service,
OFDM modulation,
Propagation losses"
Video-based activity and movement pattern analysis in overnight sleep studies,We present a non-contact monitoring system to measure the quality of sleep using near-infrared video in this paper. We envision a smart home environment in which a processing module can be installed in the bedroom to record and monitor sleep in a noninvasive manner. We describe the procedure adopted to infer motion information and discuss the method for estimating wake/sleep status from the acquired video. Performance of the proposed system was evaluated through comparison with simultaneous recordings of actigraph and polysomnography (PSG) data.,"Pattern analysis,
Video recording,
Monitoring,
Cameras,
Costs,
Image sequences,
Wrist,
Laboratories,
Sleep apnea,
Performance analysis"
Working Across Borders: Overcoming Culturally-Based Technology Challenges in Student Global Software Development,"Facilitated by the Internet, global software development has emerged as a reality. The use of shared processes and appropriate tools is considered crucial to alleviate some of its issues (e.g., space and time differences), homogenizing the environment of development and interaction, and increasing the likelihood of success. Since 2005, Pace University in the United States has been collaborating with the Institute of Technology of Cambodia (ITC) and the University of Delhi in India to bring students together to work on global software development projects. This paper reports on our experiences and lessons from spring 2007 when the focus was on these students working together on the development of a single software system. One key objective was to investigate how to create a shared and open source tooling environment to support a distributed development process that has evolved over two years. The setting is unique in that it seeks to accommodate students from a mix of established, developing and emerging countries who, as a consequence, have had varying levels of exposure to the Internet and use it in non-similar ways. The findings, lessons and recommendations from our study are reported in this paper. Not surprisingly, when the perceived professional value of assumed ‘everyday technologies’ is dissimilar across cultures, preparation for the communications tooling needs more attention than the engineering tooling. This has important implications for the emphasis placed on ‘process’ and ‘soft skills’ in the respective classrooms, and highlights some challenges facing emerging countries as they strive to become players in the global workforce.","Programming,
Software engineering,
Space technology,
Internet,
Collaborative software,
Collaborative work,
International collaboration,
Educational technology,
Teamwork,
Outsourcing"
Comparing SIFT descriptors and gabor texture features for classification of remote sensed imagery,"A richer set of land-cover classes are observable in satellite imagery than ever before due to the increased sub-meter resolution. Individual objects, such as cars and houses, are now recognizable. This work considers a new category of image descriptors based on local measures of saliency for labelling land-cover classes characterized by identifiable objects. These descriptors have been successfully applied to object recognition in standard (non-remote sensed) imagery. We show they perform comparably to state-of-the-art texture descriptors for classifying complex land-cover classes in high-resolution satellite imagery while being approximately an order of magnitude faster to compute. This speedup makes them attractive for realtime applications. To the best of our knowledge, this is the first time this new category of descriptors has been applied to the classification of remote sensed imagery.",
Pruning Support Vector Machines Without Altering Performances,"Support vector machines (SV machines, SVMs) have many merits that distinguish themselves from many other machine-learning algorithms, such as the nonexistence of local minima, the possession of the largest distance from the separating hyperplane to the SVs, and a solid theoretical foundation. However, SVM training algorithms such as the efficient sequential minimal optimization (SMO) often produce many SVs. Some scholars have found that the kernel outputs are frequently of similar levels, which insinuate the redundancy of SVs. By analyzing the overlapped information of kernel outputs, a succinct separating-hyperplane-securing method for pruning the dispensable SVs based on crosswise propagation (CP) is systematically developed. The method also circumvents the problem of explicitly discerning SVs in feature space as the SVM formulation does. Experiments with the famous SMO-based software LibSVM reveal that all typical kernels with different parameters on the data sets contribute the dispensable SVs. Some 1% ~ 9% (in some scenarios, more than 50%) dispensable SVs are found. Furthermore, the experimental results also verify that the pruning method does not alter the SVMs' performances at all. As a corollary, this paper further contributes in theory a new lower upper bound on the number of SVs in the high-dimensional feature space.","Support vector machines,
Kernel,
Upper bound,
Support vector machine classification,
Neural networks,
Computer science,
Solids,
Information analysis,
Concrete,
Operations research"
GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models,"Bayesian filtering is a general framework for recursively estimating the state of a dynamical system. The most common instantiations of Bayes filters are Kalman filters (extended and unscented) and particle filters. Key components of each Bayes filter are probabilistic prediction and observation models. Recently, Gaussian processes have been introduced as a non-parametric technique for learning such models from training data. In the context of unscented Kalman filters, these models have been shown to provide estimates that can be superior to those achieved with standard, parametric models. In this paper we show how Gaussian process models can be integrated into other Bayes filters, namely particle filters and extended Kalman filters. We provide a complexity analysis of these filters and evaluate the alternative techniques using data collected with an autonomous micro-blimp.","Predictive models,
Kernel,
Robots,
Training data,
Prediction algorithms,
Data models,
Computational modeling"
Top-k Spatial Joins of Probabilistic Objects,"Probabilistic data have recently become popular in applications such as scientific and geospatial databases. For images and other spatial datasets, probabilistic values can capture the uncertainty in extent and class of the objects in the images. Relating one such dataset to another by spatial joins is an important operation for data management systems. We consider probabilistic spatial join (PSJ) queries, which rank the results according to a score that incorporates both the uncertainties associated with the objects and the distances between them. We present algorithms for two kinds of PSJ queries: Threshold PSJ queries, which return all pairs that score above a given threshold, and top-k PSJ queries, which return the k top-scoring pairs. For threshold PSJ queries, we propose a plane sweep algorithm that, because it exploits the special structure of the problem, runs in O(n (log n + k)) time, where n is the number of points and k is the number of results. We extend the algorithms to 2-D data and to top-k PSJ queries. To further speed up top-k PSJ queries, we develop a scheduling technique that estimates the scores at the level of blocks, then hands the blocks to the plane sweep algorithm. By finding high-scoring pairs early, the scheduling allows a large portion of the datasets to be pruned. Experiments demonstrate speed-ups of two orders of magnitude.","Uncertainty,
Spatial databases,
Image databases,
Biomedical imaging,
Satellites,
Airports,
Microscopy,
Image analysis,
Computer science,
Application software"
Toward secure network coding in wireless networks: Threats and challenges,"In recent years, network coding has emerged as a new communication paradigm that can significantly improve the efficiency of network protocols by requiring intermediate nodes to mix packets before forwarding them. Recently, several real-world systems have been proposed to leverage network coding in wireless networks. Although the theoretical foundations of network coding are well understood, a real-world system needs to solve a plethora of practical aspects before network coding can meet its promised potential. These practical design choices expose network coding systems to a wide range of attacks. In this paper, we identify two general frameworks that encompass several network coding-based systems proposed for unicast in wireless networks. Our systematic analysis of the components of these frameworks reveals vulnerabilities to a wide range of attacks, which may severely degrade system performance. Adequate understanding of these threats is essential to effectively design secure practical network coding systems.","Encoding,
Protocols,
Security,
Routing protocols,
Pollution,
Wireless networks,
Unicast"
A New Provably Secure Certificateless Signature Scheme,"Certificateless public key cryptography was introduced by Al-Riyami and Paterson to overcome the key escrow problem of ID-PKC. In this paper, we present an efficient certificateless signature scheme using bilinear maps. The scheme can be proved secure in the strongest security model of certificateless signature schemes. In terms of computational cost, totally, only two pairing operations are required for signing and verification. It is more efficient than the other existing certificateless signature schemes secure against a super type I/II adversary.","Public key,
Public key cryptography,
Educational institutions,
Mathematics,
Computer science,
Identity-based encryption,
Information security,
Communications Society,
Computational efficiency,
Telephony"
FOODS: A Food-Oriented Ontology-Driven System,"In this paper the authors present the design and development of a counseling system for food or menu planning in a restaurant, clinic/hospital, or at home, the Food-Oriented Ontology-Driven System (FOODS). FOODS comprises (a) a food ontology, (b) an expert system using the ontology, and some knowledge about cooking methods and prices, and (c) a user interface suitable for novices in computers and diets as well as for experts. The ontology contains specifications of ingredients, substances, nutrition facts, recommended daily intakes for different regions, dishes, and menus. The expert system assists in finding the appropriate dish or menu for the consumer, client or customer, who use FOODS by entering their favorite ingredients, ingredients to avoid, favorite flavors, and so on. In the health section users can provide their gender, age, height and weight, which will be used to calculate such data as the body mass index. With FOODS enterprises can assist customers through an appropriate suggestion of dishes and meals with the help of individual nutritional profiles. SMEs that might be interested in using FOODS are institutions for training and instruction of cooking, restaurants, clinics, hospitals, together with clinical and therapeutical dietitians and nutritional therapists. In the long run such systems might become part of the emerging consumer health informatics portfolio.",
Designing an agile methodology for mobile software development: A hybrid method engineering approach,"New Advances in mobile computer technology and the rapid growth of wireless networks in quality and quantity has introduced new applications and concerns in computer science and industry. The unique requirements and constraints associated with mobile systems have brought new challenges to software development for such environments, as it demands extensive improvements to traditional systems development methodologies in order to fulfill the special needs of this field. We examine the challenges of developing software for mobile systems, starting by reviewing mobile systems’ characteristics and investigating the status quo of mobile software development methods. It has been shown that Agile methodologies are appropriate methods for the development of such systems; based on this assumption, we identify specific requirements for a mobile software development methodology, based on which a new agile method is engineered using the Hybrid Methodology Design approach. We claim that this methodology, and the approach used for its construction, can facilitate the application of a software engineering approach to the production of mobile software systems.","Mobile communication,
Programming,
Software,
Design methodology,
Product development,
Computer architecture,
Mobile computing"
Efficient synthesis of compressor trees on FPGAs,"FPGA performance is currently lacking for arithmetic circuits. Large sums of k > 2 integer values is a computationally intensive operation in applications such as digital signal and video processing. In ASIC design, compressor trees, such as Wallace and Dadda trees, are used for parallel accumulation; however, the LUT structure and fast carry-chains employed by modern FPGAs favor trees of carry-propagate adders (CPAs), which are a poor choice for ASIC design. This paper presents the first method to successfully synthesize compressor trees on LUT-based FPGAs. In particular, we have found that generalized parallel counters (GPCs) map quite well to LUTs on FPGAs; a heuristic, presented within, constructs a compressor tree from a library of GPCs that can efficiently be implemented on the target FPGA. Compared to the ternary adder trees produced by commercial synthesis tools, our heuristic reduces the combinational delay by 27.5%, on average, within a tolerable average area increase of 5.7%.","Field programmable gate arrays,
Application specific integrated circuits,
Table lookup,
Adders,
Circuit synthesis,
Signal synthesis,
Arithmetic,
Signal processing,
Video compression,
Process design"
A backlog-based CSMA mechanism to achieve fairness and throughput-optimality in multihop wireless networks,We propose and analyze a distributed backlog-based CSMA policy to achieve fairness and throughput-optimality in wireless multihop networks. The analysis is based on a CSMA fixed point approximation that is accurate for large networks with many small flows and a small sensing period.,"Multiaccess communication,
Spread spectrum communication,
Wireless networks,
Wireless sensor networks,
Interference constraints,
Telecommunication traffic,
Computer science,
Throughput,
Optical feedback,
Optical fiber networks"
SVATS: A Sensor-Network-Based Vehicle Anti-Theft System,"Today vehicle theft rate is very high, thus tracking/alarming systems are being deployed with an increasingly popularity. These systems however bear some limitations such as high cost, high false-alarm rate, and easy to be disabled. This paper describes the design, implementation and evaluation of a Sensor-network-based Vehicle Anti-Theft System (SVATS) to address these limitations. In this system, the sensors in the vehicles that are parked within the same parking area first form a sensor network, then monitor and identify possible vehicle thefts by detecting unauthorized vehicle movement. When an unauthorized movement is detected, an alert will be reported to a base station in the parking area, which sends warning messages to the security office. This paper focuses on the technical issues specific to the system such as topology management, theft detection, and intra-vehicle networking.","Vehicle detection,
Global Positioning System,
Costs,
Base stations,
Computer science,
Sensor systems,
Monitoring,
Alarm systems,
Broadcasting,
Communications Society"
HomeMesh: a low-cost indoor wireless mesh for home networking,"Wi-Fi access technology has become popular in recent years. Many users nowadays use Wi-Fi to gain wireless access to the Internet from offices, public libraries, shopping malls, homes, and other places. However, current Wi-Fi deployment is limited to areas where wired LAN is available. Due to its relatively short transmission range in indoor environments (typically several tens of meters), Wi-Fi coverage needs to be extended significantly to full coverage of a certain area. The wireless mesh network (WMN) is a practical and effective solution. In this article we present HomeMesh, an off-the-shelf, simple, and cost-effective WMN for the indoor home environment. HomeMesh is based on simple protocols, implementable in normal notebooks or PCs, and is compatible with existing Wi-Fi APs and clients (i.e., no AP and client modifications). To achieve better end-to-end delay and throughput, HomeMesh dynamically selects its access path based on the ETX metric. We have implemented HomeMesh and conducted proofof- concept experiments in an indoor environment. Our mesh solution is shown to be effective in improving Wi-Fi services.","Internet,
Throughput,
Home computing,
Radio spectrum management,
Hardware,
Computer architecture,
Costs,
Frequency,
Protocols"
Toward a Perceptual Theory of Flow Visualization,"Currently, most researchers in visualization pay very little attention to vision science. The exception is when the effective use of color is the subject. Little research in flow visualization includes a discussion of the related perceptual theory. Nor does it include an evaluation of effectiveness of the display techniques that are generated. This is so, despite Laidlaw's paper showing that such an evaluation is relatively straightforward. Of course, it's not always necessary to relate visualization research to perceptual theory. If the purpose of the research is to increase the efficiency of an algorithm, then the proper test is one of efficiency, not of perceptual validity. But when a new representation of data is the subject of research, addressing how perceptually effective it is - either by means of a straightforward empirical comparison with existing methods or analytically, relating the new mapping to perceptual theory - should be a matter of course. A strong interdisciplinary approach, including the disciplines of perception, design, and computer science will produce better science and better design in that empirically and theoretically validated visual display techniques will result.","Data visualization,
Neurons,
Testing,
Humans,
Filtering,
Algorithm design and analysis,
Guidelines,
Biological system modeling,
Displays,
Psychology"
Automatic registration of aerial imagery with untextured 3D LiDAR models,"A fast 3D model reconstruction methodology is desirable in many applications such as urban planning, training, and simulations. In this paper, we develop an automated algorithm for texture mapping oblique aerial images onto a 3D model generated from airborne Light Detection and Ranging (LiDAR) data. Our proposed system consists of two steps. In the first step, we combine vanishing points and global positioning system aided inertial system readings to roughly estimate the extrinsic parameters of a calibrated camera. In the second step, we refine the coarse estimate of the first step by applying a series of processing steps. Specifically, We extract 2D corners corresponding to orthogonal 3D structural corners as features from both images and the untextured 3D LiDAR model. The correspondence between an image and the 3D model is then performed using Hough transform and generalized M-estimator sample consensus. The resulting 2D corner matches are used in Lowe’s algorithm to refine camera parameters obtained earlier. Our system achieves 91% correct pose recovery rate for 90 images over the downtown Berkeley area, and overall 61% accuracy rate for 358 images over the residential, downtown and campus portions of the city of Berkeley.","Laser radar,
Cameras,
Clouds,
Image reconstruction,
Urban planning,
Geometry,
Solid modeling,
Image generation,
Application software,
Computational modeling"
The MPEG Query Format: Unifying Access to Multimedia Retrieval Systems,The growth of multimedia is increasing the need for standards for accessing and searching distributed repositories. The moving picture experts group (MPEG) is developing the MPEG query format (MPQF) to standardize this interface as part of MPEG-7. The objective is to make multimedia access and search easier and interoperable across search engines and repositories. This article describes the MPQF and highlights some of the ways it goes beyond today's query languages by providing capabilities for multimedia query-by-example and spatiotemporal queries.,"Multimedia systems,
Database languages,
Multimedia databases,
MPEG 7 Standard,
XML,
Information retrieval,
Diversity reception,
Laboratories,
Mobile handsets,
TV"
Fast and Memory Efficient Mining of High Utility Itemsets in Data Streams,"Efficient mining of high utility itemsets has become one of the most interesting data mining tasks with broad applications. In this paper, we proposed two efficient one-pass algorithms, MHUI-BIT and MHUI-TID, for mining high utility itemsets from data streams within a transaction-sensitive sliding window. Two effective representations of item information and an extended lexicographical tree-based summary data structure are developed to improve the efficiency of mining high utility itemsets. Experimental results show that the proposed algorithms outperform than the existing algorithms for mining high utility itemsets from data streams.","Data mining,
Itemsets,
Transaction databases,
Computer science,
Electronic mail,
Application software,
Tree data structures,
Association rules,
Costs,
Filtering"
Application-specific topology-aware mapping for three dimensional topologies,"The fastest supercomputers today such as Blue Gene/L and XT3 are connected by a 3-dimensional torus/mesh interconnect. Applications running on these machines can benefit from topology-awareness while mapping tasks to processors at runtime. By co-locating communicating tasks on nearby processors, the distance traveled by messages and hence the communication traffic can be minimized, thereby reducing communication latency and contention on the network. This paper describes preliminary work utilizing this technique and performance improvements resulting from it in the context of a n-dimensional k-point stencil program. It shows that for a fine-grained application with a high communication to computation ratio, topology-aware mapping has a significant impact on performance. Automated topology-aware mapping by the runtime using similar ideas can relieve the application writer from this burden and result in better performance. Preliminary work towards achieving this for a molecular dynamics application, NAMD, is also presented. Results on up to 32,768 processors of IBM’s Blue Gene/L and 2,048 processors of Cray’s XT3 support the ideas discussed in the paper.","Delay,
Runtime,
Network topology,
Supercomputers,
Context,
Load management,
Computer science,
Application software,
Telecommunication traffic,
High performance computing"
Constructing a Message-Pruning Tree with Minimum Cost for Tracking Moving Objects in Wireless Sensor Networks Is NP-Complete and an Enhanced Data Aggregation Structure,"Wireless sensor networks have often been used to monitor and report the locations of moving objects. Since sensors can also be used for storage, a wireless sensor network can be considered a distributed database, enabling us to update and query the location information of moving objects. Many researchers have studied the problem of how to construct message-pruning trees that can update a database and query objects with minimum cost (the Minimum-Cost Message-Pruning Tree problem). The trees are constructed in such a way that the total cost of updating the database and querying objects is kept as minimal as possible, while the hardness of the Minimum-Cost Message-Pruning Tree problem remains unknown. In this paper, we first show that the Minimum-Cost Message-Pruning Tree problem is NP-complete. Subsequently, since the message-pruning tree with minimum cost is hard to construct in polynomial time, we propose a new data aggregation structure, a message-pruning tree with shortcuts, instead of the message- pruning tree. Simulation results show that the proposed data aggregation structure significantly reduces the total cost of updating the database and querying objects as compared to the message-pruning tree.","wireless sensor networks,
distributed databases,
optimisation,
polynomials,
query processing,
tracking"
A Novel Time-Based Readout Scheme for a Combined PET-CT Detector Using APDs,"This paper summarizes CERN R&D work done in the framework of the European Commission's FP6 BioCare Project. The objective was to develop a novel ""time-based"" signal processing technique to read out LSO-APD photodetectors for medical imaging. An important aspect was to employ the technique in a combined scenario for both computer tomography (CT) and positron emission tomography (PET) with effectively no tradeoffs in efficiency and resolution compared to traditional single mode machines. This made the use of low noise and yet very high-speed monolithic front-end electronics essential so as to assure the required timing characteristics together with a high signal-to-noise ratio. Using APDs for photon detection, two chips, traditionally employed for particle physics, could be identified to meet the above criteria. Although both were not optimized for their intended new medical application, excellent performance in conjunction with LSO-APD sensors could be derived. Whereas a measured energy resolution of 16% (FWHM) at the 511 keV photo peak competes favorably with that of 'classical' PMTs, the coincidence time resolution of 1.6 ns FWHM with dual APD readout is typically lower. This is attributed to the stochastic photon production mechanism in LSO and the photon conversion characteristic of the photo diode, as well as to the fluctuations in photon conversion, albeit the APD's superior quantum efficiency. Also in terms of CT counting speed, the chosen readout principle is limited by the intrinsic light decay in LSO (40 ns) for each impinging X-ray.","Detectors,
Computed tomography,
Positron emission tomography,
Signal to noise ratio,
Optoelectronic and photonic sensors,
Energy resolution,
Biomedical signal processing,
Photodetectors,
Biomedical imaging,
Signal resolution"
An Artificial Emergency-Logistics-Planning System for Severe Disasters,"To help government and disaster relief organizations prepare for and manage severe disasters, the authors propose the artificial emergency-logistics-planning system (Aelps). Aelps, which is based on artificial-society theory, concurrently considers different subsystems dealing with weather, geology, epidemics, and so on. Aelps can form the basis of a complex computational platform that generates logistics phenomena during disaster relief and gives intuitive results that can be used in emergency-logistics planning.","Weather forecasting,
Geology,
Transportation,
Logistics,
Earthquakes,
Pollution control,
Disaster management,
Capacity planning,
Roads,
Bridges"
Speeded-up Bag-of-Words algorithm for robot localisation through scene recognition,"This paper describes a new scalable scheme for the real-time detection of identical scenes for mobile robot localisation, allowing fast retraining to learn new environments. It uses the Image Bag-of-Words algorithm, where images are described by a set of local feature descriptors mapped to a discrete set of ‘image words’. This scheme uses descriptors consisting of a combination of a descriptor of shape (SURF) and a hue histogram, and this combination is shown to perform better than either descriptor alone. K-medoids clustering is shown to be suitable for quantising these composite descriptors (or any arbitrary descriptor) into visual words. The scheme can identify in real-time (0.036 seconds per query) multiple images of the same object from a standard dataset of 10200 images, showing robustness to differences in perspective and changes in the scene, and can detect loops in a video stream from a mobile robot.","Layout,
Object recognition,
Mobile robots,
Navigation,
Simultaneous localization and mapping,
Object detection,
Image recognition,
Testing,
Robot sensing systems,
Robot vision systems"
An Embedded Systems Curriculum Based on the IEEE/ACM Model Curriculum,"The Department of Electrical and Computer Engineering at The University of Alabama, Tuscaloosa, has recently completed a major restructuring of its computer engineering curriculum. The main goal of this reform is to integrate a broad set of embedded systems concepts into the course sequence thereby creating an embedded systems focus throughout the curriculum. Breadth of embedded systems concepts is addressed by using the embedded systems component of the 2004 IEEE/ACM computer engineering model curriculum as the basis for the new curriculum content. Depth is attained by overlapping coverage of most topics using multiple courses and integrating forward and reverse references to these concepts among the courses. This paper presents the rationale behind the curriculum reform, the revised curriculum, lessons learned, and the results of a comprehensive assessment of its effectiveness.","Embedded system,
Computational modeling,
Computers,
Programming,
Education,
Software,
Laboratories"
A Content-Centric Development Process Model,"Working from the belief that when content is king, content experts should lead, a storyboard-driven approach provides a sound methodology for developing educational games that helps ensure that no good storyboard becomes a bad game. The storyboard-driven approach provides a sound methodology for developing games that have as their keystone the final product's content. Adventure games focus on content. When we add the educational goal into the mix, this becomes even more relevant. From this notion, instead of adapting the content to fit the technology, we adapt the technology to fit the content. Even though the resulting process might seem burdensome compared to more streamlined processes, this approach has a specific focus that ensures that no good storyboard becomes a bad game.","Games,
Programming profession,
Investments,
Animals,
Fluid flow measurement,
Rhythm,
Lightning,
Writing,
Computer science,
Engines"
A dual-layer estimator architecture for long-term localization,"In this paper, we present a localization algorithm for estimating the 3D position and orientation (pose) of a moving vehicle based on visual and inertial measurements. The main advantage of the proposed method is that it provides precise pose estimates at low computational cost. This is achieved by introducing a two-layer estimation architecture that processes measurements based on their information content. Inertial measurements and feature tracks between consecutive images are processed locally in the first layer (Multi-State-Constraint Kalman filter) providing estimates for the motion of the vehicle at a high rate. The second layer comprises a bundle adjustment iterative estimator that operates intermittently so as to (i) reduce the effect of the linearization errors, and (ii) update the state estimates every time an area is re-visited and features are re-detected (loop closure). Through this process reliable state estimates are available continuously, while the estimation errors remain bounded during long-term operation. The performance of the developed system is demonstrated in large-scale experiments, involving a vehicle localizing within an urban area.","Vehicles,
State estimation,
Computer architecture,
Position measurement,
Computational efficiency,
Motion measurement,
Tracking,
Motion estimation,
Error correction,
Estimation error"
Markov Random Field-Based Statistical Character Structure Modeling for Handwritten Chinese Character Recognition,"This paper proposes a statistical-structural character modeling method based on Markov random fields (MRFs) for handwritten Chinese character recognition (HCCR). The stroke relationships of a Chinese character reflect its structure, which can be statistically represented by the neighborhood system and clique potentials within the MRF framework. Based on the prior knowledge of character structures, we design the neighborhood system that accounts for the most important stroke relationships. We penalize the structurally mismatched stroke relationships with MRFs using the prior clique potentials and derive the likelihood clique potentials from Gaussian mixture models, which encode the large variations of stroke relationships statistically. In the proposed HCCR system, we use the single-site likelihood clique potentials to extract many candidate strokes from character images and use the pair-site clique potentials to determine the best structural match between the input candidate strokes and the MRF-based character models by relaxation labeling. The experiments on the Korea Advanced Institute of Science and Technology (KAIST) character database demonstrate that MRFs can statistically model character structures, and work well in the HCCR system.","Character recognition,
Impedance matching,
Statistical analysis,
Markov random fields,
Active shape model,
Image segmentation,
Labeling,
Image databases,
Humans"
Affective MTV analysis based on arousal and valence features,"Nowadays, MTV has become an important favorite pastime to modern people because of its conciseness, convenience to play and the characteristic that can bring both audio and visual experiences to audiences. In this paper, we propose an affective MTV analysis framework, which realizes MTV affective state extraction, representation and clustering. Firstly, affective features are extracted from both audio and visual signals. Then, the affective state of each MTV is modeled with 2D dimensional affective model and visualized in the Arousal-Valence space. Finally the MTVs having similar affective states are clustered into same categories. The validity of proposed framework is proved by subjective user study. The comparisons between our selected features and those in related work prove that our features improve the performance by a significant margin.","Feature extraction,
Music,
Computational modeling,
Mood,
Visualization,
Gallium,
Motion pictures"
Input vector control for post-silicon leakage current minimization in the presence of manufacturing variability,"We present the first approach for post-silicon leakage power reduction through input vector control (IVC) that takes into account the impact of the manufacturing variability (MV). Because of the MV, the integrated circuits (ICs) implementing one design require different input vectors to achieve their lowest leakage states. We address two major challenges. The first is the extraction of the gate- level characteristics of an IC by measuring only the overall leakage power for different inputs. The second problem is the rapid generation of input vectors that result in a low leakage for a large number of unique ICs that implement a given design, but are different in the post-manufacturing phase. Experimental results on a large set of benchmark instances demonstrate the efficiency of the proposed methods. For example, the leakage power consumption could be reduced in average by more than 10.4%, when compared to the previously published IVC techniques that did not consider MV.","Leakage current,
Inverters,
Computer aided manufacturing,
Algorithm design and analysis,
Integrated circuit synthesis,
Minimization,
Computer science,
Power measurement,
Permission,
Power engineering computing"
A Security Punctuation Framework for Enforcing Access Control on Streaming Data,"The management of privacy and security in the context of data stream management systems (DSMS) remains largely an unaddressed problem to date. Unlike in traditional DBMSs where access control policies are persistently stored on the server and tend to remain stable, in streaming applications the contexts and with them the access control policies on the real-time data may rapidly change. A person entering a casino may want to immediately block others from knowing his current whereabouts. We thus propose a novel ""stream-centric"" approach, where security restrictions are not persistently stored on the DSMS server, but rather streamed together with the data. Here, the access control policies are expressed via security constraints (called security punctuations, or short, sps) and are embedded into data streams. The advantages of the sp model include flexibility, dynamicity and speed of enforcement. DSMSs can adapt to not only data-related but also security-related selectivities, which helps reduce the waste of resources, when few subjects have access to data. We propose a security-aware query algebra and new equivalence rules together with cost estimations to guide the security-aware query plan optimization. We have implemented the sp framework in a real DSMS. Our experimental results show the validity and the performance advantages of our sp model as compared to alternative access control enforcement solutions for DSMSs.","Data security,
Access control,
Protection,
Computer security,
Computer science,
Data privacy,
Consumer electronics,
Hospitals,
Algebra,
Cost function"
Network coding vs. erasure coding: Reliable multicast in ad hoc networks,"Providing reliable and efficient networking services in wireless ad hoc networks is extremely challenging due to high mobility and unstable wireless nature: a significant number of packets can be corrupt or lost. To increase the reliability in packet erasure networks, various coding schemes have been proposed. Network coding (NC) and erasure coding (EC) are such well-known coding techniques recently considered to be used for multicast communications. Both schemes are able to encode original packets into a potentially infinite data stream of encoded packets. Receivers can reconstruct the original packets once they have collected a certain number of encoded packets. The main difference of these schemes is that NC allows intermediate nodes to encode packets they have received so far whereas EC is an end-to-end coding which allows only sources to encode. Both schemes are considered to be able to provide excellent ammunition against erasure networks. However, ‘the jury is still out’ regarding which scheme is suitable in ad hoc networks. In this paper, based on simulations and analysis study, we present information on the performance of both schemes which may be useful for selecting the better coding scheme.","Network coding,
Ad hoc networks,
Telecommunication network reliability,
Multicast communication,
Automatic repeat request,
Computer network reliability,
Mobile ad hoc networks,
Intelligent networks,
National electric code,
Computer science"
AS-MAC: An asynchronous scheduled MAC protocol for wireless sensor networks,"Energy efficiency of the MAC protocol is a key design factor for wireless sensor networks (WSNs). Due to the importance of the problem, a number of energy efficient MAC protocols have been developed for WSNs. Preamble-sampling based MAC protocols (e.g., B-MAC and X-MAC) have overheads due to their preambles, and are inefficient at large wakeup intervals. SCP-MAC, a very energy efficient scheduling MAC protocol, minimizes the preamble by combining preamble sampling and scheduling techniques; however, it does not prevent energy loss due to overhearing; in addition, due to its synchronization procedure, it results in increased contention and delay. In this paper, we present an energy efficient MAC protocol for WSNs that avoids overhearing and reduces contention and delay by asynchronously scheduling the wakeup time of neighboring nodes. To validate our design and analysis, we implement the proposed scheme on the MicaZ platform. Experimental results show that AS-MAC considerably reduces energy consumption, packet loss and delay when compared with SCP-MAC.","Media Access Protocol,
Receivers,
Delay,
Wireless sensor networks,
Sleep,
Energy consumption,
Synchronization"
Efficient fault diagnosis using incremental alarm correlation and active investigation for internet and overlay networks,"Fault localization is the core element in fault management. Symptom-fault map is commonly used to describe the symptom-fault causality in fault reasoning. For Internet service networks, a well-designed monitoring system can effectively correlate the observable symptoms (i.e., alarms) with the critical network faults (e.g., link failure). However, the lost and spurious symptoms can significantly degrade the performance and accuracy of a passive fault localization system. For overlay networks, due to limited underlying network accessibility, as well as the overlay scalability and dynamics, it is impractical to build a static overlay symptom-fault map. In this paper, we firstly propose a novel active integrated fault reasoning (AIR) framework to incrementally incorporate active investigation actions into the passive fault reasoning process based on an extended symptom-fault-action (SFA) model. Secondly, we propose an overlay network profile (ONP) to facilitate the dynamic creation of an overlay symptom-fault-action (called O-SFA) model, such that the AIR framework can be applied seamlessly to overlay networks (called O-AIR). As a result, the corresponding fault reasoning and action selection algorithms are elaborated. Extensive simulations and Internet experiments show that AIR and O-AIR can significantly improve both accuracy and performance in the fault reasoning for Internet and overlay service networks, especially when the ratio of the lost and spurious symptoms is high.","Fault diagnosis,
IP networks,
Web and internet services,
Condition monitoring,
Scalability,
Computer science,
Computerized monitoring,
Degradation,
Information technology,
Management information systems"
High resolution motion layer decomposition using dual-space graph cuts,"We introduce a novel energy minimization method to decompose a video into a set of super-resolved moving layers. The proposed energy corresponds to the cost of coding the sequence. It consists of a data term and two terms imposing regularity of the geometry and the intensity of each layer. In contrast to existing motion layer methods, we perform graph cut optimization in the (dual) layer space to determine which layer is visible at which video position. In particular, we show how arising higher-order terms can be accounted for by a generalization of alpha expansions. Moreover, our model accurately captures long-term temporal consistency. To the best of our knowledge, this is the first work which aims at modeling details of the image formation process (such as camera blur and downsampling) in the context of motion layer decomposition. The experimental results demonstrate that energy minimization leads to a reconstruction of a video in terms of a superposition of multiple high-resolution motion layers.","Energy resolution,
Spatial resolution,
Costs,
Constraint optimization,
Computer vision,
Geometry,
Image reconstruction,
Motion estimation,
Minimization methods,
Video compression"
Fast and Highly-Available Stream Processing over Wide Area Networks,"We present a replication-based approach that realizes both fast and highly-available stream processing over wide area networks. In our approach, multiple operator replicas send outputs to each downstream replica so that it can use whichever data arrives first. To further expedite the data flow, replicas run independently, possibly processing data in different orders. Despite this complication, our approach always delivers what non-replicated processing would produce without failures. We call this guarantee replication transparency. In this paper, we first discuss semantic issues for replication transparency and extend stream-processing primitives accordingly. Next, we develop an algorithm that manages replicas at geographically dispersed servers. This algorithm strives to achieve the best latency guarantee, relative to the cost of replication. Finally, we substantiate the utility of our work through experiments on PlanetLab servers as well as simulations based on real network traces.",
Understanding the Customer: What Do We Know about Requirements Elicitation?,"Getting the requirements right is one of the most important activities in software development. Making a crucial misstep at this phase can easily lead to large amounts of rework when the customer simply can't accept a system the way it was developed. When used correctly, approaches such as incremental development or agile methods can mitigate the risks of getting the requirements wrong by making sure that systems are developed in smaller chunks and that each chunk can be shown to the customer for approval. However, the best way to develop a high-quality system with minimal effort is still to get the requirements right the first time.","Prototypes,
Software prototyping,
Programming,
Materials science and technology,
Application software,
Engineering profession,
Feedback,
Concrete,
Protocols,
Sorting"
Imaging concert hall acoustics using visual and audio cameras,"Using a recently developed real time audio camera, that uses the output of a spherical microphone array beamformer steered in all directions to create central projection to create acoustic intensity images, we present a technique to measure the acoustics of rooms and halls. A panoramic mosaiced visual image of the space is also create. Since both the visual and the audio camera images are central projection, registration of the acquired audio and video images can be performed using standard computer vision techniques. We describe the technique, and apply it to the examine the relation between acoustical features and architectural details of the Dekelbaum concert hall at the Clarice Smith Performing Arts Center in College Park, MD.","Acoustic imaging,
Cameras,
Microphone arrays,
Acoustic scattering,
Raman scattering,
Acoustic measurements,
Humans,
Educational institutions,
Layout,
Reverberation"
Beyond Traditional Kernels: Classification in Two Dissimilarity-Based Representation Spaces,"Proximity captures the degree of similarity between examples and is thereby fundamental in learning. Learning from pairwise proximity data usually relies on either kernel methods for specifically designed kernels or the nearest neighbor (NN) rule. Kernel methods are powerful, but often cannot handle arbitrary proximities without necessary corrections. The NN rule can work well in such cases, but suffers from local decisions. The aim of this paper is to provide an indispensable explanation and insights about two simple yet powerful alternatives when neither conventional kernel methods nor the NN rule can perform best. These strategies use two proximity-based representation spaces (RSs) in which accurate classifiers are trained on all training objects and demand comparisons to a small set of prototypes. They can handle all meaningful dissimilarity measures, including non-Euclidean and nonmetric ones. Practical examples illustrate that these RSs can be highly advantageous in supervised learning. Simple classifiers built there tend to outperform the NN rule. Moreover, computational complexity may be controlled. Consequently, these approaches offer an appealing alternative to learn from proximity data for which kernel methods cannot directly be applied, are too costly or impractical, while the NN rule leads to noisy results.","Kernel,
Neural networks,
Prototypes,
Supervised learning,
Computer science,
Design methodology,
Nearest neighbor searches,
Computational complexity,
Statistical learning,
Humans"
Feedback FET: A novel transistor exhibiting steep switching behavior at low bias voltages,A novel transistor design which utilizes positive feedback to achieve steep switching behavior is proposed and demonstrated. The feedback (FB) FET exhibits very low subthreshold swing (~2 mV/dec) and high ION/IOFF ratio (~108) to allow for significant reductions in gate voltage swing (to below 0.5V). It is a new candidate to replace the MOSFET for future low-power electronic devices.,"Feedback,
FETs,
Low voltage,
MOSFET circuits,
Charge carrier processes,
Threshold voltage,
Power MOSFET,
Temperature,
Tunneling,
Diodes"
Scientific Exploration in the Era of Ocean Observatories,"The authors introduce an ocean observatory, offer a vision of observatory-enabled scientific exploration, and discuss the requirements and approaches for generating provenance-aware products in such environments.","Observatories,
Sea measurements,
Rivers,
Temperature sensors,
Ecosystems,
Marine technology,
Tides,
Temperature measurement,
Ocean temperature,
Telemetry"
Using Dependency Tracking to Provide Explanations for Policy Management,"Explanations for decisions made by a policy framework allow end users to understand how the results were obtained, increase trust in the policy decision and enforcement process, and enable policy administrators to ensure the correctness of the policy. In our framework, an explanation for any statement including a policy decision is a representation of the list of reasons (known as dependencies) associated with its derivation. Dependency tracking involves maintaining the list of reasons (statements and rules) for the derivation of a new statement. In this paper, we describe our policy approach that (i) provides explanations for policy decisions, (ii) provides more efficient and expressive reasoning through the use of nested sub-rules and goal direction, and (iii) is grounded in Semantic Web technologies. We discuss the characteristics of our approach and provide a brief overview of the AIR policy language that implements it. We also discuss how relevant explanation information is identified and presented to end users and describe our preliminary graphical user interface.","Pattern matching,
Artificial intelligence,
Semantic Web,
Conferences,
Computer network management,
Computer science,
Graphical user interfaces,
Production systems,
Data mining,
Tree graphs"
Simulation of piezoelectric excitation of guided waves using waveguide finite elements,"A numerical method for computing the time response of infinite constant cross-section elastic waveguides excited by piezoelectric transducers was developed. The method combined waveguide finite elements (semi-analytical finite elements) for modeling the waveguide with conventional 3-D piezoelectric finite elements for modeling the transducer. The frequency response of the coupled system was computed and then used to simulate the time response to tone-burst electrical excitation. A technique for identifying and separating the propagating modes was devised, which enabled the computation of the response of a selected reduced number of modes. The method was applied to a rail excited by a piezoelectric patch transducer, and excellent agreement with measured responses was obtained. It was found that it is necessary to include damping in the waveguide model if the response near a ldquocut-onrdquo frequency is to be simulated in the near-field.",
Construct Small Worlds in Wireless Networks Using Data Mules,"A small world phenomenon has been discovered in a wide range of disciplines, such as physics, biology, social science, information system, computer networks, etc. In a wireless network, the small world phenomenon is used in the development of novel routing strategies. However, past studies made use of wire lines as shortcuts to construct a small world in a wireless network. First, it’s difficult to determine the length of these wired shortcuts in advance. Second, wired lines are unsuitable in certain circumstances such as rural area, battlefield, etc. They are more costly in deployment and more vulnerable to unexpected damage. Finally, in wireless networks that lack central infrastructures such as mobile ad hoc networks (MANET) and wireless vehicular networks, fixed-length wired line shortcuts cannot be employed. This study proposes a new method to construct a small world in a wireless network.  Instead of deploying wired lines as shortcuts. Variable length shortcuts are constructed by using mobile router nodes called data mules. Data mules move data between nodes which don’t have direct wireless communication link.  These data mules imitate shortcuts in a small world. The small world phenomenon in connected and disconnected wireless networks containing various numbers of data mules is then discussed. Finally the small world phenomenon is considered in wireless sensor networks.","Wireless networks,
Mobile ad hoc networks,
Physics,
Biology,
Information systems,
Computer networks,
Routing,
Wire,
Wireless communication,
Wireless sensor networks"
"Man-In-The-Middle attacks on bluetooth: a comparative analysis, a novel attack, and countermeasures","We provide a comparative analysis of the existing MITM (Man-In-The-Middle) attacks on Bluetooth. In addition, we propose a novel Bluetooth MITM attack against Bluetooth-enabled printers that support SSP (Secure Simple Pairing). Our attack is based on the fact that the security of the protocol is likely to be limited by the capabilities of the least powerful or the least secure device type. Moreover, we propose improvements to the existing Bluetooth SSP in order to make it more secure.",
Toward Fully Automatic Geo-Location and Geo-Orientation of Static Outdoor Cameras,"Automating tools for geo-locating and geo-orienting static cameras is a key step in creating a useful global imaging network from cameras attached to the Internet. We present algorithms for partial camera calibration that rely on access to accurately time-stamped images captured over time from cameras that do not move. To support these algorithms we also offer a method of camera viewpoint change detection, or ""tamper detection"", which determines if a camera has moved in the challenging case when images are only captured every half hour. These algorithms are tested on a subset of the AMOS (Archive of Many Outdoor Scenes) database, and we present preliminary results that highlight the promise of these approaches.","Cameras,
Calibration,
Layout,
IP networks,
Change detection algorithms,
Sun,
Jacobian matrices,
Principal component analysis,
Computer science,
Testing"
New Monotones and Lower Bounds in Unconditional Two-Party Computation,"Since oblivious transfer, a primitive of paramount importance in secure two- and multiparty computation, cannot be realized in an unconditionally secure way for both parties from scratch, reductions to weak information-theoretic primitives as well as between different variants of the functionality are of great interest. In this context, various monotones-quantities that cannot be increased by any protocol-are introduced and then used to derive lower bounds on the possibility and efficiency of such reductions.","Computer science,
Secure storage,
Information security,
Public key cryptography,
Information theory,
Computer security,
Public key"
Multi-unit Auctions with Budget Limits,"We study multi-unit auctions where the bidders have a budget constraint, a situation very common in practice that has received very little attention in the auction theory literature. Our main result is an impossibility: there are no incentive-compatible auctions that always produce a Pareto-optimal allocation. We also obtain some surprising positive results for certain special cases.","Cost accounting,
Computer science,
Constraint theory,
Upper bound,
Pollution measurement,
Concrete"
Computing iconic summaries of general visual concepts,"This paper considers the problem of selecting iconic images to summarize general visual categories. We define iconic images as high-quality representatives of a large group of images consistent both in appearance and semantics. To find such groups, we perform joint clustering in the space of global image descriptors and latent topic vectors of tags associated with the images. To select the representative iconic images for the joint clusters, we use a quality ranking learned from a large collection of labeled images. For the purposes of visualization, iconic images are grouped by semantic “theme” and multidimensional scaling is used to compute a 2D layout that reflects the relationships between the themes. Results on four large-scale datasets demonstrate the ability of our approach to discover plausible themes and recurring visual motifs for challenging abstract concepts such as “love” and “beauty.”","Visualization,
Image retrieval,
Computer science,
Multidimensional systems,
Large-scale systems,
Cultural differences,
Heart,
Performance analysis,
Positron emission tomography,
Image quality"
Applications of super capacitors for PMSG wind turbine power smoothing,"The focus of this paper is wind power smoothing using super capacitors. Due to increasing penetration of the wind power in power system, the quality of the wind power has become a major concern to keep the stability of the power grid. One of the major problems of wind turbines is power fluctuations because of the wind speed oscillations. This paper studies the pros and cons of the integration of the super capacitors in the Permanent Magnet Synchronous Generator (PMSG) wind turbine system. The dynamic of the wind turbine is developed to design a control algorithm for the wind turbine system. The system is simulated using PSIM software and the results are presented.",
An active connection mechanism for modular self-reconfigurable robotic systems based on physical latching,"This article presents a robust and heavy duty physical latching connection mechanism, which can be actuated with DC motors to actively connect and disconnect modular robot units. The special requirements include a lightweight and simple construction providing an active, strong, hermaphrodite, completely retractable connection mechanism with a 90 degree symmetry1 and a no-energy consumption in the locked state. The mechanism volume is kept small to fit multiple copies into a single modular robot unit and to be used on as many faces of the robot unit as possible. This way several different lattice like modular robot structures are possible. The large selection for dock-able connection positions will likely simplify self-reconfiguration strategies. Tests with the implemented mechanism demonstrate its applicative potential for self-reconfiguring modular robots.","Robot kinematics,
Robustness,
Robot sensing systems,
DC motors,
Humanoid robots,
Robotics and automation,
Robotic assembly,
Prototypes,
Orbital robotics,
Connectors"
OutlinAR: an assisted interactive model building system with reduced computational effort,"This paper presents a system that allows online building of 3D wire-frame models through a combination of user interaction and automated methods from a handheld camera-mouse. Crucially, the model being built is used to concurrently compute camera pose, permitting extendable tracking while enabling the user to edit the model interactively. In contrast to other model building methods that are either off-line and/or automated but computationally intensive, the aim here is to have a system that has low computational requirements and that enables the user to define what is relevant (and what is not) at the time the model is being built. OutlinAR hardware is also developed which simply consists of the combination of a camera with a wide field of view lens and a wheeled computer mouse.","Three dimensional displays,
Computational modeling,
Cameras,
Solid modeling,
Visualization,
Buildings,
Augmented reality"
Object image retrieval by exploiting online knowledge resources,"We describe a method to retrieve images found on web pages with specified object class labels, using an analysis of text around the image and of image appearance. Our method determines whether an object is both described in text and appears in a image using a discriminative image model and a generative text model. Our models are learnt by exploiting established online knowledge resources (Wikipedia pages for text; Flickr and Caltech data sets for image). These resources provide rich text and object appearance information. We describe results on two data sets. The first is Berg’s collection of ten animal categories; on this data set, we outperform previous approaches [7, 33]. We have also collected five more categories. Experimental results show the effectiveness of our approach on this new data set.","Image retrieval,
Web pages,
Wikipedia,
Animals,
Search engines,
Computer science,
Image analysis,
Labeling,
Displays,
Bicycles"
A discrete wavelet transform based technique for image data hiding,"In this paper, a new image data hiding technique based on Discrete Wavelet Transform (DWT) is proposed. The new technique will be used for hiding a secret image S inside a cover image C using two secret keys to obtain a stego-image G. It shows high robustness against many of image processing operations such as lossy compression, blurring, cropping, median filter, sharpen, and addition of noise. The embedded secret image can be extracted with high visual quality. The stego-image is perceptually similar to the original cover image. The proposed technique does not require the original cover image to extract the embedded secret image. The comparative analysis between the proposed technique and the other existing techniques has shown the superiority of the proposed technique.",
"Real-time cardiac MRI without triggering, gating, or breath holding","State-of-the-art cardiac MRI can perform real-time 2D scans without cardiac triggering during a single breath hold; however, real-time cardiac MRI in rats is difficult due to the high heart rate (330 bpm) and presence of respiratory motion. These challenges are overcome by using a dynamic imaging method based on Partially Separable Function (PSF) theory with an acceleration factor of 256. This paper demonstrates that this method can be used in the study of transplanted rat hearts for both anatomical and perfusion applications. The study was carried out with a 200 μm in-plane resolution with a 17.2 msec temporal resolution, and the results show improved spatial resolution (2x) and reduced acquisition time (3x) relative to Electrocardiogram (ECG) triggered, respiratory gated cine imaging.","Animals,
Contrast Media,
Electrocardiography,
Gadolinium,
Heart,
Heart Rate,
Heart Transplantation,
Image Enhancement,
Image Processing, Computer-Assisted,
Magnetic Resonance Imaging,
Models, Statistical,
Perfusion,
Rats,
Reference Values,
Reproducibility of Results"
Querying Data under Access Limitations,"Data sources on the web are often accessible through web interfaces that present them as relational tables, but require certain attributes to be mandatorily selected, e.g., via a web form. In a scenario where we integrate a set of such sources, and we pose queries over them, the values needed to access a source may have to be retrieved from other sources that are possibly not even mentioned in the query: answering queries at best can then be done only with a potentially recursive query plan that gets all obtainable answers to the query. Since data sources are typically distributed over a network, a major cost indicator for the execution of a query plan is the number of accesses to remote sources. In this paper we present an optimization technique for conjunctive queries that produces a query plan that: (1) minimizes the number of accesses according to a strong notion of minimality; (2) excludes all sources that are not relevant for the query. We introduce Toorjah, a prototype system that answers queries posed on sources with limitations by means of optimized query plans. Toorjah adopts a strategy that is aimed to retrieve answers as early as possible during query processing, and to present them to the user as they are computed. We provide experimental evidence of the effectiveness of our optimization, by showing the reduction of the number of accesses in a large number of cases.","Query processing,
Computer interfaces,
Laboratories,
Finance,
Computer science,
Costs,
Prototypes,
Data mining,
Information retrieval,
Scattering"
Front-view Gait Recognition,"We present a new method for front-view gait biometrics which uses a single non-calibrated camera and extracts unique signatures from descriptors of a silhouette's deformation. The proposed approach is particularly suitable for identification by gait in the real world, where the advantages of completely unobtrusiveness, remoteness and covertness of the biometric system preclude the availability of camera information and where the CCTV images usually present subjects from an upper front-view. Tests on three different gait databases with subjects walking towards the camera have been performed. The obtained results, with mean CCR of 96.3%, show that gait recognition of individuals observed the front can be achieved without any knowledge of camera parameters. Moreover, the method has been applied to three different walking directions and the results have been compared with the algorithms found in literature. The performance of the proposed system is particularly encouraging for its appliance in surveillance scenarios.","Cameras,
Biometrics,
Legged locomotion,
Data mining,
Availability,
Testing,
Image databases,
Performance evaluation,
Home appliances,
Surveillance"
A Distributed Cooperative Target Tracking with Binary Sensor Networks,"Target tracking is a typical and important cooperative sensing application of wireless sensor networks. We study it in its most basic form, assuming the binary sensing model in which each sensor can return only 1-bit information regarding target's presence or absence within its sensing range. A novel, real-time and distributed target tracking algorithm is proposed. The algorithm reduces the uncertainty of the target location from a two-dimensional area into a one-dimensional arc and estimates the target velocity and trajectory in a distributed and asynchronous manner. Extensive simulations show that our algorithm achieves good performance by yielding highly accurate estimates of the target's location, velocity and trajectory.","Target tracking,
Trajectory,
Wireless sensor networks,
Particle filters,
Delay estimation,
Communications Society,
Conferences,
Computer science,
Pervasive computing,
USA Councils"
Dense 3D motion capture from synchronized video streams,"This paper proposes a novel approach to non-rigid, markerless motion capture from synchronized video streams acquired by calibrated cameras. The instantaneous geometry of the observed scene is represented by a polyhedral mesh with fixed topology. The initial mesh is constructed in the first frame using the publicly available PMVS software for multi-view stereo [7]. Its deformation is captured by tracking its vertices over time, using two optimization processes at each frame: a local one using a rigid motion model in the neighborhood of each vertex, and a global one using a regularized nonrigid model for the whole mesh. Qualitative and quantitative experiments using seven real datasets show that our algorithm effectively handles complex nonrigid motions and severe occlusions.","Streaming media,
Layout,
Motion estimation,
Cameras,
Tracking,
Shape,
Image motion analysis,
Deformable models,
Surface reconstruction,
Image reconstruction"
Rate adaptation algorithms for IEEE 802.11 networks: A survey and comparison,"Rate adaptation is the determination of the optimal data transmission rate most appropriate for current wireless channel conditions. It consists of assessing channel conditions and accordingly adjusting the rate. Rate adaptation is fairly challenging due to wild channel conditions fluctuations. In the last decade, rate adaptation for IEEE 802.11 networks has been extensively investigated. This paper presents a comprehensive and detailed study of the advances of rate adaptation schemes proposed for IEEE 802.11 networks, and summarizes their characteristics. We also categorize these rate adaptation schemes based on their support of loss differentiation and their methods to sense the channel conditions. Then, this paper compares the performance of three representative schemes through simulations. Finally, open issues for rate adaptation are raised.",
k-Anonymization Revisited,"In this paper we introduce new notions of k-type anonymizations. Those notions achieve similar privacy goals as those aimed by Sweenie and Samarati when proposing the concept of k-anonymization: an adversary who knows the public data of an individual cannot link that individual to less than k records in the anonymized table. Every anonymized table that satisfies k-anonymity complies also with the anonymity constraints dictated by the new notions, but the converse is not necessarily true. Thus, those new notions allow generalized tables that may offer higher utility than k-anonymized tables, while still preserving the required privacy constraints. We discuss and compare the new anonymization concepts, which we call (1, k )-, (k, k)- and global (1, k)-anonymizations, according to several utility measures. We propose a collection of agglomerative algorithms for the problem of finding such anonymizations with high utility, and demonstrate the usefulness of our definitions and our algorithms through extensive experimental evaluation on real and synthetic datasets.","Data mining,
Databases,
Data privacy,
Protection,
Data security,
Information security,
Computer science,
Hospitals,
Cost function"
XStream: a Signal-Oriented Data Stream Management System,"Sensors capable of sensing phenomena at high data rates on the order of tens to hundreds of thousands of samples per second are now widely deployed in many industrial, civil engineering, scientific, networking, and medical applications. In aggregate, these sensors easily generate several million samples per second that must be processed within milliseconds or seconds. The computation required includes both signal processing and event stream processing. XStream is a stream processing system for such applications. XStream introduces a new data type, the signal segment, which allows applications to manipulate isochronous (regularly spaced in time) collections of sensor samples more conveniently and efficiently than the asynchronous representation used in previous work. XStream includes a memory manager and scheduler optimizations tuned for processing signal segments at high speeds. In benchmark comparisons, we show that XStream outperforms a leading commercial stream processing system by more than three orders of magnitude. On one application, the commercial system processed 72.7 Ksamples/sec, while XStream processed 97.6 Msamples/sec.","Signal processing,
Filters,
Aggregates,
Signal design,
Intelligent sensors,
Sensor phenomena and characterization,
Memory management,
Acoustic signal processing,
Performance gain,
Computer network management"
Hardware Implementation of a Bio-plausible Neuron Model for Evolution and Growth of Spiking Neural Networks on FPGA,"We propose a digital neuron model suitable for evolving and growing heterogeneous spiking neural networks on FPGAs by introducing a novel flexible dendrite architecture and the new PLAQIF (Piecewise-Linear Approximation of Quadratic Integrate and Fire) soma model. A network of 161 neurons and 1610 synapses was simulated, implemented, and verified on a Virtex-5 chip with 4210 times real-time speed with 1 ms resolution. The parametric flexibility of the soma model was shown through a set of experiments.","Biological system modeling,
Neurons,
Artificial neural networks,
Hardware,
Reservoirs,
Field programmable gate arrays,
Adders"
MediNet: Personalizing the self-care process for patients with diabetes and cardiovascular disease using mobile telephony,"This paper describes MediNet, a mobile healthcare system that is being developed to personalize the self-care process for patients with both diabetes and cardiovascular disease. These two diseases were chosen based on their interrelationship. Patients with diabetes are at least twice as likely to have heart disease or a stroke as compared to persons without diabetes. Furthermore, persons with diabetes also tend to develop heart disease or have strokes at an earlier age than other people. MediNet uses a reasoning engine to make recommendations to a patient based on current and previous readings from monitoring devices connected to the patient and on information that is known about the patient. It caters for the uniqueness of each patient by personalizing its recommendations based on individual level characteristics of the patient, as well as on characteristics that groups of patients tend to share.","Cardiovascular Diseases,
Cellular Phone,
Computer Communication Networks,
Diabetes Mellitus,
Diagnosis, Computer-Assisted,
Humans,
Self Care,
Software,
Telemedicine,
Therapy, Computer-Assisted,
User-Computer Interface"
A New Approach to Automated Epileptic Diagnosis Using EEG and Probabilistic Neural Network,"Epilepsy is one of the most common neurological disorders that greatly impair patients' daily lives. Traditional epileptic diagnosis relies on tedious visual screening by neurologists from lengthy EEG recording that requires the presence of seizure (ictal) activities. Nowadays, there are many systems helping the neurologists to quickly find interesting segments from the lengthy signal by automatic seizure detection. However, we notice that it is very difficult, if not impossible, to obtain long-term EEG data with seizure activities for epilepsy patients in areas lack of medical resources and trained neurologists. Therefore, we propose to study automated epileptic diagnosis using interictal EEG data that is much easier to collect than ictal data. The authors are not aware of any report on automated EEG diagnostic system that can accurately distinguish patients' interictal EEG from the EEG of normal people. The research presented in this paper, therefore, aims to develop an automated diagnostic system that can use interictal EEG data to diagnose whether the person is epileptic. Such a system should also detect seizure activities for further investigation by doctors and potential patient monitoring. To develop such a system, we extract three classes of features from the EEG data and build a Probabilistic Neural Network (PNN) fed with these features. Leave-one-out cross-validation (LOO-CV) on a widely used epileptic-normal data set reflects an impressive 99.3% accuracy of our system on distinguishing normal people's EEG from patients' interictal EEG. We also find our system can be used in patient monitoring (seizure detection) and seizure focus localization, with 96.7% and 76.5% accuracy respectively on the data set.","Epilepsy,
Electroencephalography,
Neural networks,
Artificial neural networks,
Medical diagnostic imaging,
Patient monitoring,
Medical services,
Artificial intelligence,
Computer science,
Computer networks"
Multiscale Simulation of Nanobiological Flows,A new multiscale approach for simulating nanobiological flows uses concurrent coupling of constrained molecular dynamics for long biomolecules with a mesoscopic lattice Boltzmann treatment of solvent hydrodynamics. The approach is based on a simple scheme of space- time information exchange between the atomistic and mesoscopic scales.,"Biological system modeling,
Biology computing,
Biological systems,
Kinetic theory,
Equations,
Nanobioscience,
Lattice Boltzmann methods,
Solvents,
Physics computing,
Evolution (biology)"
Simultaneous FU and Register Binding Based on Network Flow Method,"With the rapid increase of design complexity and the decrease of device features in nano-scale technologies, interconnection optimization in digital systems becomes more and more important. In this paper we develop a simultaneous FU and register (SFR) binding algorithm for multiplexer optimization based on min-cost network flow. Unlike most of the prior approaches in which functional unit binding and register binding are performed sequentially, our approach performs these two highly correlated tasks gradually and concurrently. We also present an ILP formulation of the combined functional unit and register binding problem for the optimality study of heuristics. Experimental results show that when compared to traditional binding algorithms, our simultaneous resource binding algorithm is close to optimal solutions for small-size designs (only 5% more MUX) and achieves significant reduction for MUX area (12%) and timing (10%) for a set of real-life benchmark designs.","Registers,
Multiplexing,
Integrated circuit interconnections,
Design optimization,
Algorithm design and analysis,
Partitioning algorithms,
Costs,
Power system interconnection,
Field programmable gate arrays,
Resource management"
Design and analysis of micro-solar power systems for Wireless Sensor Networks,"Wireless Sensor Networks are fundamentally limited by their energy storage resources and the power they obtain from their environment. Several micro-solar powered designs have been developed to address this important problem but little analysis is available on key design trade-offs. We develop a taxonomy of the micro-solar design space identifying key components, design choices, interactions, challenges, and trade-offs. Based on this taxonomy, we provide an empirical and mathematical analysis of two prominent designs of micro-solar power systems (Heliomote and Trio), and interpret the results to propose design guidelines for micro-solar power systems.",
Fast scale prototyping for folded millirobots,"We present a set of tools and a process, making use of inexpensive and environmentally friendly materials, that enable the rapid realization of fully functional large scale prototypes of folded mobile millirobots. By mimicking the smart composite microstructure (SCM) process at a 2–10X scale using posterboard, and commonly available polymer films, we can realize a prototype design in a matter of minutes compared with days for a complicated SCM design at the small scale. The time savings enable a significantly shorter design cycle by allowing for immediate discovery of design flaws and introduction of design improvements prior to beginning construction at the small scale. In addition, the technology eases the difficulty of visualizing and creating folded 3D structures from 2D parts. We use the example of a fully functional hexapedal crawling robot design to illustrate the process and to verify a scaling law which we propose.","Prototypes,
Robotics and automation,
Actuators,
Hip,
Robot sensing systems,
Silicon,
Micromechanical devices,
Laser beam cutting,
Couplings,
USA Councils"
EK-SVD: Optimized dictionary design for sparse representations,"Sparse representations using overcomplete dictionaries are used in a variety of field such as pattern recognition and compression. However, the size of dictionary is usually a tradeoff between approximation speed and accuracy. In this paper we propose a novel technique called the Enhanced K-SVD algorithm (EK-SVD), which finds a dictionary of optimized size-for a given dataset, without compromising its approximation accuracy. EK-SVD improves the K-SVD dictionary learning algorithm by introducing an optimized dictionary size discovery feature to K-SVD. Optimizing strict sparsity and MSE constraints, it starts with a large number of dictionary elements and gradually prunes the under-utilized or similar-looking elements to produce a well-trained dictionary that has no redundant elements. Experimental results show the optimized dictionaries learned using EK-SVD give the same accuracy as dictionaries learned using the K-SVD algorithm while substantially reducing the dictionary size by 60%.","Design optimization,
Dictionaries,
Matching pursuit algorithms,
Clustering algorithms,
Pursuit algorithms,
Pattern recognition,
Shape,
Partitioning algorithms,
Information science,
Design engineering"
Coordination of multiple AGVs in an industrial application,"In this paper we propose a methodology for coordinating a group of mobile robots following predefined paths in a dynamic industrial environment. Coordination diagrams are used for representing the possible collisions among the robots. Exploiting the structure of the industrial application we are dealing with, we propose an algorithm for efficiently composing the coordination diagram. Furthermore, we classify the possible collisions that can take place and the induced geometry of the resulting coordination diagram. Finally, we exploit this information for developing a planning algorithm that allows to coordinate the robots and to take into account unexpected events that can occur in an industrial environment.",
Answering conceptual queries with Ferret,"Programmers seek to answer questions as they investigate the functioning of a software system, such as ""which execution path is being taken in this case?"" Programmers attempt to answer these questions, which we call conceptual queries, using a variety of tools. Each type of tool typically highlights one kind of information about the system, such as static structural information or control-flow information. Unfortunately for the programmer, the tools seldom directly answer the programmer's conceptual queries. Instead, the programmer must piece together results from different tools to determine an answer to the initial query. At best, this process is time consuming and at worst, this process can lead to data overload and disorientation. In this paper, we present a model that supports the integration of different sources of information about a program. This model enables the results of concrete queries in separate tools to be brought together to directly answer many of a programmer's conceptual queries. In addition to presenting this model, we present a tool that implements the model, demonstrate the range of conceptual queries supported by this tool, and present the results of use of the conceptual queries in a small field study.","Programming profession,
Concrete,
Software systems,
Information resources,
Computer science,
Software engineering,
Permission,
Control systems,
Programming environments,
Software algorithms"
Complex Network Measurements: Estimating the Relevance of Observed Properties,"Complex networks, modeled as large graphs, received much attention during these last years. However, topological information on these networks is only available through intricate measurement procedures. Until recently, most studies assumed that these procedures eventually lead to samples large enough to be representative of the whole, at least concerning some key properties. This has a crucial impact on network modeling and simulation, which rely on these properties. Recent contributions proved that this assumption may be misleading, but no solution has been proposed. We provide here the first practical methodology to distinguish between cases where it is indeed misleading, and cases where the observed properties may be trusted. It consists in studying how the properties of interest evolve when the sample grows, and in particular whether they reach a steady state or not. In order to illustrate this method and to demonstrate its relevance, we apply it to data-sets on complex network measurements that are representative of the ones commonly used. The obtained results show that the method fulfills its goals very well. We moreover identify some properties which seem easier to evaluate in practice, thus opening interesting perspectives.",
Task-Driven Color Coding,"Color coding is a widely used visualization method for scalar data. To generate expressive and effective visual representations, it is extremely important to carefully design the mapping from data to color. In this paper, we describe a color coding approach that accounts for the different tasks users might pursue when analyzing data. Our task description is based on the task model of Andrienko & Andrienko. We apply different color scales and introduce strategies to adapt the color mapping function to support tasks like comparison, localization, or identification of data values.","image colour analysis,
data visualisation"
Channel Adaptive One Hop Broadcasting for VANETs,"One-hop broadcasting is the predominate form of network traffic in VANETs. Exchanging status information by broadcasting among the vehicles enhances vehicular active safety. Since there is no MAC layer broadcasting recovery for 802.11 based VANETs, efforts should be made towards more robust and effective transmission of such safety-related information. In this paper, a channel adaptive broadcasting method is proposed. It relies solely on channel condition information available at each vehicle by employing standard supported sequence number mechanisms. The proposed method is fully compatible with 802.11 and introduces no communication overhead. Simulation studies show that it outperforms standard broadcasting in term of reception rate and channel utilization.","Broadcasting,
Vehicle safety,
Vehicle dynamics,
Telecommunication traffic,
Unicast,
Intelligent transportation systems,
Road accidents,
Power control,
Communication standards,
Vehicles"
Design and implementation of the Smith-Waterman algorithm on the CUDA-compatible GPU,"This paper describes a design and implementation of the Smith-Waterman algorithm accelerated on the graphics processing unit (GPU). Our method is implemented using compute unified device architecture (CUDA), which is available on the nVIDIA GPU. The method efficiently uses on-chip shared memory to reduce the data amount being transferred between off-chip memory and processing elements in the GPU. Furthermore, it reduces the number of data fetches by applying a data reuse technique to query and database sequences. We show some experimental results comparing the proposed method with an OpenGL-based method. As a result, the speedup over the OpenGL-based method reaches a factor of 6.4 when using amino acid sequence database.We also find that shared memory reduces the amount of data fetches to 1/140, providing a peak performance of 5.65 giga cell updates per second (GCUPS). This performance is approximately three times faster than a prior CUDA-based implementation.","Algorithm design and analysis,
Acceleration,
Graphics,
Databases,
Computer architecture,
Libraries,
Amino acids,
Helium,
Biology computing,
Computational complexity"
Effects of Stereo Viewing Conditions on Distance Perception in Virtual Environments,"Several studies from different research groups investigating perception of absolute, egocentric distances in virtual environments have reported a compression of the intended size of the virtual space. One potential explanation for the compression is that inaccuracies and cue conflicts involving stereo viewing conditions in head mounted displays result in an inaccurate absolute scaling of the virtual world. We manipulate stereo viewing conditions in a head mounted display and show the effects of using both measured and fixed inter-pupilary distances, as well as bi-ocular and monocular viewing of graphics, on absolute distance judgments. Our results indicate that the amount of compression of distance judgments is unaffected by these manipulations. The equivalent performance with stereo, bi-ocular, and monocular viewing suggests that the limitations on the presentation of stereo imagery that are inherent in head mounted displays are likely not the source of distance compression reported in previous virtual environment studies.",
Dynamic Positron Emission Tomography Data-Driven Analysis Using Sparse Bayesian Learning,"A method is presented for the analysis of dynamic positron emission tomography (PET) data using sparse Bayesian learning. Parameters are estimated in a compartmental framework using an over-complete exponential basis set and sparse Bayesian learning. The technique is applicable to analyses requiring either a plasma or reference tissue input function and produces estimates of the system's macro-parameters and model order. In addition, the Bayesian approach returns the posterior distribution which allows for some characterisation of the error component. The method is applied to the estimation of parametric images of neuroreceptor radioligand studies.","Positron emission tomography,
Data analysis,
Bayesian methods,
Biological system modeling,
Blood,
Power system modeling,
Gunn devices,
Parameter estimation,
Plasma measurements,
Computer science"
Self-Adaptive Configuration of Visualization Pipeline Over Wide-Area Networks,"Next-generation scientific applications require the capability to visualize large archival data sets or on-going computer simulations of physical and other phenomena over wide-area network connections. To minimize the latency in interactive visualizations across wide-area networks, we propose an approach that adaptively decomposes and maps the visualization pipeline onto a set of strategically selected network nodes. This scheme is realized by grouping the modules that implement visualization and networking subtasks and mapping them onto computing nodes with possibly disparate computing capabilities and network connections. Using estimates for communication and processing times of subtasks, we present a polynomial-time algorithm to compute a decomposition and mapping to achieve minimum end-to-end delay of the visualization pipeline. We present experimental results using geographically distributed deployments to demonstrate the effectiveness of this method in visualizing data sets from three application domains.","Data visualization,
Pipelines,
Electronic mail,
Middleware,
Computational modeling,
Computer science,
Dynamic programming,
Adaptation model,
Data communication,
Distributed databases"
Parallelizing CAD: A timely research agenda for EDA,"The relative decline of single-threaded processor performance, coupled with the ongoing shift towards on chip parallelism requires that CAD applications run efficiently on parallel microprocessors. We believe that an ad hoc approach to parallelizing CAD applications will not lead to satisfactory results: neither in terms of return on engineering investment nor in terms of the computational efficiency of end applications. Instead, we propose that a key area of CAD research is to identify the design patterns underlying CAD applications and then build CAD application frameworks that aid efficient parallel software implementations of these design patterns. Our initial results indicate that parallel patterns exist in a broad range of CAD problems. We believe that frameworks for these patterns will enable CAD to successfully capitalize on increased processor performance through parallelism.","Electronic design automation and methodology,
Parallel processing,
Design automation,
Concurrent computing,
Application software,
Multicore processing,
Parallel programming,
Microprocessors,
Investments,
Logic testing"
Performance Evaluation of IEEE 802.15.4 LR-WPAN for Industrial Applications,"We present a number of performance studies of the IEEE 802.15.4 protocol. We put a special focus on application scenarios in industrial sensor network applications, which is one of the intended application domains for this protocol. The primary requirements are reduced end-to-end latency and energy consumption. Our studies are based on our new implementation of IEEE 802.15.4 developed for the simulation framework OMNeT++. We performed extensive simulations that demonstrate the capabilities of this protocol in the selected scenarios but also the limitations. In particular, we investigated the dependency of the protocol on protocol-inherent parameters such as the beacon order and the superframe order but also to different traffic load. Our results can be used for planning and deploying IEEE 802.15.4 based sensor networks with specific performance demands.","Protocols,
Peer to peer computing,
Application software,
Energy efficiency,
Network topology,
Communication industry,
Energy consumption,
Physical layer,
Wireless LAN,
Stress"
Great deluge with non-linear decay rate for solving course timetabling problems,"Course timetabling is the process of allocating, subject to constraints, limited rooms and timeslots for a set of courses to take place. Usually, in addition to constructing a feasible timetable (all constraints satisfied), there are desirable goals like minimising the number of undesirable allocations (e.g. courses timetabled in the last timeslot of the day). The construction of course timetables is regarded as a complex problem common to a wide range of educational institutions. The great deluge algorithm explores neighbouring solutions which are accepted if they are better than the best solution so far or if the detriment in quality is no larger than the current water level. In the original great deluge, the water level decreases steadily in a linear fashion. In this paper, we propose a modified version of the great deluge algorithm in which the decay rate of the water level is non-linear. The proposed method produces new best results in 4 of the 11 course timetabling problem instances used in our experiments.","Computer science,
Intelligent systems,
Educational institutions,
Testing,
Heuristic algorithms"
Dense Subsets of Pseudorandom Sets,"A theorem of Green, Tao, and Ziegler can be stated (roughly) as follows: ifR
is a pseudorandom set, and D
is a dense subset of R
, then D
maybe modeled by a set M
that is dense in the entire domain such that D
and M
are indistinguishable. (The precise statement refers to``measures'' ordistributions rather than sets.) The proof of this theorem is very general,and it applies to notions of pseudorandomness and indistinguishability definedin terms of any family of distinguishers with some mild closure properties.% \snote{added `with appropriate closure properties'}The proof proceeds via iterative partitioningand an energy increment argument, in the spirit of the proof of theweak Szemer\'edi regularity lemma. The ``reduction'' involved in the proofhas exponential complexity in the distinguishing probability.We present a new proof inspired by Nisan's proof of Impagliazzo's hardcoreset theorem. The reduction in our proof has polynomial complexity in thedistinguishing probability and provides a new characterization of thenotion of ``pseudoentropy'' of a distribution. A proof similar to ours hasalso been independently discovered by Gowers \cite{G08}.We also follow the connection between the two theorems and obtain a new proof ofImpagliazzo's hardcore set theorem via iterative partitioning andenergy increment. While our reduction has exponential complexity in someparameters, it has the advantage that the hardcore set is efficiently recognizable.",
Distributed welfare games with applications to sensor coverage,"Traditionally resource allocation problems are approached in a centralized manner; however, often centralized control is impossible. We consider a distributed, non-cooperative approach to resource allocation. In particular, we consider the situation where the global planner does not have the authority to assign players to resources; rather, players are self-interested. The question that emerges is how can the global planner entice the players to settle on a desirable allocation with respect to the global welfare? To study this question, we focus on a class of games that we refer to as distributed welfare games. Within this context, we investigate how the global planner should distribute the global welfare to the players. We measure the efficacy of a distribution rule in two ways: (i) Does a pure Nash equilibrium exist? (ii) How efficient are the Nash equilibria as compared with the global optimum? We derive sufficient conditions on the distribution rule that ensures the existence of a pure Nash equilibrium in any single-selection distributed welfare game. Furthermore, we derive bounds on the efficiency of these distribution rules in a variety of settings. Lastly, we highlight the implications of these results in the context of the sensor coverage problem.","Resource management,
Nash equilibrium,
Centralized control,
Sufficient conditions,
Space missions,
Event detection,
Transportation,
Fellows,
Laboratories,
Computer science"
Genetic Algorithm Based Wireless Sensor Network Localization,"In most sensor network applications, the information gathered by sensors will be meaningless without the location of the sensor nodes. Node localization has been a topic of active research in recent years. Accurate self-localization capability is highly desirable in wireless sensor network. This paper proposes a genetic algorithm based localization (GAL). The proposed genetic algorithm adopts two new genetic operators: single-vertex-neighborhood mutation and the descend-based arithmetic crossover. Four example problems are used to evaluate the performance of the proposed algorithm. Simulation results show that our algorithm can achieve higher accurate position estimation than semi-definite programming with gradient search localization (SDPL) [11] and simulated annealing based localization (SAL)[13]. Compared to the usual crossover operator: simple arithmetic crossover, whole arithmetic crossover and single-point crossover, the proposed crossover can obtain a lower mean position error.","Genetic algorithms,
Wireless sensor networks,
Global Positioning System,
Temperature sensors,
Arithmetic,
Patient monitoring,
Event detection,
Fires,
Computer networks,
Computer science"
Learning predictive terrain models for legged robot locomotion,"Legged robots require accurate models of their environment in order to plan and execute paths. We present a probabilistic technique based on Gaussian processes that allows terrain models to be learned and updated efficiently using sparse approximation techniques. The major benefit of our terrain model is its ability to predict elevations at unseen locations more reliably than alternative approaches, while it also yields estimates of the uncertainty in the prediction. In particular, our nonstationary Gaussian process model adapts its covariance to the situation at hand, allowing more accurate inference of terrain height at points that have not been observed directly. We show how a conventional motion planner can use the learned terrain model to plan a path to a goal location, using a terrain-specific cost model to accept or reject candidate footholds. In experiments with a real quadruped robot equipped with a laser range finder, we demonstrate the usefulness of our approach and discuss its benefits compared to simpler terrain models such as elevations grids.","Robots,
Adaptation model,
Predictive models,
Foot,
Leg,
Gaussian processes,
Kernel"
SVM-based discriminative accumulation scheme for place recognition,"Integrating information coming from different sensors is a fundamental capability for autonomous robots. For complex tasks like topological localization, it would be desirable to use multiple cues, possibly from different modalities, so to achieve robust performance. This paper proposes a new method for integrating multiple cues. For each cue we train a large margin classifier which outputs a set of scores indicating the confidence of the decision. These scores are then used as input to a Support Vector Machine, that learns how to weight each cue, for each class, optimally during training. We call this algorithm SVM-based Discriminative Accumulation Scheme (SVM-DAS). We applied our method to the topological localization task, using vision and laser-based cues. Experimental results clearly show the value of our approach.","Support vector machines,
Support vector machine classification,
Robustness,
Robot sensing systems,
Mobile robots,
Indoor environments,
Lighting,
Robotics and automation,
USA Councils,
Computer science"
Network coding with multi-generation mixing,"Connectivity, losses, and buffering are factors that directly affect the performance of network coding. These factors affect the ability of intermediate nodes to generate useful encodings; these are encodings that contribute in propagating and recovering data at receiver node(s). It has been shown that in particular scenarios network coding performance can be improved by simply increasing the generation size (k); however, this leads to increasing transmission overhead, encoding complexity and (more importantly) buffer sizes at intermediate nodes. In this paper, we propose a new network coding approach where we employ Multi-Generation Mixing (MGM). MGM eliminates the need to increasing buffer sizes while improving the performance of network coding. Under MGM, we define a mixing set of size m generations that can be network coded (mixed) together. Within each MGM mixing set, a new set of generation packets are mixed with previously transmitted (network coded) generations. This generalized approach provides a great deal of resilience against losses when compared with traditional generation-based network coding. Our analysis of the performance of MGM-based network coding demonstrates significant reduction in required overhead for a given recovery performance at the receivers. We also illustrate the performance of MGM using extensive simulations that provide a useful insight into the viability of MGM-based network coding.","Network coding,
Encoding,
Performance loss,
Throughput,
Degradation,
Decoding,
Bandwidth,
Resilience,
IEEE members,
Galois fields"
Automatic Extraction of Useful Facet Hierarchies from Text Databases,"Databases of text and text-annotated data constitute a significant fraction of the information available in electronic form. Searching and browsing are the typical ways that users locate items of interest in such databases. Faceted interfaces represent a new powerful paradigm that proved to be a successful complement to searching. Thus far the identification of the facets was either a manual procedure or relied on apriori knowledge of the facets that can potentially appear in the underlying collection. In this paper we present an unsupervised technique for automatic extraction of facets useful for browsing text databases. In particular we observe through a pilot study that facet terms rarely appear in text documents showing that we need external resources to identify useful facet terms. For this we first identify important phrases in each document. Then we expand each phrase with ""context"" phrases using external resources such as WordNet and Wikipedia causing facet terms to appear in the expanded database. Finally we compare the term distributions in the original database and the expanded database to identify the terms that can be used to construct browsing facets. Our extensive user studies using the Amazon Mechanical Turk service show that our techniques produce facets with high precision and recall that are superior to existing approaches and help users locate interesting items faster.","Image databases,
YouTube,
TV,
Data mining,
Computer science,
Information management,
Keyword search,
Motion pictures,
Concrete,
Taxonomy"
The Catalog Archive Server Database Management System,The multiterabyte Sloan Digital Sky Survey's (SDSS's) catalog data is stored in a commercial relational database management system with SQL query access and a built-in query optimizer. The SDSS catalog archive server adds advanced data mining features to the DBMS to provide fast online access to the data.,"Database systems,
Content addressable storage,
Data mining,
Photometry,
Pipelines,
Data models,
Cameras,
Relational databases,
Web server,
Query processing"
An integrated optoacoustic transducer combining etalon and black PDMS structures,"An integrated optoacoustic transducer combining etalon and black polydimethylsiloxane (PDMS) structures has been designed and developed. The device consists of an 11-mum-thick black PDMS film confined to a 2-mm-diameter circular region acting as an optoacoustic transmitter, surrounded by a 5.9-mum Fabry-Perot polymer etalon structure serving as an optoacoustic detector array. A pulsed laser is focused onto a 30-mum spot on the black PDMS film, defining the transmit element, while a CW laser probes a 20-mum spot on the etalon for ultrasound detection. Pulse-echo signals display center frequencies of above 30 MHz with bandwidths of at least 40 MHz. A theta-array is formed for 3-D ultrasound imaging by mechanically scanning the generation laser along a 1-D array and the detection laser around an annular array. Preliminary images with 3 metal wires as imaging targets are presented. Characterization of the device's acoustical properties, as well as preliminary imaging results, suggest that all-optical ultrasound transducers are potential alternatives to piezoelectric techniques for high-frequency 2-D arrays enabling 3-D high-resolution ultrasound imaging.","Ultrasonic imaging,
Acoustic imaging,
Ultrasonic transducer arrays,
High-resolution imaging,
Polymer films,
Optical arrays,
Transmitters,
Fabry-Perot,
Detectors,
Sensor arrays"
Peak Power Reduction Through Dynamic Partitioning of Scan Chains,"Serial shift operations in scan-based testing impose elevated levels of power dissipation, endangering the reliability of the chip being tested. Scan chain partitioning techniques are quite effective in reducing test power, as the rippling in the clock network, in the scan chains, and in the combination logic is reduced altogether. Partitioning approaches implemented in a static manner may fail to reduce peak power down to the desired level, however, depending on the transition distribution of the problematic pattern in the statically constructed scan chain partitions. In this paper, we propose a dynamic partitioning approach capable of adapting to the transition distribution of any test pattern, and thus of delivering near-perfect peak power reductions. We formulate the scan chain partitioning problem via Integer Linear Programming (ILP) and also propose an efficient greedy heuristic. The proposed partitioning hardware allows for the partitioning reconfiguration on a per test pattern basis, enabling the dynamic partitioning. Significant peak power reductions are thus attained cost-effectively.","Circuit testing,
Clocks,
Power dissipation,
Logic testing,
Hardware,
Integer linear programming,
Pins,
Power engineering computing,
Reliability engineering,
Power engineering and energy"
Entity-based collaboration tools for intelligence analysis,"Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.","Collaborative tools,
Collaborative work,
Software tools,
Collaborative software,
Information analysis,
Text analysis,
System testing,
Laboratories,
Design optimization,
Organizing"
Challenges and Advances in Parallel Sparse Matrix-Matrix Multiplication,"We identify the challenges that are special to parallel sparse matrix-matrix multiplication (PSpGEMM). We show that sparse algorithms are not as scalable as their dense counterparts, because in general, there are not enough non-trivial arithmetic operations to hide the communication costs as well as the sparsity overheads. We analyze the scalability of 1D and 2D algorithms for PSpGEMM. While the 1D algorithm is a variant of existing implementations, 2D algorithms presented are completely novel. Most of these algorithms are based on the previous research on parallel dense matrix multiplication. We also provide results from preliminary experiments with 2D algorithms.","Program processors,
Algorithm design and analysis,
Sparse matrices,
Artificial neural networks,
Scalability,
Kernel,
Clustering algorithms"
Printed electronics for low-cost electronic systems: Technology status and application development,"In recent years, printing has received substantial interest as a technique for realizing low cost, large area electronic systems. Printing allows the use of purely additive processing, thus lowering process complexity and material usage. Coupled with the use of low-cost substrates such as plastic, metal foils, etc., it is expected that printed electronics will enable the realization of a wide range of easily deployable electronic systems, including displays, sensors, and RFID tags. We review our work on the development of technologies and applications for printed electronics. By combining synthetically derived inorganic nanoparticles and organic materials, we have realized a range of printable electronic “inks”, and used these to demonstrate printed passive components, multilayer interconnection, diodes, transistors, memories, batteries, and various types of gas and biosensors. By exploiting the ability of printing to cheaply allow for the integration of diverse functionalities and materials onto the same substrate, therefore, it is possible to realize printed systems that exploit the advantages of printing while working around the disadvantages of the same.","Printing,
Costs,
Additives,
Plastics,
Flat panel displays,
Sensor systems,
RFID tags,
Nanoparticles,
Organic materials,
Nonhomogeneous media"
Plagiarism Detection Using the Levenshtein Distance and Smith-Waterman Algorithm,"Plagiarism in texts is issues of increasing concern to the academic community. Now most common text plagiarism occurs by making a variety of minor alterations that include the insertion, deletion, or substitution of words. Such simple changes, however, require excessive string comparisons. In this paper, we present a hybrid plagiarism detection method. We investigate the use of a diagonal line, which is derived from Levenshtein distance, and simplified SmithWaterman algorithm that is a classical tool in the identification and quantification of local similarities in biological sequences, with a view to the application in the plagiarism detection. Our approach avoids globally involved string comparisons and considers psychological factors, which can yield significant speed-up by experiment results. Based on the results, we indicate the practicality of such improvement using Levenshtein distance and Smith-Waterman algorithm and to illustrate the efficiency gains. In the future, it would be interesting to explore appropriate heuristics in the area of text comparison","Distance measurement,
Plagiarism,
Dynamic programming,
Heuristic algorithms,
Arrays,
Algorithm design and analysis,
Biology"
Drift-free tracking of rigid and articulated objects,"Model-based 3D tracker estimate the position, rotation, and joint angles of a given model from video data of one or multiple cameras. They often rely on image features that are tracked over time but the accumulation of small errors results in a drift away from the target object. In this work, we address the drift problem for the challenging task of human motion capture and tracking in the presence of multiple moving objects where the error accumulation becomes even more problematic due to occlusions. To this end, we propose an analysis-by-synthesis framework for articulated models. It combines the complementary concepts of patch-based and region-based matching to track both structured and homogeneous body parts. The performance of our method is demonstrated for rigid bodies, body parts, and full human bodies where the sequences contain fast movements, self-occlusions, multiple moving objects, and clutter. We also provide a quantitative error analysis and comparison with other model-based approaches.","Humans,
Target tracking,
Image motion analysis,
Cameras,
Biological system modeling,
Motion analysis,
Biomedical optical imaging,
Computer science,
Error analysis,
Medical diagnosis"
A Class of Quantum LDPC Codes Constructed From Finite Geometries,"Low-density parity check (LDPC) codes are a significant class of classical codes with many applications. Several good LDPC codes have been constructed using random, algebraic, and finite geometries approaches, with containing cycles of length at least six in their Tanner graphs. However, it is impossible to design a self-orthogonal parity check matrix of an LDPC code without introducing cycles of length four. In this paper, a new class of quantum LDPC codes based on lines and points of finite geometries is constructed. The parity check matrices of these codes are adapted to be self- orthogonal with containing only one cycle of length four in each pair of two rows. Also, the column and row weights, and bounds on the minimum distance of these codes are given. As a consequence, these codes can be encoded using shift-register encoding algorithms and can be decoded using iterative decoding algorithms over various quantum depolarizing channels.","Parity check codes,
Iterative decoding,
Iterative algorithms,
Quantum computing,
Sparse matrices,
Galois fields,
Computational geometry,
Computer science,
Application software,
Turbo codes"
Implementation of a New Protection Scheme on a Real Distribution System in Presence of DG,"Conventional electric distribution systems are radial in nature, and are supplied through a main source. These networks have a very simple protection system which is usually implemented using fuses, re-closers, and over-current relays. Recently, great attention has been paid to applying Distributed Generation (DG) throughout electric distribution systems. Presence of such generation units in a network, leads to losing coordination of protection devices. Therefore, it is required to have an algorithm in hand which is capable of protecting distribution systems that include DG, through diagnosis and isolation of occurred faults. In this paper, a new approach for protecting of distribution networks in presence of DGs will be presented. The algorithm is based on dividing an existing distribution network into several zones; each of them is capable of operating in island operation. The proposed scheme has been implemented on some part of a real distribution network in Shiraz which is a large city in Iran and performance of the proposed scheme is tested on it. For simulating the distribution network and for implementing the relay algorithm, DIgSILENT Power Factory 13.2 and MATLAB are used respectively.","Protection,
Fuses,
Protective relaying,
Distributed control,
Fault diagnosis,
Cities and towns,
Testing,
Relays,
Production facilities,
MATLAB"
An Adaptive Median Filter for Image Denoising,"In this paper, a new image-denoising filter that is based on the standard median (SM) filter is proposed. In our method, a threshold and the standard median is used to detect noise and change the original pixel value to a newer that is closer to or the same as the standard median. We also incorporate the center weighted median (CWM) filter in our method. With our experimental results, we have made a comparison among our method, the standard median (SM) filter, the center weighted median (CWM) filter, and the Tri-State Median (TSM) filter, in which our method proves to be superior.","Adaptive filters,
Image denoising,
Information filtering,
Information filters,
Computer science,
Samarium,
Pixel,
Resists,
Sorting,
Information technology"
Using Memcached for Data Distribution in Industrial Environment,This article focuses on data exchange in a distributed industrial application. It presents data distribution mechanism that imitates distributed shared memory by caching heavily used objects in memory. When configured and used properly such system is suitable for transparent data sharing in a distributed real-time image processing environment.,
Constrained spectral clustering through affinity propagation,"Pairwise constraints specify whether or not two samples should be in one cluster. Although it has been successful to incorporate them into traditional clustering methods, such as K-means, little progress has been made in combining them with spectral clustering. The major challenge in designing an effective constrained spectral clustering is a sensible combination of the scarce pairwise constraints with the original affinity matrix. We propose to combine the two sources of affinity by propagating the pairwise constraints information over the original affinity matrix. Our method has a Gaussian process interpretation and results in a closed-form expression for the new affinity matrix. Experiments show it outperforms state-of-the-art constrained clustering methods in getting good clusterings with fewer constraints, and yields good image segmentation with user-specified pairwise constraints.","Clustering methods,
Gaussian processes,
Clustering algorithms,
Kernel,
Covariance matrix,
Closed-form solution,
Image segmentation,
Machine learning algorithms,
Machine learning,
Data mining"
The Impact of Educational Background on the Effectiveness of Requirements Inspections: An Empirical Study,"While the inspection of various software artifacts increases the quality of the end product, the effectiveness of an inspection depends largely on the individual inspectors involved. To address that issue, a large-scale controlled inspection experiment with over 70 professionals was conducted at Microsoft Corporation that focused on the relationship between an inspector's background and their effectiveness during a requirements inspection. The results of the study showed that inspectors with university degrees in majors not related to computer science found significantly more defects than those with degrees in computer science majors. We also observed that level of education (Masters, PhD), prior industrial experience or other job related experiences did not significantly impact the effectiveness of an inspector. The only other type of experience that had a significant impact on effectiveness was experience in writing requirements, i.e. professionals with prior experience writing requirements found statistically significant more defects than their counterparts.","Inspection,
Software quality,
Computer science,
Writing,
Bioreactors,
Large-scale systems,
Computer science education,
Software measurement,
Programming"
An Analysis of Hard Drive Energy Consumption,The increasing storage capacity and necessary redundancy of data centers and other large-scale IT facilities has drawn attention to the issue of reducing the power consumption of hard drives. This work comprehensively investigates the power consumption of hard drives to determine typical runtime power profiles. We have instrumented at a fine-grained level and present our findings which show that (i) the energy consumed by the electronics of a drive is just as important as the mechanical energy consumption; (ii) the energy required to access data is affected by physical location on a drive; and (iii) the size of data transfers has measurable effect on power consumption.,"Energy consumption,
Protocols,
Delay,
Telecommunication traffic,
Internet,
Prototypes,
Speech,
Jacobian matrices,
Computer science,
Communications technology"
Routing in intermittently connected sensor networks,"To prolong the lifetime of sensor networks, various scheduling schemes have been designed to reduce the number of active sensors. However, some scheduling strategies, such as partial coverage scheduling and target coverage scheduling, may result in disconnected network topologies, due to the low density of the active nodes. In such cases, traditional routing algorithms cannot be applied, and the shortest path discovered by these algorithms may not have the minimum packet delivery latency. In this paper, we address the problem of finding minimum latency routes in intermittently connected sensor networks by proposing an on-demand minimum latency (ODML) routing algorithm. Since on-demand routing algorithm does not work well when the source and destination frequently communicate with each other, we propose two proactive minimum latency routing algorithms: optimal-PML and quick-PML. Theoretical analysis and simulation results show that (1) ODML can effectively identify minimum latency routes which have much smaller latency than the shortest path, and (2) optimal-PML can minimize the routing message overhead and quick-PML can significantly reduce the route acquisition delay.","Routing,
Delay,
Processor scheduling,
Computer science,
Monitoring,
Network topology,
Surveillance,
Design engineering,
Educational institutions,
Analytical models"
Reliable motion artifact detection for ECG monitoring systems with dry electrodes,"Reliable signals are the basic prerequisite for most mobile ECG monitoring applications. Especially when signals are analyzed automatically, capable motion artifact detection algorithms are of great importance. This article presents different artifact detection algorithms for ECG systems with dry electrodes. The algorithms are based on the measurement of additional parameters that are correlated with the artifacts. We describe a mobile measurement system and the procedure used for the evaluation of these algorithms. The algorithms are assessed based upon their effect on QRS detection. The best algorithm improved sensitivity (Se) from 98.7% to 99.8% and positive predictive value (+P) from 98.3% to 99.9%, while 15% of the signal was marked as artifact. This corresponds to a decrease in false positive and false negative detected beats by 89.9%. Different metrics to evaluate the performance of an artifact detection algorithm are presented.",
Head pose estimation: Classification or regression?,"Head pose estimation has many useful applications in practice. How to estimate the head pose automatically and robustly is still a challenging problem. In pose estimation, different pose angles can be used as regression values or viewed as different class labels. Thus a question is raised in our study: which is proper for pose estimation - classification or regression? We investigate representative classification and regression methods on the same problem to see any difference. A method that combines regression and classification approaches is also examined. Preliminary experiments show some interesting results which might prompt further exploration of related issues in pose estimation.","Magnetic heads,
Support vector machines,
Support vector machine classification,
Application software,
Robustness,
Humans,
Training data,
Databases,
Face recognition,
Computer science"
Semantic Information Retrieval for Personalized E-Learning,"We present an approach for personalized retrieval in an e-learning platform, that takes advantage of semantic Web standards to represent the learning content and the user/learner profiles as ontologies, and that re-ranks search results/lectures based on how the contained terms map to these ontologies. One important aspect of our approach is the combination of an authoritatively supplied taxonomy by the colleges, with the data driven extraction (via clustering) of a taxonomy from the documents themselves, thus making it easier to adapt to different learning platforms, and making it easier to evolve with the document/lecture collection. Our experimental results show that the learner's context can be effectively used for improving the precision and recall in e-learning content retrieval, particularly by re-ranking the search results based on the learner's past activities.",
Conceptual modelling: Knowledge acquisition and model abstraction,"Conceptual modelling has gained a lot of interest in recent years and simulation modellers are particularly interested in understanding the processes involved in arriving at a conceptual model. This paper contributes to this understanding by discussing the artifacts of conceptual modelling and two specific conceptual modelling processes: knowledge acquisition and model abstraction. Knowledge acquisition is the process of finding out about the problem situation and arriving at a system description. Model abstraction refers to the simplifications made in moving from a system description to a conceptual model. Soft Systems Methodology has tools that can help a modeller with knowledge acquisition and model abstraction. These tools are drawing rich pictures, undertaking analyses ‘one’, ‘two’, ‘three’, and constructing a root definition and the corresponding purposeful activity model. The use of these tools is discussed with respect to a case study in health care.",
Scheduling Model for Cognitive Radio,"The priority queue model divides CR(cognitive radio) system users into primary user and secondary user. The primary user has preemptive priority over secondary user, while the same priority level users are FIFO served. This paper models the CR system based on scheduling technology. The secondary users are further divided into different priority levels. A hybrid priority dynamic policy, which indicates primary user's preemptive priority and secondary user's nonpreemptive priority, is developed to reduce spectrum switch overhead during spectrum leasing process. Theoretical analyzing shows that hybrid priority reduces the spectrum switch overhead and the hybrid priority improvement is a non-negative mono-increasing function of priority level. Based on the proposed policy, CR scheduling model is built and CR scheduling rule is provided and proved. Experimental comparison between CR scheduling and priority queue shows that CR scheduling gets a smaller weighted staying time sum than priority queue and the improvement increases as system becomes more congested.",
Automatic Service Composition Based on Enhanced Service Dependency Graph,"Service dependency graph (SDG) is an AND/OR graph showing input output dependencies among service operations. As dependencies in an SDG are indirectly expressed by reasoning on data models used by service interface definitions, their re-usability and expressiveness are limited. In this paper, we propose an enhanced version of service dependency graph, namely SDG+. SDG+ enhances SDG with explicit dependency declaration, which expresses dependencies directly with static explicit declarations. Based on SDG+, we developed our automatic service composition algorithm for WS-Challenge 2007, which wins the championship of composition efficiency in the competition.","Data models,
Web services,
Computer science,
Web and internet services,
Heuristic algorithms,
Costs,
XML"
Interference-Aware Routing Metric for Improved Load Balancing in Wireless Mesh Networks,"Multihop wireless mesh networks are an attractive solution for providing last-mile connectivity. However, the shared nature of the transmission medium makes it challenging to fully exploit these networks. Nodes interfere with each other, resulting in packet loss and degraded network performance. In this paper, a routing metric specifically designed for WMNs is proposed. The Interference-Aware Routing metric (IAR) uses MAC-level information to measure the share of the channel that each link is able to utilize effectively. As a result, paths are selected that exhibit the least interference. Simulations show that utilizing this metric provides significant performance improvements in terms of end-to-end delay compared to several existing metrics.",
Two Types of Attacks against  Cognitive Radio Network MAC Protocols,"Some typical MAC protocols have been proposed for multi-hop CR network recently. In a multi-hop MAC protocol, a node uses the common control channel to perform channel negotiation before data transmission. Recent research findings indicate that insecure transmission of control channels open vulnerable holes for the Denial of Service attacks. This paper makes a security analysis for CR network MAC protocols. There are two types of attacks against CR network MAC protocols. Firstly, we study how denial of service (DoS) attack is launched in multi-hop CR network MAC protocols. Then, we explore MAC layer greedy behaviors in CR networks. Our analysis and simulations indicate that such attacks can greatly affect the performance of CR networks. And the key factors for attack efficiency are presented in this paper.",
Block Matching Algorithm Based on Particle Swarm Optimization for Motion Estimation,"In this paper, based on Particle Swarm Optimization (PSO), we propose a fast block matching algorithm for motion estimation (ME) and compare the algorithm with other popular fast block-matching algorithms for ME. A real-world example shows that the block matching algorithm based on PSO for ME is more feasible than others. Moreover, the initial values of parameters in PSO are empirically discussed, since they directly affect the computational complexity. Thus, an improved PSO algorithm for ME is empirically given to reduce computational complexity.","Particle swarm optimization,
Pixel,
Algorithm design and analysis,
Software algorithms,
Complexity theory,
Optimization,
Equations"
Background Subtraction for Temporally Irregular Dynamic Textures,"In the traditional mixture of Gaussians background model, the generating process of each pixel is modeled as a mixture of Gaussians over color. Unfortunately, this model performs poorly when the background consists of dynamic textures such as trees waving in the wind and rippling water. To address this deficiency, researchers have recently looked to more complex and/or less compact representations of the background process. We propose a generalization of the MoG model that handles dynamic textures. In the context of background modeling, we achieve better, more accurate segmentations than the competing methods, using a model whose complexity grows with the underlying complexity of the scene (as any good model should), rather than the amount of time required to observe all aspects of the texture.","Gaussian processes,
Kernel,
Context modeling,
Color,
Layout,
Pixel,
Optical noise,
Colored noise,
Image motion analysis,
History"
Real-time gait mode intent recognition of a powered knee and ankle prosthesis for standing and walking,"This paper describes a real-time gait mode intent recognition approach for the supervisory control of a powered transfemoral prosthesis. The proposed approach infers user intent by recognizing patterns in the prosthesis sensor’s signals in real-time, eliminating the need for sound-side instrumentation and allowing fast mode switching. Simple time based features extracted from frames of prosthesis signals are reduced to lower dimensions. Gaussian Mixture Models are trained using an experimental database for gait mode classification. A voting scheme is applied as a post-processing step to increase the robustness of decision making. The effectiveness of the proposed method is shown via gait experiments on a treadmill with a healthy subject using an able bodied adapter.","Knee,
Prosthetics,
Legged locomotion,
Supervisory control,
Pattern recognition,
Instruments,
Feature extraction,
Spatial databases,
Voting,
Robustness"
"Insertable surgical imaging device with pan, tilt, zoom, and lighting","This paper describes work we have done in developing an insertable surgical imaging device with multiple degrees-of-freedom for minimally invasive surgery. The device is fully insertable into the abdomen using standard 12mm trocars. It consists of a modular camera and lens system which has pan and tilt capability provided by 2 small DC servo motors. It also has its own integrated lighting system that is part of the camera assembly. Once the camera is inserted into the abdomen, the insertion port is available for additional tooling, motivating the idea of single port surgery. A third zoom axis has been designed for the camera as well, allowing close-up and far-away imaging of surgical sites with a single camera unit.","Cameras,
Minimally invasive surgery,
Laparoscopes,
Abdomen,
Testing,
Performance evaluation,
Lenses,
Servomechanisms,
DC motors,
Servomotors"
Massively Parallel Network Coding on GPUs,"Network coding has recently been widely applied in various networks for system throughput improvement and/or resilience to network dynamics. However, the computational overhead introduced by the network coding operations is not negligible and has become the cornerstone for real deployment of network coding. In this paper, we exploit the computing power of contemporary Graphic Processing Units (GPUs) to accelerate the network coding operations. We proposed three parallel algorithms that maximize the parallelism of the encoding and decoding processes, i.e., the power of GPUs is fully utilized. This paper also shares our optimization design choices and our workarounds to the challenges encountered in working with GPUs. With our implementation of the algorithms, we are able to achieve up to 12 times of speedup over the highly optimized CPU counterpart, using the NVIDIA GPU and the Computer Unified Device Architecture (CUDA) programming model.","Network coding,
Computer networks,
Throughput,
Resilience,
Graphics,
Acceleration,
Parallel algorithms,
Parallel processing,
Encoding,
Decoding"
Ensemble Methods for Ontology Learning - An Empirical Experiment to Evaluate Combinations of Concept Acquisition Techniques,"Most approaches to ontology learning combine techniques from different areas (hybrid approaches) to increase the efficiency of the ontology learning process. However, the results from the ontology learning process do not fully satisfy the users at present. An important problem is that there is a lack of quantitative and comparative data about the efficiency of techniques and technique combinations applied to ontology learning. Combination methods are an effective way of improving system performance, but there is not enough information about how to use, configure and combine techniques from a diverse spectrum of fields, and what the contribution of a specific technique or technique combination. In this paper we  present a quantitative comparison of technique combinations for concept extraction and a software system (OntoLancs) to support the evaluation of techniques. By applying OntoLancs, users are able to assist the process of building ontologies by semi-automatically acquiring concepts from large-scale domain document collections and experiment with different combinations of knowledge acquisition techniques to refine  and organize domain concepts into a taxonomy. Quantitative and comparative studies about the performance of several techniques and user experiences indicate the applicability and usefulness of  our approach.","Ontologies,
Frequency,
Terminology,
Filtering,
Software systems,
Large-scale systems,
Filters,
Humans,
Information science,
Guidelines"
On Geometric Modeling of the Human Intracranial Venous System,"We describe a process aiming to construct a 3-D geometric model of the human normal intracranial venous system from MRA data. An analysis of geometric properties of the intracranial veins and sinuses results in proposing three models: circular, elliptic, and free-shape. We formulate a rule based on which a suitable geometric venous model can be selected. The cross-sectional shape of different parts of dural venous sinuses is found to be better approximated by ellipses and free shapes, while for veins the circular and elliptic models are comparable. An analysis of using splines for radii smoothing is also provided. The approach is useful for building venous models in education and clinical applications.","Solid modeling,
Humans,
Shape,
Veins,
Arteries,
Biomedical imaging,
Radiology,
Isosurfaces,
Labeling,
Data analysis"
Support vector machines and dynamic time warping for time series,"Effective use of support vector machines (SVMs) in classification necessitates the appropriate choice of a kernel. Designing problem specific kernels involves the definition of a similarity measure, with the condition that kernels are positive semi-definite (PSD). An alternative approach which places no such restrictions on the similarity measure is to construct a set of inputs and let each example be represented by its similarity to all the examples in this set and then apply a conventional SVM to this transformed data. Dynamic time warping (DTW) is a well established distance measure for time series but has been of limited use in SVMs since it is not obvious how it can be used to derive a PSD kernel. The feasibility of the similarity based approach for DTW is investigated by applying the method to a large set of time-series classification problems.","Kernel,
Support vector machines,
Distance measurement,
Time series analysis,
Training,
Artificial neural networks,
Accuracy"
An Event-Based Near Real-Time Data Integration Architecture,"Extract-Transform-Load (ETL) tools feed data from operational databases into data warehouses. Traditionally, these ETL tools use batch processing and operate offline at regular time intervals, for example on a nightly or weekly basis. Naturally, users prefer to have up-to-date data to make their decisions, therefore there is a demand for real-time ETL tools. In this paper we investigate an event-based near real-time ETL layer for transferring and transforming data from the operational database to the data warehouse. One of our main concerns in this paper is master data management in the ETL layer. We present the architecture of a novel, general purpose, event-driven, and near real-time ETL layer that uses a Database Queue (DBQ), works on a push technology principle and directly supports content enrichment. We also observe that the system architecture is consistent with the information architecture of a classical Online Transaction Processing (OLTP) application, allowing us to distinguish between different kinds of data to increase the clarity of the design. Keywords: event-based architecture, content enrichment, master data, extract-transform-load, enterprise service bus.",
Optimal Precoding for Digital Subscriber Lines,"We determine the linear precoding policy that maximizes the mutual information for general multiple-input multiple-output (MIMO) Gaussian channels with arbitrary input distributions, by capitalizing on the relationship between mutual information and minimum mean squared error (MMSE). The optimal linear precoder can be computed by means of a fixed- point equation as a function of the channel and the input constellation. We show that diagonalizing the channel matrix does not maximize the information transmission rate for nonGaussian inputs. A full precoding matrix may significantly increase the information transmission rate, even for parallel non-interacting channels. We illustrate the application of our results to typical Gigabit DSL systems.","DSL,
Mutual information,
MIMO,
Bit error rate,
Equations,
Communications Society,
Telecommunications,
Computer science,
Gaussian channels,
Telephony"
Modeling Resource Sharing Dynamics of VoIP Users over a WLAN Using a Game-Theoretic Approach,"We consider a scenario in which users share an access point and are mainly interested in VoIP applications. Each user is allowed to adapt to varying network conditions by choosing the transmission rate at which VoIP traffic is received. We denote this adaptation process by end-user congestion control, our object of study. The two questions that we ask are: (1) what are the performance consequences of letting the users to freely choose their rates? and (2) how to explain the adaptation process of the users? We set a controlled lab experiment having students as subject to answer the first question, and we extend an evolutionary game-theoretic model to address the second. Our partial answers are the following: (1) free users with local information can reach an equilibrium which is close to optimal from the system perspective. However, the equilibrium can be unfair; (2) the adaptation of the users can be explained using a game theoretic model. We propose a methodology to parameterize the latter, which involves active network measurements, simulations and an artificial neural network to estimate the QoS perceived by the users in each of the states of the model.","Resource management,
Wireless LAN,
Automatic control,
Internet telephony,
Codecs,
Aggregates,
Communications Society,
Systems engineering and theory,
Computer science,
Application software"
Max Margin AND/OR Graph learning for parsing the human body,"We present a novel structure learning method, Max Margin AND/OR Graph (MM-AOG), for parsing the human body into parts and recovering their poses. Our method represents the human body and its parts by an AND/OR graph, which is a multi-level mixture of Markov Random Fields (MRFs). Max-margin learning, which is a generalization of the training algorithm for support vector machines (SVMs), is used to learn the parameters of the AND/OR graph model discriminatively. There are four advantages from this combination of AND/OR graphs and max-margin learning. Firstly, the AND/OR graph allows us to handle enormous articulated poses with a compact graphical model. Secondly, max-margin learning has more discriminative power than the traditional maximum likelihood approach. Thirdly, the parameters of the AND/OR graph model are optimized globally. In particular, the weights of the appearance model for individual nodes and the relative importance of spatial relationships between nodes are learnt simultaneously. Finally, the kernel trick can be used to handle high dimensional features and to enable complex similarity measure of shapes. We perform comparison experiments on the base ball datasets, showing significant improvements over state of the art methods.","Humans,
Biological system modeling,
Machine learning,
Tree graphs,
Statistics,
Maximum likelihood estimation,
Kernel,
Shape measurement,
Performance analysis,
Computer vision"
Considering IEC 61131-3 and IEC 61499 in the context of component frameworks,"Automation and control systems provide their own programming and modeling paradigms in parallel to theory well known from computer science. Especially the concepts of component-based software engineering highly influence the design of embedded systems, which include automation and control systems. This work analyses what is and is not a ldquosoftware componentrdquo in the context of the two standards IEC 61131-3 and IEC 61499. Utilizing the definition of component frameworks and component system architectures, the potentials and problems of an integrative approach in order to utilize both standards within the same platform was discussed.","software architecture,
control engineering computing,
embedded systems,
IEC standards,
object-oriented programming,
programmable controllers"
Flexible HALS algorithms for sparse non-negative matrix/tensor factorization,"In this paper we propose a family of new algorithms for non-negative matrix/tensor factorization (NMF/NTF) and sparse nonnegative coding and representation that has many potential applications in computational neuroscience, multi-sensory, multidimensional data analysis and text mining. We have developed a class of local algorithms which are extensions of Hierarchical Alternating Least Squares (HALS) algorithms proposed by us in [1]. For these purposes, we have performed simultaneous constrained minimization of a set of robust cost functions called alpha and beta divergences. Our algorithms are locally stable and work well for the NMF blind source separation (BSS) not only for the over-determined case but also for an under-determined (over-complete) case (i.e., for a system which has less sensors than sources) if data are sufficiently sparse. The NMF learning rules are extended and generalized for N-th order nonnegative tensor factorization (NTF). Moreover, new algorithms can be potentially accommodated to different noise statistics by just adjusting a single parameter. Extensive experimental results confirm the validity and high performance of the developed algorithms, especially, with usage of the multi-layer hierarchical approach [1].","Sparse matrices,
Tensile stress,
Computer applications,
Neuroscience,
Multidimensional systems,
Data analysis,
Text mining,
Least squares methods,
Robustness,
Cost function"
ParColl: Partitioned Collective I/O on the Cray XT,"Collective I/O orchestrates I/O from parallel processes by aggregating fine-grained requests into large ones. However, its performance is typically a fraction of the potential I/O bandwidth on large scale platforms such as Cray XT. Based on our analysis, the time spent in global process synchronization dominates the actual time in file reads/writes, which imposes a 'collective wall' on the performance of collective I/O. In this paper, we introduce a novel technique called partitioned collective I/O (ParColl). ParColl augments the original two-phase collective I/O protocol with new mechanisms for file area partitioning, I/O aggregator distribution and intermediate file views. Through these mechanisms, a group of processes and their targeted file are consistently divided into a collection of small subgroups, each performing I/O aggregation in a disjoint manner. File consistency is maintained through intermediate file views when necessary. Together, these mechanisms greatly reduce the cost of global synchronization. Our experimental results demonstrate that ParColl significantly improves the performance and the scalability of collective I/O. In one case, we show a 416% improvement on 1024 processes for a visualization I/O benchmark. We also show that the I/O patterns in scientific applications can benefit significantly from this technique, e.g. BT-I/O and Flash I/O.","Synchronization,
Protocols,
Scalability,
Tiles,
File systems,
Optimization,
Servers"
Multi-agent application in protection coordination of power system with distributed generations,"This paper presents new explorations into the use of agent technology applied to substations protection coordination of power system. The impact of distributed generation on protection is firstly discussed. Then a coordination multi-agent system is proposed with the functions of the agents described. In the proposed system, communication will play an important role to provide more information for the relay coordination besides the relay settings. Relay coordination strategy is also discussed and communication simulation between different substations has been carried out on Java Agent Development Framework (JADE) platform. The simulation results show that proper protection coordination can be achieved by the proposed coordination strategy and information communication.","Substations,
Relays,
Containers,
Generators,
Circuit faults,
Load flow,
Distributed control"
Automatic Generation of Complex Properties for Hardware Designs,"Property checking is a promising approach to prove the correctness of today's complex designs. However, in practice this requires the formulation of formal properties which is a time consuming and non-trivial task. Therefore the acceptance and efficiency of formal verification techniques can be raised by an automated support for formulating design properties. In this paper we propose a new methodology to automatically generate complex properties for a given design. The tool, Dianosis, implements this methodology by analyzing a simulation trace. The extracted properties describe the abstract design behavior and are presented in a format that is easy to read and can be added to the set of properties used for formal or assertion-based verification. We provide experimental results on industrial hardware designs that show the effectiveness of Dianosis and motivate the practical use.",
Post-silicon verification for cache coherence,"Modern processor designs are extremely complex and difficult to validate during development, causing a growing portion of the verification effort to shift to post-silicon, after the first few hardware prototypes become available. Extremely slow simulation speeds during pre-silicon verification result in functional errors escaping into silicon, a problem that is further exacerbated by the growing complexity of the memory subsystem in multi-core platforms. In this work we present CoSMa, a novel technology offering high coverage functional post-silicon validation of cache coherence protocols in multi-core systems. It enables the detection and diagnosis of functional errors in the memory subsystem by recording at runtime a compact encoding of the operations occurring at each cache line and checking their correctness at regular intervals. We leverage the system’s existing memory resources to store the required activity, thus minimizing area overhead. When the system is finally ready for customer shipment, CoSMa can be completely disabled, eliminating any performance or memory overhead. We reproduce in our experiments a set of coherence protocol bugs based on published errata documents of commercial multi-core designs, and show that CoSMa is highly effective in detecting them.","Protocols,
Coherence,
Process design,
Hardware,
Prototypes,
Silicon,
Error correction,
Runtime,
Encoding,
Computer bugs"
Automated Planning and Optimization of Lumber Production Using Machine Vision and Computed Tomography,An automated system for planning and optimization of lumber production using Machine Vision and Computed Tomography (CT) is proposed. Cross-sectional CT images of hardwood logs are analyzed using machine vision algorithms. Internal defects in the hardwood logs pockets are identified and localized. A virtual in silico 3-D reconstruction of the hardwood log and its internal defects is generated using Kalman filter-based tracking algorithms. Various sawing operations are simulated on the virtual 3-D reconstruction of the log and the resulting virtual lumber products automatically graded using rules stipulated by the National Hardwood Lumber Association (NHLA). Knowledge of the internal log defects is suitably exploited to formulate sawing strategies that optimize the value yield recovery of the resulting lumber products. A prototype implementation shows significant gains in value yield recovery when compared with lumber processing strategies that use only the information derived from the external log structure.,"Production planning,
Machine vision,
Computed tomography,
Three dimensional displays,
Sawing,
Production systems,
Image analysis,
Algorithm design and analysis,
Kalman filters,
Prototypes"
Practical Localized Network Coding in Wireless Mesh Networks,"In this paper, BFLY-a practical localized network coding protocol for wireless mesh networks-is proposed. To supplement forwarding packets in classical networks, intermediate wireless nodes code packets from different sources, so that each transmission's information content is increased. Prior work allowed intermediate nodes to code (i.e, XOR) packets such that the recipient of the coded message must decode the message before forwarding. BFLY, however, allows intermediate recipients to, in addition to XOR-ing, forward coded packets; and thus further exploits network coding opportunities in multihop wireless networks. BFLY utilizes knowledge of local topologies and source route information in packet headers. We have developed network coding modules in ns-2 that facilitate simulations with large networks. Simulation studies show that BFLY can increase overall network throughput by a factor of 1.2 - 2 and reduce end-to-end latency.",
Analysis and design of book-ahead bandwidth-sharing mechanisms,"In this article, we present a novel discrete-time Markov chain model of book-ahead bandwidth-sharing mechanisms. We use this analytical model and a simulation model to understand the benefits of book-ahead (BA) bandwidth-sharing when compared to the immediate-request (IR) call-blocking mode of bandwidth-sharing in circuit-switched networks. We study two different BA schemes, BA-all, in which the caller accepts any set of available timeslots, and BA-n, in which the caller specifies n call-initiation time options. Numerical results show that the BA-all achieves 95% utilization with a call-blocking probability of only 1%, while in the IR mode, call blocking probability is 23% even when utilization is only 80%. The BA-n schemes perform as well as the BA-all scheme if the call-initiation time options are restricted to fall on timeslot boundaries separated by the minimum call holding time. The length of the advance reservation horizon, K, is shown to increase linearly with call holding time, H. The ratio K/H is primarily dependent on the link capacity in channels. For example, if the link is divided into 10 channels, to achieve a 2% call blocking probability, the advance-reservation horizon needs to be a factor of 4 times the call holding time. In other words, the extra data storage and processing required to accept and maintain advance reservations is not significant.","Bandwidth,
Circuit testing,
Routing protocols,
Analytical models,
Optical fiber networks,
Visualization,
Computer networks,
Instruments,
Educational institutions,
Switches"
Decision Tree Ensemble: Small Heterogeneous Is Better Than Large Homogeneous,"Using decision trees that split on randomly selected attributes is one way to increase the diversity within an ensemble of decision trees. Another approach increases diversity by combining multiple tree algorithms. The random forest approach has become popular because it is simple and yields good results with common datasets. We present a technique that combines heterogeneous tree algorithms and contrast it with homogeneous forest algorithms. Our results indicate that random forests do poorly when faced with irrelevant attributes, while our heterogeneous technique handles them robustly. Further, we show that large ensembles of random trees are more susceptible to diminishing returns than our technique. We are able to obtain better results across a large number of common datasets with a significantly smaller ensemble.","Decision trees,
Diversity reception,
Robustness,
Bagging,
Machine learning,
Application software,
Computer science,
Accuracy,
Algorithm design and analysis,
Training data"
An Evaluation of a Mobile Game Concept for Lectures,"This paper describes an evaluation of a new game concept, Lecture Quiz, which can be used in lectures in higher education to promote strong student participation and variation in how lectures are taught. The lecture game uses the equipment and infrastructure already available in lecture halls like the teacher’s portable PC, a large screen and a video projector, network connections, and the students’ mobile phones. The main game runs on the teacher’s portable PC projected on a large screen, whereas the students will interact with the game using their own mobile phones. Lecture Quiz is a multiplayer quiz game, which offers a variation in game modes where unlimited number of players can play simultaneously. Games like Lecture Quiz are most useful for testing and rehearsing theory. As a bonus, the teacher will get quantitative data on how much of the theory the students actually have learned. The evaluation of Lecture Quiz was performed in a software architecture lecture where twenty students first played the game and then were asked to fill in an evaluation form. The focus of the evaluation was on usability of the system and the perceived usefulness of using Lecture Quiz in lectures. The results of the evaluation show that Lecture Quiz was easy to use and that it contributed to increased learning. Further, Lecture Quiz was perceived as entertaining, and half of the students  claimed they would attend more lectures if such systems were used regularly.",
An integrated background model for video surveillance based on primal sketch and 3D scene geometry,"This paper presents a novel integrated background model for video surveillance. Our model uses a primal sketch representation for image appearance and 3D scene geometry to capture the ground plane and major surfaces in the scene. The primal sketch model divides the background image into three types of regions — flat, sketchable and textured. The three types of regions are modeled respectively by mixture of Gaussians, image primitives and LBP histograms. We calibrate the camera and recover important planes such as ground, horizontal surfaces, walls, stairs in the 3D scene, and use geometric information to predict the sizes and locations of foreground blobs to further reduce false alarms. Compared with the state-of-the-art background modeling methods, our approach is more effective, especially for indoor scenes where shadows, highlights and reflections of moving objects and camera exposure adjusting usually cause problems. Experiment results demonstrate that our approach improves the performance of background/foreground separation at pixel level, and the integrated video surveillance system at the object and trajectory level.","Solid modeling,
Video surveillance,
Layout,
Geometry,
Cameras,
Context modeling,
Gaussian distribution,
Gaussian processes,
Histograms,
Statistics"
REVIEW - A reference data set for retinal vessel profiles,"This paper describes REVIEW, a new retinal vessel reference dataset. This dataset includes 16 images with 193 vessel segments, demonstrating a variety of pathologies and vessel types. The vessel edges are marked by three observers using a special drawing tool. The paper also describes the algorithm used to process these segments to produce vessel profiles, against which vessel width measurement algorithms can be assessed. Recommendations are given for use of the dataset in performance assessment. REVIEW can be downloaded from http://ReviewDB.lincoln.ac.uk.",
Probabilistic localization with a blind robot,"Researchers have addressed the localization problem for mobile robots using many different kinds of sensors, including rangefinders, cameras, and odometers. In this paper, we consider localization using a robot that is virtually “blind”, having only a clock and contact sensor at its disposal. This represents a drastic reduction in sensing requirements, even in light of existing work that considers localization with limited sensing. We present probabilistic techniques that represent and update the robot’s position uncertainty and algorithms to reduce this uncertainty. We demonstrate the experimental effectiveness of these methods using a Roomba autonomous vacuum cleaner robot in laboratory environments.","Robot sensing systems,
Mobile robots,
Uncertainty,
USA Councils,
Robotics and automation,
Computer science,
Clocks,
Robot control,
Robot vision systems,
Cameras"
Use of Tethered Small Unmanned Aerial System at Berkman Plaza II Collapse,"A tethered Small Unmanned Aerial System (sUAS) provided structural forensic inspection of the collapsed Berkman Plaza II six-story parking garage. The sUAS, an iSENSYS IP3 miniature helicopter, was tethered to meet US Federal Aviation Administration (FAA) requirements for unregulated flight below 45 m (150 ft). This created new platform control, human-robot interaction, and safety issues in addition to the challenges posed by the active, city environment. A new technique, viewpoint-oriented Cognitive Work Analysis (CWA), was used to generate the 4:1 human-robot crew organization and operational protocol. The sUAS over three flights was able to provide useful imagery to structural engineers that had been difficult to obtain from manned helicopters due to dust obscurants. Based on these flights this work shows that tethered operations decreases team effectiveness, increases overall safety liability, and in general is not a recommended solution for sUAS flight.",
Building a Service-Oriented Ontology for Wireless Sensor Networks,"Wireless sensor networks (WSNs) provide various environment data in the real-world, and also WSNs's middleware is able to offer field data in real-time by user queries. For materialization of the future ubiquitous computing which enables networking with things at anytime, anywhere and any-devices, WSNs occupy the important position with RFID technologies, and it has evolved and advanced currently. This paper proposes a service-oriented sensor ontology which enables service-oriented services in future ubiquitous computing. Taking reuse of ontology into consideration, ServiceProperty, LocationProperty and PhysicalProperty classes were derived from Geography Markup Language (GML), Sensor Web Enablement (SWE), SensorML and Suggested Upper Merged Ontology (SUMO) and OntoSensor ontology, and its properties and constraints were also defined newly as service-oriented service. We presented the validation and consistency check of the proposed ontology using Protégé 3.3.1 and RACER 1.9.0, respectively, and indicated the results of service query which used the SPARQL query language.","Ontologies,
Wireless sensor networks,
Sensor phenomena and characterization,
Ubiquitous computing,
Computer networks,
Pervasive computing,
Telecommunication computing,
Geography,
Markup languages,
Information retrieval"
Tempering Kademlia with a Robust Identity Based System,"The lack of a trusted authority, responsible for peers' identity verification or for authentication purposes, makes actual P2P systems extremely vulnerable to a large spectrum of attacks. The main purpose of this paper is to present Likir (Layered Identity-based Kademlia-like InfRastructure), a framework that includes an identity-based scheme and a secure communication protocol, built on top of Kademlia, that may provide an effective defense against well known attacks. This will be accomplished with the adoption of a certification service, with the use of an authentication protocol between nodes and with the introduction of credentials to make non-repudiable the ownership of the contents and messages inserted in the DHT. For sake of interoperability with other social networking services, Likir enables identity management under the Identity 2.0 framework. Under this perspective, the IBS (Identity-Based Signature) scheme is taken into consideration and analyzed as well.","Peer to peer computing,
Routing,
Protocols,
Indexes,
IP networks,
Cryptography,
Public key"
Measurement and analysis of variability in 45nm strained-Si CMOS technology,"A test-chip in a low-power 45nm technology, featuring uniaxial strained Si, has been built to study variability in CMOS circuits. Systematic layout-induced variation, die-to-die (D2D), wafer-to-wafer (W2W) and within-die (WID) variability has been measured and analyzed. Delay is characterized using an array of ring-oscillators and transistor leakage current is measured with an on-chip ADC. Results show that systematic variations are small and layout-induced variation is dominated by strain effects.","CMOS technology,
Tensile strain,
Tensile stress,
MOS devices,
Current measurement,
Capacitive sensors,
Lithography,
Circuit testing,
Leakage current,
Dielectric substrates"
Visual cluster analysis of trajectory data with interactive Kohonen Maps,"Visual-interactive cluster analysis provides valuable tools for effectively analyzing large and complex data sets. Due to desirable properties and an inherent predisposition for visualization, the Kohonen Feature Map (or Self-Organizing Map, or SOM) algorithm is among the most popular and widely used visual clustering techniques. However, the unsupervised nature of the algorithm may be disadvantageous in certain applications. Depending on initialization and data characteristics, cluster maps (cluster layouts) may emerge that do not comply with user preferences, expectations, or the application context. Considering SOM-based analysis of trajectory data, we propose a comprehensive visual-interactive monitoring and control framework extending the basic SOM algorithm. The framework implements the general Visual Analytics idea to effectively combine automatic data analysis with human expert supervision. It provides simple, yet effective facilities for visually monitoring and interactively controlling the trajectory clustering process at arbitrary levels of detail. The approach allows the user to leverage existing domain knowledge and user preferences, arriving at improved cluster maps. We apply the framework on a trajectory clustering problem, demonstrating its potential in combining both unsupervised (machine) and supervised (human expert) processing, in producing appropriate cluster results.",
Lossless Data Hiding Based on Difference Expansion without a Location Map,"This paper proposes a reversible data hiding method based on expanding difference between a pair of neighboring pixels. The proposed method adopts the relation among pixels in a block to determine whether a pair of pixels can be expanded. During the embedding process, the pixels used to determine the expanding status of pixels always remain unchanged and, later, they can be adopted by the decoder to identify the expanding status of a difference when extracting a message. As the expanding status is not saved in the location map, all of the embedding capacities can be used to embed user’s message. In addition, the embedding capacity can be finely tuned by a parameter which determines whether a difference value can be expanded. Experimental results show that both the capacity for embedding user’s message and the image quality are significantly improved, when compared with Tian’s study [IEEE Trans. Circuits Syst. Video Technol., 13(8), pp. 890-896, 2003].","Data encapsulation,
Pixel,
Decoding,
Art,
Image coding,
Signal processing,
Information management,
Data engineering,
Computer science,
Image quality"
E-Learning meets the Social Semantic Web,"The Social Semantic Web has recently emerged as a paradigm in which ontologies (aimed at defining, structuring and sharing information) and collaborative software (used for creating and sharing knowledge) have been merged together. Ontologies provide an effective means of capturing and integrating knowledge for feedback provisioning, while using collaborative activities can support pedagogical theories, such as social constructivism. Both technologies have developed separately in the e-learning domain; representing respectively a teacher-centered and a learner–centered approach for learning environments. In this paper we bridge the gap between these two approaches by leveraging the Social Semantic Web paradigm, and propose a collaborative semantic-rich learning environment in which folksonomies created from students’ collaborative tags contribute to ontology maintenance, and teacher-directed feedback.","Electronic learning,
Semantic Web,
Ontologies,
Feedback,
Collaborative work,
Blogs,
Video sharing,
International collaboration,
Tagging,
Merging"
Semi-Supervised Discriminant Analysis using robust path-based similarity,"Linear Discriminant Analysis (LDA), which works by maximizing the within-class similarity and minimizing the between-class similarity simultaneously, is a popular dimensionality reduction technique in pattern recognition and machine learning. In real-world applications when labeled data are limited, LDA does not work well. Under many situations, however, it is easy to obtain unlabeled data in large quantities. In this paper, we propose a novel dimensionality reduction method, called Semi-Supervised Discriminant Analysis (SSDA), which can utilize both labeled and unlabeled data to perform dimensionality reduction in the semi-supervised setting. Our method uses a robust path-based similarity measure to capture the manifold structure of the data and then uses the obtained similarity to maximize the separability between different classes. A kernel extension of the proposed method for nonlinear dimensionality reduction in the semi-supervised setting is also presented. Experiments on face recognition demonstrate the effectiveness of the proposed method.","Linear discriminant analysis,
Scattering,
Semisupervised learning,
Machine learning,
Matrices,
Pattern recognition,
Face recognition,
Null space,
Noise robustness,
Pattern analysis"
Estimating value in service systems: A case study of a repair service system,"The economic structure of service systems has steadily increased in complexity in recent years. This is due not only to specialization in direct material production and services offered, but also in the ownership and management of resources, the role of intangible assets such as process knowledge, and the context in which goods and services are consumed. This increase in complexity represents both a challenge and an opportunity in a service-oriented economy. In this paper, we offer a descriptive structure for the analysis of this complexity which combines graph theory and network flows with economic tools. Our analysis is based on publicly observable information and can be used to analyze service systems in terms of the value they deliver, how they deliver it, and how value can be discovered and increased. We show how this analysis can be applied (in the example of a car manufacturer and its service system for suppliers and dealerships) to improve customer satisfaction and provide options and analysis models for outsourcing decision makers.",
Using a Low-Cost SoC Computer and a Commercial RTOS in an Embedded Systems Design Course,"This paper describes the author's experiences using a low-cost system-on-a-chip (SoC) embedded computer system and a commercial real-time operating system (RTOS) in the laboratory component of an undergraduate embedded system design class. The target hardware is a small low-cost X86 SoC computer system that has a wide range of I/O features. For software development, a popular commercial hard RTOS is used that has been designed for use in embedded devices. This course covers both hardware and software topics in embedded systems, and the course culminates in a final team-based design project. A full set of course materials including a textbook with laboratory tutorials, instructor slides, and code examples has been developed and is available online in electronic form.","Laboratories,
Hardware,
Computers,
System-on-a-chip,
Driver circuits,
Programming,
Software"
Large-scale read/write margin measurement in 45nm CMOS SRAM arrays,Distributions of read and write noise margins in large CMOS SRAM arrays are investigated by directly measuring the bit-line current during bitline / wordline (write) or cell supply (read) voltage sweep in a 768Kb 45nm CMOS SRAM test-chip. Good correlation between write/read margin estimates through the bit-line measurements and the DC read SNM (RSNM) and IW measurements in small on-chip SRAM macros with wired-out storage nodes are demonstrated. Four common writeability metrics are correlated and compared. Array-level characterization of SRAM cell read stability and writeability allow fast and accurate characterization of high-density SRAM arrays is scalable for capturing up to 6 standard deviations of parameter variations.,"Random access memory,
Current measurement,
Correlation,
Semiconductor device measurement,
Voltage measurement,
CMOS integrated circuits,
Stability analysis"
Effective lateral modulations with applications to shear modulus reconstruction using displacement vector measurement,"High accuracy in measuring target motions can be realized by combined use of our previously developed lateral Gaussian envelope cosine modulation method (LGECMM) and displacement vector measurement methods that enable simultaneous axial and lateral displacement measurements, such as the multidimensional autocorrelation method (MAM). In this paper, LGECMM is improved by using parabolic functions and Hanning windows instead of Gaussian functions in the apodization function, i.e., parabolic apodization and Hanning apodization. The new modulations enable decreases in effective aperture length (i.e., channels) and yield more accurate displacement vector measurements than LGECMM due to increased echo signal-to-noise ratio and lateral spatial resolution. That is, on the basis of a priori knowledge about ultrasound propagation using the focusing scheme and shape of the apodization function, we stopped using Fraunhofer approximation. As practical applications of the modulations, for an agar phantom that is deformed in a lateral direction, stable and accurate 2-D shear modulus reconstructions are performed using our previously developed direct inversion approach together with 2-D strain tensor measurements using MAM.",
A recursive filter for linear systems on Riemannian manifolds,"We present an online, recursive filtering technique to model linear dynamical systems that operate on the state space of symmetric positive definite matrices (tensors) that lie on a Riemannian manifold. The proposed approach describes a predict-and-update computational paradigm, similar to a vector Kalman filter, to estimate the optimal tensor state. We adapt the original Kalman filtering algorithm to appropriately propagate the state over time and assimilate observations, while conforming to the geometry of the manifold. We validate our algorithm with synthetic data experiments and demonstrate its application to visual object tracking using covariance features.","Nonlinear filters,
Linear systems,
Tensile stress,
State-space methods,
Symmetric matrices,
Vectors,
State estimation,
Kalman filters,
Filtering algorithms,
Geometry"
Hole Filling of a 3D Model by Flipping Signs of a Signed Distance Field in Adaptive Resolution,"When we use range finders to observe the shape of an object, many occluded areas may occur. These become holes and gaps in the model and make it undesirable for various applications. We propose a novel method to fill holes and gaps to complete this incomplete model. As an intermediate representation, we use a signed distance field (SDF), which stores euclidean signed distances from a voxel to the nearest point of the mesh model. By using an SDF, we can obtain interpolating surfaces for holes and gaps. The proposed method generates an interpolating surface that becomes smoothly continuous with real surfaces by minimizing the area of the interpolating surface. Since the isosurface of an SDF can be identified as being a real or interpolating surface from the magnitude of signed distances, our method computes the area of an interpolating surface in the neighborhood of a voxel both before and after flipping the sign of the signed distance of the voxel. If the area is reduced by flipping the sign, then our method changes the sign for the voxel. Therefore, we minimize the area of the interpolating surface by iterating this computation until convergence. Unlike methods based on partial differential equations (PDEs), our method does not require any boundary condition and the initial state that we use is automatically obtained by computing the distance to the closest point of the real surface. Moreover, because our method can be applied to an SDF of adaptive resolution, our method efficiently interpolates large holes and gaps of high curvature. We tested the proposed method with both synthesized and real objects and evaluated the interpolating surfaces.","Filling,
Solid modeling,
Shape,
Surface fitting,
Interpolation,
Topology,
Isosurfaces,
Convergence,
Partial differential equations,
Boundary conditions"
Dynamic Access Network Selection with QoS Parameters Estimation: A Step Closer to ABC,"Always best connected (ABC) services allows multi- mode mobile terminals to stay connected to the best available networks, at anytime according to user preferences. One of the key aspects in realizing such ABC service is mainly attributed to an effective and dynamic access network selection process. However, most of the previous works consider the access network selection process as a static optimization problem which fails to address the dynamic QoS conditions intrinsic in wireless networks. One of the main challenges remains as no efficient way in obtaining dynamic QoS parameters such as packet delay, packet loss and jitter. In this paper, we proposed a novel dynamic access network selection algorithm capable of adapting to prevailing network conditions. Our algorithm is a dual stage estimation process where network selection performed using sequential Bayesian estimation relies on dynamic QoS parameters estimated through bootstrap approximation. Simulations demonstrate the effectiveness of our proposed algorithm which outperforms static optimization approach in a highly efficient manner.","Parameter estimation,
Quality of service,
Rats,
WiMAX,
Information analysis,
Delay estimation,
Telecommunication traffic,
Computer science,
Mobile computing,
Wireless networks"
Enhanced Yoking Proof Protocols for RFID Tags and Tag Groups,"The RFID system is a contactless automatic identification system that identifies tags attached on goods through radio frequency communication. This system is expected to supplant barcode systems, the contact reading technique that is most widely used at present. The RFID system can be applied in a variety of areas. Among those, Ari Juels proposed an environment to prove that a pair of tags has been scanned simultaneously. And he presented a ""yoking proof"" protocol for this. But the yoking-proof protocol is vulnerable to replay attack. Although modified yoking-proof protocols for alleviating this drawback have been proposed, they are not immune to replay attack, either. In this paper, we analyze problems of existing yoking-proof protocols and present a new protocol, which will make replay attack difficult, based on this analysis. We have also extend this protocol so that it can provide yoking proofs for a tag group, including a multiple number of tags.","Protocols,
RFID tags,
Radiofrequency identification,
Computer science,
Logistics,
Privacy,
Application software,
Computer networks,
Systems engineering and theory,
Educational institutions"
Minimizing Energy Consumption Using Cognitive Radio,"In this paper, we show how cognitive radio can help minimize energy consumption of a wireless mobile communication device. We propose an energy optimization framework using cognitive radio for a given quality of service requirement based on the channel and the radio capabilities. The cognitive radio not only adjusts modulation, coding, and radiated power, as with conventional adaptive modulation, but also adjusts component characteristics (e.g., power amplifier characteristics) so that the radio operates with the highest energy efficient possible way. Simulation results show that significant energy savings (up to 75%) can be achieved compared to conventional adaptive modulation. This framework also can be applied to optimize radio operations to achieve additional goals.","Energy consumption,
Cognitive radio,
Chromium,
Delay,
Modulation coding,
Wireless communication,
Quality of service,
Power generation,
Power amplifiers,
Power engineering and energy"
Low-voltage green transistor using ultra shallow junction and hetero-tunneling,"A novel hetero-tunnel transistor (HtFET) with a heterostructure and ultra shallow junction parallel to the dielectric interface is proposed for low-voltage (low-power) electronics. Its potential of scaling Vdd down to 0.2 V is examined with quantum mechanical tunneling theory. Data from high-K metal-gate, Si on Ge hetero-tunnel transistor verifies the HtFET concept.","Tunneling,
MOSFETs,
Electrons,
High-K gate dielectrics,
Transistors,
Quantum mechanics,
Voltage,
Energy consumption,
Thermal management,
Medical simulation"
A polynomial-time bound for matching and registration with outliers,"We present a framework for computing optimal transformations, aligning one point set to another, in the presence of outliers. Example applications include shape matching and registration (using, for example, similarity, affine or projective transformations) as well as multiview reconstruction problems (triangulation, camera pose etc.). While standard methods like RANSAC essentially use heuristics to cope with outliers, we seek to find the largest possible subset of consistent correspondences and the globally optimal transformation aligning the point sets. Based on theory from computational geometry, we show that this is indeed possible to accomplish in polynomial-time. We develop several algorithms which make efficient use of convex programming. The scheme has been tested and evaluated on both synthetic and real data for several applications.","Polynomials,
Shape,
Image reconstruction,
Cameras,
Computational geometry,
Application software,
Councils,
Testing,
Computer vision,
Algorithms"
CODES: An Integrated Approach to Composable Modeling and Simulation,"In component-based simulation, models developed in different locations and for specific purposes can be selected and assembled in various combinations to meet diverse user requirements. This paper proposes CODES (COmposable Discrete-Event scalable Simulation), an approach to component-based modeling and simulation that supports model reuse across multiple application domains. A simulation component is viewed by the modeller as a black box with an in- and/or out-channel. The attributes and behavior of the component abstracted as a meta-component are described using COML (COmponent Markup Language), a markup language we propose for representing simulation components. The integrated approach, supported by a proposed COSMO (COmponent-oriented Simulation and Modeling Ontology) ontology, consists of four main steps. Component discovery returns a set of syntactically valid model components. Syntactic composability is determined by our proposed EBNF syntactic composition rules. Validation of semantic composability is performed using our proposed data and behavior alignment algorithms. The semantically valid simulation component is subsequently stored in a model repository for reuse. As proof of concept, we discuss a prototype implementation of the CODES frameworkusing queueing system as an application domain example.",
On the Complexity of Classification Functions,"A classification function is a multiple-valued input function specified by a set of rules, where each rule is a conjunction of range functions. The function is useful for packet classification for internet, network intrusion detection system, etc.This paper considers the complexity of range functions and classification functions represented by sum-of-products expressionsof binary variables. It gives tighter upper bounds on the number of products for range functions.","Intrusion detection,
Upper bound,
Web and internet services,
Hardware,
Multivalued logic,
Computer science,
IP networks,
Minimization,
Table lookup,
Cams"
Performance Analysis of Anonymous Communication Channels Provided by Tor,"Providing anonymity for end-users on the Internet is a very challenging and difficult task. There are currently only a few systems that are of practical relevance for the provision of low-latency anonymity. One of the most important to mention is the Tor network that is based on onion routing. Practical usage of the system often leads to delays which are not tolerated by the average end-user. This, in return, discourages many of them from the use of such systems and hence indirectly lowers the protection of remaining users due to a smaller user base. In this paper we show to which extend overloaded nodes and links, as well as geographical diversity of nodes have an influence on the general performance of Tor communication channels. After that, we propose new methods of path selection for performance-improved onion routing which are based on actively measured latencies and estimated available capacities using passive observations of link-wise throughput.","Performance analysis,
Communication channels,
Delay,
Protection,
Routing,
Availability,
Computer security,
Computer science,
Internet,
Performance evaluation"
Developing scenarios for robot assisted play,"This paper describes the user-centred development of play scenarios for robot assisted play, as part of the IROMEC project that develops a novel robotic toy for children with special needs. The project investigates how robotic toys can become social mediators, encouraging children with special needs to discover a range of play styles, from solitary to collaborative play (with peers, carers/teachers, parents etc). This paper presents the developmental process of constructing relevant play scenarios for children with different special needs. This process is driven by a) a comprehensive literature review that is related to play activities of children from different target user groups with existing technology, consultation with panel of experts (therapist, teachers, parents) and b) by the result of experimental investigations of user requirements in trials with children with special needs. An important step (reported here) towards the development of the final play scenarios is the development of Outline Play Scenarios — a set of abstract scenarios that reflect the users’ requirements and which are not related to any specific technological solution. The general methodological approach, as well as the outline play scenarios, may benefit the development of scenarios for other human-robot interaction research in robot assisted play and related areas. In future, these outline scenarios will be further developed to reflect and utilise the specific functionalities to be implemented in the new IROMEC robot and its different modules.",Robots
Combining the Power of Taverna and caGrid: Scientific Workflows that Enable Web-Scale Collaboration,"Service-oriented architecture represents a promising approach to integrating data and software across different institutional and disciplinary sources, thus facilitating Web-scale collaboration while avoiding the need to convert different data and software to common formats. The US National Cancer Institute's Biomedical Information Grid program seeks to create both a service-oriented infrastructure (caGrid) and a suite of data and analytic services. Workflow tools in caGrid facilitate both the use and creation of services by accelerating service discovery, composition, and orchestration tasks. The authors present caGrid's workflow requirements and explain how they met these requirements by adopting and extending the Taverna system.","Collaborative work,
Service oriented architecture,
Cancer,
Collaborative software,
Ecosystems,
Data analysis,
Acceleration,
Cities and towns,
Information analysis,
Collaboration"
Efficient Complex Event Processing over RFID Data Stream,"RFID technology holds the promise of real-time identifying, locating and monitoring physical objects. To achieve these goals, RFID events need to be collected efficiently and composed expressively. Furthermore, these events have unique characteristics, such as locomotive, temporal and history oriented which should be considered and integrated into an event engine model. The diversity of RFID applications poses further challenges to a generalized framework for RFID events processing. In this paper, the Expressive Stream Language is utilized to collect vast number of primitive events efficiently. Moreover, we introduce a novel semantics to meet requirement of expressive event composition. At last, we use Timed Petri Net to model our newly RFID complex event engine. By introducing typical applications scenarios, we evaluate the validity and effectiveness of our RFID event processing system.","Radiofrequency identification,
Search engines,
Filtering,
History,
Filters,
Logic,
Computer networks,
Physics computing,
Information science,
IP networks"
Greedy Heuristic Algorithms to Generate Variable Strength Combinatorial Test Suite,"Combinatorial testing is a practical software testing approach that has been widely used in practice. Most research and applications of such approach focus on N-way combinatorial testing that provides a minimum coverage of all N-way interactions among factor. However, the strengths of different interactions may not be a fixed integer N, but a variable. Therefore, variable strength combinatorial testing approach is necessary in applications. Existing variable strength combinatorial testing, which allows some interactions have a higher strength than others, has a limitation that such higher-strength interactions must be disjoint. To avoid such a limitation, an improved variable strength combinatorial testing approach, which makes a more sufficient consideration on actual interaction relationship, is proposed in this article. Furthermore, two greedy heuristic algorithms are also proposed to generate combinatorial test suite. Compared to some existing algorithms and tools, the proposed algorithms have advantages on both the execution effectiveness and the optimality of generated test suite. Experimental results can prove such advantages.","Testing,
Arrays,
Fault currents,
Circuit faults,
Current transformers,
Computed tomography,
Algorithm design and analysis"
Spherical Piecewise Constant Basis Functions for All-Frequency Precomputed Radiance Transfer,"This paper presents a novel basis function, called spherical piecewise constant basis function (SPCBF), for precomputed radiance transfer. SPCBFs have several desirable properties: rotatability, ability to represent all-frequency signals, and support for efficient multiple product. By smartly partitioning the illumination sphere into a set of subregions and associating each subregion with an SPCBF valued 1 inside the region and 0 elsewhere, we precompute the light coefficients using the resulting SPCBFs. Efficient rotation of the light representation in SPCBFs is achieved by rotating the domain of SPCBFs. During runtime rendering, we approximate the BRDF and visibility coefficients using the set of SPCBFs for light, possibly rotated, through fast lookup of summed-area table (SAT) and visibility distance table (VDT), respectively. SPCBFs enable new effects such as object rotation in all-frequency rendering of dynamic scenes and on-the-fly BRDF editing under rotating environment lighting. With graphics hardware acceleration, our method achieves real-time frame rates.","Layout,
Rendering (computer graphics),
Lighting,
Computer science,
Computer Society,
Runtime,
Graphics,
Hardware,
Acceleration,
Shadow mapping"
SPRAT: Runtime processor selection for energy-aware computing,"A commodity personal computer (PC) can be seen as a hybrid computing system equipped with two different kinds of processors, i.e. CPU and a graphics processing unit (GPU). Since the superiorities of GPUs in the performance and the power efficiency strongly depend on the system configuration and the data size determined at the runtime, a programmer cannot always know which processor should be used to execute a certain kernel. Therefore, this paper presents a runtime environment that dynamically selects an appropriate processor so as to improve the energy efficiency. The evaluation results clearly indicate that the runtime processor selection at executing each kernel with given data streams is promising for energy-aware computing on a hybrid computing system.","Kernel,
Runtime,
Engines,
Programming,
Switches,
Computer languages,
Energy consumption"
Narratives: A visualization to track narrative events as they develop,"Analyzing unstructured text streams can be challenging. One popular approach is to isolate specific themes in the text, and to visualize the connections between them. Some existing systems, like ThemeRiver, provide a temporal view of changes in themes; other systems, like In-Spire, use clustering techniques to help an analyst identify the themes at a single point in time. Narratives combines both of these techniques; it uses a temporal axis to visualize ways that concepts have changed over time, and introduces several methods to explore how those concepts relate to each other. Narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time. Users can relate articles through time by examining the topical keywords that summarize a specific news event. By tracking the attention to a news article in the form of references in social media (such as weblogs), a user discovers both important events and measures the social relevance of these stories.","Blogs,
Event detection,
Text processing,
Computer graphics,
Application software,
Visual analytics,
Data mining,
Reflection,
Data visualization,
Displays"
A statistical approach for full-chip gate-oxide reliability analysis,"Gate oxide breakdown is a key factor limiting the useful lifetime of an integrated circuit. Unfortunately, the conventional approach for full chip oxide reliability analysis assumes a uniform oxide-thickness for all devices. In practice, however, gate-oxide thickness varies from die-to-die and within-die and as the precision of process control worsens an alternative reliability analysis approach is needed. In this work, we propose a statistical framework for chip level gate oxide reliability analysis while considering both die-to-die and within-die components of thickness variation. The thickness of each device is modeled as a distinct random variable and thus the full chip reliability estimation problem is defined on a huge sample space of several million devices. We observe that the full chip oxide reliability is independent of the relative location of the individual devices. This enables us to transform the problem such that the resulting representation can be expressed in terms of only two distinct random variables. Using this transformation we present a computationally efficient and accurate approach for estimating the full chip reliability while considering spatial correlations of gateoxide thickness. We show that, compared to Monte Carlo simulation, the proposed method incurs an error of only 1∼6% while improving the runtime by around three orders.","Electric breakdown,
Random variables,
Voltage,
Integrated circuit reliability,
Performance analysis,
Temperature,
Dielectrics,
Leakage current,
Electron traps,
Lead compounds"
Realization of a fast current control system of PMSM based on model predictive control,"This paper describes realization of a fast current control system of permanent magnet synchronous motor (PMSM) based on model predictive control (MPC). This current controller directly selects one of the switching modes of a PWM inverter, while conventional one generates voltage reference regarding an inverter as ideal amplifier. The problem with selecting the switching mode to output is formulated based on MPC. By considering an inverter specifically, we can use not only a sinusoidal PWM mode but also an overmodulation mode without special handing. And in this paper, real-time implementation method using a look-up table, which is designed beforehand, is discussed and the experimental results are shown.",
A Partial order approach to decentralized control,"In this paper we employ the theory of partially ordered sets to model and analyze a class of decentralized control problems. We show that posets provide a natural way of modeling problems where communication constraints between subsystems have a hierarchical structure. We show that such problems have appealing algebraic properties that can be exploited to parameterize the set of stabilizing controllers. While much of the paper is devoted to problems where the plant and controller have identical communication constraints, we also generalize our theory to case where they may have different communication constraints.","Distributed control,
Communication system control,
Constraint theory,
Centralized control,
Transfer functions,
Control theory,
Large-scale systems,
Decision making,
Algebra,
Combinatorial mathematics"
Development of a Weather Forecasting Code: A Case Study,"Computational science is increasingly supporting advances in scientific and engineering knowledge. The unique constraints of these types of projects result in a development process that differs from the process more traditional information technology projects use. This article reports the results of the sixth case study conducted under the support of the Darpa High Productivity Computing Systems Program. The case study aimed to investigate the technical challenges of code development in this environment, understand the use of development tools, and document the findings as concrete lessons learned for other developers' benefit. The project studied here is a major component of a weather forecasting system of systems. It includes complex behavior and interaction of several individual physical systems (such as the atmosphere and the ocean). This article describes the development of the code and presents important lessons learned.",
Consensus control of observer-based multi-agent system with Communication Delay,"This paper proposes an observer-based consensus control strategy for multi-agent system (MAS) with communication time delay. The condition of stability for MIMO agents is derived by Lyapunov theorem. It gives systematic design procedure under assumed unidirectional network. Furthermore, new consensus control law using observers is proposed for the networked MAS with communication delays. Experimental results show effectiveness of our proposed output consensus approaches.","stability,
control system synthesis,
MIMO systems,
observers"
A Novel Image Text Extraction Method Based on K-Means Clustering,"Texts in web pages, images and videos contain important clues for information indexing and retrieval. Most existing text extraction methods depend on the language type and text appearance. In this paper, a novel and universal method of image text extraction is proposed. A coarse-to-fine text location method is implemented. Firstly, a multi-scale approach is adopted to locate texts with different font sizes. Secondly, projection profiles are used in location refinement step. Color-based k-means clustering is adopted in text segmentation. Compared to grayscale image which is used in most existing methods, color image is more suitable for segmentation based on clustering. It treats corner-points, edge-points and other points equally so that it solves the problem of handling multilingual text. It is demonstrated in experimental results that best performance is obtained when k is 3. Comparative experimental results on a large number of images show that our method is accurate and robust in various conditions.","Data mining,
Image segmentation,
Web pages,
Videos,
Indexing,
Information retrieval,
Image retrieval,
Gray-scale,
Color,
Robustness"
Federation: Repurposing scalar cores for out-of-order instruction issue,"Future SoCs will contain multiple cores. For workloads with significant parallelism, prior work has shown the benefit of many small, multi-threaded, scalar cores. For workloads that require better single-thread performance, a dedicated, larger core can help but comes at a large opportunity cost in the number of scalar cores that could be provisioned instead. This paper proposes a way to repurpose a pair of scalar cores into a 2-way out-of-order issue core with minimal area overhead. ""Federating"" scalar cores in this way nevertheless achieves comparable performance to a dedicated out-of-order core and dissipates less power as well.","Out of order,
Costs,
Hardware,
Throughput,
VLIW,
Yarn,
Aggregates,
Permission,
Instruction sets,
Computer science"
Quickest spectrum sensing in cognitive radio,Quickest detection is applied to frequency spectrum sensing in cognitive radio systems. Distribution change in frequency domain is detected for vacating secondary radio networks from licensed frequency band. A successive refinement based quickest detection is proposed to tackle the problem of unknown parameters of primary radio signal. Cooperative quickest detection is used to enhance the performance of quickest spectrum sensing in secondary radio systems without data fusion centers. Performance is evaluated using theoretical analysis and numerical simulations.,"Cognitive radio,
Testing,
Performance analysis,
Monitoring,
Frequency domain analysis,
Digital TV,
RF signals,
Radio network,
Numerical simulation,
Detection algorithms"
Defending DoS Attacks on Broadcast Authentication in Wireless Sensor Networks,"Security is critical for wireless sensor networks deployed in military, homeland security and other hostile environments. In this paper, we study a security issue related with broadcast in sensor networks. Due to the broadcast nature of wireless communications, often it is more efficient to broadcast packets to sensor nodes. Typically, broadcast authentication is achieved by digital signatures. Since digital signature operations are expensive for small sensor nodes, an attacker can launch a serious denial of service (DoS) attack. That is, an attacker may forge a large number of broadcast messages with digital signatures, and then force sensor nodes to verify these signatures, which can cause them run out of power. In this paper, we present an effective and efficient scheme that can defend such DoS attack on broadcast authentication. Our performance evaluation shows that the scheme is much more secure and efficient than an existing scheme.","Computer crime,
Broadcasting,
Authentication,
Wireless sensor networks,
Digital signatures,
National security,
Elliptic curve cryptography,
Peer to peer computing,
Computer science,
USA Councils"
A Novel Cryptosystem Based on Iris Key Generation,"Biometric cryptography is a technique using biometric features to encrypt data, which can improve the security of the encrypted data and overcome the shortcomings of the traditional cryptography. This paper proposes a novel biometric cryptosystem based on the most accurate biometric feature -- iris. In encryption phase, a quantified 256-dimension textural feature vector is firstly extracted from the preprocessed iris image using a set of 2-D Gabor filters. At the same time, an error-correct-code (ECC) is generated using Reed-Solomon algorithm. Then the feature vector is translated to a cipher key using Hash function. Some general encryption algorithms use this cipher key to encrypt the secret information. In decryption phase, a feature vector extracted from the input iris is firstly corrected using the ECC. Then it is translated to the cipher key using the same Hash function. Finally, the corresponding general decryption algorithms use the key to decrypt the information. Experimental results demonstrate the feasibility of the proposed system.","Iris,
Biometrics,
Elliptic curve cryptography,
Reed-Solomon codes,
Feature extraction,
Information security,
Data mining,
Error correction codes,
Computer science,
Computer security"
A factorization approach for cone-beam reconstruction on a circular short-scan,"In this paper, we introduce a new algorithm for 3-D image reconstruction from cone-beam (CB) projections acquired along a partial circular scan. Our algorithm is based on a novel, exact factorization of the initial 3-D reconstruction problem into a set of independent 2-D inversion problems, each of which corresponds to finding the object density on one, single plane. Any such 2-D inversion problem is solved numerically using a projected steepest descent iteration scheme. We present a numerical evaluation of our factorization algorithm using computer-simulated CB data, without and with noise, of the FORBILD head phantom and of a disk phantom. First, we study quantitatively the impact of the reconstruction parameters on the algorithm performance. Next, we present reconstruction results for visual assessment of the achievable image quality and provide, for comparison, results obtained with two other state-of-the-art reconstruction algorithms for the circular short-scan.","Image reconstruction,
X-ray imaging,
Imaging phantoms,
Image quality,
Reconstruction algorithms,
Computed tomography,
Data acquisition,
Geometry,
Medical services,
Radiology"
Flexible capacitive sensors for high resolution pressure measurement,"Thin, flexible, robust capacitive pressure sensors have been the subject of research in many fields where axial strain sensing with high spatial resolution and pressure resolution is desirable for small loads, such as tactile robotics and biomechanics. Simple capacitive pressure sensors have been designed and implemented on flexible substrates in general agreement with performance predicted by an analytical model. Two designs are demonstrated for comparison. The first design uses standard flex circuit technology, and the second design uses photolithography techniques to fabricate capacitive sensors with higher spatial and higher pressure resolution. Sensor arrays of varying sensor size and spacing are tested with applied loads from 0 to 1 MPa. Pressure resolution and linearity of the sensors are significantly improved with the miniaturized, custom fabricated sensor array compared to standard flexible circuit technology.","Capacitive sensors,
Pressure measurement,
Sensor arrays,
Spatial resolution,
Biosensors,
Robustness,
Tactile sensors,
Robot sensing systems,
Biomechanics,
Analytical models"
Green Transistor - A VDD Scaling Path for Future Low Power ICs,IC power consumption is not only a package thermal issue but also a significant and fast growing part of the world electricity consumption. A new low voltage transistor could contribute greatly to the need for a new Vdd scaling scenario. Green transistor (gFET) is based on tunneling and provides Ion and Ioff far superior to MOSFET at 0.2V if suitable low-Eg material is introduced into IC manufacturing.,"Energy consumption,
MOSFET circuits,
Semiconductor materials,
Photonic band gap,
Tunneling,
Voltage,
Packaging,
Thermal management,
Effective mass,
Inverters"
Visualization of Tree-Structured Data Through Generative Topographic Mapping,"In this paper, we present a probabilistic generative approach for constructing topographic maps of tree-structured data. Our model defines a low-dimensional manifold of local noise models, namely, (hidden) Markov tree models, induced by a smooth mapping from low-dimensional latent space. We contrast our approach with that of topographic map formation using recursive neural-based techniques, namely, the self-organizing map for structured data (SOMSD) (Hagenbuchner et al., 2003). The probabilistic nature of our model brings a number of benefits: (1) naturally defined cost function that drives the model optimization; (2) principled model comparison and testing for overfitting; (3) a potential for transparent interpretation of the map by inspecting the underlying local noise models; (4) natural accommodation of alternative local noise models implicitly expressing different notions of structured data similarity. Furthermore, in contrast with the recursive neural-based approaches, the smooth nature of the mapping from the latent space to the local model space allows for calculation of magnification factors-a useful tool for the detection of data clusters. We demonstrate our approach on three data sets: a toy data set, an artificially generated data set, and on a data set of images represented as quadtrees.","Data visualization,
Neurons,
Hidden Markov models,
Lattices,
Gaussian processes,
Cost function,
Computer science,
Training data,
Testing,
Neural networks"
Local properties at interfaces in nanodielectrics: An ab initio computational study,"First-principles computational methodologies are presented to study the impact of surfaces and interfaces on the dielectric and electronic properties of emerging technologically important systems over length scales of the order of inter-atomic distances. The variation of dielectric constant across Si-SiO2, Si-HfO2 and SiO2-polymer interfaces has been correlated to interfacial chemical bonding environments, using the theory of the local dielectric permittivity. The local electronic structure variation across Si-HfO2 and SiO2-polymer interfaces, including band bending, band offsets and the creation of interfacial trap states have been investigated using a layer-decomposed density of states analysis. These computational methods form the groundwork for a more thorough analysis of the impact of surfaces, interfaces, and atomic level defects on dielectric and electronic properties of a wide variety of nano-structured systems.","Computer interfaces,
Dielectric materials,
Polymers,
Permittivity,
Polarization,
Density functional theory,
Nanostructured materials,
High-K gate dielectrics,
Materials science and technology,
Chemical technology"
A new silane-ammonia surface passivation technology for realizing inversion-type surface-channel GaAs N-MOSFET with 160 nm gate length and high-quality metal-gate/high-k dielectric stack,"We report a novel surface passivation technology employing a silane-ammonia gas mixture to realize very high quality high-k gate dielectric on GaAs. This technology eliminates the poor quality native oxide while forming an ultrathin silicon oxynitride (SiOxNy) interfacial passivation layer between the high-k dielectric and the GaAs surface. Interface state density Dit of about 1 × 1011 eV-1cm-2 was achieved, which is the lowest reported value for a high-k dielectric formed on GaAs by CVD, ALD, or PVD techniques. This enables the formation of high quality gate stack on GaAs for high performance CMOS applications. We also realized the smallest reported (160 nm gate length) inversion-type enhancement-mode surface channel GaAs MOSFET. The surface-channel GaAs MOSFETs in this work has demonstrated one of the highest peak electron mobility of ~2100 cm2/V·s. The lowest reported subthreshold swing (~100 mV/decade) for surface-channel GaAs MOSFETs was also achieved for devices with longer gate length. Extensive bias-temperature instability (BTI) characterization was performed to evaluate the reliability of the gate stack.","Passivation,
Gallium arsenide,
MOSFET circuits,
High-K gate dielectrics,
Fabrication,
III-V semiconductor materials,
Vacuum systems,
Surface cleaning,
Dielectric substrates,
CMOS technology"
Graph commute times for image representation,"We introduce a new image representation that encompasses both the general layout of groups of quantized local invariant descriptors as well as their relative frequency. A graph of interest points clusters is constructed and we use the matrix of commute times between the different nodes of the graph to obtain a description of their relative arrangement that is robust to large intra class variation. The obtained high dimensional representation is then embedded in a space of lower dimension by exploiting the spectral properties of the graph made of the different images. Classification tasks can be performed in this embedding space. We expose classification and labelling results obtained on three different datasets, including the challenging PASCAL VOC2007 dataset. The performances of our approach compare favorably with the standard bag of features, which is a particular case of our representation.","Image representation,
Labeling,
Frequency,
Robustness,
Computer vision,
Laplace equations,
Automation,
Shape,
Application software,
Graph theory"
Sketching and Streaming Entropy via Approximation Theory,"We give near-optimal sketching and streaming algorithms for estimating Shannon entropy in the most general streaming model, with arbitrary insertions and deletions. This improves on prior results that obtain suboptimal space bounds in the general model, and near-optimal bounds in the insertion-only model without sketching. Our high-level approach is simple: we give algorithms to estimate Tsallis entropy, and use them to extrapolate an estimate of Shannon entropy. The accuracy of our estimates is proven using approximation theory arguments and extremal properties of Chebyshev polynomials. Our work also yields the best-known and near-optimal additive approximations for entropy, and hence also for conditional entropy and mutual information.","Entropy,
Approximation methods,
Frequency estimation,
Approximation algorithms,
Chebyshev approximation,
Computer science,
Polynomials,
Internet,
Additives,
Mutual information"
Towards Measuring Knowledge Management Success,"Discussions at previous HICSS conferences have revealed that there is no general agreement on definitions of knowledge management (KM) and knowledge management system (KMS) success. We developed these concepts and presented them earlier this year. Using an expert panel approach followed by two exploratory surveys, we identify KM success measures. The research demonstrates that measures for KM success are required on multiple dimensions. This paper thus also presents a set of dimensions with measures that can be used to determine if KM in an organization is successful.","Knowledge management,
Management training,
Resource management,
Instruments,
Databases,
Computer networks,
Computer network management,
Multidimensional systems,
Visualization,
Process design"
A low cost quadratic level ECG compression algorithm and its hardware optimization for body sensor network system,"A low cost quadratic level compression algorithm is proposed for body sensor network system. The proposed algorithm reduces the encoding delay and the hardware cost, while maintaining the reconstructed signal quality. The quadratic compression level determined by the mean deviation value is used to preserve the critical information with high compression ratio. The overall CR is 8.4:1, the PRD is 0.897% and the encoding rate is 6.4Mbps. The 16-bit sensor node processor is designed, which supports the proposed compression algorithm. The processor consumes 0.56nJ/bit at 1V supply voltage with 1MHz operating frequency in 0.25-μm CMOS process.",
Towards combining UAV and sensor operator roles in UAV-enabled visual search,"Wilderness search and rescue (WiSAR) is a challenging problem because of the large areas and often rough terrain that must be searched. Using mini-UAVs to deliver aerial video to searchers has potential to support WiSAR efforts, but a number of technology and human factors problems must be overcome to make this practical. At the source of many of these problems is a desire to manage the UAV using as few people as possible, so that more people can be used in ground-based search efforts. This paper uses observations from two informal studies and one formal experiment to identify what human operators may be unaware of as a function of autonomy and information display. Results suggest that progress is being made on designing autonomy and information displays that may make it possible for a single human to simultaneously manage the UAV and its camera in WiSAR, but that adaptable displays that support systematic navigation are probably needed.","Cameras,
Humans,
Robot sensing systems,
Search problems,
Monitoring,
Navigation,
Mice"
Feature analysis and selection for acoustic event detection,"Speech perceptual features, such as Mel-frequency Cepstral Coefficients (MFCC), have been widely used in acoustic event detection. However, the different spectral structures between speech and acoustic events degrade the performance of the speech feature sets. We propose quantifying the discriminative capability of each feature component according to the approximated Bayesian accuracy and deriving a discriminative feature set for acoustic event detection. Compared to MFCC, feature sets derived using the proposed approaches achieve about 30% relative accuracy improvement in acoustic event detection.","Event detection,
Speech analysis,
Cepstral analysis,
Mel frequency cepstral coefficient,
Bayesian methods,
Speech enhancement,
Signal to noise ratio,
Decorrelation,
Acoustical engineering,
Computer vision"
Multi-element Free-Space-Optical spherical structures with intermittent connectivity patterns,"Due to its high bandwidth spectrum, Free-Space-Optical (FSO) communication has the potential to bridge the capacity gap between backbone fiber links and mobile ad-hoc links, especially in the last-mile. Though FSO can solve the wireless capacity problem, it brings new challenges, like frequent disruption of physical link (intermittent connectivity) and the line of sight (LOS) requirements. In this paper, we study a spherical FSO structure as a basic building block and examine the effects of such FSO structures to upper layers, especially to TCP behavior for stationary and mobile nodes.","Transceivers,
Mobile communication,
Light emitting diodes,
Optical transmitters,
Optical receivers,
Radio frequency,
Laser beams,
Ad hoc networks,
Internet,
Space technology"
Stereo depth with a Unified Architecture GPU,This paper describes how the calculation of depth from stereo images was accelerated using a GPU. The Compute Unified Device Architecture (CUDA) from NVIDIA was employed in novel ways to compute depth using BT cost matching and the Semi-Global Matching algorithm. The challenges of mapping a sequential algorithm to a massively parallel thread environment and performance optimization techniques are considered.,"Costs,
Acceleration,
Feedback,
Computer architecture,
Graphics,
Taxonomy,
Computer science,
Yarn,
Optimization,
Pixel"
Laser RIN and linewidth requirements for direct detection optical OFDM,"We identify experimentally the effects of laser linewidth and intensity noise on optical OFDM systems, and show that commercial DFB lasers are suitable transmitters even when operated at low powers.","Optical detectors,
OFDM,
Optical receivers,
Optical noise,
Optical modulation,
Power lasers,
Fiber lasers,
Laser noise,
Optical transmitters,
Quadrature phase shift keying"
Histogram-based search: A comparative study,"Histograms represent a popular means for feature representation. This paper is concerned with the problem of exhaustive histogram-based image search. Several standard histogram construction methods are explored, including the conventional approach, Huang’s method, and the state-of-the-art integral histogram. In addition, we present a novel multiscale histogram-based search algorithm, termed the distributive histogram, that can be evaluated exhaustively in a fast and memory efficient manner. An extensive systematic empirical evaluation is presented that explores the computational and storage consequences of altering the search image and histogram bin sizes. Experiments reveal up to an eight-fold decrease in computation time and hundreds- to thousands-fold decrease of memory use of the proposed distributive histogram in comparison to the integral histogram. Finally, we conclude with a discussion on the relative merits between the various approaches considered in the paper.","Histograms,
Image storage,
Concurrent computing,
Filtering,
Hardware,
Object detection,
Computer science,
Distributed computing,
Information technology,
Paper technology"
Asymmetric quantum LDPC codes,"Recently, quantum error-correcting codes were proposed that capitalize on the fact that many physical error models lead to a significant asymmetry between the probabilities for bit flip and phase flip errors. An example for a channel which exhibits such asymmetry is the combined amplitude damping and dephasing channel, where the probabilities of bit flips and phase flips can be related to relaxation and dephasing time, respectively. We give systematic constructions of asymmetric quantum stabilizer codes that exploit this asymmetry. Our approach is based on a CSS construction that combines BCH and finite geometry LDPC codes.","Parity check codes,
Geometry,
Distance measurement,
Damping,
Electronic mail,
Fault tolerance,
Linear code"
Subgradient methods in network resource allocation: Rate analysis,"We consider dual subgradient methods for solving (nonsmooth) convex constrained optimization problems. Our focus is on generating approximate primal solutions with performance guarantees and providing convergence rate analysis. We propose and analyze methods that use averaging schemes to generate approximate primal optimal solutions. We provide estimates on the convergence rate of the generated primal solutions in terms of both the amount of feasibility violation and bounds on the primal function values. The feasibility violation and primal value estimates are given per iteration, thus providing practical stopping criteria. We provide a numerical example that illustrate the performance of the subgradient methods with averaging in a network resource allocation problem.","Resource management,
Constraint optimization,
Convergence,
Computer industry,
Systems engineering and theory,
Performance analysis,
Lagrangian functions,
Large-scale systems,
Optimization methods,
Engineering profession"
Coding Dojo: An Environment for Learning and Sharing Agile Practices,"A Coding Dojo is a meeting where a group of programmers gets together to learn, practice, and share experiences. This report describes the authors' experience of creating and running an active Coding Dojo in São Paulo, Brazil, sharing the lessons learned from the experience. The role of the Dojo in the learning process is discussed, showing how it creates an environment for fostering and sharing Agile practices such as Test-Driven Development, Refactoring and Pair Programming, among others.","Encoding,
Software,
Programming,
Portable computers,
Computer languages,
Algorithm design and analysis,
Problem-solving"
Broadcast gossip algorithms: Design and analysis for consensus,"Motivated by applications to wireless sensor, peer-to-peer, and ad hoc networks, we have recently proposed a broadcasting-based gossiping protocol to compute the (possibly weighted) average of the initial measurements of the nodes at every node in the network. The class of broadcast gossip algorithms achieve consensus almost surely at a value that is in the neighborhood of the initial node measurements’ average. In this paper, we further study the broadcast gossip algorithms: we derive and analyze the optimal mixing parameter of the algorithm when approached from worst-case convergence rate, present theoretical results on limiting mean square error performance of the algorithm, and find the convergence rate order of the proposed protocol.","Algorithm design and analysis,
Broadcasting,
Peer to peer computing,
Wireless sensor networks,
Convergence,
Ad hoc networks,
Wireless application protocol,
Computer networks,
Performance analysis,
Mean square error methods"
In Praise of Scripting: Real Programming Pragmatism,"The academic programming language community continues to reject the change in programming practices brought about by scripting. We need a programming language pragmatics to go past the analysis of syntax and semantics in the same way that linguistics studies perlocution and illocution. Pragmatic questions are not the easiest for mathematically inclined computer scientists to address. They refer by nature to people and their habits, sociology, and the day's technological demands. An industrial psychology literature, apart from computing, has sometimes addressed questions of this kind. But this kind of study must become part of programming language theory within computing. It's the importance of just these kinds of questions that makes programmers choose scripting languages. The author recommends that scripting, not Java, be taught first, asserting that students should learn to love their own possibilities before they learn to loathe other people's restrictions.","Java,
Object oriented programming,
Computer languages,
Databases,
Usability,
Lamps,
Deafness,
Education,
Application software,
Software engineering"
Symbolic mining of temporal specifications,"Program specifications are important in many phases of the software development process, but they are often omitted or incomplete. An important class of specifications takes the form of temporal properties that prescribe proper usage of components of a software system. Recent work has focused on the automated inference of temporal specifications from the static or runtime behavior of programs. Many techniques match a specification pattern (represented by a finite state automaton) to all possible combinations of program components and enumerate the possible matches. Such approaches suffer from high space complexity and have not scaled beyond simple, two-letter alternating patterns (e.g. (ab)*). In this paper, we precisely define this form of specification mining and show that its general form is NP-complete. We observe a great deal of regularity in the representation and tracking of all possible combinations of system components. This motivates us to introduce a symbolic algorithm, based on binary decision diagrams (BDDs), that exploits this regularity. Our results show that this symbolic approach expands the tractability of this problem by orders of magnitude in both time and space. It enables us to mine more complex specifications, such as the common three-letter resource acquisition, usage, and release pattern ((ab+c)*). We have implemented our algorithm in a practical tool and used it to find significant specifications in real systems, including Apache Ant and Hibernate. We then used these specifications to find previously unknown bugs.","Learning automata,
Programming,
Pattern matching,
Data structures,
Boolean functions,
Computer bugs,
Computer science,
Software systems,
Runtime,
Algorithm design and analysis"
Efficient peer-to-peer file sharing using network coding in MANET,"Mobile peer-to-peer (P2P) systems have recently got in the limelight of the research community that is striving to build efficient and effective mobile content addressable networks. Along this line of research, we propose a new peer-to-peer file sharing protocol suited to mobile ad hoc networks (MANET). The main ingredients of our protocol are network coding and mobility assisted data propagation, i.e., single-hop communication. We argue that network coding in combination with single-hop communication allows P2P file sharing systems in MANET to operate in a more efficient manner and helps the systems to deal with typical MANET issues such as dynamic topology and intermittent connectivity as well as various other issues that have been disregarded in previous MANET P2P researches such as addressing, node/user density, non-cooperativeness, and unreliable channel. Via simulation, we show that our P2P protocol based on network coding and single-hop communication allows shorter file downloading delays compared to an existing MANET P2P protocol.","Peer to peer computing,
Mobile ad hoc networks,
Network coding,
Protocols,
Encoding,
Vectors,
Delay"
Performance of IEEE 802.15.4 in wireless sensor networks with a mobile sink implementing various mobility strategies,"In this work, we investigate the advantages and challenges of deploying a single mobile sink in IEEE 802.15.4/ZigBee wireless sensor networks (WSNs). The first part of the paper provides an overview of the most recent research on sink mobility in WSNs, placing a special emphasis on different types of sink mobility (random, predictable and controlled) and discussing the application scenarios most suitable for their respective deployment. In the second part of the paper, our OPNET model for simulation of large-scale and ZigBee-based wireless sensor networks is presented. The model enables effective evaluation of random and predictable sink mobility under varying conditions and forms of routing in the underlying ZigBee WSN. The results obtained using this model show that in terms of energy efficiency ZigBee’s tree-based routing outperforms ZigBee’s mesh routing, both in the case of random and predictable sink mobility. At the same time, under both mobility models, tree-based routing generates longer delays in the delivery of data reporting packets. Furthermore, when compared against each other assuming identical network conditions, random mobility is shown to achieve higher energy efficiency and shorter packet delays than predictable mobility.","Wireless sensor networks,
Zigbee,
Mobile communication,
Routing,
Topology,
Sensors,
Network topology"
Research on Path Completion Technique in Web Usage Mining,"An implementation of data preprocessing system for web usage mining and the details of algorithm for path completion are presented. After user session identification, the missing pages in user access paths are appended by using the referer-based method which is an effective solution to the problems introduced by using proxy servers and local caching. The reference length of pages in complete path is modified by considering the average reference length of auxiliary pages which is estimated in advance through the maximal forward references and the reference length algorithms. As verified by practical web access log, the proposed path completion algorithm efficiently appends the lost information and improves the reliability of access data for further web usage mining calculations.","Data preprocessing,
Data mining,
Computer science,
Data engineering,
Cleaning,
Web sites,
Research and development,
Network servers,
Telecommunication traffic,
Impedance"
Gaussian interference networks: Sum capacity in the low interference regime,"Genie based arguments are used to derive new outer bounds on the sum capacity of the two user, symmetric three user, one-to-many and many-to-one Gaussian interference channels. Using these bounds, it is shown that treating interference as noise achieves the sum capacity in a low interference regime, where the interference parameters are below certain thresholds.","Interference,
Interference channels,
Receivers,
Noise,
Signal to noise ratio,
Transmitters,
Random variables"
2D to 3D convertion based on edge defocus and segmentation,"This paper presents a depth estimation method which converts two-dimensional images into three-dimensional data. Based on two-dimensional wavelet analysis of Lipschitz regularity for defocus estimation on edges, this method can effectively eliminate the horizontal stripes in the depth map resulted from traditional one-dimensional wavelet based approaches. Besides, we also propose several techniques such as edge enhancement, color-based segmentation, and depth optimization to obtain a more reliable and smoother depth map. The experimental results demonstrate the effectiveness of our proposed techniques.","Wavelet analysis,
Focusing,
Image analysis,
Image segmentation,
Frequency,
Cameras,
Image converters,
Rendering (computer graphics),
Computer vision,
Stereo vision"
GRAID: A Green RAID Storage Architecture with Improved Energy Efficiency and Reliability,"Existing power-aware optimization schemes for disk-array systems tend to strike a delicate balance between energy consumption and performance while ignoring reliability. To achieve a reasonably good trade-off among these three important design objectives in this paper we introduce an energy efficient disk array architecture, called a Green RAID (or GRAID), which extends the data mirroring redundancy of RAID 10 by incorporating a dedicated log disk. The goal of GRAID is to significantly improve energy efficiency or reliability of existing RAID-based systems without noticeably sacrificing their reliability or energy efficiency. The main idea behind GRAID is to update the mirroring disks only periodically while storing all updates since the last mirror-disk update in a log disk, thus being able to spin down all the mirroring disks (or half of the total disks) most of the time to a lower power mode to save energy without sacrificing reliability. Reliability analysis shows that the reliability of GRAID, in terms of MTTDL (Mean Time To Data Loss), is only slightly worse than RAID 10. On the other hand, our prototype implementation of GRAID and performance evaluation show that GRAID's energy efficiency is significantly better than that of RAID 10 by up to 32.1% and an average of 25.4%.","Energy storage,
Energy efficiency,
Power system reliability,
Costs,
Computer science,
Educational technology,
Redundancy,
Energy consumption,
Cooling,
Computer architecture"
Hierarchical distributed control for search and tracking by heterogeneous aerial robot networks,"This paper presents a hierarchical control architecture that enables cooperative surveillance by a heterogeneous aerial robot network comprised of mothership unmanned aircraft and daughtership micro air vehicles. Combining the endurance, range, and processing capabilities of the motherships with the stealth, flexibility, and maneuverability of swarms of daughterships enables robust control of aerial robot networks conducting collaborative operations. The hierarchical control structure decomposes the system into components that take advantage of the abilities of the different types of vehicles. The motherships act as distributed databases, fusion centers, negotiation agents, and task supervisors while daughtership control is achieved using cooperative vector field tracking. This paper describes the overall arcitecture and then focuses on the assignment and tracking algorithms used once subteams of daughtership vehicles have been deployed. A summary of the communication, command, and control structure of a heterogeneous unmanned aircraft system is also given in this paper along with hardware-in-the-loop and software simulation results verifying several components of the distributed control architecture.","Distributed control,
Robots,
Unmanned aerial vehicles,
Communication system control,
Computer architecture,
Surveillance,
Robust control,
Collaboration,
Control systems,
Distributed databases"
Antenna Packing in Low-Power Systems: Communication Limits and Array Design,"In this correspondence, we study design of transceiver antenna arrays and its impact on spectral efficiency of low-power systems. Our primary motivation is construction of practical and portable multi-antenna configurations with a very small and a-priori fixed volume for placing antennas. Using spectral efficiency as a target metric for array optimization, we show that any array configuration, transmit or receive, can be characterized via a parameter that we interpret as ldquoeffective degrees of freedom.rdquo For any array configuration, effective degrees of freedom describes an equivalent uncorrelated array, which results in the same low-power behavior of spectral efficiency. Joint optimization of transmit and receive antenna configurations decouples into maximizing effective degrees of freedom for transmitter and receiver separately. To achieve this goal, we introduce and study a theoretical benchmark of ldquolimiting degrees of freedom, rdquo which is the least upper bound on effective degrees of freedom, evaluated over all configurations with finite number of antennas. Limiting degrees of freedom therefore describes the best possible performance for any transceiver array which confines its elements inside a given space. We compute a closed-form expression for limiting degrees of freedom of a circular geometry. Finally, we present numerical procedure and examples for designing linear and square arrays with nonuniform spacing, which typically exhibit significant spectral efficiency gains over uniform arrays.","Antenna arrays,
Decoding,
Hamming weight,
Linearity,
Computer science,
Channel capacity"
Marama,"We describe the Marama suite of meta-tools. This Eclipse-based toolset permits rapid specification of notational elements, meta-models, view editors and view-model mappings. It has a novel set of behavioural specification tools for both visual and model level behaviours. An integrated mapping tool provides model transformation and code generation support. The toolset has been applied to several significant application development tasks and has undergone a variety of evaluations.","Connectors,
Model driven engineering,
Shape,
Computer aided software engineering,
Character generation,
Software tools,
DSL,
Context modeling,
Computer science,
Computer architecture"
Coil current analysis method for predictive maintenance of circuit breakers,"This paper presents a method for analyzing the operating performance of a circuit breaker through the trip/close coil current signature. A time-domain based approach is used to detect the critical points in the coil current, which can be used by an intelligent system, human, or machine, to assess and predict the health of the breaker. The proposed algorithm is computationally inexpensive and can be easily embedded in a low cost microprocessor. Experimental data were used to verify the robustness of the algorithm. The method provides a practical extremum detection technique in a noisy environment, which can be extended for other digital signal processing applications in computer science and biomedical engineering.","Coils,
Predictive maintenance,
Circuit breakers,
Signal processing algorithms,
Performance analysis,
Time domain analysis,
Intelligent systems,
Computational intelligence,
Machine intelligence,
Humans"
iWalker: Toward a Rollator-Mounted Wayfinding System for the Elderly,"Research on intelligent walkers aims at helping elderly individuals to maintain their independence in familiar and unfamiliar environments. Several walkers have been developed by researchers at Carnegie Mellon University and the University of Pittsburgh. This article contributes to this research venue by describing the design and initial evaluations of iWalker, a multi-sensor rollator-mounted wayfinding system for the elderly. The primary difference of the proposed navigation aid from other intelligent walkers is that iWalker is assumed to operate in a smart world (SW), a physical space equipped with embedded sensors. By integrating inexpensive sensors into the environment, the cost and complexity of the walker can be reduced.",
ADMAD: Application-Driven Metadata Aware De-duplication Archival Storage System,"There is a huge amount of duplicated or redundant data in current storage systems. So Data De-duplication, which uses lossless data compression schemes to minimize the duplicated data at the inter-file level, has been receiving broad attention in recent years. But there are still research challenges in current approaches and storage systems, such as: how to chunking the files more efficiently and better leverage potential similarity and identity among dedicated applications; how to store the chunks effectively and reliably into secondary storage devices. In this paper, we propose ADMAD: an Application-Driven Metadata Aware De-duplication Archival Storage System, which makes use of certain meta-data information of different levels in the I/O path to direct the file partitioning into more Meaningful data Chunks (MC) to maximally reduce the inter-file level duplications. However, the chunks may be with different lengths and variable sizes, storing them into storage devices may result in a lot of fragments and involve a high percentage of random disk accesses, which is very inefficient. Therefore, in ADMAD, chunks are further packaged into fixed sized Objects as the storage units to speed up the I/O performance as well as to ease the data management. Preliminary experiments have demonstrated that the proposed system can further reduce the required storage space when compared with current methods (from 20% to near 50% according to several datasets), and largely improves the writing performance (about 50%-70% in average).","Fingerprint recognition,
Application software,
Computer science,
Data compression,
Network servers,
Conferences,
Computer architecture,
Operating systems,
Data engineering,
USA Councils"
Computing Models for FPGA-Based Accelerators,"Field-programmable gate arrays are widely considered accelerators for compute-intensive applications. A critical phase of FPGA application development is finding and mapping to the appropriate computing model. These models differ from models generally used in programming. For example, whereas parallel computing models are often based on thread execution and interaction, FPGA computing can exploit more degrees of freedom than are available in software. This enables models with highly flexible fine-grained parallelism and associative operations such as broadcast and collective response. Several case studies demonstrate the effectiveness of using FPGA-based accelerators in molecular modeling.","Acceleration,
Field programmable gate arrays,
Application software,
Computer architecture,
Parallel processing,
Hardware,
Concurrent computing,
Broadcasting,
Microprocessors,
Programmable logic arrays"
Asymmetric cooperation among wireless relays with linear precoding,"Wireless relays extend coverage, improve spectral efficiency, and enhance reliability and rates of wireless cellular communication systems. In this work, we introduce the fundamental notion of asymmetric cooperation among cooperating relays in cellular downlinks - different relays are party to different but overlapping knowledge about the messages transmitted from the base station. We argue that asymmetric cooperation arises naturally in most two-phase protocols in which the base station first transmits information to multiple relays that then cooperatively forward the information to the recipient mobile stations in the cell. For a system in which two relays are of the decode-and-forward type and cooperate using linear precoding to communicate with two mobile stations, we formulate the general, but complicated, throughput optimization problem and derive several results that considerably simplify the optimization. We show that under different channel configurations and fairness criteria, asymmetric cooperation is often the throughput-maximizing option. Under typical configurations, a 20-30% throughput enhancement is achieved compared to conventional full-cooperation systems.","Relays,
Downlink,
Base stations,
Throughput,
Fading,
Ad hoc networks,
Transmitters,
Wireless communication,
Protocols,
Decoding"
GPU implementation of belief propagation using CUDA for cloud tracking and reconstruction,This paper describes an efficient CUDA-based GPU implementation of the belief propagation algorithm that can be used to speed up stereo image processing and motion tracking calculations without loss of accuracy. Preliminary results in using belief propagation to analyze satellite images of Hurricane Luis for real-time cloud structure and tracking are promising with speed-ups of nearly a factor of five.,"Belief propagation,
Costs,
Stereo vision,
Tracking,
Cloud computing,
Graphics,
Scattering,
Inference algorithms,
Image reconstruction,
Computer science"
A study of a new misclassification measure for minimum classification error training of prototype-based pattern classifiers,"In this paper, we revisit the formulation of minimum classification error (MCE) training and propose a sample separation margin (SSM) based misclassification measure for MCE training of multiple-prototype-based pattern classifiers. Comparative experiments are conducted on the task of the recognition of isolated online handwritten Japanese Kanji characters using Nakayosi and Kuchibue databases. Experimental results demonstrate that MCE training with the new misclassification measure achieves significant character recognition error rate reduction compared with MCE training using two traditional misclassification measures.","Prototypes,
Computer errors,
Character recognition,
Helium,
Computer science,
Asia,
Handwriting recognition,
Databases,
Error analysis,
Pattern classification"
Automatic breast cancer grading of histopathological images,"Breast cancer grading of histopathological images is the standard clinical practice for the diagnosis and prognosis of breast cancer development. In a large hospital, a pathologist typically handles 100 grading cases per day, each consisting of about 2000 image frames. It is, therefore, a very tedious and time-consuming task. This paper proposes a method for automatic computer grading to assist pathologists by providing second opinions and reducing their workload. It combines the three criteria in the Nottingham scoring system using a multi-resolution approach. To our best knowledge, there is no existing work that provide complete grading according to the Nottingham criteria.",
"Can feedback, cooperation, relays and full duplex operation increase the degrees of freedom of wireless networks?","We consider a fully connected network with S full duplex source nodes, D full duplex destination nodes and R relay nodes, perfect feedback to source and relay nodes, and noisy cooperation between all source, relay and destination nodes. We show that this network has SD/S+D−1 degrees of freedom if the channel gains are time-varying/frequency selective. The implication of the result is that, the techniques mentioned in the title (i.e relays etc.) can affect the capacity of a network only up to a o(log(SNR)) term and therefore cannot improve the degrees of freedom of a network. Certain communication scenarios excluded by our system model where these techniques improve the degrees of freedom are also identified. Bounds on the degrees of freedom of a fully connected K node network emerge as a by-product of our study.","Relays,
Interference,
Wireless networks,
Gain,
AWGN,
Noise measurement,
Signal to noise ratio"
A Hierarchical Multiprocessor Bandwidth Reservation Scheme with Timing Guarantees,"A multiprocessor scheduling scheme is presented forsupporting hierarchical containers that encapsulate spo-radic soft and hard real-time tasks. In this scheme, eachcontainer is allocated a specified bandwidth, which it usesto schedule its children (some of which may also be con-tainers). This scheme is novel in that, with only soft real-time tasks, no utilization loss is incurred when provisioningcontainers, even in arbitrarily deep hierarchies. Presentedexperiments show that the proposed scheme performs wellcompared to conventional real-time scheduling techniquesthat do not provide container isolation.","real-time systems,
bandwidth allocation,
processor scheduling"
Development of New Hole-Type Avalanche Detectors and the First Results of Their Applications,"We have developed a new detector of photons and charged particles-a hole-type structure with electrodes made of a double layered resistive material: a thin low resistive layer coated with a layer having much higher resistivity. One of the unique features of this detector is its capability to operate at high gas gains (up to ) in air or in gas mixtures with air. They can also operate in a cascaded mode or be combined with other detectors, for example with GEM. This opens new avenues in their applications. Several prototypes based on this new detector and oriented to practical applications were developed and successfully tested: a detector of soft X-rays and alpha particles, a flame sensor, a detector of dangerous gases. All of these devices could operate stably even in humid air and/or in dusty conditions. The main advantages of these devices are their simplicity, low cost, and high sensitivity. For example, due to the avalanche multiplication, the detectors of flames and dangerous gases have a sensitivity of 10-100 times higher than commercial devices. We therefore believe that new detectors will have a great future.","X-ray detection,
X-ray detectors,
Fires,
Gases,
Electrodes,
Conductivity,
Computer vision,
Prototypes,
Testing,
Alpha particles"
Development of Waseda flutist robot WF-4RIV: Implementation of auditory feedback system,"Up to now, different kinds of musical performance robots (MPRs) and robotic musicians (RMs) have been developed. MPRs are designed to closely reproduce the motor skills displayed by humans in order to play musical instruments. From this approach, MPRs are used as benchmarks to study the human motor control from an engineering point of view and to understand better the human-robot interaction from a musical point of view. In contrast, RMs are conceived as automated mechanisms designed to create new ways of musical expression from a musical engineering point of view. Our research, at Waseda University, has been focused on developing an anthropomorphic flutist robot. Our research aims in studying the human motor control from an engineering point of view, understanding the ways to facilitate the human-robot interaction and proposing new applications for humanoid robot in musical terms. As a result of our research, the Waseda Flutist Robot No.4 Refined IV (WF-4RIV) has been developed. In a previous research, we focused on improving the mechanical system in order to enhance the clarity of the sound. However, we require performing further improvements to the control system in order to enable the robot to autonomously improve the quality of the sound during the flute performance. For this purpose, we proposed to implement an auditory feedback control system on the flutist robot. The proposed system is composed by a music expressive generator, feed-forward air pressure control system and a pitch evaluation module. From the experimental results with the WF-4RIV, we could confirm the improvements on the flute performance.",
Control of an experimental mini quad-rotor UAV,"The design and the initial realization of control on an experimental in-door unmanned autonomous quadrotor helicopter is presented. This is a hierarchical embedded model-based control scheme that is built upon the concept of back-stepping, and is applied on an electric motor-driven quadrotor UAV hardware that is equipped with an embedded on-board computer, inertial sensor unit, as well as facilities that make it suitable to be involved in an in-door positioning system, and wireless digital communication network. This realization forms an important step in the development process of a more advanced realization of an UAV suitable for practical applications; it aims clarification of the control principles, acquiring experience in solving control tasks, and getting skills for the development of further realizations.","Automation,
Conferences"
Non-Renegable Selective Acknowledgments (NR-SACKs) for SCTP,"In both TCP and SCTP, selectively acked (SACKed) out-of-order data is implicitly renegable; that is, the receiver can later discard SACKed data. The possibility of reneging forces the transport sender to maintain copies of SACKed data in the send buffer until they are cumulatively acked. In this paper, we investigate the situation where all out-of-order data is non-renegable, such as when the data has been delivered to the application, or when the receiver simply never reneges. Using simulations, we show that SACKs result in inevitable send buffer wastage, which increases as frequency of loss events and loss recovery durations increase. We introduce a fundamentally new ack mechanism, Non-Renegable Selective Acknowledgments (NR-SACKs), for SCTP. Using NR-SACKs, an SCTP receiver can explicitly identify some or all out-of-order data as being non-renegable, allowing the sender to free up send buffer sooner than if the data were only SACKed. Simulation comparisons show that NR-SACKs enable efficient utilization of a transport sender’s memory. Further investigations show that NR-SACKs also improve throughput in Concurrent Multipath Transfer (CMT) [4].","Out of order,
Transport protocols,
Frequency,
Collaboration,
Computational Intelligence Society,
Computer science,
Operating systems,
Discrete event simulation,
Throughput,
Topology"
Hyperbolic Location Fingerprinting: A Calibration-Free Solution for Handling Differences in Signal Strength (concise contribution),"Differences in signal strength among wireless network cards, phones and tags are a fundamental problem for location fingerprinting. Current solutions require manual and error-prone calibration for each new client to address this problem. This paper proposes hyperbolic location fingerprinting, which records fingerprints as signal-strength ratios between pairs of base stations instead of absolute signal-strength values. The proposed solution has been evaluated by extending two well-known location fingerprinting techniques to hyperbolic location fingerprinting. The extended techniques have been tested on ten-hour-long signal-strength traces collected with five different IEEE 802.11 network cards. The evaluation shows that the proposed solution solves the signal-strength difference problem without requiring extra manual calibration and provides a performance equal to that of existing manual solutions.","Fingerprint recognition,
Base stations,
Calibration,
Manuals,
Current measurement,
Pervasive computing,
GSM,
Nearest neighbor searches,
Computer science,
Wireless networks"
Robust speaker identification using auditory features and computational auditory scene analysis,"The performance of speaker recognition systems drop significantly under noisy conditions. To improve robustness, we have recently proposed novel auditory features and a robust speaker recognition system using a front-end based on computational auditory scene analysis. In this paper, we further study the auditory features by exploring different feature dimensions and incorporating dynamic features. In addition, we evaluate the features and robust recognition in a speaker identification task in a number of noisy conditions. We find that one of the auditory features performs substantially better than a conventional speaker feature. Furthermore, our recognition system achieves significant performance improvements compared with an advanced front-end in a wide range of signal-to-noise conditions.","Image analysis,
Speaker recognition,
Noise robustness,
Feature extraction,
Crosstalk,
Cepstral analysis,
Humans,
Mel frequency cepstral coefficient,
Speech analysis,
Signal to noise ratio"
Similar Document Detection with Limited Information Disclosure,"Similar document detection plays important roles in many applications, such as file management, copyright protection, and plagiarism prevention. Existing protocols assume that the contents of files stored on a server (or multiple servers) are directly accessible. This assumption limits more practical applications, e.g., detecting plagiarized documents between two conferences, where submissions are confidential. We propose novel protocols to detect similar documents between two entities where documents cannot be openly shared with each other. We also conduct experiments to show the practical value of the proposed protocols.","Plagiarism,
Access protocols,
File servers,
Privacy,
Copyright protection,
Computer science,
Application software,
Fingerprint recognition,
Information retrieval,
Monitoring"
Broadband variable passive delay elements based on an inductance multiplication technique,"A new technique for making broadband and variable passive delay elements is described. By introducing a variable inductance structure and using it along with available varactors, synthesized transmission lines are implemented with variable delay while maintaining a constant Zo over the line bandwidth. Inductance tuning is realized through the effect of mutual inductance. As a demonstration prototype, a single unit cell and two cascaded unit cells were implemented in 90nm digital CMOS process. Delay values ranging from 14ps – 40ps were obtained from DC to 8GHz while maintaining matched condition over the bandwidth with delay variations of less than ±%5. These delay cells could be used in broadband impulse-based beamforming systems to provide variable delays in each RF path.","Inductance,
Capacitance,
Transmission lines,
Bandwidth,
Inductors,
Varactors,
Capacitors,
Tunable circuits and devices,
Delay lines,
Array signal processing"
Towards a Theory of Robust Localization Against Malicious Beacon Nodes,"Localization in the presence of malicious beacon nodes is an important problem in wireless networks. Although significant progress has been made on this problem, some fundamental theoretical questions still remain unanswered: in the presence of malicious beacon nodes, what are the necessary and sufficient conditions to guarantee a bounded error during 2-dimensional location estimation? Under these necessary and sufficient conditions, what class of localization algorithms can provide that error bound? In this paper, we try to answer these questions. Specifically, we show that, when the number of malicious beacons is greater than or equal to some threshold, there is no localization algorithm that can have a bounded error. Furthermore, when the number of malicious beacons is below that threshold, we identify a class of localization algorithms that can ensure that the localization error is bounded. We also outline two algorithms in this class, one of which is guaranteed to finish in polynomial time (in the number of beacons providing information) in the worst case, while the other is based on a heuristic and is practically efficient. For completeness, we also extend the above results to the 3-dimensional case. Experimental results demonstrate that our solution has very good localization accuracy and computational efficiency.","Robustness,
Peer to peer computing,
Wireless networks,
Sufficient conditions,
Polynomials,
Computational efficiency,
Communications Society,
Computer science,
Broadcasting,
Computational complexity"
Using overlays for efficient data transfer over shared wide-area networks,"Data-intensive applications frequently transfer large amounts of data over wide-area networks. The performance achieved in such settings can often be improved by routing data via intermediate nodes chosen to increase aggregate bandwidth. We explore the benefits of overlay network approaches by designing and implementing a service-oriented architecture that incorporates two key optimizations - multi-hop path splitting andmulti-pathing - within the GridFTP file transfer protocol. We develop a file transfer scheduling algorithm that incorporates the two optimizations in conjunction with the use of available file replicas. The algorithm makes use of information from past GridFTP transfers to estimate network bandwidths and resource availability. The effectiveness of these optimizations is evaluated using several application file transfer patterns: one-to-all broadcast, all-to-one gather, and data redistribution, on a wide-area testbed. The experimental results show that our architecture and algorithm achieve significant performance improvement.","Bandwidth,
Routing,
Aggregates,
Service oriented architecture,
Design optimization,
Spread spectrum communication,
Protocols,
Scheduling algorithm,
Availability,
Broadcasting"
XML processing in DHT networks,"We study the scalable management of XML data in P2P networks based on distributed hash tables (DHTs). We identify performance limitations in this context, and propose an array of techniques to lift them. First, we adapt the DHT platform's index store and communication primitives to the needs of massive data processing. Second, we introduce a distributed hierarchical index and associated efficient algorithms to speed up query processing. Third, we present an innovative, XML-specific flavor of Bloom filters, to reduce data transfers entailed by query processing. Our approach is fully implemented in the KadoP system, used in a real-life software manufacturing application. Our experiments demonstrate the benefits of the proposed techniques.","XML,
Filters,
Peer to peer computing,
Query processing,
Delay,
Sun,
Computer science,
Computer network management,
Context,
Data processing"
Scientific Sketching for Collaborative VR Visualization Design,"We present four studies investigating tools and methodologies for artist-scientist-technologist collaboration in designing multivariate virtual reality (VR) visualizations. Design study 1 identifies the promise of 3D interfaces for rapid VR design and also establishes limitations of the particular tools tested with respect to precision and support for animation. Design study 2 explores animating artist-created visualization designs with scientific 3D fluid flow data. While results captured an accurate sense of flow that was advantageous as compared to the results of study 1, the potential for visual exploration using the design tools tested was limited. Design study 3 reveals the importance of a new 3D interface that overcomes the precision limitation found in study 1 while remaining accessible to artist collaborators. Drawing upon previous results, design study 4 engages collaborative teams in a design process that begins with traditional paper sketching and moves to animated interactive VR prototypes ""sketched"" by designers in VR using interactive 3D tools. Conclusions from these four studies identify important characteristics of effective artist-accessible VR visualization design tools and lead to a proposed formalized methodology for successful collaborative design that we expect to be useful in guiding future collaborations. We call this proposed methodology scientific sketching.","Collaboration,
Virtual reality,
Collaborative work,
Collaborative tools,
Data visualization,
Animation,
Guidelines,
Testing,
Process design,
Design methodology"
An experimental analysis of Zigbee networks,"Zigbee has been touted as a technology that can be embedded in a wide range of products and applications across consumer, commercial, industrial and government markets. However, given the varying requirements for applications in these sectors, we question if Zigbee can really satisfy the needs of these diverse markets. We performed several experiments using commercially available Zigbee software and hardware in order to determine several aspects concerning the reach and limitations of the technology. We analyze the results of our tests and show evidence of where Zigbee can be applied and where it is not suited for.","Zigbee,
Payloads,
Protocols,
Distance measurement,
Topology,
Time factors,
Network topology"
Transforming Distributed Acyclic Systems into Equivalent Uniprocessors under Preemptive and Non-Preemptive Scheduling,"Many scientific disciplines provide composition primitives whereby overall properties of systems are composed from those of their components. Examples include rules for block diagram reduction in control theory and laws for computing equivalent circuit impedance in circuit theory. No general composition rules exist for real-time systems whereby a distributed system is transformed to an equivalent single stage analyzable using traditional uniprocessor schedulability analysis techniques. Towards such a theory, in this paper, we extend our previous result on pipeline delay composition for preemptive and non-preemptive scheduling to the general case of distributed acyclic systems. Acyclic systems are defined as those where the superposition of all task flows gives rise to a Directed Acyclic Graph (DAG). The new extended analysis provides a worst-case bound on the end-to-end delay of a job under both preemptive as well as non-preemptive scheduling, in the distributed system. A simple transformation is then shown of the distributed task system into an equivalent uniprocessor task-set analyzable using traditional uniprocessor schedulability analysis. Hence, using the transformation described in this paper, the wealth of theory available for uniprocessor schedulability analysis can be easily applied to a larger class of distributed systems.","processor scheduling,
directed graphs"
Orthogonal frequency coded SAW sensors and RFID design principles,"Orthogonal frequency coded (OFC) SAW reflectors and transducers have been recently introduced for use in communication, sensor and RFID tag applications.[1,2] The OFC SAW technology approach has been funded by NASA for possible inclusion in ground, space flight and space exploration sensor applications. In general, SAW technology has advantages over possible competing technologies: passive, wireless, radiation hard, operation from cryogenic to furnace temperature ranges, small, rugged, variable frequency and bandwidth operation, encoding and commercially available. SAW sensor embodiments can provide onboard device sensor integration, or can provide integration with an external sensor that uses the SAW device for encoding the sensor information and transmission to the receiver. SAW OFC device technology can provide RFID tags and sensors with low loss, large operating temperatures and a multi-use sensor platform. This paper will discuss the key parameters for OFC device design, which include reflector and transducer design, coding diversity approaches, and insertion loss considerations. Examples of several OFC device sensors and RFID tags will be presented to show the current state-of-the-art performance for several NASA applications, as well as projections for future sensor and RFID tag platform performance.","Sensors,
Transducers,
Temperature sensors,
Surface acoustic waves,
Encoding,
Bandwidth,
Correlation"
High-speed serial-kinematic AFM scanner: Design and drive considerations,"In this paper, we describe the design of a flexure guided, two-axis nanopositioner driven by piezoelectric stack actuators. The scanner is specifically designed for high-speed scanning probe microscope (SPM) applications. A high-speed atomic force microscope (AFM) is required to acquire high resolution, three-dimensional, time-lapse images of fast processes such as the rapid movement of cells and the diffusion of DNA molecules. High-speed scanner designs have been proposed, for example, by Ando and co-workers as well as Schitter and co-workers, for AFM imaging. In the proposed design, the slow and fast scanning axes are serially connected and both axes are flexure-guided to minimize runout. The achievable scan range is 10 × 10 μm. The scanner’s mechanical resonance frequencies were optimized using finite element analysis. Experimental results show a first major resonance, in the slow and fast axis respectively, at approximately 1.5 kHz and 29 kHz. In addition to evaluating the proposed design, this paper also discusses the various tradeoffs between speed, range, and required control hardware. Electrical requirements and scan trajectory design are also considered.","Atomic force microscopy,
Resonance,
Nanopositioning,
Piezoelectric actuators,
Scanning probe microscopy,
Image resolution,
DNA,
High-resolution imaging,
Resonant frequency,
Finite element methods"
Agent-based modeling and simulation: ABMS examples,"Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents. ABMS promises to have far-reaching effects on the way that businesses use computers to support decision-making and researchers use electronic laboratories to support their research. Some have gone so far as to contend that ABMS “is a third way of doing science,” in addition to traditional deductive and inductive reasoning (Axelrod 1997). Computational advances have made possible a growing number of agent-based models across a variety of application domains. Applications range from modeling agent behavior in the stock market, supply chains, and consumer markets, to predicting the spread of epidemics, the threat of bio-warfare, and the factors responsible for the fall of ancient civilizations. This tutorial describes the theoretical and practical foundations of ABMS, identifies toolkits and methods for developing agent models, and illustrates the development of a simple agent-based model.","Decision making,
Computational modeling,
Laboratories,
Artificial intelligence,
Ant colony optimization,
Consumer electronics,
Mobile agents,
Adaptive systems,
Predictive models,
Stock markets"
Statistical Multimode Transmit Antenna Selection for Limited Feedback MIMO Systems,"In a wireless multiple-input multiple-output (MIMO) system, transmit antenna selection is an effective means of achieving good performance with low complexity. We consider spatial multiplexing with linear receivers, and equal power and equal rate allocation over different selected transmit antennas in order to reduce feedback overhead. Under these constraints, we address the problem of statistical multimode transmit antenna subset selection to improve the capacity of spatially correlated MIMO fading channels. In particular, we first derive an analytical closed-form expression for the expectation of the lower bound on the capacity using the smallest eigenvalue distribution of a Wishart matrix. Then, we propose a transmit antenna subset selection criterion of maximizing this average lower-bound capacity.","Transmitting antennas,
Antenna feeds,
Feedback,
MIMO,
Receiving antennas,
Fading,
Transmitters,
Adaptive arrays,
Computer science,
Statistics"
An adaptive learning method for target tracking across multiple cameras,"This paper proposes an adaptive learning method for tracking targets across multiple cameras with disjoint views. Two visual cues are usually employed for tracking targets across cameras: spatio-temporal cue and appearance cue. To learn the relationships among cameras, traditional methods used batch-learning procedures or hand-labeled correspondence, which can work well only within a short period of time. In this paper, we propose an unsupervised method which learns both spatio-temporal relationships and appearance relationships adaptively and can be applied to long-term monitoring. Our method performs target tracking across multiple cameras while also considering the environment changes, such as sudden lighting changes. Also, we improve the estimation of spatio-temporal relationships by using the prior knowledge of camera network topology.","Learning systems,
Target tracking,
Cameras,
Network topology,
Monitoring,
Brightness,
Transfer functions,
Training data,
Computer science,
Information science"
Detecting multiple moving objects in crowded environments with coherent motion regions,"We propose an object detection system that uses the locations of tracked low-level feature points as input, and produces a set of independent coherent motion regions as output. As an object moves, tracked feature points on it span a coherent 3D region in the space-time volume defined by the video. In the case of multi-object motion, many possible coherent motion regions can be constructed around the set of all feature point tracks. Our approach is to identify all possible coherent motion regions, and extract the subset that maximizes an overall likelihood function while assigning each point track to at most one motion region. We solve the problem of finding the best set of coherent motion regions with a simple greedy algorithm, and show that our approach produces semantically correct detections and counts of similar objects moving through crowded scenes.","Object detection,
Motion detection,
Tracking,
Humans,
Shape,
Feature extraction,
Greedy algorithms,
Layout,
Spatiotemporal phenomena,
Assembly"
Learning Respiratory System Function in BME Studies by Means of a Virtual Laboratory: RespiLab,"One of the career areas included in the field of Biomedical Engineering (BME) is the application of engineering system analysis: physiologic modeling, simulation, and control. This paper describes a virtual laboratory and related practical sessions designed for the analysis and the study of the human respiratory system. The laboratory is based on the compilation of several models described in the literature. Presented application has been built using MATLAB/Simulink and Easy Java Simulations (EJS), combining good computation capabilities that are completely interactive. The virtual laboratory is designed in order to understand the operation of the respiratory system under normal conditions and pathological situations, and to predict respiratory variables at different levels of ventilator stimuli and conditions. The presented virtual laboratory has been used and evaluated by students of the Master of Science degree on BME. Experience has shown that this virtual laboratory is a very useful tool for learning the complex response of the human respiratory system.","Mathematical model,
Laboratories,
Lung,
Respiratory system,
Biological system modeling,
Diseases,
Equations"
Discovering human routines from cell phone data with topic models,"We present a framework to automatically discover people's routines from information extracted by cell phones. The framework is built from a probabilistic topic model learned on novel bag type representations of activity-related cues (location, proximity and their temporal variations over a day) of peoples' daily routines. Using real-life data from the Reality Mining dataset, covering 68 000+ hours of human activities, we can successfully discover location-driven (from cell tower connections) and proximity-driven (from Bluetooth information) routines in an unsupervised manner. The resulting topics meaningfully characterize some of the underlying co-occurrence structure of the activities in the dataset, including “going to work early/late”, “being home all day”, “working constantly”, “working sporadically” and “meeting at lunch time”.","Humans,
Cellular phones,
Poles and towers,
Bluetooth,
Data mining,
Training data,
Information retrieval,
Image retrieval,
Genetics,
Histograms"
UDB: Using Directional Beacons for Localization in Underwater Sensor Networks,"Underwater Sensor Networks (UWSN) are widely used in many applications, such as oceanic resource exploration, pollution monitoring, tsunami warnings and mine reconnaissance. In UWSNs, determining the location information of each sensor node is a critical issue, because many services are based on the local-ization results. In this paper, we introduce a novel underwater localization approach based on directional signals, which are transmitted by an Autonomous Underwater Vehicle (AUV). Our method utilizes directional beacons (UDB) to replace traditional omni-directional localization which provides more accurate and efficient ways to locate the sensors themselves by simple calculations. The advantage of this novel scheme is that the communications between AUV and sensors are not necessary because the AUV broadcasts signals and sensors only need to passively listen to the signals. Since the energy consumption for transmissions in underwater environments is a nontrivial factor, our localization scheme not only supports accurate positioning, but also reduces energy consumption of sensors. We evaluate our scheme by simulations. The results show that our new approach is very precise in a strap area. At the same time, we minimize the number of beacons issued from the AUV.","Sensor phenomena and characterization,
Acoustic sensors,
Chemical sensors,
Wireless sensor networks,
Underwater acoustics,
Energy consumption,
Temperature sensors,
Sensor systems and applications,
Computer science,
Underwater vehicles"
Medical image authentication and self-correction through an adaptive reversible watermarking technique,"With the advent of Information Technology in the medical world, various radiological modalities produce a variety of digital medical files most often datasets and images. These files as any digital asset should be protected from unwanted modification of their contents, especially as they contain vital medical information. Thus their protection and authentication seems of great importance and this need will rise along with the future standardization of exchange of data between hospitals or between patients and doctors. Watermarking, a technique first introduced for multimedia files, provides a method for authentication and protection and has been recently applied to medical images. In this paper, we propose a novel watermarking technique where the region of non-interest (RONI) of medical Magnetic Resonance Imaging (MRI) images, is used to embed the region of interest (ROI). In this way, any tampering attempt, not only will be detected, but also the image could be self-restored, back to its previous, “original” form by extracting the ROI from the RONI.","Biomedical imaging,
Authentication,
Watermarking,
Medical diagnostic imaging,
Protection,
Hospitals,
Magnetic resonance imaging,
Information technology,
Standardization,
Medical signal detection"
Depth-of-Field Blur Effects for First-Person Navigation in Virtual Environments,"Depth-of-field blur effects are well-known depth cues in human vision. Computer graphics pipelines added DOF effects early to enhance imagery realism, but real-time VR applications haven't yet introduced visual blur effects. The authors describe new techniques to improve blur rendering and report experimental results from a prototype video game implementation.","Navigation,
Virtual environment,
Humans,
Computer graphics,
Pipelines,
Virtual reality,
Application software,
Rendering (computer graphics),
Prototypes,
Games"
PRISM: Privacy-friendly routing in suspicious MANETs (and VANETs),"Mobile Ad-Hoc Networks (MANETs) are particularly useful and well-suited for critical scenarios, including military, law enforcement as well as emergency rescue and disaster recovery. When operating in hostile or suspicious settings, MANETs require communication security and privacy, especially, in underlying routing protocols. This paper focuses on privacy aspects of mobility. Unlike most networks, where communication is based on long-term identities (addresses), we argue that the location-centric communication paradigm is better-suited for privacy in suspicious MANETs. To this end, we construct an on-demand location-based anonymous MANET routing protocol (PRISM) that achieves privacy and security against both outsider and insider adversaries. We analyze security, privacy and performance of PRISM and compare it to alternative techniques. Results show that PRISM is more computationally efficient and offers better privacy than prior work.","Privacy,
Mobile ad hoc networks,
Peer to peer computing,
Routing protocols,
Ad hoc networks,
Military computing,
Military aircraft,
Law enforcement,
Cryptography,
Tracking"
Time-Scale Modification of Audio Signals Using Enhanced WSOLA With Management of Transients,"In this paper, we present an algorithm for time-scale modification of music signals, based on the waveform similarity overlap-and-add technique (WSOLA). A well-known disadvantage of the standard WSOLA is the uniform time-scaling of the entire signal, including the perceptually significant transient sections (PSTs), where temporal envelope changes as well as significant spectral transitions occur. Time-scaling of PSTs can severely degrade the music quality. We address this problem by detecting the PSTs and leaving them intact, while time-scaling the remainder of the signal, which is relatively steady-state. In the proposed algorithm, the PSTs are detected using a Mel frequency cepstrum nonstationarity measure and the normalized cross-correlation, with time-varying threshold functions. Our study shows that the accurate detection of PSTs within the WSOLA framework makes it possible to achieve a higher quality of time-scaled music, as confirmed by subjective listening tests.","Multiple signal classification,
Music,
Degradation,
Speech,
Computer science,
Frequency,
Cepstrum,
Instruments,
Audio recording,
Time domain analysis"
Provenance in Comparative Analysis: A Study in Cosmology,"Provenance - the logging of information about how data came into being and how it was processed - is an essential aspect of managing large-scale simulation and data-intensive projects. Using a cosmology code comparison project as an example, this article presents how a provenance system can play a key role in such applications.","Large-scale systems,
Computational modeling,
Analytical models,
Databases,
Data analysis,
Information analysis,
Project management,
Predictive models,
Robustness,
Laboratories"
Elliptical local vessel density: A fast and robust quality metric for retinal images,"A great effort of the research community is geared towards the creation of an automatic screening system able to promptly detect diabetic retinopathy with the use of fundus cameras. In addition, there are some documented approaches for automatically judging the image quality. We propose a new set of features independent of field of view or resolution to describe the morphology of the patient's vessels. Our initial results suggest that these features can be used to estimate the image quality in a time one order of magnitude shorter than previous techniques.",
3D Video Quality Evaluation with Depth Quality Variations,"In this paper we present the effect of depth quality on 3D video perception on autostereoscopic displays. This study was done using objective as well as subjective evaluation (using Phillips monitor). The goal of this work was to understand the impact of depth image quality and compression on the perceived 3D experience. Another objective is to evaluate the use of depth image quality as a measure of 3D quality. The experiments were conducted using a multi-view point Philips 3D display that uses a single view and depth image. The displays are tested using subjective evaluation experiments. The results show that depth image can be compressed significantly, to 0.0007 bits per pixel, without affecting the 3D perception significantly. We found that there is a relation between the perception of depth, depth complexity and motion. Motion and complexity of the depth image have a strong influence on the acceptable depth quality in 3D videos. While depth quality gives useful information about 3D perception, additional work is necessary for developing objective metrics based on depth quality.","Three dimensional displays,
Video compression,
Pixel,
Image quality,
Motion pictures,
Image resolution,
Image coding,
Glass,
Rendering (computer graphics),
Quantization"
Indoor Location Estimation Technique using UHF band RFID,"This paper investigates an indoor location estimation system based on UHF band RFID. Tags, separated by 50 cm, are attached to the ceiling and an RFID reader is attached to the person of interest. The location of the person is estimated using the tags' coordinate which are read by the RFID reader. Three simple location estimation algorithms are proposed, and their estimation accuracy is evaluated by an experiment. This paper also presents a ray trace simulation; tag allocation, RFID reader parameters, and location estimation algorithms are evaluated by simulation to minimize the estimation error.","Radiofrequency identification,
Senior citizens,
RFID tags,
Estimation error,
Costs,
Position measurement,
Mobile handsets,
Parameter estimation,
Transmitting antennas,
Antenna radiation patterns"
Understanding social robots: A user study on anthropomorphism,"Anthropomorphism is one of the keys to understand the expectations people have about social robots. In this paper we address the question of how a robotpsilas actions are perceived and represented in a human subject interacting with the robot and how this perception is influenced only by the appearance of the robot. We present results of an interaction-study in which participants had to play a version of the classical Prisonerspsila Dilemma Game (PDG) against four opponents: a human partner (HP), an anthropomorphic robot (AR), a functional robot (FR), and a computer (CP). As the responses of each game partner were randomized unknowingly to the participants, the attribution of intention or will to an opponent (i.e. HP, AR, FR or CP) was based purely on differences in the perception of shape and embodiment. We hypothesize that the degree of human-likeness of the game partner will modulate what the people attribute to the opponents - the more human like the robot looks the more people attribute human-like qualities to the robot.",Robots
Using reinforcement learning for city site selection in the turn-based strategy game Civilization IV,This paper describes the design and implementation of a reinforcement learner based on Q-Learning. This adaptive agent is applied to the city placement selection task in the commercial computer game Civilization IV. The city placement selection determines the founding sites for the cities in this turn-based empire building game from the Civilization series. Our aim is the creation of an adaptive machine learning approach for a task which is originally performed by a complex deterministic script. This machine learning approach results in a more challenging and dynamic computer AI. We present the preliminary findings on the performance of our reinforcement learning approach and we make a comparison between the performance of the adaptive agent and the original static game AI. Both the comparison and the performance measurements show encouraging results. Furthermore the behaviour and performance of the learning algorithm are elaborated and ways of extending our work are discussed.,"Cities and towns,
Artificial intelligence,
Games,
Testing,
Machine learning,
Machine learning algorithms,
Buildings,
Measurement,
Learning systems,
Computer science"
Enhancing Trusted Platform Modules with Hardware-Based Virtualization Techniques,"We present the design of a trusted platform module (TPM) that supports hardware-based virtualization techniques. Our approach enables multiple virtual machines to use the complete power of a hardware TPM by providing for every virtual machine (VM) the illusion that it has its own hardware TPM. For this purpose, we introduce an additional privilege level that is only used by a virtual machine monitor to issue management commands, such as scheduling commands, to the TPM. Based on a TPM Control Structure, we can ensure that state information of a virtual machine's TPM cannot corrupt the TPM state of another VM. Our approach uses recent developments in the virtualization technology of processor architectures.","Software,
Hardware,
Security,
Cryptography,
Chromium,
Registers,
Radiation detectors"
Face Recognition Using Principal Component Analysis and RBF Neural Networks,"In this paper, an efficient method for face recognition using principal component analysis (PCA) and radial basis function (RBF) neural networks is presented. Recently, the PCA has been extensively employed for face recognition algorithms. It is one of the most popular representation methods for a face image. It not only reduces the dimensionality of the image, but also retains some of the variations in the image data. After performing the PCA, the hidden layer neurons of the RBF neural networks have been modelled by considering intra-class discriminating characteristics of the training images. This helps the RBF neural networks to acquire wide variations in the lower-dimensional input space and improves its generalization capabilities. The proposed method has been evaluated using the AT&T (formerly ORL) and UMIST face databases. Experimental results show that the proposed method has encouraging recognition performance.","Training,
Neurons,
Databases,
Face,
Artificial neural networks,
Principal component analysis,
Face recognition"
Multiple-Image Encryption by Rotating Random Grids,"It is the well-known visual secret sharing (VSS) technique that encrypts a secret image into several share images and, later, decrypts the secret by stacking the share images and recognizing by the human visual system. Due to the perfect secrecy, VSS is one of well-candidate for achieving secure e-commerce. Furthermore, the other visual secret sharing technique is constructed by random grids. The main advantages of VSS by adopting random grids compared with VC include no pixel expansion, and no cost of sophisticated codebook design. In this paper, the authors present the new scheme which encrypts two secret images into two random grids without any pixel expansion and, later, decrypts the original secrets by directly stacking two random grids in an additional way of rotating one random grid at 90, 180 or 270 degrees. The proposed scheme not only has no pixel expansion so that the overhead of storage and communication can be reduced but also raises the capacity of secret communication.","Cryptography,
Variable structure systems,
Roentgenium,
Stacking,
Virtual colonoscopy,
Pixel,
Intelligent systems,
Application software,
Computer science,
Design engineering"
Nanophotonic Optical Interconnection Network Architecture for On-Chip and Off-Chip Communications,"An architecture for an integrated low-power, high-bandwidth optical interconnection network based on microring resonator technology is presented. The layout of the non-blocking network is described and a simulation-based performance evaluation is conducted.","Optical interconnections,
Network-on-a-chip,
Optical resonators,
Optical buffering,
Communication switching,
Optical switches,
High speed optical techniques,
Power system interconnection,
Packet switching,
Computer architecture"
B-APT: Bayesian Anti-Phishing Toolbar,"Identity theft is one of the fastest growing crimes in the nation, and phishing has been a primary tool used for this type of theft. In this paper, we present B-APT, a Bayesian anti-phishing toolbar designed to help users identify phishing Websites and protect their sensitive information. Bayesian filters have shown great performance in content-based spam filtering and we adapt a Bayesian filter to detect phishing attacks in the Web browser. The experimental results show that our toolbar effectively detects phishing sites, and is also efficient in terms of page load delay. Among the phishing sites in our testbed, B-APT detected 100% of phishing sites while IE and Firefox only detected 64% and 55%, respectively. Netcraft and SpoofGuard show better accuracy, 98% and 90%, respectively.","Bayesian methods,
Information filtering,
Information filters,
Computer science,
Cities and towns,
Protection,
Uniform resource locators,
Delay effects,
Testing,
Internet"
Group Divisible Codes and Their Application in the Construction of Optimal Constant-Composition Codes of Weight Three,"The concept of group divisible codes, a generalization of group divisible designs with constant block size, is introduced in this paper. This new class of codes is shown to be useful in recursive constructions for constant-weight and constant-composition codes. Large classes of group divisible codes are constructed which enabled the determination of the sizes of optimal constant-composition codes of weight three (and specified distance), leaving only four cases undetermined. Previously, the sizes of constant-composition codes of weight three were known only for those of sufficiently large length.","Feedback,
Binary codes,
Memoryless systems,
Modulation coding,
DNA,
Spread spectrum communication,
Educational programs,
Educational technology,
Mathematics,
Computer science"
Query-Driven Visualization of Time-Varying Adaptive Mesh Refinement Data,"The visualization and analysis of AMR-based simulations is integral to the process of obtaining new insight in scientific research. We present a new method for performing query-driven visualization and analysis on AMR data, with specific emphasis on time-varying AMR data. Our work introduces a new method that directly addresses the dynamic spatial and temporal properties of AMR grids that challenge many existing visualization techniques. Further, we present the first implementation of query-driven visualization on the GPU that uses a GPU-based indexing structure to both answer queries and efficiently utilize GPU memory. We apply our method to two different science domains to demonstrate its broad applicability.","Data visualization,
Adaptive mesh refinement,
Computational modeling,
Biological system modeling,
Analytical models,
Medical simulation,
Rendering (computer graphics),
Data analysis,
Mesh generation,
Grid computing"
A Method of Self-Adaptive Inertia Weight for PSO,"The particle swarm optimization algorithm (PSO) has successfully been applied to many engineering optimization problems. However, the most of existing improved PSO algorithms work well only for small-scale problems on low-dimensional space. In this new self-adaptive PSO, a special function, which is defined in terms of the particle fitness, swarm size and the dimension size of solution space, is introduced to adjust the inertia weight adaptively. In a given generation, the inertia weight for particles with good fitness is decreased to accelerate the convergence rate, whereas the inertia weight for particles with inferior fitness is increased to enhance the global exploration abilities. When the swarm size is large, a smaller inertia weight is utilized to enhance the local search capability for fast convergence rate. If the swarm size is small, a larger inertia weight is employed to improve the global search capability for finding the global optimum. For an optimization problem on multi-dimension complex solution space, a larger inertia weight is employed to strengthen the ability to escape from local optima. In case of small dimension size of solution space, a smaller inertia weight is used for reinforcing the local search capability. This novel self-adaptive PSO can greatly accelerate the convergence rate and improve the capability to reach the global minimum for large-scale problems. Moreover, this new self-adaptive PSO exhibits a consistent methodology: a larger swarm size leads to a better performance.","Acceleration,
Computer science,
Particle swarm optimization,
Large-scale systems,
Equations,
Software engineering,
Power engineering computing,
Microelectronics,
Information technology,
Educational institutions"
The Meaning and Use of the Volume Under a Three-Class ROC Surface (VUS),"Previously, we have proposed a method for three-class receiver operating characteristic (ROC) analysis based on decision theory. In this method, the volume under a three-class ROC surface (VUS) serves as a figure-of-merit (FOM) and measures three-class task performance. The proposed three-class ROC analysis method was demonstrated to be optimal under decision theory according to several decision criteria. Further, an optimal three-class linear observer was proposed to simultaneously maximize the signal-to-noise ratio (SNR) between the test statistics of each pair of the classes provided certain data linearity condition. Applicability of this three-class ROC analysis method would be further enhanced by the development of an intuitive meaning of the VUS and a more general method to calculate the VUS that provides an estimate of its standard error. In this paper, we investigated the general meaning and usage of VUS as a FOM for three-class classification task performance. We showed that the VUS value, which is obtained from a rating procedure, equals the percent correct in a corresponding categorization procedure for continuous rating data. The significance of this relationship goes beyond providing another theoretical basis for three-class ROC analysis - it enables statistical analysis of the VUS value. Based on this relationship, we developed and tested algorithms for calculating the VUS and its variance. Finally, we reviewed the current status of the proposed three-class ROC analysis methodology, and concluded that it extends and unifies decision theoretic, linear discriminant analysis, and psychophysical foundations of binary ROC analysis in a three-class paradigm.","Testing,
Statistical analysis,
Linear discriminant analysis,
Decision theory,
Psychology,
Radiology,
Biomedical imaging,
Medical diagnostic imaging,
Volume measurement"
ETAHM: An energy-aware task allocation algorithm for heterogeneous multiprocessor,"In demand of more computing power and less energy use, multiprocessor with power management facility emerges in embedded system design recently. Dynamic Voltage Scaling is such a facility that varies clock speed and supply voltage to save more energy. In this paper, we propose ETAHM to allocate tasks on a target multiprocessor system. In pursuit of global optimal solution, it mixes task scheduling, mapping and DVS utilization in one phase and couples ant colony optimization algorithm. Extensive experiments show ETAHM could save 22.71% more energy than CASPER [1], a state-of-the-art integrated framework that tackles the identical problem with genetic algorithm instead.","Dynamic voltage scaling,
Embedded computing,
Energy management,
Power system management,
Embedded system,
Clocks,
Multiprocessing systems,
Voltage control,
Ant colony optimization,
Pursuit algorithms"
A Memetic Algorithm for the University Course Timetabling Problem,"The design of course timetables for academic institutions is a very hectic job due to the exponential number of possible feasible timetables with respect to the problem size. This process involves lots of constraints that must be respected and a huge search space to be explored, even if the size of the problem input is not significantly large. On the other hand, the problem itself does not have a widely approved definition, since different institutions face different variations of the problem. This paper presents a memetic algorithm that integrates two local search methods into the genetic algorithm for solving the university course timetabling problem (UCTP). These two local search methods use their exploitive search ability to improve the explorative search ability of genetic algorithms. The experimental results indicate that the proposed memetic algorithm is efficient for solving the UCTP.","Search methods,
Genetic algorithms,
Computer science,
Space exploration,
Artificial intelligence,
Job design,
Algorithm design and analysis,
Polynomials,
NP-hard problem,
Operations research"
A limited feedback precoding system with hierarchical codebook and linear receiver,"In this paper, the conventional Grassmannian codebook for precoding is first analyzed, showing that the performance loss caused by linear receivers was not taken into account. To tackle the performance loss issue, a novel hierarchical codebook consisting of a Grassmannian subcodebook and a perturbation subcodebook is then proposed for precoding systems with linear receivers. A two-step codeword selection scheme that uses the product of two codewords selected from the subcodebooks as the precoder is also presented. Our analysis shows that the perturbation subcodebook is able to compensate for the performance loss from linear receivers. Compared with the Grassmannian codebook, the superiority of the proposed codebook in terms of search complexity as well as throughput/ BER is further confirmed by computer simulations.","Performance loss,
Performance analysis,
Transmitters,
Throughput,
Bit error rate,
MIMO,
Computer simulation,
Vectors,
State feedback,
Channel state information"
Orchestrating Data-Centric Workflows,"When orchestrating data-centric workflows as are commonly found in the sciences, centralised servers can become a bottleneck to the performance of a workflow; output from service invocations are normally transferred via a centralised orchestration engine, when they should be passed directly to where they are needed at the next service in the workflow. To address this performance bottleneck, this paper presents a lightweight hybrid workflow architecture and concrete API, based on a centralised control flow, distributed data flow model. Our architecture maintains the robustness and simplicity of centralised orchestration, but facilitates choreography by allowing services to exchange data directly with one another, reducing data that needs to be transferred through a centralised server.  Furthermore our architecture is standards compliment, flexible and is a non-disruptive solution; service definitions do not have to be altered prior to enactment.","Web services,
Centralized control,
Engines,
Service oriented architecture,
Collaboration,
Data visualization,
Concrete,
Distributed control,
Computer architecture,
Visual databases"
A practical voltage-stability-constrained optimal power flow,"This paper proposes a novel and practical method to enforce a voltage stability constraint (VSC) in an Optimal Power Flow (OPF) auction model, based on the minimum singular value and minimum singular vectors of the power flow Jacobian. The proposed technique is based on a singular value decomposition of the power flow Jacobian at a given solution point, plus an iterative process to satisfy the VSC. A small, but realistic, 6-bus test system is used to analyze the performance of the proposed technique, and to compare it with a “standard” Security-Constrained OPF. The results demonstrate the advantages and practical feasibility of the proposed VSC-OPF technique.","Load flow,
Jacobian matrices,
Stability analysis,
Stability criteria,
Security,
Power system stability,
Matrix decomposition"
Scalable adaptive mantle convection simulation on petascale supercomputers,"Mantle convection is the principal control on the thermal and geological evolution of the Earth. Mantle convection modeling involves solution of the mass, momentum, and energy equations for a viscous, creeping, incompressible non-Newtonian fluid at high Rayleigh and Peclet numbers. Our goal is to conduct global mantle convection simulations that can resolve faulted plate boundaries, down to 1 km scales. However, uniform resolution at these scales would result in meshes with a trillion elements, which would elude even sustained petaflops supercomputers. Thus parallel adaptive mesh refinement and coarsening (AMR) is essential. We present RHEA, a new generation mantle convection code designed to scale to hundreds of thousands of cores. RHEA is built on ALPS, a parallel octree-based adaptive mesh finite element library that provides new distributed data structures and parallel algorithms for dynamic coarsening, refinement, rebalancing, and repartitioning of the mesh. ALPS currently supports low order continuous Lagrange elements, and arbitrary order discontinuous Galerkin spectral elements, on octree meshes. A forest-of-octrees implementation permits nearly arbitrary geometries to be accommodated. Using TACC's 579 teraflops Ranger supercomputer, we demonstrate excellent weak and strong scalability of parallel AMR on up to 62,464 cores for problems with up to 12.4 billion elements. With RHEA's adaptive capabilities, we have been able to reduce the number of elements by over three orders of magnitude, thus enabling us to simulate large-scale mantle convection with finest local resolution of 1.5 km.","Supercomputers,
Adaptive mesh refinement,
Energy resolution,
Geology,
Earth,
Equations,
Finite element methods,
Libraries,
Data structures,
Parallel algorithms"
Haptic Identification of Stiffness and Force Magnitude,"As haptics becomes an integral component of scientific data visualization systems, there is a growing need to study ""haptic glyphs"" (building blocks for displaying information through the sense of touch) and quantify their information transmission capability. The present study investigated the channel capacity for transmitting information through stiffness or force magnitude. Specifically, we measured the number of stiffness or force- magnitude levels that can be reliably identified in an absolute identification paradigm. The range of stiffness and force magnitude used in the present study, 0.2-3.0 N/mm and 0.1-5.0 N, respectively, was typical of the parameter values encountered in most virtual reality or data visualization applications. Ten individuals participated in a stiffness identification experiment, each completing 250 trials. Subsequently, four of these individuals and six additional participants completed 250 trials in a force-magnitude identification experiment. A custom-designed 3 degrees-of-freedom force-feedback device, the ministick, was used for stimulus delivery. The results showed an average information transfer of 1.46 bits for stiffness identification, or equivalently, 2.8 correctly-identifiable stiffness levels. The average information transfer for force magnitude was 1.54 bits, or equivalently, 2.9 correctly-identifiable force magnitudes. Therefore, on average, the participants could only reliably identify 2-3 stiffness levels in the range of 0.2-3.0 N/mm, and 2-3 force- magnitude levels in the range of 0.1-5.0 N. Individual performance varied from 1 to 4 correctly-identifiable stiffness levels and 2 to 4 correctly-identifiable force-magnitude levels. Our results are consistent with reported information transfers for haptic stimuli. Based on the present study, it is recommended that 2 stiffness or force-magnitude levels (i.e., high and low) be used with haptic glyphs in a data visualization system, with an additional third level (medium) for more experienced users.","Haptic interfaces,
Data visualization,
Signal processing,
Channel capacity,
Laboratories,
Rendering (computer graphics),
Information analysis,
Auditory displays,
Feedback,
Guidelines"
Architectural dependability evaluation with Arcade,"This paper proposes a formally well-rooted and extensible framework for dependability evaluation: Arcade (architectural dependability evaluation). It has been designed to combine the strengths of previous approaches to the evaluation of dependability. A key feature is its formal semantics in terms of input/output-interactive Markov chains, which enables both compositional modeling and compositional state space generation and reduction. The latter enables great computational reductions for many models. The Arcade approach is extensible, hence adaptable to new circumstances or application areas. The paper introduces the new modeling approach, discusses its formal semantics and illustrates its use with two case studies.","Markov processes,
distributed databases,
formal verification"
Saliency Cuts: An automatic approach to object segmentation,"Interactive graph cuts are widely used in object segmentation but with some disadvantages: 1) Manual interactions may cause inaccurate or even incorrect segmentation results and involve more interactions especially for novices. 2) In some situations, the manual interactions are infeasible. To overcome these disadvantages, we propose a novel approach, namely Saliency Cuts, to segment object from background automatically. By exploring the effects of labels to graph cuts, the so called “Professional Labels” is introduced to evaluate labels. With the aid of saliency detection, a multiresolution framework is designed to provide “Professional Labels” automatically and implement object segmentation using graph cuts. The experiments demonstrate the promising performance of Saliency Cuts.","Object segmentation,
Labeling,
Object detection,
Image segmentation,
Humans,
Laboratories,
Pattern recognition,
Automation,
Computer vision,
Application software"
An Improved Decision-Based Algorithm for Impulse Noise Removal,"The paper proposes an improved fast and efficient decision-based algorithm for the restoration of images that are highly corrupted by Salt-and-Pepper noise. The new algorithm utilizes previously processed neighboring pixel values to get better image quality than the one utilizing only the just previously processed pixel value. The proposed algorithm is faster and also produces better result than a Standard Median Filter (SMF), Adaptive Median Filters (AMF), Cascade and Recursive non-linear filters. The proposed method removes only the noisy pixel either by the median value or by the mean of the previously processed neighboring pixel values. Different images have been tested by using the proposed algorithm (PA) and found to produce better PSNR and SSIM values.","Nonlinear filters,
PSNR,
Adaptive filters,
Signal processing algorithms,
Filtering,
Image restoration,
Pixel,
Computer science,
Noise level,
Circuit noise"
Virtual Organization Support within a Grid-Wide Operating System,"Despite grids' popularity, virtual organizations (VOs) have yet to become a commodity technology in modern computing environments due to the complexity of managing them and difficulty of assuring user and VO isolation. Here, the authors describe the VO management approach taken by XtreemOS, a new grid operating system with native support for VOs that supports a wide range of computing resources, from clusters to mobiles. They also discuss the requirements for the VO model and management within XtreemOS and introduce an expandable VO model and a system architecture that supports it.","Operating systems,
Grid computing,
Councils,
Isolation technology,
Resource management,
Middleware,
Linux,
Optical computing,
Environmental management,
Technology management"
Securing location aware services over VANET using geographical secure path routing,"We propose to secure location aware services over vehicular ad-hoc networks (VANET) with our geographical secure path routing protocol (GSPR). GSPR is an infrastructure free geographic routing protocol, which is resilient to disruptions caused by malicious or faulty nodes. Geographic locations of anonymous nodes are authenticated in order to provide location authentication and location privacy simultaneously. Our protocol also authenticates the routing paths taken by individual messages. This paper presents the design of the GSPR secure geographic routing protocol. The overhead of location authentication is investigated under various scenarios through network simulation. Results show that although the presence of malicious nodes increases the routing path length, a data delivery rate of larger than 80% is sustained even if 40% of the nodes are malicious.",
Loss Recovery in Application-Layer Multicast,"Application-layer multicast (ALM), sometimes called overlay multicast, can help circumvent the limitations in IP multicast and unicast. In this article, we discuss and compare different recovery mechanisms in ALM. The major challenge in loss recovery is how to achieve low residual loss rate with low recovery overhead. As discussed, a promising approach might be a combination of proactive and reactive techniques.",
Using STATCOM to mitigate voltage fluctuations due to aerodynamic aspects of wind turbines,"This paper investigates how a STATic COMPensator (STATCOM) mitigates the voltage fluctuations caused by the aerodynaimc aspects of a wind turbine in a wind power system (e.g., due to yaw error and turbulence). In this regard, a small wind farm equipped with fixed speed wind turbines driving conventional induction generators connected to a relatively weak distribution system is studied, where a STATCOM is utilized to improve the power quality. The paper describes how the aerodynamic and mechanical aspects of a wind turbine can be simulated using TurbSim, AeroDyn, and FAST, where the electrical parts of the wind turbine, STATCOM, and network are modelled by Simulink blocks. Moreover, the STATCOM control scheme implemented in a Simulink environment is explained. Simulation results obtained from the model are used to observe the effects of yaw error and turbulence on the power and voltage variations at Point of Common Coupling (PCC). The performance of the STATCOM and its controller is then evaluated based on these observations.",
MRBench: A Benchmark for MapReduce Framework,"MapReduce is Google's programming model for easy development of scalable parallel applications which process huge quantity of data on many clusters. Due to its conveniency and efficiency, MapReduce is used in various applications (e.g., web search services and online analytical processing.) However, there are only few good benchmarks to evaluate MapReduce implementations by realistic testsets. In this paper, we present MRBench that is a benchmark for evaluating MapReduce systems. MRBench focuses on processing business oriented queries and concurrent data modifications. To this end, we build MRBench to deal with large volumes of relational data and execute highly complex queries. By MRBench, users can evaluate the performance of MapReduce systems while varying environmental parameters such as data size and the number of (Map/Reduce) tasks. Our extensive experimental results show that MRBench is a useful tool to benchmark the capability of answering critical business questions.","Benchmark testing,
Web search,
Parallel programming,
Application software,
Parallel processing,
Computer science,
Data engineering,
Costs,
Fault tolerance,
Functional programming"
Energy-Based Hierarchical Edge Clustering of Graphs,"Effectively visualizing complex node-link graphs which depict relationships among data nodes is a challenging task due to the clutter and occlusion resulting from an excessive amount of edges. In this paper, we propose a novel energy-based hierarchical edge clustering method for node-link graphs. Taking into the consideration of the graph topology, our method first samples graph edges into segments using Delaunay triangulation to generate the control points, which are then hierarchically clustered by energy-based optimization. The edges are grouped according to their positions and directions to improve comprehensibility through abstraction and thus reduce visual clutter. The experimental results demonstrate the effectiveness of our proposed method in clustering edges and providing good high level abstractions of complex graphs.",
A Load-Balanced Guiding Navigation Protocol in Wireless Sensor Networks,"One of the major applications of wireless sensor networks is guiding navigation service with its goal to assist moving objects in leaving a hazardous region safely and quickly. In this paper, we propose a distributed guiding navigation protocol that can guide moving objects to multiple exits with load balancing among multiple navigation paths to the exits. With the assistances of sensor nodes, moving objects are guided to different navigation paths so that they can move to exits as soon as possible without causing congestion. In some traditional navigation algorithms, a sensor may select a wrong guiding direction when an information- updating packet has not flooded through the whole network. Hence, the guiding direction of the sensor may oscillate in a short time and guided objects will confuse with this phenomenon, called direction oscillation problem. This problem will be eliminated in our proposed protocol. Simulation results indicate that our protocol can guide moving objects to exits in shorter time and solve the direction oscillation problem effectively.","Navigation,
Wireless application protocol,
Wireless sensor networks,
Sensor phenomena and characterization,
Monitoring,
Computer science,
Load management,
Application software,
Indoor environments,
Image sensors"
Wearable context-aware food recognition for calorie monitoring,"We propose DiaWear, a novel assistive mobile phone-based calorie monitoring system to improve the quality of life of diabetes patients and individuals with unique nutrition management needs. Our goal is to achieve improved daily semi-automatic food recognition using a mobile wearable cell phone. DiaWear currently uses a neural network classification scheme to identify food items from a captured image. It is difficult to account for the varying and implicit nature of certain foods using traditional image recognition techniques. To overcome these limitations, we introduce the role of the mobile phone as a platform to gather contextual information from the user and system in obtaining better food recognition.","Biomedical monitoring,
Neural networks,
Patient monitoring,
Diabetes,
Cellular phones,
Image recognition,
Computerized monitoring,
Remote monitoring,
Probability,
Computer science"
Eleven Guidelines for Implementing Pair Programming in the Classroom,"Utilizing pair programming in the classroom requires specific classroom management techniques. We have created nine guidelines for successfully implementing pair programming in the classroom. These guidelines are based on pair programming experiences spanning seven years and over one thousand students at North Carolina State University. In Fall 2007, pair programming was adopted in the undergraduate human-computer interaction (HCI) course at Virginia Tech. We present the pair programming guidelines in the context of the HCI course, discuss how the guidelines were implemented, and evaluate the general applicability and sufficiency of the guidelines. We find that eight of the nine guidelines were applicable to the Virginia Tech experience. We amended our peer evaluation guideline to account for constantly supervised pairing, as was the case at Virginia Tech. We add two guidelines stating that a pair should always be working toward a common goal and that pairs should be encouraged to find their own answers to increase their independence and self-confidence.","Programming,
Programming profession,
Guidelines,
Education,
Navigation,
Human computer interaction,
Driver circuits"
Extraction of acne lesion in acne patients from multispectral images,"In acne treatment, it is important to accurately evaluate the severity of Acne. The acne should be classified into several skin lesions including comedo, reddish papule, pustule, and scar. However, in some cases, a visual detection from RGB image maybe difficult for the proper evaluation of acne skin lesions. This paper proposes an extraction method using the spectral information of the various type of acne skin lesions calculated from the multispectral images (MSI) of the lesions. In the experiment, we showed the possibility of classifying acne lesion types by applying a combination of several linear discriminant functions (LDF's).",
Video forgery detection using correlation of noise residue,"We propose a new approach for locating forged regions in a video using correlation of noise residue. In our method, block-level correlation values of noise residual are extracted as a feature for classification. We model the distribution of correlation of temporal noise residue in a forged video as a Gaussian mixture model (GMM). We propose a two-step scheme to estimate the model parameters. Consequently, a Bayesian classifier is used to find the optimal threshold value based on the estimated parameters. Two video inpainting schemes are used to simulate two different types of forgery processes for performance evaluation. Simulation results show that our method achieves promising accuracy in video forgery detection.","Noise,
Correlation,
Forgery,
Streaming media,
Cameras,
Classification algorithms,
Image coding"
"Analytical, numerical, and experimental methods for through-the-wall radar imaging","In this paper a physics-based approach for image formation of targets behind complex wall structures is presented. Analytical and numerical techniques are used for the development of forward scattering models which are then exploited in construction of matched filters for ultra-wideband synthetic aperture radars operating over a wide rang of incidence angles. Special scattering models for different wall types including cinder block and reinforced concrete walls are presented using efficient numerical and approximate analytical techniques. These allow for construction of SAR images as well as development of a refocusing algorithm. An experimental ultra-wideband radar is set up in the laboratory environment for the evaluation of the models presented. Also, a radar measurement configuration is proposed that allows for elimination of direct reflection from the walls.","Image analysis,
Radar imaging,
Concrete,
Frequency,
Erbium,
Electromagnetic scattering,
Radar scattering,
Ultra wideband technology,
Object detection,
Periodic structures"
Photogeometric structured light: A self-calibrating and multi-viewpoint framework for accurate 3D modeling,"Structured-light methods actively generate geometric correspondence data between projectors and cameras in order to facilitate robust 3D reconstruction. In this paper, we present Photogeometric Structured Light whereby a standard structured light method is extended to include photometric methods. Photometric processing serves the double purpose of increasing the amount of recovered surface detail and of enabling the structured-light setup to be robustly self-calibrated. Further, our framework uses a photogeometric optimization that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate multi-view 3D model which best complies with photometric and geometric data.","Image reconstruction,
Photometry,
Cameras,
Robustness,
Surface reconstruction,
Solid modeling,
Reflectivity,
Photography,
Hardware,
Computer science"
On Robotics Applications in Service-Oriented Architecture,"Service-Oriented Computing (SOC) research and applications have been largely limited to software development in electronic and Web based applications. Service-oriented robotics software development extends SOC from its current fields to a new domain, which was considered not feasible because of the efficiency issues in terms of computing and communication. This paper presents the concepts, principles, and methods in SOC and SOC-based robotics software development and applies them in the design of distributed robotics applications. Two case studies, Intel security robot design and a maze-traversing robot application in Microsoft Robotics Studio, are used to illustrate the concepts and methods.","systems analysis,
robots,
software architecture,
software development management"
Year,,
Dynamic clustering for vigilance analysis based on EEG,"Electroencephalogram (EEG) is the most commonly studied signal for vigilance estimation. Up to now, many researches mainly focus on using supervised learning methods for analyzing EEG data. However, it is hard to obtain enough labeled EEG data to cover the whole vigilance states, and sometimes the labeled EEG data may be not reliable in practice. In this paper, we propose a dynamic clustering method based on EEG to estimate vigilance states. This method uses temporal series information to supervise EEG data clustering. Experimental results show that our method can correctly discriminate between the wakefulness and the sleepiness for every 2 seconds through EEG, and can also distinguish two other middle states between wakefulness and sleepiness.",
Reducing Power Supply Noise in Linear-Decompressor-Based Test Data Compression Environment for At-Speed Scan Testing,"Yield loss caused by excessive power supply noise has become a serious problem in at-speed scan testing. Although X-filling techniques are available to reduce the launch cycle switching activity, their performance may not be satisfactory in the linear-decompressor-based test compression environment. This work is the first to solve this problem by proposing a novel integrated ATPG scheme that efficiently and effectively performs compressible X-filling. Related theoretical principles are established, based on which the problem size is substantially reduced. The proposed scheme is validated by large benchmark circuits as well as an industry design in the embedded deterministic test (EDT) environment.","Power supplies,
Noise reduction,
Working environment noise,
Test data compression,
Circuit testing,
Circuit faults,
Clocks,
Delay,
Electronic equipment testing,
Automatic test pattern generation"
Exploring FPGA network on chip implementations across various application and network loads,"The network on chip will become a future general purpose interconnect for FPGAs much like today’s standard OPB or PLB bus architectures. However, performance characteristics and reconfigurable logic resource utilization of different network on chip architectures vary greatly relative to bus architectures. Current mainstream FPGA parts only support very small network on chip topologies, due to the high resource utilization of virtual channel based implementations. This observation is reflected in related research where only modest 2x2 or 2x3 networks are demonstrated on FPGAs. Naively it would be assumed that these complex network on chip architectures would perform better than simplified implementations. We show this assumption to be incorrect under light network loading conditions across 3 separate application domains. Using statistical based network loading, a synthetic benchmarking application, a cryptographic accelerator, and a 802.11 transmitter are each demonstrated across network on chip architectures. From these experiments, it can be seen that network on chips with complex routing and switching functionality are still useful under high network loading conditions. Additionally, it is also shown for our network on chip implementations, a simple solution that uses 4–5x less logic resources can provide better network performance under certain conditions.","Field programmable gate arrays,
Cryptography,
Loading,
Tiles,
Routing,
Resource management,
Transmitters"
Intelligent moving of groups in real-time strategy games,"This paper investigates the intelligent moving and path-finding of groups in real-time strategy (RTS) games exemplified by the open source game Glest. We utilize the technique of flocking for achieving a smooth and natural movement of a group of units and expect grouping to decrease the amount of unit losses in RTS games. Furthermore, we present a setting in which flocking will improve the game progress. But we also demonstrate a situation where flocking fails. To prevent these annoying situations, we combined flocking with influence maps (IM) to find safe paths for the flock in real time. This combination turns out to be an excellent alternative to normal movement in Glest and most likely in other RTS games.","Artificial intelligence,
Humans,
Assembly,
Self organizing feature maps,
Evolutionary computation,
Computational intelligence,
Computer science,
Filters,
Artificial neural networks,
Visualization"
Rice disease identification using pattern recognition techniques,"The techniques of machine vision are extensively applied to agricultural science, and it has great perspective especially in the plant protection field, which ultimately leads to crops management. The paper describes a software prototype system for rice disease detection based on the infected images of various rice plants. Images of the infected rice plants are captured by digital camera and processed using image growing, image segmentation techniques to detect infected parts of the plants. Then the infected part of the leaf has been used for the classification purpose using neural network. The methods evolved in this system are both image processing and soft computing technique applied on number of diseased rice plants.","Diseases,
Pattern recognition,
Machine vision,
Plants (biology),
Protection,
Crops,
Software systems,
Software prototyping,
Digital cameras,
Image segmentation"
Monitoring Network Evolution using MDL,"Given publication titles and authors, what can we say about the evolution of scientific topics and communities over time? Which communities shrunk, which emerged, and which split, over time? And, when in time were the turning points? We propose TimeFall, which can automatically answer these questions given a social network/graph that evolves over time. The main novelty of the proposed approach is that it needs no user-defined parameters, relying instead on the principle of Minimum Description Length (MDL), to extract the communities, and to find good cut-points in time when communities change abruptly: a cut-point is good, if it leads to shorter data description. We illustrate our algorithm on synthetic and large real datasets, and we show that the results of the TimeFall agree with human intuition.","Visualization,
Communities,
Social network services,
Clustering algorithms,
Condition monitoring,
Machine learning,
Turning,
Data mining,
Humans,
Databases"
Appraisal of the Effectiveness and Efficiency of an Information Security Management System Based on ISO 27001,"The ISO27001:2005, as an information security management system (ISMS), is establishing itself more and more as the security standard in enterprises. In 2008 more than 4457 certified enterprises could be registered worldwide. Nevertheless, the registering an ISMS still says nothing about the quality and performance of its implementation. Therefore, in this article, a method for measuring the performance of the implementation and operation of an ISMS is presented.","Security,
ISO,
Business,
Economics,
Biological system modeling,
Radiation detectors,
Information security"
Rainbow Network Flow of Multiple Description Codes,"This paper is an enquiry into the interaction between multiple description coding (MDC) and network routing. We are mainly concerned with rate-distortion optimized network flow of a multiple description (MD) source from multiple servers to multiple sinks. We aim at maximizing a collective metric of the quality of source reconstruction at all sinks, by optimally routing the MD source streams from the server nodes to the sinks. This problem turns out to be very different from conventional maximum network flow. The objective function involves not only the flow volume but also the diversity of the flow contents (i.e., distinction of descriptions), hence, the term rainbow network flow (RNF). For a general network topology, a general fidelity function, and an arbitrary distribution of MDC descriptions on the servers, we prove the RNF problem to be Max-SNP-hard. However, the problem becomes tractable in many practical scenarios, such as when MDC is balanced with descriptions of the same length and importance, when all source nodes have the complete set of MDC descriptions, and when the network topology is a tree or has only one sink. Polynomial-time RNF algorithms are developed for these cases.","Network servers,
Routing,
Network topology,
Computer science,
Quality of service,
Polynomials,
Packet switching,
IP networks,
Wireless networks,
Delay"
On the Approximability of Budgeted Allocations and Improved Lower Bounds for Submodular Welfare Maximization and GAP,"In this paper we consider the following {\em maximum budgeted allocation}(MBA) problem: Given a set of
m
indivisible items and
n
agents; each agent
i
willing to pay
Undefined control sequence \bij
on item
j
and with a maximum budget of
B
i
, the goal is to allocate itemsto agents to maximize revenue.The problem naturally arises as auctioneer revenue maximization in budget-constrained auctions and as winner determinationproblem in combinatorial auctions when utilities of agents are budgeted-additive. Our main results are:- We give a
3/4
-approximation algorithm for MBA improving upon the previous best of
≃0.632
\cite{AM,FV}. Our techniques are based on a natural LP relaxation of MBA and our factor is optimal in the sense that it matches the integrality gap of the LP.- We prove it is NP-hard to approximate MBA to any factor better than
15/16
, previously only NP-hardness was known \cite{SS,LLN}.  Our result also implies NP-hardness of approximating maximum submodular welfare with {\em demand oracle} to a factor better than
15/16
, improving upon the best known hardness of
275/276
\cite{FV}.-  Our hardness techniques can be modified to prove that it is NP-hard to approximate the {\em Generalized Assignment Problem} (GAP)  to any factor better than
10/11
. This improves upon the
422/423
hardness of \cite{CK,CC}.We use {\em iterative rounding} on a natural LP relaxation of MBA to obtain the
3/4
-approximation. We also give a
(3/4−ϵ)
-factoralgorithm based on the primal-dual schema which runs in
O
̃ 
(nm)
time, for any constant
ϵ≫0
.",
An efficient retraction-based RRT planner,"We present a novel optimization-based retraction algorithm to improve the performance of sample-based planners in narrow passages for 3D rigid robots. The retraction step is formulated as an optimization problem using an appropriate distance metric in the configuration space. Our algorithm computes samples near the boundary of C-obstacle using local contact analysis and uses those samples to improve the performance of RRT planners in narrow passages. We analyze the performance of our planner using Voronoi diagrams and show that the tree can grow closely towards any randomly generated sample. Our algorithm is general and applicable to all polygonal models. In practice, we observe significant speedups over prior RRT planners on challenging scenarios with narrow passages.","Orbital robotics,
Performance analysis,
Iterative algorithms,
Algorithm design and analysis,
Robotics and automation,
USA Councils,
Computer science,
Path planning,
Degradation,
Sampling methods"
Augmented Reality Environmental Monitoring Using Wireless Sensor Networks,"Environmental monitoring brings many challenges to wireless sensor networks: including the need to collect and process large volumes of data before presenting the information to the user in an easy to understand format. This paper presents SensAR, a prototype augmented reality interface specifically designed for monitoring environmental information. The input of our prototype is sound and temperature data which are located inside a networked environment. Participants can visualise 3D as well as textual representations of environmental information in real-time using a lightweight handheld computer.","wireless sensor networks,
augmented reality,
computerised monitoring,
data visualisation,
environmental science computing"
Problem-Based Learning via Web 2.0 Technologies,"During the last few decades, medical education is shifting is increasingly embracing active learning approaches. This shift from teaching to learning is also strongly related to an involvement of information and communication technology, and especially the Internet and the Web. The emergence of Internet 2.0 is indeed being stressed as a promising tool for advanced support of medicine and medical education. Although Web 2.0 emphasizes on participation, in its early days is still used in the majority of cases to hold and provide content (albeit created dynamically and via peer participation and collaboration) and then systematically deliver it to students. In this paper, we propose the use of wikis and blogs not just for creation and promotion of information, but as active tools to support problem based learning in medicine. In this approach, students and instructors use the web as a virtual place to collaborate and create new knowledge and new educational experiences.","Collaboration,
Internet,
Educational technology,
Blogs,
Information technology,
Collaborative work,
Books,
Educational programs,
Computer science education,
Communications technology"
Filtering large fingerprint database for latent matching,"Latent fingerprint identification is of critical importance to law enforcement agencies in apprehending criminals. Considering the huge size of fingerprint databases maintained by law enforcement agencies, exhaustive one-to-one matching is impractical and a database filtering technique is necessary to reduce the search space. Due to low image quality and small finger area of latent fingerprints, it is necessary to use several features for an efficient and reliable filtering system. A multi-stage filtering system is proposed, which utilizes pattern type, singular points and orientation field. We have tested our system by searching 258 latent fingerprints in NIST SD27 against a background database containing 10,258 rolled fingerprints (obtained by combining 2,000 in NIST SD4, 8,000 in SD14 and 258 in SD27). Although latent fingerprints contain very limited information, the filtering system not only improved the matching speed by three fold but also improved the rank-1 matching accuracy from 70.9% to 73.3%.","Matched filters,
Filtering,
Fingerprint recognition,
Image databases,
Spatial databases,
Law enforcement,
NIST,
Maintenance,
Image quality,
Fingers"
Development and evaluation of a flexible interface for a wheelchair mounted robotic arm,"Accessibility is a challenge for people with disabilities. Differences in cognitive ability, sensory impairments, motor dexterity, behavioral skills, and social skills must be taken into account when designing interfaces for assistive devices. Flexible interfaces tuned for individuals, instead of custom-built solutions, may benefit a larger number of people. The development and evaluation of a flexible interface for controlling a wheelchair mounted robotic arm is described in this paper. There are four versions of the interface based on input device (touch screen or joystick) and a moving or stationary shoulder camera. We describe results from an eight week experiment conducted with representative end users who range in physical and cognitive ability.","Cameras,
Robot vision systems,
Wheelchairs,
Mobile robots,
Switches"
Taxonomy and Implementation of Redirection Techniques for Ubiquitous Passive Haptic Feedback,"Traveling through immersive virtual environments (IVEs) by means of real walking is an important activity to increase naturalness of VR-based interaction. However, the size of the virtual world often exceeds the size of the tracked space so that a straightforward implementation of omni-directional and unlimited walking is not possible. Redirected walking is one concept to solve this problem of walking in IVEs by inconspicuously guiding the user on a physical path that may differ from the path the user visually perceives. When the user approaches a virtual object she can be redirected to a real proxy object that is registered to the virtual counterpart and provides passive haptic feedback. In such passive haptic environments, any number of virtual objects can be mapped to proxy objects having similar haptic properties, e.g., size, shape and texture. The user can sense a virtual object by touching its real world counterpart. Redirecting a user to a registered proxy object makes it necessary to predict the user's intended position in the IVE. Based on this target position we determine a path through the physical space such that the user is guided to the registered proxy object. We present a taxonomy of possible redirection techniques that enable user guidance such that inconsistencies between visual and proprioceptive stimuli are imperceptible.We describe how a user's target in the virtual world can be predicted reliably and how a corresponding real-world path to the registered proxy object can be derived.","Taxonomy,
Haptic interfaces,
Feedback,
Legged locomotion,
Virtual environment,
Space exploration,
Computer science,
Navigation,
Large-scale systems,
Graphics"
Universal Multi-Factor Authentication Using Graphical Passwords,"In this paper, we present a series of methods to authenticate a user with a graphical password. To that end, we employ the user’s personal handheld device as the password decoder and the second factor of authentication. In our methods, a service provider challenges the user with an image password. To determine the appropriate click points and their order, the user needs some hint information transmitted only to her handheld device. We show that our method can overcome threats such as key-loggers, weak password, and shoulder surfing. With the increasing popularity of handheld devices such as cell phones, our approach can be leveraged by many organizations without forcing the user to memorize different passwords or carrying around different tokens.","Authentication,
Handheld computers,
Keyboards,
Security,
Internet,
Computer science,
Decoding,
Cellular phones,
Pixel,
Rendering (computer graphics)"
Automatic computer aided diagnosis tool using component-based SVM,"Alzheimer type dementia (ATD) is a progressive neurodegenerative disorder first affecting memory functions and then gradually affecting all cognitive functions with behavioral impairments and eventually causing death. Functional brain imaging including single-photon emission computed tomography (SPECT) is commonly used to guide the clinician’s diagnosis. However, conventional evaluation of these scans often relies on manual reorientation, visual reading and semiquantitative analysis of certain regions of the brain. These steps are time consuming, subjective and prone to error. This paper shows a fully automatic computer-aided diagnosis (CAD) system for improving the accuracy in the early diagnosis of the Alzheimer’s disease. The proposed approach is based on a first automatic feature selection, and secondly a combination of component-based support vector machine (SVM) classification and a pasting votes technique of ensemble SVM classifiers.",
Performance estimation and slack matching for pipelined asynchronous architectures with choice,"This paper presents a fast analytical method for estimating the throughput of pipelined asynchronous systems, and then applies that method to develop a fast solution to the problem of pipelining “slack matching.” The approach targets systems with hierarchical topologies, which typically result when high-level (block structured) language specifucations are compiled into data-driven circuit implementations. A significant contribution is that our approach is the first to efficiently handle architectures with choice (i.e., the presence of conditional computation constructs such if-then-else and conditional loops).","Performance analysis,
Throughput,
Runtime,
Pipeline processing,
Circuits,
Computer architecture,
Information analysis,
Design optimization,
Delay,
Latches"
Consensus learning for distributed coverage control,"A decentralized controller is presented that causes a network of robots to converge to a near optimal sensing configuration, while simultaneously learning the distribution of sensory information in the environment. A consensus (or flocking) term is introduced in the learning law to allow sharing of parameters among neighbors, greatly increasing learning convergence rates. Convergence and consensus is proven using a Lyapunov-type proof. The controller with parameter consensus is shown to perform better than the basic controller in numerical simulations.","Distributed control,
Robot sensing systems,
Automatic control,
Convergence,
Optimal control,
Robotics and automation,
Control systems,
Numerical simulation,
Q measurement,
Learning"
End-to-End Enforcement of Erasure and Declassification,"Declassification occurs when the confidentiality of information is weakened; erasure occurs when the confidentiality of information is strengthened, perhaps to the point of completely removing the information from the system. This paper shows how to enforce erasure and declassification policies. A combination of a type system that controls information flow and a simple runtime mechanism to overwrite data ensures end-to-end enforcement of policies. We prove that well-typed programs satisfy the semantic security condition noninterference according to policy. We extend the Jif programming language with erasure and declassification enforcement mechanisms and use the resulting language in a large case study of a voting system.","Information security,
Data security,
Control systems,
Runtime,
Computer languages,
Voting,
Government,
Computer security,
Computer science,
Medical diagnostic imaging"
Bridging Global and Local Models of Service-Oriented Systems,"A service-oriented system is a collection of independent services that interact with one another through message exchanges. Languages such as the Web Services Description Language (WSDL) and the Business Process Execution Language (BPEL) allow developers to capture the interactions in which an individual service can engage, both from a structural and from a behavioral perspective. However, in large service-oriented systems, stakeholders may require a global picture of the way services interact with each other, rather than multiple small pictures focusing on individual services. Such global models are especially useful when a set of services interact in such a way that none of them sees all messages being exchanged, yet interactions between some services may affect the way other services interact. Unfortunately, global models of service interactions may sometimes capture behavioral constraints that cannot be enforced locally. In other words, some global models may not be translatable into a set of local models such that the sum of the local models equals the original global model. Starting from a previously proposed language for global modeling of service interactions, this paper defines an algorithm for determining if a global model is locally enforceable and an algorithm for generating local models from global ones. It also shows how local models are mapped into templates of BPEL process definitions.","Web services,
Australia,
XML,
Unified modeling language,
Simple object access protocol,
Distributed computing,
Computer science,
Service oriented architecture,
Information systems,
Information technology"
On Purely Automated Attacks and Click-Based Graphical Passwords,"We present and evaluate various methods for purely automated attacks against click-based graphical passwords. Our purely automated methods combine click-order heuristics with focus-of-attention scan-paths generated from a computational model of visual attention. Our method results in a significantly better automated attack than previous work, guessing 8-15% of passwords for two representative images using dictionaries of less than 2^24.6 entries, and about 16% of passwords on each of these images using dictionaries of less than 2^31.4 entries (where the full password space is 2^43). Relaxing our click-order pattern substantially increased the efficacy of our attack albeit with larger dictionaries of 2^34.7 entries, allowing attacks that guessed 48-54% of passwords (compared to previous results of 0.9% and 9.1% on the same two images with 2^35 guesses). These latter automated attacks are independent of focus-of-attention models, and are based on image-independent guessing patterns. Our results show that automated attacks, which are easier to arrange than human-seeded attacks and are more scalable to systems that use multiple images, pose a significant threat.","Dictionaries,
Layout,
Focusing,
Computer security,
Application software,
Computer science,
Computational modeling,
Image processing,
Humans,
Hair"
Modularization of the UML Metamodel Using Model Slicing,"The UML metamodel has been increased in its size and complexity due to many needs for supporting various platforms and domains. The large size of the metammodel can prevent tool developers from understanding the UML metamodel and thus from developing UML-based tools. In this paper, we propose an approach to managing the complexity of the UML metamodel by modularizing the metamodel into a set of small metamodels for each UML diagram type. To that goal, we propose a slicing algorithm for extracting diagram-specific metamodels from the UML metamodel.","Unified modeling language,
Information technology,
Computer science,
Real time systems,
Navigation,
Visualization,
Gas discharge devices"
A flexible weighted clustering algorithm based on battery power for Mobile Ad hoc Networks,"Mobile Ad hoc Networks (MANET) consist of a number of wireless hosts that communicate with each other through multi-hop wireless links in the absence of fixed infrastructure. The previous research on mobile ad-hoc network suggested the use of clustering algorithm because clustering makes it possible to guarantee basic levels of system performance, such as throughput and delay, in the presence of both mobility and a large number of mobile terminals. In this paper, we propose that the Flexible Weighted Clustering Algorithm based on Battery Power (FWCABP), leads to a high degree of stability in the network, minimizing the number of clusters, and minimizing the overhead for the clustering formation and maintenance by keeping a node with weak battery power from being elected as a cluster-head. Simulation experiments are conducted to evaluate the performance of our algorithm in terms of the number of clusters formed, reaffiliation frequency, and number of cluster-head change. Results show that our algorithm performs better than existing ones and is also tunable to different kinds of network conditions.","Clustering algorithms,
Batteries,
Mobile ad hoc networks,
Wireless communication,
Spread spectrum communication,
Ad hoc networks,
System performance,
Throughput,
Stability,
Frequency"
Dynamic Performance of Mobile Haptic Interfaces,"The increasing demand for virtual reality applications in several scientific disciplines feeds new research perspectives dealing with robotics, automation, and computer science. In this context, one of the topics is the design of advanced force-feedback devices allowing not only kinesthetic interaction with virtual objects but also locomotion and navigation inside virtual worlds. This has the main advantage to stimulate human vestibular apparatus, thus increasing the overall realism of simulation. Particularly, this paper deals with mobile haptic interfaces (MHIs), built by combining standard force-feedback devices with mobile platforms. We investigated which factors may affect the transparency of this kind of devices, identifying in mobile robot dynamics a possible cause of loss of transparency. Hence, in this paper, we present a method to analyze dynamic performance of an MHI and some basic guidelines to design controller in order to meet desired specifications. Experimental validation of the theoretical results is reported.","Haptic interfaces,
Robotics and automation,
Virtual reality,
Application software,
Feeds,
Computer science,
Navigation,
Humans,
Computational modeling,
Mobile robots"
Program slicing,"Program slicing is a decomposition technique that slides program components not relevant to a chosen computation, referred to as a slicing criterion. The remaining components form an executable program called a slice that computes a projection of the original programpsilas semantics. Using examples coupled with fundamental principles, a tutorial introduction to program slicing is presented. Then applications of program slicing are surveyed, ranging from its first use as a debugging technique to current applications in property verification using finite state models. Finally, a summary of research challenges for the slicing community is discussed.","Testing,
Debugging,
Software maintenance,
Software systems,
Educational institutions,
Distance measurement,
Roads"
Massively parallel genomic sequence search on the Blue Gene/P architecture,"This paper presents our first experiences in mapping and optimizing genomic sequence search onto the massively parallel IBM Blue Gene/P (BG/P) platform. Specifically, we performed our work on mpiBLAST, a parallel sequence-search code that has been optimized on numerous supercomputing environments. In doing so, we identify several critical performance issues. Consequently, we propose and study different approaches for mapping sequence-search and parallel I/O tasks on such massively parallel architectures.We demonstrate that our optimizations can deliver nearly linear scaling (93% efficiency) on up to 32,768 cores of BG/P. In addition, we show that such scalability enables us to complete a large-scale bioinformatics problem - sequence searching a microbial genome database against itself to support the discovery of missing genes in genomes - in only a few hours on BG/P. Previously, this problem was viewed as computationally intractable in practice.","Genomics,
Bioinformatics,
Computer science,
Sequences,
Databases,
Concurrent computing,
Mathematics,
Laboratories,
Permission,
Computer architecture"
Year,,
Parallel Inferencing for OWL Knowledge Bases,"We examine the problem of parallelizing the inferencing process for OWL knowledge-bases. A key challenge in this problem is  partitioning the computational workload of this process to minimize duplication of computation and the amount of data communicated among processors. We investigate two approaches to address this challenge. In the data partitioning approach, the data-set is partitioned into smaller units, which are then processed independently. In the rule partitioning approach the rule-base is partitioned and the smaller rule-bases are applied to the complete data set. We present various algorithms for the partitioning and analyze their advantages and disadvantages. A parallel inferencing algorithm is presented which uses the partitions that are created by the two approaches. We then present an implementation based on a popular open source OWL reasoner and on a networked cluster. Our experimental results show significant speedups for some popular benchmarks, thus making this a promising approach.","OWL,
Partitioning algorithms,
Ontologies,
Complexity theory,
Algorithm design and analysis,
Benchmark testing,
Knowledge based systems"
Feedback Codes Achieving the Capacity of the Z-Channel,"Given the 1 to 0 bit error probability, pisin[0, 1], the capacity of the Z-channel is given by Cz=log2(1+pp/(1-p)-p1/(1-p)). Some new error free feedback coding schemes that achieve the Z-channel capacity are presented.","Error correction codes,
Capacity planning,
Error probability,
Optical feedback,
Computer science,
Redundancy,
Channel capacity,
Semiconductor memory,
Entropy,
Random variables"
NeuralWISP: An energy-harvesting wireless neural interface with 1-m range,"We present the NeuralWISP, a wireless neural interface operating from harvested RF energy. The NeuralWISP is compatible with commercial RFID readers and operates at a range up to 1m. It includes a custom low-noise, low power amplifier IC for processing the neural signal and an analog spike detection circuit for reducing digital computational requirements and communications bandwidth. Our system monitors the neural signal and periodically transmits the spike density in a user-programmable time window. The entire system draws an average 20μA from the harvested 1.8V supply.","Radiofrequency identification,
Radio frequency,
RFID tags,
Circuits,
Voltage,
Coils,
Wireless sensor networks,
Microcontrollers,
Strain measurement,
Temperature measurement"
Cortical correspondence using entropy-based particle systems and local features,"This paper presents a new method of constructing compact statistical point-based models of populations of human cortical surfaces with functions of spatial locations driving the correspondence optimization. The proposed method is to establish a tradeoff between an even sampling of the surfaces (a low surface entropy) and the similarity of corresponding points across the population (a low ensemble entropy). The similarity metric, however, isn’t constrained to be just spatial proximity, but can be any function of spatial location, thus allowing the integration of local cortical geometry as well as DTI connectivity maps and vasculature information from MRA images. This method does not require a spherical parameterization or fine tuning of parameters. Experimental results are also presented, showing lower local variability for both sulcal depth and cortical thickness measurements, compared to other commonly used methods such as FreeSurfer.","Entropy,
Biomedical imaging,
Biomedical computing,
Humans,
Optimization methods,
Diffusion tensor imaging,
Shape,
Cost function,
Mesh generation,
Computer science"
Modular Code Generation from Triggered and Timed Block Diagrams,"In previous work we have shown how modular code can be automatically generated from a synchronous block diagram notation where all blocks fire at all times. Here, we extend this work to triggered and timed diagrams, where some blocks fire only when their trigger is true, or at statically specified times. We show that, although triggers can be eliminated, this is not desirable since it destroys modularity and may also result in rejecting some diagrams that could be accepted. To avoid this we propose a modular code generation method that directly accounts for triggers. We also propose methods specialized to timed diagrams. Although timed diagrams are special cases of triggered diagrams, treating them directly allows us to obtain efficient code. We achieve this by enriching the interface of a macro block with firing time information and using this information to avoid firing the block unnecessarily. Existing firing time representations are generally conservative, in the sense that they cannot represent the exact set of firing times of a macro block, but a super-set. To remedy this, we devise a novel and accurate (exact) representation. This representation uses finite automata and is amenable to algebraic manipulation and generation of efficient code.","Synchronous generators,
Context modeling,
Application software,
USA Councils,
Fires,
Embedded software,
Computer science,
Laboratories,
Automata,
Software design"
Who can help me with this source code change?,"An approach to recommend a ranked list of developers to assist in performing software changes to a particular file is presented. The ranking is based on change expertise, experience, and contributions of developers, as derived from the analysis of the previous commits involving the specific file in question. The commits are obtained from a software system’s version control repositories (e.g., Subversion). The basic premise is that a developer who has substantially contributed changes to specific files in the past is likely to best assist for their current or future change. Evaluation of the approach on a number of open source systems such as koffice, Apache httpd, and GNU gcc is also presented. The results show that the accuracy of the correctly recommended developers is between 43% and 82%. New developers to a long-lived software project, or project managers, can use this approach to assist them in undertaking maintenance tasks, e.g., bug fix or adding a new feature. The approach can be realized as a plug-in to development environments such as Eclipse.","History,
Software,
Software systems,
Maintenance engineering,
Euclidean distance,
Frequency measurement,
Control systems"
Ecological Interface Design in the Nuclear Domain: An Application to the Secondary Subsystems of a Boiling Water Reactor Plant Simulator,"Accident investigations have revealed that unanticipated events are often precursors of major accidents. Unfortunately, conventional approaches to interface design for complex systems do not explicitly support problem solving during unanticipated events. Ecological Interface Design (EID) is a theoretical framework for designing computer interfaces that explicitly aims to support worker adaptation, especially during unanticipated events, leading to more robust user interfaces. However, limited verification and validation research in representative settings is impeding the adoption of the EID framework in the nuclear domain. This article presents an example by applying EID to the secondary side of a boiling water reactor plant simulator. The interface designers constructed abstraction hierarchy, causal, and part-whole models to acquire pertinent knowledge of the work domain and designed five ecological displays to represent the plant processes. These displays are analytically shown to contain visualization properties that could support monitoring and diagnosing unanticipated events in accordance to the claims of the EID framework. The analytical evaluation of the visualization features of the displays also illustrates that the EID framework could be applied to improve current verification practice. A companion article reports an empirical evaluation of these ecological displays to validate whether these properties could enhance operator performance.","Inductors,
Biological system modeling,
Displays,
Accidents,
Visualization,
Computational modeling,
Problem-solving,
Computer interfaces,
Robustness,
User interfaces"
Transpositions and move groups in Monte Carlo tree search,"Monte Carlo search, and specifically the UCT (Upper Confidence Bounds applied to Trees) algorithm, has contributed to a significant improvement in the game of Go and has received considerable attention in other applications. This article investigates two enhancements to the UCT algorithm. First, we consider the possible adjustments to UCT when the search tree is treated as a graph (and information amongst transpositions are shared). The second modification introduces move groupings, which may reduce the effective branching factor. Experiments with both enhancements were performed using artificial trees and in the game of Go. From the experimental results we conclude that both exploiting the graph structure and grouping moves may contribute to an increase in the playing strength of game programs using UCT.","Monte Carlo methods,
Tree graphs,
Computer science,
History,
Algorithm design and analysis,
Electronic mail,
Automation,
Tree data structures,
Statistics"
Active query selection for semi-supervised clustering,"Semi-supervised clustering allows a user to specify available prior knowledge about the data to improve the clustering performance. A common way to express this information is in the form of pair-wise constraints. A number of studies have shown that, in general, these constraints improve the resulting data partition. However, the choice of constraints is critical since improperly chosen constraints might actually degrade the clustering performance. We focus on constraint (also known as query) selection for improving the performance of semi-supervised clustering algorithms. We present an active query selection mechanism, where the queries are selected using a min-max criterion. Experimental results on a variety of datasets, using MPCK-means as the underlying semi-clustering algorithm, demonstrate the superior performance of the proposed query selection procedure.","Clustering algorithms,
Partitioning algorithms,
Skeleton,
Degradation,
Semisupervised learning,
Computer science,
Data engineering,
Knowledge engineering,
Unsupervised learning,
Terminology"
Velocity and disturbance observer for non-model based load and friction compensation,"In this paper, a scheme for the simultaneous estimation of the external forces acting on a rigid body together with its velocity is presented. This estimation scheme is based only on the knowledge of the position information and it allows to compensate undesired effects acting on the rigid body, like friction or external unknown loads, without requiring any knowledge of these spurious effects. This goal is achieved by means of two interacting observers, one for the observation of the generalized momenta and another for velocity observation. This compensation scheme can be also used to give to the system the desired dynamics, therefore avoiding the problem of parameter uncertainties. The effectiveness of the proposed scheme is proved both in simulation and experimentally, with a laboratory setup composed by a linear electric drive.","Friction,
Force control,
Actuators,
Control systems,
Uncertain systems,
Degradation,
Mathematical model,
Nonlinear filters,
Manipulator dynamics,
Robots"
Localized generalization error based active learning for image annotation,"Content-based image auto-annotation becomes a hot research topic owing to the development of image retrieval system and the storing technology of multimedia information. It is a key step in most of those image processing applications. In this work, we adopt active learning to image annotation for reducing the number of labeled images required for supervised learning procedure. Localized Generalization Error Model (L-GEM) based active learning uses localized generalization error bound as the sample selection criterion. In each turn, the most informative sample from a set of unlabeled samples is selected by the L-GEM based active learning will be labeled and added to the training dataset. A heuristic and a Q value selection improvement methods are introduced in this paper. The experimental results show that the proposed active learning efficiently reduces the number of labeled training samples. Moreover, the improvement method improve the performances in both testing accuracy and training time which are both essential in image annotation applications.",
Indoor Access Points Location Optimization Using Differential Evolution,"Wireless indoor positioning systems have become very popular and attractive in recent years. These systems have been successfully used in many fields such as asset tracking and inventory management, and location fingerprinting schemes are the most promising technique because of technical restrictions. However, how to optimize the location of access points to improve the positioning accuracy is still a challenging and difficult problem because of the complexity of indoor radio propagation environments. In this paper, we firstly discussed the indoor radio propagation model and proposed a novel optimization model for access points location optimization in which the Euclidean distance of received signal strength array among all the sampling points should be maximize in order to increase the differentia and diversity of the signal strength array, and thus improve the positioning accuracy of location fingerprinting schemes. Then we presented the Differential Evolution algorithm which was used to optimize the proposed problem, and the experimental testbed, experimental results and analysis were discussed. Experimental results show that the proposed model for access points location optimization can improve the positioning accuracy remarkably. Finally, we summarized the paper and gave possible future directions for research on access points location optimization for indoor environments.","Fingerprint recognition,
Indoor radio communication,
Software engineering,
Laboratories,
Inventory management,
Euclidean distance,
Sampling methods,
Mobile communication,
Wireless communication,
Wireless LAN"
Skyline-join in distributed databases,"The database research community has recently recognized the usefulness of skyline query. As an extension of existing database operator, the skyline query is valuable for multi-criteria decision making. However, current research tends to assume that the skyline operator is applied to one table which is not true for many applications on web databases. In web databases, tables are distributed in different sites, and a skyline query may involve attributes of multiple tables. In this paper, we address the problem of processing skyline queries on multiple tables in a distributed environment. We call the new operator skyline-join, as it is a hybrid of skyline and join operations. We propose two efficient approaches to process skyline-join queries which can significantly reduce the communication cost and processing time. Experiments are conducted and results show that our approaches are efficient for distributed skyline-join queries.","Distributed databases,
Iterative algorithms,
Sorting,
Decision making,
Query processing,
Sun,
Computer science,
Distributed computing,
Costs,
Educational institutions"
A survey of architecture and node deployment in Wireless Sensor Network,"Wireless Sensor Networks consist of small nodes with sensing, computation and wireless communication capabilities. Various architectures and node deployment strategies have been developed for wireless sensor network, depending upon the requirement of application. Sensor networks are used in different applications e.g. environmental monitoring, habitat monitoring, home automation, military application etc. In this paper we present survey of state-of-the-art of architecture and node deployment in wireless sensor network. We present the characteristics of the environment in which the sensor networks may deploy. Node deployment in wireless sensor network is application dependent and can be either deterministic or randomized. But in both the cases coverage of interested area is the main issue. We also explain the routing protocols for wireless sensor network.","wireless sensor networks,
routing protocols"
Image-set matching using a geodesic distance and cohort normalization,"An image-set based face recognition algorithm is proposed that exploits the full geometrical interpretation of Canonical Correlation Analysis (CCA). CCA maximizes the correlation between two linear subspaces associated with image-sets, where an image-set is assumed to contain multiple images of a person's face. When these linear subspaces are viewed as points on a Grassmann manifold, then geodesic distance on the manifold becomes the natural way to compare image-sets. The proposed method is tested on the ORL data set where it achieves a rank one identification rate of 98.75%. The proposed method is also tested on a subset of the Face Recognition Grand Challenge Experiment 4 data. Specifically, 82 probe and 230 gallery subjects with 32 images per probe and gallery image-set. Our algorithm achieves a rank one identification rate of 87% and a verification rate of 81% at a false accept rate of 1/1;000. These results on FRGC are significantly better than the well-known image-set matching algorithm, Mutual Subspace Method (MSM), which does not use geodesic distance. Another important finding is that cohort normalization boosts verification performance by 50% when used in conjunction with image-set matching. These results suggest that excellent levels of face recognition performance are possible when using image-sets, geodesic distance and cohort normalization. Finally, the proposed approach is generic in the sense that no training is required.","Face recognition,
Probes,
Testing,
Geometry,
Gabor filters,
Computer science,
Mathematics,
Image analysis,
Image color analysis,
Algorithm design and analysis"
Comparing performance of solid state devices and mechanical disks,"In terms of performance, solid state devices promise to be superior technology to mechanical disks. This study investigates performance of several up-to-date high-end consumer and enterprise Flash solid state devices (SSDs) and relates their performance to that of mechanical disks. For the purpose of this evaluation, the IOZone benchmark is run in single-threaded mode with varying request size and access pattern on an ext3 filesystem mounted on these devices. The price of the measured devices is then used to allow for comparison of price per performance. Measurements presented in this study offer an evaluation of cost-effectiveness of a Flash based SSD storage solution over a range of workloads. In particular, for sequential access pattern the SSDs are up to 10 times faster for reads and up to 5 times faster than the disks. For random reads, the SSDs provide up to 200× performance advantage. For random writes the SSDs provide up to 135× performance advantage. After weighting these numbers against the prices of the tested devices, we can conclude that SSDs are approaching price per performance of magnetic disks for sequential access patterns workloads and are superior technology to magnetic disks for random access patterns.","Solid state circuits,
Sequential analysis,
System testing,
Magnetic devices,
File systems,
Computer science,
Performance analysis,
Particle measurements,
Size measurement,
Performance evaluation"
Host-Centric Model Checking for Network Vulnerability Analysis,"Recent research has successfully applied model checking, a formal verification technique, to automatically generate chains of vulnerability exploits that an attacker can use to reach his goal. Due to the combinatorial explosion of the chain generation problem space, model checkers do not scale well to networks containing a large number of hosts. This paper proposes a methodology that uses a host-centric modeling approach together with a monotonicity assumption to alleviate the scalability problem of model checkers. We describe the proposed approach, its limitations, and show how it can reduce the time complexity of chain generation to a quadratic polynomial of the number of hosts, both theoretically and empirically. We also compare its advantages over similar customized graph-based approaches.","Network servers,
Tree graphs,
Explosions,
Scalability,
Polynomials,
Computer security,
Application software,
Computer science,
Logic testing,
Counting circuits"
A Just-In-Time Architectural Knowledge Sharing Portal,"In recent years, management of architectural knowledge has become a more prominent theme in software architecture research. Although various specialized tools have been proposed for use in the architecting process, observations show that architects in industry have yet to meet a tool environment that matches their knowledge needs. In order to discover what architectural knowledge needs architects have, we conducted a study in a large organization. In this study we discovered that architects are especially in need for 'just-in-time architectural knowledge'. To fulfill this need we designed and implemented an architectural knowledge sharing portal. Our portal's integrated functionality supports architects in their decision-making process, by providing easy access to the right architectural knowledge at any given point in time.","Portals,
Knowledge management,
Software architecture,
Decision making,
Computer architecture,
Computer science,
Conference management,
Programming,
Guidelines,
Connectors"
A Web-Based Mashup Environment for On-the-Fly Service Composition,"The web-based service composition, e.g. mashup, is becoming a popular style to reuse web services. From the perspective of reuse, existing work has limitations on qualifying whether the service or the service composition satisfies user requirements and adapting the service or composition according to the qualification results. For addressing these limitations, this paper proposes an on-the-fly approach to web-based service composition. Firstly, we do not distinguish the design-time and run-time of services and their composition so that they can be qualified in a what you see is what you get manner when services are selected or assembled. Secondly, we propose a component model for separating the service business and user interface so that they can be changed dynamically and independently in the adaptation of service selection and composition. This approach is demonstrated by a browser-based mashup tool.","Mashups,
Assembly,
Runtime,
Qualifications,
User interfaces,
Systems engineering and theory,
Educational technology,
Computer science education,
Systems engineering education,
Computer science"
Detecting obstacles and drop-offs using stereo and motion cues for safe local motion,"A mobile robot operating in an urban environment has to navigate around obstacles and hazards. Though a significant amount of work has been done on detecting obstacles, not much attention has been given to the detection of drop-offs, e.g., sidewalk curbs, downward stairs, and other hazards where an error could lead to disastrous consequences. In this paper, we propose algorithms for detecting both obstacles and drop-offs (also called negative obstacles) in an urban setting using stereo vision and motion cues. We propose a global color segmentation stereo method and compare its performance at detecting hazards against prior work using a local correlation stereo method. Furthermore, we introduce a novel drop-off detection scheme based on visual motion cues that adds to the performance of the stereo-vision methods. All algorithms are implemented and evaluated on data obtained by driving a mobile robot in urban environments.","Image edge detection,
Safety,
Robots,
Image segmentation,
Three dimensional displays,
Image color analysis,
Robot kinematics"
Netlist-level IP protection by watermarking for LUT-based FPGAs,"This paper presents a novel approach to watermark FPGA designs on the netlist level. We restrict the dynamically addressable part of the logic table, thus freeing space for insertion of signature bits into lookup tables (LUTs). In this way, we tightly integrate the watermark with the design so that simply removing mark carrying components would damage the intellectual property core. Converting functional LUTs to LUT-based RAMs or shift registers prevents deletion due to optimization. With this technique, we take watermark carrying components out of the scope of optimization algorithms to achieve complete transparency towards development environments. We can extract the marks from the bitfile of an FPGA. The method was tested on a Xilinx Virtex-II Pro FPGA and showed low overhead in terms of timing and resources at a reasonable number of water-marked cells.","Protection,
Watermarking,
Field programmable gate arrays,
Table lookup,
Hardware,
Intellectual property,
Security,
Computer science,
Logic,
Shift registers"
On the impact of limited-capacity backhaul and inter-users links in cooperative multicell networks,"Cooperation based technologies are expected to play a major role in future cellular or, more generally, infrastructure networks. Both multicell processing (cooperation at the base station level) and relaying (cooperation at the user level) are currently being studied. Here, recent works dealing with the performance of multicell processing and user cooperation under the assumption of error-free but limited-capacity inter-base station and inter-user links, respectively, are considered. The survey focuses on related results derived for non-fading uplink and downlink channels of simple cellular setups. The analytical treatment, facilitated by these simple models, enhances the insight into the limitations imposed by capacity constraints on the performance gains provided by cooperative techniques.","Decoding,
Land mobile radio cellular systems,
Interference,
Cellular networks,
Base stations,
Relays,
Downlink,
Receiving antennas,
Transmitting antennas,
Spine"
A brief introduction to the analysis and design of Networked Control Systems,"Networked Control has emerged in recent years as a new and exciting area in systems science. The topic has many potential applications in diverse areas ranging from control of microrobots to biological and economic systems. The supporting theory is very rich and combines aspects of control, signal processing, telecommunications and information theory. In this paper, we will give a brief overview of recent developments in Networked Control with an emphasis on our contributions. We also point to several open problems in this emerging area.","Signal to noise ratio,
Quantization,
Encoding,
Optimized production technology,
Stability analysis,
Control systems,
Additives"
"A cyborg beetle: Insect flight control through an implantable, tetherless microsystem","We present an implantable flight control microsystem for a cyborg beetle. The system consists of multiple inserted neural and muscular stimulators, a visual stimulator, a polyimide assembly and a microcontroller. The system is powered by two size 5 cochlear microbatteries. The insect platform is Cotinis texana, a 2 cm long, 1-2 gram Green June Beetle. We also provide data on the implantation of silicon neural probes, silicon chips, microfluidic tubes, and LED's introduced during the pupal stage of the beetle.","Insects,
Aerospace control,
Microcontrollers,
Probes,
Assembly,
Polyimides,
Muscles,
Silicon,
Wire,
Electrodes"
Static Execute After/Before as a replacement of traditional software dependencies,"The paper explores Static Execute After (SEA) dependencies in the program and their dual Static Execute Before (SEB) dependencies. It empirically compares the SEA/SEB dependencies with the traditional dependencies that are computed by System Dependence Graph (SDG) and program slicers. In our case study we use about 30 subject programs that were previously used by other authors in empirical studies of program analysis. We report two main results. The computation of SEA/SEB is much less expensive and much more scalable than the computation of the SDG. At the same time, the precision declines only very slightly, by some 4% on average. In other words, the precision is comparable to that of the leading traditional algorithms, while intuitively a much larger difference would be expected. The paper then discusses whether based on these results the computation of the SDG should be replaced in some applications by the computation of the SEA/SEB.","Software,
Algorithm design and analysis,
Software algorithms,
Image color analysis,
Flow graphs,
Computer architecture,
Software systems"
Verification of Knowledge-Based Systems Using Predicate/Transition Nets,"As expert-system technology gains broader acceptance, the need to build and maintain large-scale knowledge-based systems (KBSs) will assume greater importance. Traditional approaches to KBS verification generally contain no predicate/transition (PrT) net models, thus making them slow for the large-scale KBS with chained errors. This paper proposes an attractive alternative to KBS verification, in which the KBS is modeled as a PrT-net model. Then, the least fixpoint semantics of the PrT-net model can be introduced into the KBS for the purpose of speeding up the computations of the KBSs. The significance of this paper is that seven propositions are formulated to detect errors of redundancy, subsumption, unnecessary condition, circularity, inconsistency, dead end, and unreachable goal. Thus, the performance of a computer-aided-design tool for KBSs can be improved to some extent. Meanwhile, specification languages, including Programming in Logic, Frame-and-Rule-Oriented Requirements Specification Language, and the like, are suitable to this approach.","Knowledge based systems,
Logic programming,
Specification languages,
Computer science,
Costs,
Large-scale systems,
Computer errors,
Redundancy,
Standards development,
Design automation"
Avalanche-to-Streamer Transition in Particle Simulations,"The avalanche-to-streamer transition is studied and illustrated in a particle model. The results are similar to those of fluid models. However, when superparticles are introduced, numerical artifacts become visible. This underscores the need of models that are hybrid in space.","Electrons,
Computational modeling,
Ionization,
Atmospheric modeling,
Nitrogen,
Mathematics,
Computer science,
Physics,
Chaos,
Plasma applications"
Holographic Algorithms by Fibonacci Gates and Holographic Reductions for Hardness,We propose a new method to prove complexity dichotomy theorems. First we introduce Fibonacci gates which provide a new class of polynomial time holographic algorithms. Then we develop holographic reductions. We show that holographic reductions followed by interpolations provide a uniform strategy to prove \#P-hardness.,"Holography,
Computer science,
Polynomials,
Interpolation,
Bipartite graph,
Computational complexity"
Pseudolikelihood EM for Within-network Relational Learning,"In this work, we study the problem of \emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting:  disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \emph{collective learning} and \emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.","Predictive models,
Inference algorithms,
Computer science,
Autocorrelation,
Data mining,
Statistics,
Training data,
Semisupervised learning,
Testing"
Scientific Software as Workflows: From Discovery to Distribution,"Scientific workflows-models of computation that capture the orchestration of scientific codes to conduct in silico research-are gaining recognition as an attractive alternative to script-based orchestration. Even so, researchers developing scientific workflow technologies still face fundamental challenges, including developing the underlying science of scientific workflows. You can classify scientific-workflow environments according to three major phases of in silico research: discovery, production, and distribution. On the basis of this classification, scientists can make more-informed decisions regarding the adoption of particular workflow environments.","Propulsion,
Instruments,
Mathematical model,
NASA,
Gas insulated transmission lines,
Chemical technology,
Space technology,
Testing,
Computer simulation,
Distributed computing"
Evolution of image filters on graphics processor units using Cartesian Genetic Programming,"Graphics processor units are fast, inexpensive parallel computing devices. Recently there has been great interest in harnessing this power for various types of scientific computation, including genetic programming. In previous work, we have shown that using the graphics processor provides dramatic speed improvements over a standard CPU in the context of fitness evaluation. In this work, we use Cartesian Genetic Programming to generate shader programs that implement image filter operations. Using the GPU, we can rapidly apply these programs to each pixel in an image and evaluate the performance of a given filter. We show that we can successfully evolve noise removal filters that produce better image quality than a standard median filter.","Pixel,
Noise,
Graphics,
Genetic programming,
Field programmable gate arrays,
Evolution (biology),
Evolutionary computation"
A lower-bound on the number of rankings required in recommender systems using collaborativ filtering,"We consider the situation where users rank items from a given set, and each user ranks only a (small) subset of all items. We assume that users can be classified into C classes, and users in a given class c have the same ranking for all items. For this situation we are interested in the following question. As a function of the number of users N in a given class c and the numbers of items IN to be ranked, how many rankings mN per user are needed in order to be able to correctly identify all user in class c? This question is of interest because correctly identifying all users in a class allows to accurately predict the ranking of an item by a given user that the user has not ranked, but that was ranked by another user in the same class. This is exactly the goal recommender systems using collaborative filtering. Therefore, being able to answer the above questions allows us to characterize how much data (i.e. how many rankings per user) is required by a recommender system using collaborative filtering to accurately predict user-item ranking pairs. We study the above question using a random graph model. Even though the resulting random graph is not a Erdos-Renyi graph, this allows us to use for our analysis similar techniques that have been developed for the analysis of Erdos-Renyi graphs.","Recommender systems,
Collaboration,
Filtering,
Books,
Motion pictures,
Computer science,
H infinity control,
Social network services,
Collaborative work,
Algorithm design and analysis"
Algorithms for Single-Source Vertex Connectivity,"In the Survivable Network Design Problem (SNDP) the goal is tofind a minimum cost subset of edges that satisfies a given set ofpairwise connectivity requirements among the vertices. Thisgeneral network design framework has been studied extensively andis tied to the development of major algorithmic techniques. Forthe edge-connectivity version of the problem, a
2
-approximationalgorithm is known for arbitrary pairwise connectivityrequirements. However, no non-trivial algorithms are known for itsvertex connectivity counterpart. In fact, even highly restrictedspecial cases of the vertex connectivity version remain poorlyunderstood.We study the single-source
k
-vertex connectivity version ofSNDP. We are given a graph
G(V,E)
with a subset
T
of terminalsand a source vertex
s
, and the goal is to find a minimum costsubset of edges ensuring that every terminal is
k
-vertexconnected to
s
. Our main result is an
O(klogn)
-approximation algorithm for this problem; this improves uponthe recent
2
O(
k
2
)
log
4
n
-approximation. Our algorithm isbased on an intuitive rerouting scheme. The analysis relies on astructural result that may be of independent interest: we showthat any solution can be decomposed into a disjoint collection ofmultiple-legged spiders, which are then used to re-route flow fromterminals to the source via other terminals.We also obtain the first non-trivial approximation algorithm forthe vertex-cost version of the same problem, achieving an
O(
k
7
log
2
n)
-approximation.","Approximation algorithms,
Algorithm design and analysis,
Iterative algorithms,
Costs,
Joining processes,
Computer science,
Tree graphs,
Polynomials"
Hiding I/O latency with pre-execution prefetching for parallel applications,"Parallel applications are usually able to achieve high computational performance but suffer from large latency in I/O accesses. I/O prefetching is an effective solution for masking the latency. Most of existing I/O prefetching techniques, however, are conservative and their effectiveness is limited by low accuracy and coverage. As the processor-I/O performance gap has been increasing rapidly, data-access delay has become a dominant performance bottleneck. We argue that it is time to revisit the “I/O wall” problem and trade the excessive computing power with data-access speed. We propose a novel pre-execution approach for masking I/O latency. We describe the pre-execution I/O prefetching framework, the pre-execution thread construction methodology, the underlying library support, and the prototype implementation in the ROMIO MPI-IO implementation in MPICH2. Preliminary experiments show that the pre-execution approach is promising in reducing I/O access latency and has real potential.","Delay,
Prefetching,
Computer science,
Concurrent computing,
Application software,
Yarn,
Libraries,
Parallel processing,
File systems,
Throughput"
Topographic Class Grouping with applications to 3D object recognition,"The cerebral cortex uses a large number of top-down connections, but the roles of the top-down connections remain unclear. Through end-to-end (sensor-to-motor) multilayered networks that use three types of connections (bottom-up, lateral, and top-down), the new Topographic Class Grouping (TCG) mechanism shown in this paper explains how the top-down connections influence (1) the type of feature detectors (neurons) developed and (2) their placement in the neuronal plane. The top-down connections boost the variations in the neuronal between class directions during the training phase. The first outcome of this top-down boosted input space is the facilitation of the emergence of feature detectors that are purer, measured statistically by the average entropy of the neurons’ development. The relatively purer neurons are more “abstract,” i.e., characterizing class-specific (or motor-specific) input information, resulting in better classification rates. The second outcome of this top-down boosted input space is the increase of the distance between input samples that belong to different classes, resulting in a farther separation of neurons according to their class. Therefore, neurons that respond to the same class become relatively nearer. This results in TCG, measured statistically by a smaller within-class scatter of responses when the neuronal plane has a fixed size. Although these mechanisms are potentially applicable to any pattern recognition applications, we report quantitative effects of these mechanisms for 3D object recognition of center-normalized, background-controlled objects. TCG has enabled a significant reduction of the recognition errors.","Neurons,
Object recognition,
Manifolds,
Sensors,
Three dimensional displays,
Training,
Distance measurement"
Thermal-aware reliability analysis for Platform FPGAs,"Increasing levels of integration in Field Programmable Gate Arrays, have resulted in high on-chip power densities, and temperatures. The heterogeneity of components and scaled feature sizes in Platform FPGAs have made them vulnerable to various temperature dependent failure mechanisms. Hence, we need to introduce temperature awareness in tackling such failures that affect the lifetime reliability of FPGAs. In this paper, we present a Dynamic Thermal-aware Reliability Management (DTRM) framework to analyze the impact of temperature variations on the longterm/lifetime reliability of Platform FPGAs. We first study the temperature variations, both across and with-in designs, due to the use of various hard-blocks within a 65nm Platform FPGA. In the presence of such variations, we demonstrate the vulnerability of Platform FPGAs to two different hard-failures, namely, Electromigration, and Time Dependent Dielectric Breakdown (TDDB). We also analyze the performance degradation caused by Negative Bias Temperature Instability (NBTI) in the presence of thermal-variations. We validate the temperature variations estimated by the DTRM framework using a ring oscillator based real-time temperature measurement technique.",
MIMO cognitive radio: A game theoretical approach,"The concept of cognitive radio has recently received great attention from the researcherspsila community as a promising paradigm to achieve efficient use of the frequency resource by allowing the co-existence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper we propose and analyze a totally decentralized approach, based on game theory, to design cognitive MIMO transceivers. We consider underlay/interweave networks, where primary users establish proper null and/or soft shaping constraints on the transmit covariance matrix of secondary users, so that the interference generated by secondary users be confined within the interference-temperature limits. We formulate the resource allocation problem among secondary users as a strategic noncooperative game, where each transmit/receive pair competes against the others to maximize the information rate over its own MIMO channel, under transmit power and/or null/soft shaping constraints. We first characterize the Nash equilibria of the proposed game, showing that they can be equivalently rewritten as the solutions of a MIMO waterfilling nonlinear fixed-point equation. Based on this result, we then design low-complex asynchronous distributed algorithms that converge to the Nash equilibria of the games.","Games,
MIMO,
Covariance matrix,
Interference,
Equations,
Game theory,
Cognitive radio"
Time-series clustering by approximate prototypes,"Clustering time-series data poses problems, which do not exist in traditional clustering in Euclidean space. Specifically, cluster prototype needs to be calculated, where common solution is to use cluster medoid. In this work, we define an optimal prototype as an optimization problem and propose a local search solution to it. We experimentally compare different time-series clustering methods and find out that the proposed prototype with agglomerative clustering followed by k-means algorithm provides best clustering accuracy.","Prototypes,
Clustering methods,
Shape,
Euclidean distance,
Speech processing,
Image processing,
Computer science,
Statistics,
Clustering algorithms,
Bioinformatics"
Channel models for wireless body area networks,"Wireless patient monitoring using wearable sensors is a promising application. This paper provides stochastic channel models for wireless body area network (WBAN) on the human body. Parameters of the channel models are extracted from measured channel transfer functions (CTFs) in a hospital room. Measured frequency bands are selected so as to include permissible bands for WBAN; ultra wideband (UWB), the industry, science and medical (ISM) bands, and wireless medical telemetry system (WMTS) bands. As channel models, both a path loss model and a power delay profile (PDP) model are considered. But, even though path loss models are derived for the all frequency bands, PDP model is only for the UWB band due to the highly frequency selectiveness of UWB channels. The parameters extracted from the measurement results are summarized for each channel model.","Computer Communication Networks,
0,
0,
1,
0,
0,
Electronics, Medical,
0,
0,
1,
0,
0,
Humans,
0,
0,
1,
0,
0,
Models, Theoretical,
0,
0,
1,
0,
0,
Monitoring, Ambulatory,
0,
0,
1,
0,
0,
Telemetry,
0,
0,
1,
0,
0"
Accelerating Nussinov RNA secondary structure prediction with systolic arrays on FPGAs,"RNA structure prediction, or folding, is a compute-intensive task that lies at the core of several search applications in bioinformatics. We begin to address the need for high-throughput RNA folding by accelerating the Nussinov folding algorithm using a 2D systolic array architecture. We adapt classic results on parallel string parenthesization to produce efficient systolic arrays for the Nussinov algorithm, elaborating these array designs to produce fully realized FPGA implementations. Our designs achieve estimated speedups up to 39× on a Xilinx Virtex-II 6000 FPGA over a modern x86 CPU.","Pipeline processing,
RNA,
Arrays,
Field programmable gate arrays,
Algorithm design and analysis,
Pipelines,
Schedules"
Congestion Avoidance and Fairness in Wireless Sensor Networks,"Designing a sensor network congestion avoidance algorithm is a challenging task due to the application specific nature of these networks. The frequency of event sensing is a deciding factor in the occurence of congestion. Numerous sensors, simultaneously transmitting data, increase the probability of packet drops due to congestion close to the base station(s). In this paper, we propose a novel distributed congestion avoidance algorithm which uses the ratio of the number of downstream and upstream nodes along with available queue sizes of the downstream nodes to detect incipient congestion. Monitoring queue sizes of candidate downstream nodes helps ensure effective load balancing and fairness in our avoidance algorithm. Through simulation studies we observe a greater packet delivery ratio and higher network lifetime in comparison with other prevalent mechanisms.","Wireless sensor networks,
Routing,
Monitoring,
Sensor phenomena and characterization,
Chromium,
Load management,
Buffer overflow,
Protocols,
Microstrip,
Computer science"
Proportional fair scheduling in relay enhanced cellular OFDMA systems,We propose an efficient proportional fair (PF) scheduling algorithm for multi-user OFDMA systems employing fixed relays. An PF metric maximization problem is first formulated for relay enhanced OFDMA cellular systems. A two-step algorithm is then proposed to solve the problem; the user routing step and the resource allocation step. The simulation results confirm the near-optimal performance of the proposed algorithm in PF sense.,
feedback free DVC architecture using machine learning,"Most of the reported Distributed Video Coding (DVC) architectures have a serious limitation that hinders its practical application. The uses of a feedback channel between the encoder and the decoder require an interactive decoding procedure which is a limitation for applications such as offline processing. On the other hand, the decoder needs an efficient way to estimate the probability of error without assuming the availability of the original video at the decoder. In this paper we continue with our previous works into a more practical DVC architecture which solves both problems based on the use of machine learning. The proposed approach is based on extracting the relationships that exist between the residual frame and the number of requests over this feedback channel. We apply these concepts to pixel-domain Wyner-Ziv coding demonstrating significant savings in bitrates with a little loss of quality.","Feedback,
Machine learning,
Iterative decoding,
Computer architecture,
Bit error rate,
Video coding,
Wireless sensor networks,
Cameras,
Informatics,
Computer science"
Copy or Discard execution model for speculative parallelization on multicores,"The advent of multicores presents a promising opportunity for speeding up sequential programs via profile-based speculative parallelization of these programs. In this paper we present a novel solution for efficiently supporting software speculation on multicore processors. We propose the Copy or Discard (CorD) execution model in which the state of speculative parallel threads is maintained separately from the nonspeculative computation state. If speculation is successful, the results of the speculative computation are committed by copying them into the non-speculative state. If misspeculation is detected, no costly state recovery mechanisms are needed as the speculative state can be simply discarded. Optimizations are proposed to reduce the cost of data copying between nonspeculative and speculative state. A lightweight mechanism that maintains version numbers for non-speculative data values enables misspeculation detection. We also present an algorithm for profile-based speculative parallelization that is effective in extracting parallelism from sequential programs. Our experiments show that the combination of CorD and our speculative parallelization algorithm achieves speedups ranging from 3.7 to 7.8 on a Dell PowerEdge 1900 server with two Intel Xeon quad-core processors.","Multicore processing,
Yarn,
Concurrent computing,
Parallel processing,
Cost function,
Partitioning algorithms,
Computer science,
Data mining"
Improved differential evolution for dynamic optimization problems,"This article reports improvements on DynDE, a approach to using Differential Evolution to solve dynamic optimization problems. Three improvements are suggested, namely favored populations, migrating individuals and a combination of these approaches. The effects of varying the change frequency, peak widths and the number of dimensions of the dynamic environment are investigated. Experimental results are presented that indicate that the suggested approaches constitute considerable improvements on previous research.","Heuristic algorithms,
Evolution (biology),
Optimization,
Benchmark testing,
Tracking,
Evolutionary computation,
Correlation"
An Efficient Algorithm for Computing the Reliability of Consecutive-k-Out-Of-n:F Systems,"Many algorithms for computing the reliability of linear or circular consecutive-k-out-of-n:F systems appeared in this Transactions. The best complexity estimate obtained for solving this problem is O(k3 log(n/k)) operations in the case of i.i.d. components. Using fast algorithms for computing a selected term of a linear recurrence with constant coefficients, we provide an algorithm having arithmetic complexity O(k log (k) log(log(k)) log(n)+komega) where 2<omega< 3 is the exponent of linear algebra. This algorithm holds generally for linear, and circular consecutive-k-out-of-n:F systems with independent but not necessarily identical components.","Polynomials,
Linear algebra,
Difference equations,
Random variables,
Petroleum,
Mathematics,
Computer science,
Digital arithmetic,
Application software"
Logic Minimization and Testability of 2-SPP Networks,"The 2-SPP networks are three-level EXOR-AND-OR forms, with EXOR gates being restricted to fan-in 2. This paper presents a heuristic algorithm for the synthesis of these networks in a form that is fully testable in the stuck-at fault model (SAFM). The algorithm extends the EXPAND-IRREDUNDANT-REDUCE paradigm of ESPRESSO in heuristic mode, and it iterates local minimization and reshape of a solution until no further improvement can be achieved. This heuristic could escape from local minima using a LAST_GASP-like procedure. Moreover, the testability of 2-SPP networks under the SAFM is studied, and the notion of EXOR-irredundancy is introduced to prove that the computed 2-SPP networks are fully testable under the SAFM. Finally, this paper reports a large set of experiments showing high-quality results with affordable run times, handling also examples whose exact solutions could not be computed.",
Automatic modularity conformance checking,"According to Parnas's information hiding principle and Baldwin and Clark's design rule theory, the key step to decomposing a system into modules is to determine the design rules (or in Parnas's terms, interfaces) that decouple otherwise coupled design decisions and to hide decisions that are likely to change in independent modules. Given a modular design, it is often difficult to determine whether and how its implementation realizes the designed modularity. Manually comparing code with abstract design is tedious and error-prone. We present an automated approach to check the conformance of implemented modularity to designed modularity, using design structure matrices as a uniform representation for both. Our experiments suggest that our approach has the potential to manifest the decoupling effects of design rules in code, and to detect modularity deviation caused by implementation faults. We also show that design and implementation models together provide a comprehensive view of modular structure that makes certain implicit dependencies within code explicit.","Algorithm design and analysis,
Computer science,
Software maintenance,
Permission,
Data structures,
Clustering algorithms,
Fault detection,
Software engineering,
Software architecture,
Software systems"
Anonymizing Streaming Data for Privacy Protection,"In many applications, transaction data arrive in the form of high speed data streams. These data contain a lot of information about customers, not just transactions, and thus have to be carefully managed to protect customers' privacy. This paper presents a novel method called SKY (Stream K-anonYmity) to continuously facilitate k-anonymity on data streams. Experimental results show that SKY is efficient and effective.","Data privacy,
Protection,
Joining processes,
Books,
Computer science,
Intelligent systems,
Application software,
Wireless sensor networks,
Engines,
Telephony"
Energy Optimal Scheduling on Multiprocessors with Migration,"We show that the problem of finding an energy minimal schedule for execution of a collection of jobs on a multiprocessor with job migration allowed has polynomial complexity. Each job is specified by a release time, a deadline, and an amount of work to be performed. All of the  processors have the same, convex power-speed trade-off of the form P = phi(s), where P is power, s is speed, and phi is convex.  Unlike previous work on multiprocessor scheduling, we place no restriction  on the release times, deadlines, or amount of work to be done. We show that the scheduling problem is convex, and give an algorithm based on linear programming. We show that the optimal schedule is the same for any convex power-speed trade-off function.",
WCET-driven Cache-based Procedure Positioning Optimizations,"Procedure Positioning is a well known compiler opti-mization aiming at the improvement of the instruction cachebehavior. A contiguous mapping of procedures callingeach other frequently in the memory avoids overlapping ofcache lines and thus decreases the number of cache conflictmisses. In standard literature, these positioning techniquesare guided by execution profile data and focus on an im-proved average-case performance.We present two novel positioning optimizations drivenby worst-case execution time (WCET) information to effec-tively minimize the program's worst-case behavior. WCETreductions by 10% on average are achieved. Moreover, acombination of positioning and the WCET-driven Proce-dure Cloning optimization proposed in [14] is presented im-proving the WCET analysis by 36% on average.","program diagnostics,
cache storage,
optimising compilers"
Color Visual Cryptography Scheme Using Meaningful Shares,"Visual cryptography (VC) schemes hide the secret image into two or more images which are called shares. The secret image can be recovered simply by stacking the shares together without any complex computation involved. The shares are very safe because separately they reveal nothing about the secret image. In this paper, a color visual cryptography scheme producing meaningful shares is proposed. These meaningful shares will not arouse the attention of hackers. The proposed scheme utilizes the halftone technique, cover coding table and secret coding table to generate two meaningful shares. The secret image can be decrypted by stacking the two meaningful shares together. Experimental results have demonstrated that the new scheme is perfectly applicable and achieves a high security level.","Cryptography,
Pixel,
Virtual colonoscopy,
Computer science,
Stacking,
Internet,
Design engineering,
Cities and towns,
Information security,
Intelligent systems"
Exploring Parallel I/O Concurrency with Speculative Prefetching,"Parallel applications can benefit greatly from massive computational capability, but their performance usually suffers due to large latency in I/O accesses. Conventional I/O prefetching techniques are conservative and are limited by low accuracy and coverage. As the processor performance has been increasing rapidly and the computing power is virtually free, we introduce a novel speculative approach for comprehensive and aggressive parallel I/O prefetching in this study. We present the design of our approach as well as challenges, solutions, and our prototype implementation. The experiments have shown promising results in reducing I/O access latency.","Prefetching,
Libraries,
File systems,
Algorithm design and analysis,
Construction industry,
Bandwidth,
Computer science"
Guided Problem Diagnosis through Active Learning,"There is widespread interest today in developing tools that can diagnose the  cause of a system failure accurately and efficiently based on monitoring data collected from the system. Over time, the system monitoring data will contain two types of failure data: (i) annotated failure data L, which is monitoring data collected from failure states of the system, where the cause of failure has been diagnosed and attached as annotations with the data; and (ii) unannotated failure data U. Previous work on wholly- or partially-automated diagnosis focused on L or U in isolation. In this paper, we argue that it is important to consider both L and U together to improve the overall accuracy  of diagnosis; and in particular, to proactively move instances from U to  L.  However, such movement requires manual diagnosis effort from system  administrators. Since manual diagnosis is expensive and time-consuming, we propose an algorithm to make the best use of manual effort while maximizing  the benefit gained from newly diagnosed instances. We report an experimental evaluation of our algorithm using data from a variety  of failures---both single failures and multiple correlated failures---injected in a testbed, as well as with synthetic data.",
User evaluation of see-through vision for mobile outdoor augmented reality,"We have developed a system built on our mobile AR platform that provides users with see-through vision, allowing visualization of occluded objects textured with real-time video information. We present a user study that evaluates the user’s ability to view this information and understand the appearance of an outdoor area occluded by a building while using a mobile AR computer. This understanding was compared against a second group of users who watched video footage of the same outdoor area on a regular computer monitor. The comparison found an increased accuracy in locating specific points from the scene for the outdoor AR participants. The outdoor participants also displayed more accurate results, and showed better speed improvement than the indoor group when viewing more than one video simultaneously.","Augmented reality,
Visualization,
Accuracy,
Streaming media,
Cameras,
Three dimensional displays,
Buildings"
Isotropic PCA and Affine-Invariant Clustering,"We present an extension of Principal Component Analysis (PCA) and a new algorithm for clustering points in \R^n
based on it. The key property of the algorithm is that it is affine-invariant. When the input is a sample from a mixture of two arbitrary Gaussians, the algorithm correctly classifies the sample assuming only that the two components are separable by a hyperplane, i.e., there exists a halfspace that contains most of one Gaussian and almost none of the other in probability mass. This is nearly the best possible, improving known results substantially. For k≫2 components, the algorithm requires only that there be some (k-1)-dimensional subspace in which the ``overlap'' in every direction is small. Our main tools are isotropic transformation, spectral projection and a simple reweighting technique. We call this combination isotropic PCA.","Principal component analysis,
Gaussian processes,
Clustering algorithms,
Pattern recognition,
Covariance matrix,
Computer science,
Gaussian distribution,
Labeling,
Polynomials"
Power reduction techniques for Dynamically Reconfigurable Processor Arrays,"The power consumption of Dynamically Reconfigurable Processing Array (DRPA) is quantitatively analyzed by using a real chip layout and applications taking into account the reconfiguration power. Evaluation result shows that processing power for PEs is dominant and reconfiguration power is about 20.7% of the total dynamic power consumption. Based on the above evaluation results, we proposed two dynamic power reduction techniques: functional unit-level operand isolation and selective context fetch. Evaluation results demonstrate that the functional unit-level operand isolation can reduce up to 20.8% of the dynamic power with only 2.2% area overhead. On the selective context fetch, the power reduction is limited by the increasing of the additional hardware.","Switches,
Clocks,
Power demand,
Arrays,
Hardware,
Routing,
Layout"
Integrating mobile devices into the computer science curriculum,"Mobile devices such as cellular phones and smart personal digital assistants out-ship personal computers (PCs) 20 to 1, and for many students the mobile device is becoming the computer. Such devices are becoming more powerful than the PCs of twenty years ago and they represent a useful tool for conveying important computer science concepts. This calls for innovations in the computer science curriculum, not only in some specific courses but across the curriculum to create a motivating framework for computer science students. After all, students expect faculty to integrate leading edge technology in the classroom. Here we present our approach for integrating mobile devices into the Computer Science curriculum, supported by an example of our experience in integrating BlackBerry devices into two programming courses, a distributed systems course, and senior capstone projects. Some of the courses are lab-intensive where students experiment with the devices, and develop and deploy applications for them. Teaching computer science and programming in the context of mobile applications provides a motivating framework for students and inspires them to excel due to the practical experience they gain allowing them to develop applications for their own mobile devices.","Mobile computing,
Computer science,
Application software,
Personal communication networks,
Cellular phones,
Personal digital assistants,
Microcomputers,
Technological innovation,
Handheld computers,
Student experiments"
Compressed sensing for OFDM/MIMO radar,"In passive radar, two main challenges are: mitigating the direct blast, since the illuminators broadcast continuously, and achieving a large enough integration gain to detect targets. While the first has to be solved in part in the analog part of the processing chain, due to the huge difference of signal strength between the direct blast and weak target reflections, the second is about combining enough signal efficiently, while not sacrificing too much performance. When combining this setup with digital multicarrier waveforms like orthogonal frequency division multiplex (OFDM) in digital audio/video broadcast (DAB/DVB), this problem can be seen to be a version of multiple-input multiple-output (MIMO) radar. We start with an existing approach, based on efficient fast Fourier transform (FFT) operation to detect target signatures, and show how this approach is related to a standard matched filter approach based on a piece-wise constant approximation of the phase rotation caused by Doppler shift. We then suggest two more applicable algorithms, one based on subspace processing and one based on sparse estimation. We compare these various approaches based on a detailed simulation scenario with two closing targets and experimental data recorded from a DAB network in Germany.","Compressed sensing,
OFDM,
MIMO,
Radar detection,
Signal processing,
Digital video broadcasting,
Passive radar,
Reflection,
Frequency division multiplexing,
Digital audio broadcasting"
Measurement and Analysis of Interconnect Crosstalk Due to Single Events in a 90 nm CMOS Technology,The presence of single event (SE) induced interconnect crosstalk has been measured and demonstrated experimentally using single and two photon laser absorption techniques in the IBM 90 nm CMOS9SF process. The dependency of SE interconnect crosstalk on the interconnect length and on the amount of deposited charge has been quantified through 3D mixed-mode simulations at this technology for two different supply voltages. Experimental and simulation results show this effect to increase the cross-section susceptible to SEs requiring careful design considerations to assure desired hardness levels.,"CMOS technology,
Crosstalk,
Circuits,
Capacitance,
Computational modeling,
Voltage,
Single event upset,
CMOS process,
Photonics,
Pulsed laser deposition"
BAKE: A Balanced Kautz Tree Structure for Peer-to-Peer Networks,"In order to improve scalability and reduce maintenance overhead for structured peer-to-peer systems, researchers design optimal architectures with constant degree and logarithmical diameter. The expected topologies, however, require the number of peers to be some given values determined by the average degree and the diameter. Hence, existing designs fail to address the issue due to the fact that (1) we cannot guarantee how many peers to join a P2P system at a given time, and (2) a P2P system is typically dynamic with peers frequently coming and leaving. In this work, we propose BAKE scheme based on balanced Kautz tree structure with logdn in diameter and constant degree even the number of peers is an arbitrary value. Resources that are similar in single or multi-dimensional attributes space are stored on a same peer or neighboring peers. Through formal analysis and comprehensive simulations, we show that BAKE achieves optimal diameter and good connectivity as the Kautz digraph does. Indeed, the concepts of balanced Kautz tree introduced in this work can also be extended and applied to other interconnection networks after minimal modifications, for example, de Bruijn digraph.","Tree data structures,
Peer to peer computing,
Topology,
Robustness,
Routing,
Algorithm design and analysis,
Computer science,
Scalability,
Analytical models,
Multiprocessor interconnection networks"
A Methodology and Framework for Creating Domain-Specific Development Infrastructures,"Domain-specific architectures, middleware platforms, and analysis techniques leverage domain knowledge to help engineers build systems more effectively. An integrated set of these elements is called a domain-specific development infrastructure (DSDI). DSDIs are commonly created in a costly, ad-hoc fashion because current model-driven engineering (MDE) technologies lack sufficient mechanisms for capturing the semantics of domain concepts. In this paper, we propose a methodology for incorporating semantics within MDE frameworks to simplify and automate DSDI integration. We also present and evaluate a framework, called XTEAM, that implements our approach, resulting in structured processes and enforceable guidelines for DSDI integration. We have applied our approach to several DSDIs, and report on the benefits accrued.","Analytical models,
Middleware,
Computer architecture,
Connectors,
Computational modeling,
Manuals,
Construction industry"
Distributed coordination algorithms for multiple fractional-order systems,"This paper studies distributed coordination algorithms for multiple fractional-order systems over a directed communication graph. A general fractional-order consensus model is introduced by summarizing three different cases: (i) fractional-order agent dynamics with integer-order consensus algorithms, (ii) fractional-order agent dynamics with fractional-order consensus algorithms, and (iii) integer-order agent dynamics with fractional-order consensus algorithms. We show sufficient conditions on the communication graph and the fractional order such that consensus can be achieved using the general model. The consensus equilibrium is also given explicitly. In addition, we characterize the relationship between the number of agents and the fractional order to ensure consensus. Furthermore, we compare the convergence speed of consensus for fractional-order systems with that for integer-order systems. It is shown that the convergence speed of the fractional-order consensus algorithms can be improved by varying the fractional orders with time. Finally, simulation results are presented as a proof of concept.","Vehicle dynamics,
Convergence,
Heuristic algorithms,
Fluid dynamics,
Communication system control,
Control systems,
Sufficient conditions,
History,
Multiagent systems,
Vehicles"
Yet another approach to compositional synthesis of discrete event systems,"A two-pass algorithm for compositional synthesis of modular supervisors for large-scale systems of composed finite-state automata is proposed. The first pass provides an efficient method to determine whether a supervisory control problem has a solution, without explicitly constructing the synchronous composition of all components. If a solution exists, the second pass yields an over-approximation of the least restrictive solution which, if nonblocking, is a modular representation of the least restrictive supervisor. Using a new type of equivalence of nondeterministic processes, called synthesis equivalence, a wide range of abstractions can be employed to mitigate state-space explosion throughout the algorithm.","Discrete event systems,
Conferences"
Combining Virtual Machine migration with process migration for HPC on multi-clusters and Grids,"The renewed interest in virtualization gives rise to new opportunities for running High Performance Computing (HPC) applications on clusters and Grids. These include the ability to create a uniform (virtual) run-time environment on top of a multitude of hardware and software platforms, and the possibility for dynamic resource allocation towards the improvement of process performance, e.g., by Virtual Machine (VM) migration as a means for load-balancing. This paper deals with issues related to running HPC applications on multi-clusters and Grids using VMware, a virtualization package running on Windows, Linux and OS X. The paper presents the “Jobrun” system for transparent, on-demand VM launching upon job submission, and its integration with the MOSIX cluster and Grid management system. We present a novel approach to job migration, combining VM migration with process migration using Jobrun, by which it is possible to migrate groups of processes and parallel jobs among different clusters in a multi-cluster or in a Grid. We use four real HPC applications to evaluate the overheads of VMware (both on Linux and Windows), the MOSIX cluster extensions and their combination, and present detailed measurements of the performance of Jobrun.","Linux,
Virtual machining,
Memory management,
Servers,
Resource management,
Software,
Hardware"
Web Services Discovery Based on Latent Semantic Approach,"With an ever-increasing number of Web services being available, finding desired Web service is crucial for service users. Current keyword search and most existing approaches are inefficient in two main aspects: poor scalability and lack of semantics. Firstly, users are overwhelmed by the huge number of irrelevant services returned. Secondly, the intentions of users and the semantics in Web services are ignored. Inspired by the success of the Divide and Conquer approach used to handle the complex information decomposition, we use a novel approach to partition a large set of search results into a set of smaller groups by employing a clustering approach. Then we utilize Singular Value Decomposition (SVD) to capture the main semantics hidden behind the words in a query and the descriptions in the services, so that service matching can be carried out at the concept level. We report here on the preliminary experimental evaluation that shows improvements overall precision.","Web services,
Scalability,
Natural languages,
Ontologies,
Humans,
Singular value decomposition,
Web and internet services,
Computer science,
Mathematics,
Helium"
0.374 Pflop/s trillion-particle kinetic modeling of laser plasma interaction on roadrunner,"We demonstrate the outstanding performance and scalability of the VPIC kinetic plasma modeling code on the heterogeneous IBM Roadrunner supercomputer at Los Alamos National Laboratory. VPIC is a three-dimensional, relativistic, electromagnetic, particle-in-cell (PIC) code that self-consistently evolves a kinetic plasma. VPIC simulations of laser plasma interaction were conducted at unprecedented fidelity and scale—up to 1.0 × 1012 particles on as many as 136 × 106 voxels—to model accurately the particle trapping physics occurring within a laser-driven hohlraum in an inertial confinement fusion experiment. During a parameter study of laser reflectivity as a function of laser intensity under experimentally realizable hohlraum conditions [1], we measured sustained performance exceeding 0.374 Pflop/s (s.p.) with the inner loop itself achieving 0.488 Pflop/s (s.p.). Given the increasing importance of data motion limitations, it is notable that this was measured in a PIC calculation—a technique that typically requires more data motion per computation than other techniques (such as dense matrix calculations, molecular dynamics N-body calculations and Monte-Carlo calculations) often used to demonstrate supercomputer performance. This capability opens up the exciting possibility of using VPIC to model, from first-principles, an issue critical to the success of the multi-billion dollar DOE/NNSA National Ignition Facility.","Kinetic theory,
Laser modes,
Plasma simulation,
Laser fusion,
Laser theory,
Supercomputers,
Scalability,
Laboratories,
Plasma confinement,
Physics"
Large Workspace Haptic Devices - A New Actuation Approach,"Large workspace haptic devices have unique requirements, requiring increased power capabilities along with increased safety considerations. While there are numerous haptic devices available, large workspace systems are hampered by the limitations of current actuation technology. To address this, the Distributed Macro-Mini (DM2) actuation method has been applied to the design of a large workspace haptic device. In this paper, the DM2 method is described and we present experimental results which demonstrate its effectiveness. Finally, the control design is presented along with a discussion of the unique challenges associated with its robustness.",
Undersea wireless sensor network for ocean pollution prevention,"The ability to effectively communicate underwater has numerous applications, such as oceanographic data collection, pollution monitoring, disaster prevention, assisted navigation, tactical surveillance applications, and exploration of natural underwater sea resources. In this paper, we have developed a completely decentralized ad-hoc wireless sensor network for the ocean pollution detection. We mainly emphasize on the deployment of sensors, protocol stack, the synchronization algorithm and the routing algorithm in order to maximize the lifetime of the network and also to improve its Quality of Service (QoS).","Wireless sensor networks,
Oceans,
Marine pollution,
Petroleum,
Routing protocols,
Fault location,
Rivers,
Quality of service,
Monitoring,
Accidents"
A subthreshold single ended I/O SRAM cell design for nanometer CMOS technologies,"Lowering supply voltage is an effective technique for power reduction in memory design, however traditional memory cell design fails to operate, as shown in [3], [10], at ultra-low voltages. Therefore, to operate cells in the subthreshold regime, new cell structures needs to be explored. Towards this, we present a single-ended I/O (SEIO) bit-line latch style 7-transistor static random access memory (SRAM) cell (7T-LSRAM) as an alternative for nanometer CMOS technology which can function in ultra-low voltage regime. Compared to existing 6-transistor (6T) cell or 10-transistor cell design, the proposed cell has 2X improved read stability and 36% better write-ability at lower supply voltage. Furthermore, the 7T-LSRAM has improved process variation tolerance. The area analysis shows that there is 18% increase in area penalty compared to the standard 6T cell, however the improved performance and process variation tolerance could justify the overhead.","Random access memory,
Inverters,
Transistors,
Noise,
Stability analysis,
Logic gates,
Circuit stability"
Motion planning for steerable needles in 3D environments with obstacles using rapidly-exploring Random Trees and backchaining,"Steerable needles composed of a highly flexible material and with a bevel tip offer greater mobility compared to rigid needles for minimally invasive medical procedures. In this paper, we apply sampling-based motion planning technique to explore motion planning for the steerable bevel-tip needle in 3D environments with obstacles. Based on the Rapidly-exploring Random Trees (RRTs) method, we develop a motion planner to quickly build a tree to search the configuration space using a new exploring strategy, which generates new states using randomly sampled control space instead of the deterministically sampled one used in classic RRTs. Notice the fact that feasible paths might not be found for any given entry point and target configuration, we also address the feasible entry point planning problem to find feasible entry points in a specified entry zone for any given target configuration. To solve this problem, we developed a motion planning algorithm based on RRTs with backchaining, which grow backward from the target to explore the configuration space. Finally, simulation results with a approximated realistic prostate needle insertion environment demonstrate the performance of the proposed motion planner.","Bridges,
USA Councils,
Automation,
Conferences"
Effective feature extraction by trace transform for insect footprint recognition,"The paper discusses insect footprint recognition. Footprint segments are extracted from scanned footprints, and appropriate features are calculated for those segments (or cluster of segments) in order to discriminate species of insects. The selection or identification of such features is crucial for this classification process.","Feature extraction,
Animals,
Image segmentation,
Clustering algorithms,
Transforms,
Classification algorithms,
Foot"
Compiler generated systolic arrays for wavefront algorithm acceleration on FPGAs,"Wavefront algorithms, such as the Smith-Waterman algorithm, are commonly used in bioinformatics for exact local and global sequence alignment. These algorithms are highly computationally intensive and are therefore excellent candidates for FPGA-based code acceleration. However, there is no standard form of these algorithms, they are used in a wide variety of situations with various constraints. It is therefore not practical to have a standard kernel that can be mapped to an FPGA, hence the importance of being able to compile such codes from a high level language. ROCCC is a C to VHDL compiler, which optimizes and parallelizes the most frequently executed kernel loops in applications such as in multimedia, scientific and high-performance computing. In this paper we describe the transformations performed by ROCCC, which transformed the kernel of the Smith-Waterman algorithm into a hardware systolic array that is mapped onto the FPGA on the SGI Altix RASC blade. We report a throughput increase by over 3,000X over a 2.8 GHz Xeon.","Field programmable gate arrays,
Throughput,
Hardware,
Blades,
System-on-a-chip,
Kernel,
Heuristic algorithms"
Channel assignment in an IEEE 802.11 WLAN based on Signal-To-Interference Ratio,"In this paper, we propose a channel-assignment algorithm at the Access Points (APs) of a Wireless Local Area Network (WLAN) in order to maximize Signal-to-Interference Ratio (SIR) at the user level. We start with the channel assignment at the APs, which is based on minimizing the total interference between APs. Based on this initial assignment, we calculate the SIR for each user. The algorithm can be applied to any WLAN, irrespective of the user distribution and user load. Results show that the proposed algorithm is capable of significantly increasing the SIR over the WLAN, which in turn improves throughput.","Wireless LAN,
Interference,
Throughput,
Indoor environments,
Computer science,
Performance evaluation,
Mathematical model,
Degradation"
Wireless reliability: Rethinking 802.11 packet loss,"Wireless enabled devices are ubiquitous in today’s computing environment. Businesses, universities, and home users alike are taking advantage of the easy deployment of wireless devices to provide network connectivity without the expense associated with wired connections. Unfortunately, the wireless medium is inherently unreliable resulting in significant work having been performed to better understand the characteristics of the wireless environment. Notably, many works attribute the primary source of wireless losses to errors in the physical medium. In contrast, our work shows that the wireless device itself plays a significant role in 802.11 packet loss. In our experiments, we found that the correlation of loss between multiple closely located (within one λ) receivers is low with the majority of loss instances only occurring at one of the receivers. We conducted extensive experiments on the individual loss characteristics of five common wireless cards, showing that while the cards behave similarly on the macro-level (e.g. similar overall loss rates), the cards perform quite differently on the micro-level (e.g. burstiness, correlation, and consistency).","Wireless communication,
Correlation,
Receivers,
Distance measurement,
USA Councils,
Presses,
Ad hoc networks"
Toward Verified Biological Models,"The last several decades have witnessed a vast accumulation of biological data and data analysis. Many of these data sets represent only a small fraction of the system's behavior, making the visualization of full system behavior difficult. A more complete understanding of a biological system is gained when different types of data (and/or conclusions drawn from the data) are integrated into a larger scale representation or model of the system. Ideally, this type of model is consistent with all available data about the system, and it is then used to generate additional hypotheses to be tested. Computer-based methods intended to formulate models that integrate various events and to test the consistency of these models with respect to the laboratory-based observations on which they are based are potentially very useful. In addition, in contrast to informal models, the consistency of such formal computer-based models with laboratory data can be tested rigorously by methods of formal verification. We combined two formal modeling approaches in computer science that were originally developed for nonbiological system design. One is the interobject approach using the language of live sequence charts (LSCs) with the Play-Engine tool, and the other is the intraobject approach using the language of statecharts and Rhapsody as the tool. Integration is carried out using InterPlay, a simulation engine coordinator. Using these tools, we constructed a combined model comprising three modules. One module represents the early lineage of the somatic gonad of Caenorhabditis elegans in LSCs, whereas a second more detailed module in statecharts represents an interaction between two cells within this lineage that determine their developmental outcome. Using the advantages of the tools, we created a third module representing a set of key experimental data using LSCs. We tested the combined statechart-LSC model by showing that the simulations were consistent with the set of experimental LSCs. This small-scale modular example demonstrates the potential for using similar approaches for verification by exhaustive testing of models by LSCs. It also shows the advantages of these approaches for modeling biology.","Biological system modeling,
Laboratories,
Data analysis,
Data visualization,
Biological systems,
System testing,
Formal verification,
Computer science,
Sequences,
Engines"
Injector: Mining Background Knowledge for Data Anonymization,"Existing work on privacy-preserving data publishing cannot satisfactorily prevent an adversary with background knowledge from learning important sensitive information. The main challenge lies in modeling the adversary's background knowledge. We propose a novel approach to deal with such attacks. In this approach, one first mines knowledge from the data to be released and then uses the mining results as the background knowledge when anonymizing the data. The rationale of our approach is that if certain facts or background knowledge exist, they should manifest themselves in the data and we should be able to find them using data mining techniques. One intriguing aspect of our approach is that one can argue that it improves both privacy and utility at the same time, as it both protects against background knowledge attacks and better preserves the features in the data. We then present the Injector framework for data anonymization. Injector mines negative association rules from the data to be released and uses them in the anonymization process. We also develop an efficient anonymization algorithm to compute the injected tables that incorporates background knowledge. Experimental results show that Injector reduces privacy risks against background knowledge attacks while improving data utility.","Data privacy,
Protection,
Data mining,
Computer science,
Publishing,
Association rules,
Data security,
Databases,
Diseases,
Remuneration"
A Direct Product Theorem for Discrepancy,"Discrepancy is a versatile bound in communication complexity which can be used to show lower bounds in randomized, quantum, and even weakly-unbounded error models of communication. We show an optimal product theorem for discrepancy, namely that for any two Boolean functions f, g, disc(f xor g)=Theta(disc(f) disc(g)). As a consequence we obtain a strong direct product theorem for distributional complexity, and direct sum theorems for worst-case complexity, for bounds shown by the discrepancy method. Our results resolve an open problem of Shaltiel (2003) who showed a weaker product theorem for discrepancy with respect to the uniform distribution, disc_{U^k}(f^(k)) = O(disc_U(f))^(k/3). The main tool for our results is semidefinite programming, in particular a recent characterization of discrepancy in terms of a semidefinite programming quantity by Linial and Shraibman (2006).","Complexity theory,
Protocols,
Quantum computing,
Quantum mechanics,
Boolean functions,
Mathematical programming,
Computational modeling,
Circuit testing,
Computational complexity,
Computer science"
IP address configuration in VANET using centralized DHCP,"Vehicular ad-hoc networks (VANET) are a mobile adhoc networking technology to facilitate vehicle-to-vehicle and vehicle-to-roadside communication. A vehicle in VANET is considered to be an intelligent mobile node capable of communicating with its neighbors and other vehicles in the network. As in a mobile ad-hoc network (MANET) it is necessary to identify or address each vehicle in the vehicular ad-hoc network with a unique address. The current addressing mechanisms in VANET do not succeed in configuring the vehicle with a unique address. Furthermore, there is a need for address reconfigurations depending on the mobility patterns. In order to deal with these problems, we have presented a centralized addressing scheme for VANET using DHCP (Dynamic Host Configuration Protocol). Results obtained in our approach are compared against the results presented in one of the existing addressing mechanism in VANET. It is observed that our approach is efficient and feasible for vehicular ad-hoc networks.","Vehicles,
Lead,
Servers,
Ad hoc networks,
Protocols,
IP networks,
Vehicle dynamics"
Integrated automated design approach for building automation systems,"The planning and design of building automation systems is a time consuming, error prone and nowadays more and more expensive task, consisting of a lot of repeated manual design steps done by specialized engineers. To reduce the engineering costs for such systems, the authors present a new automated top-down design approach within this paper. A knowledge-based system supports the planner at the requirement analysis by means of a guided dialog. Subsequently, the complete automation system is automatically designed in two steps. The abstract design proceeds a design based on platform- and manufacturer-independent function blocks via generative programming. The detailed design replaces the function blocks by platform- and manufacturer-specific profiles by means of evolutionary techniques.","Automation,
Buildings,
Sensors,
Planning,
Knowledge based systems,
Software,
Temperature sensors"
Linear programming models for jamming attacks on network traffic flows,"We present a new class of network attacks, referred to as flow-jamming attacks, in which an adversary with multiple jammers throughout the network jams packets to reduce traffic flow. We propose a linear programming framework for flow-jamming attacks, providing a foundation for the design of future protocols to mitigate flow-jamming. We propose metrics to evaluate the effect of a flow-jamming attack on network flow and the resource expenditure of the jamming adversary. We develop, evaluate, and compare a variety of flow-jamming attacks using the proposed metrics and the linear programming formulation. In addition, we formulate two approaches for distributed flow-jamming attacks for a set of jammers operating without centralized control and compare the performance to the centralized attacks using the linear programming formulation.","Linear programming,
Jamming,
Telecommunication traffic,
Traffic control,
Protocols,
Computer crime,
US Government,
Information security,
Centralized control,
Spread spectrum communication"
Circularly polarized compact passive RFID tag antenna,"This paper presents a compact (35times36.5 mm) RFID tag antenna which has large RCS patterns and easy conjugate impedance matching property by use of an inductively-coupled feeding. Its simulated maximum (match state) and minimum (short state) RCS are -15.36 dBm2 and -22.0 dBm2, with a difference of 6.69 dB between the two states. We have also calculated the detection distance of the tag for different values of reader antenna gain.","Polarization,
Passive RFID tags,
Impedance,
Radiofrequency identification,
Antenna feeds,
Dipole antennas,
Transmitting antennas,
Application specific integrated circuits,
Antenna theory,
RFID tags"
Dual high-frequency difference excitation for contrast detection,"Stimulating high-frequency nonlinear oscillations of ultrasound contrast agents is helpful to distinguish microbubbles from background tissues. Nevertheless, inefficiency of such oscillations from most commercially available contrast agents and intense attenuation of the resultant high-frequency harmonics limit microbubble detection with high-frequency ultrasound. To avoid this high-frequency nature, we devised and explored a dual-frequency difference excitation technique to induce efficiently low-frequency, rather than high-frequency, nonlinear scattering from microbubbles by using high-frequency ultrasound. The proposed excitation pulse is comprised of 2 high-frequency sinusoids with frequency difference subject to the microbubble resonance frequency. Its envelope, with frequency being the difference between the 2 frequencies, is used to stimulate nonlinear oscillation of microbubbles for the consonant low-frequency harmonic generation, whereas high-imaging resolution is retained because of narrow high-frequency transmit beams. Hydrophone measurements and phantom experiments of speckle-generating flow phantoms were performed to demonstrate the efficacy of the proposed technique. The results show that, especially when the envelope frequency is near the microbubbleiquests resonance frequency, the envelope of the proposed excitation pulse can induce significant nonlinear scattering from microbubbles, the induced nonlinear responses tend to increase with the pulse pressures, and up to 26 dB and 36 dB contrast-to-tissue ratios with second- and fourth-order nonlinear responses, respectively, can be obtained. Potential applications of this method include microbubble fragmentation and cavitation with high-frequency ultrasound.","Ultrasonic imaging,
Scattering,
Resonance,
Resonant frequency,
Imaging phantoms,
Attenuation,
Frequency conversion,
Sonar equipment,
Fluid flow measurement,
Performance evaluation"
Fast annotation and modeling with a single-point laser range finder,"This paper presents methodology for integrating a small, single-point laser range finder into a wearable augmented reality system. We first present a way of creating object-aligned annotations with very little user effort. Second, we describe techniques to segment and pop-up foreground objects. Finally, we introduce a method using the laser range finder to incrementally build 3D panoramas from a fixed observer’s location. To build a 3D panorama semi-automatically, we track the system’s orientation and use the sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the user’s position. Using full 3D panoramic geometry, it is possible for new virtual objects to be placed in the scene with proper lighting and occlusion by real world objects, which increases the expressivity of the AR experience.","Three dimensional displays,
Laser applications,
Pixel,
Image color analysis,
Solid modeling,
Lasers,
Cameras"
On Locating Byzantine Attackers,"We examine networks that employ network coding and are subject to Byzantine attacks. We assume that an appropriate network error correcting scheme is employed that is able to correct (up to a certain number of) Byzantine errors. Given this setup, we formulate the problem of locating these malicious nodes that insert errors. We utilize the sub- space properties of (randomized) network coding to develop algorithms to locate the Byzantine attackers.","Error correction codes,
Error correction,
Network topology,
Computer networks,
Network coding,
Redundancy,
Protection,
Computer errors,
Decoding,
Vectors"
Efficient partial shape matching using Smith-Waterman algorithm,"This paper presents an efficient partial shape matching method based on the Smith-Waterman algorithm. For two contours of m and n points respectively, the complexity of our method to find similar parts is only O(mn). In addition to this improvement in efficiency, we also obtain comparable accurate matching with fewer shape descriptors. Also, in contrast to arbitrary distance functions that are used by previous methods, we use a probabilistic similarity measurement, p-value, to evaluate the similarity of two shapes. Our experiments on several public shape databases indicate that our method outperforms state-of-the-art global and partial shape matching algorithms in various scenarios.","Shape measurement,
Turning,
Databases,
Computer vision,
Object recognition,
Stereo vision,
Image retrieval,
Content based retrieval,
Skeleton,
Design optimization"
A comparison of the target tracking in marine navigational radars by means of GRNN filter and numerical filter,The article presents research results on the application of artificial GRNN neural network in the process of radar tracking in marine navigational radars. A comparison has been presented of the tracking process by means of a numerical filter implemented in a real radar device and a neural filter prepared by the authors. The test situation was registered on a real vessel and next introduced into a simulator estimating the vector of target movement by means of a GRNN filter. Results have been presented for various GRNN filters.,"Target tracking,
Navigation,
Radar tracking,
Filters,
Neurons,
Education,
Smoothing methods,
Neural networks,
Radar applications,
Information filtering"
Optimization-based framework for simultaneous circuit-and-system design-space exploration: A high-speed link example,"Connecting system-level performance models with circuit information has been a long-standing problem in analog/mixed-signal front-ends, like radios and high-speed links. High-speed links are particularly hard to analyze because of the complex interplay of device/circuit parasitics and channel filtering operation. In this paper we introduce optimization-based framework for link design-space exploration, connecting the link transmission quality and top-level filter settings with circuit power, sizing and biasing. We derive a special analytical discrete time representation that avoids the size explosion of the symbolic problem description improving the parsing and solver time by orders of magnitude and making this joint optimization possible in real-time. This robust and accurate problem formulation is derived in signomial form and is compatible with existing optimization approaches to circuit sizing. We demonstrate this optimization framework on a link design-space exploration example, investigating trade-offs between the transmit pre-emphasis and linear receiver equalizer and their impact on overall link power vs. data rate.","Design optimization,
Power system modeling,
Equations,
Circuit optimization,
Circuits and systems,
Phase locked loops,
Decision feedback equalizers,
Radio transmitters,
Timing,
Filters"
Transmit power allocation for successive interference cancellation in multicode MIMO systems,"Multiple-input multiple-output (MIMO) system with multicode transmission can provide high speed data services by transmitting independent parallel substreams from multiple antennas and through multicode channelization. In this paper, we first introduce an iterative two-stage successive interference cancellation (SIC) detection scheme for a multicode MIMO system. The proposed technique cancels the interference signals successively in the space domain followed by the code domain. Next, we develop various transmit power allocation schemes over different data substreams for the proposed detection process to improve error rate performance. The joint transmit power allocation is derived to make the post-detection signal-to-interference-plus-noise ratio (SINR) become the same for all substreams in both the space and code domains. As a computationally efficient scheme, we propose a two-stage power allocation scheme, which allocates the total transmit power to the substreams in the code domain at the first stage, and allocates this code domain power to the substreams in the space domain at the second stage. Furthermore, variable and constant power ratio (PR) schemes are derived to reduce the feedback overhead. In particular, the constant PR scheme utilizes the transmit power ratio determined by the long-term statistical properties of the fading channel amplitudes, and achieves significantly reduced feedback rate. Numerical results show that the proposed transmit power allocation schemes for the two-stage SIC significantly outperform the equal power allocation scheme.","Interference cancellation,
MIMO,
Silicon carbide,
Fading,
Multiaccess communication,
Transmitting antennas,
Error analysis,
Feedback,
Receiving antennas,
Computer science"
Comparison of sensitivity of OFDM and Wavelet Packet Modulation to time synchronization error,"Wavelet Packet Modulation (WPM) is a novel multicarrier modulation (MCM) technique and a promising alternative to the well established OFDM. However a few key research questions remain to be addressed before WPM can become practically viable. One of them is their sensitivity and vulnerability to time synchronization errors. This is particularly interesting because WPM symbols overlap in the time domain. In this paper we investigate the BER performance degradation of WPM systems in the presence of timing offset and compare it with OFDM. Several well-known wavelets such as Daubechies, Symlets, Coiflets, discrete Meyer and biorthogonal wavelets are selected and studied. The result of this research show how these wavelet-based systems cope with time synchronization errors and how their performances compare with OFDM.","Wavelet packets,
OFDM modulation,
Frequency synchronization,
Interference,
Timing,
Degradation,
Discrete wavelet transforms,
Wavelet domain,
Bit error rate,
Additive white noise"
Convergence analysis of distributed subgradient methods over random networks,"We consider the problem of cooperatively minimizing the sum of convex functions, where the functions represent local objective functions of the agents. We assume that each agent has information about his local function, and communicate with the other agents over a time-varying network topology. For this problem, we propose a distributed subgradient method that uses averaging algorithms for locally sharing information among the agents. In contrast to previous works that make worst-case assumptions about the connectivity of the agents (such as bounded communication intervals between nodes), we assume that links fail according to a given stochastic process. Under the assumption that the link failures are independent and identically distributed over time (possibly correlated across links), we provide convergence results and convergence rate estimates for our subgradient algorithm.","Convergence,
Cost function,
Wireless sensor networks,
Stochastic processes,
Computer networks,
Network servers,
Optimization methods,
Operations research,
Network topology,
Communication system control"
Integrated Micromechanical Radio Front-Ends,An overview of MEMS technologies capable of realizing the RF front-end frequency gating function needed by true software-defined cognitive radios is presented. Among the technologies described are vibrating disk micromechanical resonators that exhibit record on-chip frequency-Q products; medium-scale integrated micromechanical circuits that implement on/off switchable filter banks; and a process technology that integrates nickel MEMS together with foundry CMOS transistors in a fully monolithic single-chip process.,"Micromechanical devices,
Integrated circuit technology,
CMOS technology,
Radio frequency,
Cognitive radio,
Radiofrequency integrated circuits,
Switching circuits,
Filter bank,
Nickel,
Foundries"
Software Risk Assessment and Estimation Model,"Just like any development projects, there is inherent risk in software development projects. Most of the software's fails due to over budget, delay in the delivery of the software's and so on. In this paper we have proposed a software risk assessment and estimation model (SRAEM). Using this model it is possible to predict the possible results of software projects with good accuracy. Proposed model not only assess the risk but it also estimate the risk. In this paper the risk is estimated using risk exposure and software metrics of risk management and this metric is based on mission critical requirements stability risk metrics (MCRSRM). This software metrics is used when there are changes in requirements such as addition, subtraction, or deletion. The proposed model gives the incremental risk for every phase and also the total cumulative risk as the software progress from phase to phase.","Risk management,
Software,
Estimation,
Uncertainty,
Planning,
Measurement errors,
Book reviews"
A comparison of search heuristics for empirical code optimization,"This paper describes the application of various search techniques to the problem of automatic empirical code optimization. The search process is a critical aspect of auto-tuning systems because the large size of the search space and the cost of evaluating the candidate implementations makes it infeasible to find the true optimum point by brute force. We evaluate the effectiveness of Nelder-Mead Simplex, Genetic Algorithms, Simulated Annealing, Particle Swarm Optimization, Orthogonal search, and Random search in terms of the performance of the best candidate found under varying time limits.","Optimization,
Tuning,
Simulated annealing,
Gallium,
Generators,
Genetic algorithms,
Arrays"
"Can Programming Be Liberated, Period?","We have come a long way since programming had to be done by tediously listing machine-level instructions that prescribed how a specific computer was to modify and move bits and words in its memory. The author describes his dream about freeing ourselves from the straightjackets of programming, making the process of getting computers to do what we want intuitive, natural, and also fun. He recommends harnessing the great power of computing and transforming a natural and almost playful means of programming so that it becomes fully operational and machine-doable. Once liberated, programmers will probably have new kinds of work to do, possibly including the need to set up specialized features of the new sophisticated computational tools that would be running in the background.","Computer aided instruction,
Algebra,
Functional programming,
History,
Assembly,
Computer languages,
Logic programming,
Object oriented programming,
Application software,
Writing"
Discovery Architecture for the Tracing of Products in the EPCglobal Network,"The EPCglobal Network is a network providing a shared view of the disposition of EPC-bearing objects between EPCglobal subscribers, within a relevant business context. In the EPCglobal Network, product data is distributed to several EPCISes(EPC Information Services) via movement of the product. The ONS (Object Naming Service) and the EPCISDS (EPCIS Discovery Service) are used to identify the distributed data for tracing the product. However, there is neither a standard scenario for access to the services when tracing the product nor a defined standard for EPCISDS. This paper suggests a novel architecture for tracing product data with EPCISDS and ONS. Our architecture provides a concrete substructure for product tracing services.","Radiofrequency identification,
Computer architecture,
RFID tags,
Software standards,
Ubiquitous computing,
Computer science,
Concrete,
Hardware,
Supply chains,
Filtering"
A Generative Probabilistic Model for Multi-label Classification,"Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.","Space technology,
Laboratories,
Text categorization,
Data mining,
Intelligent systems,
Intelligent structures,
Information science,
Computer science,
Labeling,
Random variables"
Learning robot motion control with demonstration and advice-operators,"As robots become more commonplace within society, the need for tools to enable non-robotics-experts to develop control algorithms, or policies, will increase. Learning from Demonstration (LfD) offers one promising approach, where the robot learns a policy from teacher task executions. Our interests lie with robot motion control policies which map world observations to continuous low-level actions. In this work, we introduce Advice-Operator Policy Improvement (A-OPI) as a novel approach for improving policies within LfD. Two distinguishing characteristics of the A-OPI algorithm are data source and continuous state-action space. Within LfD, more example data can improve a policy. In A-OPI, new data is synthesized from a student execution and teacher advice. By contrast, typical demonstration approaches provide the learner with exclusively teacher executions. A-OPI is effective within continuous state-action spaces because high level human advice is translated into continuous-valued corrections on the student execution. This work presents a first implementation of the A-OPI algorithm, validated on a Segway RMP robot performing a spatial positioning task. A-OPI is found to improve task performance, both in success and accuracy. Furthermore, performance is shown to be similar or superior to the typical exclusively teacher demonstrations approach.","Robots,
Humans,
Mobile robots,
Accuracy,
Robot motion,
Robot sensing systems,
Sensors"
Beating the Random Ordering is Hard: Inapproximability of Maximum Acyclic Subgraph,"We  prove  that  approximating  the  Max.  Acyclic  Subgraphproblem  within a  factor better  than 1/2  is  Unique Gameshard.  Specifically,  for  every  constant \eps
  ≫  0  thefollowing  holds: given  a directed  graph G
  that  has anacyclic subgraph consisting of  a fraction (1-\eps)
of itsedges, if  one can efficiently  find an acyclic  subgraph ofG
with more  than (1/2+\eps)
of its edges,  then the UGCis  false.  Note that  it  is  trivial  to find  an  acyclicsubgraph with 1/2
the  edges, by taking either the forwardor backward  edges in an arbitrary ordering  of the verticesof  G
. The existence  of a  \rho
-approximation algorithmfor \rho ≫ 1/2$ has been a basic open problem for a while.Our result  is the first tight  inapproximability result foran ordering problem. The  starting point of our reduction isa  directed acyclic  subgraph (DAG)  in which  every  cut isnearly-balanced in the sense  that the number of forward andbackward edges crossing the  cut are nearly equal; such DAGswere constructed by Charikar et  al. Using this, we are ableto  study  Max.  Acyclic  Subgraph, which  is  a  constraintsatisfaction  problem  (CSP) over  an  unbounded domain,  byrelating it to a proxy CSP over a bounded domain. The latteris  then  amenable  to  powerful  techniques  based  on  theinvariance principle.Our    results   also    give   a    super-constant   factorinapproximability result  for the Feedback  Arc Set problem.Using our  reductions, we  also obtain SDP  integrality gapsfor both the problems.","Approximation algorithms,
Computer science,
User-generated content,
Mathematics"
Experimental comparison of peer-to-peer streaming overlays: An application perspective,"We compare two representative streaming systems using mesh-based and multiple tree-based overlay routing through deployments on the PlanetLab wide-area experimentation platform. To the best of our knowledge, this is the first study to compare streaming overlay architectures in real Internet settings, considering not only intuitive aspects such as scalability and performance under churn, but also less studied factors such as bandwidth and latency heterogeneity of overlay participants. Overall, our study indicates that mesh-based systems are superior for nodes with high bandwidth capabilities and low round trip times, while multi-tree based systems currently cope better with stringent real time deadlines under heterogeneous conditions.",
Direct Volume Rendering: A 3D Plotting Technique for Scientific Data,"The use of plotting techniques to comprehend scalar functions is ubiquitous in science and engineering. An effective plot uses features such as first and second derivatives to convey critical and inflection points, which help portray the overall behavior of functions around a region of interest. Direct volume rendering is an effective method for plotting 3D scientific data, but it's not used as frequently as it could be. In this article, the authors summarize direct volume rendering and discuss barriers to taking advantage of this powerful technique.","Data visualization,
Optical scattering,
Packaging,
Computed tomography,
Hardware,
Biomedical optical imaging,
Stimulated emission,
Equations,
Power engineering and energy,
Level set"
Predictable Interrupt Management and Scheduling in the Composite Component-Based System,"This paper presents the design of user-level scheduling hierarchies in the Composite component-based system. The motivation for this is centered around the design of a system that is both dependable and predictable, and which is configurable to the needs of specific applications. Untrusted application developers can safely develop services and policies, that are isolated in protection domains outside the kernel. To ensure predictability, Composite needs to enforce timing control over user-space services. Moreover, it must provide a means by which asynchronous events, such as interrupts, are handled in a timely manner without jeopardizing the system. Towards this end, we describe the features of Composite that allow user-defined scheduling policies to be composed for the purposes of combined interrupt and task management. A significant challenge arises from the need to synchronize access to shared data structures (e.g., scheduling queues), without allowing untrusted code to disable interrupts or use atomic instructions that lock the memory bus. Additionally, efficient upcall mechanisms are needed to deliver asynchronous event notifications in accordance with policy-specific priorities, without undue recourse to schedulers. We show how these issues are addressed in Composite, by comparing several hierarchies of schedulingpolices, to manage both tasks and the interrupts on which they depend. Studies show how it is possible to implement guaranteed differentiated services as part of the handling of I/O requests from a network device while avoiding livelock.  Microbenchmarks indicate that the costs of implementing and invoking user-level schedulers in Composite are on par with, or less than, those in other systems, with thread switches more than twice as fast as in Linux.","Yarn,
Kernel,
Protection,
Costs,
Processor scheduling,
Real time systems,
Computer science,
Timing,
Data structures,
Switches"
Reliability-aware Dynamic Voltage Scaling for energy-constrained real-time embedded systems,"The Dynamic Voltage Scaling (DVS) technique is the basis of numerous state-of-the-art energy management schemes proposed for real-time embedded systems. However, recent research has illustrated the alarmingly negative impact of DVS on task and system reliability. In this paper, we consider the problem of processing frequency assignment to a set of real-time tasks in order to maximize the overall reliability, under given time and energy constraints. First, we formulate the problem as a non-linear optimization problem and show how to obtain the static optimal solution. Then, we propose on-line (dynamic) algorithms that detect early completions and adjust the task frequencies at run-time, to improve overall reliability. Our simulation results indicate that our algorithms perform comparably to a clairvoyant optimal scheduler that knows the exact workload in advance.","Dynamic voltage scaling,
Real time systems,
Embedded system,
Voltage control,
Frequency,
Energy management,
Reliability,
Time factors,
Heuristic algorithms,
Runtime"
Token tenure: PATCHing token counting using directory-based cache coherence,"Traditional coherence protocols present a set of difficult tradeoffs: the reliance of snoopy protocols on broadcast and ordered interconnects limits their scalability, while directory protocols incur a performance penalty on sharing misses due to indirection. This work introduces PATCH (Predictive/Adaptive Token Counting Hybrid), a coherence protocol that provides the scalability of directory protocols while opportunistically sending direct requests to reduce sharing latency. PATCH extends a standard directory protocol to track tokens and use token counting rules for enforcing coherence permissions. Token counting allows PATCH to support direct requests on an unordered interconnect, while a mechanism called token tenure uses local processor timeouts and the directory’s per-block point of ordering at the home node to guarantee forward progress without relying on broadcast. PATCH makes three main contributions. First, PATCH introduces token tenure, which provides broadcast-free forward progress for token counting protocols. Second, PATCH deprioritizes best-effort direct requests to match or exceed the performance of directory protocols without restricting scalability. Finally, PATCH provides greater scalability than directory protocols when using inexact encodings of sharers because only processors holding tokens need to acknowledge requests. Overall, PATCH is a “one-size-fits-all” coherence protocol that dynamically adapts to work well for small systems, large systems, and anywhere in between.","Broadcasting,
Scalability,
Delay,
Permission,
Encoding,
Proposals,
Access protocols,
Information science,
Costs,
Bandwidth"
A new framework for direction-of-arrival estimation,"A new approach for spatial direction-of-arrival (DOA) estimation is developed based on the minimum mean-square error (MMSE) framework. Unlike many traditional DOA estimators, the MMSE approach, denoted as Re-Iterative Super-Resolution (RISR), does not employ spatial sample covariance information which may significantly degrade DOA estimation if spatially-separated sources are temporally correlated. Instead, RISR is a recursive algorithm that relies on a structured signal covariance matrix comprised of the set of possible spatial steering vectors each weighted by an associated power estimate from the previous iteration. Furthermore, RISR can naturally accommodate prior information on spatially colored noise, does not require knowledge of the number of sources, and can also exploit multiple time samples in a non-coherent manner to improve performance. For low to moderate time sample support, RISR is demonstrated to provide super-resolution performance superior to MUSIC and spatially-smoothed MUSIC.","Music,
Signal to noise ratio,
Estimation,
Correlation,
Direction of arrival estimation,
Arrays,
Spatial resolution"
Developing a Course on Designing Software in Globally Distributed Teams,"Present-day software engineering combines technical and social skills, as well as collaboration among people with different backgrounds (e.g. due to global development and outsourcing). In this paper we address the problem of teaching ""globally distributed development"", and specifically software design. Our goal is to develop a joint Master course teaching software design in a global setting. To this end, we contribute with a list of characteristics to be developed, in the form of an 'orientation map' for educators. We use this map to build a joint course between two European universities.","Education,
Joints,
Unified modeling language,
Software,
Documentation,
Companies,
Software design"
Gap Filling Strategies for 3-D-FBP Reconstructions of High-Resolution Research Tomograph Scans,"The high-resolution research tomograph (HRRT) is a dedicated human brain positron emission tomography scanner. Currently available iterative reconstruction algorithms show bias due to nonnegativity constraints. Consequently, implementation of 3-D filtered backprojection (3-D-FBP) is of interest. To apply 3-D-FBP all missing data including those due to gaps between detector heads need to be estimated. The aim of this study was to evaluate various gap filling strategies for 3-D-FBP reconstructions of HRRT data, such as linear and bilinear interpolation or constraint Fourier space gap filling (confosp). Furthermore, missing planes were estimated using segment 0 image data only (noniterative) or by using reconstructed images based on all previous segments (iterative method). Use of bilinear interpolation showed worst correspondence between reconstructed and true activity concentration, especially for small structures. Moreover, phantom data indicated that use of linear interpolation resulted in artifacts in planes located near the edge of the field-of-view. Use of confosp did not show these artifacts. Iterative estimations of the missing planes for |segments| > 0 improved image quality at the cost of more computation time. Therefore, use of confosp for filling sinogram gaps with both iterative and noniterative estimation of missing planes are recommended for quantitative 3-D-FBP of HRRT studies.","Filling,
Image reconstruction,
Interpolation,
Image segmentation,
Humans,
Positron emission tomography,
Reconstruction algorithms,
Detectors,
Head,
Iterative methods"
Assessing Software Archives with Evolutionary Clusters,"The way in which a system's software archive is partitioned influences the evolvability of that system. The partition of a software archive, e.g. subsystem decomposition, is mostly assessed by looking at the static (include, call) relations between the parts. In the literature history information is also taken into account to assess the partition. In this paper we describe our history-based approach to (automatically) assess the extent in which a certain partition allows its parts to evolve independently. We use the assumption that software entities which co-evolved often in the past are likely to be modified together in the near future as well. Hence, the elements of such a set should in principle belong to the same part. Our approach, therefore, identifies sets of co-evolving software entities, where each set has elements from more than one part of the archive. We illustrate our approach with a case study of a large software system that evolved during more than a decade, and has over 7 million lines of code.","Software systems,
History,
Embedded system,
Medical services,
System software,
Degradation,
Embedded software,
Computer science,
Testing,
Time to market"
An evaluation of models for predicting opponent positions in first-person shooter video games,"A well-known Artificial Intelligence (AI) problem in video games is designing AI-controlled humanoid characters. It is desirable for these characters to appear both skillful and believably human-like. Many games address the former objective by providing their agents with unfair advantages. Although challenging, these agents are frustrating to humans who perceive the AI to be cheating. In this paper we evaluate hidden semi-Markov models and particle filters as a means for predicting opponent positions. Our results show that these models can perform with similar or better accuracy than the average human expert in the game Counter-Strike: Source. Furthermore, the mistakes these models make are more human-like than perfect predictions.","Predictive models,
Game theory,
Humans,
Artificial intelligence,
Clocks,
Accuracy,
Statistics,
Particle filters,
Decision making,
Control systems"
Improving LBP features for gender classification,"Automatic gender classification aims at analyzing the face image to recognize gender with computer, in which feature extraction is one key step. The LBP (Local Binary Pattern) feature has essential applications in face analysis and has been applied in gender recognition. The normally adopted LBP feature will encounter dimension explosion with the increase of sampling density of LBP operator, which could not remarkably improve the performance of gender classification. In this paper, we present two simple methods to improve the common LBP feature, i.e., fusing low-density LBP features and decreasing the dimension of high density LBP feature with PCA (Principle Component Analysis), both of which could drastically lower the feature dimension while preserving the precision. Experiments are performed on FERET upright face database. The results illustrate the drawbacks of general LBP feature and identify the merit of our improved feature extraction algorithms.",
Quantum realization of some quaternary circuits,"We present the design of quaternary quantum version of reversible circuits such as Toffoli gate, modified Fredkin gate, mux, demux, encoder-decoder using linear ion realizable quaternary Muthukrishnan-Stroud gates. Our realization of quaternary Toffoli gate is more efficient than the previous realization and other quaternary circuits are realized for the time in literature.","Quantum computing,
Logic functions,
Multivalued logic,
DH-HEMTs,
Logic gates,
Galois fields,
Logic circuits,
Adders,
Decoding,
Information security"
Combining Multiple Feature Extraction Techniques for Handwritten Devnagari Character Recognition,"In this paper we present an OCR for Handwritten Devnagari Characters. Basic symbols are recognized by neural classifier. We have used four feature extraction techniques namely, intersection, shadow feature, chain code histogram and straight line fitting features. Shadow features are computed globally for character image while intersection features, chain code histogram features and line fitting features are computed by dividing the character image into different segments. Weighted majority voting technique is used for combining the classification decision obtained from four Multi Layer Perceptron(MLP) based classifier. On experimentation with a dataset of 4900 samples the overall recognition rate observed is 92.80% as we considered top five choices results. This method is compared with other recent methods for Handwritten Devnagari Character Recognition and it has been observed that this approach has better success rate than other methods.","Feature extraction,
Character recognition,
Handwriting recognition,
Optical character recognition software,
Region 10,
Histograms,
Voting,
Neural networks,
Artificial neural networks,
Natural languages"
The CALO meeting speech recognition and understanding system,"The CALO Meeting Assistant provides for distributed meeting capture, annotation, automatic transcription and semantic analysis of multiparty meetings, and is part of the larger CALO personal assistant system. This paper summarizes the CALO-MA architecture and its speech recognition and understanding components, which include real-time and offline speech transcription, dialog act segmentation and tagging, question-answer pair identification, action item recognition, decision extraction, and summarization.","Speech recognition,
Web server,
Network servers,
Natural languages,
Automatic speech recognition,
Tagging,
Portable computers,
Linux,
Data processing,
Internet telephony"
RiTE: Providing On-Demand Data for Right-Time Data Warehousing,"Data warehouses (DWs) have traditionally been loaded with data at regular time intervals, e.g., monthly, weekly, or daily, using fast bulk loading techniques. Recently, the trend is to insert all (or only some) new source data very quickly into DWs, called near-realtime DWs (right-time DWs). This is done using regular INSERT statements, resulting in too low insert speeds. There is thus a great need for a solution that makes inserted data available quickly, while still providing bulk-load insert speeds. This paper presents RiTE (""Right-Time ETL""), a middleware system that provides exactly that. A data producer (ETL) can insert data that becomes available to data consumers on demand. RiTE includes an innovative main-memory based catalyst that provides fast storage and offers concurrency control. A number of policies controlling the bulk movement of data based on user requirements for persistency, availability, freshness, etc. are supported. The system works transparently to both producer and consumers. The system is integrated with an open source DBMS, and experiments show that it provides ""the best of both worlds"", i.e., INSERT-like data availability, but with bulk-load speeds (up to 10 times faster).","Warehousing,
Databases,
Computer science,
Middleware,
Data warehouses,
Concurrency control,
Feeds,
Multidimensional systems,
Chemical processes,
Availability"
Cost Curve Evaluation of Fault Prediction Models,"Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the ""best model"" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the ""best"" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.","Costs,
Predictive models,
Software quality,
Testing,
Application software,
Fault diagnosis,
Quality assurance,
Software reliability,
Reliability engineering,
Computer science"
Observer-based consensus control strategy for multi-agent system with communication time delay,"This paper proposes an observer-based consensus control strategy for multi-agent system (MAS) with communication time delay. The condition of stability for MIMO agents is derived by Lyapunov theorem. It gives systematic design procedure under assumed unidirectional network. Furthermore, new consensus control law using observers is proposed for the networked MAS with communication delays. Experimental results show effectiveness of our proposed output consensus approaches.","Control systems,
USA Councils,
Conferences"
Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids,"We present results of cortical activity during phoneme pronunciation, recorded using miniaturized electrocorticography grids with high spatial resolution. A patient implanted with the miniature grid was instructed to audibly pronounce one of four phonemes. For each phoneme, we observed distinct spatial correlation patterns at the 3mm electrode spacing. We applied a support vector machine classification scheme and, for the first time, were able to distinguish discrete phonemes with high accuracy. In addition, we found that sub-regions of our miniature array were specific for distinct pairs of phonemes, showing that cortical phoneme processing occurs at a higher resolution than previously though.",
Text-tracking wearable camera system for visually-impaired people,"Disability of visual text reading has a huge impact on the quality of life for visually disabled people. One of the most anticipated devices is a wearable camera capable of finding text regions in natural scenes and translating the text into another representation such as sound or braille. In order to develop such a device, text tracking in video sequences is required as well as text detection. We need to group homogeneous text regions to avoid multiple and redundant speech syntheses or braille conversions. We have developed a prototype system equipped with a head-mounted video camera. Text regions are extracted from the video frames using a revised DCT feature. Particle filtering is employed for fast and robust text tracking. We have tested the performance of our system using 1,000 video frames of a hall way with eight signboards. The number of text candidate images is reduced to 0.98%.","Cameras,
Layout,
Video sequences,
Speech synthesis,
Prototypes,
Discrete cosine transforms,
Filtering,
Robustness,
Particle tracking,
System testing"
On Anti-Corruption Privacy Preserving Publication,"This paper deals with a new type of privacy threat, called ""corruption"", in anonymized data publication. Specifically, an adversary is said to have corrupted some individuals, if s/he has already obtained their sensitive values before consulting the released information. Conventional generalization may lead to severe privacy disclosure in the presence of corruption. Motivated by this, we advocate an alternative anonymization technique that integrates generalization with perturbation and stratified sampling. The integration provides strong privacy guarantees, even if an adversary has corrupted any number of individuals. We verify the effectiveness of the proposed technique through experiments with real data.","Data privacy,
Diseases,
Lungs,
Hospitals,
Joining processes,
Sampling methods,
Computer science,
Data engineering,
Tin,
Information science"
Development of software effort and schedule estimation models using Soft Computing Techniques,"Accurate estimation of the software effort and schedule affects the budget computation. Bidding for contracts depends mainly on the estimated cost. Inaccurate estimates will lead to failure of making a profit, increased probability of project incompletion and delay of the project delivery date. In this paper, we explore the use of Soft Computing Techniques to build a suitable model structure to utilize improved estimations of software effort for NASA software projects. In doing so, we plan to use Particle Swarm Optimization (PSO) to tune the parameters of the famous COnstructive COst MOdel (COCOMO). We plan also to explore the advantages of Fuzzy Logic to build a set of linear models over the domain of possible software Line Of Code (LOC). The performance of the developed model was evaluated using NASA software projects data set [1]. A comparison between COCOMO tuned-PSO, Fuzzy Logic (FL), Halstead, Walston-Felix, Bailey-Basili and Doty models were provided.","Software,
Mathematical model,
Computational modeling,
Estimation,
Biological system modeling,
Equations,
Schedules"
A moving extended cell concept for seamless communication in 60 GHz radio-over-fiber networks,"We demonstrate a moving extended cell concept that provides seamless communication with high end-user mobility in broadband 60 GHz Radio-over-Fiber networks. Mathematical and simulation-based performance analysis are presented, showing that this technique can guarantee seamless connectivity irrespective of the overlapping area between adjacent cells providing zero packet loss and call dropping probability values for up to 40m/sec mobile speeds.","Mobile communication,
Performance analysis,
Vehicles,
Analytical models,
Wireless networks,
Optical attenuators,
Optical network units,
Optical fiber networks,
Millimeter wave communication,
Millimeter wave technology"
Efficient surface gateway deployment for underwater sensor networks,"Deploying multiple surface-level radio-capable gateways enhances the performance of underwater acoustic sensor network. The locations of gateways have to be carefully selected to maximize the benefit in a cost-effective way. In this paper, we show how to efficiently solve the surface gateway deployment optimization problem, using heuristic approaches. The results of applying these proposed algorithms to a variety of practical deployment scenarios suggest that these heuristics are nearly optimal for practical cases.","Logic gates,
Delay,
Optimization,
Runtime,
Acoustics,
Surface acoustic waves,
Monitoring"
Size Bounds and Query Plans for Relational Joins,"Relational joins are at the core of relational algebra, which in turn is the  core of the standard database query language SQL. As their evaluation is  expensive and very often dominated by the output size, it is an important  task for database query optimisers to compute estimates on the size of joins  and to find good execution plans for sequences of joins. We study these  problems from a theoretical perspective, both in the worst-case model, and  in an average-case model where the database is chosen according to a known  probability distribution. In the former case, our first key observation is  that the worst-case size of a query is characterised by the fractional edge  cover number of its underlying hypergraph, a combinatorial parameter  previously known to provide an upper bound.  We complete the picture by  proving a matching lower bound, and by showing that there exist queries for  which the join-project plan suggested by the fractional edge cover approach  may be substantially better than any join plan that does not use  intermediate projections.","Algebra,
Relational databases,
Engines,
Database languages,
Cost function,
Computer science,
Probability distribution,
Upper bound,
Database systems,
Query processing"
WSDL-Based Automated Test Data Generation for Web Service,"With the increase of the popularity of Web Service, more and more web applications are developed with this new kind of components. Web Service quality validation and control is becoming very critical to vendors, brokers and application builders. Both brokers and users usually have to use black-box testing because design and implementation details of Web Service are unavailable. This paper proposes a formal approach for automated test data generation for Web Service single operation based on WSDL specification. Firstly, we define a formal model for the datatype of input element of the operation. Secondly, the paper presents an algorithm that derives the model from WSDL. Thirdly, the paper presents a method that generates test data for operations from the model. Examples of using this approach are given in order to give evidence of its usefulness.","Automatic testing,
Web services,
XML,
Software testing,
Application software,
Computer science,
Software engineering,
Educational institutions,
Microelectronics,
Automatic control"
Lip segmentation in color images,"Lip feature extraction is one of the most challenging tasks in the lip reading systems' performance. In this paper, a new approach for lip contour extraction based on fuzzy clustering is proposed. The algorithm employs a stochastic cost function to partition a color image into lip and non-lip regions such that the joint probability of the two regions is maximized. First, the mouth location is determined and then, lip region is preprocessed using pseudo hue transformation. Fuzzy c-means clustering is applied to each transformed image along with b components of CIELAB color space. To delete the clustered pixels around lip, an ellipse and a Gaussian mask were used. In order to show the performance of the proposed method, the pseudo hue segmentation and fuzzy c-mean clustering without preprocessing are compared. The compared methods were applied to the VidTIMIT and M2VTS databases and the results show the superiority of the proposed method in comparison with other methods.","Image segmentation,
Color,
Working environment noise,
Face detection,
Clustering algorithms,
Automatic speech recognition,
Lips,
Active contours,
Shape,
Computer science"
Wireless implantable EMG sensing microsystem,"This paper presents a wireless, subfascially implantable electromyogram (EMG) sensing microsystem design for intelligent myoelectric control of powered prostheses. The implantable system consists of two Pt-Ir epimysial EMG electrodes, a custom-designed ASIC, and an RF telemetry coil and is capable of wirelessly transmitting digitized EMG data to an external telemeter mounted in a prosthetic socket. The prototype microsystem is powered by a near-field inductive link operating at 8 MHz with 10% DC power transfer efficiency. On-chip rectification and regulation produce stable 2 V and 2.7 V supplies with a DC current driving capability up to 100 μA. The EMG electrodes are interfaced with a differential capacitively-coupled amplifier with 38 dB closed-loop gain, 1 kHz bandwidth, and 78 nV√Hz input-referred noise floor. The amplified EMG signal is then digitized on chip using an 11-bit algorithmic ADC. The digital EMG data can be Manchester-coded and transmitted to the external telemeter using phase shift keying (PSK) modulation scheme on the same wireless link as the inductive powering system.","Wireless sensor networks,
Electromyography,
Prosthetics,
Electrodes,
Phase shift keying,
Intelligent control,
Application specific integrated circuits,
Radio frequency,
Telemetry,
Coils"
A service-oriented approach for increasing flexibility in manufacturing,"Manufacturing environments are characterized by a multitude of heterogeneous devices, networks, specific protocols and applications. Therefore, static structures, close coupling and vendor-specific solutions have been established over decades. To increase flexibility and interoperability we introduce a service-oriented middleware approach in form of an integration layer between shop-floor equipment and enterprise applications. The integration layer addresses the actual capabilities and data flows on the field and control layer individually. It offers core services that are applicable in several manufacturing domains. We describe a detailed case study to discuss pros and cons of a SOA approach in manufacturing systems and to show the feasibility of our approach.",
Distributed video coding with zero motion skip and efficient DCT coefficient encoding,"In this paper, we propose a suite of efficient schemes for the representation of spatial and temporal correlations of video signals in distributed video coding (DVC). These schemes consist of zero motion skipping, Gray codes, and sign bits coding of DCT coefficients developed to improve the overall rate-distortion (R-D) performance of distributed video coding. Existing schemes have focused on exploiting the spatial and temporal correlation without sufficient investigation on the efficient representation of the correlation. We believe this is one of the reasons that there still exists sub-stantial gap between current DVC and traditional hybrid video coding. In traditional hybrid coding, a suite of efficient representation schemes such as zig-zag scan, run length code, and skipped macroblock have contributed significantly to excellent R-D performance. The efficient representation schemes we developed for DVC in this research will also lead to improved rate-distortion performance comparing with existing DVC schemes. We present in this paper the overall framework for the proposed zero motion skip and the analysis for efficient representations of both DCT coefficients and their signs. The experiment results show that the distributed video coding based on these efficient representations is able to achieve considerably improved rate-distortion performance over existing schemes.","Video coding,
Discrete cosine transforms,
Decoding,
Encoding,
Correlation,
Reflective binary codes,
Rate-distortion"
Circuit failure prediction for robust system design in scaled CMOS,The idea behind circuit failure prediction is to predict the occurrence of a circuit failure before errors actually appear in system data and states. This concept enables a sea change in robust system design by overcoming major reliability challenges such as circuit aging and early-life failures (infant mortality).,"Robustness,
Error correction,
Aging,
Hardware,
Integrated circuit interconnections,
Costs,
Built-in self-test,
Logic,
Power system interconnection,
Niobium compounds"
Toward a multicore architecture for real-time ray-tracing,"Significant improvement to visual quality for real-time 3D graphics requires modeling of complex illumination effects like soft-shadows, reflections, and diffuse lighting interactions. The conventional Z-buffer algorithm driven GPU model does not provide sufficient support for this improvement. This paper targets the entire graphics system stack and demonstrates algorithms, a software architecture, and a hardware architecture for real-time rendering with a paradigm shift to ray-tracing. The three unique features of our system called Copernicus are support for dynamic scenes, high image quality, and execution on programmable multicore architectures. The focus of this paper is the synergy and interaction between applications, architecture, and evaluation. First, we describe the ray-tracing algorithms which are designed to use redundancy and partitioning to achieve locality. Second, we describe the architecture which uses ISA specialization, multi-threading to hide memory delays and supports only local coherence. Finally, we develop an analytical performance model for our 128-core system, using measurements from simulation and a scaled-down prototype system. More generally, this paper addresses an important issue of mechanisms and evaluation for challenging workloads for future processors. Our results show that a single 8-core tile (each core 4-way multithreaded) can be almost 100% utilized and sustain 10 million rays/second. Sixteen such tiles, which can fit on a 240mm2 chip in 22nm technology, make up the system and with our anticipated improvements in algorithms, can sustain real-time rendering. The mechanisms and the architecture can potentially support other domains like irregular scientific computations and physics computations.",
MC-Sim: An efficient simulation tool for MPSoC designs,"The ability to integrate diverse components such as processor cores, memories, custom hardware blocks and complex network-on-chip (NoC) communication frameworks onto a single chip has greatly increased the design space available for system-on-chip (SoC) designers. Efficient and accurate performance estimation tools are needed to assist the designer in making design decisions. In this paper, we present MC-Sim, a heterogeneous multi-core simulator framework which is capable of accurately simulating a variety of processor, memory, NoC configurations and application specific coprocessors. We also describe a methodology to automatically generate fast, cycle-true behavioral, C-based simulators for coprocessors using a high-level synthesis tool and integrate them with MC-Sim, thus augmenting it with the capacity to simulate coprocessors. Our C-based simulators provide on an average 45× improvement in simulation speed over that of RTL descriptions. We have used this framework to simulate a number of real-life applications such as the MPEG4 decoder and litho-simulation, and experimented with a number of design choices. Our simulator framework is able to accurately model the performance of these applications (only 7% off the actual implementation) and allows us to explore the design space rapidly and achieve interesting design implementations.","Network-on-a-chip,
Coprocessors,
Circuit simulation,
Fabrics,
Space exploration,
Costs,
Operating systems,
Application specific integrated circuits,
Manufacturing processes,
Computational modeling"
Optimizing Parallel Performance of Streamline Visualization for Large Distributed Flow Datasets,"Parallel performance has been a challenging topic in streamline visualization for large unstructured flow datasets on parallel distributed-memory computers. It depends strongly on domain partitions. Unsuitable partitions often lead to severe load imbalance and high frequent communications among the domain partitions. To address the problem, we present an approach to flow data partitioning taking account of flow directions and features. Multilevel spectral graph bisection method is employed to reduce communication and synchronization overhead among distributed domains. Edge weights in the corresponding adjacent matrix is defined based on an anisotropic local diffusion operator which assigns strong coupling along flow direction and weak coupling orthogonal to flow. Meanwhile, the distributions of seed points and flow features such as vortex structure are also considered in partitioning so as to obtain good load balance. The experimental results are given to show the feasibility and effectiveness of our method.","Data flow computing,
Distributed computing,
Computer graphics,
Grid computing,
Data visualization,
Concurrent computing,
Anisotropic magnetoresistance,
Large-scale systems,
Parallel algorithms,
Streaming media"
Detection of Plagiarism in Programming Assignments,"Laboratory work assignments are very important for computer science learning. Over the last 12 years many students have been involved in solving such assignments in the authors' department, having reached a figure of more than 400 students doing the same assignment in the same year. This number of students has required teachers to pay special attention to conceivable plagiarism cases. A plagiarism detection tool has been developed as part of a full toolset for helping in the management of the laboratory work assignments. This tool defines and uses four similarity criteria to measure how similar two assignment implementations are. The paper describes the plagiarism detection tool and the experience of using it over the last 12 years in four different programming assignments, from microprogramming a CPU to system programming in C.","computer science education,
C language"
FPGA remote laboratory for hardware e-learning courses,"This paper describes the archicture of a hardware remote laboratory based on using Field Programmable Gate Array (FPGA) development boards for hardware of digital electronic circuits design. The introduced FPGA remote laboratory is suitable for delivering digital design courses as full e-learning courses. By using the FPGA remote laboratory the students can remotely access the FPGA lab through the internet, either by using the Microsoft XP remote connection or through a webpage that has the facilities of configuring the FPGA chip online by sending the bit stream through the internet to the lab PC which is connected through the internet to the lab PC which is connected through USB port to the FPGA board. The FPGA hardware remote laboratory introduced in this article is mainly consists of 20th PCs, and 20th FPGA kits. FPGA Spartan 3E Starter kits from Diligent has been used as a development board for configuring the FPGA Spartan 3E chip from Xilinx; each PC is connected to one FPGA kit and to the campus local area network. The students can remotely login to the lab machine and program the FPGA with their designs as well as testing their designs remotely. VHDL has been used as the design entry to Xilinx ISE Foundation.","Laboratories,
Electronic learning,
Field programmable gate arrays,
Region 8,
Hardware"
Using Wikipedia for Co-clustering Based Cross-Domain Text Classification,"Traditional approaches to document classification requires labeled data in order to construct reliable and accurate classifiers. Unfortunately, labeled data are seldom available, and often too expensive to obtain. Given a learning task for which training data are not available, abundant labeled data may exist for a different but related domain. One would like to use the related labeled data as auxiliary information to accomplish the classification task in the target domain. Recently, the paradigm of transfer learning has been introduced to enable effective learning strategies when auxiliary data obey a different probability distribution. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, we extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. This goal is achieved with the use of Wikipedia. As a result, the pathway that allows to propagate labels between the two domains not only captures common words, but also semantic concepts based on the content of documents. We empirically demonstrate the efficacy of our semantic-based approach to cross-domain classification using a variety of real data.","Wikipedia,
Text categorization,
Training data,
Probability distribution,
Classification algorithms,
Dictionaries,
Bridges,
Data mining,
Computer science,
Asia"
MUTE-AES: A multiprocessor architecture to prevent power analysis based side channel attack of the AES algorithm,"Side channel attack based upon the analysis of power traces is an effective way of obtaining the encryption key from secure processors. Power traces can be used to detect bitflips which betray the secure key. Balancing the bitflips with opposite bitflips have been proposed, by the use of opposite logic. This is an expensive solution, where the balancing processor continues to balance even when encryption is not carried out in the processor.","Algorithm design and analysis,
Cryptography,
Information analysis,
Embedded system,
Performance analysis,
Hamming weight,
Information security,
Power system security,
Mobile handsets,
Personal digital assistants"
GPULib: GPU Computing in High-Level Languages,GPULib helps scientists and engineers take advantage of GPUs from within high-level programming environments without requiring any detailed knowledge of the GPU architecture.,"High level languages,
Computer architecture,
Kernel,
Graphics,
Programming,
Distributed computing,
Tellurium,
Software algorithms,
Costs"
Adaptive Radio Modes in Sensor Networks: How Deep to Sleep?,"Energy-efficient performance is a central challenge in sensor network deployments, and the radio is a major contributor to overall energy node consumption. Current energy- efficient MAC protocols for sensor networks use a fixed low power radio mode for putting the radio to sleep. Fixed low power modes involve an inherent tradeoff: deep sleep modes have low current draw and high energy cost and latency for switching the radio to active mode, while light sleep modes have quick and inexpensive switching to active mode with a higher current draw. This paper proposes adaptive radio low power sleep modes based on current traffic conditions in the network, as an enhancement to our recent RFID impulse low power wake-up mechanism. The paper also introduces a comprehensive node energy model, that includes energy components for radio switching, transmission, reception, listening, and sleeping, as well as the often disregarded micro-controller energy component to evaluate energy performance for both MicaZ and TelosB platforms, which use different MCU's. We then use the model for comparing the energy-related performance of RFIDImpulse enhanced with adaptive low power modes with BMAC and IEEE 802.15.4 for the two node platforms under varying data rates. The comparative analysis confirms that RFIDImpulse with adaptive low power modes provides up to 20 times lower energy consumption than IEEE 802.15.4 in low traffic scenario. The evaluation also yields the optimal settings of low power modes on the basis of data rates for each node platform, and it provides guidelines for the selection of appropriate MAC protocol, low power mode, and node platform for a given set of traffic requirements of a sensor network application.",
A Grid-Inspired Mechanism for Coarse-Grained Experiment Execution,"Stochastic simulations may require many replications until their results are statistically significant. Each replication corresponds to a standalone simulation job, so that these can be computed in parallel. This paper presents a grid-inspired approach to distribute such independent jobsover a set of computing resources that host simulation services, all of which are managed by a central master service. Our method is fully integrated with alternative ways of distributed simulation in JAMES II, hides all execution details from the user, and supports the coarse-grained parallel execution of any sequential simulator available in JAMES II. A thorough performance analysis of the new execution mode illustrates its efficiency.","Computational modeling,
Grid computing,
Computer simulation,
Concurrent computing,
Distributed computing,
Resource management,
Analytical models,
Partitioning algorithms,
Application software,
Computer science"
Probabilistic Event Extraction from RFID Data,"We present PEEX, a system that enables applications to define and extract meaningful probabilistic high-level events from RFID data. PEEX effectively copes with errors in the data and the inherent ambiguity of event extraction.","Data mining,
Radiofrequency identification,
Uncertainty,
Event detection,
Cleaning,
Face detection,
Computer science,
Data engineering,
Application software,
Computer errors"
Filtering Internet image search results towards keyword based category recognition,"In this work we aim to capitalize on the availability of Internet image search engines to automatically create image training sets from user provided queries. This problem is particularly difficult due to the low precision of image search results. Unlike many existing dataset gathering approaches, we do not assume a category model based on a small subset of the noisy data or an ad-hoc validation set. Instead we use a nonparametric measure of strangeness [8] in the space of holistic image representations, and perform an iterative feature elimination algorithm to remove the most strange examples from the category. This is the equivalent of keeping only features that are found to be consistent with others in the class. We show that applying our method to image search data before training improves average recognition performance, and demonstrate that we obtain comparative precision and recall results to the current state of the art, all the while maintaining a significantly simpler approach. In the process we also extend the strangeness-based feature elimination algorithm to automatically select good threshold values and perform filtering of a single class when the background is given.","Information filtering,
Information filters,
Internet,
Image recognition,
Training data,
Search engines,
Filtering algorithms,
Testing,
Noise reduction,
Computer science"
OptRR: Optimizing Randomized Response Schemes for Privacy-Preserving Data Mining,"The randomized response (RR) technique is a promising technique to disguise private categorical data in Privacy-Preserving Data Mining (PPDM). Although a number of RR-based methods have been proposed for various data mining computations, no study has systematically compared them to find optimal RR schemes. The difficulty of comparison lies in the fact that to compare two PPDM schemes, one needs to consider two conflicting metrics: privacy and utility. An optimal scheme based on one metric is usually the worst based on the other metric. In this paper, we first describe a method to quantify privacy and utility. We formulate the quantification as estimate problems, and use estimate theories to derive quantification. We then use an evolutionary multi-objective optimization method to find optimal disguise matrices for the randomized response technique. The experimental results have shown that our scheme has a much better performance than the existing RR schemes.","Data mining,
Data privacy,
Optimization methods,
Data engineering,
Computer science,
Estimation theory,
Protection,
Aggregates,
Mean square error methods"
Machine Learning Approach for Ontology Mapping Using Multiple Concept Similarity Measures,"This paper presents a new framework for the ontology mapping problem. We organized the ontology mapping problem into a standard machine learning framework, which uses multiple concept similarity measures. We presented several concept similarity measures for the machine learning framework and conducted experiments for testing the framework using real-world data. Our experimental results show that our approach has increased performance with respect to precision, recall and F-measure in comparison with other methods.","Machine learning,
Ontologies,
Internet,
Informatics,
Measurement standards,
Semantic Web,
Information science,
Testing,
Decision making,
Joining processes"
High-dimensional underactuated motion planning via task space control,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.","Aerospace electronics,
Joints,
Planning,
Trajectory,
Robots,
Torque,
Control systems"
Grid-interfacing active power filters to improve the power quality in a microgrid,"This paper proposes the application of the strategy control approach used for the shunt active power filter previously proposed by the authors to the inverter interface of each single micro-sources present in a Microgrid. In particular the attention has been focused on the power quality and var supporting issues. The main advantage of the proposed strategy control approach lies on the fact that all sensitive loads connected to the Point of Common Coupling (PCC) are immunized from the power quality problems. Moreover, the proposed control approach requests only local information. The effectiveness of the proposed control strategy approach applied to a microgrid is illustrated.","Active filters,
Power quality,
Voltage,
Inverters,
Power electronics,
Switches,
Control systems,
Reactive power,
Circuit faults,
Rectifiers"
Design and Optimization of Low-Noise Wide-Bandwidth Charge Preamplifiers for High Purity Germanium Detectors,"Design criteria for low-noise wide-bandwidth charge-sensitive preamplifiers for highly-segmented HPGe detectors are presented. The attention is focused on the optimization of the preamplifier noise, long-term gain stability, and bandwidth. The charge-sensitive preamplifiers of AGATA, i.e., the Advanced GAmma Tracking Array detector for next generation nuclear physics experiments, have been designed, realized and optimized using the proposed techniques. The circuit, in conjunction with the detector, provided an Equivalent Noise Charge of 101 electrons r.m.s., a rise time of ~ 8.3 ns, no appreciable line shift in long term acquisitions, a dynamic range of as much as 92 dB. An analytical study of the circuit is made. Computer simulations and experimental results are shown and critically discussed.","Design optimization,
Preamplifiers,
Germanium,
Circuit noise,
Circuit stability,
Bandwidth,
Sensor arrays,
Gamma ray detection,
Gamma ray detectors,
Nuclear physics"
Performance evaluation of state-of-the-art discrete symmetry detection algorithms,"Symmetry is one of the important cues for human and machine perception of the world. For over three decades, automatic symmetry detection from images/patterns has been a standing topic in computer vision. We present a timely, systematic, and quantitative performance evaluation of three state of the art discrete symmetry detection algorithms. This evaluation scheme includes a set of carefully chosen synthetic and real images presenting justified, unambiguous single or multiple dominant symmetries, and a pair of well-defined success rates for validation. We make our 176 test images with associated hand-labeled ground truth publicly available with this paper. In addition, we explore the potential contribution of symmetry detection for object recognition by testing the symmetry detection algorithm on three publicly available object recognition image sets (PASCAL VOC'07, MSRC and Caltech-256). Our results indicate that even after several decades of effort, symmetry detection in real-world images remains a challenging, unsolved problem in computer vision. Meanwhile, we illustrate its future potential in object recognition.","Detection algorithms,
Reflection,
Computer vision,
Testing,
Object detection,
Object recognition,
Image edge detection,
Detectors,
Image databases,
Computer science"
Optimizing Text Summarization Based on Fuzzy Logic,In this paper we first analyze some state of the art methods for text summarization. We discuss what the main disadvantages of these methods are and then propose a new method using fuzzy logic. Comparisons of results show that our method beats most methods which use machine learning as their core,"Fuzzy logic,
Data mining,
Internet,
Information science,
Optimization methods,
Fuzzy systems,
Information analysis,
Machine learning,
Natural language processing,
Automatic generation control"
Height Ridge Computation and Filtering for Visualization,"Motivated by the growing interest in the use of ridges in scientific visualization, we analyze the two height ridge definitions by Eberly and Lindeberg. We propose a raw feature definition leading to a superset of the ridge points as obtained by these two definitions. The set of raw feature points has the correct dimensionality, and it can be narrowed down to either Eberly's or Lindeberg's ridges by using Boolean filters which we formulate. While the straight-forward computation of height ridges requires explicit eigenvalue calculation, this can be avoided by using an equivalent definition of the raw feature set, for which we give a derivation. We describe efficient algorithms for two special cases, height ridges of dimension one and of co-dimension one. As an alternative to the aforementioned filters, we propose a new criterion for filtering raw features based on the distance between contours which generally makes better decisions, as we demonstrate on a few synthetic fields, a topographical dataset, and a fluid flow simulation dataset. The same set of test data shows that it is unavoidable to use further filters to eliminate false positives. For this purpose, we use the angle between feature tangent and slope line as a quality measure and, based on this, formalize a previously published filter.","Filtering,
Surface topography,
Data visualization,
Filters,
Computer vision,
Integral equations,
Eigenvalues and eigenfunctions,
Fluid flow,
Computational modeling,
Testing"
A Counterexample to Strong Parallel Repetition,"The parallel repetition theorem states thatfor any {\em two-prover game}, with value 1- \epsilon
(for, say, \epsilon \leq 1/2
), the value of the game repeatedin paralleln
times is at most(1- \epsilon^c)^{\Omega(n/s)}
, where s
is the answers' length(of the original game) and c
is a universalconstant~\cite{R}.Several researchers asked wether this bound could be improvedto (1- \epsilon)^{\Omega(n/s)}
; this question is usually referred toas the {\em strong parallel repetition problem}.We show that the answer for this question is negative.More precisely, we consider the{\em odd cycle game} ofsize m
; a two-prover game with value 1-1/2m
. We show that thevalue of the odd cycle game repeated in parallel n
times is at least1- (1/m) \cdot O(\sqrt{n})
. This implies thatfor large enough n
(say, n \geq \Omega(m^2)
), thevalue of the odd cycle game repeated in parallel n
times is at least(1- 1/4m^2)^{O(n)}
.Thus:\begin{enumerate}\item For parallel repetition of general games:the bounds of $(1- \epsilon^c)^{\Omega(n/s)}$ given in~\cite{R,Hol} areof the right form, up to determining the exact value of the constant$c \geq 2$.\item For parallel repetition of XOR games, unique gamesand projection games:the bounds of $(1- \epsilon^2)^{\Omega(n)}$ givenin~\cite{FKO} (for XOR games) andin~\cite{Rao} (for unique and projection games) are tight.\item For parallel repetition of the odd cycle game:the bound of $1- (1/m) \cdot \tilde{\Omega}(\sqrt{n})$ givenin~\cite{FKO} is almost tight.\end{enumerate}
A major motivationfor the recent interest in the strong parallel repetitionproblem is that a strong parallel repetition theoremwould have implied that the{\em unique game conjecture} is equivalentto the NP hardness of distinguishing between instances of Max-Cutthat are at least 1 - \epsilon^2
satisfiable  from instancesthat are at most 1 - (2/\pi) \cdot \epsilon
satisfiable.Our results suggest that this cannot be proved just by improvingthe known bounds on  parallel repetition.","Game theory,
Computer science,
Radio access networks,
Protocols"
Automating phishing website identification through deep MD5 matching,"The timeliness of Phishing Incident Response is hindered by the need for human verification of whether suspicious URLs are actually phishing sites. This paper presents a method for automating the determination, and demonstrates the effectiveness of this method in reducing the number of suspicious URLs that need human review through a method of comparing new URLs and their associated web content with previously archived content of confirmed phishing sites. The results can be used to automate shutdown requests, to supplement traditional “URL black list” toolbars allowing blocking of previously unreported URLs, or to indicate dominant phishing site patterns which can be used to prioritize limited investigative resources.","Humans,
Uniform resource locators,
Security,
Computer crime,
Forensics,
Internet,
Delay,
Face,
Statistics,
HTML"
Design consideration of high temperature SiC power modules,"SiC power semiconductors can safely operate at a junction temperature of 500°C. Such a high operating temperature range can substantially relax or completely eliminate the need for bulky and costly cooling components commonly used in silicon-based power electronic systems. However, a major limitation to fully realizing the potential of SiC and other wide band-gap semiconductor materials is the lack of qualified high-temperature packaging systems, particularly those with high-current and high-voltage capabilities required for power conversion applications. This paper proposes a new hybrid power module architecture that allows wide bandgap semiconductor power devices to operate at a junction temperature of 300°C. The concept is based on the use of double metal or DCB leadframes, direct leadframe-to-chip bonding, and high temperature encapsulation materials. The leadframes, serving as both the external leads and the internal interconnect to the semiconductor chips, need to provide excellent high temperature stability, adequate electrical and thermal conductivity, and a coefficient of thermal expansion (CTE) closely matching that of SiC. The SiC chips are sandwiched between and bonded to the top and bottom leadframes using a brazing or adhesion process. Extensive electrical, thermal, and mechanical modeling has been performed on this new concept. Several prototypes are fabricated, and a finite element model is evaluated. Packaging architecture and materials considerations are discussed.","Temperature,
Silicon carbide,
Multichip modules,
Thermal conductivity,
Wide band gap semiconductors,
Semiconductor materials,
Packaging,
Bonding,
Lead compounds,
Thermal expansion"
Stereo-based Free Space Computation in Complex Traffic Scenarios,This paper presents a method for the computation of free space in complex traffic scenarios. Dynamic depth information is estimated by integrating stereo disparity images over time. Disparity and disparity speed are computed pixel-wise with Kalman filters. The stereo information is used to compute stochastic occupancy grids. Dynamic programming on a polar-like occupancy grid yields the free space. An analysis of the calculated free space allows the detection of the available free corridor in front of the ¿ego-vehicle¿. The method runs at a frame rate of 20 Hz in our demonstrator vehicle.,"Grid computing,
Stochastic resonance,
Pixel,
Dynamic programming,
Space vehicles,
Time measurement,
Noise measurement,
Computer science,
Vehicle dynamics,
Stochastic processes"
Bias and Controversy in Evaluation Systems,"Evaluation is prevalent in real life. With the advent of Web 2.0, online evaluation has become an important feature in many applications that involve information (e.g., video, photo, and audio) sharing and social networking (e.g., blogging). In these evaluation settings, a set of reviewers assign scores to a set of objects. As part of the evaluation analysis, we want to obtain fair reviews for all the given objects. However, the reality is that reviewers may deviate in their scores assigned to the same object, due to the potential ""bias"" of reviewers or ""controversy"" of objects. The statistical approach of averaging deviations to determine bias and controversy assumes that all reviewers and objects should be given equal weight. In this paper, we look beyond this assumption and propose an approach based on the following observations: 1) evaluation is ""subjective,"" as reviewers and objects have varying bias and controversy, respectively, and 2) bias and controversy are mutually dependent. These observations underlie our proposed reinforcement-based model to determine bias and controversy simultaneously. Our approach also quantifies ""evidence,"" which reveals the degree of confidence with which bias and controversy have been derived. This model is shown to be effective by experiments on real-life and synthetic data sets.","Social network services,
Video sharing,
Blogs,
Cost accounting,
Peer to peer computing,
Measurement standards,
Equations"
A Multi-mode Real-Time Calculus,"The Real-Time Calculus (RTC) framework proposed in [Chakraborty et al., DATE 2003] and subsequently extended in [Wandeler et al., Real-Time Systems 29(2-3), 2005] and a number of other papers is geared towards the analysis of real-time systems that process various types of streaming data. The main strength of RTC is a count-based abstraction, where arrival patterns of event streams are specified as constraints on the number of events that may arrive over any specified time interval. In this framework, algebraic techniques can be used to compute system properties in a compositional way. However, the main drawback of RTC is that it cannot model state information in a natural way. For example, when a scheduling policy depends on the fill-level of a certain buffer or there is a shift from one type of data stream into another. In this paper, we extend RTC in a manner that enables state information to be easily captured while limiting the state-space explosion caused by fine grained state-based models such as timed automata. Our model, called ""multi-mode RTC"", specifies event streams as finite automata whose states are annotated with functions that specify constraints on the arrival patterns of event streams or the service available to process them. Our new framework combines the expressiveness of state-based models with the algebraic and compositional features of the RTC formalism. In particular, system properties within a single mode can be analyzed using the RTC-based algebraic techniques and state-space exploration can be used to piece together the results obtained algebraically for the individual modes. We show how to determine typical system properties with the focus on efficient approximate techniques and illustrate the advantages of multi-mode RTC using two case studies.",
Simulation Tool for the Prediction of Heavy Ion Cross Section of Innovative 130-nm SRAMs,"The prediction of heavy ions cross sections of an innovative SRAM is carried out in a 130-nm CMOS technology. The work is realized with a simulation tool generally used to predict standard SRAM sensitivity. This simulation tool needs input parameters to describe both standard and innovative SRAM. From the standard to the innovative structure, input parameters are simply evaluated using SPICE simulations. Experimental and simulated cross sections are then shown to be in good agreement.","Predictive models,
Random access memory,
CMOS technology,
SPICE,
Computer aided manufacturing,
Design automation,
Capacitors,
Computational modeling,
Analytical models,
Single event upset"
Automated Aspect Recommendation through Clustering-Based Fan-in Analysis,"Identifying code implementing a crosscutting concern (CCC) automatically can benefit the maintainability and evolvability of the application. Although many approaches have been proposed to identify potential aspects, a lot of manual work is typically required before these candidates can be converted into refactorable aspects. In this paper, we propose a new aspect mining approach, called clustering-based fan-in analysis (CBFA), to recommend aspect candidates in the form of method clusters, instead of single methods. CBFA uses a new lexical based clustering approach to identify method clusters and rank the clusters using a new ranking metric called cluster fan- in. Experiments on Linux and JHotDraw show that CBFA can provide accurate recommendations while improving aspect mining coverage significantly compared to other state-of-the-art mining approaches.","Linux,
Java,
Kernel,
Clustering algorithms,
Synchronization,
Data mining,
Manuals"
A fast Multi-Scale Retinex algorithm for color image enhancement,"This paper puts forward a Fast Multi-Scale Retinex Algorithm in order to solve the problem of color distortion and to improve the disadvantages of a slowly time-consuming arithmetic in the course of image enhancement which is based on the multi-scale retinex algorithm. Through construction of average value templates in advance, Gaussian convolution operation is simplified to be average arithmetic. A new rational color space is constructed which possesses the advantages of simple space conversion and small quantity of calculation. Meanwhile it keeps the color image in a better way. The experimental results demonstrate that the Fast Multi-Scale Retinex Algorithm can ensure the treated effect and improve the efficiency of execution; furthermore it can run in the personal computers and the embedded environment. Therefore it has much better practical value.","Image color analysis,
Algorithm design and analysis,
Color,
Convolution,
Wavelet analysis,
Pattern recognition,
Filtering"
An optimal checkpoint/restart model for a large scale high performance computing system,"The increase in the physical size of high performance computing (HPC) platform makes system reliability more challenging. In order to minimize the performance loss (rollback and checkpoint overheads) due to unexpected failures or unnecessary overhead of fault tolerant mechanisms, we present a reliability-aware method for an optimal checkpoint/restart strategy. Our scheme aims at addressing fault tolerance challenge, especially in a large-scale HPC system, by providing optimal checkpoint placement techniques that are derived from the actual system reliability. Unlike existing checkpoint models, which can only handle Poisson failure and a constant checkpoint interval, our model can deal with a varying checkpoint interval and with different failure distributions. In addition, the approach considers optimality for both checkpoint overhead and rollback time. Our validation results suggest a significant improvement over existing techniques.","Large-scale systems,
High performance computing,
Checkpointing,
Reliability,
Cost function,
Fault tolerant systems,
Mathematical model,
Mathematics,
Stochastic processes,
Data analysis"
3D Facial Landmark Localisation by Matching Simple Descriptors,"We present our graph matching approach for 3D facial feature localisation. The work here uses a basic graph model (three vertices and three arcs) to locate the inner eye corners and the nose tip simultaneously. We intend to extend this to a larger set of the eleven features that exist in our ground truth of the Face Recognition Grand Challenge (FRGC) database. We apply the structural matching algorithm ""relaxation by elimination"" using a simple ""distance to local plane"" node property and a ""Euclidean distance"" arc property. After the graph matching process has eliminated unlikely candidates, the most likely feature combination (left eye, right eye and nose tip) is selected, by exhaustive search, as the minimum Mahalanobis distance over a six dimensional space, corresponding to three node variables and three arc variables. Our results on the 3D FRGC database are presented and discussed.","Nose,
Spatial databases,
Facial animation,
Facial features,
Face recognition,
Image databases,
Shape,
Robustness,
Support vector machines,
Support vector machine classification"
Transmitter Optimization and Beamforming Optimality Conditions for Double-Scattering MIMO Channels,"We investigate the capacity of a general class of MIMO channels, known as double-scattering channels. Our results assume perfect channel state information (CSI) at the receiver and statistical CSI at the transmitter. We first derive the optimal capacity-achieving signaling directions, and show that they correspond to the eigenvectors of the transmit spatial correlation matrix. We then derive new simple closed-form power allocation policies which are shown to yield negligible capacity loss compared with the optimal power allocation policy, without requiring complicated numerical optimization methods to compute. Finally we derive a necessary and sufficient condition for which a beamforming approach (i.e. rank-1 transmission) achieves the ergodic capacity.","Transmitters,
Array signal processing,
MIMO,
Channel state information,
Rayleigh scattering,
Wireless communication,
Receiving antennas,
Rayleigh channels,
Optimization methods,
Sufficient conditions"
Troubleshooting thousands of jobs on production grids using data mining techniques,Large scale production computing grids introduce new challenges in debugging and troubleshooting. A user that submits a workload consisting of tens of thousands of jobs to a grid of thousands of processors has a good chance of receiving thousands of error messages as a result. How can one begin to reason about such problems? We propose that data mining techniques can be employed to classify failures according to the properties of the jobs and machines involved. We demonstrate this technique through several case studies on real workloads consisting of tens of thousands of jobs. We apply the same techniques to a year’s worth of data on a 3000 CPU production grid and use it to gain a high level understanding of the system behavior.,"Decision trees,
Data mining,
Production,
Linux,
Debugging,
Distance measurement,
Radio access networks"
Segmented Digital Clock Manager- FPGA based Digital Pulse Width Modulator Technique,"A new Digital Pulse Width Modulator (DPWM) design for a Field Programmable Gate Array (FPGA) based systems is presented in this paper. The proposed architecture fully utilizes the Digital Clock Manager (DCM) resources available on new FPGA boards. The proposed Segmented DCM DPWM is a digital modulator architecture with low power that allows for high switching frequency operation. It relies on the power-optimized resources already existing on new FPGAs. The inherit phase shifting properties of the DCM blocks simplify the duty cycle generation. The architecture can be applied to achieve various number of bits for the DPWM resolution, and is implemented and verified experimentally on a Virtex4 FPGA board.","Clocks,
Radiation detectors,
Switching frequency,
Field programmable gate arrays,
Delay,
Simulation,
Oscillators"
Face illumination normalization on large and small scale features,"It is well known that the effect of illumination is mainly on the large-scale features (low-frequency components) of a face image. In solving the illumination problem for face recognition, most (if not all) existing methods either only use extracted small-scale features while discard large-scale features, or perform normalization on the whole image. In the latter case, small-scale features may be distorted when the large-scale features are modified. In this paper, we argue that large-scale features of face image are important and contain useful information for face recognition as well as visual quality of normalized image. Moreover, this paper suggests that illumination normalization should mainly perform on large-scale features of face image rather than the whole face image. Along this line, a novel framework for face illumination normalization is proposed. In this framework, a single face image is first decomposed into large- and small- scale feature images using logarithmic total variation (LTV) model. After that, illumination normalization is performed on large-scale feature image while small-scale feature image is smoothed. Finally, a normalized face image is generated by combination of the normalized large-scale feature image and smoothed small-scale feature image. CMU PIE and (Extended) YaleB face databases with different illumination variations are used for evaluation and the experimental results show that the proposed method outperforms existing methods.","Lighting,
Large-scale systems,
Face recognition,
Image generation,
Principal component analysis,
Discrete wavelet transforms,
Sun,
Feature extraction,
Independent component analysis,
Linear discriminant analysis"
An Algorithm of Webpage Information Hiding Based on Attributes Permutation,"Based on attributes permutation, an algorithm of webpage information hiding is proposed, which overcomes the shortcoming of poor imperceptibility of webpage algorithms currently in use, and improves the hiding capacity of the webpage algorithm based on the order of attributes pair. The message , which is embedded into the webpage, is firstly changed to a large number , and then hidden in the webpage by modifying the order of attributes, according to the embedded rule. The algorithm doesn’t length the file and has preferable imperceptibility. Although a number of messages are embedded, statistical singularity could never be generated. So the algorithm could contradict with detection, and be used to webpage protection and covert communication.","Web pages,
Artificial neural networks,
Argon,
Signal processing algorithms,
HTML,
World Wide Web,
Security"
Detecting Bots Based on Keylogging Activities,"A bot is a piece of software that is usually installed on an infected machine without the user’s knowledge. A bot is controlled remotely by the attacker under a Command and Control structure. Recent statistics show that bots represent one of the fastest growing threats to our network by performing malicious activities such as email spamming or keylogging.  However, few bot detection techniques have been developed to date. In this paper, we investigate a behavioural algorithm to detect a single bot that uses keylogging activity. Our approach involves the use of function calls analysis for the detection of the bot with a keylogging component. Correlation of the frequency of function calls made by the bot with other system signals during a specified time-window is performed to enhance the detection scheme. We perform a range of experiments with the spybot.  Our results show that there is a high correlation between some function calls executed by this bot which indicates abnormal activity in our system.","Command and control systems,
Protocols,
Communication system control,
Computer security,
Information security,
Frequency,
Mice,
Availability,
Computer science,
Information technology"
Palmprint Verification using SIFT features,"This paper describes the design and development of a prototype of robust biometric system for personnel verification. The system uses features extracted using Scale Invariant Feature Transform (SIFT) operator of human hand. The hand image for features is acquired using a low cost scanner. The palmprint region extracted is robust to hand translation and rotation on the scanner. The use of SIFT operator for feature extractions makes the system robust to scale or spatial resolution of the hand images acquired. The system is tested on IITK database of 200 images and PolyU database of 7751 images. The design of the system with low cost scanner as input device, robustness to translation, rotation and spatial resolution, and testing performance, FAR 0.02%, FRR 0.62%, and accuracy 99.67% suggests that the system can be used for civilian applications and high-security environments.","Robustness,
Feature extraction,
Costs,
Spatial resolution,
System testing,
Image databases,
Spatial databases,
Prototypes,
Biometrics,
Personnel"
On Maintaining Sensor-Actor Connectivity in Wireless Sensor and Actor Networks,"In wireless sensor and actor networks (WSANs), a group of sensors and actors are connected by a wireless medium to perform distributed sensing and acting tasks. Sensors usually gather information in an event area and pass it on to actors, which are resource-rich devices that make decisions and perform necessary actions. Therefore, it is vital to maintain connections between sensors and actors for effective sensor- actor coordination. In this paper, we first define several sensor- actor connection requirements, including weak and strong actor-connectivity, and then propose several local solutions that put as many sensors as possible to sleep for energy saving purposes, while meeting different actor-connectivity requirements. We also prove the relationship between the proposed actor-connectivity and the connectivity in regular graphs, which helps with the implementation of the proposed solutions. Comprehensive performance analysis is conducted through simulations.","Wireless sensor networks,
Sensor phenomena and characterization,
Routing,
Computer science,
Communications Society,
Maintenance engineering,
Sleep,
Performance analysis,
Analytical models,
Actuators"
Impact of workspace decompositions on discrete search leading continuous exploration (DSLX) motion planning,"We have recently proposed DSLX, a motion planner that significantly reduces the computational time for solving challenging kinodynamic problems by interleaving continuous state-space exploration with discrete search on a workspace decomposition. An important but inadequately understood aspect of DSLX is the role of the workspace decomposition on the computational efficiency of the planner. Understanding this role is important for successful applications of DSLX to increasingly complex robotic systems. This work shows that the granularity of the workspace decomposition directly impacts computational efficiency: DSLX is faster when the decomposition is neither too fine-nor too coarse-grained. Finding the right level of granularity can require extensive fine-tuning. This work demonstrates that significant computational efficiency can instead be obtained with no fine-tuning by using conforming Delaunay triangulations, which in the context of DSLX provide a natural workspace decomposition that allows an efficient interplay between continuous state-space exploration and discrete search. The results of this work are based on extensive experiments on DSLX using grid, trapezoidal, and triangular decompositions of various granularities to solve challenging first and second-order kinodynamic motion-planning problems.",
Tracking source locations,Many programming tools require information to be associated with source locations. Current tools do this in different ways with different degrees of effectiveness. This paper is an investigation into the various approaches to maintaining source locations. It is based on an experiment that attempts to track a variety of locations over the evolution of a source file. The results demonstrate that relatively simple techniques can be very effective.,"Position measurement,
Programming profession,
Animation,
Computer science,
Computer bugs,
Java,
Visualization,
Software algorithms,
Radio access networks,
Maintenance engineering"
Mobile Sensor Relocation to Prolong the Lifetime of Wireless Sensor Networks,"The wireless sensor network (WSN) has recently attracted considerable attention due to the low price and ease to deploy it. In particular, in a hostile or harsh regions where sensors cannot be deployed manually, WSNs can be established just by dropping sensors from the air. In this case, however, most likely sensors are not placed at optimal positions, although the location of sensors does have a drastic impact on the WSN performance. Moreover, randomized deployment algorithm can leave holes in terms of coverage in the sensing area. This paper proposes a sensor relocation scheme where mobile sensors move to patch up the holes by appropriate coverage. Simulation results show that the proposed algorithm outperforms prior existing schemes in terms of coverage and lifespan of WSNs.","Wireless sensor networks,
Mechanical sensors,
Costs,
Mobile computing,
Sensor phenomena and characterization,
Computer science,
Computer networks,
Distributed computing,
Temperature sensors,
Humidity"
Distributed interference pricing for OFDM wireless networks with non-separable utilities,"We present a distributed algorithm for allocating power among multiple interfering transmitters in a wireless network using Orthogonal Frequency Division Multiplexing (OFDM). The algorithm attempts to maximize the sum over user utilities, where each user¿s utility is a function of his total transmission rate. Users exchange interference prices reflecting the marginal cost of interference on each sub-channel, and then update their power allocations given the interference prices and their own channel conditions. A similar algorithm was studied earlier assuming that each user¿s utility function is a separable function of the user¿s rate per sub-channel. Here, we do not assume this separability. We give a different algorithm for updating each user¿s power allocation and show that this algorithm converges monotonically. Numerical results comparing this algorithm to several others are also presented.","Interference,
Pricing,
OFDM,
Wireless networks,
Iterative algorithms,
Distributed algorithms,
Transmitters,
Costs,
Signal to noise ratio,
Control systems"
Fingerprint Liveness Detection Using Curvelet Energy and Co-Occurrence Signatures,"This paper proposes a new curvelet transform-based method to detect spoof fingerprint attacks in fingerprint biometric systems. It uses only one image to differentiate a real fingerprint from a spoof one. It is based on the observation that, real and spoof fingerprints exhibit different textural characteristics. Textural measures based on curvelet energy signatures and curvelet co-occurrence signatures are used to characterize fingerprint texture. Dimensionalities of the feature sets are reduced by running Pudil's Sequential Forward Floating Selection (SFFS) algorithm. We test two feature sets independently on various classifiers like: AdaBoost. M1, support vector machine and k-nearest neighbor; then we fuse all the mentioned classifiers using the ""Majority Voting Rule"" to form an Ensemble classifier. Classification rates achieved with these classifiers for energy signatures range from ~94.12% to ~97.41%. Likewise, classification rates for co-occurrence signatures range from ~94.35% to ~98.12%. Thus, the performance of a new liveness detection approach is very promising, as it needs only one fingerprint and no extra hardware to detect vitality.","Fingerprint recognition,
Transforms,
Wavelet transforms,
Classification algorithms,
Fingers,
Support vector machines,
Materials"
Position-based unicast routing for city scenarios,"In Vehicular Ad-Hoc Networks unicast packet forwarding can be separated into the one-dimensional highway case and the two-dimensional city case; in this paper, we deal with the latter. We survey existing position-based routing protocols and present GRANT, our own approach of greedy routing with an abstract neighbor table. We simulate each protocol in our city scenario of Karlsruhe, consisting of streets with a length of 66 km, 390 junctions, and radio obstacles derived from high-definition satellite images. We also simulate the protocols with a FACE-2- and a distance vector-based recovery strategy. As a result we propose GRANT as a routing protocol for unicast city scenarios.","Routing,
Cities and towns,
Junctions,
Distance measurement,
Protocols,
Routing protocols,
Road transportation"
Normalized tree partitioning for image segmentation,"In this paper, we propose a novel graph based clustering approach with satisfactory clustering performance and low computational cost. It consists of two main steps: tree fitting and partitioning. We first introduce a probabilistic method to fit a tree to a data graph under the sense of minimum entropy. Then, we propose a novel tree partitioning method under a normalized cut criterion, called Normalized Tree Partitioning (NTP), in which a fast combinatorial algorithm is designed for exact bipartitioning. Moreover, we extend it to k-way tree partitioning by proposing an efficient best-first recursive bipartitioning scheme. Compared with spectral clustering, NTP produces the exact global optimal bipartition, introduces fewer approximations for k-way partitioning and can intrinsically produce superior performance. Compared with bottom-up aggregation methods, NTP adopts a global criterion and hence performs better. Last, experimental results on image segmentation demonstrate that our approach is more powerful compared with existing graph-based approaches.","Image segmentation,
Tree graphs,
Computational efficiency,
Joining processes,
Clustering methods,
Clustering algorithms,
Algorithm design and analysis,
Pattern recognition,
Asia,
Entropy"
New Coupling and Cohesion Metrics for Evaluation of Software Component Reusability,"An account of new measure of coupling and cohesiondeveloped to assess the reusability of Java components isproposed in this paper. These measures differ from themajority of established metrics in two respects: they reflectthe degree to which entities are coupled or resemble eachother, and they take account of indirect couplings orsimilarities. An empirical comparison of the new measureswith eight established metrics is described. The newmeasures are shown to be consistently superior atmeasuring component reusability.","Software reusability,
Computer science,
Software measurement,
Java,
Software maintenance,
Search engines,
Internet,
Software systems"
Application of adaptive probing for fault diagnosis in computer networks,This dissertation presents an adaptive probing based tool for fault diagnosis in computer networks by addressing the problems of probe station selection and probe selection. We first present algorithms to place probe stations to monitor the network in the presence of various failures in the network. We then present algorithms for probe selection in an adaptive manner to perform fault diagnosis. We present algorithms considering both deterministic as well as non-deterministic environments. We present evaluation of the proposed algorithms through comprehensive simulation studies. The dissertation is available at http://www.cis.udel.edu/∼natu/papers/dissertation.pdf.,"Application software,
Fault diagnosis,
Computer networks,
Probes,
Telecommunication traffic,
Condition monitoring,
Instruments,
Computer network management,
Government,
Adaptive systems"
Applying causal inference to understand emergent behavior,"Emergent behaviors in simulations require explanation, so that valid behaviors can be separated from design or coding errors. Validation of emergent behavior requires accumulation of insight into the behavior and the conditions under which it arises. Previously, we have introduced an approach, Explanation Exploration (EE), to gather insight into emergent behaviors using semi-automatic model adaptation. We improve our previous work by iteratively applying causal inference procedures to samples gathered from the semi-automatic model adaptation. Iterative application of causal inference procedures reveals the interactions of identified abstractions within the model that cause the emergent behavior. Uncovering these interactions gives the subject matter expert new insight into the emergent behavior and facilitates the validation process.","Debugging,
Computational modeling,
Testing,
Probability distribution,
Adaptation model,
Research initiatives,
Computer science,
Computer simulation,
Computer errors,
Process control"
LKE: A Self-Configuring Scheme for Location-Aware Key Establishment in Wireless Sensor Networks,"Symmetric key agreement is significant to security provisioning in sensor networks with resource limitations. A number of pairwise key pre-distribution protocols have been proposed, but their scalability is often constrained by the conflict between the desired probability of sharing keys between neighboring nodes and the resilience against node capture attacks under a given budget for storing keying information within each sensor. In this paper, we propose LKE, a self-configuring in-situ key establishment scheme targeting large-scale sensor networks. LKE employs location information for a deterministic key space generation and keying information distribution. For uniformly distributed networks, LKE exhibits strong resilience against node capture attacks and achieves a high key-sharing probability (close to 1) at the expense of a small amount of memory overhead. An improvement over LKE, termed as iLKE, is also proposed. iLKE is topology-adaptive, and therefore works well for both uniform and non-uniform network models. We conduct both theoretic analysis and simulation study to evaluate the performances of LKE and iLKE.","Wireless sensor networks,
Intelligent sensors,
Information security,
Protocols,
Scalability,
Resilience,
Large-scale systems,
Cryptography,
Computer science,
Performance analysis"
"A High Payload Steganographic Scheme Based on (7, 4) Hamming Code for Digital Images","High payload information hiding schemes with the good visual quality of stego images are suitable for steganographic applications such as online content distribution systems. This paper proposes a novel steganographic scheme based on the (7, 4) Hamming code for digital images.The proposed scheme embeds a segment of seven secret bits into a group of seven cover pixels at a time. The experimental results show that the proposed scheme achieves a double embedding payload and a slightly lower visual quality of stego images compared with the related works.","Pixel,
Payloads,
Parity check codes,
Encoding,
Receivers,
Image segmentation,
Computer science"
Object-Blog System for Environment-Generated Content,"The object-blog service application automatically converts raw sensor data to environment-generated content (EGC), including texts, graphs, and figures. This conversion facilitates data searching and browsing. Generated content can serve several purposes, including memory aids, security, and communication media. In object-blog, personified objects automatically post entries to a Weblog about sensor data obtained from sensors attached to the objects. Feedback thus far from participants working with object-blog in an experimental environment has been positive.","Sensor phenomena and characterization,
Event detection,
Cameras,
Accelerometers,
Acoustic sensors,
Thermal sensors,
Wearable sensors,
Sensor systems,
Videos,
Microphones"
Predicting neutron induced soft error rates: Evaluation of accelerated ground based test methods,"In this work, heavy ion and energetic proton single event upset (SEU) cross sections are measured for a 4 Mbit CMOS, static random access memory (SRAM). Heavy ion upset cross sections were used to define a dosimetry model suitable for use in a Monte-Carlo, physics-based transport code, which is shown to be predictive for experimentally measured proton single event upset (SEU) cross sections. The simulator was used to quantify the difference between neutron and proton SEU cross sections and to evaluate the fidelity of currently established rate prediction methods. Simulations indicate that established test methods under-predict the FIT rate between 26- 35% for this technology.","Neutrons,
Error analysis,
Life estimation,
Testing,
Single event upset,
Protons,
Predictive models,
Energy measurement,
SRAM chips,
Random access memory"
Content-Based Intelligent Routing and Message Processing in Enterprise Service Bus,"Enterprise Service Bus (ESB) provides a layer on top of an implementation of an enterprise messaging system. The ESB provides a highly distributed, event-driven Service Oriented Architecture (SOA) that combines Message Oriented Middleware (MOM), Web services,intelligent routing based on content, and XML data transformation. ESB transmits and receives standard set of messages. When it receives a message, it routes it to the appropriate application. Content-based intelligent routing supports service selection and composition. However, current ESB software can only support fixed routing by static configuration files. This paper proposes framework to enable the content-based intelligent routing path construction and message routing. In proposed framework there are 3 main layers: Business Process, ESB and Implementation Layer to define the routing tables and mechanisms of message routings, to support service selection preferences and facilitate the service selection based on message content.","Routing,
Business,
Web services,
Servers,
Service oriented architecture,
Software,
Computer architecture"
Simple coding for achieving mean square stability over bit-rate limited channels,"The problem of characterizing lower bounds on data-rates needed for closed loop stability has been solved in a variety of settings. However, the available results lead to coding schemes which are very complex and, thus, of limited practical interest. In this paper, we show how simple coding systems comprising only LTI filters and memoryless entropy coded dithered scalar quantizers can be used to stabilize strongly stabilizable SISO LTI plant models over error-free bit-rate limited feedback channels. Despite the simplicity of the building blocks employed, we prove that the data-rates incurred do not exceed absolute lower bounds by more than 1.25 bits per sample.",
Computing Phylogenetic Diversity for Split Systems,"In conservation biology, it is a central problem to measure, predict, and preserve biodiversity as species face extinction. In 1992, Faith proposed measuring the diversity of a collection of species in terms of their relationships on a phylogenetic tree and using this information to identify collections of species with high diversity. Here, we are interested in some variants of the resulting optimization problem that arise when considering species whose evolution is better represented by a network rather than a tree. More specifically, we consider the problem of computing phylogenetic diversity relative to a split system on a collection of species of size n. We show that, for general split systems, this problem is NP-hard. In addition, we provide some efficient algorithms for some special classes of split systems, in particular presenting an optimal O(n) time algorithm for phylogenetic trees and an O(n log n + nk) time algorithm for choosing an optimal subset of size k relative to a circular split system.","Phylogeny,
Biodiversity,
Biology computing,
Evolution (biology),
Genomics,
Bioinformatics,
Systems biology,
Large-scale systems,
DNA,
Greedy algorithms"
Design of an anthropomorphic prosthetic hand driven by Shape Memory Alloy actuators,"This paper presents the mechanical design of an ultra-lightweight, biomimetically actuated anthropomorphic hand for prosthetic purposes. The proposed design is based on an underactuated configuration of 16 joints and 7 active degrees of freedom. Shape Memory Alloy wires are used as motive elements of a specially designed actuation system installed within the envelope of the hand and forearm; due to their inherent contraction ability when heated, these innovative micro-actuators produce linear motion which is imparted to the fingers via a tendon transmission system. The overall design is completed with the integration of the necessary locking mechanisms for power saving. An experimental prototype of the suggested prosthesis has been fabricated using rapid prototyping techniques and it will be used as an evaluation platform for further research towards the development of a truly multifunctional, silent and cosmetically appealing hand for upper limb amputees.","Anthropomorphism,
Prosthetic hand,
Shape memory alloys,
Actuators,
Humans,
Prototypes,
Fingers,
Wires,
Materials science and technology,
Joints"
Training Linear Discriminant Analysis in Linear Time,"Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. It has been widely used in many fields of information processing, such as machine learning, data mining, information retrieval, and pattern recognition. However, the computation of LDA involves dense matrices eigen-decomposition which can be computationally expensive both in time and memory. Specifically, LDA has O(mnt + t3) time complexity and requires O(mn + mt + nt) memory, where m is the number of samples, n is the number of features and t = min(m, n). When both m and n are large, it is infeasible to apply LDA. In this paper, we propose a novel algorithm for discriminant analysis, called Spectral Regression Discriminant Analysis (SRDA). By using spectral graph analysis, SRDA casts discriminant analysis into a regression framework which facilitates both efficient computation and the use of regularization techniques. Our theoretical analysis shows that SRDA can be computed with O(ms) time and O(ms) memory, where s(¿ n) is the average number of non-zero features in each sample. Extensive experimental results on four real world data sets demonstrate the effectiveness and efficiency of our algorithm.","Linear discriminant analysis,
Spectral analysis,
Algorithm design and analysis,
Scattering,
Data mining,
Large-scale systems,
Information processing,
Information retrieval,
Pattern recognition,
Helium"
Epileptic Seizure Detection Using Empirical Mode Decomposition,"In this paper, we attempt to analyze the performance of the Empirical Mode Decomposition (EMD) for discriminating epileptic seizure data from the normal data. The Empirical Mode Decomposition (EMD) is a general signal processing method for analyzing nonlinear and nonstationary time series. The main idea of EMD is to decompose a time series into a finite and often small number of intrinsic mode functions (IMFs). EMD is an adaptive decomposition since the extracted information is obtained directly from the original signal. By utilizing this method to obtain the features of normal and epileptic seizure signals, we compare them with traditional features such as wavelet coefficients through two classifiers. Our results confirmed that our proposed features could potentially be used to distinguish normal from seizure data with success rate up to 95.42%.","Epilepsy,
Electroencephalography,
Signal analysis,
Data mining,
Wavelet transforms,
Biomedical signal processing,
Databases,
Process control,
Intelligent control,
Biomedical computing"
Searching for Rare Objects Using Index Replication,"Searching for objects is a fundamental problem for popular peer-to-peer file-sharing networks that contribute to much of the traffic on today's Internet. While existing protocols can effectively locate highly popular files, studies show that they fail to locate a significant portion of existing files in the network. High recall for these ""rare"" objects would drastically improve the user experience, and make these networks the ideal distribution infrastructure for user-generated content such as home videos and photo albums. In this paper, we examine simple techniques that can improve search recall for rare objects while minimizing the overhead incurred by participating peers. We propose several strategies for multi-hop index replication, and demonstrate their effectiveness and efficiency through both analysis and simulation. We further evaluate our simple techniques using detailed traces from a real Gnutella network, and show that they improve the performance of these overlays by orders of magnitude in both lookup success and overhead.","Bandwidth,
Peer to peer computing,
Telecommunication traffic,
Floods,
Communications Society,
Computer science,
IP networks,
Protocols,
User-generated content,
Videos"
Integrated single-inductor dual-input dual-output boost converter for energy harvesting applications,"An integrated single-inductor dual-input dual-output (SI DIDO) boost converter for energy harvesting applications was designed in a 0.35μm CMOS process. It provides two regulated output voltages for the load and the charge storage device, and two sources, the energy harvesting source and the charge storage device, are multiplexed to serve as the input. The implementation has several special features. (1) The input power MUX is driven by an internal charge pump for a larger gate drive to save area. (2) The power stage is implemented with an active diode core to eliminate gate drive circuitry. (3) A 1:7 timeslot scheduling with a fixed peak inductor current is adopted to deliver energy to the two outputs with a large difference in load currents. The proposed converter could operate at 1V with up to 85% efficiency at 200mW.","Voltage,
Energy storage,
Batteries,
Diodes,
Circuits,
Application software,
Energy management,
Inductors,
Switches,
Power electronics"
Communication using pheromone field for multiple robots,"In this paper, we consider a issue that the reliable and the inexpensive communication method in swarm robotics. The ants forage for preys by using pheromone trails. They lay down the pheromone trails between preys and a nest. By detecting the trail pheromone, they can find the preys. Though they do not have excellent intelligence, they can communicate with each other and cooperate by adding information to the environment, like a pheromone. This communication method has a merit that an agent does not need to memorize the place of the preys. We consider to answer the issue that ldquoHow do the swarm robots communicate using pheromone trail?rdquo. We construct a swarm behavior simulator and develop swarm robots that communicate using the pheromone trail. We demonstrate the effectiveness of the communication using the pheromone trail by computer simulations and experiments using swarm robots. To realize this purpose, we design a swarm behavior algorithm, based on 4 perceptual signs (stimuli) and 3 effector signs (actions). In the simulations, an experimental field is discretized by computational grids, and evaporation and diffusion are phenomena of the pheromone modeled by discretized equations. The proposed algorithm is demonstrated by the simulation. Simulation result shows that proposed algorithms act effectively. Based on the simulation results, we set three robots, one nest and one prey in the flat experimental field. We observe three robotspsila behavior and the state of the environment for 20 minutes. The robots laid down the pheromone trail between the nest and the prey, and reinforced the pheromone trail many times. This fact means that swarm robots can realize the function of the chemical, indirect, plastic and local communication like ants by using the pheromone trail.","Robots,
Robot sensing systems,
Robot kinematics,
Computational modeling,
Mobile robots,
Collision avoidance,
Ethanol"
Multiple Access Outerbounds and the Inseparability of Parallel Interference Channels,"It is known that the capacity of parallel (multi-carrier) Gaussian point-to-point, multiple access and broadcast channels can be achieved by separate encoding for each subchannel (carrier) subject to a power allocation across carriers. In this paper we show that such a separation does not apply to parallel Gaussian interference channels in general. A counter-example is provided in the form of a 3 user interference channel where separate encoding can only achieve a sum capacity of log(SNR) +o(log(SNR)) per carrier while the actual capacity, achieved only by joint-encoding across carriers, is 3/2(log(SNR))+o(log(SNR)) per carrier. As a byproduct of our analysis, we propose a class of multiple-access-outerbounds on the capacity of the 3 user interference channel.","Interference channels,
Broadcasting,
Frequency,
Channel capacity,
Signal to noise ratio,
Computer science,
Helium,
Power system modeling,
Communications Society"
Minimization of Quaternary Galois Field Sum of Products Expression for Multi-Output Quaternary Logic Function Using Quaternary Galois Field Decision Diagram,"A quaternary logic function expressed as quaternary Galois field sum of products (QGFSOP) expression can be realized as a cascade of quaternary 1-qudit, Feynman, and Toffoli gates. In this paper, we have presented a heuristic algorithm for simultaneous variable ordering and quaternary Galois field expansion selection for constructing optimal quaternary Galois field decision diagram (QGFDD). We have also shown the way of flattening the QGFDD for generating QGFSOP expression. We have written Java program to construct QGFDD for multi-output quaternary functions and provided experimental results.","Galois fields,
Minimization,
Logic functions,
Multivalued logic,
Heuristic algorithms,
Java,
Computer science,
Arithmetic,
Wire"
Writer Identification in Old Handwritten Music Scores,"The aim of writer identification is determining the writer of a piece of handwriting from a set of writers. In this paper we present a system for writer identification in old handwritten music scores. Even though an important amount of compositions contains handwritten text in the music scores, the aim of our work is to use only music notation to determine the author. The steps of the system proposed are the following. First of all, the music sheet is preprocessed and normalized for obtaining a single binarized music line, without the staff lines. Afterwards, 100 features are extracted for every music line, which are subsequently used in a k-NN classifier that compares every feature vector with prototypes stored in a database. By applying feature selection and extraction methods on the original feature set, the performance is increased. The proposed method has been tested on a database of old music scores from the 17th to 19th centuries, achieving a recognition rate of about 95%.","Feature extraction,
Text analysis,
Computer science,
Prototypes,
Spatial databases,
Image recognition,
Handwriting recognition,
Computer vision,
Mathematics,
Testing"
Live Baiting for Service-Level DoS Attackers,"Denial-of-service (DoS) attacks remain a challenging problem in the Internet. By making resources unavailable to intended legitimate clients, DoS attacks have resulted in significant loss of time and money for many organizations, thus, many DoS defense mechanisms have been proposed. In this paper we propose live baiting, a novel approach for detecting the identities of DoS attackers. Live baiting leverages group-testing theory, which aims at discovering defective members in a population using the minimum number of dasiadasiatestspsilapsila. This leverage allows live baiting to detect attackers using low state overhead without requiring models of legitimate requests nor anomalous behavior. The amount of state needed by live baiting is in the order of number of attackers not number of clients. This saving allows live baiting to scale to large services with millions of clients. We analyzed the coverage, effectiveness (detection time, false positive and false negative probabilities), and efficiency (memory, message overhead, and computational complexity) of our approach. We validated our analysis using NS-2 simulations modeled after real Web traces.","Computer crime,
Testing,
Network servers,
Communications Society,
Computer science,
Web and internet services,
Computational complexity,
Computational modeling,
Analytical models,
Computer security"
Resource-aware video multicasting via access gateways in wireless mesh networks,"This paper studies video multicasting in large scale areas using wireless mesh networks. The focus is on the use of Internet access gateways that allow a choice of alternative routes to avoid potentially lengthy multi-hop wireless paths with low capacity. A set of heuristic-based algorithms are described that together aim to maximize network capacity: the two-tier integrated architecture algorithm, the weighted gateway uploading algorithm, the link-controlled routing tree algorithm, and the alternative channel assignment algorithm. These algorithms use different approaches to arrange multicast group members into a clustered and two-tier integrated architecture in which network protocols can make use of multiple gateways to improve system throughput. Simulation results are used to determine the performance of the different approaches.","Wireless mesh networks,
Internet,
Routing,
Clustering algorithms,
Multicast algorithms,
Telecommunication traffic,
Videoconference,
Large-scale systems,
Multicast protocols,
Throughput"
Attribute-based Signature Scheme,"In real life, one requires signatures from people who satisfy certain criteria like that they should possess some specific attributes. For example, Alice wants a document to be signed by some employee in Bob’s company. This employee must have certain attributes such as being part of the IT staff and at least a junior manager in the cryptography team or a senior manager in the biometrics team. In order to satisfy these kinds of needs, we defined a common Attribute-based signature scheme where the signing member has to have certain attributes or belong to a certain group, and we also proved our scheme to be secure.","Information security,
Computer science,
Biometrics,
Public key cryptography,
Public key,
Polynomials"
A Cautionary Note on Checking Software Engineering Papers for Plagiarism,"Several tools are marketed to the educational community for plagiarism detection and prevention. This article briefly contrasts the performance of two leading tools, TurnItIn and MyDropBox, in detecting submissions that were obviously plagiarized from articles published in IEEE journals. Both tools performed poorly because they do not compare submitted writings to publications in the IEEE database. Moreover, these tools do not cover the Association for Computing Machinery (ACM) database or several others important for scholarly work in software engineering. Reports from these tools suggesting that a submission has ldquopassedrdquo can encourage false confidence in the integrity of a submitted writing. Additionally, students can submit drafts to determine the extent to which these tools detect plagiarism in their work. Because the tool samples the engineering professional literature narrowly, the student who chooses to plagiarize can use this tool to determine what plagiarism will be invisible to the faculty member. An appearance of successful plagiarism prevention may in fact reflect better training of students to avoid plagiarism detection.","software engineering,
computer science education,
copyright,
data integrity,
educational administrative data processing"
Localization in urban environments by matching ground level video images with an aerial image,"This paper presents the design of a monocular vision based particle filter localization system for urban settings that uses aerial orthoimagery as the reference map. One of the design objectives is to provide a low cost method for outdoor localization using a single camera. This relaxes the need for global positioning system (GPS) which may experience degraded reliability in urban settings. The second objective is to study the achievable localization performance with the aforementioned resources. Image processing techniques are employed to create a feature map from an aerial image, and also to extract features from camera images to provide observations that are used by a particle filter for localization.","Cameras,
Robot vision systems,
Particle filters,
Global Positioning System,
Image databases,
Degradation,
Image processing,
Navigation,
Buildings,
Robot sensing systems"
SPUMONE: Lightweight CPU Virtualization Layer for Embedded Systems,"Recently, the engineering cost of embedded systems is rapidly increasing due to growing sophistication of services.  To deal with the problem,  hybrid operating system environments have been proposed.  This enables to run a RTOS and a general purpose OS concurrently and to reuse software resources on both of them. This approach is efficient in reducing engineering costs. We reconfigured the requirement for these hybrid operating system environment and build a new architecture which fulfills these requirements by using virtualization techniques. Our system provides the facilities to build multiple operating system environment easily. There are two contributions in our systems. One is that the modification cost of the guest OS is small. The second contribution is improvement in system availability by enabling guest OS to reboot independently. Although we used virtualization layer to construct a hybrid operating system environment, the performance overhead is considering small. Therefor our approach is very practical and efficient for recent sophisticated embedded systems.",
Magnet: A novel scheduling policy for power reduction in cluster with virtual machines,"The concept of green computing has attracted much attention recently in cluster computing. However, previous local approaches focused on saving the energy cost of the components in a single workstation without a global vision on the whole cluster, so it achieved undesirable power reduction effect. Other cluster-wide energy saving techniques could only be applied to homogeneous workstations and specific applications. This paper describes the design and implementation of a novel approach that uses live migration of virtual machines to transfer load among the nodes on a multilayer ring-based overlay. This scheme can reduce the power consumption greatly by regarding all the cluster nodes as a whole. Plus, it can be applied to both the homogeneous and heterogeneous servers. Experimental measurements show that the new method can reduce the power consumption by 74.8% over base at most with certain adjustably acceptable overhead. The effectiveness and performance insights are also analytically verified.","Magnetic multilayers,
Workstations,
Servers,
Lead,
Magnetic separation,
Power demand,
Virtual machining"
"CAPTAN: A hardware architecture for integrated data acquisition, control, and analysis for detector development",The Electronic Systems Engineering Department of the Computing Division at the Fermi National Accelerator Laboratory has developed a data acquisition system flexible and powerful enough to meet the needs of a variety of high energy physics applications. The system described in this paper is called CAPTAN (Compact And Programmable daTa Acquisition Node) and its architecture and capabilities are presented in detail here.,"Hardware,
Data acquisition,
Detectors,
Computer architecture,
Data engineering,
Power engineering and energy,
Systems engineering and theory,
Physics computing,
Power engineering computing,
Electron accelerators"
Adaptive Feature Thresholding for off-line signature verification,"This paper introduces Adaptive Feature Thresholding (AFT) which is a novel method of person-dependent off-line signature verification. AFT enhances how a simple image feature of a signature is converted to a binary feature vector by significantly improving its representation in relation to the training signatures. The similarity between signatures is then easily computed from their corresponding binary feature vectors. AFT was tested on the CEDAR and GPDS benchmark datasets, with classification using either a manual or an automatic variant. On the CEDAR dataset we achieved a classification accuracy of 92% for manual and 90% for automatic, while on the GPDS dataset we achieved over 87% and 85% respectively. For both datasets AFT is less complex and requires fewer images features than the existing state of the art methods, while achieving competitive results.","Handwriting recognition,
Image converters,
Discrete wavelet transforms,
Digital images,
Forgery,
Machine learning,
Computer science,
Automatic testing,
Benchmark testing,
Government"
Texture and Wavelet-Based Spoof Fingerprint Detection for Fingerprint Biometric Systems,"This paper describes an image-based system to detect spoof fingerprint attacks in fingerprint biometric systems. It is based on the observation that, real and spoof fingerprints exhibit different textural characteristics. These are based on structural, orientation, roughness, smoothness and regularity differences of diverse regions in a fingerprint image. Local binary pattern (LBP) histograms are used to capture these textural details. Wavelet energy features characterizing ridge frequency and orientation information are also used for improving the efficiency of the proposed method. Dimensionality of the integrated feature set is reduced by running Pudil’s Sequential Forward Floating Selection (SFFS) algorithm. We propose to use a hybrid classifier, formed by fusing three classifiers: neural network, support vector machine and k-nearest neighbor using the “Product Rule”. Classification rates achieved with these classifiers, including a hybrid classifier are in the range ~94% to ~97%. Experimental results indicate that, the new liveness detection approach is a very promising technique, as it needs only one fingerprint and no extra hardware to detect vitality.","Fingerprint recognition,
Fingers,
Histograms,
Feature extraction,
Image matching,
Materials,
Pixel"
On-Demand Cluster Analysis for Product Line Functional Requirements,"We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.","Books,
Libraries,
Clustering algorithms,
Software,
Entropy,
Data mining,
Algorithm design and analysis"
The Sloan Digital Sky Survey Data Archive Server,The Sloan Digital Sky Survey's data archive server (DAS) provides public access to data files produced by the SDSS data reduction pipeline. This article discusses challenges in public distribution of data of this complexity and how the project addressed them.,
Distributed precoder optimization for interfering MISO channels,"In this paper, we examine the problem of two multiple-input-single-output (MISO) links that interfere with each other. We discuss some straightforward strategies for the users to optimize their rates and show how a certain degree of cooperation between the users can lead to an improvement of both individual links. We propose an algorithm that is based on the users exchanging 'interference prices' and is realizable in a distributed manner. The algorithm converges towards a locally sum-rate optimal solution and exhibits nearly identical performance as a non-distributed gradient ascent algorithm, outperforming the straightforward strategies over the complete range of noise powers.","Interference,
Transmitters,
Receiving antennas,
Transmitting antennas,
Time sharing computer systems,
Additive noise,
Array signal processing,
Signal processing,
Computer science,
Throughput"
Spline-Based Cardiac Motion Tracking Using Velocity-Encoded Magnetic Resonance Imaging,"This paper deals with the problem of tracking cardiac motion and deformation using velocity-encoded magnetic resonance imaging. We expand upon an earlier described method and fit a spatiotemporal motion model to measured velocity data. We investigate several different spatial elements both qualitatively and quantitatively using phantom measurements and data from human subjects. In addition, we also use optical flow estimation by the Horn-Schunk method as complementary data in regions where the velocity measurements are noisy. Our results show that it is possible to obtain good motion tracking accuracy in phantoms with relatively few spatial elements, if the type of element is properly chosen. The use of optical flow can correct some measurement artifacts but may give an underestimation of the magnitude of the deformation. In human subjects the different spatial elements perform quantitatively in a similar way but qualitative differences exists, as shown by a semiquantitative visual scoring of the different methods.","Spline,
Tracking,
Magnetic resonance imaging,
Velocity measurement,
Imaging phantoms,
Humans,
Image motion analysis,
Spatiotemporal phenomena,
Motion measurement,
Optical noise"
A model-based approach to security flaw detection of network protocol implementations,"A lot of efforts have been devoted to the analysis of network protocol specification for reliability and security properties using formal techniques. However, faults can also be introduced during system implementation; it is indispensable to detect protocol implementation flaws, yet due to the black-box nature of protocol implementation and the unavailability of protocol specification most of the approaches resort to random or manual testing. In this paper we propose a model-based approach for security flaw detection of protocol implementation with a high fault coverage, measurability, and automation. Our approach first synthesizes an abstract behavioral model from a protocol implementation and then uses it to guide the testing process for detecting security and reliability flaws. For protocol specification synthesis we reduce the problem a trace minimization with a Finite State Machine model and an efficient algorithm is presented for state space reduction. Our method is implemented and applied to real network protocols. Guided by the synthesized model our testing tool reveals a number of unknown reliability and security issues by automatically crashing the implementations of the Microsoft MSN instant messaging (MSNIM) protocol. Analytical comparison between our model-based and prevalent syntax-based flaw detection schemes is also provided with the support of experimental results.","Protocols,
Fault detection,
Network synthesis,
System testing,
Security,
Automation,
Minimization methods,
Automata,
State-space methods,
Vehicle crash testing"
Hepatic Perfusion Imaging Using Factor Analysis of Contrast Enhanced Ultrasound,"Contrast enhanced ultrasound imaging provides a real-time tool for evaluating vasculature in the liver. Primary liver cancer is known to be perfused exclusively by blood from the hepatic artery, whereas normal liver is also supplied by the portal vein. Visual separation of two different phases of enhancement from the independent feeding vessels is important for diagnosis but remains a challenge. This paper presents a method of using factor analysis for extracting distinct time-intensity curves. A key component to this extraction is the clustering of measured bolus curves and their projection onto a positivity domain to obtain nonnegative curves. This technique provides complementary images representing spatial loadings on each curve. As little as 1% of the data is required to contain unmixed signals to extract time-intensity curves that correlate well with true curves. A method of combining this information to display a regional hepatic perfusion image is proposed, and results are tested on a set of 10 patients. Region of interest analysis suggests it is possible to detect changes in the hepatic perfusion index of liver lesions relative to normal liver parenchyma using contrast ultrasound.","Ultrasonic imaging,
Image analysis,
Liver,
Data mining,
Cancer,
Blood,
Arteries,
Portals,
Veins,
Independent component analysis"
Improved sum-rate optimization in the multiuser MIMO downlink,"We consider linear precoding and decoding in the downlink of a multiuser multiple-input, multiple-output (MIMO) system. In this scenario, the transmitter and the receivers may each be equipped with multiple antennas, and each user may receive more than one data stream. We examine the relationship between the sum capacity for the broadcast channel with channel state information at the transmitter under a sum power constraint and the achievable sum rates under linear precoding. We show that achieving the optimum sum throughput under linear precoding is equivalent to minimizing the product of mean squared error (MSE) matrix determinants. The resulting nonconvex optimization problem is solved numerically, guaranteeing local convergence only. The performance of this approach is analyzed via comparison to the sum capacity and to existing approaches for linear precoding.",
Hierarchical optical path cross-connect node architecture using WSS/WBSS,"We propose a new hierarchical optical cross-connect node architecture that utilizes 3D MEMS-based WSS and WBSS (Waveband Selective Switch). We evaluate the total number of MEMS mirrors necessary to implement the node, a key parameter in determining node reliability and cost. It is demonstrated that the proposed hierarchical architecture requires a lot less mirrors than conventional single-layer WSS based node architectures.","Mirrors,
Optical crosstalk,
Optical switches,
Optical fiber networks,
Micromechanical devices,
Costs,
Telecommunication traffic,
Switching circuits,
Computer architecture,
Mechanical systems"
Detection and tracking of marine vehicles in video,"This work presents a novel technique for automatic detection and tracking of marine vehicles in video of open sea. The source of video is a video camera mounted on a buoy platform in open sea. Such system is intended to work autonomously, taking video of the surrounding ocean surface and analyzing them on presence of marine vehicles. The proposed technique is based on detection of marine vehicles in individual video frames and tracking the detected targets through the video sequence with the help of a tracking algorithm. Several performance metrics are utilized for performance evaluation of the proposed approach. Accuracy of detection in 90% range is shown on a dataset of 30 short video sequences taken by a prototype of the system.","Vehicle detection,
Marine vehicles,
Image edge detection,
Video sequences,
Object detection,
Cameras,
Sea surface,
Target tracking,
Monitoring,
Radar detection"
Using SIFT features in palmprint authentication,"As a new branch of biometrics, palmprint authentication has attracted increasing amount of attention because palmprints are abundant of line features so that low resolution images can be used. In this paper, we present two novel approaches for palmprints authentication. Firstly, we employ the SIFT (Scale Invariant Feature Transformation) for palmprint authentication. Point-wise matching is used to match SIFT key points extracted form palmprint images. Secondly, we extend a time series technology, SAX (Symbolic Aggregate approximation), to 2D data for the palmprint representation and matching. Using a public palmprint database, we demonstrate that the two proposed approaches, when combined together, can achieve the palmprint authentication accuracy comparable to that of the state of the art algorithms.",
Scaling alltoall collective on multi-core systems,"MPI Alltoall is one of the most communication intense collective operation used in many parallel applications. Recently, the supercomputing arena has witnessed phenomenal growth of commodity clusters built using InfiniBand and multi-core systems. In this context, it is important to optimize this operation for these emerging clusters to allow for good application scaling. However, optimizing MPI Alltoall on these emerging systems is not a trivial task. InfiniBand architecture allows for varying implementations of the network protocol stack. For example, the protocol can be totally on-loaded to a host processing core or it can be off-loaded onto the NIC or can use any combination of the two. Understanding the characteristics of these different implementations is critical in optimizing a communication intense operation such as MPI Alltoall. In this paper, we systematically study these different architectures and propose new schemes for MPI Alltoall tailored to these architectures. Specifically, we demonstrate that we cannot use one common scheme which performs optimally on each of these varying architectures. For example, on-loaded implementations can exploit multiple cores to achieve better network utilization, and in offload interfaces aggregation can be used to avoid congestion on multi-core systems. We employ shared memory aggregation techniques in these schemes and elucidate the impact of these schemes on multi-core systems. The proposed design achieves a reduction in MPI Alltoall time by 55% for 512Byte messages and speeds up the CPMD application by 33%.",
Meet In the Middle Cross-Layer Adaptation for Audiovisual Content Delivery,"This paper describes a new architecture and implementation of an adaptive streaming system (e.g., television over IP, video on demand) based on cross-layer interactions. At the center of the proposed architecture is the meet in the middle concept involving both bottom-up and top-down cross layer interactions. Each streaming session is entirely controlled at the RTP layer where we maintain a rich context that centralizes the collection of (i) instantaneous network conditions measured at the underlying layers (i.e.: link, network, and transport layers) and (ii) user- and terminal-triggered events that impose new real-time QoS adaptation strategies. Thus, each active multimedia session is tied to a broad range of parameters, which enable it to coordinate the QoS adaptation throughout the protocol layers and thus eliminating the overhead and preventing counter-productiveness among separate mechanisms implemented at different layers. The MPEG-21 framework is used to provide a common support for implementing and managing the end-to-end QoS of audio/video streams. Performance evaluations using peak signal to noise ratio (PSNR) and structural similarity index (SSIM) objective video quality metrics show the benefits of using the proposed Meet In the Middle cross-layer design compared to traditional media delivery approaches.","Streaming media,
Quality of service,
Educational institutions,
Forward error correction,
PSNR,
Video compression,
Computer science,
Wireless networks,
Interference,
Adaptive systems"
"Deep, Narrow Sigmoid Belief Networks Are Universal Approximators","In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.",
An efficient energy saving mechanism for IEEE 802.16e wireless MANs,"This paper presents an energy conservation scheme, maximum unavailability interval (MUI), to improve the energy efficiency for the Power Saving Class of Type II in IEEE 802.16e. By applying the Chinese remainder theorem, the proposed MUI is guaranteed to find the maximum Unavailability Interval, during which the transceiver can be powered down. The proposed MUI only dynamically adjusts one parameter defined in the standard. In addition, it is fully compatible with 802.16e standard. We also propose a new technique to reduce the computational complexity when solving the Chinese remainder theorem problem. Simulation and analysis have been conducted to evaluate the performance.","Sleep,
Computer science,
Computer buffers,
Energy conservation,
Energy efficiency,
Transceivers,
Computational complexity,
Computational modeling,
Analytical models,
Performance analysis"
Using NEAT for continuous adaptation and teamwork formation in Pacman,"Despite games often being used as a testbed for new computational intelligence techniques, the majority of artificial intelligence in commercial games is scripted. This means that the computer agents are non-adaptive and often inherently exploitable because of it. In this paper, we describe a learning system designed for team strategy development in a real time multi-agent domain. We test our system in the game of Pacman, evolving adaptive strategies for the ghosts in simulated real time against a competent Pacman player. Our agents (the ghosts) are controlled by neural networks, whose weights and structure are incrementally evolved via an implementation of the NEAT (Neuro-Evolution of Augmenting Topologies) algorithm. We demonstrate the design and successful implementation of this system by evolving a number of interesting and complex team strategies that outperform the ghosts' strategies of the original arcade version of the game.","Teamwork,
Games,
Artificial intelligence,
Computational intelligence,
Real time systems,
Humans,
Learning systems,
System testing,
Computational modeling,
Neural networks"
Locally Assembled Binary (LAB) feature with feature-centric cascade for fast and accurate face detection,"In this paper, we describe a novel type of feature for fast and accurate face detection. The feature is called Locally Assembled Binary (LAB) Haar feature. LAB feature is basically inspired by the success of Haar feature and Local Binary Pattern (LBP) for face detection, but it is far beyond a simple combination. In our method, Haar features are modified to keep only the ordinal relationship (named by binary Haar feature) rather than the difference between the accumulated intensities. Several neighboring binary Haar features are then assembled to capture their co-occurrence with similar idea to LBP. We show that the feature is more efficient than Haar feature and LBP both in discriminating power and computational cost. Furthermore, a novel efficient detection method called feature-centric cascade is proposed to build an efficient detector, which is developed from the feature-centric method. Experimental results on the CMU+MIT frontal face test set and CMU profile test set show that the proposed method can achieve very good results and amazing detection speed.","Assembly,
Face detection,
Computer vision,
Detectors,
Content addressable storage,
Computational efficiency,
Testing,
Intelligent robots,
Humans,
Skin"
Subcarrier allocation algorithms for multicellular OFDMA networks without Channel State Information,"Scope of this paper is to analyse four subcarrier allocation algorithms for multicellular OFDMA networks, namely coordinated, sequential, random and an innovative technique of cell splitting, that uses both random and coordinated subcarrier allocation. Common characteristic of these subcarrier allocation algorithms is that they do not require knowledge of Channel State Information (CSI) at the transmitter, a fact that leads to more efficient use of available bandwidth, lower algorithmic complexity and faster decision making. The algorithms are studied for an OFDMA multicellular network. Simulation results show that a multicellular OFDMA network is able to provide real broadband wireless access, with offered bit rate to reach 20Mbps per cell, even without CSI knowledge.",
"e5Learning, an E-Learning Environment Based on Eye Tracking","Users’ eyes can be a meaningful source of information for e-learning systems. What we look at, and the way we do that, can in fact be exploited to improve the learning process, disclosing information which would otherwise remain concealed. In this paper we describe an e-learning environment where eye tracking is used to observe user behavior, in order to adapt content presentation in real-time. To achieve such purpose, we consider both the way learning activities are carried out and those eye signals that can be related to the user’s “emotional states”.","Electronic learning,
Monitoring,
Eyes,
History,
Information resources,
Education,
Machine learning,
Costs,
Software agents,
Feedback"
Haptic Simulation of Elbow Joint Spasticity,"Spasticity is a human motor system disorder in which reflexive muscle activity becomes unregulated, causing unwanted contractions that can interfere with voluntary movement. We present a simulator that replicates spastic arm dynamics for clinical training of physical therapists and neurologists. Accurate clinical assessment of spasticity is critical in the determination of patient treatment, although physical evidence of spasticity is often confused with that of related neuromuscular disorders. By repeatably simulating different levels of spastic severity, we hope to improve clinician training for rating spasticity and consequently decrease the variability of ratings between raters and within raters. Our haptic device, designed to replicate the spastic elbow of a child, uses a brake actuator and high-resolution optical encoder. Two competing spasticity models from the literature are implemented. Preliminary experiments indicate that the decreased stretch reflex threshold model is more realistic than the increased stiffness model. The simulator improves on training with patients, since spastic severity can be readily adjusted under controlled and repeatable conditions.","Haptic interfaces,
Elbow,
Humans,
Muscles,
Medical treatment,
Neuromuscular,
Optical design,
Pediatrics,
Actuators,
Optical devices"
Tunable Band-notched Ultra Wideband (UWB) Planar Monopole Antennas Using Varactor,"A new tunable band-notched ultra wideband (UWB) planar monopole antenna is presented. The proposed antenna consists of a wideband planar monopole antenna, resonating stubs and varactor diodes. In order to generate tunable band-notched characteristic, we applied quarter-wavelength stubs with varactor diode to the wideband planar monopole antenna. We interpret the mechanism for notch characteristics using surface current distributions. This new approach is suitable for designing wideband antenna with narrow band interferer rejection characteristics.",
Modeling and generating complex motion blur for real-time tracking,"This article addresses the problem of real-time visual tracking in presence of complex motion blur. Previous authors have observed that efficient tracking can be obtained by matching blurred images instead of applying the computationally expensive task of deblurring [11]. The study was however limited to translational blur. In this work, we analyse the problem of tracking in presence of spatially variant motion blur generated by a planar template. We detail how to model the blur formation and parallelise the blur generation, enabling a real-time GPU implementation. Through the estimation of the camera exposure time, we discuss how tracking initialisation can be improved. Our algorithm is tested on challenging real data with complex motion blur where simple models fail. The benefit of blur estimation is shown for structure and motion.","Tracking,
Kernel,
Cameras,
Charge-coupled image sensors,
Motion estimation,
Computer vision,
Layout,
Lighting,
Cost function,
Digital images"
Model Checking of Analog Systems using an Analog Specification Language,In this contribution an advanced methodology for model checking of analog systems is introduced. A new analog specification language (ASL)for efficient property specifications is defined and model checking algorithms for implementing this language are presented. This allows verification of complex static and dynamic circuit properties like oscillation and startup time that have not yet been formally verifiable with previous approaches. The new verification methodology is applied to example circuits and experimental results are discussed and compared to conventional circuit simulation.,Specification languages
Evaluating the relationship between user interaction and financial visual analysis,"It has been widely accepted that interactive visualization techniques enable users to more effectively form hypotheses and identify areas for more detailed investigation. There have been numerous empirical user studies testing the effectiveness of specific visual analytical tools. However, there has been limited effort in connecting a user’s interaction with his reasoning for the purpose of extracting the relationship between the two. In this paper, we present an approach for capturing and analyzing user interactions in a financial visual analytical tool and describe an exploratory user study that examines these interaction strategies. To achieve this goal, we created two visual tools to analyze raw interaction data captured during the user session. The results of this study demonstrate one possible strategy for understanding the relationship between interaction and reasoning both operationally and strategically.","Human computer interaction,
Testing,
Visual analytics,
Wire,
Data visualization,
Joining processes,
Data mining,
Data analysis,
User interfaces,
Graphical user interfaces"
Measurement of narrowband channel characteristics in single-phase three-wire indoor power-line channels,"This manuscript reports the measurement results of narrowband signal propagation of PLC channels. In Japan, in-house wiring is single-phase three-wire, and each branch between an outlet of 100V and the panel board is connected to one of two live conductors and the neutral. Thus a pair of outlets can be classified into three types: connected to different live conductors, connected to the same live conductors by different branches from the panel board, and on the same branch to the same live conductor. It is confirmed that the results of the measurements can be classified by these three types of paths. The results show that frequency responses of narrowband PLC channels are relatively smooth, compared with that of wideband PLC. It is also found that propagation loss in lower frequency range is larger than in higher frequency range. The time independence of narrowband PLC channels are confirmed when no electric appliance is connected to the same live conductor. Power-line communication (PLC)","Narrowband,
Programmable control,
Conductors,
Coupling circuits,
Wiring,
Frequency,
Wideband,
Power system modeling,
Home appliances,
Signal generators"
Type-Checking Software Product Lines - A Formal Approach,"A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test all variants and ensure properties like type-safety for the entire SPL. While first steps to type-check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java (FJ) calculus with feature annotations to be used for SPLs. By extending FJ's type system, we guarantee that - given a well-typed SPL - all possible program variants are well- typed as well. We show how results from this formalization reflect and help implementing our own language-independent SPL tool CIDE.",
"Free-Riding, Fairness, and Firewalls in P2P File-Sharing","Peer-to-peer file-sharing networks depend on peers uploading data to each other. Some peers, called free-riders, will not upload data unless there is an incentive to do so. Algorithms designed to prevent free-riding typically assume that connectivity is not a problem. However, on the Internet, a large fraction of the peers resides behind a firewall or NAT, making them unable to accept incoming connections. In this paper, we will prove that it is impossible to prevent free-riding when more than half of the peers are firewalled, and we will provide bounds on the sharing ratios (defined as the number of bytes uploaded divided by the number of bytes downloaded) of both firewalled and non-firewalled peers. Firewall puncturing techniques are complex but can be used to connect two firewalled peers; we will provide a bound on their required effectiveness in order to achieve fairness.We confirm our theory by simulating individual BitTorrent swarms (sets of peers that download the same file), and show that the theoretical bounds can be met in systems with many firewalled peers. We have also collected statistics covering thousands of BitTorrent swarms in several communities, both open and closed; the latter ban peers if their sharing ratios drop below a certain treshhold. We found 45% of the peers to be firewalled in the closed communities, as opposed to 66% in the open communities, which correlates with our theory that to obtain fair sharing ratios for all peers, at most half of them can be behind firewalls.","Fires,
Communities,
Peer to peer computing,
Security,
Computers,
Streaming media,
Computational modeling"
Testing Java Components based on Algebraic Specifications,"This paper presents a method of component testing based on algebraic specifications. An algorithm for generating checkable test cases is proposed. A proto-type testing tool called CASCAT for testing Java En-terprise Beans is developed. It has the advantages of high degree of automation, which include test case generation, test harness construction and test result checking. It achieves scalability by allowing incre-mental integration. It also allows testing to focus on a subset of used functions and key properties, thus suit-able for component testing. The paper also reports an experimental evaluation of the method and the tool.","Java,
Automatic testing,
Software testing,
Prototypes,
Automation,
Computer science,
Scalability,
Built-in self-test,
Software systems,
User interfaces"
A Novel Network Intrusion Detection System (NIDS) Based on Signatures Search of Data Mining,"Network security has been a very important issue, since the rising evolution of the Internet. There has been an increasing need for security systems against the external attacks from the hackers. One important type is the intrusion detection system (IDS). There are two major categories of the analysis techniques of IDS: the anomaly detection and the misuse detection. Here we forcus on misuse detection, the misuse detection collected the attack signatures in a database as the same as virus protection software to detect the relate attacks, we propose an algorithm to use the known signature to find the signature of the related attack quickly.",
LogView: Visualizing Event Log Clusters,"Event logs or log files form an essential part of any network management and administration setup. While log files are invaluable to a network administrator, the vast amount of data they sometimes contain can be overwhelming and can sometimes hinder rather than facilitate the tasks of a network administrator. For this reason several event clustering algorithms for log files have been proposed, one of which is the event clustering algorithm proposed by Risto Vaarandi, on which his Simple Log file Clustering Tool (SLCT) is based. The aim of this work is to develop a visualization tool that can be used to view log files based on the clusters produced by SLCT. The proposed visualization tool, which is called LogView, utilizes treemaps to visualize the hierarchical structure of the clusters produced by SLCT. Our results based on different application log files show that LogView can ease the summarization of vast amount of data contained in the log files. This in turn can help to speed up the analysis of event data in order to detect any security issues on a given application.","Data visualization,
Clustering algorithms,
Visualization,
Security,
Algorithm design and analysis,
Protocols,
Layout"
Opacity-enforcing supervisory strategies for secure discrete event systems,"Initial-state opacity emerges as a key property in numerous security applications of discrete event systems including key-stream generators for cryptographic protocols. Specifically, a system is initial-state opaque if the membership of its true initial state to a set of secret states remains uncertain (opaque) to an outside intruder who observes system activity through a given projection map. In this paper, we consider the problem of constructing a minimally restrictive opacity-enforcing supervisor (MOES) which limits the system’s behavior within some pre-specified legal behavior while enforcing the initial-state opacity requirement. To tackle this problem, we extend the state-based definition of initial-state opacity to languages and characterize the solution to MOES in terms of the supremal element of certain controllable, observable and opaque languages. We also derive conditions under which this supremal element exists and show how the initial-state estimator, which was introduced in our earlier work for verifying initial-state opacity, can be used to implement the solution to MOES.","Discrete event systems,
Law,
Legal factors,
Automata,
Control systems,
Information security,
Power system modeling,
Cryptographic protocols,
Privacy,
Observability"
An Automatic Algorithm for Evaluating the Precision of Iris Segmentation,"Recent developments in the field of nonideal iris recognition have shown that the presence of the degradations such as insufficient contrast, unbalanced illumination, out-of-focus, motion blur, specular reflections, and partial area affect performance of iris recognition systems. Most iris recognition systems are designed to implement a number of processing steps with iris segmentation being one of the first steps. If segmentation is not performed at a certain precision, the error of segmentation will further propagate and will be amplified during the proceeding processing, encoding, and matching steps. This emphasizes a critical need in designing robust iris segmentation algorithms and together with it a need of automatic algorithms evaluating the precision (accuracy) of iris segmentation. Automatic algorithm evaluating the precision of segmentation plays important role for two reasons: (1) it can be placed into a feedback loop to enforce another run of segmentation algorithm that may include more sophisticated steps for high precision segmentation and (2) the outcome of this evaluation can be treated as a quality factor and thus can be used to design a quality driven adaptive iris recognition system. This work analyzes effects of degradations on iris segmentation and proposes and tests an automatic algorithm evaluating the precision of iris segmentation.","Iris recognition,
Algorithm design and analysis,
Degradation,
Lighting,
Reflection,
Encoding,
Robustness,
Feedback loop,
Q factor,
Adaptive systems"
Two-stage model predictive control for voltage collapse prevention,"The paper proposes a two-stage model predictive control (MPC) strategy for alleviating voltage collapse. The first stage uses a static load shedding algorithm to obtain stable MPC prediction simulations. The second stage uses the MPC simulations, and associated trajectory sensitivities, to establish a linear program (LP) that optimizes the control action. This LP builds on trajectory approximations that are generated from the sensitivities. The first stage prestabilizing process improves the accuracy of the trajectory approximations, with consequent improvement in the LP optimization results. The control action determined by the LP is subsequently applied to the actual system. The paper discusses the integration of this MPC algorithm into an energy management system (EMS) environment. A standard 10 bus voltage collapse case is used to illustrate the performance of the overall control strategy.","Predictive models,
Predictive control,
Power system dynamics,
Power system modeling,
Trajectory,
Voltage control,
Power system security,
Nonlinear dynamical systems,
Control systems,
Power system stability"
An Adjustable Target Coverage Method in Directional Sensor Networks,"The coverage problem for targets with directional sensor nodes has been studied in this paper. To improve the coverage rate for targets, we propose a Weighted Centralized Greedy Algorithm (WCGA). The weight function added in the WCGA could be adjusted according to different density of sensor nodes in directional sensor networks. A serious of experiments have been performed to evaluate the performance of the proposed method. Simulation results show the improvement of the target coverage rate and the effectiveness of the proposed method.","Wireless sensor networks,
Sleep,
Directional antennas,
Computer networks,
Computer science,
Electronic mail,
Greedy algorithms,
Performance evaluation,
Object detection,
Collaboration"
Biometric Inspired Digital Image Steganography,"Steganography is defined as the science of hiding or embedding “data” in a transmission medium. Its ultimate objectives, which are undetectability, robustness (i.e., against image processing and other attacks) and capacity of the hidden data (i.e., how much data we can hide in the carrier file), are the main factors that distinguish it from other “sisters-in science” techniques, namely watermarking and Cryptography. This paper provides an overview of well known Steganography methods. It identifies current research problems in this area and discusses how our current research approach could solve some of these problems. We propose using human skin tone detection in colour images to form an adaptive context for an edge operator which will provide an excellent secure location for data hiding.","Biometrics,
Digital images,
Steganography,
Image edge detection,
Robustness,
Image processing,
Watermarking,
Cryptography,
Humans,
Skin"
Dynamic background modeling and subtraction using spatio-temporal local binary patterns,"Traditional background modeling and subtraction methods have a strong assumption that the scenes are of static structures with limited perturbation. These methods will perform poorly in dynamic scenes. In this paper, we present a solution to this problem. We first extend the local binary patterns from spatial domain to spatio-temporal domain, and present a new online dynamic texture extraction operator, named spatio- temporal local binary patterns (STLBP). Then we present a novel and effective method for dynamic background modeling and subtraction using STLBP. In the proposed method, each pixel is modeled as a group of STLBP dynamic texture histograms which combine spatial texture and temporal motion information together. Compared with traditional methods, experimental results show that the proposed method adapts quickly to the changes of the dynamic background. It achieves accurate detection of moving objects and suppresses most of the false detections for dynamic changes of nature scenes.",
The Knowledge Grid Environment,"The knowledge grid environment is an autonomous human-machine environment evolving with science, technology, culture, and society. It consists of autonomous individuals, self-organized semantic communities, an adaptive-networking mechanism, and an evolving semantic networking mechanism. A memex extension (ME) offers a general model for autonomous individuals in this environment. MEs are configurable, adaptive, and context-aware digital organisms that model various types of network resources and host distributed network software and devices. MEs organize themselves to perform tasks according to social and economical principles. The evolving environment supports the generation of new MEs through inheritance. The ME model is also a knowledge model that can actively detect problems and fuse with and inherit from others' knowledge to obtain a reputation in providing knowledge services. The ME models advance information and knowledge in the knowledge grid environment.","Knowledge management,
Humans,
Resource management,
Text mining,
Grid computing,
Technological innovation,
Peer to peer computing,
Intelligent systems,
Intelligent networks,
Intelligent structures"
Compositing for small cameras,"To achieve a realistic integration of virtual and real imagery in video see-through augmented reality, the rendered images should have a similar appearance and quality to those captured by the video camera. This paper describes a compositing method which models the artefacts produced by a small low-cost camera, and adds these effects to an ideal pinhole image produced by conventional rendering methods. We attempt to model and simulate each step of the imaging process, including distortions, chromatic aberrations, blur, bayer masking, noise and colour-space compression, all while requiring only an RGBA image and an estimate of camera velocity as inputs.","Graphics,
Pixel,
Image generation,
Cameras,
Image color analysis,
Lenses,
Noise"
Clock buffer polarity assignment combined with clock tree generation for power/ground noise minimization,"A new approach to the problem of clock buffer polarity assignment for minimizing power/ground noise on the clock network is presented. The previous approaches solve the assignment problem in two separate steps: (step 1) generating a clock routing tree of minimum total wirelength, satisfying the clock skew constraint and then (step 2) inserting buffering elements with their polarities under the objective of minimizing power/ground noise while satisfying the clock skew constraint. Yet, there is no easy way to predict the result of step 2 during step 1. In our approach, we place the primary importance on the cost of power/ground noise. Consequently, we try to minimize the cost of power/ground noise first and then to construct a clock routing tree later while satisfying the clock skew constraint. Through experimentation using several benchmark circuits, it is shown that this approach is quite effective and produces very good solutions, reducing the power/ground noise by 75% and the peak current by 26% at the expense of 5% wirelength overhead compared to that produced by the conventional approach.","Clocks,
Power generation,
Noise generators,
Routing,
Circuit noise,
Noise reduction,
Switches,
Inverters,
Costs,
Power supplies"
Decision-level fusion strategies for correlated biometric classifiers,"The focus of this paper is on designing decision-level fusion strategies for correlated biometric classifiers. In this regard, two different strategies are investigated. In the first strategy, an optimal fusion rule based on the likelihood ratio test (LRT) and the Chair Varshney Rule (CVR) is discussed for correlated hypothesis testing where the thresholds of the individual biometric classifiers are first fixed. In the second strategy, a particle swarm optimization (PSO) based procedure is proposed to simultaneously optimize the thresholds and the fusion rule. Results are presented on (a) a synthetic score data conforming to a multivariate normal distribution with different covariance matrices, and (b) the NIST BSSR dataset. We observe that the PSO-based decision fusion strategy performs well on correlated classifiers when compared with the LRT-based method as well as the average sum rule employing z-score normalization. This work highlights the importance of incorporating the correlation structure between classifiers when designing a biometric fusion system.","Biometrics,
Testing,
Particle swarm optimization,
NIST,
Fusion power generation,
Engines,
Computer science,
Light rail systems,
Gaussian distribution,
Covariance matrix"
Testing Consequences of Grime Buildup in Object Oriented Design Patterns,"Evidence suggests that as software ages the original realizations of design patterns remain in place, and participants in design pattern realizations accumulate “grime” – non-pattern-related code.  This research examines the consequences that grime buildup has on the testability of general purpose design patterns.  Test cases put in place during the design phase and initial implementation of a project can become ineffective as the system matures.  The evolution of a design due to added functionality or defect fixing increases the coupling and dependencies between many classes that must be tested.  We show that as systems age, the growth of grime and the appearance of anti-patterns increase testing requirements.  Early recognition and removal of grime and anti-patterns can potentially improve system testability.","Software testing,
System testing,
Software systems,
Aging,
Computer science,
History,
Telecommunication switching,
Frequency,
Impedance,
Fault detection"
Motor initiated expectation through top-down connections as abstract context in a physical world,"Recently, it has been shown that top-down connections improve recognition in supervised learning. In the work presented here, we show how top-down connections represent temporal context as expectation and how such expectation assists perception in a continuously changing physical world, with which an agent interacts during its developmental learning. In experiments in object recognition and vehicle recognition using two types of networks (which derive either global or local features), it is shown how expectation greatly improves performance, to nearly 100% after the transition periods. We also analyze why expectation will improve performance in such real world contexts.","Neurons,
Training,
Firing,
Sensors,
Pixel,
Vehicles,
Smoothing methods"
Heuristic search planning to reduce exploration uncertainty,The path followed by a mobile robot while mapping an environment (i.e. an exploration trajectory) plays a large role in determining the efficiency of the mapping process and the accuracy of any resulting metric map of the environment. This paper examines some important aspects of path planning in this context: the trade-offs between the speed of the exploration process versus the accuracy of resulting maps; and alternating between exploration of new territory and planning through known maps. The resulting motion planning strategy and associated heuristic are targeted to a robot building a map of an environment assisted by a Sensor Network composed of uncalibrated monocular cameras. An adaptive heuristic exploration strategy based on A* search over a combined distance and uncertainty cost function allows for adaptation to the environment and improvement in mapping accuracy. We assess the technique using an illustrative experiment in a real environment and a set of simulations in a parametric family of idealized environments.,
Dual Poisson-Disk Tiling: An Efficient Method for Distributing Features on Arbitrary Surfaces,"This paper introduces a novel surface-modeling method to stochastically distribute features on arbitrary topological surfaces. The generated distribution of features follows the Poisson disk distribution, so we can have a minimum separation guarantee between features and avoid feature overlap. With the proposed method, we not only can interactively adjust and edit features with the help of the proposed Poisson disk map, but can also efficiently re-distribute features on object surfaces. The underlying mechanism is our dual tiling scheme, known as the dual Poisson-disk tiling. First, we compute the dual of a given surface parameterization, and tile the dual surface by our specially-designed dual tiles; during the pre-processing, the Poisson disk distribution has been pre-generated on these tiles. By dual tiling, we can nicely avoid the problem of corner heterogeneity when tiling arbitrary parameterized surfaces, and can also reduce the tile set complexity. Furthermore, the dual tiling scheme is non-periodic, and we can also maintain a manageable tile set. To demonstrate the applicability of this technique, we explore a number of surface-modeling applications: pattern and shape distribution, bump-mapping, illustrative rendering, mold simulation, the modeling of separable features in texture and BTF, and the distribution of geometric textures in shell space.","Surface texture,
Solid modeling,
Geometry,
Application software,
Rendering (computer graphics),
Topology,
Distributed computing,
Computational modeling"
Channel-hopping based single transceiver MAC for cognitive radio networks,"To improve the utilization of the precious radio spectrum, we propose a channel-hopping based cognitive radio medium access control (MAC) protocol for synchronized wireless networks that can enable the secondary (unlicensed) users to opportunistically utilize the unused licensed-spectrum without interfering with the primary (licensed) users. In our proposed scheme, the secondary users switch across the licensed channels with their distinct channel-hopping sequences. In particular, when a secondary user sender wants to send packets to its intended secondary user receiver, the secondary user sender changes its hopping schedule and follows the hopping sequence of the intended receiver to conduct the negotiation and then transmit data packets if the channel is not currently used by primary users. The main advantages of our proposed scheme include the followings: 1) no extra control channel is needed; 2) it overcomes the single control channel bottleneck; and 3) one transceiver is sufficient. We develop an Markov chain based analytical model to analyze the performance of our proposed scheme in terms of throughputs. We also identify the tradeoff between the channel utilization and the packet transmission delay.","Transceivers,
Cognitive radio,
Media Access Protocol,
Switches,
Wireless application protocol,
Access protocols,
Wireless networks,
Analytical models,
Performance analysis,
Throughput"
Multiple UAV coalition formation,"Unmanned aerial vehicles (UAVs) have the potential to carry munitions in support of battlefield operations, however they have limited sensor range and can carry only small quantities of resources. Often, to fully prosecute a target, a variety of assets may be required, and it may be necessary to deliver these assets simultaneously. Therefore, a team of UAVs that satisfies the target resource requirement needs to be assigned to a single target, and this team is called a coalition. Other desired requirements for the coalition are (i) minimize the target prosecution delay and (ii) minimize the size of the coalition. In this paper, we propose a two-stage optimal coalition formation algorithm that assigns appropriate numbers of UAVs satisfying the desired requirements. We developed a Dubins curves based simultaneous strike scheme. Simulation results are presented to show that the two-stage coalition formation algorithm has low computational overhead and can be applied in real-time.",
Robotic routers,"Mobile robots equipped with wireless networking capabilities can act as robotic routers and provide network connectivity to mobile users. Robotic routers provide cost efficient solutions for deployment of a wireless network in a large environment with limited number of users. In this paper, we present motion planning algorithms for robotic routers to maintain the connectivity of a single user to a base station. We consider two motion models for the user. In the first model, we assume that the target’s motion is known in advance. In the second model, user moves in an adversarial fashion and tries to break the connectivity.",
Real-time 3D pointing gesture recognition in mobile space,"In this paper, we present a real-time 3D pointing gesture recognition algorithm for natural human-robot interaction (HRI). The recognition errors in previous pointing gesture recognition algorithms are mainly caused by the low performance of the hands tracking module and by the unreliability of the direction estimate itself, therefore our proposed algorithm uses 3D particle filter for achieving reliability in hand tracking and cascade hidden Markov model (HMM) for a robust estimate for the pointing direction. When someone enters the field of view of the camera, his or her face and two hands are located and tracked using the particle filters. The first stage HMM takes the hand position estimate and maps it to a more accurate position by modeling the kinematic characteristics of finger pointing. The resulting 3D coordinates are used as an input to the second stage HMM that discriminates pointing gestures from others. Finally the pointing direction is estimated in the case of pointing state. The proposed method can deal with both large and small pointing gestures. The experiment shows better than 89% gesture recognition results and 99% target selection results.","Face detection,
Hidden Markov models,
Particle tracking,
Robustness,
Humans,
Mice,
Particle filters,
Fingers,
Target recognition,
Target tracking"
Detecting with PMUs the onset of voltage instability caused by a large disturbance,"This paper deals with long-term voltage instability triggered by the outage of transmission or generation equipments. It is shown how the onset of voltage instability could be detected in real-time from post-disturbance bus voltages provided by phasor measurement units (PMUs), assuming observability of the whole region prone to voltage instability. An extended set of equilibrium equations is fitted to the measurements and the change in sign of sensitivities is monitored. The latter are easily computed even for a large system. The important effects of overexcitation limiters and load tap changers are taken into account. The approach is illustrated on a 52-bus 20-machine system, where PMU outputs are simulated from detailed time simulation.","Mathematical model,
Equations,
Jacobian matrices,
Voltage control,
Generators,
Sensitivity,
Monitoring"
Evolution and Enhancement of BitTorrent Network Topologies,"This paper describes an experimental study that closely examines the underlying topologies of multiple complex networks formed in BitTorrent swarms. Our results demonstrate that the networks exhibit fundamental differences during different stages of a swarm, suggesting that the initial stage is not predictive of the overall performance. We also find a power-law degree distribution in the network of peers that are unchoked by others, which indicates the presence of a robust scale-free network. However, unlike previous studies, we find no clear evidence of persistent clustering in any of the networks, precluding the presence of a small-world that is potentially efficient for peer-to-peer downloading. These results suggest an interesting venue for improving BitTorrent's performance. We present a first attempt to introduce clustering into BitTorrent. Our approach is theoretically proven and makes minimal changes to the tracker only. Its effectiveness is verified through a series of simulations and experiments.","Network topology,
Peer to peer computing,
Complex networks,
Robustness,
Internet,
Resilience,
Computer networks,
Computer science,
Paper technology,
Traffic control"
An Efficient and Verifiable Concealed Data Aggregation Scheme in Wireless Sensor Networks,"Data aggregation is one of the most important techniques in wireless sensor networks to save energy through reducing lots of transmission. However, plaintext aggregation is insecure since eavesdropping or modifying messages is possible. Due to this, concealed data aggregation schemes based on homomorphic encryption have been proposed. Ciphertexts can be operated algebraic computations without decryption in those schemes. Unfortunately, they only provide data confidentiality. While compromising secret in captured sensor nodes, an adversary can still create forged ciphertexts. In this paper, we combines Boneh et al.'s aggregate signature scheme and Mykletun et al.'s concealed data aggregation scheme to overcome the above problems. The proposed scheme aggregates not only ciphertexts but also signatures. Through verifying aggregated signature, data integrity of each plaintext can be guaranteed. Furthermore, the communication overhead for each cluster head is still constant. Each cluster head sends an aggregated signature and an aggregated ciphertext to the base station. For resource constrained environment, the proposed scheme is secure and efficient practically.","Cryptography,
Aggregates,
Wireless sensor networks,
Public key,
Sensors,
Base stations,
Tin"
Implementing and Exploiting Inevitability in Software Transactional Memory,"Transactional Memory (TM) takes responsibility for concurrent, atomic execution of labeled regions of code, freeing the programmer from the need to manage locks. Typical implementations rely on speculation and rollback, but this creates problems for irreversible operations like interactive I/O.A widely assumed solution allows a transaction to operate in an inevitable mode that excludes all other transactions and is guaranteed to complete, but this approach does not scale. This paper explores a richer set of alternatives for software TM, and demonstrates that it is possible for an inevitable transaction to run in parallel with (non-conflicting) non-inevitable transactions, without introducing significant overhead in the non-inevitable case. We report experience with these alternatives in a graphical game application. We also consider the use of inevitability to accelerate certain common-case transactions.","Concurrent computing,
Instruments,
Writing,
Scalability,
Software,
Runtime,
Permission"
Coding-Aware Multi-path Routing in Multi-Hop Wireless Networks,"To overcome the inherent lossy property of wireless links and increase network throughput, many multi-path routing protocols have been proposed to improve the reliability and latency of packet delivery in wireless networks. Multi-path routing protocols, however, do not take advantage of existing coding opportunities to maximize network throughput. In this paper, we propose a novel coding-aware multi-path routing protocol (CAMP), which forwards packets over multiple paths dynamically based on path reliability and coding opportunity. CAMP employs a route discovery mechanism which returns to the source multiple paths along with ETX (Expected Transmission Count) of all links on each path. Using a novel forwarding mechanism, CAMP splits the traffic among multiple paths and actively creates instead of passively waiting for coding opportunity by switching its path to maximize the switching gain. Experimental results demonstrate that CAMP can achieve much higher throughput than comparable schemes for delivering packets in wireless networks.","Spread spectrum communication,
Wireless networks,
Routing protocols,
Throughput,
Network coding,
Telecommunication traffic,
Switches,
Packet switching,
Computer networks,
Laboratories"
A Printed Rampart-Line Antenna with a Dielectric Superstrate for UHF RFID Applications,A printed Rampart line antenna with a dielectric superstrate for passive Radio Frequency Identification (RFID) tags is presented. A design process is outlined to determine the number of elements used in the Rampart line antenna to achieve the required gain for the desired read range. An inductive loop is then added to the port to match the antenna with the passive tag circuitry. It is shown that a passive tag with a printed Rampart line antenna and a dielectric superstrate can achieve comparable read ranges to commercially available passive RFID tags.,"UHF antennas,
Radiofrequency identification,
Passive RFID tags,
Microstrip antennas,
Impedance,
Circuits,
Neodymium,
Cows,
Dielectric materials,
RFID tags"
"Using Hardware Memory Protection to Build a High-Performance, Strongly-Atomic Hybrid Transactional Memory","We demonstrate how fine-grained memory protection can be used in support of transactional memory systems: first showing how a software transactional memory system (STM) can be made strongly atomic by using memory protection on transactionally-held state, then showing how such a strongly-atomic STM can be used with a bounded hardware TM system to build a hybrid TM system in which zero-overhead hardware transactions may safely run concurrently with potentially-conflicting software transactions. We experimentally demonstrate how this hybrid TM organization avoids the common-case overheads associated with previous hybrid TM  proposals, achieving performance rivaling an unbounded HTM system without the hardware complexity of ensuring completion of arbitrary transactions in hardware. As part of our findings, we identify key policies regarding contention management within and across the hardware and software TM components that are key to achieving robust performance with a hybrid TM.","Hardware,
Protection,
Proposals,
Read-write memory,
Software safety,
Software performance,
Computer architecture,
Computer science,
Software systems,
Content management"
Fractional Order Linear Quadratic Regulator,"In this paper, we formulate the fractional linear quadratic regulator (LQR) problem. The analytical solution of fractional optimal control near the origin and infinity are derived. It is shown that the optimal control to the linear fractional system can be computed through the corresponding fractional Euler-Lagrange equations. Moreover, the analytical analysis of right-sided fractional equation is discussed, which relates to the construction and solution of fractional LQR problems. The matrix Mittag-Leffler function is used here to serve as the fundamental solution of high-dimension linear fractional equations. More detailed discussions about fractional systems and right-sided fractional operators are provided and summarized. Finally, two illustrated simulation results are provided as a proof of concept.","Regulators,
Optimal control,
Equations,
Fractional calculus,
Control systems,
State-space methods,
H infinity control,
Controllability,
Chemical processes,
Process control"
A Simpler Linear Time Algorithm for Embedding Graphs into an Arbitrary Surface and the Genus of Graphs of Bounded Tree-Width,"For every fixed surface
S
, orientable or non-orientable, and a given graph
G
, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of
G
in
S
or a minor of
G
which is not embeddable in
S
and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following:  The proof is considerably simpler: it needs only about 10 pages, and some results (with rather accessible proofs) from graph minors theory, while Mohar's original algorithm and its proof occupy more than 100 pages in total.   The hidden constant (depending on the genus
g
of the surface
S
) is much smaller. It is singly exponential in
g
, while it is doubly exponential in Mohar's algorithm.As a spinoff of our main result, we give another linear time algorithm, which is of independent interest. This algorithm computes the genus and constructs minimum genus embeddings of graphs of bounded tree-width. This resolves a conjecture by Neil Robertson and solves one of the most annoying long standing open question about complexity of algorithms on graphs of bounded tree-width.",
Wavelet energy signature and GLCM features-based fingerprint anti-spoofing,"This paper proposes a texture-based method to spoof-proof a fingerprint biometric system. The fundamental basis of this anti-spoofing method is that, real fingerprint exhibits different textural characteristics from a spoof one. Textural measures based on wavelet energy signatures and gray level co-occurrence matrix (GLCM) features are used to characterize fingerprint texture. Dimensionalities of the feature sets are reduced by running Pudil’s Sequential Forward Floating Selection (SFFS) algorithm. We test two feature sets independently on various classifiers like: AdaBoost.M1, support vector machine and OneR. Then, we fuse all the mentioned classifiers using the “Product Rule” to form a hybrid classifier. Classification rates achieved for wavelet energy signatures range from ∼94.35% to ∼96.71%. Likewise, classification rates for GLCM features range from ∼94.82% to ∼97.65%. Thus, the performance of a proposed method is very promising and it can be efficiently used to spoof-proof a real-time fingerprint biometric system.",
Vector Routing for Delay Tolerant Networks,"Recently, much research work has paid attention to delay tolerant networks (DTNs), which are networks with a frequent occurrences of network partitioning. Since the successful establishment of an end-to-end path between source and destination nodes is not guaranteed in these networks, routing is a challenging issue. In typical routing protocols for DTNs such as epidemic routing, they depend on data replication techniques over multiple paths for reliable data delivery. However, they invoke a large number of duplicated packets in the network. This paper therefore proposes an efficient routing (called vector routing) by utilizing the vector of node movements. In vector routing, the direction and velocity of nodes are calculated from the location information of nodes, and then nodes efficiently decide which nodes should take replicated packets as well as the number of packets to replicate. Using ns-2 simulation with two different mobility models, namely random waypoint and manhattan mobility models, we verify that vector routing performs better than epidemic routing in terms of less amount of traffic incurred without loss of packet delivery ratio.","Routing,
Routing protocols,
History,
Data models,
Delay,
Protocols,
Roads"
RACE: A Robust Adaptive Caching Strategy for Buffer Cache,"Although many block replacement algorithms for buffer caches have been proposed to address the well-known drawbacks of the LRU algorithm, they are not robust and cannot maintain a consistent performance improvement over all workloads. This paper proposes a novel and simple replacement scheme, called the Robust Adaptive buffer Cache management schemE (RACE), which differentiates the locality of I/O streams by actively detecting access patterns that are inherently exhibited in two correlated spaces, that is, the discrete block space of program contexts from which I/O requests are issued and the continuous block space within files to which I/O requests are addressed. This scheme combines the global I/O regularities of an application and the local I/O regularities of individual files that are accessed in that application to accurately estimate the locality strength, which is crucial in deciding which blocks are to be replaced upon a cache miss. Through comprehensive simulations on 10 real-application traces, RACE is shown to have higher hit ratios than LRU and all other state-of-the-art cache management schemes studied in this paper.",
SLA-Based Service Composition in Enterprise Computing,"The composition of services has been a useful approach to integrating business applications within and across organizational boundaries. In this approach, individual services are federated into composite services which are able to execute a given task subject to a service level agreement (SLA). An SLA is a contract agreed between a customer and a service provider who define a set of several quality of services (QoS). An SLA violation penalty is a way to ensure the credibility of an advertised SLA by a service provider. In this paper, we consider a set of computer resources used by a service broker who represents service providers to host enterprise applications for differentiated customer services subject to an SLA and its violation penalty. We present a novel framework for a QoS-constrained resource provisioning problem, and propose a capacity planning approach to optimizing computer resources for all service sites owned by service providers subject to multiple QoS metrics defined in the SLA and their violation penalties. Simulation results show that the proposed approach is efficient for reliable resource planning in service composition.",
Modelling co-operative MAC layer misbehaviour in IEEE 802.11 ad hoc networks with heterogeneous loads,"Misbehaviour due to back-off distribution manipulation has been one of the significant problems faced in IEEE 802.11 wireless ad hoc networks which has been explored recently by the research community. In addition, collusion between misbehaving nodes adds another dimension to this security problem. We examine this problem in a three-node network scenario wherein two nodes are assumed to be malicious colluding adversaries causing unfair channel access to the other legitimate node. The misbehaving nodes, through back-off manipulation, will try to minimize the channel access share got by the legitimate node and at the same time maximize the detection delay to detect such an attack. We explore this problem and its solution, analytically, in a non-saturated setting, by modelling a single IEEE 802.11 node as a Discrete Time Markov Chain (DTMC) and suggest a measure for evaluating fairness in the network. We then propose an attacker-detector non-linear optimization model through which the joint optimal attacker distribution is evaluated by applying results from the area of variational calculus. We finally use the Sequential Probability Ratio Test (SPRT) for estimating the average number of samples for detecting colluding adversaries in the network. We validate all the models using MATLAB and verify the model results by sampling values from the evaluated optimal attacker distribution using a robust statistical library called UNU.RAN.","Ad hoc networks,
Mathematical model,
Mobile ad hoc networks,
Delay effects,
Time measurement,
Calculus,
Probability,
Sequential analysis,
MATLAB,
Sampling methods"
Discretized Multinomial Distributions and Nash Equilibria in Anonymous Games,"We show that there is a polynomial-time approximation scheme for computing Nash equilibria in anonymous games with any fixed number of strategies (a very broad and important class of games), extending the two-strategy result of Daskalakis and Papadimitriou 2007. The approximation guarantee follows from a probabilistic result of more general interest: The distribution of the sum of n independent unit vectors with values ranging over {e1, e2, ...,ek}, where ei is the unit vector along dimension i of the k-dimensional Euclidean space, can be approximated by the distribution of the sum of another set of independent unit vectors whose probabilities of obtaining each value are multiples of 1/z for some integer z, and so that the variational distance of the two distributions is at most eps, where eps is bounded by an inverse polynomial in z and a function of k, but with no dependence on n. Our probabilistic result specifies the construction of a surprisingly sparse eps-cover --- under the total variation distance --- of the set of distributions of sums of independent unit vectors, which is of interest on its own right.","Game theory,
Polynomials,
Computer science,
Distributed computing,
Nash equilibrium,
Approximation algorithms,
Voting,
Internet,
Algorithm design and analysis"
Reducing Credit Re-authorization Cost in UMTS Online Charging System,"During an online charging general packet radio service (GPRS) session, a number of mid-session events, such as changes of quality of service (QoS), could dynamically affect the rating of the in-progress service. When such events occur, the GPRS support node needs to re-authorize the granted credit units with the online charging system (OCS). This paper proposes a threshold-based scheme that utilizes a threshold parameter delta to reduce the signaling traffic for the credit re-authorization procedure. By selecting an appropriate delta value, the signaling overhead in the OCS can be significantly reduced while the inaccuracy of the credit information insignificantly increases. The mobile operator can choose appropriate parameter values in the threshold-based scheme based on our study.","Costs,
3G mobile communication,
Quality of service,
Ground penetrating radar,
Streaming media,
Communication system control,
Packet radio networks,
Multimedia systems,
Laboratories,
Real time systems"
Comparison of MADM decision algorithms for interface selection in heterogeneous wireless networks,"Current mobile terminals are often equipped with several network interfaces, which may be of different access technologies, both wireless and cellular. It is possible to select dynamically the best interface according to different attributes such as the interface characteristics, user preferences and/or application preferences, … MADM is an algorithmic approach suitable to realize a dynamic interface selection with multiple alternatives (interfaces) and attributes (interface characteristics, user preferences …). In this paper, we compare the performance of three MADM algorithms e.g. SAW, WP, and TOPSIS. The simulation results show that each algorithm has its own limitations. TOPSIS suffered from “ranking abnormality” problem, SAW and WP provide less accuracy in identifying the alternative ranks compared to TOPSIS.","Wireless networks,
Surface acoustic waves,
Decision making,
Algorithm design and analysis,
Network interfaces,
Quality of service,
Base stations,
Switches,
Computer science,
Telecommunications"
"Requirements Engineering Education in the 21st Century, An Experiential Learning Approach","RE use in industry is hampered by a poor understanding of RE practices and their benefits. Teaching RE at the university level is therefore an important endeavor. This education can ideally be provided at the university level as an integrated part of developing the requisite RE and software engineering technical skills, shortly before students become engineers and enter the workforce. However, much social wisdom is packed into RE methods. It is unrealistic to expect students with little organizational experience to understand this body of knowledge. The course described in this paper uses an active, affective, experiential pedagogy giving students the opportunity of experiencing a simulated work environment that demonstrates the social/design-problem complexities and richness of a development organization in the throws of creating a new product. Emotional and technical debriefing is conducted after each meaningful experience so that students and faculty, alike, can better understand the professional relevancies of what they have just experienced. This includes an examination of the many forces experienced in industrial settings but not normally discussed in academic settings. The course uses a low-tech social simulation rather than software simulation so that students learn through interaction with real people and therefore are confronted with the complexity of true social relationships.",
Inter- and intra-relationships between communication coordination and cooperation in the scope of the 3C Collaboration Model,"Based on the 3C Collaboration Model, this article descries the mapping of a variety of collaboration forms onto inter-relationships between communication, coordination and cooperation. In order to investigate how to provide computational support for these three functional collaboration dimensions the analysis shifts from the inter-relationships between these three dimensions to their intra-relationships. Finally, Gestalt psychology principles are used to discuss the suitability of the approach to human perception.","Collaboration,
Collaborative work,
Production,
Collaborative software,
Computer science,
Psychology,
Visualization,
Taxonomy,
Space technology,
Decision making"
The HomeCare and Circadian rhythm,"The HomeCare is a special monitoring system of the basic life functions. It has been primarily designed to take care for elderly people. But many of the designed devices could be used in the other branches of biotelemetry. Our HomeCare system is designed with respect to user comfort and it uses as cheap technical solutions as possible. In cooperation with the Ostrava University a Homecare Testing Flat has been realized. There are tested our Homecare solutions in vivo. This article is focused on the designed HomeCare system, on measured parameters and on used wireless technologies. The Circadian rhythm measurement and its usage in the HomeCare system are discussed in the article too.",
Haptic Interface of the KAIST-Ewha Colonoscopy Simulator II,"This paper presents an improved haptic interface for the Korea Advanced Institute of Science and Technology Ewha Colonoscopy Simulator II. The haptic interface enables the distal portion of the colonoscope to be freely bent while guaranteeing sufficient workspace and reflective forces for colonoscopy simulation. Its force-torque sensor measures the profiles of the user. Manipulation of the colonoscope tip is monitored by four deflection sensors and triggers computations to render accurate graphic images corresponding to the rotation of the angle knob. Tack sensors are attached to the valve-actuation buttons of the colonoscope to simulate air injection or suction as well as the corresponding deformation of the colon. A survey study for face validation was conducted, and the result shows that the developed haptic interface provides realistic haptic feedback for colonoscopy simulations.","Haptic interfaces,
Colonoscopy,
Computational modeling,
Force measurement,
Monitoring,
Image sensors,
Rendering (computer graphics),
Graphics,
Deformable models,
Colon"
Building a Smart University Using RFID Technology,"Radio frequency identification (RFID) is getting popularity among identification technologies owing to its low cost, light weight, reduced size and inexpensive maintenance. Due to the recognition of RFID in the area of manufacturing, retail, pharmaceuticals and logistics, it is now in consideration for use in many different areas like ubiquitous computing, health care, agriculture, transport and security. Now a days security, power conservation and scalability are among the top issues that are in consideration for designing projects. In this paper, we contemplate the said issues and present how emerging technology of RFID can be used for building a smart university. Prototype is developed considering major use cases involved in a smart university. The system is taking care of maintaining attendance record, switching control of electrical items and security locks of rooms. Results show that consumption of energy and object tracking time is decreased while security of rooms and credibility of attendance record are increased.","Radiofrequency identification,
Security,
Pharmaceutical technology,
Costs,
Manufacturing,
Logistics,
Ubiquitous computing,
Medical services,
Agriculture,
Scalability"
A new generic model for signal propagation in Wi-Fi and WiMAX environments,"The ability to accurately predict radio propagation behavior for wireless communication systems, such as cellular mobile radio, is becoming crucial to system design. Since site measurements are costly, propagation models have been developed as suitable, low-cost, and convenient alternative. In this paper, we will propose a new generic signal propagation model for Wi-Fi and WiMAX environments. To develop this model we used existing models which are classified as: Free space models and land propagation models. This includes different types of loss: path loss, slow fading (shadowing) and fast fading. Our aim is to have a flexible model to be applicable in indoor and outdoor environments. Experiments carried out for indoor Wi-Fi and outdoor WiMAX cases have shown excellent results for the proposed model.","WiMAX,
Propagation losses,
Rayleigh channels,
Weibull fading channels,
Power system modeling,
Predictive models,
Radio propagation,
Shadow mapping,
Radio transmitters,
Antennas and propagation"
An OSD-based approach to managing directory operations in parallel file systems,"Distributed file systems that use multiple servers to store data in parallel are becoming commonplace. Much work has already gone into such systems to maximize data throughput. However, metadata management has historically been treated as an afterthought. In previous work we focused on improving metadata management techniques by placing file metadata along with data on Object-based Storage Devices (OSDs). However, we did not investigate directory operations. This work looks at the possibility of designing directory structures directly on OSDs, without the need for intervening servers. In particular, the need for atomicity is a fundamental requirement that we explore in depth. Through performance results of benchmarks and applications we show the feasibility of using OSDs directly for metadata, including directory operations.","File systems,
Servers,
Kernel,
Protocols,
Access protocols,
Performance evaluation,
Computer architecture"
Dynamic Reliable Service Routing in Enterprise Service Bus,"The Enterprise Service Bus (ESB) is the core department of the Service Oriented Architecture (SOA) which takes charge of managing mass services in the SOA. And the message routing among services is a very important mechanism for service communication in ESB, it is the main function provided by ESB. At the present, there have already been several patterns of message routing in ESB, but they only support static configurable routing, and they also can not ensure the reliability of message routing, so this paper presents a new pattern of message routing. By integrating the service discovery engine and designing dynamic routing component, we solve the limitation of existent static configurable routing and accomplish dynamically reliable message routing","Routing,
Service oriented architecture,
Engines,
Simple object access protocol,
Web services,
Runtime,
Computer science,
Conference management,
Technology management,
Software architecture"
Predictive design space exploration using genetically programmed response surfaces,Exponential increases in architectural design complexity threaten to make traditional processor design optimization techniques intractable. Genetically programmed response surfaces (GPRS) address this challenge by transforming the optimization process from a lengthy series of detailed simulations into the tractable formulation and rapid evaluation of a predictive model. We validate GPRS methodology on realistic processor design spaces and compare it to recently proposed techniques for predictive microarchitectural design space exploration.,"Space exploration,
Response surface methodology,
Predictive models,
Computational modeling,
Genetic programming,
Ground penetrating radar,
Process design,
Analytical models,
Surface fitting,
Design optimization"
Using Security Patterns to Combine Security Metrics,"Measuring security is an important step in creating and deploying secure applications. In order to efficiently measure the level of security that an application provides, three problems need to be solved: obviously metrics need to be available, a suitable metrics framework needs to be chosen and implemented, and the resulting measurements need to be interpreted. This work focuses on the second and third problem. We propose an approach to facilitate the selection and integration of appropriate security metrics, and to support the aggregation and interpretation of measurements. Our approach associates security metrics to security patterns, and we exploit the relationships between security patterns and security objectives to enable the interpretation of measurements. The approach is illustrated in a case study.","Computer security,
Authentication,
Application software,
Runtime,
Software design,
Availability,
Computer science,
Measurement units,
Software engineering,
Software packages"
A combination of vision- and vibration-based terrain classification,"For safe navigation in outdoor environments, a mobile robot should be able to estimate the type of the current and forthcoming terrain. Based on this estimation, the robot can decide if the terrain is safe and may be traversed securely, or if the terrain is potentially dangerous and must be avoided or traversed carefully. This paper presents a terrain classification approach which fuses terrain predictions based on image data with predictions made by a vibration-based method. Using color images, the robot classifies terrain in front of it. When the robot later traverses the classified area, it uses vibration data to verify its former prediction. Our experiments on 14 different terrain types show that by fusing both predictions, the classification rates are significantly larger than predictions based on data from a single sensor alone.","Robots,
Pixel,
Support vector machines,
Robot sensing systems,
Vibrations,
Training,
Mobile robots"
Quantitative Retrieval of Geophysical Parameters Using Satellite Data,"The remote sensing information service grid node (RSIN) is a tool for dealing with climate change and quantitative environmental monitoring. Based on the high-throughput computing grid, RSIN enables a workflow management system for data placement. The accompanying unified data-and-computation-schedule algorithm helps load balancing between and within workflow steps.","Information retrieval,
Satellites,
Remote monitoring,
Pipelines,
Condition monitoring,
Aerosols,
Grid computing,
Processor scheduling,
MODIS,
Image storage"
Alert prioritization in Intrusion Detection Systems,"Intrusion Detection Systems (IDSs) are designed to monitor user and/or network activity and generate alerts whenever abnormal activities are detected. The number of these alerts can be very large; making the task of security analysts difficult to manage. Furthermore, IDS alert management techniques, such as clustering and correlation, suffer from involving unrelated alerts in their processes and consequently provide imprecise results. In this paper, we propose a fuzzy-logic based technique for scoring and prioritizing alerts generated by an IDS(1). In addition, we present an alert rescoring technique that leads to a further reduction of the number of alerts. The approach is validated using the 2000 DARPA intrusion detection scenario specific datasets and comparative results between the Snort IDS alert scoring and our scoring and prioritization scheme are presented.","Intrusion detection,
Computer science,
Information security,
Telecommunication traffic,
Inspection,
Fuzzy logic,
Computerized monitoring,
Data security,
Protection,
Pattern matching"
Epidemic routing with immunity in Delay Tolerant Networks,"In this paper, we modify and extend epidemic routing used in intermittent networks such as Delay Tolerant Networks (DTNs). In particular, we propose to include immunity-based information disseminated in the reverse direction once messages get delivered to their destination. There are many variants of epidemic routing that are intended to result in better resource utilization by reducing the number of copies or through the use of sophisticated forwarding policy. Our focus is to use the information of already delivered messages in an immunity-list that will prevent any future exchange of those messages. Through the use of this technique we expect that the percent of delivered messages at lower delays will be higher because of better buffer and network utilization. Our simulation shows statistically significant performance improvement both in delivery ratio and delay for immunity-based epidemic as compared to the basic epidemic protocol.",
Reducing Maximum Stretch in Compact Routing,"It is important in communication networks to use routes that are as short as possible (i.e have low stretch) while keeping routing tables small. Recent advances in compact routing show that a stretch of 3 can be achieved while maintaining a sub- linear (in the size of the network) space at each node [14]. It is also known that no routing scheme can achieve stretch less than 3 with sub-linear space for arbitrary networks. In contrast, simulations on real-life networks have indicated that stretch less than 3 can indeed be obtained using sub-linear sized routing tables[6]. In this paper, we further investigate the space-stretch tradeoffs for compact routing by analyzing a specific class of graphs and by presenting an efficient algorithm that (approximately) finds the optimum space-stretch tradeoff for any given network. We first study a popular model of random graphs, known as Bernoulli random graphs or Erds-Renyi graphs, and prove that stretch less than 3 can be obtained in conjunction with sub- linear routing tables. In particular, stretch 2 can be obtained using routing tables that grow roughly as n3/4 where n is the number of nodes in the network. Compact routing schemes often involve the selection of landmarks. We present a simple greedy scheme for landmark selection that takes a desired stretch s and a budget L on the number of landmarks as input, and produces a set of at most 0(L logn) landmarks that achieve stretch s. Our scheme produces routing tables that use no more than O(logn) more space than the optimum scheme for achieving stretch s with L landmarks. This may be a valuable tool for obtaining near-optimum stretch-space tradeoffs for specific graphs. We simulate this greedy scheme (and other heuristics) on multiple classes of random graphs as well as on Internet like graphs.","Routing,
Peer to peer computing,
Communications Society,
Computer science,
Computer network management,
Engineering management,
Maintenance engineering,
Communication networks,
Algorithm design and analysis,
Internet"
A Hypercontractive Inequality for Matrix-Valued Functions with Applications to Quantum Computing and LDCs,"The Bonami-Beckner hypercontractive inequality is a powerful tool in Fourier analysis of real-valued functions on the Boolean cube. In this paper we present a version of this inequality for matrix-valued functions on the Boolean cube. Its proof is based on a powerful inequality by Ball, Carlen, and Lieb. We also present a number of applications. First, we analyze maps that encode n classical bits into m qubits, in such a way that each set of k bits can be recovered with some probability by an appropriate measurement on the quantum encoding; we show that if m","Quantum computing,
Linear matrix inequalities,
Computer applications,
Computer science,
Application software,
Quantum mechanics,
Complexity theory,
History,
Encoding,
Error correction codes"
An FPGA Design Space Exploration Tool for Matrix Inversion Architectures,"Matrix inversion is a common function found in many algorithms used in wireless communication systems. As FPGAs become an increasingly attractive platform for wireless communication, it is important to understand the tradeoffs in designing a matrix inversion core on an FPGA. This paper describes a matrix inversion core generator tool, GUSTO, that we developed to ease the design space exploration across different matrix inversion architectures. GUSTO is the first tool of its kind to provide automatic generation of a variety of general purpose matrix inversion architectures with different parameterization options. GUSTO also provides an optimized application specific architecture with an average of 59% area decrease and 3X throughput increase over its general purpose architecture. The optimized architectures generated by GUSTO provide comparable results to published matrix inversion architecture implementations, but offer the advantage of providing the designer the ability to study the tradeoffs between architectures with different design parameters.","Field programmable gate arrays,
Space exploration,
Wireless communication,
Design optimization,
Matrix decomposition,
Throughput,
OFDM,
MIMO,
Resource management,
Computer architecture"
A Novel Technique for Distal Locking of Intramedullary Nail Based on Two Non-constrained Fluoroscopic Images and Navigation,"Distal locking is one of the most difficult steps in intramedullary nailing. Numerous methods can help the surgeon, but all are time-consuming and involve much irradiation. We have developed and tested a new method based on only two fluoroscopic shots that do not need to be taken in the axes of the holes. This avoids requiring the presence of an experienced fluoroscopy operator to accurately adjust the imaging device in front of the locking holes, and decreases the exposure to radiation of the patient and medical team. A 3-D model of the distal nail and of its locking holes was constructed from a pair of calibrated fluoroscopic views. Prior to this, the contours of the nail and locking holes projections had to be determined. A 3-D optical localizer allowed the tracking of reference frames fixed to the nail, imaging device, and drilling motor. A navigation system based on the model guided the surgeon during distal targeting. The robustness, accuracy, and duration of the technique were evaluated in laboratory. The range of acceptable orientations of the X-ray beam has also been determined. Twenty drilling tests were carried out on sawbones. The accuracy and the duration required by our system to perform the distal targeting shows potential suitability for clinical use. The drill passed through the nail locking holes for all of them. The accuracy was about 1.5 mm in translation and 1deg in rotation. The total time spent on drilling did not exceed 15 min. The system was also assessed in vivo on three patients.","Nails,
Navigation,
Drilling,
Surges,
Testing,
Optical imaging,
Biomedical imaging,
Biomedical optical imaging,
Optical devices,
Target tracking"
A plug-and-play model for evaluating wavefront computations on parallel architectures,"This paper develops a plug-and-play reusable LogGP model that can be used to predict the runtime and scaling behavior of different MPI-based pipelined wavefront applications running on modern parallel platforms with multi-core nodes. A key new feature of the model is that it requires only a few simple input parameters to project performance for wavefront codes with different structure to the sweeps in each iteration as well as different behavior during each wavefront computation and/or between iterations. We apply the model to three key benchmark applications that are used in high performance computing procurement, illustrating that the model parameters yield insight into the key differences among the codes. We also develop new, simple and highly accurate models of MPI send, receive, and group communication primitives on the dual-core Cray XT system. We validate the reusable model applied to each benchmark on up to 8192 processors on the XT3/XT4. Results show excellent accuracy for all high performance application and platform configurations that we were able to measure. Finally we use the model to assess application and hardware configurations, develop new metrics for procurement and configuration, identify bottlenecks, and assess new application design modifications that, to our knowledge, have not previously been explored.","Concurrent computing,
Parallel architectures,
Procurement,
Application software,
Predictive models,
Multicore processing,
Hardware,
Production,
Weapons,
Computer architecture"
Regression Testing of Composite Service: An XBFG-Based Approach,"During the evolution and maintenance of service composition, regression testing is inevitable and significant. Process alteration, static binding alteration and dynamic binding alteration are three main evolution styles. In order to guarantee the functional availability during evolution of these three types, this paper proposes a service-composition oriented regression testing approach toward services composed using BPEL. Change detection, change impact analysis and test case generation are integrated in this solution. For the sake of selecting and generating test cases for regression testing, which is the key issue discussed here, this approach uses eXtensible BPEL Flow Graph (XBFG for short) with additive information in each kind of element to identify the changes of composite service and operate change impact analysis. Case study shows that this approach could achieve the given goal. In general, this solution covers main aspects of functional regression testing on composite service.",
Advanced full wave ESD generator model for system level coupling simulation,"System level ESD tests can only be performed after hardware is available. Simulating the ESD coupling into a circuit allows at least parametric and quantitative studies of the expected ESD behavior. A complete simulation requires us to model the ESD generator, the passive elements of the DUT and the response of the ICs to injected noise. Having the ultimate objective of combining IC soft error response models with the DUT structure and the ESD generator we report on progresses in modeling the ESD generator and its coupling. The model improves the useful frequency range from a few hundred MHz to about 3 GHz.","Electrostatic discharge,
Voltage measurement,
Generators,
Integrated circuit modeling,
Current measurement,
Numerical models,
Frequency measurement"
A Simple Algorithm for Computing the Lempel Ziv Factorization,"We give a space-efficient simple algorithm for computing the Lempel--Ziv factorization of a string. For a string of length n over an integer alphabet, it runs in O(n) time independently of alphabet size and uses o(n) additional space.","Software algorithms,
Computer science,
Space technology,
Data compression,
Educational institutions,
Ecosystems,
Entropy,
Dictionaries,
Automata,
Tree data structures"
Lessons from a real world evaluation of anti-phishing training,"Prior laboratory studies have shown that PhishGuru, an embedded training system, is an effective way to teach users to identify phishing scams. PhishGuru users are sent simulated phishing attacks and trained after they fall for the attacks. In this current study, we extend the PhishGuru methodology to train users about spear phishing and test it in a real world setting with employees of a Portuguese company. Our results demonstrate that the findings of PhishGuru laboratory studies do indeed hold up in a real world deployment. Specifically, the results from the field study showed that a large percentage of people who clicked on links in simulated emails proceeded to give some form of personal information to fake phishing websites, and that participants who received PhishGuru training were significantly less likely to fall for subsequent simulated phishing attacks one week later. This paper also presents some additional new findings. First, people trained with spear phishing training material did not make better decisions in identifying spear phishing emails compared to people trained with generic training material. Second, we observed that PhishGuru training could be effective in training other people in the organization who did not receive training messages directly from the system. Third, we also observed that employees in technical jobs were not different from employees with non-technical jobs in identifying phishing emails before and after the training. We conclude with some lessons that we learned in conducting the real world study.","Laboratories,
Protection,
Conducting materials,
Demography,
Testing,
Security,
Human factors"
Color thresholding method for image segmentation algorithm of Ziehl-Neelsen sputum slide images,"Most of the thresholding procedures involved setting of boundaries based on grey values or intensities of image pixels. In this paper, the thresholding is to be done based on color values in images of Ziehl-Neelsen sputum slides. The color thresholding technique is being carried out based on the adaptation and slight modification of the grey level thresholding algorithm. Multilevel thresholding has been conducted to the RGB color information of the bacterium to extract it from the sputum and other objects. Five types of different images have been used in the study of color information. The results showed that by using the selected threshold values, the image segmentation technique has been able to separate the sputum from the mycobacterium.","Image segmentation,
Microscopy,
Pixel,
Image processing,
Biomedical engineering,
Automatic control,
Biology computing,
Mechatronics,
Biomedical imaging,
Data mining"
A robust algorithm for handling moving traffic in urban scenarios,"This paper describes an algorithm for handling moving traffic which was deployed on AnnieWAY, an autonomous vehicle successfully entering the finals of the DARPA Urban Challenge 2007 competition. The algorithm allows for a robust and effective collision check for a variety of maneuvers including turning at intersections with oncoming traffic, merging into moving traffic, changing lanes, as well as dynamic passing and can easily be integrated into a high-level decision process, such as a state machine.",
Choice of metrics used in collaborative filtering and their impact on recommender systems,"The capacity of recommender systems to make correct predictions is essentially determined by the quality and suitability of the collaborative filtering that implements them. The common memory-based metrics are Pearson correlation and cosine, however, their use is not always the most appropriate or sufficiently justified. In this paper, we analyze these two metrics together with the less common mean squared difference (MSD) to discover their advantages and drawbacks in very important aspects such as the impact when introducing different values of k-neighborhoods, minimization of the MAE error, capacity to carry out a sufficient number of predictions, percentage of correct and incorrect predictions and behavior when attempting to recommend the n-best items. The paper lists the results and practical conclusions that have been obtained after carrying out a comparative study of the metrics based on 135 experiments on the MovieLens database of 100,000 ratios.","Correlation,
Accuracy,
Films,
Collaboration,
Conferences,
Filtering,
Biological system modeling"
Computer Aided Evaluation of Ankylosing Spondylitis Using High-Resolution CT,"Ankylosing spondylitis is a disease characterized by abnormal bone structures (syndesmophytes) growing at intervertebral disk spaces. Because this growth is so slow as to be undetectable on plain radiographs taken over years, it is desirable to resort to computerized techniques to complement qualitative human judgment with precise quantitative measures. We developed an algorithm with minimal user intervention that provides such measures using high-resolution computed tomography (CT) images. To the best of our knowledge it is the first time that determination of the disease's status is attempted by direct measurement of the syndesmophytes. The first part of our algorithm segments the whole vertebral body using a 3-D multiscale cascade of successive level sets. The second part extracts the continuous ridgeline of the vertebral body where syndesmophytes are located. For that we designed a novel level set implementation capable of evolving on the isosurface of an object represented by a triangular mesh using curvature features. The third part of the algorithm segments the syndesmophytes from the vertebral body using local cutting planes and quantitates them. We present experimental work done with 10 patients from each of which we processed five vertebrae. The results of our algorithm were validated by comparison with a semi-quantitative evaluation made by a medical expert who visually inspected the CT scans. Correlation between the two evaluations was found to be 0.936 (p < 10-18).","Arthritis,
Computed tomography,
Level set,
Bone diseases,
Radiography,
Humans,
Time measurement,
Image segmentation,
Isosurfaces,
Spine"
A Statistical Information Reconstruction Method of Images Based on Multiple-Point Geostatistics Integrating Soft Data with Hard Data,"The statistical information reconstruction of images will be difficult and inaccurate when no conditional data or only hard data are available. Accuracy of reconstructed images can be improved, using soft data during the process of reconstruction. Integrating soft data with hard data, a method based on multiple-point geostatistics is proposed to reconstruct statistical information of images. During the process of regenerating characteristic patterns in a training image, the accuracy of reconstructed images is improved, using both soft data and hard data as conditional data. The experimental results show that, compared with the unconditional reconstructed images and the reconstructed images using only hard data, the structure characteristics in reconstructed images using the proposed method are more similar to those obtained from real volume data.","Reconstruction algorithms,
Image reconstruction,
Interpolation,
Stochastic processes,
Data visualization,
Statistics,
Computer science,
Geology,
Biomedical imaging,
Smoothing methods"
An Image Encryption System by Cellular Automata with Memory,"In this paper, an Image Encryption System by special kind of cellular automata (cellular automata with memory) and also an appropriate transition function for the cryptosystem have been proposed. Also a lossy idea is used to present a secure cryptosystem. The use of lossy method provides a secure method and it is shown that the result is resistant to cryptanalysis attacks, especially known plaintext and chosen plaintext. When the original image is compared with the decrypted image by human visual system, it is not recognizable which one is decrypted and which one is the original image.","Public key cryptography,
Content addressable storage,
Humans,
Visual system,
Image recognition,
Protection,
Availability,
Computer security,
Computer science,
Appropriate technology"
Building accountability into the future Internet,"This paper proposes a future Internet architecture whose security foundations prevent today’s major threats — IP spoofing, distributed denial-of-service attacks, distributed scanning and intrusions, and wide-spread worm infections.",
Early Identifying Application Traffic with Application Characteristics,"To more accurately extract the characteristics of application flows, this paper proposes a set of flow attributes to characterize the possible negotiation behaviors of each flow in application layer perspective. The discriminators are available in the early stage, so they are suitable to support real-time based traffic classification and engineering. The ability of flow attributes was tested with several machine learning algorithms. On the other hand, we also compare the accuracy of our method with other related works that addressed real-time traffic classification problem based on the same sample traffic. The result shows that our method outperforms other previous works in protocol level identification with more than 8%~21% accuracy improvement based on fixed-ratio sample flow sets. Furthermore, the proposed method is also suitable to identify encrypted protocols.","Protocols,
Application software,
Cryptography,
Telecommunication traffic,
Machine learning algorithms,
Traffic control,
Protection,
Communications Society,
Computer science,
Testing"
Bayesian Task-Level Transfer Learning for Non-linear Regression,"Mullti-task learning utilizes labeled data from other “similar” tasks and can achieve efficient knowledge-sharing between tasks. Previous research mainly focused on multi-task learning for linear regression. In this paper, a novel Bayesian multi-task learning model for non-linear regression, i.e. HiRBF, is proposed. HiRBF is constructed under a hierarchical Bayesian framework. In the model all tasks are combined in a single RBF network. The input-to-hidden weights are shared between tasks, and the hidden-to-output weights are assumed to be sampled randomly from a certain prior distribution. The HiRBF algorithm is compared with two transfer-unaware approaches. The experiments demonstrate that HiRBF significantly outperforms the others.","Bayesian methods,
Neural networks,
Computer science,
Radial basis function networks,
Linear regression,
Training data,
Software engineering,
Backpropagation,
Additive noise,
Information filtering"
Multivariate Survival Analysis (I): Shared Frailty Approaches to Reliability and Dependence Modeling,"The latest advances in survival analysis have been centered on multivariate systems. Multivariate survival analysis has two major categories of models: one is multi-state modeling; the other is shared frailty modeling. Multi-state models, although formulated differently in both fields, have been extensively studied in reliability analysis in the context of Markov chain analysis. In contrast, shared frailty modeling seems little known in reliability analysis and computer science. In this article, we focus exclusively on shared frailty modeling. Shared frailty refers to the often-unobserved factors or risks responsible for the common risks dependence between multiple events. It is well recognized as the most effective modeling approach to address common risks dependence and, more recently, the event-related dependence. The only exclusion of dependence modeling for the frailty approach is the common events type, which is best addressed by multi-state modeling. We argue that shared frailty modeling not only is perfectly applicable for engineering reliability, but also is of significant potential in other fields of computer science, such as networking and software reliability and survivability, machine learning, and prognostics and health management (PHM).","Risk analysis,
Reliability engineering,
Computer science,
Prognostics and health management,
Reliability theory,
Application software,
Software libraries,
Statistical analysis,
Biomedical engineering,
Software reliability"
A tapered cascaded multi-stage distributed amplifier with 370GHz GBW in 90nm CMOS,"A tapered cascaded multi-stage distributed amplifier (T-CMSDA) has been designed and fabricated in a 90nm digital CMOS process. The amplifier achieves a 3-dB bandwidth of 73.5 GHz with a pass-band gain of 14dB. This results in a gain-bandwidth (GBW) product of 370 GHz. The realized zero-dB BW is 83.5 GHz and the input and output matchings stay better than −9dB up to 77 and 94 GHz, respectively. The chip consumes an area of 1.5mm by 1.15mm while drawing 70mA from a 1.2V supply.","Distributed amplifiers,
CMOS technology,
Impedance,
Circuits,
Cutoff frequency,
Capacitance,
CMOS process,
Wideband,
Transmission line theory,
Inductance"
Compact ultra-wideband bandpass filter using broadside coupled hairpin structures on multilayer liquid crystal polymer substrate,"A novel ultra-wideband (UWB) bandpass filter (BPF) using a broadside-coupled hairpin structure and multilayer organic liquid crystal polymer (LCP) technology is presented. To suppress stopband harmonic response, folded stepped impedance structures were adopted as hairpin resonators in the design. The proposed filter has been investigated numerically and experimentally. Multilayer LCP technology was used to implement designed UWB BPF. Good agreement between simulated and measured results of the proposed filter was observed. They show that the fabricated UWB BPF has a good performance, including a small insertion loss, a flat group delay with a variation within 0.1 ns in most of its passband, a wide stopband from 11.0 20.0 GHz with a high rejection level up to 20.0 dB, and a very compact size of 9.8 times 7.5 mm (0.36 lambdag times 0.27 lambdag, where lambdag is the guided wavelength of 50 Omega microstrip line at 6.85 GHz).","ultra wideband technology,
band-pass filters,
liquid crystal polymers,
microwave filters,
multilayers,
resonator filters"
Globally optimal shape-based tracking in real-time,Most algorithms for real-time tracking of deformable shapes provide sub-optimal solutions for a suitable energy minimization task: The search space is typically considered too large to allow for globally optimal solutions.,"Shape,
Computer vision,
Image segmentation,
Runtime,
Level set,
Optimization methods,
Computer Society,
Pattern recognition,
Computer science,
Minimization methods"
Genetic Algorithm for Shortest Driving Time in Intelligent Transportation Systems,"The route guidance system, which provides driving advice based on traffic information about an origin and a destination, has become very popular along with the advancement of handheld devices and the global position system. Since the accuracy and efficiency of route guidance depend on the accuracy of the traffic conditions, the route guidance system needs to include more variables in calculation, such as real time traffic flows and allowable vehicle speeds. As variables considered by the route guidance system increase, the cost to compute multiplies. As handheld devices have limited resources, it is not feasible to use them to compute the exact optimal solutions by some well-known algorithm, such as the Dijkstra’s algorithm, which is usually used to find the shortest path with a map of reasonable numbers of vertices. To solve this problem, we propose to use the genetic algorithm to alleviate the rising computational cost.  We use the genetic algorithm to find the shortest time in driving with diverse scenarios of real traffic conditions and varying vehicle speeds. The effectiveness of the genetic algorithm is clearly demonstrated when applied on a real map of modern city with very large vertex numbers.","Intelligent transportation systems,
Genetic algorithms,
Handheld computers,
Navigation,
Shortest path problem,
Computational efficiency,
Genetic engineering,
Vehicles,
Multimedia systems,
Computer science"
Year,,
Analysis of the Subsume Relation between Software Architecture Testing Criteria,"Formalized testing is to improve quality of software product based on software architecture. It is an important research in the fields of software engineering. Testing criteria are conditions which software testing must satisfy. Therefore, the highly effective testing strategy is very important. This paper introduces π Behavior Graph that describes software architecture. We propose seven testing coverage criteria from black-box and white-box perspectives and give its formal description in πBG respectively. Then, we discuss the subsume relation between testing criteria and build a CASE tool. We use TRMCS as a typical architecture model to illustrate our testing technology and analyze how the test paths are changing with the increasing numbers of clients from “the fat customer” perspective.","Software architecture,
Software testing,
Computer science,
Computer architecture,
System testing,
Software engineering,
Educational institutions,
Computer aided software engineering,
Calculus,
Connectors"
Confidence Estimation in Non-RF to RF Correlation-Based Specification Test Compaction,"Several existing methodologies have leveraged the correlation between the non-RF and the RF performances of a circuit in order to predict the latter from the former and, thus, reduce test cost. While this form of specification test compaction eliminates the need for expensive RF measurements, it also comes at the cost of reduced test accuracy, since the retained non-RF measurements and pertinent correlation models do not always suffice for adequately predicting the omitted RF measurements. To alleviate this problem, we develop a methodology that estimates the confidence in the obtained test outcome. Subsequently, devices for which this confidence is insufficient are retested through the complete specification test suite. As we demonstrate on production test data from a zero-IF down-converter fabricated at IBM, the proposed method outperforms previous defect filtering and guard banding methods and enables a more efficient exploration of the tradeoff between test accuracy and number of retested devices.","Radio frequency,
Compaction,
Circuit testing,
Performance evaluation,
Costs,
Production,
Predictive models,
Integrated circuit measurements,
Industrial training,
Computer science"
Educational aspects of mechatronic control course design for collaborative remote laboratory,"Paper addresses educational aspects of a distance learning mechatronic control course, which is one of the 18 courses that compound an international collaborative remote laboratory designed for students of electrical engineering. The work is undertaken in the frame of European Leonardo da Vinci project EDIPE “E-learning Distance Interactive Practical Education” with a goal to offer Web based experimental courses to the students from 11 countries that participate in the project. Course presented in this paper is motion control of mechatronic device with nonlinear dynamics. Addressed topics are modelling of dynamics of mechatronic device, design of linear and nonlinear motion controllers, implementation and optimization of motion controllers for real mechatronic system. Three remote experiments are available as a part of the course: motion control of mechatronic device with cascade controller, motion control with PD controller and computed torque motion controller. For the course two educational strategies are developed; one for geographically distant students and one for local students. For geographically distant students the course is executed completely remote. For local students some parts of remote course are combined with traditional learning in the classroom and laboratory sessions in order to improve course’s flexibility and to achieve best educational outcome.","Mathematical model,
Laboratories,
Mechatronics,
Computational modeling,
Motion control,
Documentation,
Torque"
Recognizing partial facial action units based on 3D dynamic range data for facial expression recognition,"Research on automatic facial expression recognition has benefited from work in psychology, specifically the Facial Action Coding System (FACS). To date, most existing approaches are primarily based on 2D images or videos. With the emergence of real-time 3D dynamic imaging technologies, however, 3D dynamic facial data is now available, thus opening up an alternative to detect facial action units in dynamic 3D space. In this paper, we investigate how to use this new modality to improve action unit (AU) detection. We select a subset of AUs from both the upper and lower parts of a facial area, apply the active appearance model (AAM) method and take the correspondence between textures and range models to track the pre-defined facial features across the 3D model sequences. A Hidden Markov Model (HMM) based classifier is employed to recognize the partial AUs. The experiments show that our 3D dynamic tracking based approach outperforms the compared 2D feature tracking based approach. The results are also comparable with the manually-picked 3D facial features based method. Finally, we extend our approach to validate the experiment for recognizing six prototypic facial expressions.",
Force Estimation with a Non-Uniform Pressure Sensor Array,"Embedding pressure sensors into household fixtures enables unobtrusive occupant health and safety monitoring at home. To monitor bathroom grab bar use, the force applied to a grab bar is desired from the output of three embedded pressure sensors. We examine the measurement of applied force in a non-uniform pressure sensor array, where forces are distributed with spatial nonlinearity to the pressure sensors. Two methods that ignore the spatial nonlinearities are compared to two methods that incorporate them. These include a polynomial response curve, a theoretical model, a lookup table, and an artificial neural network. When many calibration points can be taken and location estimates are accurate, the location-based lookup table presented the lowest error However, when calibration time is limited, the theoretical model performs best, while an artificial neural network is preferred when location inputs are inaccurate.","Sensor arrays,
Force sensors,
Monitoring,
Force measurement,
Table lookup,
Artificial neural networks,
Calibration,
Fixtures,
Health and safety,
Pressure measurement"
Picking up the Pieces: Self-Healing in reconfigurable networks,"We consider the problem of self-healing in networks that are reconfigurable in the sense that they can change their topology during an attack. Our goal is to maintain connectivity in these networks, even in the presence of repeated adversarial node deletion, by carefully adding edges after each attack. We present a new algorithm, DASH, that provably ensures that: 1) the network stays connected even if an adversary deletes up to all nodes in the network; and 2) no node ever increases its degree by more than 2 log n, where n is the number of nodes initially in the network. DASH is fully distributed; adds new edges only among neighbors of deleted nodes; and has average latency and bandwidth costs that are at most logarithmic in n. DASH has these properties irrespective of the topology of the initial network, and is thus orthogonal and complementary to traditional topology-based approaches to defending against attack. We also prove lower-bounds showing that DASH is asymptotically optimal in terms of minimizing maximum degree increase over multiple attacks. Finally, we present empirical results on power-law graphs that show that DASH performs well in practice, and that it significantly outperforms naive algorithms in reducing maximum degree increase.","Peer to peer computing,
Network topology,
Bridges,
Skin,
Organisms,
Robustness,
Biological system modeling,
Delay,
Bandwidth,
Costs"
Distributed simulation in industry - a survey Part 3 - the HLA standard in industry,"Distributed simulation, more specifically the HLA standard, is hardly applied in industry. We have conducted an extensive survey with COTS (commercial off-the-shelf) simulation package vendors and simulation experts, both from defence and industry, that focuses, amongst others, on the question what the reasons are behind this phenomenon. In this paper we analyze the reactions that we obtained, categorizing them into arguments related to distributed simulation in general, arguments related to HLA and arguments pertaining to the embedding of HLA concepts in COTS packages. These answers will lead us, we believe, to insights that can serve as guidelines to make distributed simulation more attractive for the industrial simulation community.",
Scalable group-based checkpoint/restart for large-scale message-passing systems,"The ever increasing number of processors used in parallel computers is making fault tolerance support in large-scale parallel systems more and more important. We discuss the inadequacies of existing system-level checkpointing solutions for message-passing applications as the system scales up. We analyze the coordination cost and blocking behavior of two current MPI implementations with checkpointing support. A group-based solution combining coordinated checkpointing and message logging is then proposed. Experiment results demonstrate its better performance and scalability than LAM/MPI and MPICH-VCL. To assist group formation, a method to analyze the communication behaviors of the application is proposed.","Large-scale systems,
Checkpointing,
Costs,
Fault tolerance,
Computer science,
Concurrent computing,
Fault tolerant systems,
Application software,
Scalability,
Failure analysis"
Loss Differentiated Rate Adaptation in Wireless Networks,"Data rate adaptation aims to select the optimal data rates for current channel conditions leading to substantial performance improvement. This paper proposes a data rate adaptation technique that: (1) exploits the periodic IEEE 802.11 beacons; (2) discriminates between frame losses due to channel fading and those due to collisions and takes actions appropriate for each type of loss; and (3) recommends and justifies the use of the lowest data rate for the very first retransmission after a frame loss. The last feature - namely retransmitting at the lowest data rate - helps in diagnosing the real cause of a frame loss. Moreover, this work analytically shows that retransmitting at the lowest data rate is more efficient, especially in poor SNR environments or when there is no knowledge of the cause of a loss (channel degradation or transmission collision). This scheme, dubbed loss differentiated rate adaptation (LDRA), is extensively evaluated through simulations and shown to perform better especially when network traffic is heavy.","Wireless networks,
Degradation,
Wireless sensor networks,
Fluctuations,
Signal to noise ratio,
Communications Society,
Computer science,
Software engineering,
USA Councils,
Performance loss"
Cross-Monotonic Multicast,"In the routing and cost sharing of multicast towards a group of potential receivers, cross-monotonicity is a property that states a user's payment can only be smaller when serviced in a larger set. Being cross-monotonic has been shown to be the key in achieving group-strategyproofness. We study multicast schemes that target optimal flow routing, cross-monotonic cost sharing, and budget balance. We show that no multicast scheme can satisfy these three properties simultaneously, and resort to approximate budget balance instead. We derive both positive and negative results that complement each other for directed and undirected networks. We show that in directed networks, no cross-monotonic scheme can recover a constant fraction of optimal multicast cost. We provide a simple scheme that does achieve 1/k-budget-balance, where k is the number of receivers. Using a probabilistic method rooted in random graph theory, we prove an upper-bound of 2/radic(k) for the budget balance ratio. For undirected networks, we derive a constant upper-bound of 1/2 instead. We further apply a smooth dual growing technique to design a cross- monotonic scheme that recovers k+1/2kzeta of optimal multicast cost in undirected networks, where zeta is a network-dependent parameter close to 1. This is almost tight against the upper-bound |. We finally present a two-stage linear optimization model that pursues maximum budget balance in any given specific network, with trade-off in complexity. Optimization results in various network configurations confirm the theoretically established bounds.",
Estimation of non-Gaussian random variables in Gaussian noise: Properties of the MMSE,"This work studies the properties of the minimum mean-square error (MMSE) of estimating an arbitrary random variable contaminated by Gaussian noise based on the observation. The MMSE can be regarded as a function of the signal-to-noise ratio (SNR), as well as a functional or transform of the input distribution. This paper shows that the MMSE is analytic in SNR for every random variable. Simple expressions for the derivatives of the MMSE as a function of the SNR are obtained. Since the input-output mutual information can be written as the integral of the MMSE as a function of SNR, the results also lead to higher derivatives of the mutual information. The MMSE and mutual information’s convexity in the SNR and concavity in the input distribution are established. It is shown that there can be only one SNR for which the MMSE of a Gaussian random variable and that of a non-Gaussian random variable coincide. Application of the properties of the MMSE to the scalar Gaussian broadcast channel problem is presented.","Signal to noise ratio,
Random variables,
Mutual information,
Gaussian noise,
Transforms,
Entropy,
Estimation"
A noise-tolerant approach to fuzzy-rough feature selection,"In rough set based feature selection, the goal is to omit attributes (features) from decision systems such that objects in different decision classes can still be discerned. A popular way to evaluate attribute subsets with respect to this criterion is based on the notion of dependency degree. In the standard approach, attributes are expected to be qualitative; in the presence of quantitative attributes, the methodology can be generalized using fuzzy rough sets, to handle gradual (in)discernibility between attribute values more naturally. However, both the extended approach, as well as its crisp counterpart, exhibit a strong sensitivity to noise: a change in a single object may significantly influence the outcome of the reduction procedure. Therefore, in this paper, we consider a more flexible methodology based on the recently introduced Vaguely Quantified Rough Set (VQRS) model. The method can handle both crisp (discrete-valued) and fuzzy (real-valued) data, and encapsulates the existing noise-tolerant data reduction approach using Variable Precision Rough Sets (VPRS), as well as the traditional rough set model, as special cases.",
A Novel Steganographic Algorithm Based on the Motion Vector Phase,"Most data hiding techniques in digital video utilize I frame to embed the secret information so the capacity of P and B frame is wasted. In this paper, we first analyze a data-hiding algorithm using the phase angle difference of the motion vector, on this base a novel steganographic algorithm based on the phase of the motion vector is proposed. The algorithm utilizes the phase of single motion vector to embed the secret data in P or B frame, in order to improve the embedding efficiency we use the matrix encoding technique to achieve a better tradeoff between the quantities of hidden data and the motion vectors modification rate. The simulation results have validated the feasibility of the algorithm.",
Performance Analysis of Three-Class Classifiers: Properties of a 3-D ROC Surface and the Normalized Volume Under the Surface for the Ideal Observer,"Classification of a given observation to one of three classes is an important task in many decision processes or pattern recognition applications. A general analysis of the performance of three-class classifiers results in a complex 6-D receiver operating characteristic (ROC) space, for which no simple analytical tool exists at present. We investigate the performance of an ideal observer under a specific set of assumptions that reduces the 6-D ROC space to 3-D by constraining the utilities of some of the decisions in the classification task. These assumptions lead to a 3-D ROC space in which the true-positive fraction (TPF) can be expressed in terms of the two types of false-positive fractions (FPFs). We demonstrate that the TPF is uniquely determined by, and therefore is a function of, the two FPFs. The domain of this function is shown to be related to the decision boundaries in the likelihood ratio plane. Based on these properties of the 3-D ROC space, we can define a summary measure, referred to as the normalized volume under the surface (NVUS), that is analogous to the area under the ROC curve (AUC) for a two-class classifier. We further investigate the properties of the 3-D ROC surface and the NVUS for the ideal observer under the condition that the three class distributions are multivariate normal with equal covariance matrices. The probability density functions (pdfs) of the decision variables are shown to follow a bivariate log-normal distribution. By considering these pdfs, we express the TPF in terms of the FPFs, and integrate the TPF over its domain numerically to obtain the NVUS. In addition, we performed a Monte Carlo simulation study, in which the 3-D ROC surface was generated by empirical ""optimal"" classification of case samples in the multidimensional feature space following the assumed distributions, to obtain an independent estimate of NVUS. The NVUS value obtained by using the analytical pdfs was found to be in good agreement with that obtained from the Monte Carlo simulation study. We also found that, under all conditions studied, the NVUS increased when the difficulty of the classification task was reduced by changing the parameters of the class distributions, thereby exhibiting the properties of a performance metric in analogous to AUC. Our results indicate that, under the conditions that lead to our 3-D ROC analysis, the performance of a three-class classifier may be analyzed by considering the ROC surface, and its accuracy characterized by the NVUS.",
Event-Driven Architecture for Decision Support in Traffic Management Systems,"Decision support systems for traffic management systems have to cope with a high volume of events continuously generated by sensors. Conventional software architectures do not explicitly target the efficient processing of continuous event streams. Recently, event-driven architectures (EDA) have been proposed as a new paradigm for event-based applications. In this paper we propose a reference architecture for event-driven traffic management systems, which enables the analysis and processing of complex event streams in real-time and is therefore well-suited for decision support in sensor-based traffic control systems. We will illustrate our approach in the domain of road traffic management. In particular, we will report on the redesign of an intelligent transportation management system (ITMS) prototype for the high-capacity road network in Bilbao, Spain.","Computer architecture,
Roads,
Intelligent transportation systems,
Decision support systems,
Sensor systems,
Software architecture,
Electronic design automation and methodology,
Application software,
Real time systems,
Traffic control"
New model of semantic similarity measuring in wordnet,"This paper presents a new model of measuring semantic similarity in the taxonomy of WordNet. The model takes the path length between two concepts and IC value of each concept as its metric, furthermore, the weight of two metrics can be adapted artificially. In order to evaluate our model, traditional and widely used datasets are used. Firstly, coefficients of correlation between human ratings of similarity and six computational models are calculated, the result shows our new model outperforms their homologues. Then, the distribution graphs of similarity value of 65 word pairs are discussed our model having no faulted zone more centralized than other five methods. So our model can make up the insufficient of other methods which only using one metric(path length or IC value) in their model.","Integrated circuit modeling,
Knowledge engineering,
Intelligent systems,
Computer science,
Educational institutions,
Physics,
Taxonomy,
Computational modeling,
Humans,
Natural language processing"
Mining Bridge and Brick Motifs From Complex Biological Networks for Functionally and Statistically Significant Discovery,"A major task for postgenomic systems biology researchers is to systematically catalogue molecules and their interactions within living cells. Advancements in complex-network theory are being made toward uncovering organizing principles that govern cell formation and evolution, but we lack understanding of how molecules and their interactions determine how complex systems function. Molecular bridge motifs include isolated motifs that neither interact nor overlap with others, whereas brick motifs act as network foundations that play a central role in defining global topological organization. To emphasize their structural organizing and evolutionary characteristics, we define bridge motifs as consisting of weak links only and brick motifs as consisting of strong links only, then propose a method for performing two tasks simultaneously, which are as follows: 1) detecting global statistical features and local connection structures in biological networks and 2) locating functionally and statistically significant network motifs. To further understand the role of biological networks in system contexts, we examine functional and topological differences between bridge and brick motifs for predicting biological network behaviors and functions. After observing brick motif similarities between E. coli and S. cerevisiae, we note that bridge motifs differentiate C. elegans from Drosophila and sea urchin in three types of networks. Similarities (differences) in bridge and brick motifs imply similar (different) key circuit elements in the three organisms. We suggest that motif-content analyses can provide researchers with global and local data for real biological networks and assist in the search for either isolated or functionally and topologically overlapping motifs when investigating and comparing biological system functions and behaviors.","Bridge circuits,
Organizing,
Biological systems,
Sun,
Evolution (biology),
Computer science,
Complex networks,
Systems biology,
Cells (biology),
Computer vision"
Registered 3-D Ultrasound and Digital Stereotactic Mammography for Breast Biopsy Guidance,"Large core needle biopsy is a common procedure used to obtain histological samples when cancer is suspected in diagnostic breast images. The procedure is typically performed under image guidance, with freehand ultrasound and stereotactic mammography (SM) being the most common modalities used. To utilize the advantages of both modalities, a biopsy device combining three-dimensional ultrasound (3DUS) and digital SM imaging with computer-aided needle guidance was developed. An implementation of a stereo camera method was applied to SM calibration, providing a target localization error of 0.35 mm. The 3D transformation between the two imaging modalities was then derived, with a target registration error of 0.52 mm. Finally, the needle guidance error of the device was evaluated using tissue-mimicking phantoms, showing a sample mean and standard deviation of and 0.49 plusmn 0.27 mm for targets planned from 3DUS and SM images, respectively. These results suggest that a biopsy procedure guided using this device would successfully sample breast lesions at a size greater than or equal to the smallest typically detected in mammographic screening (~2mm).","Ultrasonic imaging,
Mammography,
Breast biopsy,
Samarium,
Needles,
Computer errors,
Cancer,
Cameras,
Calibration,
Imaging phantoms"
"Hierarchical recognition of activities of daily living using multi-scale, multi-perspective vision and RFID","Research on computer-based recognition of ordinary household activities of daily living (ADLs) has been spurred by the need for technology to support care of the elderly in the home environment. We address the issue of recognizing ADLs at multiple levels of detail by combining multi-view computer vision and radio-frequency identification (RFID)-based direct sensors. Multiple places in our smart home testbed are covered by distributed synchronized cameras with different imaging resolutions. Learning object appearance models without costly manual labeling is achieved by applying the RFID sensing. A hierarchical recognition scheme is proposed by building a dynamic Bayesian network (DBN) that encompasses both coarse-level and fine-level ADL recognition. Advantages of the proposed approach include robust segmentation of objects, view-independent tracking and representation of objects and persons in 3D space, efficient handling of occlusion, and the recognition of human activity at both a coarse and fine level of detail.",
Sensor Node Localization Using Uncontrolled Events,"Many event-driven localization methods have been proposed as low cost, energy efficient solutions for wireless senor networks. In order to eliminate the requirement of accurately controlled events in existing approaches, we present a practical design using totally uncontrolled events for stationary sensor node positioning. The novel idea of this design is to estimate both the event generation parameters and the location of each sensor node by processing node sequences easily obtained from uncontrolled event distribution. To demonstrate the generality of our design, both straight-line scan and circular wave propagation events are addressed in this paper, and we evaluated our approach through theoretical analysis, extensive simulation and a physical test bed implementation with 41 MICAz motes. The evaluation results illustrate that with only randomly generated events, our solution can effectively localize sensor nodes with excellent flexibility while adding no extra cost at the resource constrained sensor node side. In addition, localization using uncontrolled events provides a nice potential option of achieving node positioning through natural ambient events.","Estimation,
Joints,
Parameter estimation,
Image segmentation,
Distance measurement,
Measurement by laser beam,
Laser beams"
Energy efficient packet classification hardware accelerator,"Packet classification is an important function in a router’s line-card. Although many excellent solutions have been proposed in the past, implementing high speed packet classification reaching up to OC-192 and even OC-768 with reduced cost and low power consumption remains a challenge. In this paper, the HiCut and HyperCut algorithms are modified making them more energy efficient and better suited for hardware acceleration. The hardware accelerator has been tested on large rulesets containing up to 25,000 rules, classifying up to 77 Million packets per second (Mpps) on a Virtex5SX95T FPGA and 226 Mpps using 65nm ASIC technology. Simulation results show that our hardware accelerator consumes up to 7,773 times less energy compared with the unmodified algorithms running on a StrongARM SA-1100 processor when classifying packets. Simulation results also indicate ASIC implementation of our hardware accelerator can reach OC-768 throughput with less power consumption than TCAM solutions.","Energy efficiency,
Hardware,
Acceleration,
Energy consumption,
Costs,
Throughput,
Clocks,
Application specific integrated circuits,
Telecommunication traffic,
Classification algorithms"
Accelerating Reed-Solomon coding in RAID systems with GPUs,"Graphical Processing Units (GPUs) have been applied to more types of computations than just graphics processing for several years. Until recently, however, GPU hardware has not been capable of efficiently performing general data processing tasks. With the advent of more general-purpose extensions to GPUs, many more types of computations are now possible. One such computation that we have identified as being suitable for the GPU’s unique architecture is Reed-Solomon coding in a manner appropriate for RAID-type systems. In this paper, we motivate the need for RAID with triple-disk parity and describe a pipelined architecture for using a GPU for this purpose. Performance results show that the GPU can outperform a modern CPU on this problem by an order of magnitude and also confirm that a GPU can be used to support a system with at least three parity disks with no performance penalty.","Acceleration,
Reed-Solomon codes,
Computer architecture,
Laboratories,
Hardware,
Data processing,
Central Processing Unit,
Fault tolerance,
Mathematics,
Computer graphics"
Delay-insensitive asynchronous ALU for cryogenic temperature environments,"This paper details the design and performance of a delay-insensitive asynchronous 8-bit ALU for an asynchronous 8051-compliant microcontroller intended for extreme environments. The ALU was designed using a quasi-delay-insensitive logic called NULL Convention Logic (NCL), in order to allow for reliable circuit operation over a wide temperature range and enable extreme supply voltage scaling for low power consumption. The ALU was fabricated along with several other 8051-essential components at MOSIS using the IBM SiGe5AM 0.5 μm process. A series of tests at both room and cryogenic temperatures has been performed, which has demonstrated that the designed ALU is able to operate correctly from 2K (−271 °C) to 297K (23 °C), as well as over wide supply voltage variations.","Temperature measurement,
Voltage measurement,
Cryogenics,
Temperature sensors,
Temperature distribution,
Logic gates,
Power measurement"
Video coding with spatio-temporal texture synthesis and edge-based inpainting,"This paper proposes a video coding scheme, in which textural and structural regions are selectively removed in the encoder, and restored in the decoder by spatio-temporal texture synthesis and edge-based inpainting. In the proposed scheme, two types of regions are classified based on two motion models: local motion and global motion. In local motion regions, conventional block-based motion estimation is employed for region removal and spatio-temporal texture synthesis is applied for recovery of the removed regions. In global motion regions, edge-based image inpainting is utilized to recover removed regions, and sprite generation is used as an auxiliary tool to keep temporal consistency. In the proposed scheme, both structures and textures are handled and some kinds of assistant information which can guide restoration are extracted and coded. This approach is blockbased and thus is flexible and generic to be implemented into standard-compliant video coding schemes. It has been implemented into H.264/AVC and achieves up to 35% bitrate saving at similar visual quality levels compared with H.264/AVC without our approach.","Sprites (computer),
Pixel,
Decoding,
Image edge detection,
Image restoration,
Video coding,
Image reconstruction"
Seamless Handover Scheme for Proxy Mobile IPv6,"In a network-based approach such as Proxy Mobile IPv6 (PMIPv6), the serving network controls mobility management on behalf of the Mobile Node (MN). Thus, the MN is not required to participate in any mobility-related signaling. PMIPv6 is being standardized in the IETF NetLMM WG. However, the PMIPv6 still suffers from a lengthy handover latency and packet loss during a handover. In this paper, we propose a seamless handover scheme for PMIPv6. The proposed handover scheme uses the Neighbor Discovery message of IPv6 to reduce the handover latency and packet buffering at the Mobile Access Gateway (MAG) to avoid the on-the-fly packet loss during a handover. In addition, it uses an additional packet buffering at the Local Mobility Anchor (LMA) to solve the packet ordering problem. Simulation results demonstrate that the proposed scheme could avoid the on-the-fly packet loss and ensure the packet sequence.","Manganese,
Mobile communication,
Mobile radio mobility management,
Neodymium,
Throughput,
IP networks,
Protocols"
A Weighted Utility Framework for Mining Association Rules,"Association rule mining (ARM) identifies frequent itemsets from databases and generates association rules by assuming that all items have the same significance and frequency of occurrence in a record i.e. their weight and utility is the same (weight=1 and utility=1) which is not always the case. However, items are actually different in many aspects in a number of real applications such as retail marketing, nutritional pattern mining etc. These differences between items may have a strong impact on decision making in many application unlike the use of standard ARM. Our framework, Weighted Utility ARM (WUARM), considers the varied significance and different frequency values of individual items as their weights and utilities. Thus, weighted utility mining focuses on identifying the itemsets with weighted utilities higher than the user specified weighted utility threshold. We conduct experiments on synthetic and real data sets using standard ARM, weighted ARM and Weighted Utility ARM (WUARM) and present analysis of the results.","Itemsets,
Association rules,
Data mining,
Databases,
Algorithm design and analysis,
Object oriented modeling,
Computational modeling"
Enhancing Exploration in Graph-like Worlds,This paper explores two enhancements that can be made to single and multiple robot exploration in graph-like worlds. One enhancement considers the order in which potential places are explored and another considers the exploitation of local neighbor information to help disambiguate possible locations. Empirical evaluations show that both enhancements can produce a significant reduction in exploration effort in terms of the number of mechanical steps required over the original exploration algorithms and that for some environments up to 60% reduction in mechanical steps can be achieved.,
RFID Tag Anti-Collision Protocol: Query Tree with Reversed IDs,"RFID system is a contactless automatic identification system using small and low-cost RFID tag. It allows to recognize the information of tag via radio frequency attaching RFID tag to an object such as a material object, human or animal. Since RFID system has an advantage to recognize massive information simultaneously, it will be able to replace the bar-code system. For this RFID system to be widely spread, the problem of multiple tag identification, which a reader identifies a multiple number of tags in a very short time, has to be solved. So far, several anti- collision algorithms are developed. We present an RFID tag anti-collision protocol, called the query tree protocol with reversed IDs (QTR protocol). This protocol works by reversing the IDs of the tags and then applying the query tree (QT) protocol. And we present the performances of the QTR protocol by simulation. QTR protocol outperforms QT protocol if the tag IDs are consecutive integers.","RFID tags,
Protocols,
Intrusion detection,
Radiofrequency identification,
Computer science,
Radio frequency,
Broadcasting,
Humans,
Animals,
Supply chain management"
Model predictive control in urban traffic network management,"The paper investigates a model predictive control (MPC) strategy specialized in urban traffic management in order to relieve traffic congestion, reduce travel time and improve homogenous traffic flow. Over the theory the realization of the control method is also presented. To validate the effectiveness of the controller a busy traffic network was chosen for test field. The MPC strategy was implemented into the test network control system (hardware in loop simulation). The applied environment is a microscopic traffic simulator with mathematical software and proper computational applications for the evaluation. The simulation results prove the effectiveness of the designed MPC based traffic control strategy. The system is able to improve the network efficiency and reduce travel time, creating optimal flow in the network subjected to control input constraints.","Automation,
Conferences"
Complexity of decoding Gabidulin codes,"In this paper, we analyze the complexity of decoding Gabidulin codes using the analogs in rank metric codes of the extended Euclidean algorithm or the Berlekamp-Massey algorithm. We show that a subclass of Gabidulin codes reduces the complexity and the memory requirements of the decoding algorithm. We also simplify an existing algorithm for finding roots of linearized polynomials for decoding Gabidulin codes. Finally we analyze and compare the asymptotic complexities of different decoding algorithms for Gabidulin codes.","Decoding,
Algorithm design and analysis,
Polynomials,
Error correction codes,
Network coding,
Public key cryptography,
Distributed computing,
Galois fields,
Algebra,
Reed-Solomon codes"
Hierarchical Methods for Landmine Detection with Wideband Electro-Magnetic Induction and Ground Penetrating Radar Multi-Sensor Systems,"A variety of algorithms are presented and employed in a hierarchical fashion to discriminate both Anti-Tank (AT) and Anti-Personnel (AP) landmines using data collected from Wideband Electro-Magnetic Induction (WEMI) and Ground Penetrating Radar (GPR) sensors mounted on a robotic platform. The two new algorithms for WEMI are based on the In-phase vs. Quadrature plot (the Argand diagram) of the complex measurement obtained at a single spatial location. The Angle Prototype Match method uses the sequence of angles as a feature vector. Prototypes are constructed from these feature vectors and used to assign mine confidence to a test sample. The Angle Model Based KNN method uses a two parameter model; where the parameters are fit to the In-phase and Quadrature data. For the GPR data, the Linear Prediction Processing and Spectral Features are calculated. All four features from WEMI and GPR are used in a Hierarchical Mixture of Experts model to increase the landmine detection rate. The EM algorithm is used to estimate the parameters of the hierarchical mixture. Instead of a two way mine/non-mine decision, the HME structure is trained to make a five way decision which aids in the detection of the low metal anti personnel mines.","Landmine detection,
Ground penetrating radar,
Prototypes,
Vectors,
Electromagnetic induction,
Robot sensing systems,
Electromagnetic measurements,
Testing,
Parameter estimation,
Personnel"
Nonnegative Tucker decomposition with alpha-divergence,"Nonnegative Tucker decomposition (NTD) is a recent multiway extension of nonnegative matrix factorization (NMF), where nonnegativity constraints are incorporated into Tucker model. In this paper we consider α-divergence as a discrepancy measure and derive multiplicative updating algorithms for NTD. The proposed multiplicative algorithm includes some existing NMF and NTD algorithms as its special cases, since α-divergence is a one-parameter family of divergences which accommodates KL-divergence, Hellinger divergence, χ2 divergence, and so on. Numerical experiments on face images show how different values of α affect the factorization results under different types of noise.","Tensile stress,
Signal processing algorithms,
Matrix decomposition,
Brain modeling,
Machine learning,
Electroencephalography,
Iterative algorithms,
Computer science,
Data analysis,
Pattern analysis"
Adaptive motion planning for humanoid robots,"Motion planning for robots with many degrees of freedom (DoF) is a generally unsolved problem in the robotics context. In this work an approach for trajectory planning is presented, which takes account of the different kinematic parts of a humanoid robot. Since not all joints of the robot are important for different planning phases, the RRT-based planner is able to adapt the number of DoF on the fly to improve the performance and the quality of the results. The runtime of the approach is evaluated in comparison to a standard RRT planner. Futhermore several extensions to the algorithm are investigated.","Planning,
Robots,
Joints,
Distance measurement,
Bidirectional control,
Torso,
Collision avoidance"
Designing an Efficient Kernel-Level and User-Level Hybrid Approach for MPI Intra-Node Communication on Multi-Core Systems,"The emergence of multi-core processors has made MPI intra-node communication a critical component in high performance computing. In this paper, we use a three-stepmethodology to design an efficient MPI intra-node communication scheme from two popular approaches: shared memory and OS kernel-assisted direct copy. We use an Intel quad-core cluster for our study. We first run microbenchmarks to analyze the advantages and limitations of these two approaches, including the impacts of processor topology, communication buffer reuse, process skew effects, and L2 cache utilization. Based on the results and the analysis, we propose topology-aware and skew-aware thresholds to build an optimized hybrid approach. Finally, we evaluate the impact of the hybrid approach on MPI collective operations and applications using IMB, NAS, PSTSWM, and HPL benchmarks. We observe that the optimized hybrid approach can improve the performance of MPI collective operations by up to 60%, and applications by up to 17%.","Magnetic cores,
Program processors,
Benchmark testing,
Bandwidth,
Topology,
Sockets,
Kernel"
Test Case Prioritization Based on Analysis of Program Structure,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement
Apros
, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",
Biotelemetric system architecture for patients and physicians - solutions not only for homecare,"Need of existence of software platform, which will allow us to monitor the patients bio-parameters and provide us with services which help with full health care, is more than relevant these days. Increasing amount of information, a new trend in home health care or desire of individuals to increase their life quality are only some aspects which confirm this need. Project Guardian concerns with this problem. Its aim is to provide solution which can be used in different spheres of health care and which will be available through PDA, web or desktop clients. Because of that the Guardian platform is based on client-server model where the web service presents server and communicates directly with database. Clients are represented by applications which use services of web service. Applying of web services provide large extension of Guardian to different spheres so Guardian is not limited only for existing clients which are indispensable part of platform, but allow us the implementation of the third parties clients.",
GP^2S: Generic Privacy-Preservation Solutions for Approximate Aggregation of Sensor Data (concise contribution),"Protecting privacy in sensor networks poses new challenges because of the potential incompatibilities between new privacy-preserving mechanisms and mechanisms already implemented in sensor networks (such as in-network data aggregation). To address this problem, we propose in this paper a set of new privacy-preservation data aggregation schemes. Different from past research, our solutions have the following features: supporting data aggregation for a variety of queries; providing privacy protection for both individual data and aggregate data; being resilient to any number of node collusion; being highly efficient.","Intelligent sensors,
Histograms,
Data privacy,
Cryptography,
Protection,
Aggregates,
Bandwidth,
Collaboration,
Pervasive computing,
Computer science"
An Examination of Genre Attributes for Web Page Classification,"In this paper, we describe a set of experiments to examine the effect of various attributes of web genre on the automatic identification of the genre of web pages. Four different genres are used in the data set, namely, FAQ, News, E-Shopping and Personal Home Pages. The effects of the number of features used to represent the web pages (5, 20, or 100) as well as the types of attributes, <content, form, functionality>, singly and in various combinations are examined. The results indicate that fewer features produce better precision but more features produce better recall, and that attributes in combinations will always perform better than single attributes.","Web pages,
Web sites,
Lifting equipment,
Computer science,
Web search,
Machine learning,
Graphics,
Navigation,
Search engines"
Color face recognition based on 2DPCA,"This paper presents a novel color face recognition approach based on 2DPCA. A matrix-representation model, which encodes the color information directly, is proposed to describe the color face image. The matrix-representation model defines the pixel in color face image as the basic unit, the color information of the pixel as the basic component, and then represents the color face image efficiently in the format of matrix. Based on the representation model, color-Eigenfaces are computed for feature extraction using 2DPCA. Nearest neighborhood classification approach is adopted to identify the color face samples. Experimental results on CVL and CMU PIE color face database show the good performance of the proposed color face recognition approach.","Face recognition,
Feature extraction,
Pixel,
Image recognition,
Covariance matrix,
Principal component analysis,
Sparse matrices,
Educational institutions,
Image databases,
Spatial databases"
EESM for IEEE 802.16e: WiMaX,"For Link Level interfacing packet switched simulation adopt the AVI (actual value interface) approach for mapping the computed SNR to the Block Error Rate (BLER) Statistics. However in OFDM based system the BLER is invariant within Radio transport channel due to fading channel and adjutant bit carriers mapping, rendering the AVI to be inaccurate. Due to large variability of the channel in frequency domain, two links with same average SNR can experience drastically different performance, thus making it difficult to accurately predict the instantaneous link performance such as Frame Error Rate (FER). To overcome this problem with AVI the EESM has been used as a low complexity approach. In this paper the feasibility of EESM as an abstraction method is investigated, and then calibrated and evaluated for Link Level Interface in WiMax System IEEE802.16e.This methods is then extended to handle more advanced link enhancement such as Alamouti encoding. The Exp-ESM method has better accuracy than the existing link error methods, and preferred link error predictor for system simulator.","WiMAX,
Error analysis,
Packet switching,
Computational modeling,
Computer interfaces,
OFDM,
Fading,
Rendering (computer graphics),
Frequency domain analysis,
Predictive models"
A Simulation Study of Common Mobility Models for Opportunistic Networks,"Understanding mobility characteristics is important for the design and analysis of routing schemes for mobile ad hoc networks (MANETs). This is especially true for mobile opportunistic networks where node mobility is utilized to achieve message delivery. In this paper, we study the properties of common mobility models. Specifically, we study inter-contact times of mobile nodes in Random Waypoint and Random Direction mobility models under opportunistic network settings. We also introduce a modified Random Waypoint model with hot-spots to study its mobility properties. Through extensive simulation study, we provide simulation results for mobility properties of Random Waypoint and Random Direction models. Further, our modified Random Waypoint with hot-spots model is also found to show an approximate power-law and exponential inter-contact time dichotomy found in real-world mobility traces as described in recent literature.","Routing,
Analytical models,
Mobile ad hoc networks,
Computational modeling,
Computer simulation,
Relays,
Algorithm design and analysis,
Delay effects,
Measurement,
Computer science"
Construction Tele-robot System With Virtual Reality,"In this paper, a tele-robotics system for a construction machine is investigated. The system consists of a servo-controlled construction robot, two joysticks for controlling the robot, and a 3-dimensional virtual environment. Computer graphics (CG) of the robot and task objects are presented to the operator in virtual scene as a tool for assisting teleoperation. The operator performs remote operation of the construction robot by manipulating the graphic robot directly in virtual environment using the joysticks. The position and shape of the task objects in virtual world are updated in real time on basis of image information obtained by a stereo camera fixed in remote field. To improve the efficiency as well as security of teleoperation, and overcome the shortcomings (such as time-consuming judgement and mistaken selection of screen to be observed) of conventional three-screens visual display, the methods of auto point of view (APV) and semi-transparent object (STO) are introduced in this paper. Finally, A comparative study of a one-screen visual display combining APV and STO with the conventional three-screens visual display is carried out based on the results of evaluation experiments (task efficiency, risk indexes, questionnaire). These experiments confirmed that one-screen visual display combining APV and STO is superior in operability, safety, and reduction of stress.","Virtual reality,
Displays,
Virtual environment,
Computer graphics,
Robot control,
Control systems,
Character generation,
Layout,
Shape,
Robot vision systems"
PE File Header Analysis-Based Packed PE File Detection Technique (PHAD),"In order to conceal malware, malware authors use the packing and encryption techniques. If the malware is packed or encrypted, then it is very difficult to analyze. Therefore, to prevent the harmful effects of malware and to generate signatures for malware detection, the packed and encrypted executable codes must initially be unpacked. The first step of unpacking is to detect the packed executable files. In this paper, a packed file detection technique (PHAD) based on a PE Header Analysis is proposed. In many cases, to pack and unpack the executable codes, PE files have unusual attributes in their PE headers. In this paper, these characteristics are utilized to detect the packed files. A Characteristic Vector (CV) that consists of eight elements is defined, and the Euclidean distance (ED) of the CV is calculated. The EDs of the packed files are calculated and represent the base threshold for the detection of packed files.","Cryptography,
Euclidean distance,
Entropy,
Internet,
Distance measurement,
Conferences,
Security"
HCH: A New Tweakable Enciphering Scheme Using the Hash-Counter-Hash Approach,"The notion of tweakable block ciphers was formally introduced by Liskov-Rivest-Wagner at Crypto 2002 (the 2002 Annual International Cryptology Conference). The extension and the first construction, called CMC, of this notion to tweakable enciphering schemes which can handle variable length messages was given by Halevi-Rogaway at Crypto 2003. In this paper, we present HCH, which is a new construction of such a scheme. The construction uses two universal hash computations with a counter mode of encryption in-between. This approach was first proposed by McGrew-Viega to build a scheme called XCB and later used by Wang-Feng-Wu, to obtain a scheme called HCTR. A unique feature of HCH compared to all known tweakable enciphering schemes is that HCH uses a single key, can handle arbitrary length messages, and has a quadratic security bound. An important application of a tweakable enciphering scheme is disk encryption. HCH is well suited for this application. We also describe a variant, which can utilize precomputation and makes one less block cipher call. This compares favorably to other hash-encrypt-hash-type constructions, supports better key agility and requires less key material.",
An intrusion detection system for wireless process control systems,"A recent trend in the process control system (PCS) is to deploy sensor networks in hard-to-reach areas. Using wireless sensors greatly decreases the wiring costs and increases the volume of data gathered for plant monitoring. However, ensuring the security of the deployed sensor network, which is part of the overall security of PCS, is of crucial importance. In this paper, we design a model-based intrusion detection system (IDS) for sensor networks used for PCS. Given that PCS tends to have regular traffic patterns and a well-defined request-response communication, we can design an IDS that models normal behavior of the entities and detects attacks when there is a deviation from this model. Model-based IDS can prove useful in detecting unknown attacks.","Monitoring,
SCADA systems,
Wireless sensor networks,
Wireless communication,
Process control,
Security,
Intrusion detection"
A peer-to-peer multidimensional trust model for digital ecosystems,"The aim of the Digital Ecosystem (DE) initiative is to encourage Small and Medium-sized Enterprises (SMEs) to use the Internet and to adopt ICT technologies that would make them more innovative and competitive in the market. In order for a DE to take-off, specific solutions are required that are practical and easy to adopt, and that address the organizational and infrastructural particularities of the networked communities. This paper proposes a new trust model for DEs which has several innovative features. The model is based on the concept of social networks and addresses trust at different levels: user, data, service and node. The model allows fast bootstrapping of trust by importing existing trust relationships from outside DE systems and by relying on certificates issued by trusted authorities external to the DE. Furthermore, trust can be measured in a variety of contexts by using user-defined tags - folksonomy. The model abstracts from specific reputation algorithms by providing necessary interfaces for plugging-in those on one’s own choice.","Biological system modeling,
Peer to peer computing,
Social network services,
Ecosystems,
Unified modeling language,
Organizations,
Context modeling"
Determination of food portion size by image processing,"Overweight and obesity have become an epidemic in many parts of the world threatening the health of over one billion people. In order to combat this epidemic effectively, it is desirable to develop new methods to monitor individual's food intake and provide quantitative information about the nutrients and calories consumed in people's daily life. We present an electronic photographic approach and associated image processing algorithms to estimate food portion size, which is then utilized to obtain the required information. Our experiments show that our approach is accurate, providing an effective tool for people to track their nutritional and energy intake.",
Adapting to Run-Time Changes in Policies Driving Autonomic Management,"The use of policies within autonomic computing has received significant interest in the recent past. Policy-driven management offers significant benefit since it makes it more straight forward to define and modify systems behavior at run-time, through policy manipulation, rather than through re-engineering. In this paper, we present an adaptive policy-driven autonomic management system which makes use of reinforcement learning methodologies to determine how to best use a set of active policies to meet different performance objectives. The focus, in particular, is on strategies for adapting what has been learned for one set of policy actions to a ""similar'' set of policies when run-time policy modifications occur. We illustrate the impact of the adaptation strategies on the behavior of a multi-component Web server.",
Resource Bundles: Using Aggregation for Statistical Wide-Area Resource Discovery and Allocation,"Resource discovery is an important process for finding suitable nodes that satisfy application requirements in large loosely-coupled distributed systems. Besides inter-node heterogeneity, many of these systems also show a high degree of intra-node dynamism, so that selecting nodes based only on their recently observed resource capacities for scalability reasons can lead to poor deployment decisions resulting in application failures or migration overheads. In this paper, we propose the notion of a resource bundle ---a representative resource usage distribution for a group of nodes with similar resource usage patterns --- that employs two complementary techniques to overcome the limitations of existing techniques: resource usage histograms to provide statistical guarantees for resource capacities, and clustering-based resource aggregation to achieve scalability. Using trace-driven simulations and data analysis of a month-long Planet Lab trace, we show that resource bundles are able to provide high accuracy for statistical resource discovery (up to 56% better precision than using only recent values), while achieving high scalability (up to 55% fewer messages than a non-aggregation algorithm). We also show that resource bundles are ideally suited for identifying group-level characteristics such as finding load hot spots and estimating total group capacity (within 8% of actual values).","Histograms,
Clustering algorithms,
Scalability,
Accuracy,
Data analysis,
Analytical models,
Algorithm design and analysis"
Bounding Worst-Case Response Time for Tasks with Non-Preemptive Regions,"Real-time schedulability theory requires {\em a priori} knowledge of the worst-case execution time (WCET) of every task in the system. Fundamental to the calculation of WCET is a scheduling policy that determines priorities among tasks.  Such policies can be non-preemptive or preemptive.  While the former reduces analysis complexity and overhead in implementation, the latter provides increased flexibility in terms of schedulability for higher utilizations of arbitrary task sets. In practice, tasks often have non-preemptive regions but are otherwise scheduled preemptively.  To bound the WCET of tasks, architectural features have to be considered in the context of a scheduling scheme. In particular, preemption affects caches, which can be modeled by bounding the cache-related preemption delay (CRPD) of a task. In this paper, we propose a framework that provides safe and tightbounds of the data-cache related preemption delay (D-CRPD), the WCETand the worst-case response times, not just for homogeneous tasks under fully preemptive or fully non-preemptive systems, but for tasks with a non-preemptive region. By retaining the option of preemption where legal, task sets become schedulable that might otherwise not be. Yet, by requiring a region within a task to be non-preemptive, correctness is ensured in terms of arbitration of access to shared resources. Experimental results confirm an increase in schedulability of a task set with non-preemptive regions over an equivalent task set where only those tasks with non-preemptive regions are scheduled non-preemptively altogether. Quantitative results further indicate that D-CRPD bounds and response-time bounds comparable to task sets with fully non-preemptive tasks can be retained in the presence of short non-preemptive regions. To the best of our knowledge, this is the first framework that performs D-CRPD calculations in a system for tasks with a non-preemptive region.","Delay,
Real time systems,
Processor scheduling,
Application software,
Computer science,
Reliability theory,
Embedded computing,
Law,
Legal factors,
Upper bound"
Emergent dynamics of turn-taking interaction in drumming games with a humanoid robot,"We present results from an empirical study investigating emergent turn-taking in a drumming experience involving Kaspar, a humanoid child-sized robot, and adult participants. In this work, our aim is to have turn-taking and role switching which is not deterministic but emerging from the social interaction between the human and the humanoid. Therefore the robot is not just ‘following’ and imitating the human, but could be the leader in the game and being imitated by the human. Data from the first implementation of a human-robot interaction experiment are presented and analysed qualitatively (in terms of participants’ subjective experiences) and quantitatively (concerning the drumming performance of the human-robot pair). Results are analysed statistically and show significant differences for the three games (with different probabilistic models) where the models enabling more interaction and more “natural” turn-taking were preferred by the human participants.",Robots
Performance Evaluation of Wireless Cellular Networks with Mixed Channel Holding Times,"In most analytical models for wireless cellular networks, the channel holding times for both new and handoff calls are usually assumed to be independent and identically distributed. However, simulation study and field data show that this assumption is invalid. In this paper, we present a new general analytical model in wireless cellular networks where channel holding times for new and handoff calls are distinctly distributed with different average values. For our proposed model, we first derive the explicit matrix product-form solution of the stationary probability for number of new and handoff calls in the system. We then show that the expression of the stationary probability for total number of calls in the system possesses a scalar product-form solution if and only if the expected channel holding times for both new and handoff calls are the same. Moreover, we derive analytical results for the blocking probabilities of new and handoff calls. Finally, we compare our new theoretical results with the corresponding simulation results and two already existing approximations. Through this comparison study, we show that our analytical results are indeed the same as the simulation results and that there are certainly significant estimation errors for the existing approximations.","Land mobile radio cellular systems,
Analytical models,
Wireless cellular systems,
Estimation error,
Random variables,
Computer science,
Cellular networks,
Telecommunication traffic,
Traffic control,
Base stations"
An Adaptive Framework for Multiprocessor Real-Time System,"In this paper, we develop an adaptive scheduling framework for changing theprocessor shares of tasks---a process called reweighting---on real-timemultiprocessor platforms.  Our particular focus is  adaptive frameworks that aredeployed in environments in which tasks may frequently require significant sharechanges.  Prior work on enabling real-time adaptivity on multiprocessors hasfocused exclusively on scheduling algorithms that can enact needed adaptations. The algorithm proposed in this paper uses both feedback and optimizationtechniques to determine at runtime which adaptations are needed.",processor scheduling
Intrinsically motivated hierarchical manipulation,We present a framework for the programming of manipulation behavior by means of an intrinsic reward function that encourages the building of deep control knowledge. We show how this framework can be used to teach new manipulation skills in a hierarchical and incremental fashion. We demonstrate the contributions of this paper on a humanoid robot through three incremental learning stages,"Robot sensing systems,
Optimal control,
Feedback,
Robotics and automation,
Robot programming,
Learning,
Force control,
USA Councils,
Laboratories,
Computer science"
Element failure detection in linear antenna arrays using case-based reasoning,"The present work proposes a novel case-based reasoning system for fault diagnosis in moderate or large linear antenna arrays. This system identifies the set of elements that are most likely to be defective, helping to significantly reduce the computational costs of their detection (e.g., using an optimization technique such as a genetic algorithm).","Linear antenna arrays,
Antenna radiation patterns,
Genetic algorithms,
Antenna arrays,
Support vector machines,
Support vector machine classification,
Computational efficiency,
Radar antennas,
Antennas and propagation,
Computer science"
When the fingers do the talking: A study of group participation with varying constraints to a tabletop interface,"A user study is presented that investigates how different configurations of input can influence equity of participation around a tabletop interface. Groups of three worked on a design task requiring negotiation in four interface conditions that varied the number (all members can act or only one) and type (touch versus mice) of input. Our findings show that a multi-touch surface increases physical interaction equity and perceptions of dominance, but does not affect levels of verbal participation. Dominant people still continue to talk the most, while quiet ones remain quiet. Qualitative analyses further revealed how other factors can affect how participants contribute to the task. The findings are discussed in terms of how the design of the physical technological set-up can affect the desired form of collaboration.","Mice,
Collaboration,
Atmospheric measurements,
Particle measurements,
Indexes,
Analysis of variance,
Fingers"
"Realtime face detection and tracking using a single Pan, Tilt, Zoom camera","Surveillance cameras generally have a wide field of view and do not capture the human face with a resolution that is sufficient for machine recognition. To overcome this problem, this paper presents a realtime face detection and tracking algorithm using a single PTZ (Pan, Tilt, Zoom) camera. Unlike existing algorithms which track the human face in the field of view of a fixed camera, the proposed algorithm adjusts the pan, tilt and zoom of a PTZ camera in real-time so that human faces can be acquired at the camera’s maximum resolution. The proposed algorithm addresses challenges like network delays, camera movement lags, oscillations and lost faces. Experiments were performed in realistic surveillance scenarios and accurate real-time tracking with pan, tilt and zoom was achieved.","Face detection,
Cameras,
Face recognition,
Surveillance,
Humans,
Lighting,
Target tracking,
Videos,
Monitoring,
Computer science"
A characterization of instruction-level error derating and its implications for error detection,"In this work, we characterize a significant source of software derating that we call instruction-level derating. Instruction-level derating encompasses the mechanisms by which computation on incorrect values can result in correct computation. We characterize the instruction-level derating that occurs in the SPEC CPU2000 INT benchmarks, classifying it (by source) into six categories: value comparison, sub-word operations, logical operations, overflow/precision, lucky loads, and dynamically-dead values. We also characterize the temporal nature of this derating, demonstrating that the effects of a fault persist in architectural state long after the last time they are referenced. Finally, we demonstrate how this characterization can be used to avoid unnecessary error recoveries (when a fault will be masked by software anyway) in the context of a dual modular redundant (DMR) architecture.","instruction sets,
error detection"
Minimum variance control over a Gaussian communication channel,"We consider the problem of minimizing the response of a plant output to a stochastic disturbance using a control law that relies on the output of a noisy communication channel. We discuss a lower bound on the performance achievable at a specified terminal time using nonlinear time-varying communication and control strategies, and show that this bound may be achieved using strategies that are linear.",
On the Bit Error Probability of Noisy Channel Networks With Intermediate Node Encoding,"We investigate the calculation approach of the sink bit error probability (BEP) for a network with intermediate node encoding. The network consists of statistically independent noisy channels. The main contributions are, for binary network codes, an error marking algorithm is given to collect the error weight (the number of erroneous bits). Thus, we can calculate the exact sink BEP from the channel BEPs. Then we generalize the approach to nonbinary codes. The coding scheme works on the Galois field 2m, where m is a positive integer. To reduce computational complexity, a subgraph decomposition approach is proposed. In general, it can significantly reduce computational complexity, and the numerical result is also exact. For approximate results, we discuss the approach of only considering error events in a single channel. The results well approximate the exact results in low BEP regions with much lower complexity.","Error probability,
Network coding,
Error correction codes,
Computer errors,
Computer science,
Computational complexity,
Monte Carlo methods,
Information theory,
Materials science and technology,
Galois fields"
A Distributed Multi-channel Cognitive MAC Protocol for IEEE 802.11s Wireless Mesh Networks,"In this paper, we propose a novel distributed frequency agile medium access control (MAC) extension to the IEEE 802.11s amendment for the next generation wireless mesh networks (WMNs). The proposed scheme has complete backward compatibility with the legacy IEEE 802.11 and the emerging 802.11s while, it is perfectly capable of multi-channel deployment of available frequency opportunities in order to coordinate concurrent multiple data transmissions. The root concept of the proposed enhancement is mainly based on the deployment of well-known ISM frequency bands, where the existing 802.11-based wireless equipments nowadays operate, as the common control channel in order to establish concurrent data transmissions. Here, we apply the aforementioned key concept to the IEEE 802.11s common channel framework (CCF) to attain two important goals: on one hand, the proposed scheme improves the channel utilization and capacity using the concept of cognitive radio and on the other hand, using the same concept it leads to lower access delay due to smarter decision making procedures exploited for link layer connection establishment. Through extensive simulations, which also take into account primary user (PU) appearance in non-ISM frequency opportunities, performance of the proposed medium access control (MAC) enhancement is evaluated showing its remarkable efficiency and better wireless medium access management.","Media Access Protocol,
Wireless application protocol,
Wireless mesh networks,
Frequency,
Data communication,
Access protocols,
Next generation networking,
Channel capacity,
Cognitive radio,
Delay"
Knee Point Detection on Bayesian Information Criterion,"The main challenge of cluster analysis is that the number of clusters or the number of model parameters is seldom known, and it must therefore be determined before clustering. Bayesian Information Criterion (BIC) often serves as a statistical criterion for model selection, which can also be used in solving model-based clustering problems, in particular for determining the number of clusters. Conventionally, a correct number of clusters can be identified as the first decisive local maximum of BIC; however, this is intractable due to the overtraining problem and inefficiency of clustering algorithms. To circumvent this limitation, we proposed a novel method for identifying the number of clusters by detecting the knee point of the resulting BIC curve instead. Experiments demonstrated that the proposed method is able to detect the correct number of clusters more robustly and accurately than the conventional approach.","Knee,
Bayesian methods,
Clustering algorithms,
Detection algorithms,
Speech analysis,
Parameter estimation,
Artificial intelligence,
Speech processing,
Image processing,
Computer science"
OSGi-Based Smart Home Architecture for heterogeneous network,"With the development of home network and service applications, different protocols and transmission modes are proposed. More digital devices and home appliance compliance to the protocols in the development. The proposed protocols are usually unable to communicate with each other; we design and implement a Service-Oriented Smart-Home Architecture to integrate popular protocols such as UPnP, Jini, DPWS on OSGi framework and collaborating Tmote, Zigbee and Bluetooth to converge various service oriented applications. Furthermore, with the well-developed Tmote, Zigbee and Bluetooth technology, majority of devices developed with these technologies supported, we propose the three new base drivers to integrate different devices communication on our platform. Additionally, we propose a Service Resolving Bundle to complement the drawbacks of OSGi mechanisms. This architecture with service-oriented mechanisms accommodates applications implemented across different domains and allows system components to interact with one another.","Smart homes,
Home automation,
Protocols,
ZigBee,
Bluetooth,
Computer architecture,
Application software,
Home appliances,
Context,
Java"
Cluster-Based Ensemble Classification for Hyperspectral Remote Sensing Images,"Hyperspectral remote sensing images play a very important role in the discrimination of spectrally similar land-cover classes. In order to obtain a reliable classifier, a larger amount of representative training samples are necessary compared to multi-spectral remote sensing data. In real applications, it is difficult to obtain a sufficient number of training samples for supervised learning. Besides, the training samples may not represent the real distribution of the whole space. To attack the quality problems of training samples, we proposed a Cluster-based ENsemble Algorithm (CENA) for the classification of hyperspectral remote sensing images. Data set collected from ROSIS university validates the effectiveness of the proposed approach.","Hyperspectral sensors,
Hyperspectral imaging,
Remote sensing,
Clustering algorithms,
Kernel,
Supervised learning,
Computer science,
Reliability engineering,
Robustness,
Semisupervised learning"
Automated Generation of Pointcut Mutants for Testing Pointcuts in AspectJ Programs,"Aspect-Oriented Programming (AOP) provides new modularization of software systems by encapsulating cross-cutting concerns. AspectJ, an AOP language, uses abstractions such as pointcuts, advice, and aspects to achieve AOP’s primary functionality. Faults in pointcuts can cause aspects to fail to satisfy their requirements. Hence, testing pointcuts is necessary in order to ensure correctness of aspects. In mutation testing of pointcuts (a type of fault-based pointcut testing), the number of mutants (i.e., variations) for pointcuts is usually large due to the usage of wildcards. It is tedious to manually identify effective mutants that are of appropriate strength and resemble closely the original pointcut expression, reflecting the kind of mistakes that developers may make. To reduce developers’ effort in this process, we have developed a new framework that automatically identifies the strength of each pointcut and generates pointcut mutants with different strengths. Developers can inspect the pointcut mutants and their join points for pointcut correctness or choose the mutants for conducting mutation testing. We conducted an empirical study on applying our framework on pointcuts from existing AspectJ programs. The results show that our framework can provide valuable assistance in generating effective mutants that are close to the original pointcuts and are of appropriate strength.","Automatic testing,
Genetic mutations,
Software systems,
Fault detection,
Software reliability,
Reliability engineering,
Software testing,
System testing,
Computer science,
Automatic programming"
Using grid for accelerating density-based clustering,"Clustering analysis is a primary method for data mining. The ever increasing volumes of data in different applications forces clustering algorithms to cope with it. DBSCAN is a well-known algorithm for density-based clustering. It is both effective so it can detect arbitrary shaped clusters of dense regions and efficient especially in existence of spatial indexes to perform the neighborhood queries efficiently. In this paper we introduce a new algorithm GriDBSCAN to enhance the performance of DBSCAN using grid partitioning and merging, yielding a high performance with the advantage of high degree of parallelism. We verified the correctness of the algorithm theoretically and experimentally, studied the performance theoretically and using experiments on both real and synthetic data. It proved to run much faster than original DBSCAN. We compared the algorithm with a similar algorithm, EnhancedDBSCAN, which is also an enhancement to DBSCAN using partitioning. Experiments showed the new algorithm’s superiority in performance and degree of parallelism.",
A statistical deformation prior for non-rigid image and shape registration,"Non-rigid registration is central to many problems in computer vision and medical image analysis. We propose a registration algorithm which is regularized by prior knowledge in the form of a statistical deformation model. This model is obtained from previous registrations performed on a set of noise-free training examples given by images, or shapes represented by level set functions. Contrary to similar approaches, our method does not strictly constrain the result to lie in the span of the statistical model but rather uses the model for Tikhonov regularization. Therefore, our method can be used to reduce the influence of noise and artifacts even when the model contains only a few typical examples. This automatically gives rise to a bootstrapping strategy for building statistical models from noisy data sets requiring only a limited number of high quality examples. We demonstrate the effectiveness of the approach on synthetic and medical images.","Shape,
Deformable models,
Image registration,
Biomedical imaging,
Computer vision,
Image analysis,
Noise shaping,
Image segmentation,
Computer science,
Noise level"
An Algorithm in SwinDeW-C for Scheduling Transaction-Intensive Cost-Constrained Cloud Workflows,"The concept of cloud computing has been wide spreading very recently. Cloud computing has many unique advantages which can be utilised to facilitate (cloud) workflow execution. Transaction-intensive cost-constrained cloud workflows are workflows with a large number of workflow instances (i.e. transaction intensive) bounded by a certain budget for execution (i.e. cost constrained) in a cloud computing environment (i.e. cloud workflows). However, there are not any specific scheduling algorithms so far for transaction-intensive cost-constrained cloud workflows. This paper presents a novel scheduling algorithm which considers the characteristics of cloud computing to accommodate transaction-intensive cost-constrained workflows by compromising execution time and cost with user input enabled on the fly. The simulation performed demonstrates that the algorithm can reduce the mean execution cost while meeting the user-requested deadline.","Scheduling algorithm,
Processor scheduling,
Cloud computing,
User interfaces,
Monitoring,
Computer science,
Australia,
Cost function,
Throughput,
Resource management"
IEEE 802.16 Capacity Enhancement Using an Adaptive TDD Split,"In urban areas, users have become more and more accustomed to the availability of broadband access. However, in rural and suburban areas, it is often too expensive for network providers to serve every user with traditional wired broadband access such as DSL or cable modem. In such areas, WiMAX (worldwide interoperability for microwave access) networks based on the IEEE 802.16 standard are the most promising solution. In this paper, we focus on the performance of the IEEE 802.16 time division duplex (TDD) mode in rural areas with only one cell. When using TDD, the duration of the downlink and uplink subframe can be set individually, which is called adaptive time division duplex (ATDD). However, the ratio between downlink and uplink is normally set to a fixed value. We will show the performance of different fixed downlink/uplink ratios for several traffic models. Furthermore, we propose an algorithm for an adaptive downlink/uplink boundary in dependence on the current traffic load in the cell. The results show an enormous performance gain compared to the fixed settings.","Downlink,
WiMAX,
Base stations,
Switches,
Time division multiple access,
Programmable control,
Adaptive control,
Computer science,
Bandwidth,
Scheduling"
Decimal multiplication using compact BCD multiplier,"Decimal multiplication is an integral part of financial, commercial, and internet-based computations. The basic building block of a decimal multiplier is a single digit multiplier. It accepts two Binary Coded Decimal (BCD) inputs and gives a product in the range [0, 81] represented by two BCD digits. A novel design for single digit decimal multiplication that reduces the critical path delay and area is proposed in this research. Out of the possible 256 combinations for the 8-bit input, only hundred combinations are valid BCD inputs. In the hundred valid combinations only four combinations require 4 × 4 multiplication, 64 combinations need 3 × 3 multiplication, and the remaining 32 combinations use either 3 × 4 or 4 × 3 multiplication. The proposed design makes use of this property. This design leads to more regular VLSI implementation, and does not require special registers for storing easy multiples. This is a fully parallel multiplier utilizing only combinational logic, and is extended to a Hex/Decimal multiplier that gives either a decimal output or a binary output. The accumulation of partial products generated using single digit multipliers is done by an array of multi-operand BCD adders for an (n-digit × n-digit) multiplication.","Delay,
Logic,
Internet,
Application software,
Floating-point arithmetic,
Jacobian matrices,
Very large scale integration,
Digital arithmetic,
Computer errors,
Banking"
"Fast Radix- q
and Mixed-Radix Algorithms for Type-IV DCT","This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.",
posVibEditor: Graphical authoring tool of vibrotactile patterns,"This paper presents an authoring tool, posVibEditor, for quick and easy design of vibrotactile patterns for vibration motors. The tool supports the drag-and-drop design paradigm so that novice users can easily learn and interact with the tool. Vibration patterns are managed in a database using XML formats in order to improve their reusability and extensibility. The multi-channel timeline interface is also developed for designing time-synchronized multiple vibrotactile patterns for multiple vibration motors. In addition, an internal vibration player is incorporated to allow the user to test and evaluate designed patterns immediately. The last unique feature is a module for perceptually transparent rendering that can guarantee the delivery of perceptually correct vibration effects. The authoring tool is suitable to mobile devices that contain a single vibration motor as well as applications in virtual reality and haptics that frequently employ multiple vibration motors.","Haptic interfaces,
Piezoelectric actuators,
Virtual reality,
XML,
Testing,
Feedback,
Humans,
Graphical user interfaces,
Conferences,
Computer science"
An Approach for Mapping Features to Code Based on Static and Dynamic Analysis,"System evolution depends greatly on the ability of a maintainer to locate source code that is specific to feature implementation. Existing feature location techniques require either exercising several features of the system, or rely heavily on domain experts to guide the feature location process. In this paper, we present a novel approach for feature location that combines static and dynamic analysis techniques. An execution trace is generated by exercising the feature under study (dynamic analysis). A component dependency graph (static analysis) is used to rank the components invoked in the trace according to their relevance to the feature. Our ranking technique is based on the impact of a component modification on the rest of the system. The proposed approach is automatic to a large extent relieving users from any decision that would otherwise require extensive domain knowledge of the system. A case study is presented to support and evaluate the applicability of our approach.","Software maintenance,
Information analysis,
Computer science,
Software engineering,
Indexing,
Unified modeling language,
Information resources,
Performance analysis,
Software performance,
Reconnaissance"
Large-scale image database triage via EEG evoked responses,"This paper describes an approach for target image search using human brain signals generated by perceptual processes in the brain. The human brain generates event related potentials (ERPs) in response to critical events, such as interesting/novel visual stimuli in the form of a target image. In this paper, we describe experiments involving six professional image analysts and summarize the ERP detection performance as they search for targets within a large image database. We develop a disjoint windowing scheme for data preprocessing to discard irrelevant and redundant information from the raw data to get clean training data. We apply support vector machines to detect ERPs and conduct 10-fold cross validation for parameter regularization. The results demonstrate that the ERP pattern recognition can provide reliable inference for image triage.","Large-scale systems,
Image databases,
Electroencephalography,
Enterprise resource planning,
Humans,
Signal processing,
Signal generators,
Image analysis,
Performance analysis,
Data preprocessing"
Attention Model Based SIFT Keypoints Filtration for Image Retrieval,"Effective feature extraction is a fundamental component of content-based image retrieval. Scale Invariant Feature Transform (SIFT) has been proven to be the most robust local invariant feature descriptor. However, SIFT algorithm generates hundreds of thousands of keypoints per image, and most of them comes from background. This has seriously affected the application of SIFT in real-time image retrieval. This paper addresses this problem and proposes a novel method to filter the SIFT keypoints using attention model. Based on visual attention analysis, all of the keypoints in an image are ranked with their attention saliency, and only the most distinctive keypoints will be reserved. Then we use Bag of words to efficiently index these features. Experiments demonstrate that the attention model based SIFT keypoints filtration algorithm provides significant benefits both in retrieval accuracy and matching speed.",
On Secret Reconstruction in Secret Sharing Schemes,"A secret sharing scheme typically requires secure communications in each of two distribution phases: (1) a dealer distributes shares to participants (share distribution phase); and later (2) the participants in some authorised subset send their share information to a combiner (secret reconstruction phase). While problems on storage required for participants, for example, the size of shares, have been well studied, problems regarding the communication complexity of the two distribution phases seem to have been mostly neglected in the literature so far. In this correspondence, we deal with several communication related problems in the secret reconstruction phase. Firstly, we show that there is a tradeoff between the communication costs and the number of participants involved in the secret reconstruction. We introduce the communication rate as the ratio of the secret size and the total number of communication bits transmitted from the participants to the combiner in the secret reconstruction phase. We derive a lower bound on the communication rate and give constructions that meet the bound. Secondly, we show that the point-to-point secure communication channels for participants to send share information to the combiner can be replaced with partial broadcast channels. We formulate partial broadcast channels as set systems and show that they are equivalent to the well-known combinatorial objects of cover-free family. Surprisingly, we find that the number of partial broadcast channels can be significantly reduced from the number of point-to-point secure channels. Precisely, in its optimal form, the number of channels can be reduced from n to O(log n), where is the number of participants in a secret sharing scheme. We also study the communication rates of partial broadcast channels for the secret reconstruction.","Cryptography,
Broadcasting,
Physics computing,
Complexity theory,
Costs,
Communication channels,
Information security,
Multicast communication,
Australia Council,
Computer science"
Study of different approach to clustering data by using the Particle Swarm Optimization Algorithm,"This paper proposes two new data clustering approaches using the particle swarm optimization algorithm (PSO). It is shown how the PSO can be used to find centroids of a user specified number of clusters. The proposed approaches are an attempt to improve the Merwe and Engelbrecht method using different fitness functions and considering the situation where data is uniformly distributed. The data clustering PSO algorithm, using the original and proposed fitness functions is evaluated on well known data sets. Notable improvements on the results were achieved by the modifications, this shows the potential of the PSO, not only on data clustering but also on the several areas it can be applied.",Evolutionary computation
Channel modeling based on interference temperature in underlay cognitive wireless networks,"Cognitive radio based dynamic spectrum access network is emerging as a technology to address spectrum scarcity. In this study, we assume that the channel is licensed to some primary (licensed) operator. We consider a sensor network with cognitive radio capability that acts as a secondary (unlicensed) network and uses the channel in underlay mode. The secondary network uses interference temperature model [1] to ensure that the interference to the primary devices remain below a predefined threshold. We use Hidden Markov Model (HMM) to model the interference temperature dynamics of a primary channel. The HMM is trained using Baum-Welch procedure. The trained HMM is shown to be statistically stable. Secondary nodes use this trained HMM to predict the interference temperature of the channel in future time slots and computes the value of Channel Availability Metric (CAM) for the channel. CAM is used by secondary nodes to select a primary channel for transmission. Results of application of such trained HMMs in channel selection in multi-channel wireless network are presented.","Interference,
Wireless networks,
Hidden Markov models,
Temperature sensors,
Cognitive radio,
Antenna measurements,
Power measurement,
Receiving antennas,
Computer aided manufacturing,
CADCAM"
Probabilistic allocation of tasks on desktop grids,"While desktop grids are attractive platforms for executing parallel applications, their volatile nature has often limited their use to so-called ""high-throughput"" applications. Checkpointing techniques can enable a broader class of applications. Unfortunately, a volatile host can delay the entire execution for a long period of time. Allocating redundant copies of each task to hosts can alleviate this problem by increasing the likelihood that at least one instance of each application task completes successfully. In this paper we demonstrate that it is possible to use statistical characterizations of host availability to make sound task replication decisions. We find that strategies that exploit such statistical characterizations are effective when compared to alternate approaches. We show that this result holds for real-world host availability data, in spite of only imperfect statistical characterizations.","Availability,
Grid computing,
Application software,
Distributed computing,
Checkpointing,
Delay effects,
Central Processing Unit,
Internet,
Local area networks,
Throughput"
EAD: An Efficient and Adaptive Decentralized File Replication Algorithm in P2P File Sharing Systems,"In peer-to-peer file sharing systems, file replication technology is widely used to reduce hot spots and improve file query efficiency. Most current file replication methods replicate files in all nodes or two endpoints on a client-server query path. However, these methods either have low effectiveness or come at a cost of high overhead. This paper presents an Efficient and Adaptive Decentralized file replication algorithm (EAD) that achieves high query efficiency and high replica utilization at a significantly low cost. EAD enhances the utilization of file replicas by selecting query traffic hubs and frequent requesters as replica nodes, and dynamically adapting to non-uniform and time-varying file popularity and node interest. Unlike current methods, EAD creates and deletes replicas in a decentralized self-adaptive manner while guarantees high replica utilization. Simulation results demonstrate the efficiency and effectiveness of EAD in comparison with other approaches in both static and dynamic environments. It dramatically reduces the overhead of file replication, and yields significant improvements on the efficiency and effectiveness of file replication in terms of query efficiency, replica hit rate and overloaded nodes reduction.","Peer to peer computing,
Servers,
File servers,
Algorithm design and analysis,
Telecommunication traffic,
Maintenance engineering,
Internet"
A Density-Based Algorithm for Redundant Reader Elimination in a RFID Network,"Radio frequency identification (RFID) technology has become more sophisticated in recent years and is being developed rapidly for a variety of applications. The problem of eliminating redundant readers has been reduced to the minimum cover problem and proved to be NP-hard. In this paper, a Density-based Redundant Reader Elimination Algorithm (DRRE) is presented; DRRE eliminates the redundant readers without influencing the number of usable tags. Simulation results demonstrate that the DRRE algorithm performed better in all instances in a variety of environments compared with other algorithms. In a densely deployed RFID network, the DRRE algorithm detected 85% more redundant readers than others.","Radiofrequency identification,
Low earth orbit satellites,
Time division multiple access,
Algorithm design and analysis,
Writing,
Radio frequency,
Computer science,
Information management,
Multiaccess communication,
Frequency division multiaccess"
"Low-power, high-performance analog neural branch prediction","Shrinking transistor sizes and a trend toward low-power processors have caused increased leakage, high per-device variation and a larger number of hard and soft errors. Maintaining precise digital behavior on these devices grows more expensive with each technology generation. In some cases, replacing digital units with analog equivalents allows similar computation to be performed at higher speed and lower power. The units that can most easily benefit from this approach are those whose results do not have to be precise, such as various types of predictors. We introduce the Scaled Neural Predictor (SNP), a highly accurate prediction algorithm that is infeasible in a purely digital implementation, but can be implemented using analog circuitry. Our analog implementation, the Scaled Neural Analog Predictor (SNAP), uses current summation in place of the expensive digital dot-product computation required in neural predictors. We show that the analog predictor can outperform digital neural predictors because of the reduced cost, in power and latency, of the key computations. The SNAP circuit is able to produce an accuracy nearly equivalent to an infeasible digital neural predictor that requires 128 additions per prediction. The analog version, however, can run at 3GHz, with the analog portion of the prediction computation requiring approximately 7 milliwatts at a 45nm technology, which is small compared to the power required for the table lookups in this and conventional predictors.","Analog computers,
Circuits,
Delay,
Accuracy,
Prediction algorithms,
Wire,
Computer science,
Transistors,
Computer errors,
High performance computing"
Trident: Scientific Workflow Workbench for Oceanography,"We introduce Trident, a scientific workflow workbench that is built on top of a commercial workflow system to leverage existing functionality. Trident is being developed in collaboration with the scientific community for oceanography, but the workbench itself can be used for any science project for scientific workflow.","workflow management software,
geophysics computing,
oceanography"
A Certificateless Signature Scheme for Mobile Wireless Cyber-Physical Systems,"Due to the unique characteristics of Cyber-Physical Systems (CPS) such as interaction with the physical world, many new research challenges arise. Many CPS applications will be implemented on computing devices using mobile ad hoc networks (MANETs). Before these systems can be used in multifarious environments, the security properties of these networks must be fully understood. Recently, several secure signature schemes for MANETs have been proposed based on public key cryptography and identity-based cryptography. In order to solve some problems in these schemes, such as the costly and complex key management problem in traditional public key cryptography and the “key escrow” problem in identity-based cryptography, the notion of certificateless public key cryptography was introduced. In this paper, we propose an efficient certificateless signature scheme for mobile wireless cyberphysical systems (McCLS) based on the bilinear Diffie-Hellman assumption. Empirical studies are conducted using QualNet to evaluate the effectiveness and efficiency of McCLS scheme under two most common attacks, i.e. black hole attack and rushing attack. Results show that McCLS scheme is more efficient than existing solutions and is able to resist these two kinds of attacks.","wireless sensor networks,
ad hoc networks,
digital signatures,
public key cryptography"
"Reversible Realization of Quaternary Decoder, Multiplexer, and Demultiplexer Circuits","Quaternary logic is very suitable for encoded realization of binary logic functions by grouping 2-bits together into quaternary digits. This sort of quaternary encoded reversible realization of binary logic function requires half times input/output lines than the original binary reversible realization. Quaternary decoder, multiplexer, and demultiplexer are very important building blocks of quaternary digital systems. In this paper, we show reversible realization of these circuits using quaternary reversible gates like quaternary shift gates (QSG), quaternary controlled shift gates (QCSG), and quaternary Toffoli gates (QTG). We also show the realization of multi-digit QCSG and QTG using QSG and QCSG, which are realizable using liquid ion-trap quantum technology and other reversible technologies.","Decoding,
Multiplexing,
Logic functions,
Galois fields,
Circuit synthesis,
Digital systems,
Control system synthesis,
Multivalued logic,
Computer science,
DH-HEMTs"
On the anonymity of Chaum mixes,"The information-theoretic analysis of Chaum mixing under latency constraints is considered. Mixes are relay nodes that collect packets from multiple users and modify packet timings to prevent an eavesdropper from identifying the sources of outgoing packets. In this work, an entropy-based metric of anonymity is proposed to quantify the performance of a mixing strategy under strict delay constraints. Inner and outer bounds on the maximum achievable anonymity are characterized as functions of traffic load and the delay constraint. The bounds are shown to have identical first derivatives at low traffic loads.","Delay,
Timing,
Color,
Telecommunication traffic,
Measurement uncertainty,
Uncertainty,
Upper bound"
Approximate Joins for Data-Centric XML,"In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strategies are based on some ordered tree matching technique. But in data-centric XML the order is irrelevant: two elements should match even if their subelement order varies. In this paper we give a solution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams in a three-step process: sorting the unordered tree, extending the sorted tree with dummy nodes, and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the respective unordered trees. The approximate join algorithm based on windowed pq-grams is implemented as an equality join on strings which avoids the costly computation of the distance between every pair of input trees. Our experiments with synthetic and real world data confirm the analytic results and suggest that our technique is both useful and scalable.","XML,
Sorting,
Computer science,
Application software,
Data analysis,
Internet,
Polynomials,
Shape,
Partitioning algorithms"
Quantitative Assessment of Enterprise Security System,In this paper we extend a model-based approach to security management with concepts and methods that provide a possibility for quantitative assessments. For this purpose we introduce security metrics and explain how they are aggregated using the underlying model as a frame. We measure numbers of attack of certain threats and estimate their likelihood of propagation along the dependencies in the underlying model. Using this approach we can identify which threats have the strongest impact on business security objectives and how various security controls might differ with regard to their effect in reducing these threats.,"Information security,
Application software,
Computer security,
Availability,
Reliability engineering,
Computer science,
Information technology,
Technology management,
Bridges,
Collaboration"
Complex Network Thinking in Software Engineering,"The booming of Internet has brought new challenges to software engineering. In the network age, software industry has undergone a transition from manufacturing industry to the service trade, software doesn’t have clear hierarchy structure, specific life cycle and definite system border any longer. On the contrary, both software itself and the environment it works in are all networked. In this paper, we explore an idea of software engineering from the perspective of complex networks. First, we present a skeletal description to abstract software structure into network topology. Evidence shows that software systems have the characteristics of small world, scale-free and high clustering. Then we point out three aspects of the influence to software engineering based on the characteristics of complex network. Finally, the most promising development trend of software engineering in the future is predicted.",
Financial Monte Carlo simulation on architecturally diverse systems,"Computational finance relies heavily on the use of Monte Carlo simulation techniques. However, Monte Carlo simulation is computationally very demanding. We demonstrate the use of architecturally diverse systems to accelerate the performance of these simulations, exploiting both graphics processing units and field-programmable gate arrays. Performance results include a speedup of 74× relative to an 8 core multiprocessor system (180× relative to a single processor core).","Portfolios,
Reactive power,
Acceleration,
Field programmable gate arrays,
Instruments,
Computational modeling,
Finance,
Computer graphics,
Coprocessors,
Pricing"
Facial feature tracking and expression recognition for sign language,"Expressions carry vital information in sign language. In this study, we have implemented a multi-resolution active shape model (MR-ASM) tracker, which tracks 116 facial landmarks on videos. Since the expressions involve significant amount of head rotation, we employ multiple ASM models to deal with different poses. The tracked landmark points are used to extract motion features which are used by a support vector machine (SVM) based classifier. We obtained above 90% classification accuracy in a data set containing 7 expressions.","Facial features,
Face recognition,
Handicapped aids,
Support vector machines,
Support vector machine classification,
Active shape model,
Videos,
Magnetic heads,
Tracking,
Data mining"
Performance adaptive UDP for high-speed bulk data transfer over dedicated links,"New types of networks are emerging for the purpose of transmitting large amounts of scientific data among research institutions quickly and reliably. These exotic networks are characterized by being high-bandwidth, high-latency, and free from congestion. In this environment, TCP ceases to be an appropriate protocol for reliable bulk data transfer because it fails to saturate link throughput. Of the new protocols designed to take advantage of these networks, a subclass has emerged using UDP for data transfer and TCP for control. These high-speed variants of reliable UDP, however, tend to underperform on all but high-end systems due to constraints of the CPU, network, and hard disk. It is therefore necessary to build a high-speed protocol adaptive to the performance of each system. This paper develops such a protocol, Performance Adaptive UDP (henceforth PA-UDP), which aims to dynamically and autonomously maximize performance under different systems. A mathematical model and related algorithms are proposed to describe the theoretical basis behind effective buffer and CPU management. Based on this model, we implemented a prototype under Linux and the experimental results demonstrate that PA-UDP outperforms an existing high-speed protocol on commodity hardware in terms of throughput and packet loss. PAUDP is efficient not only for high-speed research networks but also for reliable high-performance bulk data transfer over dedicated local area networks where congestion and fairness are typically not a concern.","Protocols,
Bandwidth,
Throughput,
Delay,
Mathematical model,
Communication system control,
Optical fiber networks,
Equations,
TCPIP,
Helium"
Scientific Workflow Provenance Querying with Security Views,"Provenance, the metadata that pertains to the derivation history of a data product, has become increasingly important in scientific workflow environments. In many cases, both data products and their provenance can be sensitive and effective access control mechanisms are essential to protect their confidentiality. In this paper, we propose i) a formalization of scientific workflow provenance as the basis for querying and access control; ii) a security specification mechanism for provenance at various granularity levels and the derivation of a full security specification based on inheritance, overriding, and conflict resolution rules; iii) a formalization of security views that are derived from a scientific workflow run provenance for different roles of users; and iv) a framework that integrates abstraction views and security views such that a user can examine provenance at different abstraction levels while respecting the security policy prescribed for her. We have developed the SecProv prototype to validate the effectiveness of our approach.","Security,
Access control,
Business,
Data models,
IP networks,
DNA,
XML"
A Weight-Based Dynamic Replica Replacement Strategy in Data Grids,"Data grid is a very important and useful technique to process the large number of data produced by scientific experiments and simulations. However, the high latency of the Internet turns to be the bottleneck in accessing the files in the grid. The replication strategy can shorten the time of getting the files by creating many replicas and storing the replicas in appropriate locations. Restricted by the storage capacity, it is important to design an effective algorithm for the replication replacement task. In this paper, we propose a novel replacement strategy which calculates the weight of replicas based on the access times in the future time window, the access bandwidth and the file size. Results from the simulation procedure show the better performance of our algorithm than former ones.","Computer science,
Grid computing,
Computational modeling,
Delay,
Wide area networks,
Computer simulation,
Internet,
Algorithm design and analysis,
Bandwidth,
Technology management"
A Meaningful Learning Based u-Learning Evaluation Model,"In recent years, there has been a dramatic proliferation of research concerned with the ubiquitous learning (u-learning). The u-learning systems have to be continuously evaluated and improved for ensuring the system reliability. Therefore, this work based on meaningful learning aspect to propose a u-learning evaluation model. The model blends features of u-learning and meaningful learning to construct a hierarchy decision model. According to the hierarchy structure, domain experts can develop AHP-based questionnaire survey to collect learners’ opinions. Following that, system developers can realize the relative strength and weakness of the u-learning system from a meaningful learning viewpoint by analyzing the surveyed data, and they can further to improve and refine current u-learning systems accordingly. Consequently, existing u-learning systems can be revalidated by our evaluation model, and then based on the produced suggestions to improve toward the meaningful learning.","Ubiquitous computing,
Performance evaluation,
Computer science,
Reliability engineering,
Data analysis,
Educational technology,
Context-aware services,
Problem-solving,
History,
Electronic learning"
Distribution feeder one-line diagrams automatic generation from geographic diagrams based on GIS,"Geographic Information Systems (GIS) are computer-based systems designed to support the capture, management, manipulation, analysis, and modeling of spatially referenced data. At present, GIS has been extensively used in the energy management system and provides visualized platform to electrical distribution system analysis, design and operation, that is, AM/FM/GIS. One-line diagrams are widely used to represent the electrical distribution system current state and its facilities connectivity relationship. Once AM/FM/GIS integrate with SCADA, the real time operational data obtained, one-line diagrams could represent electrical distribution system dynamic topological relationship. In the past, one-line diagrams was produced by means of manual working using CAD or more original analogy circuit board, it is inefficient and easily error. Therefore it’s significant to implement automatic generation one-line diagram, it can reduce the human errors and the huge effort in a manual generation technique. This paper presents a methodology to automatically build these diagrams based on the usage of geospatial data from AM/FM/GIS. All electrical distribution system facilities including the network buses, branches, switches and transformers were abstracted into edges and nodes. With the aid of the routing methodology in the large scale integration, an intelligent routing algorithm for the power system graph is presented. The intelligent routing algorithm permits the elimination of intersection and overlap of buses and other facilities. The approach was successfully demonstrated with some city district electrical distribution system.","Geographic Information Systems,
Routing,
Information analysis,
Energy management,
Data visualization,
Real time systems,
Printed circuits,
Humans,
Manuals,
Switches"
Quantified Synthesis of Reversible Logic,"In the last years synthesis of reversible logic functions has emerged as an important research area. Other fields such as low-power design, optical computing and quantum computing benefit directly from achieved improvements. Recently, several approaches for exact synthesis of Toffoli networks have been proposed. They all use Boolean satisfiability to solve the underlying synthesis problem. In this paper a new exact synthesis approach based on Quantified Boolean Formula (QBF) satisfiability - a generalization of Boolean satisfiability - is presented. Besides the application of QBF solvers, we propose Binary Decision Diagrams to solve the quantified problem formulation. This allows to easily support different gate libraries during synthesis. In addition, all minimal networks are found in a single step and the best one with respect to quantum costs can be chosen. Experimental results confirm that the new technique is faster than the best previously known approach and leads to cheaper realizations in terms of quantum costs.",
A Cloaking Algorithm Based on Spatial Networks for Location Privacy,"Most of research efforts have elaborated on k-anonymity for location privacy. The general architecture for implementing k-anonymity is that there is one trusted server (referred to as location anonymizer) responsible for cloaking at least k users' locations for protecting location privacy. A location anonymizer will generate cloaked regions in which there are at least k users for query processing. Prior works only explore grid shape cloaked regions. However, grid shape cloaked regions result in a considerable amount of query results, thereby increasing the overhead of filtering unwanted query results. In this paper, we propose a cloaking algorithm in which cloaked regions are generated according to the features of spatial networks. By exploring the features of spatial networks, the cloaked regions are very efficient for reducing query results and improving cache utilization of mobile devices. Furthermore, an index structure for spatial networks is built and in light of the proposed index structure, we develop a Spatial-Temporal Connective Cloaking algorithm (abbreviated as STCC). A simulator is implemented and extensive experiments are conducted. Experimental results show that our proposed algorithm outperforms prior cloaking algorithms in terms of the candidate query results and the cache utilization.",
Ankle-Foot-Orthosis Control in Inclinations and Stairs,"A control procedure is proposed for an ankle-foot-orthosis (AFO) for different gait situations, such as inclinations and stairs. This paper presents a novel AFO control of the ankle angle. A magneto-rheological damper was used to achieve ankle damping during foot down and locking at swing, thereby avoiding foot slap as well as foot drop. The controller used feedback from the ankle angle only. Still it was capable of not only adjusting damping within a gait step but also changing control behavior depending on level walking, ascending and descending stairs. As a consequence, toe strike was possible in stair gait as opposed to heel strike in level walking. Tests verified the expected behavior in stair gait and in level walking where gait speed and ground inclinations varied. The self-adjusted AFO is believed to improve gait comfort in slopes and stairs.",
Automatic Microarray Spot Segmentation Using a Snake-Fisher Model,"Inspired by Paragious and Deriche's work, which unifies boundary-based and region-based image partition approaches, we integrate the snake model and the Fisher criterion to capture, respectively, the boundary information and region information of microarray images. We then use the proposed algorithm to segment the spots in the microarray images, and compare our results with those obtained by commercial software. Our algorithm is automatic because the parameters are adaptively estimated from the data without human intervention.","Image segmentation,
Probes,
Partitioning algorithms,
Software algorithms,
Genomics,
Bioinformatics,
Biomedical imaging,
Gene expression,
Fluorescence,
Information science"
p-Sensitivity: A Semantic Privacy-Protection Model for Location-based Services,"Several methods have been proposed to support location-based services without revealing mobile users' privacy information. There are two types of privacy concerns in location-based services: location privacy and query privacy. Existing work, based on location k-anonymity, mainly focused on location privacy and are insufficient to protect query privacy. In particular, due to lack of semantics, location k-anonymity suffers from query homogeneity attack. In this paper, we introduce p-sensitivity, a novel privacy-protection model that considers query diversity and semantic information in anonymizing user locations. We propose a PE-Tree for implementing the p-sensitivity model. Search algorithms and heuristics are developed to efficiently find the optimal p-sensitivity anonymization in the tree. Preliminary experiments show that p-sensitivity provides high-quality services without compromising users' query privacy.","Privacy,
Protection,
Road transportation,
Computer science,
Mobile computing,
Heuristic algorithms,
Wireless communication"
A cluster head decision system for sensor networks using fuzzy logic and number of neighbor nodes,"Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network’s communication energy dissipation. However, the selection of cluster head is not easy in different environments which may have different characteristics. In this paper, in order to deal with this problem we propose a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbor nodes. We evaluate the proposed system by simulations and show that proposed system makes a good selection of the cluster head.","Sensor systems,
Fuzzy logic,
Electronic mail,
Sensor phenomena and characterization,
Energy dissipation,
Wireless sensor networks,
Sensor systems and applications,
Computer networks,
Informatics,
Clustering algorithms"
A fast approximate joint diagonalization algorithm using a criterion with a block diagonal weight matrix,"We propose a new algorithm for Approximate Joint Diagonalization (AJD) with two main advantages over existing state-of-the-art algorithms: Improved overall running speed, especially in large-scale (high-dimensional) problems; and an ability to incorporate specially structured weight-matrices into the AJD criterion. The algorithm is based on approximate Gauss iterations for successive reduction of a weighted Least Squares off-diagonality criterion. The proposed Matlab® implementation allows AJD of ten 100 × 100 matrices in 3–4 seconds (for the unweighted case) on a common PC (Pentium M, 1.86GHz, 2GB RAM), generally 3–5 times faster than the fastest competitor. The ability to incorporate weights allows fast large-scale realization of optimized versions of classical blind source separation algorithms, such as Second-Order Blind Identification (SOBI), whose weighted version (WASOBI) yields significantly improved separation performance.","Covariance matrix,
Source separation,
Large-scale systems,
Blind source separation,
Information theory,
Automation,
Gaussian approximation,
Computer languages,
Constraint optimization,
Yield estimation"
Communication network design using Particle Swarm Optimization,Particle Swarm Optimization is applied on an instance of single and mufti criteria network design problem. The primary goal of this study is to present the efficiency of a simple hybrid particle swarm optimization algorithm on the design of a network infrastructure including decisions concerning the locations and sizes of links. A complementary goal is to also address Quality of Service issues in the design process. Optimization objectives in this case are the network layout cost and the average packet delay in the network. Therefore a multi-objective instance of the hybrid PSO algorithm is applied. The particular hybrid PSO includes mutation to avoid premature convergence. For the same reason repulsion/attraction mechanisms are also applied on the single objective case. Mutation is passed on to the mufti-objective instance of the algorithm. Obtained results are compared with corresponding evolutionary approaches.,"Communication networks,
Particle swarm optimization"
A new word sense similarity measure in wordnet,"Recognizing similarities between words is a basic element of computational linguistics and artificial intelligence applications. This paper presents a new approach for measuring semantic similarity between words via concepts. Our proposed measure is a hybrid system based on using a new Information content metric and edge counting-based tuning function. In proposed system, hierarchical structure is used to present information content instead of text corpus and our result will be improved by edge counting-based tuning function. The result of the system is evaluated against human similarity ratings demonstration and shows significant improvement in compare with traditional similarity measures.","Computational linguistics,
Artificial intelligence,
Humans"
An Approach to Deep Web Crawling by Sampling,"Crawling deep web is the process of collecting data from search interfaces by issuing queries. With wide availability of programmable interface encoded in web services, deep web crawling has received a large variety of applications. One of the major challenges crawling deep web is the selection of the queries so that most of the data can be retrieved at a low cost. We propose a general method in this regard. In order to minimize the duplicates retrieved, we reduced the problem of selecting an optimal set of queries from a sample of the data source into the well-known set-covering problem and adopt a classical algorithm to resolve it. To verify that the queries selected from a sample also produce a good result for the entire data source, we carried out a set of experiments on large corpora including Wikipedia and Reuters. We show that our sampling-based method is effective by empirically proving that 1) The queries selected from samples can harvest most of the data in the original database; 2) The queries with low overlapping rate in samples will also result in a low overlapping rate in the original database; and 3) The size of the sample and the size of the terms from where to select the queries do not need to be very large.","Sampling methods,
Databases,
Uniform resource locators,
Web services,
Costs,
Data mining,
Intelligent agent,
Computer science,
Information retrieval,
Telecommunication traffic"
Rule-Based WiFi Localization Methods,"The rule-based localization methods proposed in this paper are based on two important observations. First, although the absolute RSS values change with time, the relative RSS (RRSS) values between several Access Points (APs) are more stable than the absolute RSSs. Thus, we can use RRSSs as rules for inferring a client's location. Second, when a unique location cannot be obtained based on RRSS rules, the localization process can backtrack to the previous observed client location. By analyzing the accessible paths on the floor plan, locations that are not reacheable from the previous location can be disqualified. Based on these two key observations, we propose several localization methods, implement them in a life environment and conduct extensive experiments to measure the localization accuracy of the proposed methods.  We found that our methods achieve much higher accuracy than the state-of-the-art localization methods, namely, RADAR, LOCADIO and WHAM!.",
A new design of wearable token system for mobile device security,"Increasingly, today's mobile devices provide users a high level of convenience and flexibility. But, mobile devices are vulnerable to theft and loss, greatly increasing the likelihood of exposing sensitive data. To address this problem, Corner and Noble gave the concept of transient authentication and built a wearable token system, called ZIA system. In this paper, we design a new wearable token system using the idea of transient authentication. Our system not only preserves the same security characteristics as ZIA system, but also is more effective than ZIA system. In our system, careful key management and prudent communication mechanism allow significant performance enhancements. Due to the power constraint of the wearable token and the wireless link, the lower the costs are, the greater the chance of success the wearable token system has in practical implementation.",
Multicast Routing in Light-Trail WDM Networks,"Recently, light-trail is becoming an appealing architecture for WDM networks which have been considered as promising backbone of the next generation network. Light-trail can inherently support multicast given its bus nature. In this paper, we study how to use the minimum number of light- trails to form a multicast tree for supporting the given multicast session. The problem for general light-trail WDM networks is proved to be NP-hard. Two auxiliary graphs will be proposed to transform the problem into minimum steiner tree problem that many effective algorithms can be applied. We then show that the same problem in light-trail WDM ring networks can be solved in polynomial time. The simulations show the effectiveness of our work.","Routing,
WDM networks,
Optical packet switching,
Optical buffering,
Optical receivers,
Optical transmitters,
High speed optical techniques,
Wavelength division multiplexing,
Bandwidth,
Computer science"
Radiometric calibration with illumination change for outdoor scene analysis,"The images of an outdoor scene collected over time are valuable in studying the scene appearance variation which can lead to novel applications and help enhance existing methods that were constrained to controlled environments. However, the images do not reflect the true appearance of the scene in many cases due to the radiometric properties of the camera : the radiometric response function and the changing exposure. We introduce a new algorithm to compute the radiometric response function and the exposure of images given a sequence of images of a static outdoor scene where the illumination is changing. We use groups of pixels with constant behaviors towards the illumination change for the response estimation and introduce a sinusoidal lighting variation model representing the daily motion of the sun to compute the exposures.","Radiometry,
Calibration,
Lighting,
Image analysis,
Layout,
Cameras,
Computer vision,
Application software,
Sun,
Pixel"
Practical application of 1588 security,"Version 2 of IEEE 1588 contains an extension to secure the given service of clock synchronization. This article describes a practical implementation of this extension for a clock synchronization network. Pitfalls also relevant to other implementations and important properties are discussed and performance results that cover normal operation and stress tests such as denial of service attacks are presented. Additionally, important concepts for integration of hardware timestamping and transparent clock support are presented, which are necessary to use the full potential in more complicated applications.","Security,
Synchronization,
Radiation detectors,
Authentication,
Clocks,
Delay,
Hardware"
Automatic building detection in aerial and satellite images,"Automatic creation of 3D urban city maps could be an innovative way for providing geometric data for varieties of applications such as civilian emergency situations, natural disaster management, military situations, and urban planning. Reliable and consistent extraction of quantitative information from remotely sensed imagery is crucial to the success of any of the above applications. This paper describes the development of an automated roof detection system from single monocular electro-optic satellite imagery. The system employs a fresh approach in which each input image is segmented at several levels. The border line definition of such segments combined with line segments detected on the original image are used to generate a set of quadrilateral rooftop hypotheses. For each hypothesis a probability score is computed that represents the evidence of true building according to the image gradient field and line segment definitions. The presented results demonstrate that the system is capable of detecting small gabled residential rooftops with variant light reflection properties with high positional accuracies.","Satellites,
Image segmentation,
Buildings,
Data mining,
Shape,
Robotics and automation,
Cities and towns,
Urban planning,
Image processing,
Image reconstruction"
Localized random access scan: Towards low area and routing overhead,"Conventional random access scan (RAS) designs, although economic in test power dissipation, test application time and test data volume, are expensive in area and routing overhead. In this paper, we present a localized RAS architecture (LRAS) to address this issue. A novel scan cell structure, which has fewer transistors than the multiplexer-type scan cell, is proposed to eliminate the global test enable signal and to localize the row enable and the column enable signals. Experimental results on ISCAS’89 and ITC’99 benchmark circuits demonstrate that LRAS has 54% less area overhead than multiplexer-type scan chain based designs, while significantly outperforms the state-of-the-art RAS scheme in routing overhead.","Routing,
Flip-flops,
Circuit testing,
Power dissipation,
Read-write memory,
Wires,
Random access memory,
Circuit synthesis,
Signal synthesis,
Design for testability"
Reduction Techniques for Model Checking Markov Decision Processes,"The quantitative analysis of a randomized system, modeled by a Markov decision process, against an LTL formula can be performed by a combination of graph algorithms, automata-theoretic concepts and numerical methods to compute maximal or minimal reachability probabilities. In this paper, we present various reduction techniques that serve to improve the performance of the quantitative analysis, and report on their implementation on the top of the probabilistic model checker \LiQuor. Although our techniques are purely heuristic and cannot improve the worst-case time complexity of standard algorithms for the quantitative analysis, a series of examples illustrates that the proposed methods can yield a major speed-up.","Probabilistic logic,
Algorithm design and analysis,
Analytical models,
Computational modeling,
Titanium,
Reactive power,
Context modeling"
Real-time signal processing of accelerometer data for wearable medical patient monitoring devices,"Elderly and other people who live at home but required some physical assistance to do so are often more susceptible injury causing falls in and around their place of residence. In the event that a fall does occur, as a direct result of a previous medical condition or the fall itself, these people are typically less likely to be able to seek timely medical help without assistance. The goal of this research is to develop a wearable sensor device that uses an accelerometer for monitoring the movement of the person to detect falls after they have occurred in order to enable timely medical assistance. The data coming from the accelerometer is processed in real-time in the device and sent to a remote monitoring station where operators can attempt to make contact with the person and/or notify medical personnel of the situation. The ADXL330 accelerometer is contained within a Nintendo WiiMote controller, which forms the basis of the wearable medical sensor. The accelerometer data can then be sent via Bluetooth connection and processed by a local gateway processor. If a fall is detected, the gateway will then contact a remote monitoring station, on a cellular network, for example, via satellite, and/or through a hardwired phone or Internet connection. To detect the occurrence of ta fall, the accelerometer data is passed through a matched filter and the data is compared to benchmark analysis data that will define the conditions that represents the occurrence of a fall.",
Neighbor discovery in wireless ad hoc networks based on group testing,"Fast and efficient discovery of all neighboring nodes by a node new to a neighborhood is critical to the deployment of wireless ad hoc networks. Different than the conventional ALOHA-type random access discovery schemes, this paper assumes that all nodes in the neighborhood simultaneously send their unique on-off signatures known to the receive node. In particular, a transmitter does not transmit any energy during an “off” mini-slot in its signature sequence. The received signal can be viewed as the outcome of a sequence of tests over the mini-slots, where the outcome of a test is positive if there is energy at the corresponding mini-slot from at least one neighbor, and negative if none of the neighboring nodes transmits energy during the mini-slot. The neighbor discovery problem is thus equivalent to a classical group testing problem. Two practical and scalable detection algorithms are developed from the group testing viewpoint. Unlike some previous neighbor discovery schemes using coherent multiuser detection, which are difficult to implement due to lack of training, the proposed scheme requires only non-coherent energy detection. The proposed algorithms are shown to achieve faster and more reliable discovery than existing random access schemes.","Mobile ad hoc networks,
Centralized control,
Transmitters,
Ad hoc networks,
Multiuser detection,
Multiaccess communication,
Switches,
System testing,
Optimal control,
Computer science"
How to track spermatozoa using high-speed visual feedback,"In this paper, we report how to track quickly and vigorously swimming ascidian spermatozoa using high-speed visual feedback at a frame rate of 1 kHz. Ascidian spermatozoa swim as fast as 300 μm/s by rotating their flagella 50 times/s. This vigorous swimming style has prevented stable image observation and made it difficult to track them reliably with our previously developed visual tracking system. Here, we describe how we overcame these problems using image processing techniques to achieve stable tracking of fast, small ascidian spermatozoa for more than 180 s using high-speed visual feedback.",
iFill: An Impact-Oriented X-Filling Method for Shift- and Capture-Power Reduction in At-Speed Scan-Based Testing,"In scan-based tests, power consumptions in both shift and capture phases may be significantly higher than that in normal mode, which threatens circuits' reliability during manufacturing test. In this paper, by analyzing the impact of X-bits on circuit switching activities, we present an X-filling technique that can decrease both shift- and capture-power to guarantees the reliability of scan tests, called iFill. Moreover, different from prior work on X-filling for shift-power reduction which can only reduce shift-in power, iFill is able to decrease power consumptions during both shift-in and shift-out. Experimental results on ISCAS'89 benchmark circuits show the effectiveness of the proposed technique.","Circuit testing,
Energy consumption,
Power dissipation,
Benchmark testing,
System testing,
Laboratories,
Power engineering computing,
Computer architecture,
Content addressable storage,
Computer science"
Virtual Classroom Extension for Effective Distance Education,"Key requirements of effective distance learning are interactivity among participants and the student's sense of presence in the classroom. This system meets those requirements, letting the instructor perceive remote students' body language and facial expressions as they listen and speak, and letting remote students participate in the on-campus classroom.","Distance learning,
Hardware,
Computer graphics,
Costs,
Bandwidth,
DSL,
Modems,
Educational institutions,
Computer science,
Educational technology"
Validating the Performance of Haptic Motor Skill Training,"The effect of haptic interfaces on motor skill training has been widely studied. However, relatively little is known about whether haptic training can promote long-term motor skill acquisition. In this paper, we report two experimental studies that investigated the effectiveness of visuohaptic (visual + haptic) interfaces in helping people develop short-term and long-term motor skills. Our first study compared training outcomes of visuohaptic training, visual training, and no-assistance training. We found that the training outcomes for the tested methods were similar when helping participants develop short-term motor skills. Our second experiment assessed the potential of visual training and visuohaptic training in promoting the development of long-term motor skills. Participants were trained during a four-day-long period. The results showed that the participants gained long-term skills through both training methods, and that the training outcomes for both methods were similar. The results also showed that visuohaptic training is a promising method, but that it needs to be further developed to be useful.","Haptic interfaces,
Shape measurement,
Auditory displays,
Imaging phantoms,
Learning systems,
Brushes,
Force measurement,
Motion measurement,
Testing,
Information processing"
Modulation classification of QAM and PSK from their constellation using Genetic Algorithm and hierarchical clustering,"Most of the approaches for recognition and classification of modulation have been founded on modulated signal's components. However, one of the best methods of modulation classification is the use of the constellation diagram of the received signal. In this paper, modulation classification for PSK and QAM is performed by Genetic Algorithm followed by hierarchical clustering algorithm, considering the constellation of the received signal. In addition this classification finds the decision boundary of the signal which is a critical information for bit detection. The simulation shows high capability of this method for recognition of modulation levels in the presence of the noise.","Quadrature amplitude modulation,
Phase shift keying,
Genetic algorithms,
Constellation diagram,
Pattern recognition,
Clustering algorithms,
Signal processing,
Biological cells,
Genetic engineering,
Noise level"
High-speed and low-power multipliers using the Baugh-Wooley algorithm and HPM reduction tree,"The modified-Booth algorithm is extensively used for high-speed multiplier circuits. Once, when array multipliers were used, the reduced number of generated partial products significantly improved multiplier performance. In designs based on reduction trees with logarithmic logic depth, however, the reduced number of partial products has a limited impact on overall performance. The Baugh-Wooley algorithm is a different scheme for signed multiplication, but is not so widely adopted because it may be complicated to deploy on irregular reduction trees. We use the Baugh-Wooley algorithm in our High Performance Multiplier (HPM) tree, which combines a regular layout with a logarithmic logic depth. We show for a range of operator bit-widths that, when implemented in 130-nm and 65-nm process technologies, the Baugh-Wooley multipliers exhibit comparable delay, less power dissipation and smaller area foot-print than modified-Booth multipliers.","Circuits,
Power dissipation,
Delay,
Energy efficiency,
Adders,
Computer science,
Logic design,
Neodymium,
Encoding,
Differential equations"
A Parallel Algorithm for Closed Cube Computation,"Closed cubing is a very efficient algorithm for data cube compression proposed recently in the literature. It losslessly condenses a group of cells into one cell if these cells have the same aggregate value and preserve roll-up/drill-down semantics. Despite its importance, parallel closed cubing solutions for huge data sets are not well studied so far to the best of the authors’ knowledge. This paper presents a parallel closed cube construction and query algorithm over low cost PC clusters using the MapReduce framework. In addition, we proved that with the number of data blocks increases, the closed cubes’ storage size decreases gradually. Thus users can specify the number of data blocks to balance the performance between cubes storage and query time. Experimental study demonstrates that our algorithm is efficient and scalable.","Parallel algorithms,
Concurrent computing,
Aggregates,
Clustering algorithms,
Costs,
Partitioning algorithms,
Information science,
Computer science,
Data engineering,
Upper bound"
A video-based face detection and recognition system using cascade face verification modules,"Face detection and recognition in a video is a challenging research topic as overall processes must be done timely and efficiently. In this paper, a novel face detection and recognition system using three fast cascade face verification modules and an ensemble classifier is presented. Firstly, the head of a tester is serially verified by our proposed three verification modules: face skin verification module, face symmetry verification module, and eye template verification module. The three verification modules can eliminate the tilted faces, the backs of the head, and any other non-face moving objects in the video. Only the frontal face images are sent to face recognition engine. The frontal face detection reliability can be adjusted by simply setting the verification thresholds in the verification modules. Secondly, three hybrid feature sets are applied to face recognition. An ensemble classifier scheme is proposed to congregate three individual Artificial Neural Network (ANN) classifiers trained by the three hybrid feature sets. Experiments demonstrated that the frontal face detection rate can be achieved as high as 95% in the low quality video images. The overall face recognition rate and reliability are increased at the same time using the proposed ensemble classifier in the system.","Face detection,
Face recognition,
Skin,
Artificial neural networks,
Image recognition,
Head,
Engines,
Feature extraction,
Principal component analysis,
Linear discriminant analysis"
A comparative study of anomaly detection algorithms for detection of SIP flooding in IMS,"The IP multimedia subsystem (IMS) framework uses session initiation protocol (SIP) for signaling and control of sessions. In this paper, we first demonstrate that SIP flooding attacks on IMS can result in denial of service to the legitimate users. Afterwards, we report our comparative study of three well-known anomaly detection algorithms, adaptive threshold, cumulative sum, and Hellinger distance for detection of flood attacks in IMS. We evaluate the accuracy of the algorithms using a comprehensive traffic dataset that consists of varying benign and malicious traffic patterns.","Detection algorithms,
Floods,
Protocols,
Web server,
Computer crime,
Telecommunication traffic,
Internet telephony,
Computer hacking,
TCPIP,
Next generation networking"
OPERA: Opportunistic packet relaying in disconnected Vehicular Ad Hoc Networks,Vehicular Ad Hoc Networks (VANET) have recently received increasing attention in the media. And with good reason: VANET promises to integrate driving into a ubiquitous and pervasive network that will redefine the way we live and work.,"Vehicles,
Protocols,
Routing protocols,
Routing,
Road transportation,
Mobile ad hoc networks,
Distance measurement"
Numerical analysis of dielectric lens antennas using a ray-tracing method and HFSS software,"In this paper, we use for the first time the ray-tracing method combined with the HFSS software to analyze lens antennas. HFSS is a very popular commercial tool that can provide very accurate results for the simulation of antennas. However, because of limitations on computational resources, it is hard to apply to solving large electromagnetic problems. In this paper, HFSS is used to simulate the fields of the feed part of a lens antenna. The ray tracing method is adopted for the lens part. This treatment makes the combined method feasible for solving large lens antennas. In using the ray-tracing method, cubicspline interpolation is exploited to fit the face of the lens and to solve for the derivatives at each point on the curved surface. Newton's method is applied to find the intercept between the ray and the curved surface. The numerical method proposed in this paper is very suitable for the analysis of dielectric lens antennas with arbitrary feeds and lens shapes. The validity of the idea is demonstrated by comparing the simulation results of this new method with those from using CST (Computer Simulation Technology) software. Very good agreement is achieved.","Numerical analysis,
Dielectrics,
Lenses,
Ray tracing,
Computational modeling,
Antenna feeds,
Surface fitting,
Computer simulation,
Interpolation,
Surface treatment"
Interactive image matting for multiple layers,"Image matting deals with finding the probability that each pixel in an image belongs to a user specified ‘object’ or to the remaining ‘background’. Most existing methods estimate the mattes for two groups only. Moreover, most of these methods estimate the mattes with a particular bias towards the object and hence the resulting mattes do not sum up to 1 across the different groups. In this work, we propose a general framework to estimate the alpha mattes for multiple image layers. The mattes are estimated as the solution to the Dirichlet problem on a combinatorial graph with boundary conditions. We consider the constrained optimization problem that enforces the alpha mattes to take values in [0; 1] and sum up to 1 at each pixel. We also analyze the properties of the solution obtained by relaxing either of the two constraints. Experiments demonstrate that our proposed method can be used to extract accurate mattes of multiple objects with little user interaction.",
A high-power-LED driver with power-efficient LED-current sensing circuit,"For lighting application, high-power LED nowadays is driven at 350mA and a sensing resistor is used to provide feedback for LED-current regulation. This method adds an IR drop at the output branch, and limits power efficiency as LED-current is large and keeps increasing. In this paper, a power-efficient LED-current sensing circuit is proposed. The circuit does not use any sensing resistor but extracts LED-current information from the output capacitor of the driver. The sensing circuit is implemented in a Buck-boost LED driver and has been fabricated in AMS 0.35μm CMOS technology. Measurement results indicate a power-conversion efficiency of 92% and at least 90 times power reduction in sensing compared to existing approach.","Driver circuits,
Light emitting diodes,
Resistors,
Voltage,
Feedback,
Power dissipation,
LED lamps,
CMOS technology,
Cameras,
Switches"
Towards a Defect Prevention Based Process Improvement Approach,"Defect causal analysis (DCA) is a means of product focused software process improvement. A systematic literature review to identify the DCA state of the art has been undertaken. The systematic review gathered unbiased knowledge and evidence and identified opportunities for further investigation. Moreover, some guidance on how to efficiently implement DCA in software organizations could be elaborated. This paper describes the initial concept of the DBPI (Defect Based Process Improvement) approach. It represents a DCA based approach for process improvement, designed considering the results of the systematic review and the obtained guidance. Its main contributions are tailoring support for DCA based process improvement and addressing an identified opportunity for further investigation by integrating organizational learning mechanisms regarding cause-effect relations into the conduct of DCA.","Software engineering,
Application software,
Process design,
Learning systems,
ISO standards,
IEC standards,
Six sigma,
Context,
Computer science,
Proposals"
A performance evaluation of filter design and coding schemes for palmprint recognition,"Palmprint recognition, as one of the most promising biometrics, has received considerable recent biometric research interest. Among various palmprint recognition techniques, coding based methods have been very successful since of its simplicity, high precision, small size of feature and rapidness for both feature extraction and matching. Several filters, such as Gabor and Gaussian, and coding schemes, such as competitive and ordinal measure, have been proposed for palmprint verification and identification. In this paper, we evaluate three filters, Gabor, Gaussian, and the second derivative of Gaussian filter, and two coding schemes, competitive code and ordinal measure on PolyU palmprint database. Results of verification experiment show that Gabor filter and competitive coding scheme is superior to other methods.","Gabor filters,
Biometrics,
Image coding,
Feature extraction,
Filtering,
Iris,
Solids,
Fingerprint recognition,
Computer science,
Spatial databases"
A dynamic weighted data replication strategy in data grids,"Data grids deal with a huge amount of data regularly. It is a fundamental challenge to ensure efficient accesses to such widely distributed data sets. Creating replicas to a suitable site by data replication strategy can increase the system performance. It shortens the data access time and reduces bandwidth consumption. In this paper, a dynamic data replication mechanism is proposed, which is called Latest Access Largest Weight (LALW). LALW selects a popular file for replication and calculates a suitable number of copies and grid sites for replication. By setting a different weight for each data access record, the importance of each record is differentiated. The data access records in the nearer past have higher weight. It indicates that these records have higher value of references. In other words, the data access records in the long past have lower reference values. A Grid simulator OptorSim is used to evaluate the performance of this dynamic replication strategy. The simulation results show that LAHW successfully increases the effective network usage. It means that the LALW replication strategy can find out a popular file and replicates it to a suitable site.","Bandwidth,
Resource management,
History,
Computer science,
Data engineering,
System performance,
Distributed computing,
Grid computing,
Delay,
File servers"
Fast Modular Composition in any Characteristic,"We give an algorithm for modular composition of degree n univariate polynomials over a finite field F_q requiring n^{1 + o(1)}log^{1 + o(1)}q bit operations; this had earlier been achieved in characteristic n^{o(1)} by Umans (2008). As an application, we obtain a randomized algorithm for factoring degree n polynomials over F_q requiring (n^{1.5 + o(1)} + n^{1 + o(1)}log q)log^{1 + o(1)}q bit operations, improving upon the methods of von zur Gathen & Shoup (1992) and Kaltofen & Shoup (1998). Our results also imply algorithms for irreducibility testing and computing minimal polynomials whose running times are best-possible, up to lower order terms.As in Umans (2008), we reduce modular composition to certain instances of multipoint evaluation of multivariate polynomials. We then give an algorithm that solves this problem optimally (up to lower order terms), in arbitrary characteristic. The main idea is to lift to characteristic 0, apply a small number of rounds of multimodular reduction, and finish with a small number of multidimensional FFTs. The final evaluations are then reconstructed using the Chinese Remainder Theorem. As a bonus, we obtain a very efficient data structure supporting polynomial evaluation queries, which is of independent interest.Our algorithm uses techniques which are commonly employed in practice, so it may be competitive for real problem sizes. This contrasts with previous asymptotically fast methods relying on fast matrix multiplication.","Polynomials,
Galois fields,
Computer science,
Multidimensional systems,
Flexible printed circuits,
Mathematics,
Application software,
Testing,
Data structures,
Engineering profession"
Graph-shifts: Natural image labeling by dynamic hierarchical computing,"In this paper, we present a new approach for image labeling based on the recently introduced graph-shifts algorithm. Graph-shifts is an energy minimization algorithm that does labeling by dynamically manipulating, or shifting, the parent-child relationships in a hierarchical decomposition of the image. Each shift optimally reduces the energy by indirectly causing a change to the labeling; graph-shifts is able to rapidly compute and select this optimal shift at every iteration. There are no constraints on the terms of the (pairwise) energy function. The algorithm was originally presented in the context of medical image labeling using conditional random field models. In this paper, we consider the algorithm in the context of both low- and high-level natural image labeling. We show that for examples in both classes of problems, graph-shifts does labeling both accurately and rapidly. For low-level vision, we explore image restoration, and for high-level vision, we make use of a hybrid discriminative-generative model to segment and label images into semantically meaningful regions (e.g., trees, buildings, etc.). For both problems, we obtain comparable or superior results to the state-of-the-art computed in just a few seconds per image.","Labeling,
Minimization methods,
Biomedical imaging,
Image segmentation,
Tree graphs,
Belief propagation,
Computer science,
Power engineering and energy,
Statistics,
Nervous system"
A compact architecture for three-dimensional neural microelectrode arrays,"A new architecture is presented for achieving three-dimensional electronic interfaces to the nervous system using planar microfabricated two-dimensional arrays. This architecture overcomes many of the limitations of existing approaches and enables flexible electrode configurations with minimal overhead in size. A 64-channel (4×4×4) 3-D array using this architecture is demonstrated with sites on 100μm centers, interfacing with a volume of tissue less than 0.1mm3. The 1mm2 footprint of the device is the smallest ever reported.",
A new diffusion-based multilevel algorithm for computing graph partitions of very high quality,"Graph partitioning requires the division of a graph’s vertex set into k equally sized subsets such that some objective function is optimized. For many important objective functions, e. g., the number of edges incident to different partitions, the problem is NP-hard. Graph partitioning is an important task in many applications, so that a variety of algorithms and tools for its solution have been developed. Most state-of-the-art graph partitioning libraries use a variant of the Kernighan-Lin (KL) heuristic within a multilevel framework. While these libraries are very fast, their solutions do not always meet all requirements of the users. This includes the choice of the appropriate objective function and the shape of the computed partitions. Moreover, due to its sequential nature, the KL heuristic is not easy to parallelize. Thus, its use as a load balancer in parallel numerical applications requires complicated adaptations. That is why we have developed previously an inherently parallel algorithm, called BUBBLE-FOS/C (Meyerhenke et al., IPDPS’06), which optimizes the partition shapes by a diffusive mechanism. Yet, it is too slow to be of real practical use, despite its high solution quality.","Partitioning algorithms,
Shape,
Libraries,
Computer science,
Load management,
Application software,
Numerical simulation,
Parallel algorithms,
Parallel processing,
Acceleration"
Bluetooth Security in Wearable Computing Applications,"Advances in wireless sensor networking technologies have been extended to wearable computing systems and opened a plethora of applications and opportunities in the development and integration of pervasive Bluetooth Technologies with the abundance of existing specialized technologies in monitoring, data collection, and real-time analysis and reporting. Although the Bluetooth framework is acceptably secure, there are still a number of weaknesses in the Bluetooth technology. The nature of the wireless ad hoc and the device addressing schemes still make the Bluetooth protocol vulnerable to possible attacks and risks. This paper analyzes some of the vulnerabilities of Bluetooth architecture to possible attacks and risks and the best practices and countermeasures needed to thwart these attacks that target Bluetooth devices and their attached networks. It concentrates on Bluetooth technology security issues and challenges that face wireless computing systems.","Bluetooth,
Wearable computers,
Communication system security,
Wireless sensor networks,
Data security,
Sensor systems and applications,
Wearable sensors,
Monitoring,
Real time systems,
Wireless application protocol"
Dynamic Service Substitution in Service-Oriented Architectures,"The problem we deal with in this paper is the dynamic substitution of stateful services that become unavailable during the execution of service orchestrations. Previous research efforts focusing on the reconfiguration of conventional distributed systems enable the substitution of system entities with other prefabricated passive entities that serve as a backup. Nevertheless, the problem of service substitution is far more complex. In SOA, we can assume the possible existence of several semantically compatible services capable of performing the same or similar tasks. However, each one of them constantly serves requests and cannot be considered as a passive backup for other services. Therefore, we propose the SIROCO middleware platform, enabling the runtime, semantic-based service substitution. The basic concepts of SIROCO are  discussed along with an experimental evaluation of our first prototype. Our findings show that SIROCO provides the necessary means for achieving dynamic service substitution with a reasonable expense on the execution of service orchestrations.",
Cell segmentation using Hessian-based detection and contour evolution with directional derivatives,"The large amount of data produced by biological live cell imaging studies of cell behavior requires accurate automated cell segmentation algorithms for rapid, unbiased and reproducible scientific analysis. This paper presents a new approach to obtain precise boundaries of cells with complex shapes using ridge measures for initial detection and a modified geodesic active contour for curve evolution that exploits the halo effect present in phase-contrast microscopy. The level set contour evolution is controlled by a novel spatially adaptive stopping function based on the intensity profile perpendicular to the evolving front. The proposed approach is tested on human cancer cell images from LSDCAS and achieves high accuracy even in complex environments.","Evolution (biology),
Cells (biology),
Image segmentation,
Shape measurement,
Image analysis,
Algorithm design and analysis,
Level measurement,
Phase measurement,
Phase detection,
Active contours"
Part of Speech Tagging in Bengali Using Support Vector Machine,"Part of Speech (POS) tagging is the task of labeling each word in a sentence with its appropriate syntactic category called part of speech. POS tagging is a very important preprocessing task for language processing activities. This paper reports about task of POS tagging for Bengali using Support vector Machine (SVM). The POS tagger has been developed using a tagset of 26 POS tags, defined for the Indian languages. The system makes use of the different contextual information of the words along with the variety of features that are helpful in predicting the various POS classes. The POS tagger has been trained, and tested with the 72,341, and 20K wordforms, respectively. Experimental results show the effectiveness of the proposed SVM based POS tagger with an accuracy of 86.84%. Results show that the lexicon, named entity recognizer and different word suffixes are effective in handling the unknown word problems and improve the accuracy of the POS tagger significantly. Comparative evaluation results have demonstrated that this SVM based system outperforms the three existing systems based on the Hidden Markov Model (HMM), Maximum Entropy (ME) and Conditional Random Field (CRF).","Speech,
Tagging,
Support vector machines,
Hidden Markov models,
Stochastic processes,
Natural languages,
Information technology,
Labeling,
Computer science,
Testing"
Influence of contact pressure and moisture on the signal quality of a newly developed textile ECG sensor shirt,"A newly developed textile integrated sensor shirt, called “ITcares” (Intelligent Textile for CArdio REspiratory Sensing), is presented. Textile integrated ECG sensors are known to be highly depended on the electrode-skin-impedance. Two main influence factors on the skin-electrode impedance are: 1. contact pressure and 2. moisture. Systematic measurements were performed with additional sensors to evaluate the ECG signal quality. Furthermore, signal-to-noise ratios were calculated as a quantitative measure.","Moisture,
Textiles,
Electrocardiography,
Electrodes,
Intelligent sensors,
Computer science,
Conducting materials,
Biosensors,
Performance evaluation,
Surface impedance"
APL: Autonomous Passive Localization for Wireless Sensors Deployed in Road Networks,"In road networks, sensors are deployed sparsely (hundreds of meters apart) to save costs. This makes the existing localization solutions based on the ranging be ineffective. To address this issue, this paper introduces an autonomous passive localization scheme, called APL. Our work is inspired by the fact that vehicles move along routes with a known map. Using binary vehicle-detection timestamps, we can obtain distance estimates between any pair of sensors on roadways to construct a virtual graph composed of sensor identifications (i.e., vertices) and distance estimates (i.e., edges). The virtual graph is then matched with the topology of road map, in order to identify where sensors are located in roadways. We evaluate our design outdoor in Minnesota roadways and show that our distance estimate method works well despite of traffic noises. In addition, we show that our localization scheme is effective in a road network with eighteen intersections, where we found no location matching error, even with a maximum sensor time synchronization error of 0.3 sec and the vehicle speed deviation of 10 km/h.","Wireless sensor networks,
Costs,
Road vehicles,
Vehicle detection,
Surveillance,
Communications Society,
Helium,
Computer science,
Remotely operated vehicles,
Network topology"
A Novel Motion Based Lip Feature Extraction for Lip-Reading,"In a lip-reading system, one key issue is how to extract the visual features, which greatly impact on the lip-reading recognition accuracy and efficiency. In this paper, we propose a novel motion based visual feature representation. Compared with the existing methods, our approach focuses on the crucial part of lip movement, but not all pixels around lip contours for different utterance, and captures the motion tracks of each part. Accordingly, distinctive feature vectors are built to represent the whole lip motion process for the specified utterance, rather than the separate frame images. Experimental result shows the efficacy of the proposed approach.","Feature extraction,
Optical sensors,
Image motion analysis,
Tracking,
Speech analysis,
Lips,
Computational intelligence,
Computer security,
Computer science,
Brightness"
Path guidance control for a safer large scale dissipative haptic display,"The properties of dissipative haptic displays allow larger workspaces that permit a whole body interaction useful for sports, rehabilitation, and large-scale object design applications. To that end we designed and constructed the Brake Actuated Manipulator (BAM) with 2m3 workspace. Dissipative devices are capable of simulating virtual objects through resistance analogous to active devices. However, the challenge remains for path guidance paradigms because neither impedance nor admittance control can be used to actively steer limb movements. Here we first define a new way to create and track a path during path guidance with a twinned vector field to allow bilateral motion. Using this new path definition three controllers, velocity ratio, force cancelling, and force mapping are compared with and without visual feedback. The results indicate that both force controllers provide better guidance over velocity control; the force mapping technique resulted in the smoothest limb trajectory. The presence of visual feedback was found to be a critical factor for path guidance using dissipative devices.","Large-scale systems,
Haptic interfaces,
Displays,
Force control,
Velocity control,
Force feedback,
Magnesium compounds,
Immune system,
Impedance,
Admittance"
SPARCL: Efficient and Effective Shape-Based Clustering,"Clustering is one of the fundamental data mining tasks. Many different clustering paradigms have been developed over the years, which include partitional, hierarchical, mixture model based, density-based, spectral, subspace, and so on. The focus of this paper is on full-dimensional, arbitrary shaped clusters. Existing methods for this problem suffer either in terms of the memory or time complexity (quadratic or even cubic). This shortcoming has restricted these algorithms to datasets of moderate sizes. In this paper we propose SPARCL, a simple and scalable algorithm for finding clusters with arbitrary shapes and sizes, and it has linear space and time complexity. SPARCL consists of two stages -- the first stage runs a carefully initialized version of the Kmeans algorithm to generate many small seed clusters. The second stage iteratively merges the generated clusters to obtain the final shape-based clusters. Experiments were conducted on a variety of datasets to highlight the effectiveness, efficiency, and scalability of our approach. On the large datasets SPARCL is an order of magnitude faster than the best existing approaches.",
Combining LBP and Adaboost for facial expression recognition,"A novel approach to facial expression recognition based on the combination of local binary pattern (LBP) and Adaboost is proposed. Firstly, facial expression images are processed with LBP operator, which can eliminate the effect of environment lighting in a certain extent and has the powerful capability of texture feature description. And then facial expression features are presented with LBP histograms of expression image which is divided into several blocks. The features with powerful discriminability are selected by a modified Adaboost so as to predigest the design of classifier and shorten the cost time. Finally, the support vector machine (SVM) classifier is used for expression classification. The algorithm is implemented with Matlab and experimented on Japanese female facial expression database (JAFFE database). A facial expression recognition rate of 65.71% for person-independent is obtained and shows the effectiveness of the proposed algorithm.","Face recognition,
Feature extraction,
Gabor filters,
Support vector machines,
Support vector machine classification,
Spatial databases,
Filtering,
Computer science,
Psychology,
Computer vision"
The efficient VLSI design of BI-CUBIC convolution interpolation for digital image processing,"This paper presents an efficient VLSI design of bi-cubic convolution interpolation for digital image processing. The architecture of reducing the computational complexity of generating coefficients as well as decreasing number of memory access times is proposed. Our proposed method provides a simple hardware architecture design, low computation cost and is easy to implement. Based on our technique, the high-speed VLSI architecture has been successfully designed and implemented with TSMC 0.13μm standard cell library. The simulation results demonstrate that the high performance architecture of bi-cubic convolution interpolation at 279MHz with 30643 gates in a 498×498μm2 chip is able to process digital image scaling for HDTV in real-time.","Very large scale integration,
Convolution,
Interpolation,
Digital images,
Computer architecture,
Computational complexity,
Hardware,
Computational efficiency,
Software libraries,
Computational modeling"
A Real-Time Ubiquitous System for Assisted Living: Combined Scheduling of Sensing and Communication for Real-Time Tracking,"As the elderly population increases, elderly care using inexpensive technological means is becoming critical. This paper presents our prototype system that provides real-time indoor tracking of elderly residents and their belongings, which is essential to assisting and securing their independent living. For high-fidelity real-time tracking, we propose novel scheduling algorithms. Our scheduling algorithms are designed by harmonizing both sensing and communication signals and leveraging location awareness and mobility consciousness in order to improve tracking accuracy while reducing the energy consumption. We performed extensive experiments through both simulation and actual implementation. Our experimental result says that our scheduling algorithms can provide real-time tracking of residents within a 20 cm error bound in the typical range of human mobility.","tracking,
geriatrics,
handicapped aids,
mobile computing,
real-time systems,
scheduling,
telemedicine"
Efficient Reconfigurable On-Chip Buses for FPGAs,This paper presents techniques for generating on-chip buses suitable for dynamically integrating hardware modules into an FPGA-based SoC by partial reconfiguration. The buses permit direct connections of master and slave modules to the bus in combination with a flexible fine-grained module placement and with minimized latency and area overheads. A test system will demonstrate a transfer rate of 800 MB/s while providing an extreme high placement flexibility.,"Field programmable gate arrays,
System-on-a-chip,
Hardware,
Reconfigurable logic,
Runtime,
Delay,
Wires,
Routing,
Tiles,
Computer science"
LPV Model and Its Application in Web Server Performance Control,"Performance management is an important issue for Internet servers working in the unpredictable and highly dynamic environment. Feedback control is considered as a potential theoretical tool. However, due to the weakness in capturing the nonlinear natures of Internet servers, the classical linear control designs have been demonstrated ineffectiveness when handling varying workloads to provide sufficient performance guarantees. This paper attempts to apply Linear-Parameter-Varying (LPV) approaches to modeling and controlling the web server system for absolute connection delay guarantee. To achieve these goals, a LPV model, wherein the workload intensity is considered as the scheduling parameter, is identified experimentally to approximate the web server system and then a LPV controller is designed for the LPV model. Model validation proves the LPV model is much more accurate than the LTI model with the same order. Closed-loop simulations under fluctuated workloads demonstrate that, the performance of the LPV controller overpasses the proportion-integral controller. By exploring the nature of dependence of server model on the workload intensity, the proposed LPV approach for absolute delay guarantee may find its applications in many other server performance control problems e.g., admission control, Quality-of-Service (QoS) control and power control.","Web server,
Internet,
Delay,
Proportional control,
Environmental management,
Feedback control,
Control design,
Admission control,
Quality of service,
Power control"
Traffic sign recognition by fuzzy sets,"A novel fuzzy approach developed to recognize traffic signs is presented in this paper. More than 3400 images of traffic signs were collected in different light conditions by a digital camera mounted in a car and used for developing and testing this approach. Every RGB image was converted into HSV color space and segmented by using a set of fuzzy rules depending on the hue and saturation values of each pixel. Objects in each segmented image are labeled and tested for the presence of probable sign. Objects passed this test are recognized by a fuzzy shape recognizer which invokes another set of fuzzy rules. These fuzzy rules are based on four invariant shape measures which are invoked to decide the shape of the sign; rectangularity, triangularity, ellipticity, and the new shape measure octagonality. The method is tested in different environmental conditions and it shows high robustness.",Vehicles
Micro Unmanned Aerial Vehicle Visual Servoing for Cooperative Indoor Exploration,Recent advances in the field of micro unmanned aerial vehicles (MAVs) make flying robots of small dimensions suitable platforms for performing advanced indoor missions. In order to achieve autonomous indoor flight a pose estimation technique is necessary. This paper presents a complete system which incorporates a vision-based pose estimation method to allow a MAV to navigate in indoor environments in cooperation with a ground robot. The pose estimation technique uses a lightweight light emitting diode (LED) cube structure as a pattern attached to a MAV. The pattern is observed by a ground robot's camera which provides the flying robot with the estimate of its pose. The system is not confined to a single location and allows for cooperative exploration of unknown environments. It is suitable for performing missions of a search and rescue nature where a MAV extends the range of sensors of the ground robot. The performance of the pose estimation technique and the complete system is presented and experimental flights of a vertical take-off and landing (VTOL) MAV are described.,"Unmanned aerial vehicles,
Visual servoing,
Robot sensing systems,
Navigation,
Indoor environments,
Robot vision systems,
Light emitting diodes,
Space technology,
Information science,
Cameras"
Gender recognition based on fusion on face and gait information,"This paper considers the combination of face and gait biometrics from the same walking sequence to carry out gender recognition. A camera is capturing the side view of a person, while another camera is placed to record the face of the same person at the front view. After these videos are acquired, we extract the silhouette images from the gait videos and normalized frame images decomposed from the face videos. Then, for face classification, we introduce PCA to reduce the image dimension and SVM to classify gender, for gait classification, we divide the silhouette into seven parts and extract features from each and also employ SVM to classify gender. On the decision level, the sum rule is applied to implement the fusion of these two classification results. The final fusion results show an improvement on correct classification rate.","Face,
Support vector machines,
Cameras,
Principal component analysis,
Feature extraction,
Face recognition,
Databases"
A Reflective Framework to Improve the Adaptability of BPEL-based Web Service Composition,"Web services composition is receiving significant amount of interest as an important strategy to allow enterprise collaboration. Current web services composition solutions are rather restricted and inflexible as they are based on pre-defined models of the process environment. These solutions have assumed that the information in the models and consequently the compositions remain static and unchanged throughout the life cycle of the web services composition. However, web services may run in a highly dynamic environment. Therefore, a mechanism is required to support web services composition in dynamic and flexible process environment. The reflective framework presented here aims to improve the adaptability of BPEL-based web service composition. A meta-model was defined to build the self-representation of the web services composition. The meta-model will be modified to adapt to the changing environment, and then, the reflection mechanism utilized in the framework will adjust the web services composition automatically. To ensure the correctness of dynamic adaptation, a set of constraints and a verification approach are proposed. A prototype adaptive service composition environment has been developing to implement our reflective framework and demonstrate its effectiveness on providing adaptive web services composition. In summary, it is stated that the reflective framework provides a suitable solution to the adaptive service composition and reliable control flow and data flow correctness.","Web services,
business data processing"
Finding Routing Paths for Alternate Routing in All-Optical WDM Networks,"An alternate routing algorithm requires a set of predetermined routing paths between each source-destination pair. To reduce the connection blocking probability, it is desirable that the predetermined routing paths between each source-destination pair be link-disjoint. The predetermined routing paths used in previous works on alternate routing are the -shortest link-disjoint paths in terms of hop count. The shared links among the -shortest link-disjoint paths between different source-destination pairs may cause high connection blocking probability. Thus, depending on the traffic requirements of all source-destination pairs, hop-count based -shortest link-disjoint paths may not be the best choice for the predetermined routing paths. This paper proposes a method to find a set of link-disjoint routing paths between each source-destination pair to be used by an alternate routing algorithm in order to reduce the connection blocking probability. The key idea is to find a set of link-disjoint routing paths based on the routing paths that are utilized by the optimal traffic pattern in the network. Then, for each source-destination pair, we select a set of link-disjoint routing paths from the routing paths that are utilized by the optimal traffic pattern such that the selected set of link-disjoint routing paths carries the most of the traffic between the source-destination pair. Simulations are performed to compare the performance of using the link-disjoint routing paths found by the proposed method as the predetermined routing paths and those of using the hop-count based -shortest link-disjoint paths and employing the routing paths found by the capacity-balanced alternate routing method proposed method by Ho and Mouftah (in 2002) as the predetermined routing paths. Our simulation results show that using the link-disjoint routing paths found by the proposed method yields significantly lower connection blocking probability than employing the hop-count based -shortest link-disjoint paths and using the routing paths found by the capacity-balanced alternate routing method (Ho and Mouftah, 2002).","WDM networks,
Wavelength routing,
Educational institutions,
Telecommunication traffic,
Traffic control,
Wavelength division multiplexing,
Computer science,
Councils,
Information science,
Intelligent networks"
Robust ultra-low voltage ROM design,"SRAM dominates standby power consumption in many systems since the power supply cannot be gated as in logic blocks. The use of ROM for parts of instruction memory can alleviate this power bottleneck in mobile sensing applications such as implantable biomedical and environmental sensing systems, which can spend up to 99% of their lifetimes in standby mode. However, robust ROM design becomes challenging as the supply voltage is reduced aggressively. In this paper, three different ROM topologies are investigated and compared for ultra-low voltage operation. A simple method to estimate the theoretical robustness at low voltage is proposed and applied to the ROM topologies. A test circuit fabricated in a carefully-selected 0.18μm CMOS technology reveals that our proposed static NAND ROM structure improves performance by 26X, energy by 3.8X and lowest functional supply voltage by 100mV over a conventional dynamic NAND ROM.","Robustness,
Voltage,
Read only memory,
Circuit topology,
Circuit testing,
CMOS technology,
Random access memory,
Energy consumption,
Emergency power supplies,
Logic gates"
Planning and learning algorithms for routing in Disruption-Tolerant Networks,"We give an overview of algorithms that we have been developing in the DARPA Disruption-Tolerant Networking program, which aims at improving communication in networks with intermittent and episodic connectivity. Thanks to the use of network caching, this can be accomplished without the need for a simultaneous end-to-end path that is required by traditional Internet and mobile ad-hoc network (MANET) protocols. We employ a disciplined two-level approach that clearly distinguishes the dissemination of application content from the dissemination of network-related knowledge, each of which can be supported by different algorithms. Specifically, we present probabilisitc reflection, a single-message protocol enabling the dissemination of knowledge in strongly disrupted networks. For content dissemination, we present two approaches, namely a symbolic planning algorithm that exploits partially predictable temporal behavior, and a distributed and disruption-tolerant reinforcement learning algorithm that takes into account feedback about past performance.","Routing,
Disruption tolerant networking,
Protocols,
Unmanned aerial vehicles,
IP networks,
Ad hoc networks,
Mobile ad hoc networks,
Personal digital assistants,
Satellites,
Costs"
Hierarchical identity based cryptography for end-to-end security in DTNs,"Delay Tolerant Networks (DTN) arise whenever traditional assumptions about todaypsilas Internet such as continuous end-to-end connectivity, low latencies and low error rates are not applicable. These challenges impose constraints on the choice and implementation of possible security mechanisms in DTNs. The key requirements for a security architecture in DTNs include ensuring the protection of DTN infrastructure from unauthorized use as well as application protection by providing confidentiality, integrity and authentication services for end-to-end communication. In this paper, we examine the issues in providing application protection in DTNs and look at various possible mechanisms. We then propose an architecture based on Hierarchical Identity Based Encryption (HIBE) that provides end-to-end security services along with the ability to have fine-grained revocation and access control while at the same time ensuring efficient key management and distribution. We believe that a HIBE based mechanism would be much more efficient in dealing with the unique constraints of DTNs compared to standard public key mechanisms (PKI).","Security,
Cryptography,
Public key,
Access control,
Servers,
Authentication,
Payloads"
Topology Design of Network-Coding-Based Multicast Networks,"It is anticipated that a large amount of multicast traffic needs to be supported in future communication networks. The network coding technique proposed recently is promising for establishing multicast connections with a significantly lower bandwidth requirement than that of traditional Steiner-tree-based multicast connections. How to design multicast network topologies with the consideration of efficiently supporting multicast by the network coding technique becomes an important issue now. It is notable, however, that the conventional algorithms for network topology design are mainly unicast-oriented, and they cannot be adopted directly for the efficient topology design of network-coding-based multicast networks by simply treating each multicast as multiple unicasts. In this paper, we consider for the first time the novel topology design problem of network-coding-based multicast networks. Based on the characteristics of multicast and network coding, we first formulate this problem as a mixed-integer nonlinear programming problem, which is NP-hard, and then propose two heuristic algorithms for it. The effectiveness of our heuristics is verified through simulation and comparison with the exhaustive search method. We demonstrate in this paper that, in the topology design of multicast networks, adopting the network coding technique to support multicast transmissions can significantly reduce the overall topology cost as compared to conventional unicast-oriented design and the Steiner-tree-based design.","Network topology,
Network coding,
Multicast algorithms,
Algorithm design and analysis,
Telecommunication traffic,
Communication networks,
Bandwidth,
Unicast,
Heuristic algorithms,
Search methods"
MyMobiHalal 2.0: Malaysian mobile halal product verification using camera phone barcode scanning and MMS,"With the steady growth and affordability of camera phones, more mobile applications are necessary. Nowadays the mobile industry began to pay more attention to barcode applications for domestic users need. This paper describes a mobile-based support application for Muslims to identify the Halal status (prepared in accordance to Islamic law) of the product using mobile device. We argue that our MMS camera phone-based application is an economical and effective way to speed up the Halal verification process as opposed to text entry in SMS. This paper discusses the barcode concept and its applications in consumer product industry. First we analyze the issue of Halal among the Muslim consumers and how mobile technology can be used. Then we discuss the framework and design of our system that we called MyMobiHalal 2.0.","Cameras,
Consumer products,
Mobile handsets,
Mobile computing,
Application software,
Food technology,
Certification,
Image databases,
Mobile communication,
Computer science"
Secure hop-by-hop aggregation of end-to-end concealed data in wireless sensor networks,"In-network data aggregation is an essential technique in mission critical wireless sensor networks (WSNs) for achieving effective transmission and hence better power conservation. Common security protocols for aggregated WSNs are either hop-by-hop or end-to-end, each of which has its own encryption schemes considering different security primitives. End-to-end encrypted data aggregation protocols introduce maximum data secrecy with in-efficient data aggregation and more vulnerability to active attacks, while hop-by-hop data aggregation protocols introduce maximum data integrity with efficient data aggregation and more vulnerability to passive attacks. In this paper, we propose a secure aggregation protocol for aggregated WSNs deployed in hostile environments in which dual attack modes are present. Our proposed protocol is a blend of flexible data aggregation as in hop-by-hop protocols and optimal data confidentiality as in end-to-end protocols. Our protocol introduces an efficient O(1) heuristic for checking data integrity along with cost-effective heuristic-based divide and conquer attestation process which is O(ln n) in average −O(n) in the worst scenario- for further verification of aggregated results.","Wireless sensor networks,
Cryptography,
Data security,
Bandwidth,
Event detection,
Computer science,
Access protocols,
Sensor phenomena and characterization,
Base stations,
Mission critical systems"
Model-Checking of Web Services Choreography,"Web services choreography describes the global model of service interactions among a set of participants. In order to achieve a common business goal, the protocols of interaction must be correct. In this paper, we model interactions with recordings of state/channel variable changes that can occur as a result of carrying out the interactions. Thus, it is possible to verify not only normal control flow properties such as deadlock-freeness, but also channel-passing related problems such as channel-absence. Concretely, we propose a small language CDL, together with an operational semantics. We illustrate with a T-Shirts procurement protocol how service choreographies can be specified in CDL, and how the verification can be carried out using the SPIN model-checker.","Web services,
Protocols,
CD recording,
Procurement,
Application software,
Computer science education,
Systems engineering and theory,
Chaos,
Educational institutions,
Informatics"
Iris recognition performance enhancement using weighted majority voting,"Biometric authentication is a convenient and increasingly reliable way to prove one’s identity. Iris scanning in particular is among the most accurate biometric authentication technologies currently available. However, despite their extremely high accuracy under ideal imaging conditions, existing iris recognition methods degrade when the iris images are noisy or the enrollment and verification imaging conditions are substantially different. To address this issue and enable iris recognition on less-than-ideal images, we introduce a weighted majority voting technique applicable to any biometric authentication system using bitwise comparison of enrollment-time and verification-time biometric templates. In a series of experiments with the CASIA iris database, we find that the method outperforms existing majority voting and reliable bit selection techniques. Our method is a simple and efficient means to improve upon the accuracy of existing iris recognition systems.","Iris recognition,
Voting,
Biometrics,
Authentication,
Image databases,
Humans,
Image segmentation,
Infrared sensors,
CMOS image sensors,
Computer science"
Sparsesense: Application of compressed sensing in parallel MRI,"Compressed sensing (CS) has recently drawn great attentions in the MRI research community. The most desirable property of CS in MRI application is that it allows sampling of k-space well below Nyquist sampling rate, while still being able to reconstruct the image if certain conditions are satisfied. Recent work has successfully applied CS to reduce scanning time in conventional Fourier imaging. In this paper, the application of CS to parallel imaging, a fast imaging technique, is investigated to achieve an even higher imaging speed. The sampling scheme for incoherence is discussed and reconstruction method using Begman iteration is proposed. Our experiments show that the combined method, named SparseSENSE, can achieve a reduction factor higher than the number of channels.","Compressed sensing,
Magnetic resonance imaging,
Sampling methods,
Image sampling,
Application software,
Equations,
Large-scale systems,
Discrete wavelet transforms,
Shape measurement,
Image reconstruction"
Union support recovery in high-dimensional multivariate regression,"In the problem of multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* ∈ ℝp×K of regression coefficients. We study the behavior of the group Lasso using l1/l2 regularization for the union support problem, meaning that the set of s rows for which B* is non-zero is recovered exactly. Studying this problem under high-dimensional scaling, we show that group Lasso recovers the exact row pattern with high probability over the random design and noise for scalings of (n, p, s) such that the sample complexity parameter given by θ(n, p, s) := n/[2ψ(B*) log(p - s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the number of non-zero rows, and ψ(B*) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap function reveals that, if the design is uncorrelated on the active rows, block l1/l2 regularization for multivariate regression never harms performance relative to an ordinary Lasso approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the group Lasso.","Multivariate regression,
Vectors,
Predictive models,
Additive noise,
H infinity control,
Statistics,
Size measurement,
Large-scale systems,
Statistical learning,
Collaborative work"
Shape-guided superpixel grouping for trail detection and tracking,"We describe a framework for detecting and tracking continuous “trails” in images and image sequences for autonomous robot navigation. Continuous trails are extended regions along the ground such as roads, hiking paths, rivers, and pipelines which can be navigationally useful for ground-based or aerial robots. Our approach to single-image trail segmentation incorporates both bottom-up and top-down processes. First, good grouping hypotheses are efficiently generated by probabilistic clustering of superpixels based on color similarity. Second, hypotheses are robustly ranked with an objective function comprising shape, appearance, and deformation terms. The shape term measures how well a triangle, the approximate template for a trail viewed under perspective, can be fit to the grouping’s boundary. The appearance term reflects the visual contrast between the grouping and its surroundings using a between-class/within-class scatter measure. Finally, the deformation term measures the closeness of the fitted triangle to a learned distribution which captures expected size, location, and other degrees of shape variation. Although trail detection is accurate and reasonably fast on a variety of isolated images, we describe how introducing temporal filtering to both the bottom-up and top-down stages increases segmentation accuracy and per-frame speed over image sequences. Results are shown on varied sequences collected from flying and driving platforms, as well as images sampled from the Web.","Shape,
Image segmentation,
Image color analysis,
Roads,
Robots,
Distance measurement,
Rivers"
IgorFs: A Distributed P2P File System,"IgorFs is a distributed, decentralized peer-to-peer (P2P) file system that is completely transparent to the user. It is built on top of the Igor peer-to-peer overlay network, which is similar to Chord, but provides additional features like service orientation or proximity neighbor and route selection. IgorFs offers an efficient means to publish data files that are subject to frequent but minor modifications. In our demonstration we show two use cases for IgorFs: the first example is (static) software-distribution and the second example is (dynamic) file distribution.","File systems,
Cryptography,
Internet,
Software,
Permission,
Routing,
Peer to peer computing"
Bilateral control of nonlinear teleoperation with time varying communication delays,"This paper addresses the bilateral control of nonlinear teleoperation with time varying communication delays. The proposed methods are two types of simple PD-type controllers which consists of D-controls depending on (the upper bound of) the rate of change of delay and P-controls depending on the upper bound of round-trip delay. Using Lyapunov-Krasovskii function, the delay-dependent stability of the origin is shown for the ranges of gains. Furthermore the proposed strategies also achieve master-slave position coordination and bilateral static force reflection. Several experimental results show the effectiveness of our proposed methods.","Communication system control,
Delay effects,
Robot kinematics,
Master-slave,
Force feedback,
Reflection,
Scattering,
Stability,
Robot sensing systems,
Velocity control"
Specific features of a converter of web documents from Bengali to Universal Networking Language,"In this paper, we present a workable structure along with characteristic features of a subsystem that may become an integral part of a Language Server bridging Bengali and the Universal Networking Language (UNL). We try to assimilate the results of the research efforts of the UNL community and also of various machine translation projects. Vast information resources in different languages are available in the Internet, but the can not be shared (because of vastly due to the language barrier). And the UNL community is set to devise an effective and efficient system to diminish that barrier with an ultimate aim to allow automatic conversion of web based resources in one member language to that in another member language. A good number of researchers in computational linguistics all over the world have already joined hands with the UNL initiators, and research groups representing most widely used natural languages are working intensively for the purpose. This paper is to demonstrate our pioneering efforts in the field of Bengali (Bangla). Here we here outline a possible Bangla-UNL dictionary, feature an annotation editor for Bangla texts, infer significant morphological, syntactic and semantic rules for parsing Bangla web documents in connection with conversion to the UNL, and show possible ways of future contribution towards the goal.","Natural languages,
Dictionaries,
Computer networks,
Information resources,
Network servers,
Web server,
Internet,
Computational linguistics,
Costs,
Scattering"
Seamless Warping of Diffusion Tensor Fields,"To warp diffusion tensor fields accurately, tensors must be reoriented in the space to which the tensors are warped based on both the local deformation field and the orientation of the underlying fibers in the original image. Existing algorithms for warping tensors typically use forward mapping deformations in an attempt to ensure that the local deformations in the warped image remains true to the orientation of the underlying fibers; forward mapping, however, can also create ldquoseamsrdquo or gaps and consequently artifacts in the warped image by failing to define accurately the voxels in the template space where the magnitude of the deformation is large (e.g., |Jacobian| > 1). Backward mapping, in contrast, defines voxels in the template space by mapping them back to locations in the original imaging space. Backward mapping allows every voxel in the template space to be defined without the creation of seams, including voxels in which the deformation is extensive. Backward mapping, however, cannot reorient tensors in the template space because information about the directional orientation of fiber tracts is contained in the original, unwarped imaging space only, and backward mapping alone cannot transfer that information to the template space. To combine the advantages of forward and backward mapping, we propose a novel method for the spatial normalization of diffusion tensor (DT) fields that uses a bijection (a bidirectional mapping with one-to-one correspondences between image spaces) to warp DT datasets seamlessly from one imaging space to another. Once the bijection has been achieved and tensors have been correctly relocated to the template space, we can appropriately reorient tensors in the template space using a warping method based on Procrustean estimation.","Tensile stress,
Diffusion tensor imaging,
Magnetic resonance imaging,
Psychiatry,
Humans,
Psychology,
Brain,
Shape measurement,
Magnetic resonance,
Drugs"
"Software Process Simulation Modeling: Facts, Trends and Directions","Software Process Simulation Modeling (SPSM) research has increased since the first ProSim Workshop held in 1998 and Kellner, Madachy and Raffo (KMR) discussed the ""why, what and how"" of process simulation. This paper aims to assess how SPSM has evolved during the past 10 years in particular whether the reasons for SPSM, the simulation paradigms, tools, problem domains, and model scopes have changed. We performed a systematic literature review of software process simulation papers from the ProSim series publications in the last decade. We identified 96 studies from the sources and included them in this review. The papers were categorized into four major types and data needed to address each research question was extracted. We found a need for refining the reasons and the classification scheme for SPSM introduced by KMR. More emerging SPSM paradigms and model scopes were added to enhance KMR's discussion. Trends over time showed that interest in continuous modeling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration. We recommend SPSM research concentrate more on recent software processes and on making SPSM more reusable and thus easier to build.","Software engineering,
Computer simulation,
Computational modeling,
Mathematical model,
Computer science,
Software reusability,
Software tools,
Helium,
Information systems,
Mathematics"
DLM: A distributed Large Memory System using remote memory swapping over cluster nodes,"Emerging 64bitOS’s supply a huge amount of memory address space that is essential for new applications using very large data. It is expected that the memory in connected nodes can be used to store swapped pages efficiently, especially in a dedicated cluster which has a high-speed network such as 10GbE and Infiniband. In this paper, we propose the Distributed Large Memory System (DLM), which provides very large virtual memory by using remote memory distributed over the nodes in a cluster. The performance of DLM programs using remote memory is compared to ordinary programs using local memory. The results of STREAM, NPB and Himeno benchmarks show that the DLM achieves better performance than other remote paging schemes using a block swap device to access remote memory. In addition to performance, DLM offers the advantages of easy availability and high portability, because it is a user-level software without the need for special hardware. To obtain high performance, the DLM can tune its parameters independently from kernel swap parameters. We also found that DLM’s independence of kernel swapping provides more stable behavior.","Kernel,
Arrays,
Memory management,
Servers,
Bandwidth,
Performance evaluation,
Benchmark testing"
A novel cluster-based self-organization algorithm for wireless sensor networks,"Wireless sensor networks (WSNs) consist of a large number of tiny sensor nodes. Hence, a cluster-based architecture can be used to deal the self-organization issues of large networks. This cluster-based organization can prolong network lifetime and reduce broadcast overhead. In this paper, we propose an efficient self- organization algorithm for clustering (ESAC), which uses a weight-based criterion for cluster-head's election. This weight relies on the combination of k-density, residual energy and mobility. In ESAC, the node having greatest weight in its 2-hop neighborhood is chosen as cluster-head for a fixed period. ESAC enables to generate a low number of stable and balanced clusters. Simulation results show that ESAC provides better results when compared with WCA (weight clustering algorithm), and with the algorithms proposed respectively by Lin et al., and Chu et al. in terms of the number of clusters formed. On the other hand, it outperforms LCC (least cluster- head changes) algorithm in terms of the number of cluster-heads changes.","Clustering algorithms,
Wireless sensor networks,
Sensor phenomena and characterization,
Nominations and elections,
Computer networks,
Broadcasting,
Monitoring,
Energy dissipation,
Base stations,
Batteries"
Towards Semantic Analysis of Conversations: A System for the Live Identification of Speakers in Meetings,"In the following article we present an application that enables online identi¿cation of who is currently speaking using a single far-¿eld microphone in a meeting scenario. By leveraging techniques from both the ¿eld of speaker identi¿cation and speaker diarization, the system is able to recognize the current speaker after any two seconds of speech. An evaluation of the robustness of the algorithm using the AMI Meeting Corpus and the NIST Speaker Di- arization Development set resulted in a Diarization Error Rate of 12.67%.","Speech,
Hidden Markov models,
Training,
NIST,
Density estimation robust algorithm,
Computational modeling,
Adaptation model"
Skew Estimation and Correction of Text Using Bounding Box,"This paper, in addition to reporting some existing techniques, proposes a new technique for skew correction. It includes a novel document skew detection algorithm based on bounding box technique. The algorithm works quite efficiently for detecting the skew and then correcting it. The method has been experimented on various text documents and very promising results have been achieved given more than 97% accuracy. A comparative study has been reported to provide a detailed analysis of the proposed method together with some other existing methods in the literature.","Algorithm design and analysis,
Principal component analysis,
Pixel,
Estimation,
Mathematical model,
Equations,
Optical character recognition software"
An Efficient Non-dominated Sorting Method for Evolutionary Algorithms,"We present a new non-dominated sorting algorithm to generate the non-dominated fronts in multi-objective optimization with evolutionary algorithms, particularly the NSGA-II. The non-dominated sorting algorithm used by NSGA-II has a time complexity of O(MN2) in generating non-dominated fronts in one generation (iteration) for a population size N and M objective functions. Since generating non-dominated fronts takes the majority of total computational time (excluding the cost of fitness evaluations) of NSGA-II, making this algorithm faster will significantly improve the overall efficiency of NSGA-II and other genetic algorithms using non-dominated sorting. The new non-dominated sorting algorithm proposed in this study reduces the number of redundant comparisons existing in the algorithm of NSGA-II by recording the dominance information among solutions from their first comparisons. By utilizing a new data structure called the dominance tree and the divide-and-conquer mechanism, the new algorithm is faster than NSGA-II for different numbers of objective functions. Although the number of solution comparisons by the proposed algorithm is close to that of NSGA-II when the number of objectives becomes large, the total computational time shows that the proposed algorithm still has better efficiency because of the adoption of the dominance tree structure and the divide-and-conquer mechanism.",
Branch-and-bound hypothesis selection for two-view multiple structure and motion segmentation,"An efficient and robust framework for two-view multiple structure and motion segmentation is proposed. To handle this otherwise recursive problem, hypotheses for the models are generated by local sampling. Once these hypotheses are available, a model selection problem is formulated which takes into account the hypotheses likelihoods and model complexity. An explicit model for outliers is also added for robust model selection. The model selection criterion is optimized through branch-and-bound technique of combinatorial optimization which guaranties optimality over current set of hypotheses by efficient search of solution space.","Motion segmentation,
Computer vision,
Layout,
Sampling methods,
Cameras,
Cost function,
Robustness,
Computer science,
Image segmentation,
Motion compensation"
E-learning Recommendation System,"E-learning recommendation system helps learners to make choices without sufficient personal experience of the alternatives, and it is considerably requisite in this information explosion age. In our study, the user-based collaborative filtering method is chosen as the primary recommendation algorithm, combined with online education. We analyze the requirement of a web-based e-learning recommendation system, and divide the system workflow into five sections: data collection, data ETL, model generation, strategy configuration, and service supply. Moreover, an architecture is proposed, based on which further development can be accomplished. In this architecture, there are seven modules, and four of them are core modules: recommendation models database, recommendation system database, recommendation management, data/model management.","Electronic learning,
Filtering algorithms,
Collaborative work,
Information filtering,
Online Communities/Technical Collaboration,
Databases,
Information filters,
Books,
Clustering algorithms,
Computer science"
State Skip LFSRs: Bridging the Gap between Test Data Compression and Test Set Embedding for IP Cores,"We present a new type of linear feedback shift registers, state skip LFSRs. state skip LFSRs are normal LFSRs with the addition of a small linear circuit, the State Skip circuit, which can be used, instead of the characteristic-polynomial feedback structure, for advancing the state of the LFSR. In such a case, the LFSR performs successive jumps of constant length in its state sequence, since the State Skip circuit omits a predetermined number of states by calculating directly the state after them. By using State Skip LFSRs we get the well- known high compression efficiency of test set embedding with substantially reduced test sequences, since the useless parts of the test sequences are dramatically shortened by traversing them in state skip mode. The length of the shortened test sequences approaches that of test data compression methods. A systematic method for minimizing the test sequences of re- seeding-based test set embedding methods, and a low overhead decompression architecture are also presented.","Test data compression,
Circuit testing,
System testing,
Feedback circuits,
State feedback,
Automatic test pattern generation,
Linear feedback shift registers,
Linear circuits,
Circuit faults,
Random sequences"
A high-output-impedance current microstimulator for anatomical rewiring of cortical circuitry,"This paper reports on the design, implementation, and performance characterization of a high-output-impedance current microstimulator fabricated using the TSMC 0.35μm 2P/4M n-well CMOS process as part of a fully integrated neural implant for reshaping long-range intracortical connectivity patterns in an injured brain. It can deliver a maximum current of 94.5μA to the target cortical tissue with current efficiency of 86% and voltage compliance of 4.7V with a 5-V power supply. The stimulus current can be programmed via a 6-bit DAC with an accuracy better than 0.47 LSB. Stimulator functionality is also verified with in vitro experiments in saline using a silicon microelectrode with iridium oxide (IrO) stimulation sites.","Circuits,
Implants,
Injuries,
Impedance,
Voltage,
Power supplies,
Silicon,
Nerve fibers,
Proteins,
CMOS process"
Time-domain receiver design for MIMO underwater acoustic communications,"In this paper, we propose a time-domain receiver design scheme for high data rate single carrier multiple-input, multiple-output (MIMO) underwater acoustic communications. In this scheme, each received packet is artificially partitioned into blocks for processing. The MIMO channel is initially estimated using training blocks at the front of transmitted packets from all transducers. With the estimated MIMO channel, one data block following the training blocks is equalized. The phase rotation in the equalized data block is compensated by a group-wise phase correction operation, before symbol detection. The newly detected data block along with K - 1 previous data (or pilot) blocks are utilized to re-estimate the channel, which is employed to equalize the next new data block. The block-wise processing procedure is repeated until all blocks in the received packet are processed and demodulated. The proposed receiver scheme is tested with MakaiEx05 experimental data measured at Kauai, Hawaii, in September 2005. Processing results show that it works effectively with 2 × 8 BPSK, QPSK and 8PSK transmissions at the symbol period of 0.1 milliseconds. The average uncoded bit error rate (BER) is on the order of 8 × 10−4 for BPSK, 3 × 10−2 for QPSK, and 8 × 10−2 for 8PSK transmission.","Time domain analysis,
MIMO,
Underwater acoustics,
Underwater communication,
Binary phase shift keying,
Quadrature phase shift keying,
Bit error rate,
Acoustic transducers,
Phase detection,
Testing"
Background modeling from a free-moving camera by Multi-Layer Homography Algorithm,"This paper proposes a novel Multi-layer Homography Algorithm for background modeling from a free-moving camera. Background is composed of many planes. Different planes satisfy with different homographies which can be found by our algorithm. Each pixel except for the moving pixel definitely belongs to some plane. Rectified by the corresponding homography, each static pixel in the shared view can find its match in the previous frame. Thus, frames can be rectified to a specific viewpoint for background modeling. Experiment shows it is effective. Our approach can be used in motion detection from a free-moving camera.","Cameras,
Layout,
Motion detection,
Computer science,
Object detection,
Computer vision,
Optical filters,
Motion estimation,
Particle tracking,
Nonhomogeneous media"
Threshold Secret Image Sharing by Chinese Remainder Theorem,"We extend the sharing scheme proposed by Mignotte in 1983 based upon Chinese remainder theorem (CRT) to devise a threshold secret sharing scheme for digital images. Given a secret image I and a set of n participants sharing I, our scheme encrypts I into n shadows in such a way that any group of r shadows can recover I while that of less than r shadows cannot where 2 ≤ r ≤ n. As compared to the similar work by Meher and Patra in 2006, which is not a threshold scheme, our design satisfies the threshold requirements so that it is more technically sound. Our work reveals a new research area of applying CRT in secret image sharing.","Cryptography,
Cathode ray tubes,
Digital images,
Protection,
Equations,
Computer science,
Polynomials,
Interpolation,
Pixel"
A Quality Model for Evaluating Reusability of Services in SOA,"Service-Oriented Architecture (SOA) is an effective paradigm for publishing common features as services and reusing the published services in building applications. Therefore, reusability of services is a key criterion for evaluating the quality of services. The evaluation target for reusability in our model is the service, applied to both atomic and composite service. Reusability models for conventional paradigms such as OOP and CBD cannot effectively be applied to services in SOA. Hence, there is a high demand for devising a quality model to evaluate service reusability. In this paper, we propose a comprehensive quality model for evaluating reusability of services.","Service oriented architecture,
Software reusability,
Publishing,
Buildings,
Application software,
Quality of service,
Object oriented modeling,
Computer science,
Atomic measurements,
Software measurement"
Keeping the intelligent environment resident in the loop,"Recent technological advancements have increased the likelihood that smart home technologies will become part of our everyday environments. However, many of these technologies are brittle and do not adapt to the user's wishes or changes in residents' habits and lifestyle. Here we introduce CASAS, an adaptive smart home that utilizes machine learning techniques to discover patterns in user behavior and to automatically mimic these patterns. Our goal is to keep the resident in control of the automation. Users can provide feedback on proposed automation activities, modify the automation policies, and introduce new requests. In addition, CASAS can discover changes in resident's behavior patterns automatically. In this paper we describe the CASAS technologies and evaluate its ability to adapt to the resident's activities and requests.","user modelling,
behavioural sciences,
home automation,
learning (artificial intelligence)"
Automatic Selection of Application-Specific Reconfigurable Processor Extensions,This paper presents a new method for automatic selection of application-specific processor extensions and shows how applications are scheduled on these new reconfigurable architectures. The extensions are implemented as specialized sequential or parallel instructions. They correspond to identified most frequently occurring computational patterns or other interesting patterns and are finally selected during mapping and scheduling. Our methods can handle both time-constrained and resource-constrained scheduling. Experimental results show that the presented method provides high coverage of application graphs with small number of patterns and ensures high application execution speed-up both for sequential and parallel application execution with processor extensions implementing selected patterns.,"Processor scheduling,
Application specific processors,
Computer architecture,
Registers,
Computer aided instruction,
Costs,
Instruction sets,
Computer science,
Application software,
Reconfigurable architectures"
Glosser: Enhanced Feedback for Student Writing Tasks,"We describe Glosser, a system that supports students in writing essays by 1) scaffolding their reflection with trigger questions, and 2) using text mining techniques to provide content clues that can help answer those questions. A comparison with other computer generated feedback and scorings systems is provided to explain the novelty of the approach. We evaluate the system with Wiki pages produced by postgraduate students as part of their assessment.","Feedback,
Writing,
Text mining,
Computer science education,
Taxonomy,
Reflection,
Software tools,
Costs,
Process design,
Technical drawing"
Collaborative Target Detection in Wireless Sensor Networks with Reactive Mobility,"Recent years have witnessed the deployments of wireless sensor networks in a class of mission-critical applications such as object detection and tracking. These applications often impose stringent QoS requirements including high detection probability, low false alarm rate and bounded detection delay. Although a dense all-static network may initially meet these QoS requirements, it does not adapt to unpredictable dynamics in network conditions (e.g., coverage holes caused by death of nodes) or physical environments (e.g., changed spatial distribution of events). This paper exploits reactive mobility to improve the target detection performance of wireless sensor networks. In our approach, mobile sensors collaborate with static sensors and move reactively to achieve the required detection performance. Specifically, mobile sensors initially remain stationary and are directed to move toward a possible target only when a detection consensus is reached by a group of sensors. The accuracy of final detection result is then improved as the measurements of mobile sensors have higher signal-to-noise ratios after the movement. We develop a sensor movement scheduling algorithm that achieves near-optimal system detection performance within a given detection delay bound. The effectiveness of our approach is validated by extensive simulations using the real data traces collected by 23 sensor nodes.","Collaboration,
Object detection,
Wireless sensor networks,
Delay,
Batteries,
Mission critical systems,
Application software,
Sensor systems,
Spatiotemporal phenomena,
Computer science"
Load Balancing Routing in Three Dimensional Wireless Networks,"Although most existing wireless systems and protocols are based on two-dimensional design, in reality, a variety of networks operate in three-dimensions. The design of protocols for 3D networks is surprisingly more difficult than the design of those for 2D networks. In this paper, we investigate how to design load balancing routing for 3D networks. Most current wireless routing protocols are based on Shortest Path Routing (SPR), where packets are delivered along the shortest route from a source to a destination. However, under uniform communication, shortest path routing suffers from uneven load distribution in the network, such as crowed center effect where the center nodes have more load than the nodes in the periphery. Aim to balance the load, we propose a novel 3D routing method, called 3D Circular Sailing Routing (CSR), which maps the 3D network onto a sphere and routes the packets based on the spherical distance on the sphere. We describe two mapping methods for CSR and then provide theoretical proofs of their competitiveness compared to SPR. For both proposed methods, we conduct simulations to study their performance in grid and random networks.","Load management,
Wireless networks,
Routing protocols,
Telecommunication traffic,
Wireless application protocol,
Peer to peer computing,
Wireless sensor networks,
Educational programs,
Communications Society,
Computer science"
Text Steganography by Changing Words Spelling,"One of the important issues in security fields is hidden exchange of information. There are different methods for this purpose such as cryptography and steganography. Steganography is a method of hiding data within a cover media so that other individuals fail to realize their existence. In this paper a new method for steganography in English texts is proposed. In this method the US and UK spellings of words substituted in order to hide data in an English text. For example ""color"" has different spelling in UK (colour) and US (color). Therefore the data can be hidden in the text by substituting these words.",
Myocardial Perfusion Characterization From Contrast Angiography Spectral Distribution,"Despite recovering a normal coronary flow after acute myocardial infarction, percutaneous coronary intervention does not guarantee a proper perfusion (irrigation) of the infarcted area. This damage in microcirculation integrity may detrimentally affect the patient survival. Visual assessment of the myocardium opacification in contrast angiography serves to define a subjective score of the microcirculation integrity myocardial blush analysis (MBA). Although MBA correlates with patient prognosis its visual assessment is a very difficult task that requires of a highly expertise training in order to achieve a good intraobserver and interobserver agreement. In this paper, we provide objective descriptors of the myocardium staining pattern by analyzing the spectrum of the image local statistics. The descriptors proposed discriminate among the different phenomena observed in the angiographic sequence and allow defining an objective score of the myocardial perfusion.",
Fast Correlation Based Filter (FCBF) with a different search strategy,"In this paper we describe an extension of the information theoretical FCBF (Fast Correlation Based Feature Selection) algorithm. The extension, called FCBF#, enables FCBF to select any given size of feature subset and it selects features in a different order than the FCBF. We find out that the extended FCBF algorithm results in more accurate classifiers.","Filters,
Machine learning algorithms,
Embedded computing,
Optimization methods,
Collaboration,
Statistics,
Pattern recognition,
Machine learning,
Bioinformatics,
Prediction algorithms"
Using 2D systems theory to design output signal based iterative learning control laws with experimental verification,"In this paper we use a 2D systems setting to develop new results on iterative learning control for linear plants, where it is well known in the subject area that a trade-off exists between speed of convergence and the response along the trials. Here we give new results by designing the control scheme using a strong form of stability for repetitive processes/2D linear systems known as stability along the pass (or trial). The resulting design computations are in terms of Linear Matrix Inequalities (LMIs) and they are also experimentally validated on a gantry robot. The control laws only use plant output information and hence the use of a state observer is avoided",
Quota-constrained test-case prioritization for regression testing of service-centric systems,"Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web Services, integrating Web Services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web Services’ request quotas (e.g., the upper limit of the number of requests that a user can send to a Web Service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using Integer Linear Programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.","Testing,
Web services,
Software,
Partitioning algorithms,
Integer linear programming,
Computer science,
Laboratories"
Object tracking from stereo sequences using particle filter,"In this paper we present a vehicle tracking particle filter system based on gray histogram and sparse optical flow detection in stereo images. The proposed approach is based on the fact that for 2D tracked features we can compute their 3D correspondences, which are used for particle filter tracking improvement. The goal of this paper is to show how vision based particle filter tracking, optical flow and stereovision can be integrated to work together in order to achieve a robust object tracking algorithm.","Vehicles,
Histograms,
Tracking,
Particle filters,
Predictive models,
Optical filters,
Image motion analysis"
Structural Invariant Subspaces of Singular Hamiltonian Systems and Nonrecursive Solutions of Finite-Horizon Optimal Control Problems,"This note introduces an analytic, nonrecursive approach to the solution of finite-horizon optimal control problems formulated for discrete- time stabilizable systems. The procedure, which adapts to handle both the case where the final state is weighted by a generic quadratic function and the case where the final state is an admissible, sharply assigned one, provides the optimal control sequences, as well as the corresponding optimal state trajectories, in closed form, as functions of time, by exploiting an original characterization of a pair of structural invariant subspaces associated to the singular Hamiltonian system. The results hold on the fairly general assumptions which guarantee the existence and uniqueness of the stabilizing solution of the corresponding discrete algebraic Riccati equation and, as a consequence, solvability of an appropriately defined symmetric Stein equation. Some issues to be considered in the numerical implementation of the proposed approach are mentioned. The application of the suggested methodology to H2 optimal rejection with preview is also discussed.","Optimal control,
Riccati equations,
Control systems,
Linear systems,
Electrical equipment industry,
Difference equations,
Weight control,
Computer science,
Symmetric matrices,
Lattices"
Parametric Timing Analysis for Complex Architectures,"Hard real-time systems have stringent timing constraints expressed in units of time. To ensure that a task finishes within its time-frame, the designer of sucha system must be able to derive upper bounds on the task's worst-case execution time (WCET). To compute such upper bounds, timing analyses are used. These analyses require that information such as bounds on the maximum numbers of loop iterations are known statically, i.e. during design time. Parametric timing analysis softens these requirements: it yields symbolic formulas instead of single numeric values representing the upper bound on the task's execution time. In this paper, we present a new parametric timing analysis that is able to derive safe and precise results. Our method determines what the parameters ofthe program are, constructs parametric loop bounds, takes processor behavior into account and attains a formula automatically. In the end, we present tests to show that the precision and runtime of our analysis are very close to those of numeric timing analysis.","Timing,
Upper bound,
Registers,
Radiation detectors,
Pipelines,
Vectors,
Real time systems"
Design of a recommender system for mobile tourism multimedia selection,"Applications that deliver multimedia content to and display such content on mobile devices have become increasingly common in recent years. When faced with a large amount of content, unfamiliar users can make use of each otherspsila recommendations, through recommender systems, to find content of interest to them. As a case in point we present the design of a recommender system that can be used by tourists to request a travel itinerary, and subsequently browse multimedia content for each recommended tourist spot. The techniques combined in our recommender system include genetic algorithms and fuzzy logic. Recommendations are chosen to match a user profile based on a userpsilas personal preferences. Our system design has wide applicability in multimedia systems where the user requires assistance in content selection.","Recommender systems,
Multimedia systems,
Fuzzy logic,
Genetic algorithms,
Mobile computing,
Job shop scheduling,
Computer displays,
Algorithm design and analysis,
Processor scheduling,
Ubiquitous computing"
Network Coding via Opportunistic Forwarding in Wireless Mesh Networks,"Network coding has been used to increase transportation capabilities in wireless mesh networks. In mesh networks, the coding opportunities depend on the co-location of multiple traffic flows. With fixed routes given by a routing protocol, the coding opportunities are limited. This paper presents a new protocol called BEND, which combines the features of network coding and opportunistic forwarding in 802.11-based mesh networks to create more coding opportunities in the network. Taking advantage of redundancy of packets among the forwarder candidates, our protocol bends the routes locally and dynamically to attain better coding opportunities. This higher coding gain is verified using a network simulator.","Network coding,
Wireless mesh networks,
Mesh networks,
Telecommunication traffic,
Routing protocols,
Throughput,
Unicast,
Topology,
Communications Society,
Computer science"
A predictive power control scheme for energy efficient state estimation via wireless sensor networks,"We investigate state estimation via wireless sensor networks over fading channels causing random packet loss. Packet loss probabilities depend upon the time-varying channel gains and transmission power levels used by the sensors. We develop a predictive controller which trades off sensor energy expenditure versus state estimation accuracy. The latter is measured by the expected value of the future covariance matrices provided by the associated time-varying Kalman Filter. To further conserve energy at the sensors, the controller is located at the gateway and sends coarsely quantized power increment commands, only whenever necessary. Simulations based on real channel measurements show that the proposed approach gives excellent results.","Power control,
Energy efficiency,
State estimation,
Wireless sensor networks,
Fading,
Propagation losses,
Time-varying channels,
Communication system control,
Actuators,
Sockets"
Computer control system and walking pattern control for a humanoid robot,"A humanoid robot generally has more than thirty DOFs to be controlled in real-time and needs to deal with information of multiple sensors such as encoders, force and moment sensors, inertial attitude sensors and vision sensors. Therefore an effective control system is crucial for the humanoid robot. In this paper, we propose a distributed computer system consisting of the online planning sub-system and the real-time motion control sub-system based on CAN bus and Ethernet for humanoid robots. CAN bus is used for distributed real-time motion control and Ethernet is used for non-real-time and large data quantities communication between the online planning and motion control sub-systems. The Windows and RT-Linux are used as operating systems for the online planning and motion control sub-systems respectively. The walking pattern control modifies the planned walking pattern based on sensory information when there are unexpected sudden events. The effectiveness of our proposed computer system and walking pattern control method was confirmed by walking experiments on our newly-built humanoid robot.","Joints,
Computers,
Humanoid robots,
Motion control,
Real time systems,
Sensors,
Robots"
Explaining and Reformulating Authority Flow Queries,"Authority flow is an effective ranking mechanism for answering queries on a broad class of data. Systems have been developed to apply this principle on the Web (PageRank and topic sensitive PageRank), bibliographic databases (ObjectRank), and biological databases (Hubs of Knowledge project). However, these systems have the following drawbacks: (a) There is no way to explain to the user why a particular result received its current score; (b) The authority flow rates, which have been shown to dramatically affect the results' quality in ObjectRank, have to be set manually by a domain expert; (c) There is no query reformulation methodology to refine the query results according to the user's preferences. In this work, we address these shortcomings by introducing a framework and algorithms to explain query results and reformulate authority flow queries based on the user's feedback. The query reformulation process can be used to learn the user's preferences and automatically adjust the authority flow rates to facilitate personalized authority flow searching. We experimentally evaluate our algorithms in terms of performance and quality.","Databases,
Feedback,
Navigation,
Web search,
Information retrieval,
Damping"
A Virtual Password Scheme to Protect Passwords,"In this paper, we discuss how to prevent users' passwords from being stolen by adversaries. We propose a virtual password concept involving a small amount of human computing to secure users' passwords in on-line environments. We adopt user-determined randomized linear generation functions to secure users' passwords based on the fact that a server has more information than any adversary does. We analyze how the proposed scheme defends against phishing, key logger, and shoulder-surfing attacks.","Protection,
Humans,
Internet,
Authentication,
Protocols,
Invasive software,
Communications Society,
Computer science,
Information technology,
Web server"
A Connectivity Improving Mechanism for ZigBee Wireless Sensor Networks,"ZigBee is an IEEE 802.15.4-based standard for wireless sensor network. The operations of ZigBee rely on the deployment of wireless sensor devices and their self-organized network connections. In some of applications, sensor devices are randomly deployed and some of these devices may become isolated from the network due to the constraints of ZigBee network configuration parameters. The isolated devices will cause the expected network operations unreached and waste the corresponding deployment costs. This paper proposes a connectivity improving mechanism which utilizes a connection shifting scheme to increase the join ratio of deployed devices for achieving the expected ZigBee network operations and consequently reduce the waste of deployment. The result of network simulation shows the connectivity was improved in ZigBee networks by using the proposed mechanism.","ZigBee,
Wireless sensor networks,
Costs,
Network topology,
Ubiquitous computing,
Computer science,
IP networks,
Zirconium,
Routing protocols,
Physical layer"
Scheduling advance reservation requests for wavelength division multiplexed networks with static traffic demands,"Telecommunication and grid computing applications demand high bandwidth data channels that offer guarantees with respect to service availability. Such applications include: remote surgery, remote experimentation, video on-demand, teleconferencing and bulk transfers. Furthermore, by forecasting traffic patterns Internet service providers attempt to optimise network resources in order to lower operational costs during peak periods of bandwidth consumption. Advance reservation for wavelength division multiplexed networks can address some of these issues by reserving high volume communication channels (i.e. lightpaths) beforehand. The authors develop a mathematical model to solve the problem of scheduling lightpaths in advance. The optimal solution is presented as a mixed integer linear program with the assumption that all traffic is static and the network is centrally controlled. Furthermore, we have developed two novel meta-heuristics based on: 1) a greedy implementation (local search) and 2) simulated annealing. The meta-heuristics have shown to produce good approximate solutions in a reasonable amount of time.","wavelength division multiplexing,
bandwidth allocation,
grid computing,
integer programming,
linear programming,
scheduling,
simulated annealing,
telecommunication channels,
telecommunication traffic"
Hand Tracking by Particle Filtering with Elite Particles Mean Shift,"This paper presents an improved particle filtering for hand tracking in a video sequence in real time. In this method, we incorporate mean shift process of elite particles into conventional particle filtering for hand tacking which aims to improve the tracking efficiency. The elite particles are chosen from the particles with higher weights based on observation model. The proposed method can significantly improve the tracking performance, as well as reducing the computational cost compared to conventional particle filtering. Experiments result of the hand tracking show the effectiveness of proposed method.","Particle tracking,
Computational efficiency,
Paper technology,
State estimation,
Sampling methods,
Computer science,
Information filtering,
Information filters,
Computer vision,
Recursive estimation"
An Approach to Software Architecture Testing,"Software architecture has emerged as an important sub-discipline of software engineering. This paper proposes a novel software architecture testing technology using π calculus. π calculus is of rigorous mathematical foundation and well-defined semantics. Petri net provides a graphical description technique that is easy to understand and carry on analysis. We make π calculus combined with Petri net and propose mapping relationship based on them. Then we will introduce π Behavior Graph using the mapping relationship and π-ADL that takes its roots in previous works concerning the use of π calculus as semantic foundation for architecture description languages. We propose seven testing coverage criteria from black-box and white-box perspectives and give its formal description in πBG respectively. At last, we build a prototype tool that implements the π-ADL approach to the specification of software architecture and generates test paths. We use TRMCS as a typical architecture model to illustrate our testing technology.","Software architecture,
Software testing,
Calculus,
Computer architecture,
Educational institutions,
Computer science,
Mobile communication,
Software engineering,
Architecture description languages,
Software prototyping"
Image tampering detection by blocking periodicity analysis in JPEG compressed images,"Since JPEG image format has been a popularly used image compression standard, tampering detection in JPEG images now plays an important role. The artifacts introduced by lossy JPEG compression can be seen as an inherent signature for compressed images. In this paper, we propose a new approach to analyse the blocking periodicity by, 1) developing a linearly dependency model of pixel differences, 2) constructing a probability map of each pixel’s belonging to this model, and 3) finally extracting a peak window from the Fourier spectrum of the probability map. We will show that, for single and double compressed images, their peaks’ energy distribution behave very differently. We exploit this property and derive statistic features from peak windows to classify whether an image has been tampered by cropping and recompression. Experimental results demonstrate the validity of the proposed approach.","Pixel,
Image coding,
Transform coding,
Estimation,
Q factor,
Quantization,
Feature extraction"
Verifying the Safety of User Pointer Dereferences,"Operating systems divide virtual memory addresses into kernel space and user space. The interface of a modern operating system consists of a set of system call procedures that may take pointer arguments called user pointers. It is safe to dereference a user pointer if and only if it points into user space. If the operating system dereferences a user pointer that does not point into user space, then a malicious user application could gain control of the operating system, reveal sensitive data from kernel space, or crash the machine. Because the operating system cannot trust user processes, the operating system must check that the user pointer points to user space before dereferencing it. In this paper, we present a scalable and precise static analysis capable of verifying the absence of unchecked user pointer dereferences. We evaluate an implementation of our analysis on the entire Linux operating system with over 6.2 million lines of code with false alarms reported on only 0.05% of dereference sites.","Safety,
Operating systems,
Scalability,
Kernel,
Linux,
Computer science,
Gain control,
Computer crashes,
Information analysis,
Computer security"
Text Categorization Based on LDA and SVM,"Text categorization aims to assign text documents to predefined categories. In this paper, a novel text categorization algorithm that combines the LDA and SVM is proposed. The core idea of the algorithm is as follows: The high dimension text data set are first projected into a lower-dimensional text subspace. Then the SVM classifier algorithm is applied to classify the text. Experimental results on two text benchmark data sets demonstrate the effectiveness of the proposed text classification algorithm.","Text categorization,
Linear discriminant analysis,
Support vector machines,
Support vector machine classification,
Data mining,
Classification algorithms,
Large scale integration,
Information retrieval,
Educational institutions,
Space technology"
Authentication Mechanisms for Mobile Ad-Hoc Networks and Resistance to Sybil Attack,"In Sybil attack, an attacker acquires multiple identities and uses them simultaneously or one by one to attack various operation of the network. Such attacks pose a serious threat to the security of self-organized networks like mobile ad-hoc networks (MANETs) that require unique and unchangeable identity per node for detecting routing misbehavior and reliable computation of node's reputation. The purpose of this paper is to analyze the effectiveness of current authentication mechanism for MANETs in coping with the Sybil attack, the infrastructure requirement posed by these mechanisms and applicability of these mechanisms to different kinds of ad hoc networks. We identify open research issues that need to be addressed by the next generation of authentication mechanisms for MANETs.","Ad hoc networks,
Mobile computing,
Peer to peer computing,
Authentication,
Cryptography,
IP networks,
Resistance"
Using opposition-based learning to improve the performance of particle swarm optimization,"Particle swarm optimization (PSO) is a stochastic, population-based optimization method, which has been applied successfully to a wide range of problems. However, PSO is computationally expensive and suffers from premature convergence. In this paper, opposition-based learning is used to improve the performance of PSO. The performance of the proposed approaches is investigated and compared with PSO when applied to eight benchmark functions. The experiments conducted show that opposition-based learning improves the performance of PSO.","Particle swarm optimization,
Stochastic processes,
Optimization methods,
Convergence,
USA Councils,
Genetic mutations,
Birds,
Computer science,
Acceleration"
Scratch: Applications in Computer Science 1,"Programming is a complex intellectual activity. We observed through the years that it is difficult for some students to understand the logic of a program and to familiarize themselves with the control structures. In order to help smooth this initial relationship with programming, we tried Scratch (a programming language designed for young people, developed by MIT’s Media Lab). We analyze the use of Scratch in two Computer Science 1 courses: one in university degree and other in vocational studies. We use this tool in the very first weeks of those courses with the purpose of improving students’ programming experiences and motivation, and also to detect its influence, if any, in scores and dropout rates in comparison with normal courses. We developed detailed lab guides, exercises, tests and questions forms. We contrast the results with normal courses and found that students who used Scratch expressed higher motivation but there was no statistically evidence of differences neither in dropout rates nor obtained scores. In this paper we present the detailed courses, the experimentation and the results. We offer some conclusions and reflections over the particular value of including this kind of tool. We include suggestions with the intention of improving Scratch.","Application software,
Computer science,
Testing,
Education,
Java,
Logic programming,
Computer languages,
Reflection,
Object oriented programming,
Proposals"
Bipartite Graph Based Dynamic Spectrum Allocation for Wireless Mesh Networks,"The capacity of a wireless mesh network can be improved by equipping mesh nodes with multi-radios tuned to non-overlapping channels. By letting these nodes utilize the available spectrum opportunistically, we can increase the utilization of the available bandwidth in the spectrum space. The key problem is how to allocate the spectrum to these multi-radio nodes, especially when they are heterogeneous with diverse transmission types and bandwidth. Most of current work has been based on the conflict-graph model and given solutions that focused on either increasing bandwidth utilization or minimizing starvation. In this paper, we propose a new bipartite-graph based model and design an channel allocation algorithm that considers both bandwidth utilization and starvation problems. Our solution is based on using augmenting path to find a matching in the bipartite-graph and can minimize starvation and then maximize the bandwidth utilization. The simulations demonstrate that our algorithm can reduce the starvation ratio and improve the bandwidth utilization, compared with previous conflict-graph based algorithms.","wireless channels,
channel allocation,
graph theory,
radio spectrum management"
Automatic Recognition of Ship Types from Infrared Images Using Support Vector Machines,"In this paper, we present a system addressing autonomous recognition of ship types in infrared images. Firstly, segmentation is implemented after the target region is automatically found based on detection of salient features of the target. Feature extraction is then accomplished as the moment functions for both the target boundary and the solid silhouette are used as the featureset. Lastly, the classification method based on Support Vector Machines (SVMs) is adopted in the recognition stage, as the training sets are obtained through projections of three-dimensional ship models designed by investigators of Naval Postgraduate School. The system was implemented and experimentally validated using both simulated three-dimensional ship model images and real images derived from video of an AN/AAS-44V ForwardLooking Infrared(FLIR) sensor. Moreover, our proposed system is general and can be generalized for other similar pattern recognition applications.","Image recognition,
Marine vehicles,
Infrared imaging,
Support vector machines,
Image segmentation,
Computer vision,
Feature extraction,
Solids,
Support vector machine classification,
Infrared image sensors"
Alternative Energy Sources for Sensor Nodes: Rationalized Design for Long-Term Deployment,"Energy harvesting is a means of extending the lifetimes of wireless sensor nodes. Here, we describe the current state-of-the-art in energy harvesting technologies, and compare them against long-life primary batteries in terms of their total energy, economic cost and environmental impact. Issues affecting the lifetimes of energy harvesting devices, which are often overlooked, are described. We discuss the requirements for energy-awareness by wireless sensor network management algorithms, and how to deliver it for systems using batteries or energy harvesting devices and supercapacitors. A novel approach to monitoring state-of-charge, and an embedded software architecture for energy management (which has been deployed on battery-powered and energy-harvesting nodes), are introduced. This new `energy stack' structures the node's energy-related operations, while hiding their complexity from the application layer, and providing a straightforward interface. We present a complete approach to designing the energy-related aspects of a node for long-term deployment, including hardware choices and embedded software design.","Wireless sensor networks,
Energy management,
Embedded software,
Power generation economics,
Environmental economics,
Costs,
Battery management systems,
Supercapacitors,
Monitoring,
Computer architecture"
Enabling Streaming Remoting on Embedded Dual-Core Processors,"Dual-core processors (and, to an extent, multicore processors) have been adopted in recent years to provide platforms that satisfy the performance requirements of popular multimedia applications. This architecture comprises groups of processing units connected by various interprocess communication mechanisms such as shared memory, memory mapping interrupts, mailboxes, and channel-based protocols. The associated challenges include how to provide programming models and environments for developing streaming applications for such platforms. In this paper, we present middleware called streaming RPC for supporting a streaming-function remoting mechanism on asymmetric dual-core architectures. This middleware has been implemented both on an experimental platform known asthe PAC dual-core platform and in TI OMAP dual-core environments. We also present an analytic model of streaming equations to optimize the internal handshaking for our proposed streaming RPC. The usage and efficiency of the proposed methodology are demonstrated in a JPEG decoder, MP3 decoder, and QCIF H.264 decoder. The experimental results show that our approach improves the performance of the decoders of JPEG, MP3, and H.264 by 24%, 38%, and 32% on PAC, respectively. The communication load of internal handshaking has also been reduced compared to the naive use of RPC over embedded dual-core systems. The experiments also show that the performance improvement can also be achieved on OMAP dual-core platforms.","Transmitters,
Servers,
Program processors,
Programming,
Protocols,
Decoding,
Operating systems"
A Look-Ahead Task Management Unit for Embedded Multi-Core Architectures,"Efficient utilization of multi-core architectures relies on the partitioning of applications into tasks and mapping the tasks to cores. In some applications (e.g. H.264 video decoding parallelized at macro-block level) these tasks have dependencies among each other. Task scheduling, consisting of selecting a task with satisfied dependencies and mapping it to a core, is typically a functionality delegated to the Operating System. In this paper we present a hardware Task Management Unit (TMU) that looks ahead in time to find tasks to be executed by a multi-core architecture. The look-ahead functionality is shown to reduce the task management overhead by 40-50% when executing a parallelized version of an H.264 video decoder on an architecture with up to 16 cores. In overall, the TMU-based multi-core architecture reaches a speedup of more than 14x on 16 cores running H.264 video decoding, assuming CABAC is implemented in a dedicated coprocessor.","Computer architecture,
Application software,
Decoding,
Hardware,
Digital systems,
Design methodology,
Conference management,
Technology management,
Engineering management,
Computer science"
AREX: An Adaptive System for Secure Resource Access in Mobile P2P Systems,"In open environments, such as mobile peer-to-peer systems, participants may need to access resources from unknown users. A critical security concern in such systems is the access of faulty  resources, thereby wasting the requester's time and energy and possibly causing damage to her system. A common approach to mitigating the problem involves reputation mechanisms; however, since reputation relies on cooperation, a reputation mechanism's effectiveness can be significantly diminished in hostile environments.  Reputation systems also require substantial communication among peers leading to: i) vulnerability to errors caused by intermittent connectivity; ii) message delivery disruptions caused by malicious peers; and iii) energy sapping message overheads. In this paper, we present AREX, a low-cost, adaptive mechanism designed to provide security for peers in hostile and uncertain environments, which are common in mobile P2P systems. AREX features an adaptive exploration strategy that increases the system's utility for benign peers and decreases the system’s utility for malicious peers. AREX reduces vulnerabilities and energy costs by operating without communication between peers. Through simulation, we demonstrate AREX's ability to reduce energy costs, protect benign peers, and diminish malicious peers' motivation to attack in a variety of hostile environments.","Mathematical model,
Equations,
Peer to peer computing,
Nash equilibrium,
Mobile communication,
Security,
Payloads"
Delay tolerant network routing: Beyond epidemic routing,"In this paper, we identify two distinct classes of routing algorithms for Delay or Disruption Tolerant Networks (DTN). The purpose of this classification is to clearly delineate the assumptions they work under and to facilitate mapping of applications to these algorithms. Algorithms based on opportunistic contact and some variant of epidemic routing use minimal topology knowledge and the most resources due to replication. The island-based algorithms find routes between connected islands and are closer to real applications such as tactical military networks. The general consensus is that there is no single routing solution that will minimize delay at the same maximizing throughput for DTNs. Majority of the algorithms assume non-standard and diverse scenarios which makes comparative evaluation difficult. Mapping applications to algorithms also poses a problem as many of the known applications do not match in scale the rigor of the proposed algorithms. Further efforts in standardization and verifiable evaluation using application context are the way forward.","Routing,
Disruption tolerant networking,
Delay,
Prototypes,
Network topology,
Military computing,
Educational institutions,
Digital relays,
Telecommunication network topology,
Wildlife"
Engaging E-Learning in Virtual Worlds: Supporting Group Collaboration,"Current e-learning environments do not provide sufficient support for group collaboration. The absence of the visual identification of the users makes effective collaboration in e-learning very difficult. This paper argues that virtual worlds (VW) possess the necessary tools to foster effective group collaboration for e-learning initiatives. The use of avatars, the support of verbal and non-verbal communications and creative capabilities offered in VWs are suggested as the key elements that promote effective group learning.","Electronic learning,
Games,
International collaboration,
Collaborative tools,
Collaborative work,
Avatars,
Computer aided instruction,
Optimization methods,
Computer science education,
Online Communities/Technical Collaboration"
Motion Compensated Fan-Beam Reconstruction for Nonrigid Transformation,"We develop an approximate fan-beam algorithm to reconstruct an object with time-dependent nonrigid transformation such as the heart. The method is in the form of derivative back- projection filtering with compensation of affine transformations on a local basis. Computer simulations showed the proposed method significantly reduces image artifact due to nonrigid motion. Therefore, with very little motion artifact, the proposed method allowed us to reconstruct images from projections over about one motion cycle, resulting in reduced image noise level down to 40% of the current level.","Image reconstruction,
Computed tomography,
Heart,
X-ray imaging,
Biomedical imaging,
Cardiac disease,
Radiology,
Motion estimation,
Noise reduction,
Filtering"
MC2: Multiple Clients on a Multilevel Cache,"In today's networked storage environment, it is common to have a hierarchy of caches where the lower levels of the hierarchy are accessed by multiple clients. This sharing can have both positive or negative effects. While data fetched by one client can be used by another client without incurring additional delays, clients competing for cache buffers can evict each other's blocks and interfere with exclusive caching schemes. Our algorithm, MC2, combines local, per client management with a global, system-wide, scheme, to emphasize the positive effects of sharing and reduce the negative ones. The local scheme uses readily available information about the client's future access profile to save the most valuable blocks, and to choose the best replacement policy for them. The global scheme uses the same information to divide the shared cache space between clients, and to manage this space. Exclusive caching is maintained for non-shared data and is disabled when sharing is identified. Our simulation results show that the combined algorithm significantly reduces the overall I/O response times of the system.","Resource management,
Gain,
Time factors,
Benchmark testing,
Delay,
Indexes,
Partitioning algorithms"
Improving Expected Transmission Time Metric in Multi-Rate Multi-Hop Networks,"Routing metrics are critical to select paths with maximal throughput in multi-rate multi-hop wireless networks. Simple path selection based on minimal hop count often leads to poor performance due to the fact that paths with low hop count often have higher packet loss rates. Better paths can be obtained by characterizing the actual quality of wireless link. A number of link quality aware routing metrics such as per hop round trip time (RTT), expected transmission count (ETX), expected transmission time (ETT) have been developed and explored. This study highlights some shortcomings of these routing metrics and proposes the design of a novel metric called improved ETT metric, or iETT, that addresses the discussed weaknesses and works more efficiently under various link quality conditions. The performance of the iETT metric is compared against the normal ETT metric over a DSR-based routing protocol. In extensive simulations, iETT metric outperforms the normal ETT metric in terms of network throughput and average packet delay.","Spread spectrum communication,
Throughput,
Wireless networks,
Performance loss,
Routing protocols,
Computer science,
Propagation losses,
Bandwidth,
Delay,
Software engineering"
A Mixed Reality Approach for Merging Abstract and Concrete Knowledge,"Mixed reality's (MR) ability to merge real and virtual spaces is applied to merging different knowledge types, such as abstract and concrete knowledge. To evaluate whether the merging of knowledge types can benefit learning, MR was applied to an interesting problem in anesthesia machine education. The virtual anesthesia machine (VAM) is an interactive, abstract 2D transparent reality simulation of the internal components and invisible gas flows of an anesthesia machine. It is widely used in anesthesia education. However when presented with an anesthesia machine, some students have difficulty transferring abstract VAM knowledge to the concrete real device. This paper presents the augmented anesthesia machine (AAM). The AAM applies a magic-lens approach to combine the VAM simulation and a real anesthesia machine. The AAM allows students to interact with the real anesthesia machine while visualizing how these interactions affect the internal components and invisible gas flows in the real world context. To evaluate the AAM's learning benefits, a user study was conducted. Twenty participants were divided into either the VAM (abstract only) or AAM (concrete+abstract) conditions. The results of the study show that MR can help users bridge their abstract and concrete knowledge, thereby improving their knowledge transfer into real world domains.","Virtual reality,
Merging,
Concrete,
Anesthesia,
Active appearance model,
Fluid flow,
Machine learning,
Visualization,
Bridges,
Knowledge transfer"
Computer Forensics in Forensis,"Different users apply computer forensic systems, models, and terminology in very different ways. They often make incompatible assumptions and reach different conclusions about the validity and accuracy of the methods they use to log, audit, and present forensic data. This is problematic, because these fields are related, and results from one can be meaningful to the others. We present several forensic systems and discuss situations in which they produce valid and accurate conclusions and also situations in which their accuracy is suspect. We also present forensic models and discuss areas in which they are useful and areas in which they could be augmented. Finally, we present some recommendations about how computer scientists, forensic practitioners, lawyers, and judges could build more complete models of forensics that take into account appropriate legal details and lead to scientifically valid forensic analysis.","Law,
Legal factors,
Digital forensics,
Computer science,
Testing,
Terrorism,
DNA computing,
Computer errors,
Educational institutions,
Chemical analysis"
Improved fingerprint image segmentation using new modified gradient based technique,"An important step in fingerprint recognition is the segmentation of the region of interest (ROI). The objective of fingerprint segmentation is to extract the region of interest (ROI) which contains the desired fingerprint impression. Fingerprint image segmentation highly influences the performances of Automatic Fingerprint Identification System (AFIS). We present in this paper, a Modified Gradient Based Method to extract ROI. The distinct feature of our technique is that it gives high accurate segmentation percentage for fingerprint images even in case of low quality fingerprint images. The proposed algorithm is applied on FVC2004 database. Experimental results demonstrate the improved performance of the proposed scheme.","Fingerprint recognition,
Image matching,
Image segmentation,
Feature extraction,
Educational institutions,
Mechanical engineering,
Histograms,
Classification algorithms,
Image databases,
Spatial databases"
Distance estimation algorithm for both long and short ranges based on stereo vision system,"We present a distance measurement method based on stereo vision system while guaranteeing accuracy and reliability. It has been considered as difficult problem to measure both long and short distance with a stereo vision system accurately due to sampling error and camera sensor error. To resolve these problems of the stereo vision system, we utilize an algorithm which is consisted of a modified sub-pixel displacement method to enhance the accuracy of disparity and Strong Tracking Kalman filter (STKF) to reduce the camera sensor errors. Our displacement method and the usefulness of STKF are verified as compared to other displacement methods and Conventional Kalman filter (CKF) through simulating on the several distance ranges. The Monte-Carlo simulation results show that our algorithm is capable of measuring up to hundreds of meters while root mean square error (RMSE) maintains about 0.04 at all ranges, even though the target vehicle maneuvers or moves nonlinearly.",Vehicles
Selecting and Assessing Quantitative Early Ultrasound Texture Measures for Their Association With Cerebral Palsy,"Cerebral palsy (CP) develops as a consequence of white matter damage (WMD) in approximately one out of every 10 very preterm infants. Ultrasound (US) is widely used to screen for a variety of brain injuries in this patient population, but early US often fails to detect WMD. We hypothesized that quantitative texture measures on US images obtained within one week of birth are associated with the subsequent development of CP. In this retrospective study, using images from a variety of US machines, we extracted unique texture measures by means of adaptive processing and high resolution feature enhancement. We did not standardize the images, but used patients as their own controls. We did not remove speckle, as it may contain information. To test our hypothesis, we used the ldquorandom forestrdquo algorithm to create a model. The random forest classifier achieved a 72% match to the health outcome of the patients (CP versus no CP), whereas designating all patients as having CP would have resulted in 53% error. This suggests that quantitative early texture measures contain diagnostic information relevant to the development of CP.","Ultrasonic imaging,
Ultrasonic variables measurement,
Birth disorders,
Pediatrics,
Speckle,
Liver diseases,
Testing,
Magnetic resonance imaging,
Radiology,
Brain injuries"
A resource management framework for multi-tier service delivery in autonomic virtualized environments,"Large data centers usually host many different services on a shared computing infrastructure, for which on-demand resource management is necessary to maximize providers’ revenues by meeting service quality targets at least operational cost. This paper presents a novel architecture of autonomic resource management framework based on virtualized Service-Oriented Computing (SOC) environment. A non-linear continuous optimization problem is defined for adaptive resource allocation and a model-based approach is adopted to solve this problem. Different from traditional approaches, the analytic model we established provides probabilistic performance guarantees and considers non-steady-state behavior assisted by admission control. Results of prototype experiments demonstrate that the performance of multiple services has been greatly improved by taking advantage of fine-grained resource sharing, while incurring much lower resource usage cost. Also, differentiated service qualities could be provided to different client classes through our dynamic resource allocation scheme.","Resource management,
Resource virtualization,
Network servers,
Costs,
Quality of service,
Local area networks,
Web server,
Admission control,
Application specific processors,
Java"
Person name extraction from Turkish financial news text using local grammar-based approach,"Local grammar approach relies on constructing polylexical units having frozen characteristics. It has recently been shown to be superior to other named entity extraction approaches including the probabilistic, the symbolic, and the hybrid approach in terms of being able to work with untagged corpora and has successfully been applied to English, Portuguese, Korean, French and Chinese texts. In this paper, we evaluated local grammar-based approach on Turkish financial texts. We have found that although the method is successful in finding person names, the construction of frozen expressions for person name extraction is rather difficult, which can be attributed to that of Turkish word formations.","Data mining,
Natural languages,
Vocabulary,
Information systems,
Informatics,
Pattern recognition,
Machine learning,
Dictionaries,
Testing"
Dynamic spectrum access with learning for cognitive radio,"We study the problem of cooperative dynamic spectrum sensing and access in cognitive radio systems as a partially observed Markov decision process (POMDP). Assuming Markovian state-evolutions for the primary channels, we propose a greedy channel selection and access policy that satisfies an interference constraint and also outperforms some existing schemes in average throughput. When the distribution of the signal from the primary is unknown and belongs to a parameterized family, we develop an algorithm that can learn the parameter of the distribution still guaranteeing the interference constraint. This algorithm also outperforms the popular approach that assumes a worst-case value for the parameter thus illustrating the sub-optimality of the popular worst-case approach.","Cognitive radio,
Statistics,
Interference constraints,
Throughput,
Bandwidth,
Condition monitoring,
Probability,
Radio transmitters,
Uncertainty"
A prefetching scheme for energy conservation in parallel disk systems,"Large-scale parallel disk systems are frequently used to meet the demands of information systems requiring high storage capacities. A critical problem with these large-scale parallel disk systems is the fact that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel disk systems, we developed two energy-aware prefetching strategies for parallel disk systems with disk buffers. First, we introduce a new buffer disk architecture that can provide significant energy savings for parallel disk systems while achieving high performance. Second, we design a prefetching approach to utilize an extra disk to accommodate prefetched data sets that are frequently accessed. Third, we develop a second prefetching strategy that makes use of an existing disk in the parallel disk system as a buffer disk. Compared with the first prefetching scheme, the second approach lowers the capacity of the parallel disk system. However, the second approach is more cost-effective and energy-efficient than the first prefetching technique. Finally, we quantitatively compare both of our prefetching approaches against two conventional strategies including a dynamic power management technique and a non-energy-aware scheme. Using empirical results we show that our novel prefetching approaches are able to reduce energy dissipation in parallel disk systems by 44% and 50% when compared against a non-energy aware approach. Similarly, our strategies are capable of conserving 22% and 30% of the energy when compared to the dynamic power management technique.","Prefetching,
Energy conservation,
Large-scale systems,
Energy management,
Power system management,
Information systems,
Power generation economics,
Environmental economics,
Energy efficiency,
Energy dissipation"
Automatic Partitioning of Object-Oriented Programs for Resource-Constrained Mobile Devices with Multiple Distribution Objectives,"We describe a system that takes monolithic Java programs as its input and automatically converts them into distributed Java programs. Our research is situated in the context of resource-constrained mobile devices, in which there are often multiple distribution objectives, such as minimizing energy consumption on mobile devices by offloading workloads to a stationary server, or minimizing total execution time. Our method initially constructs an object relation graph (ORG), using a combination of static analysis and offline profiling. Instead of directly partitioning this ORG, we then transform it into a target graph (TG) to abstract from concrete distribution infrastructures and objectives. By applying this two-layer graph modeling, we achieve a unified strategy for different partitioning goals. Preliminary benchmarks for our prototype implementation are highly promising, with an average speedup factor of almost 1.5 and an average energy savings of 83.5% for the beneficial benchmarks.","Java,
Distributed computing,
Mobile computing,
Object oriented modeling,
Data structures,
Personal digital assistants,
Mobile handsets,
Computer science,
USA Councils,
Energy consumption"
Scalability Analysis of the Hierarchical Architecture for Distributed Virtual Environments,"A distributed virtual environment (DVE) is a shared virtual environment where multiple users at their workstations interact with each other over a network. Some of these systems may support a large number of users, for example, multiplayer online games. An important issue is how well the system scales as the number of users increases. In terms of scalability, a promising system architecture is a two-level hierarchical architecture. At the lower level, multiple servers are deployed; each server interacts with its assigned users. At the higher level, the servers ensure that their copies of the virtual environment are as consistent as possible. Although the two-level architecture is believed to have good properties with respect to scalability, not much is known about its performance characteristics. In this paper, we develop a performance model for the two-level architecture and obtain analytic results on the workload experienced by each server. Our results provide valuable insights into the scalability of the architecture. We also investigate the issue of consistency and develop a novel technique to achieve weak consistency among copies of the virtual environment at the various servers. Simulation results on the consistency/scalability trade-off are presented.","Scalability,
Virtual environment,
Delay,
Workstations,
Performance analysis,
Avatars,
Network servers,
Web server,
Multimedia systems,
Information systems"
Communication Avoiding Gaussian elimination,"We present CALU, a Communication Avoiding algorithm for the LU factorization of dense matrices distributed in a two-dimensional cyclic layout. The algorithm is based on a new pivoting strategy, which is stable in practice. The new algorithm is optimal (up to polylogarithmic factors) in the amount of communication it performs. Our experiments show that CALU leads to a reduction in the parallel time, in particular when the latency time is an important factor of the overall time. The factorization of a block-column, a subroutine of CALU, outperforms the corresponding routine PDGETF2 from ScaLAPACK up to a factor of 4.37 on an IBM POWER 5 system and up to a factor of 5.58 on a Cray XT4 system. On square matrices of order 104, CALU outperforms the corresponding routine PDGETRF from ScaLAPACK by a factor of 1.24 on IBM POWER 5 and by a factor of 1.31 on Cray XT4.","Delay,
Algorithms"
Power Attack Resistant Efficient FPGA Architecture for Karatsuba Multiplier,"The paper presents an architecture to implement Karatsuba Multiplier on an FPGA platform. Detailed analysis has been carried out on how existing algorithms utilize FPGA resources. Based on the observations the work develops a hybrid technique which has a better area delay product compared to the known algorithms. The results have been practically demonstrated through a large number of experiments. Subsequently, the work develops a masking strategy to prevent power based side channel attacks on the multiplier. It has been found that the proposed masked Hybrid Karatsuba multiplier is more compact compared to existing designs.","Field programmable gate arrays,
Galois fields,
Elliptic curve cryptography,
Public key cryptography,
Polynomials,
Elliptic curves,
Arithmetic,
Computer architecture,
Computer science,
Design engineering"
Bridging the gap between nanomagnetic devices and circuits,"This paper looks at designing circuit elements that will be constructed with nanoscale magnets within the Quantum-dot Cellular Automata (QCA) computational paradigm. In magnetic QCA (MQCA) logical operations and dataflow are accomplished by manipulating the polarizations of nanoscale magnets. Wires and gates have already been experimentally demonstrated at room temperature. However, to realize more complex circuits - and eventually systems - more than just wires and gates in isolation are required. For example, gates must be inter-connected, signals must cross, etc. All structures must be controlled by the envisioned drive circuitry. In this paper, structures that will facilitate these circuit-level tasks are presented for the first time.","Nanoscale devices,
Quantum cellular automata,
Magnets,
Wires,
Clocks,
Magnetic circuits,
Quantum dots,
CMOS technology,
Shape,
Lithography"
The Brave New World of Multiplayer Online Games: Synchronization Issues with Smart Solutions,"The technology supporting Multiplayer Online Games (MOGs) has greatly improved in the past few years. This represents great news both for players all over the world and for researchers that are struggling behind tough problems in real-time distributed systems. Indeed, MOGs represent apeculiar class of distributed systems, sharing features, requirements, challenges, and thereby also feasible solutions. To this aim, we focus on issues and state-of-art solutions related to a crucial aspect in MOG deployment: the synchronization of its nodes. In particular, we review techniques able to guarantee that the system evolves in a quick and consistent way and highlight how they canbe made more effective by exploiting specific game features. To further support this claim, we also present results from an experimental evaluation we performed.","Real time systems,
Computer science,
Distributed computing,
Mathematics,
Performance evaluation,
Resource management,
Delay,
Avatars,
System testing,
Internet"
A neural amplifier with high programmable gain and tunable bandwidth,"A neural recording amplifier having programmable gain and bandwidth is presented. The gain can be digitally programmed using 6 bits from 100× to 1100× in steps of 100×. The low-frequency cutoff can be varied from less than 10Hz to above 100Hz to accept or reject field potentials while the high-frequency cutoff is fixed at 9kHz. The input referred noise of this amplifier is 4.8μVrms and it consumes 50μW operating from ±1.5V. Implemented in a 0.5μm technology, the amplifier occupies an area of 0.098mm2. This amplifier has been successfully demonstrated in-vivo and compared to a commercial amplifier.",
Three Dimensional Face Recognition Using SVM Classifier,"In this paper, we presented a novel approach for automated 3D face recognition using range data. An object recognition system generally consists of two main parts: data registration and data comparison. In first step, the nose tip was used as the reference point and 3D face shape was normalized to standard image size. The 2DPCA was applied to the resultant range data and the corresponding principal images were used as the feature vectors. Classification was carried out by calculating the similarity score between the feature vectors. The SVM classifier was used in choosing the closest match. Recognition rate of 97% rank-four was achieved.","Face recognition,
Support vector machines,
Support vector machine classification,
Image segmentation,
Face detection,
Shape,
Nose,
Object recognition,
Lighting,
Information science"
An automatic region based methodology for facial expression recognition,"This work investigates the use of a Point Distribution Model to detect prominent features in a face (eyes, brows, mouth, etc) and the subsequent facial feature extraction and facial expression classification into seven categories (anger, fear, surprise, happiness, disgust, neutral and sadness). A multi-scale and multi-orientation Gabor filter bank, designed in such a way so as to avoid redundant information, is used to extract facial features at selected locations of the prominent features of a face (fiducial points). A region based approach is employed at the location of the fiducial points using different region sizes to allow some degree of flexibility and avoid artefacts due to incorrect automatic discovery of these points. A feed forward back propagation Artificial Neural Network is employed to classify the extracted feature vectors. The methodology is evaluated by forming 7 different regions and the feature vector is extracted at the location of 20 fiducial points.","Face recognition,
Face detection,
Facial features,
Data mining,
Feature extraction,
Computer vision,
Eyes,
Mouth,
Gabor filters,
Feeds"
Improving Security of Real-Time Wireless Networks Through Packet Scheduling [Transactions Letters],"Modern real-time wireless networks require high security level to assure confidentiality of information stored in packages delivered through wireless links. However, most existing algorithms for scheduling independent packets in real-time wireless networks ignore various security requirements of the packets. Therefore, in this paper we remedy this problem by proposing a novel dynamic security-aware packet-scheduling algorithm, which is capable of achieving high quality of security for realtime packets while making the best effort to guarantee realtime requirements (e.g., deadlines) of those packets. We conduct extensive simulation experiments to evaluate the performance of our algorithm. Experimental results show that compared with two baseline algorithms, the proposed algorithm can substantially improve both quality of security and real-time packet guarantee ratio under a wide range of workload characteristics.","Wireless networks,
Scheduling algorithm,
Communication system security,
Data security,
Wireless communication,
Information security,
Computer science,
Application software,
Data communication,
Packaging"
Evaluating and improving the TCP/UDP performances of IEEE 802.11(p)/1609 networks,"The IEEE 802.11(p)/1609 standard is an emerging technology for vehicular communication networks. It amends the IEEE 802.11-2007 standard and defines a new WAVE operational mode for vehicular environments. The WAVE mode utilizes a combined FDMA/TDMA scheme to manage network bandwidth. In this mode, a node must periodically switch its frequency channel between two different channels over time. Such a design may result in bandwidth wastage due to the residual time caused by channel switching. In this paper, we propose two easy-to-implement schemes to mitigate this problem and compare their TCP/UDP performances with those of the original scheme. Our simulation results provide many insights into the operations of these schemes on IEEE 802.11(p)/1609 networks and show that the proposed schemes always outperform the original scheme.","Bandwidth,
Throughput,
Protocols,
Delay,
Switches,
Wireless communication,
OFDM"
Cross-layer architectural framework for highly-mobile multihop airborne telemetry networks,"Highly dynamic mobile wireless networks present unique challenges to end-to-end communication, particularly caused by the time varying connectivity of high-velocity nodes combined with the unreliability of the wireless communication channel. Addressing these challenges requires the design of new protocols and mechanisms specific to this environment. Our research explores the tradeoffs in the location of functionality such as error control and location management for high-velocity multihop airborne sensor networks and presents cross-layer optimizations between the MAC, link, network, and transport layers to enable a domain specific network architecture, which provides high reliability for telemetry applications. We have designed new transport, network, and routing protocols for this environment: TCP-friendly AeroTP, IP-compatible AeroNP, and AeroRP, and show significant performance improvement over the traditional TCP/IP/MANET protocol stack.","Spread spectrum communication,
Telemetry,
Routing protocols,
Aerodynamics,
Wireless networks,
Mobile communication,
Wireless communication,
Error correction,
TCPIP,
Mobile ad hoc networks"
The Ordinal Recursive Complexity of Lossy Channel Systems,"We show that reachability and termination for lossy channel systems is exactly at level F_omega^omega in the Fast-Growing Hierarchy of recursive functions, the first level that dominates all multiply-recursive functions.","Upper bound,
Computational modeling,
Logic,
Computer science,
Automata,
Robustness,
Size measurement,
Size control"
Regional Admittivity Spectra With Tomosynthesis Images for Breast Cancer Detection: Preliminary Patient Study,"It has been known for some time that many tumors have a significantly different conductivity and permittivity from surrounding normal tissue. This high ldquocontrastrdquo in tissue electrical properties, occurring between a few kilohertz and several megahertz, may permit differentiating malignant from benign tissues. Here we show the ability of electrical impedance spectroscopy (EIS) to roughly localize and clearly distinguish cancers from normal tissues and benign lesions. Localization of these lesions is confirmed by simultaneous, in register digital breast tomosynthesis (DBT) mammography or 3-D mammograms.","Breast cancer,
Cancer detection,
Surface impedance,
Tomography,
Biomedical engineering,
Lesions,
Breast tumors,
Breast neoplasms,
Electrochemical impedance spectroscopy,
Mammography"
Cues to Deception in Online Chinese Groups,"Advancing our knowledge about cues to deception is crucial to successful deception detection. A lengthy list of cues to deception has been identified via a myriad of deception studies. Nonetheless, we identified two major limitations of existing cues to deception: the lack of cues in computer-mediated communication and in non-Western group communication. In this research, we aim to make some contributions to addressing this line of inquiry. We conducted an empirical study on cues to deception using a large real-world online Chinese community. Through hypotheses testing, we observed a number of interesting findings. For example, we found that deceivers tended to communicate less and showed low complexity and high diversity in their messages. These findings provide significant implications to deception research and the broad online communication community.","IP networks,
Face detection,
Information systems,
Globalization,
Costs,
Production,
Information filtering,
Information filters,
Delay effects,
Computer mediated communication"
Virtual structure based target-enclosing strategies for nonholonomic agents,"In this paper, we discuss a target-enclosing problem for a group of multiple nonholonomic agents in a plane. The proposed strategies guarantee that multiple agents’ coordination finally results in a circular formation enclosing the target-object which moves in the plane. Firstly, virtual agents for the feedback linearization of the real nonholonomic agents are introduced. Secondly, we propose the target-enclosing control laws based on the consensus algorithm to the virtual agents. Algebraic graph theory and consensus algorithm are employed to prove convergence and stability of the enclosing problem. Finally, experiments are provided to demonstrate the effectiveness of the proposed control laws.",
A fuzzy ant colony optimization algorithm for topology design of distributed local area networks,"Ant colony optimization (ACO) is a powerful optimization technique that has been applied to solve a number of complex optimization problems. One such optimization problem is network topology design of distributed local area networks (DLANs). The problem requires simultaneous optimization of a number of objectives, such as monetary cost, average network delay, hop count between communicating nodes, and reliability under a set of constraints. This paper presents a multi-objective ant colony optimization algorithm to efficiently solve the DLAN topology design problem. The multi-objective aspect of the problem is handled by incorporating fuzzy logic in the ACO algorithm. The performance of fuzzy ACO is evaluated through comparison with a fuzzy simulated annealing algorithm. Empirical results suggest that the fuzzy ACO produces results of equal quality when compared with a fuzzy simulated annealing algorithm.","Ant colony optimization,
Network topology,
Algorithm design and analysis,
Local area networks,
Simulated annealing,
Design optimization,
Constraint optimization,
Cost function,
Telecommunication network reliability,
Fuzzy logic"
Saliency based objective quality assessment of decoded video affected by packet losses,"In this work, we propose a novel saliency-based objective quality assessment metric, for assessing the perceptual quality of decoded video sequences affected by packet loses. The proposed method weights the error at each pixel by the visual saliency of the pixel. Different weighting methods are explored and compared. Our test results show that the predicted scores by the proposed metrics correlate very well with mean subjective scores, significantly better than the mean square error (MSE), mean absolute difference (MAD) or structure similarity (SSIM).","Quality assessment,
Decoding,
Humans,
Video compression,
Machine vision,
Focusing,
PSNR,
Image coding,
Image reconstruction,
Transform coding"
A New Model for Image Segmentation,"As a region-based approach, the Mumford-Shah (MS) model is a robust image segmentation technique. However, the solution of the MS model is not trivial. Although some alternative approaches have been presented, these methods are either inefficient or applicable only to some special cases. We present a new model which consists of two terms, the length of the segmentation curve and the high-frequency component in the regions. Because only one variable needs to be solved, the method of solution is very efficient. Using the level set method, the approach can segment objects with complicated image intensity distribution without any approximations. In addition, the new model can segment both step and roof edges.","Image segmentation,
Image edge detection,
Robustness,
Linear approximation,
Object detection,
Level set,
Differential equations,
Markov random fields,
Partial differential equations,
Computer science"
Distributed cooperative multi-robot path planning using differential evolution,"This paper provides an alternative approach to the co-operative multi-robot path planning problem using parallel differential evolution algorithms. Both centralized and distributed realizations for multi-robot path planning have been studied, and the performances of the methods have been compared with respect to a few pre-defined yardsticks. The distributed approach to this problem out-performs its centralized version for multi-robot planning. Relative performance of the distributed version of the differential evolution algorithm has been studied with varying numbers of robots and obstacles. The distributed version of the algorithm is also compared with a PSO-based realization, and the results are competitive.","Robots,
Collision avoidance,
Planning,
Path planning,
Robot kinematics,
Distance measurement,
Trajectory"
Market-Based Model Predictive Control for Large-Scale Information Networks: Completion Time and Value of Solution,"There are several important properties of modern software systems. They tend to be large-scale with distributed and component-based architectures. Also, dynamic nature of operating environments leads them to utilize alternative algorithms. However, on the other hand, these properties make it hard to provide appropriate control mechanisms due to the increased complexity. Components are sharing resources and each component can have alternative algorithms. As a result, the behavior of a software system can be controlled through resource allocation, as well as algorithm selection. This novel control problem is worthy of investigation in order to double the benefits of those properties. In this paper, we design a control mechanism for such systems. The quality-of-service we are considering is a product of the value of solution and the time for generating solution for a given problem. We build a mathematical programming model that trades off these two conflicting objectives, and decentralize the model through an auction market. By periodically opening the auction market for each existing system state, a closed-loop policy is formed. Though similar problems can be found in multiprocessor scheduling literature, they have limitations in addressing this control problem. They commonly consider so-called workflow applications in which each component only has to process one task after all of its predecessors complete their tasks. In contrast, a component in the networks under consideration processes multiple tasks in parallel with its successors or predecessors.",
SPARK: A Keyword Search Engine on Relational Databases,"Relational database is the most widely adopted and mature technology for information storage. As many services on the Web (e.g., blog and wiki sites) and advanced applications (e.g., customer relationship management systems and content management systems) are built on RDBMSs, increasing amount of text data is now stored in relational databases, accompanied by increasing demands of retrieving relevant information by free-style keyword search.",
Reduced Encoding Diffusion Spectrum Imaging Implemented With a Bi-Gaussian Model,"Diffusion spectrum imaging (DSI) can map complex fiber microstructures in tissues by characterizing their 3-D water diffusion spectra. However, a long acquisition time is required for adequate q-space sampling to completely reconstruct the 3-D diffusion probability density function. Furthermore, to achieve a high q-value encoding for sufficient spatial resolution, the diffusion gradient duration and the diffusion time are usually lengthened on a clinical scanner, resulting in a long echo time and low signal-to-noise ratio of diffusion-weighted images. To bypass long acquisition times and strict gradient requirements, the reduced-encoding DSI (RE-DSI) with a bi-Gaussian diffusion model is presented in this study. The bi-Gaussian extrapolation kernel, based on the assumption of the bi-Gaussian diffusion signal curve across biological tissue, is applied to the reduced q-space sampling data in order to fulfill the high q-value requirement. The crossing phantom model and the manganese-enhanced rat model served as standards for accuracy assessment in RE-DSI. The errors of RE-DSI in estimating fiber orientations were close to the noise limit. Meanwhile, evidence from a human study demonstrated that RE-DSI significantly decreased the acquisition time required to resolve complex fiber orientations. The presented method facilitates the application of DSI analysis on a clinical magnetic resonance imaging system.","Encoding,
Biological system modeling,
Sampling methods,
Microstructure,
Image reconstruction,
Probability density function,
Image coding,
Spatial resolution,
Signal to noise ratio,
Extrapolation"
Distributed data association and filtering for multiple target tracking,"This paper presents a novel distributed framework for multi-target tracking with an efficient data association computation. A decentralized representation of trackers’ motion and association variables is adopted. Considering the interleaved nature of data association and tracker filtering, the multi-target tracking is formulated as a missing data problem, and the solution is found by the proposed variational EM algorithm. We analytically show that 1) the posteriori distributions of trackers’ motions (the real interests in terms of tracking applications) can be effectively computed in the E-step of the EM iterations, and 2) the solution of trackers’ association variables can be pursued under a derived graph-based discrete optimization formulation, thus efficiently estimated in the M-step by the recently emerging graph optimization algorithms. The proposed approach is very general such that sophisticated data association priori and likelihood function can be easily incorporated. This general framework is tested with both simulation data and real world surveillance video. The reported qualitative and quantitative studies verify the effectiveness and low computational cost of the algorithm.","Target tracking,
Distributed computing,
Filtering algorithms,
Algorithm design and analysis,
Motion analysis,
Motion estimation,
Testing,
Computational modeling,
Surveillance,
Computational efficiency"
Multi-thresholded approach to demonstration selection for interactive robot learning,"Effective learning from demonstration techniques enable complex robot behaviors to be taught from a small number of demonstrations. A number of recent works have explored interactive approaches to demonstration, in which both the robot and the teacher are able to select training examples. In this paper, we focus on a demonstration selection algorithm used by the robot to identify informative states for demonstration. Existing automated approaches for demonstration selection typically rely on a single threshold value, which is applied to a measure of action confidence. We highlight the limitations of using a single fixed threshold for a specific subset of algorithms, and contribute a method for automatically setting multiple confidence thresholds designed to target domain states with the greatest uncertainty. We present a comparison of our multi-threshold selection method to confidence-based selection using a single fixed threshold, and to manual data selection by a human teacher. Our results indicate that the automated multi-threshold approach significantly reduces the number of demonstrations required to learn the task.","Robot sensing systems,
Training,
Learning systems,
Support vector machines,
Uncertainty,
Training data"
Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues,"This paper presents investigations on visual focus of attention (VFOA ) recognition in meetings from audio-visual perceptual cues. Rather than independently recognizing the VFOA of each participant from his own head pose, we propose to recognize participants’ VFOA jointly in order to introduce context dependent interaction models that relates to group activity and the social dynamics of communication. To this end, we designed an input-output hidden Markov model (IOHMM), whose hidden states are the joint VFOA of all participants, and whose main observations are the head poses. Interaction models are introduced in the form of contextual cues that affect the temporal evolution of the joint VFOA sequence, allowing us to model group dynamics that accounts for people’s tendency to share the same focus, or to have their VFOA driven by contextual cues such as slide activity or the participant speaking activity. The model is rigorously evaluated on a publicly available dataset of 4 real meetings of 23min on average, showing an overall 10% relative performance increase w.r.t. the independent recognition case.","Context modeling,
Hidden Markov models,
Speech recognition,
Globalization,
Computer science,
Content management,
Feedback,
Statistical analysis,
Government,
Information management"
Minimum Pseudoweight and Minimum Pseudocodewords of LDPC Codes,"In this correspondence, we study the minimum pseudoweight and minimum pseudocodewords of low-density parity-check (LDPC) codes under linear programming (LP) decoding. First, we show that the lower bound of Kelley, Sridhara, Xu, and Rosenthal on the pseudoweight of a nonzero pseudocodeword of an LDPC code whose Tanner graph has girth greater than is tight if and only if this pseudocodeword is a real multiple of a codeword. Then, the lower bound of Kashyap and Vardy on the stopping distance of an LDPC code is proved to be also a lower bound on the pseudoweight of a nonzero pseudocodeword of an LDPC code whose Tanner graph has girth , and this lower bound is tight if and only if this pseudocodeword is a real multiple of a codeword. Using these results we further obtain that for some LDPC codes, there are no other minimum pseudocodewords except the real multiples of minimum weight codewords. This means that the LP decoding for these LDPC codes is asymptotically optimal in the sense that the ratio of the probabilities of decoding errors of LP decoding and maximum-likelihood decoding approaches as the signal-to-noise ratio (SNR) tends to infinity. Finally, some LDPC codes are listed to illustrate these results.","Parity check codes,
Cryptography,
Broadcasting,
Computer science,
Cost function,
Communication channels,
Buildings,
Communication system security,
Distributed computing,
Cryptographic protocols"
DCA Using Suffix Arrays,The paper deals with an implementation of Data Compression using Antidictionaries. Suffix array is used instead of suffix trie. A dynamic version is implemented as well.,"Data compression,
Computer science,
Data engineering,
Transducers,
Compressors,
Encoding,
Optical arrays"
OCals: A Novel Overlay Construction Approach for Layered Streaming,"Layered streaming in overlay networks has drawn great interests since not only can it accomodate large scales of clients but also it handles client heterogeneities. However, to our knowledge, there's still a lack of overlay construction (i.e. neighbor selection) approach suited for layered streaming, because i) In existing works neighbors are selected only based on their network conditions. However, a neighbor with good network condition may not be able to provide sufficient layers (e.g. a neighbor in the same LAN), ii) Previous works usually select ""good"" neighbors for the new node, ignoring that the joining of the new node could also be utilized to improve the performance of existing nodes. In this paper, OCals - a two-stage QoS aware overlay construction approach for layered streaming is proposed. The main contribution of OCals is that i) when selecting neighbor, it considers existing nodes' network conditions and their providing layers as a whole; ii) it guarantees the QoS for the new node as well as improves the QoS for existing nodes so that with the joining of new nodes, the performance of the overlay could be consecutively improved; iii) it's easy to implement and low time cost. Experiments demonstrate that compared with two other approaches: SCAMP (a pure random neighbor selection method) and Narada (a QoS aware method), the throughput and average packet delay of the layered streaming on top of the overlay constructed by OCals can be remarkably improved. Besides, the time spent on joining and recovery is very short.","Peer to peer computing,
Bandwidth,
Local area networks,
Throughput,
Costs,
Communications Society,
Computer science,
Delay effects,
Stability analysis,
Performance analysis"
Biosignal and context monitoring: Distributed multimedia applications of Body Area Networks in healthcare,"We are investigating the use of Body Area Networks (BANs), wearable sensors and wireless communications for measuring, processing, transmission, interpretation and display of biosignals. The goal is to provide telemonitoring and teletreatment services for patients. The remote health professional can view a multimedia display which includes graphical and numerical representation of patients’ biosignals. Addition of feedback-control enables teletreatment services; teletreatment can be delivered to the patient via multiple modalities including tactile, text, auditory and visual. We describe the health BAN and a generic mobile health service platform and two context aware applications. The epilepsy application illustrates processing and interpretation of multi-source, multimedia BAN data. The chronic pain application illustrates multi-modal feedback and treatment, with patients able to view their own biosignals on their handheld device.","Sensors,
Visualization,
Heart rate,
Epilepsy,
Electromyography,
Electrodes,
Pain"
Transmission power control techniques for MAC protocols in wireless sensor networks,"Communication is usually the most energy-consuming event in wireless sensor networks (WSNs). The lifetime of these networks is determined by the capacity of batteries and one of the biggest challenges is to reduce the energy consumed in the communication. Several techniques to reduce the energy consumption have been proposed in routing protocols: diminish the traffic of data, optimize the routes or the control of the topology. However, the management and operation of the node’s radio is responsibility of the medium access control (MAC) protocol. This thesis presents four new transmission power control (TPC) techniques for MAC protocols in WSNs. These techniques are based on the interaction between sensor nodes and take into account the limitations of resources such as processing, memory and energy in the calculation of the minimum of transmission power. To evaluate these techniques, four MAC protocols had been developed: Iterative, Attenuation, AEWMA and Hybrid. The Iterative was the first MAC protocol with TPC developed exclusively for WSN. These protocols have been experimented in the Mica Motes2 platform, in diverse scenarios varying parameters such as internal and external environment, the distance among the communicating nodes, the occurrence of simultaneous transmissions, the use of multi-hop transmissions and node mobility. Results have shown that TPC protocols reduce the energy consumption by up to 57% in comparison to the protocols with fixed transmission power. Moreover, the protocols with TPC increase the throughput of the network while maintaining a very high delivery rate.",
Adaptive EEG signal classification using stochastic approximation methods,"Classification of time-varying electrophysiological signals is an important problem in the development of brain-computer interfaces (BCIs). Designing adaptive classifiers is a potential way to address this task. In this paper, Bayesian classifiers with Gaussian mixture models (GMMs) are adopted as the decision rule to classify electroencephalogram (EEG) signals. The stochastic approximation method (SAM) is used as the specific gradient descent method for updating the parameters of mean values and covariance matrices in the distribution of GMMs, where the parameters are simultaneously updated in a batch mode. Experimental results using data from a BCI show that the stochastic approximation method is effective for EEG classification tasks.","Electroencephalography,
Pattern classification,
Stochastic processes,
Approximation methods,
Bayesian methods,
Brain computer interfaces,
Computer science,
Brain modeling,
Communication system control,
Diseases"
Label segregation by remapping stereoscopic depth in far-field augmented reality,"This paper describes a novel technique for segregating overlapping labels in stereoscopic see-through displays. The present study investigates the labeling of far-field objects, with distances ranging 100-120 m. At these distances the stereoscopic disparity difference between objects is below 1 arcmin, so labels rendered at the same distance as their corresponding objects appear as if on a flat layer in the display. This flattening is due to limitations of both display and human visual resolution. By remapping labels to pre-determined depth layers on the optical path between the observer and the labeled object, an interlayer disparity ranging from 5 to 20 arcmin can be achieved for 5 overlapping labels. The present study evaluates the impact of such depth separation of superimposed layers, and found that a 5 arcmin interlayer disparity yields a significantly lower response time, over 20% on average, in a visual search task compared to correctly registering labels and objects in depth. Notably the performance does not improve when doubling the interlayer disparity to 10 arcmin and, surprisingly, the performance degrades significantly when again doubling the interlayer disparity to 20 arcmin, approximating the performance in situations with no interlayer disparity. These results confirm that our technique can be used to segregate overlapping labels in the far visual field, without the cost associated with traditional label placement algorithms.","Visualization,
Distance measurement,
Time factors,
Clutter,
Optical imaging,
Observers,
Layout"
Towards an Artificial Traffic Control System,"This work reports on the use of the concept of Artificial Transportation Systems to implement a framework to allow the specification and test of new generation intelligent traffic control systems. A JADE implementation of a real agent application is linked to a virtual traffic domain to test control behaviour of traffic semaphores. Mixing reality and virtual environments is expected to foster the development of new generation of urban transport solutions. First experiments were carried out, which demonstrated the feasibility of the approach.","Traffic control,
Intelligent transportation systems,
Artificial intelligence,
System testing,
Intelligent control,
Virtual environment,
Telecommunication traffic,
Roads,
Computer science,
Urban areas"
Fairness of High-Speed TCP Stacks,"We present experimental results evaluating fairness of several proposals to change the TCP congestion control algorithm, in support of operation on high bandwidth-delay-product (BDP) network paths. We examine and compare the fairness of New Reno TCP, BIC, Cubic, Hamilton-TCP, Highspeed-TCP and Scalable-TCP. We focus on four different views of fairness: TCP-friendliness, RTT-fairness, intra- and inter-protocol fairness.","Kernel,
Computer science,
Proposals,
Internet,
Stability,
Linux,
Application software,
Educational institutions,
Access protocols,
Couplings"
Achievable rates for conferencing multiway channels,"A generalization of the additive Gaussian two-way channel to M users is considered. Such channels contain implicit feedback in the sense that the channel output signals observed by the different encoders are correlated. While the benefits of feedback are shown to be negligible at high SNR, for moderate SNR, feedback can play a significant role in boosting the sum-rate performance. To highlight this potential gain, the special case of the M-user multiway channel with a common output is considered. By taking insights from Kramer’s Fourier WC, a feedback strategy is introduced and shown to strictly dominate the performance of a pre-log optimal non-feedback strategy. Furthermore, an upper bound is derived to show this feedback strategy achieves the sum-rate capacity beyond a certain SNR threshold. Under per-symbol power constraints, this upper bound can be tightened to show the feedback strategy is sum-rate optimal for all SNR values.","Signal to noise ratio,
Decoding,
Upper bound,
Covariance matrix,
Additives,
Wireless communication,
Wireless sensor networks"
Wireless multichannel acquisition of neuropotentials,"Implantable brain-machine interfaces for disease diagnosis and motor prostheses control require low-power acquisition of neuropotentials spanning a wide range of amplitudes and frequencies. Here, we present a 16-channel VLSI neuropotential acquisition system with tunable gain and bandwidth, and variable rate digital transmission over an inductive link which further supplies power. The neuropotential interface chip is composed of an amplifier, incremental ADC and bit-serial readout circuitry. The front-end amplifier has a midband gain of 40 dB and offers NEF of less than 3 for all bandwidth settings. It also features adjustable low-frequency cut-off from 0.2 to 94 Hz, and independent high-frequency cut-off from 140 Hz to 8.2 kHz. The Gm-C incremental ΔΣ ADC offers digital gain up to 4096 and 8–12 bits resolution. The interface circuit is powered by a telemetry chip which harvests power through inductive coupling from a 4 MHz link, provides a 1 MHz clock for ADC operation and transmits the bit-serial data of the neurpotential interface across 4 cm at up to 32 kbps with a BER less than 10−5. Experimental EEG recordings using the neuropotential interface and wireless module are presented.",
Understanding Radio Irregularity in Wireless Networks,"In an effort to better understand connectivity and capacity in wireless networks, the log-normal shadowing radio propagation model is used to capture radio irregularities and obstacles in the transmission path. Existing results indicate that log-normal shadowing results in higher connectivity and interference levels as shadowing (i.e., the radio irregularity) increases. In this paper we demonstrate that such a behavior is mainly caused by an unnatural bias of the log-normal shadowing radio propagation model that results in a larger transmission range as shadowing increases. To avoid this effect, we analyze connectivity and interference under log-normal shadowing using a normalization that compensates for the enlarged radio transmission range. Our analysis shows that log-normal shadowing still improves the connectivity of a wireless network and even reduces interference. We explain this behavior by studying in detail what network parameters are affected by shadowing. Our results indicate that, when it comes to connectivity and interference, an analysis based on a circular transmission range leads to worst case results.","Wireless networks,
Shadow mapping,
Radio propagation,
Context modeling,
Computer science,
Electromagnetic interference,
Throughput,
Propagation losses,
Radio control,
Shape"
Maotai: View-Oriented Parallel Programming on CMT Processors,"View-Oriented Parallel Programming (VOPP) is a novel parallel programming model which uses views for communication between multiple processes. With the introduction of views, mutual exclusion and shared data access are bundled together, which offers both convenience and high performance to parallel programming. This paper presents the implementation of VOPP on Chip-Multi threading processors, e.g. UltraSPARC T1. We demonstrate that our implementation of VOPP on multi-core platforms (namely Maotai) shows significantly better performance than directly applying the original DSM implementation of VOPP (namely VODCA) on our platform. Besides, we compare the performance of VOPP with MPI and OpenMP. The experimental results demonstrate that VOPP has better scalability than both MPI and OpenMP on our platform.","Parallel programming,
Random access memory,
Message passing,
Program processors,
Artificial neural networks,
Computational modeling,
Programming"
Challenges and perspectives in the implementation of NOTICE architecture for vehicular communications,"The NOTICE architecture is a new concept in Vehicular Ad-Hoc Networking (VANET) that aims at providing automated notification of traffic incidents on highways in order to reduce congestion and improve overall traffic safety. The basic component of NOTICE is a system of sensor belts that collect information from passing cars through a wireless radio link. An indicator of performance for the NOTICE system is the incident detection time which depends on various parameters among which we note the vehicle speed, the time required by communicating radios for connection setup and information exchange, the amount of information to be transmitted, or the radio technology employed. In this paper we present a numerical performance study of the NOTICE system with realistic values of these parameters in an attempt to provide some basic specifications for the physical layer of the system.","Belts,
Vehicles,
Transceivers,
Ad hoc networks,
Roads,
Wireless communication,
Road transportation"
Moving shape dynamics: A signal processing perspective,"This paper provides a new perspective on human motion analysis, namely regarding human motions in video as general discrete time signals. While this seems an intuitive idea, research on human motion analysis has attracted little attention from the signal processing community. Sophisticated signal processing techniques create important opportunities for new solutions to the problem of human motion analysis. This paper investigates how the deformations of human silhouettes (or shapes) during articulated motion can be used as discriminating features to implicitly capture motion dynamics. In particular, we demonstrate the applicability of two widely used signal transform methods, namely the Discrete Fourier Transform (DFT) and Discrete Wavelet Transform (DWT), for characterization and recognition of human motion sequences. Experimental results show the effectiveness of the proposed method on two state-of-the-art data sets.","Shape,
Signal processing,
Humans,
Discrete Fourier transforms,
Discrete wavelet transforms,
Motion analysis,
Fourier transforms,
Character recognition,
Hidden Markov models,
Video signal processing"
A Survey on Training Algorithms for Support Vector Machine Classifiers,"Learning from data is one of the basic ways humans perceive the world and acquire the knowledge. Support vector machine (SVM for short) has emerged as a good classification technique and achieved excellent generalization performance in a variety of applications. Training SVM on a dataset of huge size with millions of data is a challenging problem since it is computationally expensive and the memory requirement grows with the square of the number of training examples. This paper surveys SVM training algorithms and falls them into three groups. Moreover, recent advances such as finite Newton method and active learning algorithms are described.","Support vector machines,
Training,
Classification algorithms,
Kernel,
Algorithm design and analysis,
Convergence,
Machine learning"
Balanced Tidset-based Parallel FP-tree Algorithm for the Frequent Pattern Mining on Grid System,"Mining frequent patterns from transaction-oriented database is an important problem. Frequent patterns are essential for generate association rules, time series, etc. Most of frequent patterns mining algorithm can be classified into two categories: generate-and-test approach (Apriori-like) and pattern growth approach (FP-tree). In recent times, many methods have been proposed for solving this problem based on FP-tree, because this approach can reduce the number of database scan. However, even for pattern growth methods, the execution time grows rapidly when the database size is getting large and the given support is small. Therefore, parallel-distributed computing is a good strategy to solve this problem. Some parallel algorithms have been proposed, but the execution time is costly when the database size is large. In this paper, we proposed an efficient parallel and distributed mining algorithm—Balanced Tidset Parallel FP-tree (BTP-tree) algorithm on grid computing system. Grid system is a heterogeneous computing environment, our proposed method can balance the loading according to the tree depth and width. In order to exchange transactions efficiently, transaction identification set (Tidset) was used to directly select transactions instead of scanning database. BTP-tree, TPFP-tree and PFP-tree were implemented and the datasets generated by IBM Quest Synthetic Data Generator are used to verify the performance of BTP-tree. The experimental results show that BTP-tree can reduce the execution time significantly and has better loading balance capability than TPFP-tree and PFP-tree.","Grid computing,
Transaction databases,
Data mining,
Association rules,
Data engineering,
Knowledge engineering,
Data structures,
Pervasive computing,
Computer science,
Concurrent computing"
Design and control of a multifunction myoelectric hand with new adaptive grasping and self-locking mechanisms,"This paper presents a multifunction myoelectric hand that is designed with underactuated mechanisms. The finger design allows an adaptive grasp, including adaptation between fingers and phalanges with respect to the shape of an object. In addition, a self-lock is embedded in the metacarpophalangeal joint to prevent back driving when external forces act on the fingers. The thumb design also provides adaptation between phalanges and adds an intermittent rotary motion to the carpometacarpal joint. As a result, the hand can perform versatile grasping motions using only two motors, and is capable of natural and stable grasping without complex sensor and servo systems. Moreover, the adaptive grasping capabilities reduce the requirements of electromyogram pattern recognition, as analogous motions, such as cylindrical and tip grasps, can be classified as one motion.",
A Modified Simulated Annealing Algorithm for Static Task Scheduling in Grid Computing,"Grid Computing aims to allow unified access to data, computing power, sensors and other resources through a single virtual laboratory. The development or adaptation of applications for Grid environments is being challenged by the need of scheduling a large number of tasks and resources efficiently. The general problem of optimally mapping tasks to machines in a heterogeneous computing suite has been shown to be NP-complete. In this paper we propose a modified simulated annealing algorithm for scheduling independent tasks in Grid environment. Experimental results show that our proposed algorithm improves the performance of static instances compared to the results of other algorithms reported in the literature.","Processor scheduling,
Scheduling,
Mathematical model,
Gallium,
Computational modeling,
Schedules,
Equations"
A Research Agenda for Testing SOA-Based Systems,"Service-Oriented Architecture (SOA) is a paradigm that organizes and uses distributed capabilities to bring together a technical solution to a business problem. The central concept of SOA revolves around modularized implementations of business logic known as services. SOA is different from traditional systems in that functional requirements are mapped to business process models and are implemented across different networked applications running on heterogeneous technologies and platforms. Services typically do not have user-accessible interfaces; instead, other applications invoke them programmatically in a message-based manner. One large barrier to the widespread adoption of SOA-based systems is testing. The common misconception for testing SOA-based systems is that it is little different than testing non-SOA systems. Therefore, when migrating existing systems to SOA or creating new SOA-based systems, project managers often pay much less attention to the testing process of these systems. This paper outlines a possible research agenda for testing SOA-based systems, focusing on three main areas: SOA governance, underlying technologies (such as Web services), and applying traditional testing strategies to SOA-based systems.","System testing,
Service oriented architecture,
Distributed computing,
Project management,
Web services,
Contracts,
Topology,
Registers,
Logic programming,
Software testing"
DLDB2: A Scalable Multi-perspective Semantic Web Repository,"A true Semantic Web repository must scale both in terms of number of ontologies and quantity of data. It should also support reasoning using different points of view about the meanings and relationships of concepts and roles. Our DLDB2 system has these features. Our system is sound and complete on a sizable subset of Description Horn Logic when answering extensional conjunctive queries, but more importantly also computes many entailments from OWL DL. By delegating TBox reasoning to a DL reasoner, we focus on the design of the table schema, database views, and algorithms that achieve essential ABox reasoning over an RDBMS. We evaluate the system using synthetic benchmarks as well as real-world data and queries.","Semantic Web,
Ontologies,
OWL,
Intelligent agent,
Vocabulary,
Computer science,
Data engineering,
Logic design,
Algorithm design and analysis,
Spatial databases"
Behavior analysis of spam botnets,"Compromised computers, known as bots, are the major source of spamming and their detection helps greatly improve control of unwanted traffic. In this work we investigate the behavior patterns of spammers based on their underlying similarities in spamming. To our knowledge, no work has been reported on identifying spam botnets based on spammers’ temporal characteristics. Our study shows that the relationship among spammers demonstrates highly clustering structures based on features such as Content length, Time of arrival, Frequency of email, Active Time, Inter-arrival Time, and Content Type. Although the dimensions of the collected feature set is low, we perform Principal Component Analysis (PCA) on feature set to identify the features which account for the maximum variance in the spamming patterns. Further, we calculate the proximity between different spammers and classify them into various groups. Each group represents similar proximity. Spammers in the same group inherit similar patterns of spamming a domain. For classification into Botnet groups, we use clustering algorithms such as Hierarchical and K-means.We identify Botnet spammers into a particular group with a precision of 90%.","Principal component analysis,
Unsolicited electronic mail,
Filtering,
Pattern analysis,
Computer security,
Laboratories,
Computer science,
Computer networks,
Communication system traffic control,
Frequency"
"Survival Analysis Approach to Reliability, Survivability and Prognostics and Health Management (PHM)","Survival analysis, also known as failure time analysis or time-to-event analysis, is one of the most significant advancements of mathematical statistics in the last quarter of the 20th century. It has become the de facto standard in biomedical data analysis. Although reliability was conceived as a major application field by the mathematicians who pioneered survival analysis, survival analysis failed to establish itself as a major tool for reliability analysis. In this paper, we attempt to demonstrate, by reviewing and comparing the major mathematical models of both fields, that survival analysis and reliability theory essentially address the same mathematical problems. Therefore, survival analysis should become a major mathematical tool for reliability analysis and related fields such as Prognostics and Health Management (PHM). This paper is the first in a four part series in which we review state-of-the-art studies in survival (univariate) analysis, competing risks analysis, and multivariate survival analysis, with focusing on their applications to reliability and computer science. The present article discusses the univariate survival analysis (survival analysis hereafter).","Prognostics and health management,
Failure analysis,
Risk analysis,
Statistical analysis,
Bioinformatics,
Data analysis,
Mathematical model,
Reliability theory,
Application software,
Computer science"
Ontology-Based User Intention Recognition for Proactive Planning of Intelligent Robot Behavior,"To recognize user intention proactively and do a suitable action or service are one of important issues in intelligent robot. Even when a user acts the same behavior, its intention may be different according to the user’s context. It means that user intention recognition involves the uncertainties, and by minimizing the uncertainties can improve the accuracy of the user intention recognition. This paper suggests a novel ontology-based approach for user intention recognition. We propose a method of minimizing the uncertainties that are the main obstacles against the precise recognition of user intention. This approach creates an ontology for user intention, makes a hierarchy and relationship among user intentions, and  precisely recognizes user intention by using the gathered sensor data such as temperature, humidity, vision, and auditory. We developed a simulator that evaluates the performance of robot proactive planning mechanism.","Ontologies,
Intelligent robots,
Uncertainty,
Robot sensing systems,
Entropy,
Intelligent sensors,
Temperature sensors,
Computer science,
Electronic mail,
Humidity"
ECG De-noising Based on Empirical Mode Decomposition,"Electrocardiogram (ECG) signal is useful in diagnosing the heart condition. Good quality ECG is utilized by physicians for interpretation and identification of physiological and pathological phenomena. However, The electrocardiogram (ECG) signal may mix various kinds of noises while gathering and recording. In this paper, we propose a new ECG enhancement method based on the recently developed empirical mode decomposition (EMD). The proposed EMD-based method is able to remove noise from the munder a wide range of variations for noise. The method is validated through experiments on the MIT–BIH databases. The simulations show that that the proposed methods in the paper provide better performance of noise reduction than wavelet thresholding de-noising methods in aspects of remaining geometrical characteristics of ECG signal and the signal-to-noise ratio (SNR).","Electrocardiography,
Noise reduction,
Heart,
Testing,
Pathology,
Signal to noise ratio,
Wavelet analysis,
Signal processing,
Educational institutions,
Information science"
Implementation of road traffic signs detection based on saliency map model,"In this paper, we proposed a new road traffic sign detection model based on human-like selective attention mechanism for implementing interactive workload manager system. Since the road traffic sign boards have dominant color contrast against backgrounds, we consider the color opponents and its edge information with center surround difference and normalization as a pre-processing, which is effective to intensify the sign board color characteristics as well as reduce background noise influence. After constructing the road traffic sign saliency map using the edge and color feature maps, the candidate road traffic sign regions are selected by local maximum energy searching with entropy maximization algorithm to find suitable size of the sign board areas. Computational experiment results show that the proposed model can successfully detect a road traffic sign board.",Vehicles
Distance-Preserving and Distance-Increasing Mappings From Ternary Vectors to Permutations,"Permutation arrays have found applications in powerline communication. One construction method for permutation arrays is to map good codes to permutations using a distance-preserving mappings (DPM). DPMs are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of some fixed length (the same or longer) such that every two distinct vectors are mapped to permutations with the same or larger Hamming distance than that of the vectors. A DPM is called distance increasing (DIM) if the distances are strictly increased (except when the two vectors are equal). In this correspondence, we propose constructions of DPMs and DIMs from ternary vectors. The constructed DPMs and DIMs improve many lower bounds on the maximal size of permutation arrays.","Councils,
Computer science,
Hamming distance,
Contracts,
Informatics,
Application software,
Error correction codes,
Binary codes"
A static data placement strategy towards perfect load-balancing for distributed storage clusters,"Applications like cluster-based Video-On-Demand (VOD) systems are inherently data-intensive because clients frequently retrieve data stored on a distributed storage subsystem interconnected by a high-speed local network. To meet the Quality-of-Service (QoS) imposed by the clients, quick responses to access requests are fundamental for these applications. Among the numerous ways to reduce response times, data placement, has attracted much attention from researchers due to its effectiveness and low cost. In this paper, we propose a novel load-balancing and performance oriented static data placement strategy, called perfect balancing (PB), which can be applied to distributed storage subsystems in clusters to noticeably improve system responsiveness. The basic idea of PB is to balance the load across local disks and to minimize the discrepancy of service times of data on each disk simultaneously. A comprehensive experimental study shows that PB reduces mean response time up to 19.04% and 8.67% over the two well-known data placement algorithms Greedy and SP respectively.","Delay,
Cost function,
Measurement,
Heuristic algorithms,
Throughput,
Web server,
Clustering algorithms,
Computer science,
Drives,
Context"
Efficient Data Interpretation and Compression over RFID Streams,"Despite its promise, RFID technology presents numerous challenges, including incomplete data, lack of location and containment information, and very high volumes. In this work, we present a novel data interpretation and compression substrate over RFID streams to address these challenges in enterprise supply-chain environments. Our results show that our inference techniques provide good accuracy while retaining efficiency, and our compression algorithm yields significant reduction in data volume.","Radiofrequency identification,
Compression algorithms,
Computer science,
Cleaning,
Monitoring,
Information filtering,
Information filters,
Hardware,
Buildings,
Inference algorithms"
A novel blind image watermarking technique for colour RGB images in the DCT domain using green channel,"This paper presents a new algorithm for colour digital image watermarking. The 24 bits/pixel RGB images are used and the watermark is placed on the green channel of the RGB image. The green channel is chosen after an analytical investigation process was carried out using some popular measurement metrics. The analysis and embedding processes have been carried out using the discrete cosine transform DCT. The new watermarking method has shown to be resistant to JPEG compression, cropping, scaling, low-pass, median and removal attack. This algorithm produces more than 65 dB of average PSNR.",
Virtual Laboratory for Development and Execution of Biomedical Collaborative Applications,"The ViroLab Virtual Laboratory is a collaborative platform for scientists representing multiple fields of expertise while working together on common scientific goals. This environment makes it possible to combine efforts of computer scientists, virology and epidemiology experts and experienced physicians to support future advances in HIV-related research and treatment. The paper explains the challenges involved in building a modern, inter-organizational platform to support science and gives an overview of solutions to these challenges. Examples of real-world problems applied in the presented environment are also described to prove the feasibility of the solution.","Laboratories,
Drugs,
Human immunodeficiency virus,
Medical treatment,
Immune system,
Programming profession,
Biomedical computing,
International collaboration,
Application software,
Computer science"
AdaBoost algorithm with random forests for predicting breast cancer survivability,"In this paper we propose a combination of the AdaBoost and random forests algorithms for constructing a breast cancer survivability prediction model. We use random forests as a weak learner of AdaBoost for selecting the high weight instances during the boosting process to improve accuracy, stability and to reduce overfitting problems. The capability of this hybrid method is evaluated using basic performance measurements (e.g., accuracy, sensitivity, and specificity), Receiver Operating Characteristic (ROC) curve and Area Under the receiver operating characteristic Curve (AUC). Experimental results indicate that the proposed method outperforms a single classifier and other combined classifiers for the breast cancer survivability prediction.","Classification algorithms,
Accuracy,
Prediction algorithms,
Breast cancer,
Support vector machines,
Artificial neural networks,
Training"
Low-cost multi-robot exploration and mapping,"Mobile robots can perform some of the more dangerous and laborious human tasks on Earth and throughout the solar system, many times with greater efficiency and accuracy, saving both time and resources. As we explore further away from Earth, higher levels of autonomy are also becoming more desired in such applications, one of them being distributed mapping. Smaller, less expensive mobile robots are becoming more prevalent, which introduces unique challenges in terms of limited sensing accuracy and onboard computing resources. This paper presents a low-cost approach to autonomous multi-robot mapping and exploration for unstructured environments. Platform design and implementation details are discussed, along with results from a planetary style environment. Results demonstrate that mobile robots capable of SLAM can be constructed for less than $1250, and similar concepts could be used for planetary missions.","Robot kinematics,
Simultaneous localization and mapping,
Robot sensing systems,
Mobile robots,
Earth,
Navigation,
Uncertainty,
Planets,
Intelligent sensors,
Humans"
Robust and Dynamic Bin Slotted Anti-Collision Algorithms in RFID System,"In this paper, we present the Dynamic Bin Slotted memoryless anti-collision algorithm (DBS) and the Robust Dynamic Bin Slotted memoryless anti-collision algorithm (RDBS) (which is a countermeasure to the strong and weak tag problems). They are based upon a bin slot tree algorithm. Both algorithms combine deterministic and probabilistic approaches to improve performance. We suggest a simple estimator to estimate the number of tags in an interrogation zone that is optimized for bin-slot tree algorithms. Our performance evaluation shows that DBS and RDBS surpass other existing bin slotted algorithms. According to our simulation results, the total identification time of the DBS algorithm for all tags is reduced by 57.28% for 300 tags compared to the conventional Bin Slotted Algorithm (BSA). Moreover, under the strong-weak tag problem, the RDBS algorithm reduces over 29.59% of the total number of PingID commands and 3.51% of the identification time for maximum 300 tags over the Bin-slotted Hybrid Search Algorithm, which is the best reported variation of BSA.","Robustness,
Radiofrequency identification,
RFID tags,
Satellite broadcasting,
Intrusion detection,
Protocols,
USA Councils,
Monitoring,
Costs,
Delay"
Autonomy-Oriented Computing (AOC): The Nature and Implications of a Paradigm for Self-Organized Computing,"Facing the increasing needs for large-scale, robust, adaptive, and distributed/decentralized computing capabilities \cite{agha-08,brooks-08} from such fields as Web intelligence, scientific and social computing, Internet commerce, and pervasive computing, an unconventional bottom-up paradigm, based on the notions of Autonomy-Oriented Computing (AOC) and self-organization in open complex systems, offers new opportunities for developing promising architectures, methods, and technologies. The goal of this paper is to describe the key concepts in this computing paradigm, and furthermore, discuss some of the fundamental principles and mechanisms for obtaining self-organized computing solutions.","Pervasive computing,
Distributed computing,
Large-scale systems,
Robustness,
Social network services,
Internet,
Business,
Computer science,
Electronic mail,
Computer architecture"
Group Scribbles to Support Knowledge Building in Jigsaw Method,"Jigsaw method empowers students to build their own knowledge through interactively communicating and discussing in group-based cooperation. However, when students return to their original group to share with teammates what they have learned in their expert groups, they may need to review the records of the previous discussion in order to integrate and share the key ideas arising from that discussion. Paper-and-pencil records often do not meet students' in-time need, thereby affecting the learning process. Group Scribbles (GS) is an activity tool that enables the collaborative generation, collection and aggregation of ideas through a shared space. The aim of this study is to deploy GS with Tablet PCs to facilitate 12 graduate school students doing cooperative learning by the jigsaw method and to examine how GS supports knowledge building and deeper understanding through interactive questioning, dialogue and continuous improvement of ideas. Results of this exploratory study suggest that GS-mediated cooperative learning jigsaw model could positively benefit students in their interaction and knowledge building processes.","Book reviews,
Collaboration,
Software,
Mobile handsets,
Collaborative work,
Handheld computers,
Least squares approximation"
Boosting ordinal features for accurate and fast iris recognition,"In this paper, we present a novel iris recognition method based on learned ordinal features.Firstly, taking full advantages of the properties of iris textures, a new iris representation method based on regional ordinal measure encoding is presented, which provides an over-complete iris feature set for learning. Secondly, a novel Similarity Oriented Boosting (SOBoost) algorithm is proposed to train an efficient and stable classifier with a small set of features. Compared with Adaboost, SOBoost is advantageous in that it operates on similarity oriented training samples, and therefore provides a better way for boosting strong classifiers. Finally, the well-known cascade architecture is adopted to reorganize the learned SOBoost classifier into a ‘cascade’, by which the searching ability of iris recognition towards large-scale deployments is greatly enhanced. Extensive experiments on two challenging iris image databases demonstrate that the proposed method achieves state-of-the-art iris recognition accuracy and speed. In addition, SOBoost outperforms Adaboost (Gentle-Adaboost, JS-Adaboost, etc.) in terms of both accuracy and generalization capability across different iris databases.","Boosting,
Iris recognition,
Image databases,
Spatial databases,
Machine learning algorithms,
Large-scale systems,
Helium,
Sun,
Biometrics,
National security"
Fast Noise Reduction in Computed Tomography for Improved 3-D Visualization,"Computed tomography (CT) has a trend towards higher resolution and higher noise. This development has increased the interest in anisotropic smoothing techniques for CT, which aim to reduce noise while preserving structures of interest. However, existing smoothing techniques are slow, which makes clinical application difficult. Furthermore, the published methods have limitations with respect to preserving small details in CT data. This paper presents a widely applicable speed optimized framework for anisotropic smoothing techniques. A second contribution of this paper is an extension to an existing smoothing technique aimed at better preserving small structures of interest in CT data. Based on second-order image structure, the method first determines an importance map, which indicates potentially relevant structures that should be preserved. Subsequently an anisotropic diffusion process is started. The diffused data is used in most parts of the images, while structures with significant second-order information are preserved. The method is qualitatively evaluated against an anisotropic diffusion method without structure preservation in an observer study to assess the improvement of 3-D visualizations of CT series and quantitatively by determining the reduction of the difference between low and high dose CT scans of in vitro carotid plaques.","Noise reduction,
Computed tomography,
Visualization,
Cancer,
Biomedical imaging,
Medical diagnostic imaging,
Anisotropic magnetoresistance,
Smoothing methods,
Image quality,
Radiology"
Automatic non-rigid registration of 3D dynamic data for facial expression synthesis and transfer,"Automatic non-rigid registration of 3D time-varying data is fundamental in many vision and graphics applications such as facial expression analysis, synthesis, and recognition. Despite many research advances in recent years, it still remains to be technically challenging, especially for 3D dynamic, densely-sampled facial data with a large number of degrees of freedom (necessarily used to represent rich and subtle facial expressions). In this paper, we present a new method for automatic non-rigid registration of 3D dynamic facial data using least-squares conformal maps, and based on this registration method, we also develop a new framework of facial expression synthesis and transfer. Nowadays more and more 3D dynamic, densely-sampled data become prevalent with the advancement of novel 3D scanning techniques. To analyze and utilize such huge 3D data, an efficient non-rigid registration algorithm is needed to establish one-to-one inter frame correspondences. Towards this goal, a non-rigid registration algorithm of 3D dynamic facial data is developed by using least-squares conformal maps with additional feature correspondences detected by employing active appearance models (AAM). The proposed method with additional, interior feature constraints guarantees that the non-rigid data will be accurately registered. The least-squares conformal maps between two 3D surfaces are globally optimized with the least angle distortion and the resulting 2D maps are stable and one-to-one. Furthermore, by using this non-rigid registration method, we develop a new system of facial expression synthesis and transfer. Finally, we perform a series of experiments to evaluate our non-rigid registration method and demonstrate its efficacy and efficiency in the applications of facial expression synthesis and transfer.","Face detection,
Facial animation,
Computer vision,
Graphics,
Humans,
Tracking,
Algorithm design and analysis,
Heuristic algorithms,
Active appearance model,
Deformable models"
Selective Writeback: Reducing Register File Pressure and Energy Consumption,"Much of the complexity in today's superscalar microprocessors stems from the need to maintain the speculatively produced results within the on-chip storage components until these results can be safely discarded without endangering the reconstruction of the precise state or impeding the recovery from possible branch misspeculations. For this, modern designs use large, heavily-ported physical register files (RFs) to increase the instruction throughput. The high complexity and power dissipation of such RFs mainly stem from the need to maintain each and every result for a large number of cycles after the result generation. We observed that a significant fraction (about 45%) of the result values are delivered to their consumers via the bypass network (consumed ldquoon-the-flyrdquo) and are never read out from the destination registers. In this paper, we first formulate conditions for identifying such transient values and describe their microarchitectural implementation; then we propose a technique to avoid the writeback of such transient values into the RF. With 64-entry integer and floating point register files, our technique achieves an 11% performance improvement and 29% reduction in the RF energy consumption compared to the baseline machine with the same number of registers. Furthermore, for the same performance target, the selective writeback scheme results in a 38% reduction in the energy consumption of the RF compared to the baseline machine.","Energy consumption,
Radio frequency,
Registers,
Microprocessors,
Computer science,
Radiofrequency identification,
Energy storage,
Impedance,
Throughput,
Power dissipation"
Development of the evaluation system for the Airway Management Training System WKA-1R,"The emerging field of medical robotics is aiming in introducing intelligent tools. More recently, thanks to the innovations on robot technology (RT), advanced medical training systems have been introduced to improve the skills of trainees. The principal challenges of developing efficient medical training systems are simulating real-world conditions and assuring their effectiveness. Up to now, different kinds of medical training devices have been developed which are designed to reproduce with high fidelity the human anatomy. Due to their design concept, the evaluation of progress of the trainees is based on subjective assessments limiting the understanding of their effectiveness. In this paper, we are presenting our research towards developing a patient robot designed to simulate the real-world task conditions and providing objective assessments of the training achievements. Due to its complexity; in this paper, we are presenting as a first approach the development of the Waseda-Kyotokagaku Airway No. 1R (WKA-1R) which includes a human patient model with embedded sensors in order to provide objective assessments of the training progress. In particular, we have proposed an evaluation function to quantitatively evaluate the task performance by determining the weighting of coefficients. In order to determine the weighting of coefficients, we applied discriminant analysis. In order to determine the effectiveness of the proposed evaluation function to detect differences among different levels of expertise, an experimental setup was carried out. From the experimental results, we could find a significant difference between both groups (P≪0.05).","Management training,
Medical robotics,
Minimally invasive surgery,
Intelligent robots,
Robot sensing systems,
Disaster management,
Medical simulation,
Humans,
Biomedical engineering,
Mechanical engineering"
Bandwidth and Price Competitions of Wireless Service Providers in Two-Stage Spectrum Market,"Significant technology progress has been witnessed in the research area of dynamic spectrum access. However, the success of dynamic spectrum access will not be possible without the evolution of an spectrum and service market which is both stable and efficient. In this paper, we investigate the dynamic access spectrum from the economic point of view. We study three-layer spectrum market with spectrum holder, wireless service provider and end users. In a duopoly situation, two wireless service providers participate in bandwidth competition to purchase spectrum and price competition to attract end users, with the aim of maximizing their own profit. We formulate the wireless service providers' competition as a two-stage game. Under general assumptions about the pricing and demand functions, a unique equilibrium is identified as the outcome of the game, which shows the stability of the market. We further evaluate the market efficiency in a special case of symmetric wireless service providers and affine pricing and demand functions. The result shows the efficiency of the equilibrium is of reasonable level even with non-cooperative wireless service providers.","Bandwidth,
Pricing,
Radio spectrum management,
Cognitive radio,
Contracts,
Communications Society,
Computer science,
Stability,
Resource management,
Radio control"
Localized minimum-latency broadcasting in multi-radio multi-rate wireless mesh networks,"We address the problem of minimizing the worst-case broadcast delay in “multi-radio multi-channel multi-rate wireless mesh networks” (MR2-MC WMN) in a distributed and localized fashion. Efficient broadcasting in such networks is especially challenging due to the desirability of exploiting the “wireless broadcast advantage” (WBA), the interface-diversity, the channel-diversity and the rate-diversity offered by these networks. We propose a framework that calculates a set of forwarding nodes and transmission rate at these forwarding nodes irrespective of the broadcast source. Thereafter, a forwarding tree is constructed taking into consideration the source of broadcast. Our broadcasting algorithms are distributed and utilize locally available information. We present a detailed performance evaluation of our distributed and localized algorithm and demonstrate that our algorithm can greatly improve broadcast performance by exploiting the rate, interface and channel diversity of MR2-MC WMNs and match the performance of centralized algorithms proposed in literature while utilizing only limited two-hop neighborhood information.","Broadcasting,
Delay,
Topology,
Wireless communication,
Face,
Throughput,
Complexity theory"
Effects of vibratory actuation on endoscopic capsule vision,"Current research in capsule endoscopy aims at endowing the capsules with some means of actively propelling themselves inside the gastrointestinal (GO tract, as opposed Advantages of these active capsules are the sipificant potential in the duration of the associated diagnostic procedures. as well as the oossihilitv to direct the line-of-sieht of the oh-hoard cameras towards interesting features of the GI tissue. One such means of active propulsion is by vibratory actuation, employing eccentric-mass micromotors, which is shown to reduce the friction of the capsule with the GI tract. The effect of vibrations on the quality of the acquired images is explored in the present study, which demonstrates that such vibrations do not affect adversely the diagnostic effectiveness of the endoscopic capsules. The parameters of vibratory actuation are evaluated as to the loss bf high-frequency information in the acquired images, due to the induced motion blur, and appropriate design guidelines for the vibratory actuation system are established. The validity of this study has been evaluated by ex-vivo and in-vivo experiments.",
Virtual Colonoscopy Screening With Ultra Low-Dose CT and Less-Stressful Bowel Preparation: A Computer Simulation Study,"Computed tomography colonography (CTC) or CT-based virtual colonoscopy (VC) is an emerging tool for detection of colonic polyps. Compared to the conventional fiber-optic colonoscopy, VC has demonstrated the potential to become a mass screening modality in terms of safety, cost, and patient compliance. However, current CTC delivers excessive X-ray radiation to the patient during data acquisition. The radiation is a major concern for screening application of CTC. In this work, we performed a simulation study to demonstrate a possible ultra low-dose CT technique for VC. The ultra low-dose abdominal CT images were simulated by adding noise to the sinograms of the patient CTC images acquired with normal dose scans at 100 mA s levels. The simulated noisy sinogram or projection data were first processed by a Karhunen-Loeve domain penalized weighted least-squares (KL-PWLS) restoration method and then reconstructed by a filtered backprojection algorithm for the ultra low-dose CT images. The patient-specific virtual colon lumen was constructed and navigated by a VC system after electronic colon cleansing of the orally-tagged residue stool and fluid. By the KL-PWLS noise reduction, the colon lumen can successfully be constructed and the colonic polyp can be detected in an ultra low-dose level below 50 mA s. Polyp detection can be found more easily by the KL-PWLS noise reduction compared to the results using the conventional noise filters, such as Hanning filter. These promising results indicate the feasibility of an ultra low-dose CTC pipeline for colon screening with less-stressful bowel preparation by fecal tagging with oral contrast.","Virtual colonoscopy,
Computer simulation,
Computed tomography,
Colonography,
Colon,
Colonic polyps,
Noise reduction,
X-ray imaging,
Filters,
Radiation safety"
Applying Computational Science to Education: The Molecular Workbench Paradigm,The molecular workbench offers highly interactive molecular dynamics simulations to help students learn difficult scientific concepts. The software demonstrates how scientists can transform research tools into educational tools. Research studies show that students learn better using computational models.,"Computational modeling,
Power engineering computing,
Engines,
Biological system modeling,
Analytical models,
Embedded computing,
Biology computing,
Navigation,
Visualization,
Software tools"
Human action recognition using Local Spatio-Temporal Discriminant Embedding,"Human action video sequences can be considered as nonlinear dynamic shape manifolds in the space of image frames. In this paper, we address learning and classifying human actions on embedded low-dimensional manifolds. We propose a novel manifold embedding method, called Local Spatio-Temporal Discriminant Embedding (LSTDE). The discriminating capabilities of the proposed method are two-fold: (1) for local spatial discrimination, LSTDE projects data points (silhouette-based image frames of human action sequences) in a local neighborhood into the embedding space where data points of the same action class are close while those of different classes are far apart; (2) in such a local neighborhood, each data point has an associated short video segment, which forms a local temporal subspace on the embedded manifold. LSTDE finds an optimal embedding which maximizes the principal angles between those temporal subspaces associated with data points of different classes. Benefiting from the joint spatio-temporal discriminant embedding, our method is potentially more powerful for classifying human actions with similar space-time shapes, and is able to perform recognition on a frame-by-frame or short video segment basis. Experimental results demonstrate that our method can accurately recognize human actions, and can improve the recognition performance over some representative manifold embedding methods, especially on highly confusing human action types.",
Bayesian reinforcement learning in continuous POMDPs with application to robot navigation,"We consider the problem of optimal control in continuous and partially observable environments when the parameters of the model are not known exactly. Partially Observable Markov Decision Processes (POMDPs) provide a rich mathematical model to handle such environments but require a known model to be solved by most approaches. This is a limitation in practice as the exact model parameters are often difficult to specify exactly. We adopt a Bayesian approach where a posterior distribution over the model parameters is maintained and updated through experience with the environment. We propose a particle filter algorithm to maintain the posterior distribution and an online planning algorithm, based on trajectory sampling, to plan the best action to perform under the current posterior. The resulting approach selects control actions which optimally trade-off between 1) exploring the environment to learn the model, 2) identifying the system’s state, and 3) exploiting its knowledge in order to maximize long-term rewards. Our preliminary results on a simulated robot navigation problem show that our approach is able to learn good models of the sensors and actuators, and performs as well as if it had the true model.","Bayesian methods,
Learning,
Navigation,
Optimal control,
Mathematical model,
Particle filters,
Trajectory,
Sampling methods,
Robot sensing systems,
Motion planning"
Performance enhancement in solving Traveling Salesman Problem using hybrid genetic algorithm,"In this paper, a novel hybrid genetic algorithm for solving Traveling Salesman Problem (TSP) is presented based on the Nearest Neighbor heuristics and pure Genetic Algorithm (GA). The hybrid genetic algorithm exponentially derives higher quality solutions in relatively shorter time for hard combinatorial real world optimization problems such as Traveling Salesman Problem (TSP) than the pure GA. The hybrid algorithm outperformed the NN algorithm and the pure Genetic Algorithm taken separately. The hybrid genetic algorithm is designed and experimented against the pure GA and the convergence rate improved by more than 200% and the tour distance improved by 17.4% for 90 cities. These results indicate that the hybrid approach is promising and it can be used for various other optimization problems. This algorithm is also independent of the start city of travel whereas the result of NN algorithm are based on start city.","Traveling salesman problems,
Genetic algorithms,
Biological cells,
Cities and towns,
Neural networks,
Computer science,
Nearest neighbor searches,
Encoding,
Computational modeling,
Genetic mutations"
Rotation-Invariant Texture Retrieval via Signature Alignment Based on Steerable Sub-Gaussian Modeling,"This paper addresses the construction of a novel efficient rotation-invariant texture retrieval method that is based on the alignment in angle of signatures obtained via a steerable sub-Gaussian model. In our proposed scheme, we first construct a steerable multivariate sub-Gaussian model, where the fractional lower-order moments of a given image are associated with those of its rotated versions. The feature extraction step consists of estimating the so-called covariations between the orientation subbands of the corresponding steerable pyramid at the same or at adjacent decomposition levels and building an appropriate signature that can be rotated directly without the need of rotating the image and recalculating the signature. The similarity measurement between two images is performed using a matrix-based norm that includes a signature alignment in angle between the images being compared, achieving in this way the desired rotation-invariance property. Our experimental results show how this retrieval scheme achieves a lower average retrieval error, as compared to previously proposed methods having a similar computational complexity, while at the same time being competitive with the best currently known state-of-the-art retrieval system. In conclusion, our retrieval method provides the best compromise between complexity and average retrieval performance.","Image retrieval,
Image databases,
Information retrieval,
Feature extraction,
Computer science,
Spatial databases,
Iron,
Samarium,
Rotation measurement,
Performance evaluation"
An algorithm of association rules double search mining based on binary,"In order to solve these problems how to easily generate candidate frequent item sets and fast compute support of candidate item sets, an algorithm of association rules mining based on binary has been introduced. However, one presented binary mining algorithm is only suitable for mining some relative short frequent item sets since the way of generating candidate item sets is also similar to apriori, another is only suitable for mining long frequent item sets, which generates candidate item sets by up-down search strategy. And so aiming to mining general frequent item sets, this paper proposes an algorithm of association rules double search mining based on binary, which is different from tradition association rules mining algorithm based on double search strategy. The algorithm doesnpsilat use combination of set theory to generate candidate item sets but binary logic operation that is also used to compute support of candidate item sets, which can use character digital to reduce the number of scanned transaction. The algorithm gets rid of shortage about some presented algorithms based on binary. The experiment based on above three algorithms indicates that the efficiency about double search strategy is fast and efficient when mining general frequent item sets which arenpsilat confined.","Algorithm design and analysis,
Association rules,
Databases,
Arrays,
Bismuth,
Data mining,
Machine learning"
A Framework for Agent-Based Trust Management in Online Auctions,"Current electronic commerce applications such as online auction systems are not trustworthy due to a lack of effective trust management mechanisms. A trustworthy online auction system requires a dynamic trust management module that can detect abnormal bidding activities in real-time, notify the involved users, and cancel the corresponding auction immediately. In this paper, we present a general framework for agent-based trust management (ATM) in online auctions. The ATM module consists of three types of agents, namely the monitoring agent, the analysis agent and the security agent. A monitoring agent can monitor a bidder and detect any abnormal bidding behavior; while the analysis agent and the security agent can analyze state-based information and history information of a bidder, and make decisions on shill detection, respectively. We illustrate the communication protocol among various agents, and demonstrate our agent-based trust management approach for online auctions using a prototype ATM module developed with JADE.",
Target-directed attention: Sequential decision-making for gaze planning,"It is widely agreed that efficient visual search requires the integration of target-driven top-down information and image-driven bottom-up information. Yet the problem of gaze planning - that is, selecting the next best gaze location given the current observations - remains largely unsolved. We propose a probabilistic system that models the gaze sequence as a finite-horizon Bayesian sequential decision process. Direct policy search is used to reason about the next best gaze locations. The system integrates bottom-up saliency information, top-down target knowledge and additional context information through principled Bayesian priors. This results in proposal gaze locations that depend not only the featural visual saliency, but also on prior knowledge and the spatial likelihood of locating the target. The system has been implemented using state-of-the-art object detectors and evaluated on a real-world dataset by comparing it to gaze sequences proposed by a pure bottom-up saliency-based process and to an object detection approach that analyzes the full image. The target-directed attention system is shown to result in higher object detection precision than both competitors, to attend to more relevant targets than the bottom-up attention system, and to require significantly less computation time than the exhaustive approach.","Decision making,
Object detection,
Detectors,
Bayesian methods,
Image analysis,
Robot vision systems,
Cameras,
Robotics and automation,
USA Councils,
Laboratories"
Facial aging simulation based on super-resolution in tensor space,"The facial change caused by aging progression might significantly degrade the performance of a face recognition system. One major way to deal with this problem is to predict the aging process. The work presented in this paper proposed a framework to simulate the face aging process by means of super-resolution. Considered the nature of multi-modalities in face image set, multi-linear algebra is introduced into the super-resolution method to represent and process the whole image set in tensor space. The simulating results represented in the paper are compared with the ground truth face image of the same people.","Aging,
Tensile stress,
Image resolution,
Deformable models,
Computational modeling,
Face recognition,
Pixel,
Algebra,
Computer simulation,
Computer science"
Advanced QoS methods for Grid workflows based on meta-negotiations and SLA-mappings,"In novel market-oriented resource sharing models, resource consumers pay for the resource usage and expect that non-functional requirements for the application execution, termed as Quality of Service (QoS), are satisfied. QoS is negotiated between two parties following the specific negotiation protocols and is recorded using Service Level Agreements (SLAs). However, most of the existing work assumes that the communication partners know about the SLA negotiation protocols and about the SLA templates before entering the negotiation. However, this is a contradictory assumption, if we consider computational Grids and novel, commercially oriented computing Clouds where consumers and providers meet each other dynamically. In this paper, we present novel meta-negotiation and SLA-mapping solutions for Grid workflows bridging the gap between current QoS models and Grid workflows, one of the most successful Grid programming paradigms. We illustrate the open research issues with a real world case study. Thereafter, we present document models for the specification of meta-negotiations and SLA-mappings. We discuss the architecture for the management of meta-negotiations and SLAmappings as well as integration of the architecture into a Grid workflow management framework.","Quality of service,
Protocols,
Grid computing,
Cloud computing,
Resource management,
Information systems,
Laboratories,
Computer science,
Software engineering,
Application software"
Reliable adaptable Network RAM,"We present reliability solutions for adaptable Network RAM systems running on general-purpose clusters. Network RAM allows nodes with over-committed memory to swap pages over the network, storing them in the idle RAM of other nodes and avoiding swapping to slow, local disk. An adaptable Network RAM system adjusts the amount of RAM currently available for storing remotely swapped pages in response to changes in nodes’ local RAM usage. It is important that Network RAM systems provide reliability for remotely swapped page data. Without reliability, a single node failure can result in failure of unrelated processes running on other nodes by losing their remotely swapped pages. Adaptable Network RAM systems pose extra difficulties in providing reliability because each node’s capacity for storing remotely swapped pages changes over time, and because pages may move from node to node in response to these changes. Our novel dynamic RAID-based reliability solutions use idle RAM for storing page and reliability data, avoiding using slow disk for reliability. They are designed to work with the adaptive nature of our Network RAM system (Nswap), allowing page and reliability data to migrate from node to node and allowing pages to be added to or removed from different parity groups. Additionally, page recovery runs concurrently with cluster applications, so that cluster applications do not have to wait until all data from a failed node is recovered before resuming execution. We present results comparing Nswap to disk swapping for a set of benchmarks running on our gigabit cluster. Our results show that reliable Nswap is up to 32 times faster than swapping to disk, and that there is virtually no impact on the performance of applications as they run concurrently with page recovery.","Servers,
Random access memory,
Reliability,
Kernel,
Peer to peer computing,
Computer network reliability,
Linux"
The Content Pollution in Peer-to-Peer Live Streaming Systems: Analysis and Implications,"There has been significant progress in the development and deployment of Peer-to-Peer (P2P) live video streaming systems. However, there has been little study on the security aspect in such systems. Our prior experiences in Anysee exhibit that existing systems are largely vulnerable to intermediate attacks, in which the content pollution is a common attack that can significantly reduce the content availability, and consequently impair the playback quality. This paper carries out a formal analysis of content pollution and discusses its implications in P2P live video streaming systems. Specifically, we establish a probabilistic model to capture the progress of content pollution. We verify the model using a real implementation based on Anysee system; we evaluate the content pollution effect through extensive simulations. We demonstrate that (1) the number of polluted peers can grow exponentially, similar to random scanning worms. This is vital that with 1% polluters, the overall system can be compromised within minutes; (2) the effective bandwidth utilization can be sharply decreased due to the transmission of polluted packets; (3) Augmenting the number of polluters does not imply a faster progress of content pollution, in which the most influential factors are the peer degree and access bandwidth. We further examine several techniques and demonstrate that a hash-based signature scheme can be effective against the content pollution, in particular when being used during the initial phase.","Pollution,
Peer to peer computing,
Bandwidth,
Streaming media,
Availability,
Grippers,
Topology"
A new approach to content-based file type detection,"File type identification and file type clustering may be difficult tasks that have an increasingly importance in the field of computer and network security. Classical methods of file type detection including considering file extensions and magic bytes can be easily spoofed. Content-based file type detection is a newer way that is taken into account recently. In this paper, a new content-based method for the purpose of file type detection and file type clustering is proposed that is based on the PCA and neural networks. The proposed method has a good accuracy and is fast enough.","Feature extraction,
Artificial neural networks,
Principal component analysis,
Neurons,
Probability density function,
Computers,
Accuracy"
A Singlehop Collaborative Feedback Primitive for Wireless Sensor Networks,"To achieve scalability, energy-efficiency, and timeliness, wireless sensor network deployments increasingly employ in-network processing. In this paper, we identify singlehop feedback collection as a key building block for in-network processing applications, and introduce a basic singlehop primitive, pollcast. The key idea behind this primitive is to exploit the receiver-side collision detection information at the MAC-layer to speed-up collaborative feedback collection. Using pollcast, a node can get an affirmation about the existence of a node-level predicate P in its neighborhood in constant time by asking all nodes where P hold to reply simultaneously. We have implemented pollcast on Tmotes using Chipcon 2420 radio. Our results show that this primitive is indeed lightweight, resilient, and effective. Our paper is also the first time receiver-side collision detection is achieved in a practical manner for Chipcon 2420 radio.","Collaboration,
Feedback,
Wireless sensor networks,
Broadcasting,
Peer to peer computing,
Delay,
Protocols,
Voting,
Communications Society,
Computer science"
Cloaker: Hardware Supported Rootkit Concealment,"Rootkits are used by malicious attackers who desire to run software on a compromised machine without being detected. They have become stealthier over the years as a consequence of the ongoing struggle between attackers and system defenders. In order to explore the next step in rootkit evolution and to build strong defenses, we look at this issue from the point of view of an attacker. We construct Cloaker, a proof-of-concept rootkit for the ARM platform that is non-persistent and only relies on hardware state modifications for concealment and operation. A primary goal in the design of Cloaker is to not alter any part of the host operating system (OS) code or data, thereby achieving immunity to all existing rootkit detection techniques which perform integrity, behavior and signature checks of the host OS. Cloaker also demonstrates that a self-contained execution environment for malicious code can be provided without relying on the host OS for any services. Integrity checks of hardware state in each of the machine's devices are required in order to detect rootkits such as Cloaker. We present a framework for the Linux kernel that incorporates integrity checks of hardware state performed by device drivers in order to counter the threat posed by rootkits such as Cloaker.","Hardware,
Kernel,
Virtual machining,
Operating systems,
Counting circuits,
Mobile handsets,
Computer security,
Privacy,
Computer science,
Immune system"
Centralized processing and distributed I/O for robot control,"Historically, the architecture of robot controllers has been dictated by technology constraints. When computers and networks were slow, it was necessary to logically distribute the computation but to physically centralize both the computation and I/O. This is represented by a controller comprised of large rack of embedded processors, with cables to all robot sensors and actuators. As technology improved, it became feasible to distribute both computation and I/O, as illustrated by systems that use a field bus to connect a central controller to low-level processors embedded near or within the robot. This paper advocates a new architecture, centralized processing and distributed I/O, that is enabled by current computer and network technology. This architecture provides benefits in academic environments because it simplifies development of robot control software. The feasibility of this approach is demonstrated by a custom robot controller that uses IEEE 1394 (FireWire) to provide direct communication between a central computer and non-intelligent peripheral hardware devices.","Robot control,
Distributed computing,
Computer networks,
Embedded computing,
Computer architecture,
Physics computing,
Centralized control,
Robot sensing systems,
Firewire,
Cables"
A review of applications of computer games in education and training,"Scientists, engineers and educators are increasingly using environments enabled by advanced cyberinfrastructure tools for their research, formal and informal education and training, career development and life-long learning. For instance, academic institutions as well as private training and education companies have recently started to explore the potential of commercially available multi-player computer game engines for the development of virtual environments for instructional purposes. Most of these developments are still in their early stages and are focused mainly on investigating the suitability of interactive games for remote user interaction, content distribution and collaborative activities. Some of the ongoing projects have additional research objectives, such as the analysis of patterns of human behavior and the study of the collaboration between users and their interaction with virtual environments. A few other developments are aimed at utilizing computer game technologies as a platform for personnel training and educational laboratory simulations. This paper provides a review of the current state of computer game applications, with a special focus on education and training implementations.","Application software,
Computer applications,
Computer science education,
Virtual environment,
Collaboration,
Career development,
Computer aided instruction,
Engines,
Pattern analysis,
Humans"
Power management by grid-connected inverters using a voltage and current control strategy for Microgrid applications,"This paper presents the application of a suitable control strategy for multibus Microgrid applications. The control strategy, used with each distributed generation system in the Microgrid is developed so as to combine the advantages of the current control and the voltage control strategies. The performance of the proposed application in terms of power management has been verified in simulation on a Microgrid test.","Inverters,
Voltage control,
Reactive power,
Current control,
Load flow,
Impedance,
Generators"
Weighted Pixel Statistics for multispectral image classification of remote sensing signatures: Performance study,"The extraction of remote sensing signatures from a particular geographical region allows the generation of electronic signature maps, which are the basis to create a high-resolution collection atlas processed in continuous discrete time. This can be achieved using a new multispectral image classification approach based on pixel statistics for the class description. This is referred to as the Weighted Pixel Statistics Method. This paper explores the effectiveness of this novel approach developed for supervised segmentation and classification of remote sensing signatures, with a comparison with the traditional Weighted Order Statistics Method. The extraction of remote sensing signatures from real-world high-resolution environmental remote sensing imagery is reported to probe the efficiency of the developed technique.","Pixel,
Statistics,
Multispectral imaging,
Remote sensing,
Filters,
Image segmentation,
Resource management,
High performance computing,
Automatic control,
Urban planning"
Map: a scalable monitoring system for dependable 802.11 wireless networks,"Many enterprises deploy 802.11 wireless networks for mission-critical operations; these networks must be protected for dependable access. This article introduces the MAP project, which includes a scalable 802.11 measurement system that can provide continuous monitoring of wireless traffic to quickly identify threats and attacks. We discuss the MAP system architecture, design decisions, and evaluation results from a real testbed.","Wireless networks,
Protection,
Media Access Protocol,
Telecommunication traffic,
Computer science,
Switches,
Computerized monitoring,
Sampling methods,
Wireless LAN,
Particle measurements"
Use and Influence of Creative Ideas and Requirements for a Work-Integrated Learning System,"In this paper, we describe a creativity workshop that was used in a large research project, called APOSDLE, to generate creative ideas and requirements for a work-integrated learning system. We present an analysis of empirical data collected during and after the workshop. On the basis of this analysis, we conclude that the work-shop was an efficient way of generating ideas for future system development. These ideas, on average, were used at least as much as requirements from other sources in writing use cases, and 18 months after the workshop were seen to have a similar degree of influence on the project to other requirements. We make some observations about the use of more and less creative ideas, and about the techniques used to generate them. We end with suggestions for further work.","Learning systems,
Design engineering,
Human computer interaction,
Data analysis,
Productivity,
Writing,
Time factors,
Environmental economics,
Government,
Technological innovation"
Power Pole Detection Based on Graph Cut,"Power pole detection from images is an important problem in the future electric power industry application. A precise detection is essential to inspect the defects of a power pole. In this paper, we propose a novel approach to detect the power pole object from images. Graph cut for image segmentation is a newly developing graph based image segmentation technique. It is effective but takes huge computation burden. The proposed approach combines prior knowledge with graph cut into detection. Firstly, it locates the rough region of the power pole to obtain two restricted regions based on prior rules. Then, a traditional graph cut framework is used in the restricted regions to improve the precision of segmentation. Experimental results verify its efficiency and accuracy.","Image segmentation,
Power system modeling,
Object detection,
Pixel,
Computational efficiency,
Inspection,
Signal processing,
Computer science,
Industry applications,
Lighting"
Evaluation of Quality Attribute Variability in Software Product Families,"Software product family or line is a software engineering paradigm that systematizes reuse. In Software Product Line Engineering, two phases are distinguished: Domain Engineering which is in charge of developing a common infrastructure and assets and Application Engineering which makes use of those assets to generate the products. One of the key aspects of product lines is variability and its management. However, the main focus has been on functional variability and quality variability in software product lines has not received so much attention by researchers. In a product line different members of the line may require different levels of a quality requirement, for instance they could differ in terms of their availability, security, reliability, etc. Due to this variability, quality evaluation in software product lines is much more complicated that in single-systems. One alternative is to evaluate all the products of a line but it is very expensive and ways of reducing evaluation efforts are necessary. In this direction, the paper presents a method for facilitating cost-effective quality evaluation of a product line taking into consideration variability on quality attributes.",
Rounding Parallel Repetitions of Unique Games,"We show a connection between the semidefinite relaxation of uniquegames and their behavior under parallel repetition.  Specifically,denoting by
val(G)
the value of a two-prover unique game
G
, andby
sdpval(G)
the value of a natural semidefinite program toapproximate val(G), we prove that for every
ℓ∈ℕ
, if
sdpval(G)≥1−δ
, then
val(
G
ℓ
)≥1−
sℓδ
‾
‾
‾
‾
√
.
Here,
G
ℓ
denotes the
ℓ
-foldparallel repetition of
G
, and
s=O(log(k/δ))
, where
k
denotes the alphabet size of the game. For the special case where
G
is an XOR game (i.e.,
k=2
), we obtain the same bound but with
s
asan absolute constant.  Our bounds on
s
are optimal up to a factor of
O(log(1/δ))
.For games with a significant gap between the quantities
val(G)
and
sdpval(G)
, our result implies that
val(
G
ℓ
)
may be muchlarger than
val(G
)
ℓ
, giving a counterexample to the strongparallel repetition conjecture. In a recent breakthrough, Raz (FOCS'08) has shown such an example using the max-cut game on oddcycles. Our results are based on a generalization of his techniques.","Computer science,
Quantum mechanics,
Mathematics,
Contracts,
Councils"
Mobile Pee-to-Peer systems using Super Peers for Mobile Environments,"As the number of mobile device users increases, many researches on pee-to-peer (P2P) systems in mobile environments have been carried out. In this paper, we propose two mobile P2P systems that have double-layered topology. In these systems, peers are classified into two groups, super peers and sub-peers and a super peer has zero or more sub-peers. In the proposed systems, when a peer wants to search a file, there is no need of multi-broadcasting because each super peer maintains the appropriate information of its sub-peers and because a pair of super peers communicate each other along only the route from one super peer to the other via some sub-peers since there may be no direct connection between a pair of super peers. The experimental results show that the proposed systems outperform a typical mobile P2P system in terms of the average number of messages to find target files while maintaining the same search accuracy.","Network servers,
Cellular phones,
Personal digital assistants,
Motion pictures,
Routing,
Network topology,
Peer to peer computing,
Search methods"
Protein Conformational Search Using Bees Algorithm,"Proteins perform many biological functions in the human body. The structure of the protein determines its function. In order to predict the protein structure computationally, protein must be represented in a proper representation. To this end, an energy function is used to calculate its energy and a conformational search algorithm is used to search the conformational search space to find the lowest free energy conformation. In this paper, the Bees Algorithm, i.e. a Swarm Intelligence based algorithm inspired by the foraging behaviour of honey bees colony, is adapted to search the protein conformational search space. The algorithm was able to find the lowest free energy conformation of Met enkephaline using ECEPP/2 force fields.","Amino acids,
Computer science,
Particle swarm optimization,
Humans,
Nuclear magnetic resonance,
Protein sequence,
Testing,
Insects,
Neural networks,
Control charts"
Test Set Development for Cache Memory in Modern Microprocessors,"Up to 53% of the time spent on testing current Intel microprocessors is needed to test on-chip caches, due to the high complexity of memory tests and to the large amount of transistors dedicated to such memories. This paper discusses the methodology used to develop effective and efficient cache tests, and the way it is implemented to optimize the test set used at Intel to test their 512-kB caches manufactured in a 0.13- mum technology. An example is shown where a maximal test set of 15 tests with a corresponding maximum test time of 160.942 ms/chip is optimized to only six tests that require a test time of only 30.498 ms/chip.",
Block-Level Relaxation for Timing-Robust Asynchronous Circuits Based on Eager Evaluation,"As variability and timing closure become critical challenges in synchronous CAD flows, one attractive alternative is to use robust asynchronous circuits which gracefully accommodate timing discrepancies. In this approach, each gate in an initial Boolean netlist is typically replaced by a robust dual-rail asynchronous template. However, these circuits typically have significant area and latency overhead. A gate-level relaxation approach has recently been proposed: replacing selected simple gates by asynchronous templates performing eager evaluation, without affecting the circuit's overall timing robustness. In this paper, the approach has been significantly extended to block-level relaxation: handling arbitrarily complex multi-output blocks. For these circuits, a much wider range of optimizations is applicable than in the gate-level approach. A block-level relaxation algorithm is implemented, and experiments performed on several high-speed arithmetic circuits (Brent-Kung and Kogge-Stone adders, combinational multipliers). On average, 38.4\% of the blocks could be relaxed (48.4\% best-case),with area improvement of 27.2% (49.7% best-case)and delay improvement of 13.1% (25.5% best-case) for the critical path,while still preserving the circuit's overall timing robustness.","Asynchronous circuits,
Timing,
Robustness,
Delay,
Temperature,
Thin film transistors,
Wire,
Design automation,
Computer science,
Energy consumption"
Intruder detection using a wireless sensor network with an intelligent mobile robot response,"In this paper, we present an intruder detection system that uses a wireless sensor network and mobile robots. The sensor network uses an unsupervised fuzzy Adaptive Resonance Theory (ART) neural network to learn and detect intruders in a previously unknown environment. Upon the detection of an intruder, a mobile robot travels to the position where the intruder is detected to investigate. The wireless sensor network uses a hierarchical communication/learning structure, where the mobile robot is the root node of the tree. Our fuzzy ART network is based on Kulakov and Davcev’s implementation [6]. We enhanced the fuzzy ART neural network to learn a time-series and detect time-related changes using a Markov model. The proposed architecture is tested on physical hardware. Our results show that our enhanced detection system has a higher accuracy than the basic, original, fuzzy ART system.","Wireless sensor networks,
Intelligent networks,
Intelligent robots,
Intelligent sensors,
Mobile robots,
Subspace constraints,
Fuzzy neural networks,
Neural networks,
Resonance,
Mobile communication"
Costs and Benefits of Model-based Diagnosis,"Over the past 20 years, there has been much work in the area of model-based diagnosis (MBD). By this we mean diagnosis systems arising from computer science or artificial intelligence approaches where a generic software engine is developed to address a large class of diagnosis problems. Later, models are created to apply the engine to a specific problem. These techniques are very attractive, suggesting a vision of machines that repair themselves, reduced costs for all kinds of endeavors, spacecraft that continue their missions even when failing, and so on. This promise inspired a broad range of activity, including our involvement over several years in flying the Livingstone and Livingstone 2 on-board model-based diagnosis and recovery systems as experiments on two spacecraft. While a great deal was learned through a variety of applications to simulators, testbeds and flight experiments, no project adopted the technology in operations and the expected benefits have not yet come to fruition. This led us to ask what are the costs of using MBD for the operational scenarios we encountered, what are the benefits, and how do we approach the question of whether the benefits outweigh the costs? How are missions today approaching fault diagnosis and recovery during operations? If we characterize the cost and benefits of using MBD, how would it compare with traditional ways of making a system more robust? How did expectations for MBD compare to benefits seen in the field and why? The literature does provide existing cost models for related endeavors such as integrated vehicle health management. It also provides excellent narratives of why projects chose not to use MBD after considering it. However, we believe that this paper is the first to unpack and discuss the cost, benefit and risk factors that impact the net value of model-based diagnosis and recovery. We use experience with systems such as Livingstone as an example, so our focus is on-board model-based diagnosis and recovery, but we believe many of the insights and remaining questions on the costs and benefits are applicable to other diagnosis applications. Quantitative model of when on-board model-based diagnosis would be an effective choice, it lays out the cost/benefit proposition and identifies several disconnects that we believe prevent adoption as an operational tool. While we do not suggest metrics for every cost, benefit and risk factor we identify, we do discuss where each factor arises in development or operations and how model-based diagnosis and recovery tends to leverage or exacerbate each. As such we believe the analysis is of use to those developing MBD or related techniques and those who may employ them. It also serves as one example of how honest expectations based on technical capability can come to differ from the net impact on customer problems. In this paper we present a cost/benefit analysis for MBD, using expectations and experiences with Livingstone as an example. We provide an overview of common techniques for making spacecraft robust, citing fault protection schemes from recent missions. We lay out the cost, benefit and risk advantages associated with on-board MBD, and use the examples to probe each expected advantage in turn. We conclude our analysis with a summary of our method for analyzing the costs and benefits in a particular domain, and encourage others to come forward with analyses of costs and benefits for fielded systems. Finally, we discuss related work both in terms of similar analyses and fielded systems.",
Texture Segmentation by Genetic Programming,"This paper describes a texture segmentation method using genetic programming (GP), which is one of the most powerful evolutionary computation algorithms. By choosing an appropriate representation texture, classifiers can be evolved without computing texture features. Due to the absence of time-consuming feature extraction, the evolved classifiers enable the development of the proposed texture segmentation algorithm. This GP based method can achieve a segmentation speed that is significantly higher than that of conventional methods. This method does not require a human expert to manually construct models for texture feature extraction. In an analysis of the evolved classifiers, it can be seen that these GP classifiers are not arbitrary. Certain textural regularities are captured by these classifiers to discriminate different textures. GP has been shown in this study as a feasible and a powerful approach for texture classification and segmentation, which are generally considered as complex vision tasks.",
VisualJVM: A Visual Tool for Teaching Java Technology,"This paper presents a laboratory session of an advanced programming course to introduce students to the technology involved with the Java programming language. In this special lab session the educational software tool VisualJVM is used, providing a graphical front-end to a Java virtual machine (JVM). This tool helps students learn about JVM architecture, learn how JVM works, and consequently, to understand why a Java program is platform independent. The student reaction to this experience was very positive and the authors are planning to use the tool in other contexts.","Java,
Programming,
Computer languages,
Programming profession,
Education,
Computer architecture,
Linux"
ECHO: A Quality of Service Based Endpoint Centric Handover Scheme for VoIP,Existing terminal oriented handover mechanisms capable of meeting the strict delay bounds of real time applications such as VoIP do not consider the QoS of candidate handover networks. In this paper ECHO - a QoS based handover solution for VoIP- is proposed. ECHO is endpoint centric and does not require any network support; it leverages the SCTP transport protocol. ECHO incorporates network metrics that directly affect VoIP quality into the handover decision process. A dynamic variant of the ITU-T E-Model is used to calculate how the network metrics map to a user perceived voice quality metric known as the MOS. The MOS value is then used to make handover decisions between each of the available access networks. The results show that the addition of the QoS capabilities significantly improves the handover decisions that are made.,"Quality of service,
Telecommunication traffic,
Computer science,
Electronic mail,
Delay effects,
Transport protocols,
Wireless LAN,
Communications Society,
Informatics,
Educational institutions"
Segmentation of heart sound recordings from an electronic stethoscope by a duration dependent Hidden-Markov Model,"Digital stethoscopes offer new opportunities for computerized analysis of heart sounds. Segmentation of hearts sounds is a fundamental step in the analyzing process. However segmentation of heart sounds recorded with handheld stethoscopes in clinical environments is often complicated by recording and background noise. A duration-dependent hidden Markov model (DHMM) is proposed for robust segmentation of heart sounds. The DHMM model was developed and tested with heart sounds recorded at bedside with a commercially available handheld stethoscope. In a population of 60 patients, the DHMM identified 739 S1 and S2 sounds out of 744 which corresponded to a 99.3% sensitivity. There were seven incorrectly classified sounds which corresponded to a 99.1% positive predictive value. Our results suggest that DHMM could be a suitable method for segmentation of clinically recorded heart sounds.","Heart,
Stethoscope,
Hidden Markov models,
Hospitals,
Acoustic noise,
Background noise,
Cardiology,
Working environment noise,
Stochastic processes,
Cardiovascular diseases"
Output feedback control of linear systems with input and output quantization,"A considerable amount of research has been done on the use of logarithmic quantizers for networked feedback control systems. However, most results are developed for the case of a single quantizer (either measurement or control signal quantization). In this paper, we investigate the case of simultaneous input and output quantization for SISO linear output feedback systems. Firstly, we show that the problem of quadratic stabilization via quantized feedback can be addressed with no conservativeness by means of the sector bound approach. Secondly, we provide a bound on the maximal admissible sector bound via a scaled H¿ optimization problem.","Output feedback,
Linear feedback control systems,
Control systems,
Linear systems,
Quantization,
Communication system control,
Feedback control,
Communication channels,
Digital control,
Bandwidth"
Estimating the dominant person in multi-party conversations using speaker diarization strategies,"In this paper, we apply speaker diarization strategies from a single source to the task of estimating the dominant person in a group meeting. Previous work has shown that speaking length is strongly correlated with perceived dominance. Here we investigate this in more depth by considering two dominance tasks where there is full and majority agreement amongst ground-truth annotators. In addition, we investigate how 24 different speed-up and algorithmic strategies, and source types lead to interesting outcomes when applied to dominance estimation. We obtained the best performance of 77% using our slowest scheme and a single distant microphone (SDM). Within the top 3 out of 24 performing experiments in both dominance tasks, we show that we can use the furthest SDM, with no prior knowledge of the number of speakers and the fastest diarization scheme, which performs 1.3 times faster than real-time.","Computer science,
Humans,
Speech enhancement,
Microphone arrays,
Performance evaluation,
Length measurement,
Particle measurements,
Feature extraction,
Testing,
Computational complexity"
Biomimetic grasp planning for cortical control of a robotic hand,"In this paper we outline a grasp planning system designed to augment the cortical control of a prosthetic arm and hand. A key aspect of this system it the ability to combine online user input and autonomous planning to enable the execution of stable grasping tasks. While user input can ultimately be of any modality, the system is being designed to adapt to partial or noisy information obtained from grasp-related activity in the primate motor cortex. First, principal component analysis is applied to the observed kinematics of physiologic grasping to reduce the dimensionality of hand posture space and simplify the planning task for on-line use. The planner then accepts control input in this reduced-dimensionality space, and uses it as a seed for a hand posture optimization algorithm based on simulated annealing. We present two applications of this algorithm, using data collected from both primate and human subjects during grasping, to demonstrate its ability to synthesize stable grasps using partial control input in real or near-real time.",
The Effects of Computer-Assisted Learning in Teaching Permanent Magnet Synchronous Motors,"This research compares the use of computer-assisted learning (CAL) with the traditional learning methods in the teaching of permanent magnet synchronous motors (PMSMs). The present study was performed on an experimental group consisting of 16 students and a control group consisting of 15 students. In order to measure the students' success, a multiple-choice test of 23 items was used as the pretest and the posttest. The results show that the CAL method was more effective in increasing student success than the traditional method. An attitude scale of 17 items was applied to measure the attitudes of the experimental group students and a positive attitude towards CAL was found.","Fuzzy control,
Permanent magnet motors,
Computer aided instruction"
EDCA/CA: Enhancement of IEEE 802.11e EDCA by Contention Adaption for Energy Efficiency,"This paper presents a contention adaptation (CA) mechanism to improve the energy efficiency in IEEE 802.11e EDCA. By suspending some transmissions, the proposed EDCA/CA can reduce the number of collisions. Because unnecessary retransmissions are eliminated, the energy consumption is reduced. Extensive simulations have been performed to demonstrate the performance of the proposed EDCA/CA. The results show that EDCA/CA can reduce the energy consumption significantly. It reduces frame delay as well when traffic load is heavy. When traffic load is light, the proposed EDCA/CA will slightly increase the video delay, which in general is still acceptable. Furthermore, the proposed EDCA/CA is simple and easy to implement. It is fully compatible with the 802.11e EDCA. It is both effective and practical.","Energy efficiency,
Quality of service,
Telecommunication traffic,
Traffic control,
Energy consumption,
Delay,
Road accidents,
Multiaccess communication,
Computer science,
Wireless LAN"
On the sum capacity of MIMO interference channel in the low interference regime,"Treating interference as noise has recently been shown to be sum capacity achieving for the two-user single-input single-output (SISO) Gaussian interference channel in a low interference regime. In this paper, we characterize the low interference regime for the multiple-input single-output (MISO) Gaussian interference channel. We also provide partial results for the general multiple-input multiple-output (MIMO) Gaussian interference channel.",
Best practices in extreme programming course design,"Teaching (and therefore learning) extreme programming (XP) in a university setting is difficult because of course time limitations and the soft nature of XP that requires first-hand experience in order to see and really learn the methods. For example, iterations are either shorter or fewer than appropriate. In this paper we present the properties to tune when designing an eXtreme programming course. These are the properties we gathered by conducting three XP labs as part of our software engineering teaching. Within this paper we describe our set-up as well as the important properties. Lecturers and teachers can use this property system and combine it with their own constraints in order to derive a better XP lab for their curriculum.","Best practices,
Programming profession,
Software engineering,
Computer science education,
Educational programs,
Laboratories,
Software design,
Computational modeling,
Information science,
Management information systems"
Combining multiple types of biological data in constraint-based learning of gene regulatory networks,"Due to the complex structure and scale of gene regulatory networks, we support the argument that combination of multiple types of biological data to derive satisfactory network structures is necessary to understand the regulatory mechanisms of cellular systems. In this paper, we propose a simple but effective method of combining two types of biological data, namely microarray and transcription factor (TF) binding data, to construct gene regulatory networks. The proposed algorithm is based on and extends the well-known PC algorithm [23]. Further, we developed a method for measuring the significance of the interactions between the genes and the TFs. The reported test results on both synthetic and real data sets demonstrate the applicability and effectiveness of the proposed approach; we also report the results of some comparative analysis that highlights the power of the proposed approach.","Gene expression,
Proteins,
Computer science,
Biological system modeling,
Bayesian methods,
Data analysis,
Biology computing,
Predictive models,
Covariance matrix,
Cellular networks"
How to learn accurate grid maps with a humanoid,"Humanoids have recently become a popular research platform in the robotics community. Such robots offer various fields for new applications. However, they have several drawbacks compared to wheeled vehicles such as stability problems, limited payload capabilities, violation of the flat world assumption, and they typically provide only very rough odometry information, if at all. In this paper, we investigate the problem of learning accurate grid maps with humanoid robots. We present techniques to deal with some of the above-mentioned difficulties. We describe how an existing approach to the simultaneous localization and mapping (SLAM) problem can be adapted to robustly learn accurate maps with a humanoid equipped with a laser range finder. We present an experiment in which our mapping system builds a highly accurate map with a size of around 20m by 20m using data acquired with a humanoid in our office environment containing two loops. The resulting maps have a similar accuracy as maps built with a wheeled robot.","Mobile robots,
Humanoid robots,
Robotics and automation,
Simultaneous localization and mapping,
Robot sensing systems,
Vehicles,
Payloads,
Orbital robotics,
Parallel robots,
USA Councils"
Automated Effect-Specific Mammographic Pattern Measures,"We investigate the possibility to develop methodologies for assessing effect specific structural changes of the breast tissue using a general statistical machine learning framework. We present an approach of obtaining objective mammographic pattern measures quantifying a specific biological effect, such as hormone replacement therapy (HRT). We compare results using this approach to using standard density measures. We show that the proposed method can quantify both age related effects and effects caused by HRT. Age effects are significantly detected by our method where standard methodologies fail. The separation of HRT subpopulations using our approach is comparable to the best methodology, which is interactive.","Density measurement,
Biochemistry,
Breast cancer,
Computer science,
Medical treatment,
Breast tissue,
Machine learning,
Measurement standards,
Pattern classification,
Safety"
Gaussian Bounds for Noise Correlation of Functions and Tight Analysis of Long Codes,"We derive tight bounds on the expected value ofproducts of low influencefunctions defined on correlated probability spaces.The proofs are based on extending Fourier theory to an arbitrary number of correlatedprobability spaces, on a generalization of an invariance principlerecently obtained with O'Donnell andOleszkiewicz for multilinear polynomials withlow influences and bounded degree and on properties of multi-dimensionalGaussian distributions.Let
(
X
j
i
:1≤i≤k,1≤j≤n)
be a matrix of random variables whose columns
X
1
,…,
X
n
are independent and identically distributed and such that any two rows
X
i
,
X
j
for
1≤i≠j≤k
are independent.Assume further that the values that row
X
i
takes with non-zero probability are the same no matter how one conditions on the remaining rows
X
1
,…,
X
i−1
,
X
i+1
,…,
X
k
.Our results show that given
k
functions
f
1
,…,
f
k
taking values in
[0,1]
it holds that $|\E[\prod_{i=1}^k f_i(X_i)] - \prod_{i=1}^k \E[f_i(X_i)]","Gaussian noise,
Computer science,
Application software,
Random variables,
Voting,
Polynomials,
Gaussian distribution,
Testing,
Economic forecasting,
Extraterrestrial measurements"
Entity Name System: The Back-Bone of an Open and Scalable Web of Data,"Recognizing that information from different sources refers to the same (real world) entity is a crucial challenge in instance-level information integration, as it is a pre-requisite for combining the information about one entity from different sources. The required entity matching is time consuming and thus imposes a crucial limit for large-scale, dynamic information integration. An increased re-use of entity identifiers (or names) across different information collections such as RDF repositories, databases and document collections, eases this situation.In the ideal case, entity matching can be reduced to the trivial problem of spotting the same entity identifier in different information collections. In this paper we propose the use of an Entity Name System (ENS) -  as it is currently under development in the EU-funded project OKKAM - for systematically supporting the re-use of entity identifiers. The main purpose of the ENS is to provide unique and uniform names for entities for the use in information collections, so that the same name is used for an entity, even when it is referenced in different contexts.Of course the creation of an ENS that can efficiently deal with entities on the Web scale raises scalability issues of its own. This paper focuses on the role of an ENS in contributing to the scalability of ad-hoc and on demand information integration tasks.","Semantic web,
Resource description format,
Security,
Access control,
Service oriented architecture,
Vocabulary,
Maintenance engineering"
Retinal vessel tree segmentation using a deformable contour model,"This paper presents an improved version of our specific methodology to detect the vessel tree in retinal angiographies. The automatic analysis of retinal vessel tree facilitates the computation of the arteriovenous index, which is essential for the diagnosis several eye diseases. The developed system is inspired in the classical snake but incorporating domain specific knowledge, such as blood vessels topological properties. It profits from the automatic localization of the optic disc, the vessel creases extraction and, as a recent innovation, the morphological vessel segmentation, all developed in our research group. After researching and testing our system, the parameter configuration has been enhanced. Significantly better results in the detection of arteriovenous structures are obtained, keeping a high efficiency, as shown by the systems performance evaluation on the publicly available DRIVE database.","Retinal vessels,
Deformable models,
Retina,
Angiography,
Diseases,
Blood vessels,
Technological innovation,
System testing,
System performance,
Databases"
Using Dataflow Information for Concern Identification in Object-Oriented Software Systems,"Improper encapsulation of cross-cutting concerns significantly hinders software understandability and contributes to rising software maintenance costs. Concern identification covers the necessary first step towards separating and encapsulating concerns in existing object-oriented code. Because most of the current approaches rely on syntactic rather than semantic information, they do not provide sufficient support for software uderstanding. This paper proposes a new semi-automated approach for concern identification specifically designed to support software understanding, which starts from a set of related variables and uses static dataflow information to determine the concern skeleton, a data-oriented abstraction of a concern. We discuss the application of this approach to the JHotDraw case-study, the de facto standard benchmark for concern identification, and show that it can be used to identify a significant number of concerns, including several concerns not previously discussed in the existing literature.",
Overhead image statistics,"Statistical properties of high-resolution overhead images representing different land use categories are analyzed using various local and global statistical image properties based on the shape of the power spectrum, image gradient distributions, edge co-occurrence, and inter-scale wavelet coefficient distributions. The analysis was performed on a database of high-resolution (1 meter) overhead images representing a multitude of different downtown, suburban, commercial, agricultural and wooded exemplars. Various statistical properties relating to these image categories and their relationship are discussed. The categorical variations in power spectrum contour shapes, the unique gradient distribution characteristics of wooded categories, the similarity in edge co-occurrence statistics for overhead and natural images, and the unique edge co-occurrence statistics of downtown categories are presented in this work. Though previous work on natural image statistics has showed some of the unique characteristics for different categories, the relationships for overhead images are not well understood. The statistical properties of natural images were used in previous studies to develop prior image models, to predict and index objects in a scene and to improve computer vision models. The results from our research findings can be used to augment and adapt computer vision algorithms that rely on prior image statistics to process overhead images, calibrate the performance of overhead image analysis algorithms, and derive features for better discrimination of overhead image categories.",
Network-centric localization in MANETs based on particle swarm optimization,"There exist several application scenarios of mobile ad hoc networks (MANET) in which the nodes need to locate a target or surround it. Severe resource constraints in MANETs call for energy efficient target localization and collaborative navigation. Centralized control of MANET nodes is not an attractive solution due to its high network utilization that can result in congestions and delays. In nature, many colonies of biological species (such as a flock of birds) can achieve effective collaborative navigation without any centralized control. Particle swarm optimization (PSO), a popular swarm intelligence approach that models social dynamics of a biological swarm is proposed in this paper for network-centric target localization in MANETs that are enhanced by mobile robots. Simulation study of two application scenarios is conducted. While one scenario focuses on quick target localization, the other aims at convergence of MANET nodes around the target. Reduction of swarm size during PSO search is proposed for accelerated convergence. The results of the study show that the proposed algorithm is effective in network-centric collaborative navigation. Emergence of converging behavior of MANET nodes is observed.",
Fast play: a novel feature for digital consumer video devices,"The advent of digital devices with innovative features has changed the world of home video entertainment. Nowadays we have access to devices that store several hours of video material, skip commercials, and even pause the play out of a live event. Despite these innovative features, modern digital devices access to recorded material in a way very similar to old analog VCRs. In this paper we propose Fast Play, a mechanism that uses simple low-level audio and video analysis to provide a novel feature in digital video devices. This novel feature is a hybrid between the classic Play service and the Fast-Forward service, and aims at playing out videos in a time-reduced manner, by maintaining the time evolving nature of the original video, and by providing videos with completely intelligible audio. An experimental assessment shows that Fast Play produces videos that are informative, enjoyable, largely reduced, and also shows that users really appreciate the Fast Play feature as it produces a pleasant surrogate of the original video.","TV,
Video recording,
Presses,
Proposals,
Automatic programming,
Layout,
Computer science,
Cellular phones,
Digital audio players,
Voice mail"
Privacy and security in biomedical applications of wireless sensor networks,"Wireless sensor network applications in healthcare and biomedical technology have received increasing attention, while associated security and privacy issues remain open areas of consideration. The relevance of this technology to our growing elderly population, as well as our increasingly over-crowded and attention-drained healthcare systems, is promising. However, prior to the emergence of these systems as a ubiquitous technology, healthcare providers and regulatory agencies must determine an acceptable level of security and privacy. This paper will review biomedical applications of wireless sensor networks, identify security and privacy issues to be addressed, and note some of the proposed methods for securing these systems.",
Printed electronics for low-cost electronic systems: Technology status and application development,"In recent years, printing has received substantial interest as a technique for realizing low cost, large area electronic systems. Printing allows the use of purely additive processing, thus lowering process complexity and material usage. Coupled with the use of low-cost substrates such as plastic, metal foils, etc., it is expected that printed electronics will enable the realization of a wide range of easily deployable electronic systems, including displays, sensors, and RFID tags. We review our work on the development of technologies and applications for printed electronics. By combining synthetically derived inorganic nanoparticles and organic materials, we have realized a range of printable electronic “inks”, and used these to demonstrate printed passive components, multilayer interconnection, diodes, transistors, memories, batteries, and various types of gas and biosensors. By exploiting the ability of printing to cheaply allow for the integration of diverse functionalities and materials onto the same substrate, therefore, it is possible to realize printed systems that exploit the advantages of printing while working around the disadvantages of the same.","Printing,
Costs,
Additives,
Plastics,
Flat panel displays,
Sensor systems,
RFID tags,
Nanoparticles,
Organic materials,
Nonhomogeneous media"
Moving Scientific Codes to Multicore Microprocessor CPUs,"The IBM Cell processor represents the first and most extreme of a new generation of multicore CPUs. For scientific codes that can be formulated in terms of vector computing concepts, as far as we know, the Cell is the most rewarding. In this article, we present a method for implementing numerical algorithms for scientific computing so that they run efficiently on the Cell processor and other multicore CPUs. We present our method using the piecewise-parabolic method (PPM) gas dynamics algorithm but believe that many other algorithms could benefit from our approach. Nevertheless, the code transformations are difficult to perform manually, so we are undertaking an effort to build simplified tools to assist in at least the most tedious of the code transformations involved.",
A Comparison of the Perceptual Benefits of Linear Perspective and Physically-Based Illumination for Display of Dense 3D Streamtubes,"Large datasets typically contain coarse features comprised of finer sub-features. Even if the shapes of the small structures are evident in a 3D display, the aggregate shapes they suggest may not be easily inferred. From previous studies in shape perception, the evidence has not been clear whether physically-based illumination confers any advantage over local illumination for understanding scenes that arise in visualization of large data sets that contain features at two distinct scales. In this paper we show that physically-based illumination can improve the perception for some static scenes of complex 3D geometry from flow fields. We perform human-subjects experiments to quantify the effect of physically-based illumination on participant performance for two tasks: selecting the closer of two streamtubes from a field of tubes, and identifying the shape of the domain of a flow field over different densities of tubes. We find that physically-based illumination influences participant performance as strongly as perspective projection, suggesting that physically-based illumination is indeed a strong cue to the layout of complex scenes. We also find that increasing the density of tubes for the shape identification task improved participant performance under physically-based illumination but not under the traditional hardware-accelerated illumination model.","Lighting,
Three dimensional displays,
Shape,
Layout,
Data visualization,
Humans,
Visual system,
Psychology,
Aggregates,
Geometry"
Optimizing CASCADE data aggregation for VANETs,"We present an analysis of the CASCADE (Cluster-based Accurate Syntactic Compression of Aggregated Data in VANETs) data aggregation technique. CASCADE organizes known vehicles into clusters, the size of which determines both the frame size used to distribute aggregated data and the distance ahead that vehicles are aware of (local view). In this paper, we determine the optimal cluster size to balance the trade-off between local view length and expected frame size.","Vehicles,
Equations,
Chromium,
Distance measurement,
Mathematical model,
Ad hoc networks,
Logic gates"
A Unified Hard/Soft Real-Time Schedulability Test for Global EDF Multiprocessor Scheduling,"The issue of deadline tardiness is considered under globalearliest-deadline-first (GEDF) multiprocessor scheduling. Newschedulability tests are presented for determining whether a setof sporadic tasks with arbitrary relative deadlines can be scheduledunder either preemptive or non-preemptive GEDF so thatpre-defined tardiness bounds are met. These tests are of pseudopolynomialtime complexity, and can be used in hard real-time, softreal-time, and mixed contexts.","Processor scheduling,
Job shop scheduling,
Real time systems,
Scheduling algorithm,
System testing,
Multicore processing,
Timing,
Computer science,
Manufacturing,
Chip scale packaging"
Network beamforming based on second order statistics of the channel state information,"The problem of distributed beamforming is considered for a network which consists of a transmitter, a receiver, and r relay nodes. Assuming that the second order statistics of the channel coefficients are available, we design a distributed beamforming technique via maximization of the receiver signal-to-noise ratio (SNR) subject to individual relay power constraints. We show that using semi-definite relaxation, this SNR maximization can be turned into a convex feasibility semi-definite programming problem, and therefore, it can be efficiently solved using interior point methods. We also obtain a performance bound for the semi-definite relaxation and show that the semi-definite relaxation approach provides a c-approximation to the (nonconvex) SNR maximization problem, where c = O((log r)−1) and r is the number of relays.","Array signal processing,
Statistics,
Channel state information,
Relays,
Transmitters,
Statistical distributions,
Signal to noise ratio,
Wireless sensor networks,
Quality of service,
Uncertainty"
Intrusion Techniques: Comparative Study of Network Intrusion Detection Systems,"Organizations require security systems that are flexible and adaptable in order to combat increasing threats from software vulnerabilities, virus attacks and other malicious code, in addition to internal attacks. Network intrusion detection systems, which are part of the layered defense scheme, must be able to meet these organizational objectives in order to be effective. Although signature based network intrusion detection systems meet several organizational security objectives, heuristic based network intrusion detection systems are able to fully meet the objectives of the organization. Through a comparative theoretical study, this paper analyzes several organizational security objectives in order to determine the network intrusion detection system that effectively meets these objectives. Through conclusive analysis of the study, heuristic based systems are better served to meet the organizational objectives than signature based systems. The analysis was based on which system provided definitive security objectives and offered the flexibility, adaptability, and reduced vulnerability that an organization requires.","Intrusion detection,
Cost benefit analysis,
Communication system security,
Telecommunication traffic,
Information technology,
Computer science,
Computer security,
Information security,
Knowledge management,
Control systems"
Configuration Lifting: Verification meets Software Configuration,"Configurable software is ubiquitous, and the term software product line (SPL) has been coined for it lately. It remains a challenge, however, how such software can be verified over all variants. Enumerating all variants and analyzing them individually is inefficient, as knowledge cannot be shared between analysis runs. Instead of enumeration we present a new technique called lifting that converts all variants into a meta-program, and thus facilitates the configuration-aware application of verification techniques like static analysis, model checking and deduction-based approaches. As a side-effect, lifting provides a technique for checking software feature models, which describe software variants, for consistency. We demonstrate the feasibility of our approach by checking configuration dependent hazards for the highly configurable Linux kernel which possesses several thousand of configurable features. Using our techniques, two novel bugs in the kernel configuration system were found.","Software,
Kernel,
Linux,
Runtime,
Driver circuits,
Analytical models,
Software systems"
Using Tractable and Realistic Churn Models to Analyze Quiescence Behavior of Distributed Protocols,"Large-scale distributed systems are subject to churn, i.e., continuous arrival, departure and failure of processes. Analysis of protocols under churn requires one to use churn models that are tractable (easy to apply), realistic (apply to deployment settings), and general (apply to many protocols and properties). In this paper, we propose two new churn models - called train and crowd - that together achieve these goals, for a broad class of stability properties called quiescent properties, and for arbitrary distributed protocols. We show (i) how analysis of protocol quiescence in the train model can be extended to the crowd model, (ii) how to apply the train and crowd model to several distributed membership protocols, (iii) how, even under real churn traces, the train and crowd models are reasonably good at predicting system-wide stability metrics for membership protocols.","Protocols,
Predictive models,
Peer to peer computing,
Failure analysis,
Large-scale systems,
Stability analysis,
Pattern analysis,
Algorithm design and analysis,
Computer crashes,
Computer science"
A Kalman Filter Based Approach for Outlier Detection in Sensor Networks,"Outliers are common in data collection applications with wireless sensor networks, which consist of a large number of sensor nodes, embedded in physical space. The limited power supplies and noisy sensor data put challenges for outlier detection and cleaning in sensor networks. In this paper, we propose utilizing spatial and temporal dependencies that exist sensory readings. Our approach is based on Kalman filter and we design the state transition module and measuring module of the Kalman filter to exploit the temporal and spatial dependencies of sensor data respectively. The experimental results illustrate the effectiveness of our approach.","Wireless sensor networks,
Intelligent sensors,
Machine intelligence,
Laboratories,
Predictive models,
State estimation,
Base stations,
Sensor phenomena and characterization,
Bayesian methods,
Computer science"
Incremental learning of nonparametric Bayesian mixture models,"Clustering is a fundamental task in many vision applications. To date, most clustering algorithms work in a batch setting and training examples must be gathered in a large group before learning can begin. Here we explore incremental clustering, in which data can arrive continuously. We present a novel incremental model-based clustering algorithm based on nonparametric Bayesian methods, which we call Memory Bounded Variational Dirichlet Process (MB-VDP). The number of clusters are determined flexibly by the data and the approach can be used to automatically discover object categories. The computational requirements required to produce model updates are bounded and do not grow with the amount of data processed. The technique is well suited to very large datasets, and we show that our approach outperforms existing online alternatives for learning nonparametric Bayesian mixture models.",
Fine-grained parallelization of the Car-Parrinello ab initio molecular dynamics method on the IBM Blue Gene/L supercomputer,"Important scientific problems can be treated via ab initio-based molecular modeling approaches, wherein atomic forces are derived from an energy function that explicitly considers the electrons. The Car–Parrinello ab initio molecular dynamics (CPAIMD) method is widely used to study small systems containing on the order of 10 to 103 atoms. However, the impact of CPAIMD has been limited until recently because of difficulties inherent to scaling the technique beyond processor numbers about equal to the number of electronic states. CPAIMD computations involve a large number of interdependent phases with high interprocessor communication overhead. These phases require the evaluation of various transforms and non-square matrix multiplications that require large interprocessor data movement when efficiently parallelized. Using the Charm++ parallel programming language and runtime system, the phases are discretized into a large number of virtual processors, which are, in turn, mapped flexibly onto physical processors, thereby allowing interleaving of work. Algorithmic and IBM Blue Gene/L™ system-specific optimizations are employed to scale the CPAIMD method to at least 30 times the number of electronic states in small systems consisting of 24 to 768 atoms (32 to 1,024 electronic states) in order to demonstrate fine-grained parallelism. The largest systems studied scaled well across the entire machine (20,480 nodes).",
Metrics for Architecture-Level Lifetime Reliability Analysis,"This work concerns metrics for evaluating microarchitectural enhancements to improve processor lifetime reliability. A commonly reported reliability metric is mean time to failure (MTTF). Although the MTTF metric is simpler to evaluate, it does not provide information on the reliability characteristics during the relatively short operational life of commodity processors. An alternate metric is nTTF, which represents the time to failure of n% of the processor population. nTTF is a more informative metric for the (short) portion of the lifetime that is relevant to the end-user, but determining it requires knowledge of the distribution of processor failure times which is generally hard to obtain. The goals of this paper are (1) to determine if the choice of metric has a quantitative impact on architecture-level reliability analysis and modern superscalar processor designs and (2) to build a fundamental understanding of why and when MTTF- and nTTF-driven analysis result in different designs. We show through an in-depth analysis that, in general, the nTTF metric differs significantly from the MTTF metric, and using MTTF as a proxy for nTTF leads to sub-optimal designs. Additionally, our analysis introduces the concept of relative vulnerability factor (RVF) for different processor components to guide reliability-aware design. We show that the difference between nTTF-and MTTF-driven design largely occurs because the relative vulnerabilities of the processor components change over the processor lifetime, making the optimal design choice dependent on the amount of time the processor is expected to be used.","Microarchitecture,
Process design,
Failure analysis,
Rivers,
Computer science,
Accelerated aging,
Electromigration,
Electric breakdown,
Negative bias temperature instability,
Niobium compounds"
Clustering initiated multiphase active contours and robust separation of nuclei groups for tissue segmentation,"Computer assisted or automated histological grading of tissue biopsies for clinical cancer care is a long-studied but challenging problem. It requires sophisticated algorithms for image segmentation, tissue architecture characterization, global texture feature extraction, and high-dimensional clustering and classification algorithms. Currently there are no automatic image-based grading systems for quantitative pathology of cancer tissues. We describe a novel approach for tissue segmentation using fuzzy spatial clustering, vector-based multiphase level set active contours and nuclei detection using an iterative kernel voting scheme that is robust even in the case of clumped touching nuclei. Early results show that we can reach a 91% detection rate compared to manual ground truth of cell nuclei centers across a range of prostate cancer grades.",
On the Dynamic Ant Colony Algorithm Optimization Based on Multi-pheromones,"In this paper, an algorithm DACO (Dynamic Ant Colony Algorithm Optimization Based onMulti-pheromone) is put forward to apply to the dynamics of web services state and QoS in service composition optimization. Setting multiple pheromones to denote different QoS, this algorithm can apply to such conditions as service invalidation, service add, service QoS change, etc. The DACO is also improved based on experiment in order to make it better and faster converge to optimization value. Emulation experiment made in this paper shows that the DACO is more effective than Ant Colony Algorithm.","Heuristic algorithms,
Ant colony optimization,
Web services,
Genetic algorithms,
Ontologies,
Semantic Web,
Optimization methods,
Computer networks,
Telecommunication computing,
Information science"
Dynamic Consistency Checking of Domain Requirements in Product Line Engineering,"The domain requirements specification (DRS) of a product line comprises the common and variable requirements of all products of the product line. Due to the variability defined for a product line, the DRS may contain contradicting requirements. For example, it may contain requirements A and not(A) which can be included in different products. Checking the consistency of DRS in product line engineering is thus not straightforward. Variability information has to be incorporated into the consistency checks to ensure that contradicting requirements do not become part of the same product requirements specification. In this paper, we present a consistency checking technique for dynamic properties of DRS based on model checking techniques. We present a proof of correctness for the technique, sketch our tool environment, and report on the application of the approach to an industrial example.",
Analysis of population-based evolutionary algorithms for the vertex cover problem,"Recently it has been proved that the (1+1)-EA produces poor worst-case approximations for the vertex cover problem. In this paper the result is extended to the (1+λ)-EA by proving that, given a polynomial time, the algorithm can only find poor covers for an instance class of bipartite graphs. Although the generalisation of the result to the (μ+1)-EA is more difficult, hints are given in this paper to show that this algorithm may get stuck on the local optimum of bipartite graphs as well because of premature convergence. However a simple diversity maintenance mechanism can be introduced into the EA for optimising the bipartite instance class effectively. It is proved that the diversity mechanism combined with one point crossover can change the runtime for some instance classes from exponential to polynomial in the number of nodes of the graph.","Approximation methods,
Algorithm design and analysis,
Approximation algorithms,
Runtime,
Optimization,
Polynomials,
Evolutionary computation"
Optimization technique for electrical insulation design of vacuum interrupters,"Because of the excellent high insulation performance, vacuum is expected to be one of the alternatives of SF6 gas. In order to reduce the use of SF6 gas in substation equipment, the applicability of vacuum circuit breakers (VCB) and vacuum interrupters (VI) should be extended to higher (typically 145-300 kV) voltage. For the high voltage VI, the role of electrical insulation design becomes more important. For accurate insulation design of the VI, we developed an optimization technique to improve the electrical insulation performance. We evaluated several design variables necessary for optimizing the electrode contour of the main contactor and center shield in VI. The electrode area effect on vacuum breakdown characteristics as well as the electric field distribution was considered in the optimization process. In order to verify the accuracy and the efficiency of the proposed technique, we applied it to a practical model of VI. In addition, we examined the influence of the parameters of the area effect, arrangement of VI in VCB on the optimization result. From the calculation results, we confirmed that the optimization based on the area effect had higher availability and higher accuracy than the electric field optimization from the viewpoint of the improvement of the electrical insulation performance of vacuum insulated equipment.","Design optimization,
Dielectrics and electrical insulation,
Vacuum technology,
Interrupters,
Voltage,
Electrodes,
Vacuum breakdown,
Gas insulation,
Substations,
Circuit breakers"
On the Precision and Accuracy of Impact Analysis Techniques,"Several techniques and algorithms for impact analysis of software systems have been recently published in literature. Most of them, however, are not practical enough to be applied in the software industry because, among other reasons, they produce too many false results (either positive or negative). In this paper, we propose and evaluate the use of two measures from information retrieval, namely  precision and recall, to help express and compare precision and accuracy of impact analysis techniques and algorithms.","Information analysis,
Algorithm design and analysis,
Software systems,
Information retrieval,
Runtime,
Information science,
Software algorithms,
Computer industry,
Concrete"
Robust FPGA resynthesis based on fault-tolerant Boolean matching,"We present FPGA logic synthesis algorithms for stochastic fault rate reduction in the presence of both permanent and transient defects. We develop an algorithm for fault tolerant Boolean matching (FTBM), which exploits the flexibility of the LUT configuration to maximize the stochastic yield rate for a logic function. Using FTBM, we propose a robust resynthesis algorithm (ROSE) which maximizes stochastic yield rate for an entire circuit. Finally, we show that existing PLB (programmable logic block) templates for area-aware Boolean matching and logic resynthesis are not effective for fault tolerance, and propose a new robust template with path re-convergence. Compared to the state-of-the-art academic technology mapper Berkeley ABC, ROSE using the proposed robust PLB template reduces the fault rate by 25% with 1% fewer LUTs, and increases MTBF (mean time between failures) by 31%, while preserving the optimal logic depth.","Robustness,
Field programmable gate arrays,
Fault tolerance,
Stochastic processes,
Circuit faults,
Table lookup,
Boolean functions,
Programmable logic arrays,
Programmable logic devices,
Logic functions"
Adaptive Reading Assistance for the Inclusion of Students with Dyslexia: The AGENT-DYSL Approach,"Dyslexia is a major barrier to success in education and later on the job as reading skills are fundamental for personal competence development. Children with dyslexia have special learning needs (e.g., more teacher support), which currently only specialized institutions can provide. However, this takes children out of their peer group and causes social problems. On the other side, there is general-purpose reading support software, which are not geared towards children with dyslexia as they lack personalization. AGENT-DYSL brings together speech and image recognition as well as semantic technologies to build a truly adaptive reading support system for children with dyslexia.",
A survey of social software engineering,"Software engineering is a complex socio-technical activity, due to the need for discussing and sharing knowledge among team members. This has raised the need for effective ways of sharing ideas, knowledge, and artifacts among groups and their members. The social aspect of software engineering process also demands computer support to facilitate the development by means of collaborative tools, applications and environments. In this paper, we present a survey of relevant works from psychology, mathematics and computer science studies. The combination of these fields provides the required infrastructure for engineering social and collaborative applications as well as the software engineering process. We also discuss possible solutions for the encountered shortcomings, and how they can improve software development.","Software engineering,
Psychology,
Collaborative software,
Collaborative tools,
Collaborative work,
Programming profession,
Productivity,
Humans,
Project management,
Application software"
A Braess type paradox in power control over interference channels,The original Braess paradox has been predicted in a context of Wardrop equilibrium in a road traffic context where there is a continuum of (non-atomic) players. It was shown that the performance of all users at equilibrium becomes worse when adding a route. This paradox as well as various variants were also studied in the context of computer networks and telecommunications. We identify a new type of paradox occurring in wireless communications with some unusual properties with respect to previous models in which the paradox has been identified.,"Power control,
Interference channels,
Base stations,
Telecommunication traffic,
Context,
Traffic control,
Context-aware services,
Industrial engineering,
Energy management,
Engineering management"
Modeling and Adapting JPEG to the Energy Requirements of VSN,"We address the problem of modeling and adapting JPEG to the energy requirements of visual sensor networks (VSN). For JPEG modeling purposes, we develop a simplified high-level energy consumption model for each stage of JPEG-like scheme, which can be used to roughly evaluate the energy dissipated by a given visual sensor. This model is based on the basic operations needed at each stage of JPEG, and it does not take into account the complexity of implementation. For JPEG adaptation, we propose to process only a reduced part of each block of 8times8 DCT coefficients of the target image, which minimizes the dissipated energy and maximizes the system lifetime, while preserving an adequate image quality at the sink.","Discrete cosine transforms,
Image sensors,
Wireless sensor networks,
Image coding,
Transform coding,
Image quality,
Routing,
Computer science,
Energy consumption,
Image retrieval"
A Unified Framework for Numerical and Combinatorial Computing,"A rich variety of tools help researchers with high-performance numerical computing, but few tools exist for large-scale combinatorial computing. The authors describe their efforts to build a common infrastructure for numerical and combinatorial computing by using parallel sparse matrices to implement parallel graph algorithms.","Sparse matrices,
Data structures,
Algorithm design and analysis,
Partitioning algorithms,
Concurrent computing,
Scientific computing,
Large-scale systems,
High level languages,
Libraries,
Data visualization"
Analysis of Quality of Service (QoS) in WiMAX networks,"In last few years there has been significant growth in the area of wireless communication. Quality of Service (QoS) has become an important consideration for supporting variety of applications that utilize the network resources. These applications include voice over IP, multimedia services, like, video streaming, video conferencing etc. IEEE 802.16/WiMAX is a new network which is designed with quality of service in mind. This paper focuses on analysis of quality of service as implemented by the WiMAX networks. First, it presents the details of the quality of service architecture in WiMAX network. In the analysis, a WiMAX module developed based on popular network simulator ns-2, is used. Various real life scenarios like voice call, video streaming are setup in the simulation environment. Parameters that indicate quality of service, such as, throughput, packet loss, average jitter and average delay, are analyzed for different types of service flows as defined in WiMAX. Results indicate that better quality of service is achieved by using service flows designed for specific applications.","Quality of service,
WiMAX,
Streaming media,
Wireless communication,
Internet telephony,
Videoconference,
Analytical models,
Throughput,
Jitter,
Delay"
Transformation of BPEL Processes to Petri Nets,"Web service composition involves the combination of a number of existing web services to create a value-added service in ways that may not be foreseen at the time when a web service is written. BPEL is a promising language which describes web service composition in form of business processes. However, BPEL is an XML-based language and may suffer from ambiguities or some erroneous properties. It is necessary to analyze business processes specified in BPEL with a formal tool. In this paper, we put forward an approach to model and verify BPEL based on ServiceNet, a special class of Petri nets. We present some transformation rules of BPEL business processes into ServiceNet. Then the throughness of a BPEL business process can be verified by reducing the corresponding ServiceNet based on some reduction rules.","Petri nets,
Web services,
Logic,
Application software,
XML,
Proposals,
Software engineering,
Computer science,
Business communication,
Web and internet services"
Ultralarge Systems: Redefining Software Engineering?,"The harbingers of ultralarge systems are indeed emerging, although their elements seem contradictory to the ""ultralarge"" concept. ULS design will have to move beyond computer science and electrical and electronics engineering-based methodologies to include building blocks from seven major research areas: human interaction; computational emergence; design; computational engineering; adaptive system infrastructure; adaptable and predictable system quality; and policy, acquisition, and management. We need to integrate these more novel approaches with the tools and techniques of traditional software engineering, especially with regard to formal methods and to dealing with predictability and uncertainty in high-integrity software systems. Our view is not so much that we are 'redefining' software engineering but rather that we're looking to extend established software engineering tools and techniques in novel and useful ways.","Software engineering,
Design engineering,
Computer science,
Humans,
Systems engineering and theory,
Adaptive systems,
Disaster management,
Quality management,
Engineering management,
Uncertainty"
Wikipedia in Action: Ontological Knowledge in Text Categorization,"We present a new, ontology-based approach to the automatic text categorization. An important and novel aspect of this approach is that our categorization method does not require a training set, which is in contrast to the traditional statistical and probabilistic methods. In the presented method, the ontology, including the domain concepts organized into hierarchies of categories and interconnected by relationships, as well as instances and connections among them, effectively becomes the classifier. Our method focuses on (i) converting a text document into a thematic graph of entities occurring in the document, (ii) ontological classification of the entities in the graph, and (iii) determining the overall categorization of the thematic graph, and as a result, the document itself. In the presented experiments, we used an RDF ontology constructed from the full English version of Wikipedia. Our experiments, conducted on corpora of Reuters news articles, showed that our training-less categorization method achieved a very good overall accuracy.","Ontologies,
Encyclopedias,
Internet,
Text categorization,
Information services,
Electronic publishing,
Training"
"Processor, Assembler, and Compiler Design Education Using an FPGA","This paper reports the design of two courses, ""Embedded Hardware'' and ""Embedded Software"" offered in 2008 Spring semester at Hiroshima University. These courses use 16-bit processor TINYCPU, cross assembler TINYASM, and cross compiler TINYC. They are designed very simple and compact: The total number of lines of the source code is only 427. Thus, students can understandthe entire design easily, and can learn the basics of computer and embedded system, including processor architecture, assembler and compiler design, assembler programming in a unified way by experiment.","Field programmable gate arrays,
Assembly systems,
Hardware,
Embedded software,
Springs,
Embedded computing,
Embedded system,
Computer architecture,
Program processors,
Programming profession"
Compact algorithm for strictly ML ellipse fitting,"A very compact algorithm is presented for fitting an ellipse to points in images by maximum likelihood (ML) in the strict sense. Although our algorithm produces the same solution as existing ML-based methods, it is probably the simplest and the smallest of all. By numerical experiments, we show that the strict ML solution practically coincides with the Sampson solution.","Gaussian noise,
Equations,
Computer science,
Layout,
Shape,
Fitting,
Voting,
Least squares methods,
Maximum likelihood estimation,
Noise reduction"
EXCES: External caching in energy saving storage systems,"Power consumption within the disk-based storage subsystem forms a substantial portion of the overall energy footprint in commodity systems. Researchers have proposed external caching on a persistent, low-power storage device, which we term external caching device (ECD), to minimize disk activity and conserve energy. While recent simulation-based studies have argued in favor of this approach, the lack of an actual system implementation has precluded answering several key questions about external caching systems. We present the design and implementation of EXCES, an external caching system that employs prefetching, caching, and buffering of disk data for reducing disk activity. EXCES addresses important questions related to external caching, including the estimation of future data popularity, I/O indirection, continuous reconfiguration of the ECD contents, and data consistency. We evaluated EXCES with both micro- and macro- benchmarks that address idle, I/O intensive, and real-world workloads. Overall system energy savings was found to lie in the modest 2–14% range, depending on the workload, in somewhat of a contrast to the higher values predicted by earlier studies. Furthermore, while the CPU and memory overheads of EXCES were well within acceptable limits, we found that flash-based external caching can substantially degrade I/O performance. We believe that external caching systems hold promise. Further improvements in ECD technology, both in terms of their power consumption and performance characteristics can help realize the full potential of such systems.","Power demand,
Disk drives,
Kernel,
Prefetching,
Benchmark testing,
File systems,
Linux"
A new method for multiple fuzzy rules interpolation with weighted antecedent variables,"Fuzzy rule interpolation techniques have been used to handle the problems of sparse fuzzy rule bases in sparse fuzzy rule-based systems. In the existing fuzzy rule interpolation methods, there are many variables in the antecedents of fuzzy rules, where the variables in the antecedents of fuzzy rules have the same weight. If we can handle fuzzy rule interpolation with weighted antecedent variables, then there is room for more flexibility. In this paper, we present a new method for multiple fuzzy rules interpolation with weighted antecedent variables. The proposed method not only can handle fuzzy rule interpolation with polygonal membership functions, but also can preserve the convexity of fuzzy interpolative reasoning results. The fuzzy interpolative reasoning results of the proposed method also satisfy the logically consistency with respect to the ratios of fuzziness. The experimental result shows that the proposed method can generate reasonable fuzzy interpolative reasoning results for sparse fuzzy rule-based systems with weighted antecedent variables. The proposed method provides us a useful way for fuzzy rule interpolation in sparse fuzzy rule-based systems with weighted antecedent variables.","Interpolation,
Fuzzy systems,
Fuzzy reasoning,
Knowledge based systems,
Fuzzy sets,
Computer science,
Fuzzy logic,
Extrapolation,
Bismuth"
Moving-Baseline Localization,"The moving-baseline localization (MBL) problem arises when a group of nodes moves through an environment in which no external coordinate reference is available. When group members cannot see or hear one another directly, each node must employ local sensing and inter-device communication to infer the spatial relationship and motion of all other nodes with respect to itself. We consider a setting in which nodes move with piecewise-linear velocities in the plane, and any node can exchange noisy range estimates with certain sufficiently nearby nodes. We develop a distributed solution to the MBL problem in the plane, in which each node performs robust hyperbola fitting, trilateration with velocity constraints, and subgraph alignment to arrive at a globally consistent view of the network expressed in its own ""rest frame."" Changes in any node's motion cause deviations between observed and predicted ranges at nearby nodes, triggering revision of the trajectory estimates computed by all nodes.We implement and analyze our algorithm in a simulation informed by the characteristics of a commercially available UWB (ultra-wideband) radio, and show that recovering node trajectories (rather than just locations) requires substantially less computation at each node. Finally, we quantify the minimum ranging rate and local network density required for the method's successful operation.","Motion estimation,
Global Positioning System,
Piecewise linear techniques,
Algorithm design and analysis,
Analytical models,
Ultra wideband technology,
Distributed algorithms,
Information processing,
Intelligent sensors,
Computer science"
Electrical endogenous heating of insect muscles for flight control,"In our ongoing effort to reliably control insect flight with pupae-inserted electronics, we demonstrate using heat to control insect flight initiation or take-off from standing position. We have previously developed reliable surgical methods to implant neuromuscular prosthetic devices in insect flight muscle for directing flight electrically. Here, we present, for the first time, a tissue embedded micro heater to decrease the preflight warm-up duration of hybrid electronic insect micro-air-vehicles. The biological takeoff-time at room temperature of 5–10 minutes was reduced to 58–seconds by thorax heating. Minimization of takeoff-time will enable higher bandwidth and reliable control for repeated takeoff and landing applications.",
Automatic Registration of Vertex Correspondences for 3D Facial Expression Analysis,"3D facial range models can be created by static range scanners or real-time dynamic 3D imaging systems. One of the major obstacles for analyzing such data is lack of correspondences of features (or vertices) due to the variable number of vertices across individual models or 3D model sequences. In this paper, we present an effective approach to automatically establish vertex correspondences for feature registration, and further classify facial models to specific expressions. We describe our proposed approach as how to establish correspondences among individual models based on a 2D intermediary, which is generated using a conformal mapping and model adaptation algorithm. We also present our approach for 3D facial expression labeling, registration, tracking, and categorization. The feasibility of the approach is validated and demonstrated using our created 3D facial expression databases.","Solid modeling,
Conformal mapping,
Labeling,
Computer science,
Face,
Real time systems,
Surveillance,
Data analysis,
Image sequence analysis,
Computer security"
"Chemical, physical and electrical properties of aged dodecylbenzene 2: thermal ageing of single isomers in air","The linear isomer of dodecylbenzene (DDB), 1-phenyldodecane, was aged at temperatures of 105 and 135 degC in air and the resultant products were analyzed using a range of analytical techniques. On ageing, the 1-phenyldodecane darkened, the acid number, dielectric loss and water content increased and significant oxidation peaks were detected in the infrared spectrum. When aged in the presence of copper, a characteristic peak at 680 nm was also detected by UV/visible spectroscopy but, compared with previous studies of a cable-grade DDB, the strength of this peak was much increased and no appreciable precipitate formation occurred. At the same time, very high values of dielectric loss were recorded. On ageing in the absence of copper, an unusually strong infrared carbonyl band was seen, which correlates well with the detection of dodecanophenone by gas chromatography/mass spectrometry and nuclear magnetic resonance spectroscopy. It was therefore concluded that the ageing process proceeds via the initial production of aromatic ketones, which may then be further oxidized to carboxylic acids. In the presence of copper, these oxidation products are present in lower quantities, most of these oxidation products being combined with the copper present in the oil to give copper carboxylates. The behavior is described in terms of a complex autoxidation mechanism, in which copper acts as both an oxidizing and a reducing agent, depending on its oxidation state and, in particular, promotes elimination via the oxidation of intermediate alkyl radical species to carbocations.","Aging,
Chemicals,
Copper,
Oxidation,
Dielectric losses,
Infrared detectors,
Infrared spectra,
Mass spectroscopy,
Temperature distribution,
Gas chromatography"
A Rough Set Based Hybrid Method to Feature Selection,"Features selection is a process to find the optimal subset of features that satisfy certain criteria. The aim of feature selection is to remove unnecessary features to the target concept. This paper investigates some basic concepts of rough set theory and ant colony optimization. Based on these studies, a hybrid approach to feature selection on combination of ant colony optimization and rough set theory is proposed. Experimental results obtained show this hybrid approach is a promising method for feature selection.","Set theory,
Ant colony optimization,
Information systems,
Machine learning,
Knowledge acquisition,
Helium,
Educational institutions,
Computer science,
Optimization methods,
Computational modeling"
Television-Mediated Conversation: Coherence in Italian iTV SMS Chat,"Text messaging on interactive television in Italy is a media convergence phenomenon involving short message service, traditional television, and the World Wide Web. This study investigates the frequency and coherence of viewer-to-viewer textual exchanges on an Italian iTV SMS program, employing methods of interaction analysis and the visualization tool VisualDTA to represent interactional coherence. The findings show that despite numerous factors that discourage it, some users more-or-less successfully adapt the medium to engage in interpersonal exchanges. Design recommendations are advanced for fostering more coherent viewer-to-viewer interaction via iTV SMS.",
Specifying Services for ITIL Service Management,"The Information Technology Infrastructure Library (ITIL) is a collection of best practices for the management of IT services. ITIL helps organizations to become aware of the business value their IT services provide to internal and external stakeholders. Understanding this value is crucial to the definition of Service Level Agreements (SLA) between an IT department and its stakeholders. However, it is not ITIL's objective to define how this value is to be elicited from stakeholders. This creates an opportunity for the use of RE methods in businesses. This paper describes the main principles of ITIL Service Management and illustrates how the SEAM RE method can contribute to the definition of an SLA by modeling the service provided by an IT department, the stakeholders of this service and the value the stakeholders expect from this service. A real industrial example is presented and analyzed.","Technology management,
Information technology,
Libraries,
Concrete,
Business,
Information management,
Gas insulated transmission lines,
Computer science,
Best practices,
Electricity supply industry deregulation"
Efficient and accurate protocols for distributed delaunay triangulation under churn,"We design a new suite of protocols for a set of nodes in d-dimension (d ≫ 1) to construct and maintain a distributed Delaunay triangulation (DT) in a dynamic environment. The join, leave, and failure protocols in the suite are proved to be correct for a single join, leave, and failure, respectively. For a system under churn, it is impossible to maintain a correct distributed DT continually. We define an accuracy metric such that accuracy is 100% if and only if the distributed DT is correct. The suite also includes a maintenance protocol designed to recover from incorrect system states and to improve accuracy. In designing the protocols, we make use of two novel observations to substantially improve protocol efficiency. First, in the neighbor discovery process of a node, many replies to the node’s queries contain redundant information. Second, the use of a new failure protocol that employs a proactive approach to recovery is better than the reactive approaches used in prior work. Experimental results show that our new suite of protocols maintains high accuracy for systems under churn and each system converges to 100% accuracy after churning stopped. They are much more efficient than protocols in prior work.","Protocols,
Routing,
Distributed computing,
History,
Maintenance engineering,
Broadcasting"
LBF: A Labeled-Based Forecasting Algorithm and Its Application to Electricity Price Time Series,"A new approach is presented in this work with the aim of predicting time series behaviors. A previous labeling of the samples is obtained utilizing clustering techniques and the forecasting is applied using the information provided by the clustering. Thus, the whole data set is discretized with the labels assigned to each data point and the main novelty is that only these labels are used to predict the future behavior of the time series, avoiding using the real values of the time series until the process ends. The results returned by the algorithm, however, are not labels but the nominal value of the point that is required to be predicted. The algorithm based on labeled (LBF) has been tested in several energy-related time series and a notable improvement in the prediction has been achieved.",
An Energy-Efficient Routing Algorithm for Wireless Sensor Networks,"Since the nodes of wireless sensor networks are in the condition of a highly-limited and unreplenishable energy resource such as battery power, computation, and storage space, the energy efficiency is the most important key-point of the network routing designing. In this paper, A novel routing algorithm which combines with hierarchical routing and geographical routing is proposed. Based on the hierarchical network architecture, the process of forwarding packets between the source nodes in the target region and the base station consists of two phases—inter-cluster routing and intra-cluster routing, a greedy algorithm is adopted in the process of the inter-cluster routing and an multi-hop routing algorithm based on the forwarding restriction angle is designed for the intra-cluster routing. The analysis and simulation results show that our routing algorithm has better performance in terms of energy consumption and delay, it is suitable for the data transmission in a high-density wireless sensor network.",
A phishing sites blacklist generator,"Phishing is an increasing web attack both in volume and techniques sophistication. Blacklists are used to resist this type of attack, but fail to make their lists up-to-date. This paper proposes a new technique and architecture for a blacklist generator that maintains an up-to-date blacklist of phishing sites. When a page claims that it belongs to a given company, the company’s name is searched in a powerful search engine like Google. The domain of the page is then compared with the domain of each of the Google’s top-10 searched results. If a matching domain is found, the page is considered as a legitimate page, and otherwise as a phishing site. Preliminary evaluation of our technique has shown an accuracy of 91% in detecting legitimate pages and 100% in detecting phishing sites.","Uniform resource locators,
Internet,
Information filtering,
Information filters,
Computer crime,
Resists,
Computer architecture,
Search engines,
Electronic mail,
Steel"
An Adaptive Genetic Algorithm based approach for production reactive scheduling of manufacturing systems,"The problem for scheduling the manufacturing systems production involves the system modeling task and the application of a technique to solve it. There are several ways used to model the scheduling problem and search strategies have been applied on the models to find a solution. The solutions consider performance parameters like makespan. However, depending on the size and complexity of the system, the response time becomes critical, mostly when itpsilas necessary to reschedule. Researches aim to use Genetic Algorithms as a search method to solve the scheduling problem. This paper proposes the use of Adaptive Genetic Algorithm (AGA) to solve this problem having as performance criteria the minimum makespan and the response time. The probability of crossover and mutation is dynamically adjusted according to the individualpsilas fitness value. The proposed approach is compared with a traditional Genetic Algorithm (GA).","Genetic algorithms,
Production systems,
Job shop scheduling,
Manufacturing systems,
Delay,
Processor scheduling,
Search methods,
Genetic mutations,
Machinery production industries,
Computer science"
SJ-FINFET: A New Low Voltage Lateral Superjunction MOSFET,"This paper proposes a new SOI lateral superjunction (SJ) power transistor structure, SJ-FINFET, to address the requirement for low voltage lateral MOSFETs with low specific on-resistance (Ron, sp). The SJ-FINFET consists of a 3D trench gate and a SJ drift region (the fin) to reduce both the channel resistance and the drift region resistance. The SJ-FINFET with n/p-drift region pillar thickness (SOI layer thickness, Tepi) of 4¿m was simulated and found to have a Ron,sp of 0.18m¿.cm2. This is 21% lower than the well-known silicon limit at a breakdown voltage (BVdss) of 68V.","Low voltage,
MOSFET circuits,
Power MOSFET,
Surface resistance,
Power engineering computing,
Power engineering and energy,
Power semiconductor devices,
Electron devices,
Laboratories,
Silicon compounds"
"Multi-modal real-world driving data collection, transcription, and integration using Bayesian Network","In this paper we present our on-going data collection of multi-modal real-world driving. Video, speech, driving behavior, and physiological signals from 150 drivers have already been collected. To provide a more meaningful description of the collected data, we propose a transcription protocol based on six major groups: driver mental state, driver actions, driver’s secondary task, driving environment, vehicle status, and speech/background noise. Data from 30 drivers are transcribed. We then show how transcription reliability can be improved by properly training annotators. Finally, we integrate transcriptions, driving behavior, and physiological signals using a Bayesian Network for estimating a driver’s level of irritation. Estimations are compared to actual values, assessed by the drivers themselves. Preliminary results are very encouraging.",Vehicles
Hybrid Address Configuration for Tree-based Wireless Sensor Networks,This letter proposes a new scheme to alleviate the issue on address acquisition failure in wireless sensor networks (WSNs). The basic idea is to use a hierarchical address structure to make the proposed scheme less susceptible to physical distribution of WSN devices. Simulation results show that the new scheme significantly reduces the failure probability.,"Wireless sensor networks,
ZigBee,
Computer science,
Computerized monitoring,
Condition monitoring,
Medical services,
Home automation,
Traffic control,
Routing,
Table lookup"
Gaussian processes for source separation,"In this paper we present a probabilistic method for source separation in the case where each source has a certain unknown temporal structure. We tackle the problem of source separation by maximum pseudo-likelihood estimation, representing the latent function which characterizes the temporal structure of each source by a random process with a Gaussian prior. The resulting pseudo-likelihood of the data is Gaussian, determined by a mixing matrix as well as by the predictive mean and covariance matrix that can be easily computed by Gaussian process (GP) regression. Gradient-based optimization is applied to estimate the demixing matrix through maximizing the log-pseudo-likelihood of the data. Numerical experiments confirm the useful behavior of our method, compared to existing source separation methods.","Gaussian processes,
Source separation,
Covariance matrix,
Independent component analysis,
Random processes,
Machine learning,
Character generation,
Computer science,
Pattern recognition,
Signal processing"
SpecVAT: Enhanced Visual Cluster Analysis,"Given a pairwise dissimilarity matrix
D
D
of a set ofobjects, visual methods such as the VAT algorithm (for visual analysis of cluster tendency) represent
D
D
as an image
I(
D
D
~
)
where the objects are reordered to highlight cluster structure as dark blocks along the diagonal of the image. A major limitation of such visual methods is their inability to highlight cluster structure in
I(
D
D
~
)
when
D
D
contains clusters with highly complex structure. In this paper, we address this limitation by proposing a Spectral VAT (SpecVAT) algorithm, where
D
D
is mapped to
D
′
D
′
in an embedding space by spectral decomposition of the Laplacian matrix, and then reordered to
D
′
~
D
′
~
using the VAT algorithm. We also propose astrategy to automatically determine the number of clusters in
I(
D
′
~
D
′
~
)
, as well as a method for cluster formation from
I(
D
′
~
D
′
~
)
based on the difference between diagonal blocks and off-diagonal blocks. We demonstrate the effectiveness of our algorithms on several synthetic and real-world data sets that are not amenable to analysis via traditional VAT.","Clustering algorithms,
Algorithm design and analysis,
Data mining,
Image analysis,
Data engineering,
Data analysis,
Partitioning algorithms,
Pixel,
Australia,
Computer science"
Enhancing Automatic Incident Detection Techniques Through Vehicle To Infrastructure Communication,"One of the fundamental requirements of a traffic management system is the ability to determine when an incident has occurred so that proper responses can be initiated. Automatic incident detection (AID) has been considered a method for quickly detecting potential incidents on the road. Although Vehicular Ad hoc Networks (VANETs) started mainly for safety applications, surprisingly a very few work have been done in VANETs for Automatic Incident Detection while most of the research went for developing routing protocols and privacy techniques. In this paper, we introduce a novel incident detection technique for non dense traffic flow by taking advantage of communication between cars and some roadside infrastructure installed on the road every mile or so. The proposed technique can provide a great enhancement to the existing AID techniques specially under sparse traffic where most of them fail to detect non blocking incidents.","Vehicle detection,
Telecommunication traffic,
Intelligent transportation systems,
Road accidents,
Vehicles,
Ad hoc networks,
Safety,
Routing protocols,
Privacy,
Event detection"
Parameter estimation and optimal control of swarm-robotic systems: A case study in distributed task allocation,"This paper presents a methodology for finding optimal control parameters as well as optimal system parameters for robot swarm controllers using probabilistic, population dynamic models. With distributed task allocation as a case study, we show how optimal control parameters leading to a desired steady-state task distribution for two fully-distributed algorithms can be found even if the parameters of the system are unknown. First, a reactive algorithm in which robots change states independently from each other and which leads to a linear macroscopic model describing the dynamics of the system is considered. Second, a threshold-based algorithm where robots change states based on the number of other robots in this state and which leads to a non-linear model is investigated. Whereas analytical results can be obtained for the linear system, the optimization of the non-linear controller is performed numerically. Finally, we show using stochastic simulations that whereas the presented methodology and models work best if the swarm size is large, useful results can already be obtained for team-sizes below a hundred robots. The methodology presented can be applied to scenarios involving the control of large numbers of entities with limited computational and communication abilities as well as a tight energy budget, such as swarms of robots from the centimeter to nanometer range or sensor networks.","Parameter estimation,
Optimal control,
Robot sensing systems,
Robot control,
Steady-state,
Nonlinear dynamical systems,
Performance analysis,
Linear systems,
Nonlinear control systems,
Control systems"
"Pose detection of 3-D objects using images sampled on SO(3), spherical harmonics, and wigner-D matrices","Determining the pose of three-dimensional objects from two-dimensional images has become an important issue in industrial automation applications. Eigendecomposition represents one computationally efficient method for dealing with this class of problems. One major drawback of using the eigendecomposition technique is the expensive off-line computation required to calculate the optimal subspace. This off-line computational expense may preclude the use of eigendecomposition for industrial applications where time is of the essence. In this work, we address this issue by proposing a computationally efficient algorithm for estimating the eigendecomposition to a user specified accuracy. In particular, we sample the rotation group SO(3) in a manner that allows us to take advantage of the correlation in SO(3) and transform the data from the spatial domain to the spectral domain. We then present an algorithm to estimate the eigendecomposition in this domain, thus relieving the computational burden. Experimental results are presented to compare the proposed algorithm to the true eigendecomposition, as well as assess the computational savings.","Bridges,
USA Councils,
Automation,
Conferences"
A New Residue to Binary Converter Based on Mixed-Radix Conversion,"In this paper, an efficient design of the residue to binary converter for the new moduli set {2n-1, 2n, 22n+1-1} based on mixed-radix conversion (MRC) is presented. The moduli set {2n-1, 2n, 22n+1-1} is obtained by enhancing one of the moduli of the recently introduced moduli set {2n-1, 2n, 2n+1-1}. The proposed moduli set includes well-formed moduli which can result in efficient residue to binary conversion as well as binary to residue conversion. The presented residue to binary converter is adder based and requires less conversion time and lower hardware cost, compared to the residue to binary converters for a moduli set with similar dynamic range.","Dynamic range,
Digital signal processing,
Adders,
Hardware,
Digital arithmetic,
Error correction codes,
Design engineering,
Costs,
Image converters,
Image processing"
Conflict-aware schedule of software refactorings,"Software refactoring is to restructure the internal structure of object-oriented software to improve software quality, especially maintainability, extensibility and reusability while preserving its external behaviours. According to predefined refactoring rules, we may find many places in the software where refactorings can be applied. Applying each refactoring, we may achieve some effect (quality improvement). If we can apply all of the available refactorings, we can achieve the greatest effect. However, the conflicts among refactorings usually make it impossible. The application of a refactoring may change or delete elements necessary for other refactorings, and thus disables these refactorings. As a result, the application order (schedule) of the available refactorings determines which refactorings will be applied, and thus determines the total effect achieved by the refactoring activity. Consequently, conflicting refactorings had better be scheduled rationally so as to promote the total effect of refactoring activities. However, how to schedule conflicting refactorings is rarely discussed.In this paper, a conflict-aware scheduling approach is proposed. It schedules refactorings according to the conflict matrix of refactorings and effects of each individual refactoring. The scheduling model is a multi-objective optimisation model. We propose a heuristic algorithm to solve the scheduling model. We also evaluate the proposed scheduling approach in non-trivial projects. Evaluation results suggest that refactoring activities with the scheduling approach lead to greater effect (quality improvement) than refactoring activities without explicit scheduling.","software reusability,
matrix algebra,
object-oriented programming,
optimisation,
scheduling,
software maintenance,
software quality"
On the impact of caching and a model for storage-capacity measurements for energy conservation in asymmetrical wireless devices,"Traffic and channel-data rate combined with the stream oriented methodology can provide a scheme for offering optimized and guaranteed QoS. In this work a stream oriented modeled scheme is proposed based on each node’s self-scheduling energy management. This scheme is taking into account the overall packet loss in order to form the optimal effective -for the end-to-end connection- throughput response. The scheme also -quantitatively- takes into account the asymmetrical nature of wireless links and the caching activity that is used for data revocation in the ad-hoc based connectivity scenario. Through the designed middleware and the architectural layering and through experimental simulation, the proposed energy-aware management scheme is thoroughly evaluated in order to meet the parameters’ values where the optimal throughput response for each device/user is achieved.","Energy storage,
Energy measurement,
Energy conservation,
Traffic control,
Wireless sensor networks,
Telecommunication traffic,
Delay,
Protocols,
History,
Routing"
"A Novel 24 GHz One-Shot, Rapid and Portable Microwave Imaging System","Development of microwave and millimeter wave imaging systems has received significant attention in the past decade. Signals at these frequencies penetrate inside of dielectric materials and have relatively small wavelengths. Thus, imaging systems at these frequencies can produce images of the dielectric and geometrical distributions of objects. Although there are many different approaches for imaging at these frequencies, they each have their respective advantageous and limiting features (hardware, reconstruction algorithms). One method involves electronically scanning a given spatial domain while recording the coherent scattered field distribution from an object. Consequently, different reconstruction or imaging techniques may be used to produce an image (dielectric distribution and geometrical features) of the object. The ability to perform this accurately and fast can lead to the development of a rapid imaging system that can be used in the same manner as a video camera. This paper describes the design of such a system, operating at 24 GHz, using modulated scatterer technique applied to 30 resonant slots in a prescribed measurement domain.","Microwave imaging,
Frequency,
Scattering,
Millimeter wave technology,
Dielectric materials,
Hardware,
Reconstruction algorithms,
Image reconstruction,
Cameras,
Resonance"
Blind Source Extraction for Hands-Free Speech Recognition Based on Wiener Filtering and ICA-Based Noise Estimation,"In this paper, we proposed a new blind speech extraction method consisting of Wiener filtering and noise estimation based on independent component analysis (ICA). First, we provide both theoretically and experimental investigations on proficiency of ICA in noise estimation under a non-point-source noise condition. Next, computer simulation and experiment in an actual railway-station environment are conducted, and their results also indicate that ICA is proficient in noise estimation under a non-point-source noise condition. Finally, we newly propose a blind speech extraction method based on Wiener filtering and ICA-based noise estimation, and the effectiveness of the proposed method via speech recognition test in an actual railway-station environment.","Speech recognition,
Wiener filter,
Independent component analysis,
Working environment noise,
Speech enhancement,
Acoustic noise,
Source separation,
Additive noise,
Speech analysis,
Acoustic distortion"
Architecture Patterns for Mobile Games Product Lines,"Product line software engineering (PLSE) is a promising method for software reuse. For the key of success, a reusable and adaptable architecture design is necessary. In domain of mobile games product lines, architectures have some common structures because of circumstance properties of the domain and business constraints. In this paper, we classify them as architecture patterns. By doing this, we can expect increasing portability, reusability, adaptability, maintainability, and eventually productivity and improving quality.","Computer architecture,
Software engineering,
Application software,
Productivity,
Software architecture,
Software quality,
Mobile computing,
Computer science,
Programming,
Security"
Almost-Sure Model Checking of Infinite Paths in One-Clock Timed Automata,"In this paper, we define two relaxed semantics (one based on probabilities and the other one based on the topological notion of largeness) for LTL over infinite runs of timed automata which rule out unlikely sequences of events. We prove that these two semantics match in the framework of single-clock timed automata (and only in that framework), and prove that the corresponding relaxed model-checking problems are PSPACE-Complete. Moreover, we prove that the probabilistic non-Zenoness can be decided for single-clocktimed automata in NLOGSPACE.",
Human intention recognition in Smart Assisted Living Systems using a Hierarchical Hidden Markov Model,"In this paper, we propose a Smart Assisted Living (SAIL) System and design a Hierarchical Hidden Markov Model (HHMM) based algorithm for human intention recognition. We focus on the problem of classifying hand gestures by using a single inertial sensor worn on a finger of the subject. The variation of context information, which is modeled by an HMM is used to improve the accuracy of hand gesture recognition in our previous work. The obtained results prove the effectiveness of our method.","Bridges,
USA Councils,
Automation,
Conferences"
An Algorithm to Detect Stepping-Stones in the Presence of Chaff Packets,"A major concern for network intrusion detection systems is the ability of an intruder to evade the detection by routing through a chain of the intermediate hosts to attack a target machine and maintain the anonymity. Such an intermediate host is called a stepping-stone. The intruders have developed some evasion techniques such as injecting chaff packets. A number of algorithms have been proposed to detect stepping-stones, but some of them failed to detect correctly when the network traffic is somehow corrupted or with the chaff packets. We discuss the viability of solving those issues by improving a previous methodology. The algorithm is based on finding as many matched pairs of incoming and outgoing packets on the same host as possible and then decide whether it is a stepping-stone connection by the mismatched rate. We examine a number of tradeoffs in choosing the threshold values by simulating network traffic. Our experiments report a very good performance with very low false detection rates when using carefully selected parameter values.","Intrusion detection,
Telecommunication traffic,
Delta modulation,
Cryptography,
Timing,
Delay,
Detection algorithms,
Computer science,
USA Councils,
Routing"
A New Mitigation Approach for Soft Errors in Embedded Processors,"Embedded processors, like for example processor macros inside modern FPGAs, are becoming widely used in many applications. As soon as these devices are deployed in radioactive environments, designers need hardening solutions to mitigate radiation-induced errors. When low-cost applications have to be developed, the traditional hardware redundancy-based approaches exploiting m-way replication and voting are no longer viable as too expensive, and new mitigation techniques have to be developed. In this paper we present a new approach, based on processor duplication, checkpoint and rollback, to detect and correct soft errors affecting the memory elements of embedded processors. Preliminary fault injection results performed on a PowerPC-based system confirmed the efficiency of the approach.","Hardware,
Fault detection,
Costs,
Redundancy,
Registers,
Computer aided instruction,
Monitoring,
Application software,
Field programmable gate arrays,
Voting"
Image segmentation with a parametric deformable model using shape and appearance priors,"We propose a novel parametric deformable model controlled by shape and visual appearance priors learned from a training subset of co-aligned images of goal objects. The shape prior is derived from a linear combination of vectors of distances between the training boundaries and their common centroid. The appearance prior considers gray levels within each training boundary as a sample of a Markov-Gibbs random field with pairwise interaction. Spatially homogeneous interaction geometry and Gibbs potentials are analytically estimated from the training data. To accurately separate a goal object from an arbitrary background, empirical marginal gray level distributions inside and outside of the boundary are modeled with adaptive linear combinations of discrete Gaussians (LCDG). The evolution of the parametric deformable model is based on solving an Eikonal partial differential equation with a new speed function which combines the prior shape, prior appearance, and current appearance models. Due to the analytical shape and appearance priors and a simple Expectation-Maximization procedure for getting the object and background LCDG, our segmentation is considerably faster than most of the known geometric and parametric models. Experiments with various goal images confirm the robustness, accuracy, and speed of our approach.","Image segmentation,
Deformable models,
Shape control,
Vectors,
Geometry,
Training data,
Gaussian distribution,
Partial differential equations,
Parametric statistics,
Robustness"
A Novel Algorithm for Completely Hiding Sensitive Association Rules,"With rapid advance of the network and data mining techniques, the protection of the confidentiality of sensitive information in a database becomes a critical issue when releasing data to outside parties. Association analysis is a powerful and popular tool for discovering relationships hidden in large data sets. The relationships can be represented in a form of frequent itemsets or association rules. One rule is categorized as sensitive if its disclosure risk is above some given threshold. Privacy-preserving data mining is an important issue which can be applied to various domains, such as Web commerce, crime reconnoitering, health care, and customer's consumption analysis. The main approach to hide sensitive association rules is to reduce the support or the confidence of the rules. This is done by modifying transactions or items in the database. However, the modifications will generate side effects, i.e., nonsensitive rule falsely hidden (i.e., lost rules) and spurious rules falsely generated (i.e., new rules). There is a trade-off between sensitive rules hidden and side effects generated. In this study, we propose an efficient algorithm, FHSAR, for fast hiding sensitive association rules(SAR). The algorithm can completely hide any given SAR by scanning database only once, which significantly reduces the execution time. Experimental results show that FHSAR outperforms previous works in terms of execution time required and side effects generated in most cases.","Association rules,
Data mining,
Business,
Medical services,
Data privacy,
Intelligent systems,
Deductive databases,
Intelligent networks,
Application software,
Computer science"
PASS: An Approach to Personalized Automated Service Composition,"With the rapid development of SOC (Service oriented computing), the automated service composition has become an important research direction. Through automated service composition, business processes need not to be constructed in advance, which helps to improve the flexibility of service composition. The current research on automated service composition is mainly based on AI techniques, and a common domain-oriented knowledge base is usually required to perform the heuristic planning. In practice, it is impossible for the knowledge base to characterize the personalized requirements of different users, so the AI-based methods can not apply to the user-centric application scenarios. In this paper, we propose PASS, a novel approach to personalized automated service composition. With PASS, both the hard-constraints represented by user's initial state, and the soft-constraints represented by user preferences can be satisfied in the process of automated service composition. Furthermore, three algorithms are designed to implement preference-aware automated service composition. In these algorithms, the Pareto dominance principle and relaxation degree are used to select the most satisfied composite service for users. Finally, comprehensive simulations are conducted to evaluate the performance and effectiveness of the proposed algorithms.","Web services,
Pareto analysis"
Using Remote Lab Networks to Provide Support to Public Secondary School Education Level,"The advantages of networking are widely known in many areas (from business to personal ones). One particular area where networks have also proved their benefits is education. Taking the secondary school education level into account, some successful cases can be found in literature. In this paper we describe a particular remote lab network supporting physical experiments accessible to students of institutions geographically separated. The network architecture and application examples of using some of the available remote experiments are illustrated in detail.","Educational institutions,
Education,
Computers,
Internet,
Collaboration,
Materials,
Laboratories"
Performance Comparison and Analysis of Routing Strategies in Mobile Ad Hoc Networks,"The performance of routing protocols in Mobile Ad hoc network (MANET) always attracts many attentions. As many previous works have shown, routing performance is greatly dependent to the availability and stability of wireless links. Although there are some studies reported to evaluate the performance of routing protocols in MANET, little work is done for the system overall performance, which is generally referred to as the network throughput and the end-to-end delay. This paper evaluates the routing strategies of the three routing protocols (AODV, OLSR and SRMP) and compares their performance under two different mobility models. We study those protocols under varying metrics such as node mobility and network size. Our objective is to provide a qualitative assessment of overall performance of the routing strategies in different mobility model. The results illustrate that the routing strategies behind the three protocols affect their performance deeply. Meanwhile, different from previous results in the literature, the result shows that it is the combination of routing strategies behind the routing protocols to determine the performance in given scenarios.","Performance analysis,
Mobile ad hoc networks,
Routing protocols,
Ad hoc networks,
Computer science,
Throughput,
Network topology,
Software engineering,
Availability,
Stability"
Modelling and synthesising F0 contours with the discrete cosine transform,"The Discrete Cosine Transform is proposed as a basis for representing fundamental frequency (F0) contours of speech. The advantages over existing representations include deterministic algorithms for both analysis and synthesis and a simple distancemeasure in the parameter space. A two-tiermodel using the DCT is shown to be able to model F0 contours to around 10Hz RMS error. A proof-of-concept system for synthesising DCT parameters is evaluated, showing that the benefits do not come at the expense of speech synthesis applications.","Discrete cosine transforms,
Speech synthesis,
Statistical analysis,
Algorithm design and analysis,
Computer science,
Training data,
Natural languages,
Event detection,
Time measurement,
Extraterrestrial measurements"
Imperfect block diagonalization for multiuser MIMO downlink,"Recently, a multiuser MIMO system has attracted much attention. In the downlink of the multiuser MIMO system, since the base station simultaneously transmits signals to terminals, there is inter-user interference (IUI) at each terminal. Block diagonalization, which can achieve perfect IUI suppression, has been extensively studied to solve the issue. However, with the scheme we cannot obtain extra diversity gain due to the null steering at the base station. In this paper, we propose imperfect block diagonalization based on maximum eigenvectors of users and Gram-Schmidt orthonormalization, and its error rate performance is evaluated using computer simulations. The result shows that, despite its low complexity, the proposed scheme provides excellent performance, especially in many-user environments.","MIMO,
Downlink,
Base stations,
Interference,
Frequency,
Receiving antennas,
Transmitting antennas,
Diversity methods,
Computer simulation,
Space technology"
Feedback-controlled resource sharing for predictable eScience,"The emerging class of adaptive, real-time, data-driven applications are a significant problem for today's HPC systems. In general, it is extremely difficult for queuing-system-controlled HPC resources to make and guarantee a tightly-bounded prediction regarding the time at which a newly-submitted application will execute. While a reservation-based approach partially addresses the problem, it can create severe resource under-utilization (unused reservations, necessary scheduled idle slots, underutilized reservations, etc.) that resource providers are eager to avoid. In contrast, this paper presents a fundamentally different approach to guarantee predictable execution. By creating a virtualized application layer called the performance container, and opportunistically multiplexing concurrent performance containers through the application of formal feedback control theory, we regulate the job's progress such that the job meets its deadline without requiring exclusive access to resources even in the presence of a wide class of unexpected disturbances. Our evaluation using two widely-used applications, WRF and BLAST, on an 8-core server show our approach is predictable and meets deadlines with 3.4 % of errors on average while achieving high overall utilization.","Resource management,
Containers,
Resource virtualization,
Application virtualization,
Real time systems,
Supercomputers,
Physics computing,
Weather forecasting,
Computer science,
Application software"
A P2P Traffic Classification Method Based on SVM,"A method to realize the P2P network traffic classification based on the SVM is proposed. This method uses the network traffic statistical characteristic and SVM method that based on the statistical theory to classifies the different P2P traffic application. Mainly research on four kind of network traffic classification, in document sharing BitTorrent, in media flows PPLive, in network telephone Skype, in immediate communication MSN. Introduced P2P traffic classification overall framework based on the SVM. Described how gain the traffic sample and the processing method. And introduced the experimental results and constructs the traffic classifier. The experimental result confirmed the validity of proposed method; average precise rate is 92.38%.",
Performance study of wireless mesh networks routing metrics,"Multihop wireless mesh networks are an attractive solution for providing last-mile connectivity. However, the shared nature of the transmission medium makes it challenging to fully exploit these networks. In an attempt to improve the radio resource utilization, several routing metrics have been specifically designed for wireless mesh networks. However, although some evaluations have been conducted to assess the performance of these metrics in some contrived scenarios, no overall comparison has been performed. We therefore studied the performance of the most popular routing metrics currently used in wireless mesh networks: Hop Count, Blocking Metric, Expected Tranmission Count (ETX), Expected Transmission Time (ETT), Modified ETX (mETX), Network Allocation Vector Count (NAVC) and Metric of Interference and Channel-Switching (MIC). We showed under various simulation scenarios that although all the metrics except NAVC offer the same end-to-end delay and packet loss ratio, differences can be distinguished in terms of traffic load repartition. In particular the congestion-avoidance strategies of ETX, mETX, and MIC prevent the starvation of flows following longer paths and consequently provide a more uniform traffic repartition.","Wireless mesh networks,
Routing,
Interference,
Resource management,
Microwave integrated circuits,
Telecommunication traffic,
Performance evaluation,
Delay,
Costs,
Computer science"
Evaluation of charge drives for scanning probe microscope positioning stages,"Due to hysteresis exhibited by piezoelectric actuators, positioning stages in scanning probe microscopes require sensor-based closed-loop control. Although closed-loop control is effective at eliminating non-linearity at scan speeds below 10Hz, it also severely limits bandwidth and contributes sensor-induced noise. The need for high-gain feedback is reduced or eliminated if the piezoelectric actuators are driven with charge rather than voltage. Charge drives can reduce hysteresis to less than 1% of the scan range. This results in a corresponding increase in bandwidth and reduction of sensor induced noise. In this work we review the design of charge drives and compare them to voltage amplifiers for driving lateral SPMscanners. The first experimental images using charge drive are presented.","Voltage,
Hysteresis,
Scanning probe microscopy,
Piezoelectric actuators,
Bandwidth,
Feedback,
Capacitors,
Atomic force microscopy,
Drives,
Nanopositioning"
HF skywave MIMO radar: The HILOW experimental program,"In the paper we report on an experimental program undertaken by the Defence Science and Technology Organisation and partners during the period from early 2006 to late 2007. The focus of the program was to investigate MIMO radar use in over-the-horizon radar. The program revealed that a specific MIMO radar technique: Non-causal Radar-Transmit Beamforming, can be applied to this radar class. Examples of this technique were demonstrated, including non-causal fixed transmit beamforming as well as non-causal adaptive transmit beamforming and non-causal direction-of-departure estimation. This is the first case where practically useful adaptive transmitter beamforming has been demonstrated in OTHR.","Hafnium,
MIMO,
Transmitters,
Array signal processing,
Australia,
Spatial resolution,
Radar applications,
Radar cross section,
Radar antennas,
Global Positioning System"
Chord4S: A P2P-based Decentralised Service Discovery Approach,"Service-oriented computing is emerging as a paradigm for developing distributed applications. A critical issue of utilising service-oriented computing is to have a scalable, reliable and robust service discovery mechanism. However, traditional service discovery methods using centralised registries can easily suffer from problems such as performance bottleneck and vulnerability to failures in the large scalable service network, thus functioning abnormally. To address these problems, this paper proposes a peer-to-peer based decentralised service discovery approach named Chord4S. Chord4S utilises the data distribution and lookup capabilities of the popular Chord to distribute and discover services in a decentralised manner. Data availability is further improved by distributing service descriptions of functionally-equivalent services to different successor nodes that are organised into a virtual segment in the Chord circle. In addition, the Chord routing protocol is extended to support efficient discovery of multiple services with single request. This enables late negotiation of service level agreements between a service consumer and multiple service providers. The experimental evaluation shows that Chord4S achieves higher data availability and provides efficient query with reasonable overhead.","Web services,
peer-to-peer computing"
"FES: A System for Combining Face, Ear and Signature Biometrics Using Rank Level Fusion","Performance rate of unimodal biometric system is often reduced due to physiological defects, user mode and the environment. Multibiometric systems seek to alleviate some of these drawbacks by providing multiple evidences of the same identity. In this paper, we develop a multimodal biometric system, FES, based on Principal Component Analysis (PCA) and Fisher’s Linear Discriminant (FLD) methods that will use face, ear and signature for identity identification and rank level fusion for consolidate the results obtained from these monomodal matchers. The ranks of individual matchers are combined using the Borda count method and the Logistic regression method. The results indicate that fusing individual modalities improve the overall performance of the biometric system.","Ear,
Biometrics,
Principal component analysis,
Logistics,
Linear discriminant analysis,
Pattern recognition,
Information technology,
Fusion power generation,
Computer science,
Biosensors"
Some Computer Science Issues in Creating a Sustainable World,"Computer scientists have a role to play in combating global climate change. Global climate change is one of the most pressing problems of our time. Government agencies, universities, and businesses are starting to step up and invest in research, but even more change is needed, ranging from standards and policies to research innovations and new businesses.","Computer science,
Energy consumption,
Costs,
Media Access Protocol,
Computer aided manufacturing,
Energy efficiency,
Carbon dioxide,
Hardware,
Routing protocols,
Energy management"
Real Time Moving Object Tracking by Particle Filter,"Robust and Real time moving object tracking is a tricky job in computer vision problems. Particle filtering has been proven very successful for non-gaussian and non-linear estimation problems. In this paper, we first try to develop a color based particle filter. In this approach, the object tracking system relies on the deterministic search of window, whose color content matches a reference histogram model. A simple HSV histogram-based color model is used to develop our observation system. Secondly and finally, we describe a new approach for moving object tracking with particle filter by shape information. The shape similarity between a template and estimated regions in the video scene is measured by their normalized cross-correlation of distance transformed image. Our observation system of particle filter is based on shape from distance transformed edge features. Template is created instantly by selecting any object from the video scene by a rectangle. Experimental results have been presented to show the effectiveness of our proposed system.","Particle filters,
Target tracking,
Image color analysis,
Distance measurement,
Shape,
Robustness,
Correlation"
Beyond the Lambertian assumption: A generative model for Apparent BRDF fields of faces using anti-symmetric tensor splines,"Human faces are neither exactly Lambertian nor entirely convex and hence most models in literature which make the Lambertian assumption, fall short when dealing with specularities and cast shadows. In this paper, we present a novel anti-symmetric tensor spline (a spline for tensor-valued functions) based method for the estimation of the Apparent BRDF (ABRDF) field for human faces that seamlessly accounts for specularities and cast shadows. Furthermore, unlike other methods, it does not require any 3D information to build the model and can work with as few as 9 images. In order to validate the accuracy of our anti-symmetric tensor spline model, we present a novel approximation of the ABRDF using a continuous mixture of single-lobed spherical functions. We demonstrate the effectiveness of our anti-symmetric tensor-spline model in comparison to other popular models in the literature, by presenting extensive results for face relighting and face recognition using the Extended Yale B database.","Tensile stress,
Humans,
Lighting,
Rendering (computer graphics),
Image databases,
Shape control,
Information science,
Face detection,
Face recognition,
Graphics"
Graph Analysis with High-Performance Computing,"Large, complex graphs arise in many settings including the Internet, social networks, and communication networks. To study such data sets, the authors explored the use of high-performance computing (HPC) for graph algorithms. They found that the challenges in these applications are quite different from those arising in traditional HPC applications and that massively multithreaded machines are well suited for graph problems.",
Motion CAPTCHA,In some websites it is necessary to distinguish between human users and computer programs which is known as CAPTCHA (Completely Automated Public Turing test to tell Computers and Human Apart). CAPTCHA methods are mainly based on the weaknesses of OCR systems while using them are undesirable to human users. In this paper a new CAPTCHA method is introduced on the basis of showing a movie of a person’s action. Then we ask the user to describe the movement of that person. The user should select the sentence which describes the motion from a list of sentences. If the user chooses the right sentence we can guess that the user is a human and not a computer program. The main advantage of this method is its simplicity and also the difficulty of computer attacks to it. This project has been implemented by PHP scripting language.,"Motion pictures,
Computers,
Databases,
Handicapped aids,
Internet,
Conferences,
Keyboards"
Realistic mobility models for Vehicular Ad hoc Network (VANET) simulations,"Vehicular Ad-Hoc Network (VANET) is surging in popularity, in which vehicles constitute the mobile nodes in the network. Due to the prohibitive cost of deploying and implementing such a system in real world, most research in VANET relies on simulations for evaluation. A key component for VANET simulations is a realistic vehicular mobility model that ensures that conclusions drawn from simulation experiments will carry through to real deployments. In this work, we first introduce a tool MOVE that allows users to rapidly generate realistic mobility models for VANET simulations. MOVE is built on top of an open source micro-traffic simulator SUMO. The output of MOVE is a realistic mobility model and can be immediately used by popular network simulators such as ns-2 and qualnet. We evaluate the effects of details of mobility models in three case studies of VANET simulations (specifically, the existence of traffic lights, driver route choice and car overtaking behavior) and show that selecting sufficient level of details in the simulation is critical for VANET protocol design.","Ad hoc networks,
Vehicles,
Protocols,
Transportation,
Roads,
Computational modeling,
Automotive engineering,
Traffic control,
Testing,
Turning"
Representing and Tracking of Dynamics Objects using Oriented Bounding Box and Extended Kalman Filter,"Representing and tracking of dynamics objects is one of the main parts of autonomous navigation in urban areas. In the framework of the development of a multiple objects tracking system using multisensor fusion, this paper presents an oriented bounding box (OBB) representation with uncertainty computation as well as a model for object tracking. The uncertainty computation method, which takes into account Laser Range Finder sensor uncertainty and object's relative position, is evaluated. The influence of this uncertainty on the accuracy of the estimation is shown. The tracking model, based on the Extended Kalman Filter is tested and evaluated using the OBB object's representation.","Uncertainty,
Vehicle dynamics,
Navigation,
Laser fusion,
Intelligent transportation systems,
Laboratories,
Urban areas,
Cities and towns,
Remotely operated vehicles,
Personal digital assistants"
Development of a cell phone-based video streaming system for persons with early stage Alzheimer's disease,"The current paper presents details regarding the early developments of a memory prompt solution for persons with early dementia. Using everyday technology, in the form of a cell-phone, video reminders are delivered to assist with daily activities. The proposed CPVS system will permit carers to record and schedule video reminders remotely using a standard personal computer and web cam. It is the aim of the three year project that through the frequent delivery of helpful video reminders that a ‘virtual carer’ will be present with the person with dementia at all times. The first prototype of the system has been fully implemented with the first field trial scheduled to take place in May 2008. Initially, only three patient carer dyads will be involved, however, the second field trial aims to involve 30 dyads in the study. Details of the first prototype and the methods of evaluation are presented herein.",
Delay Composition Algebra: A Reduction-Based Schedulability Algebra for Distributed Real-Time Systems,"This paper presents the delay composition algebra: a set ofsimple operators for systematic transformation of distributed real-time task systems into single-resource task systems such that schedulability properties of the original system are preserved. The transformation allows performing schedulability analysis on distributed systems using uniprocessor theory and analysis tools. Reduction-based analyses techniques have been used in other contexts such as control theory and circuit theory, by defining rules to compose together components of the system and reducing them into equivalent single components that can be easily analyzed. This paper is the first to develop such reduction rules for distributed real-time systems. By successively applying operators such as PIPE and SPLIT on operands that represent workload on composed subsystems, we show how a distributed task system can be reduced to an equivalent single resource task set from which the end-to-end delay and schedulability of tasks can be inferred. We show through simulations that the proposed analysis framework is less pessimistic with increasing system scale compared to traditional approaches.","Algebra,
Real time systems,
Processor scheduling,
Performance analysis,
Control theory,
Circuit theory,
Delay systems,
Computer science,
Analytical models,
Circuit stability"
Using server-to-server communication in parallel file systems to simplify consistency and improve performance,"The trend in parallel computing toward clusters running thousands of cooperating processes per application has led to an I/O bottleneck that has only gotten more severe as the CPU density of clusters has increased. Current parallel file systems provide large amounts of aggregate I/O bandwidth; however, they do not achieve the high degrees of metadata scalability required to manage files distributed across hundreds or thousands of storage nodes. In this paper we examine the use of collective communication between the storage servers to improve the scalability of file metadata operations. In particular, we apply server-to-server communication to simplify consistency checking and improve the performance of file creation, file removal, and file stat. Our results indicate that collective communication is an effective scheme for simplifying consistency checks and significantly improving the performance for several real metadata intensive workloads.","File systems,
Scalability,
Permission,
Bandwidth,
File servers,
Data visualization,
Mathematics,
Computer science,
Laboratories,
Concurrent computing"
Evaluating the risk of unmanned aircraft ground impacts,"Currently many countries are developing regulation to allow Unmanned Aircraft Systems (UAS) operations in their National Airspace System (NAS). Successful integration requires UAS to achieve, at a minimum, an equivalent level of safety to that of manned aviation. Safety is primarily defined in terms of the risk to human life, although potential collateral damages to property can also be taken into account. This paper presents a novel, general, quite conservative and rather simple method that calculates the probability of fatalities and the fatality rates associated with a ground impact. Spatial analysis results are presented that indicate where unmanned aircraft may fly based on obtained reliability and safety levels and provide a coherent comparison of reliability requirements between unmanned aircraft of different families.","Automation,
Conferences"
Improving the k-svd facial image compression using a linear deblocking method,"The use of sparse representations in signal processing is gradually increasing in the past several years. In a previous work we proposed a new method for compressing facial images using the K-SVD algorithm, which is a novel algorithm for training overcomplete dictionaries that lead to sparse signal representations. This method was shown to be most efficient, surpassing the JPEG2000 performance significantly. In this paper we present a significant addition to our compression algorithm in the form of image deblocking. Since the encoding is done in patches, a visually disturbing artifacts of blockiness appear in the reconstructed images. We eliminate these artifacts using a linear deblocking technique, which is based on local image filters. We construct a linear filter for each relevant pixel independently, and apply these filters as post-processing. This method is limited, but nevertheless it improves the PSNR of the reconstructed images and gives visually appealing results.","Image coding,
Dictionaries,
Image reconstruction,
Signal processing algorithms,
Compression algorithms,
Nonlinear filters,
Redundancy,
Signal representations,
PSNR,
Image storage"
A Classification Model for Automating Compliance,"Various approaches to automate the observance of compliance requirements have recently been proposed. Since there is no all-round solution available within the foreseeable future, we propose a classification scheme for existing approaches that address different aspects of automating compliance. A layer model is used for linking laws and regulations with a company's IT system. The ""policy layer"" is seen as link between the non-technical compliance requirements and their implementation in the IT system. The achievable degree of automation essen-tially depends on the characteristics of the underlying policy language. Therefore, we derive and discuss crite-ria for a policy language in order to enable automation of compliance of business processes within the IT system.","Business,
Automatic control,
Companies,
Computer science,
Telematics,
Joining processes,
Design automation,
Frequency,
Computerized monitoring,
Software standards"
Improving Synthesis of Compressor Trees on FPGAs via Integer Linear Programming,"Multi-input addition is an important operation for many DSP and video processing applications. On FPGAs, multi-input addition has traditionally been implemented using trees of carry-propagate adders. This approach has been used because the traditional lookup table (LUT) structure of FPGAs is not amenable to compressor trees, which are used to implement multi-input addition and parallel multiplication in ASIC technology. In prior work, we developed a greedy heuristic method to map compressor trees onto the general logic of an FPGA using a component called generalized parallel counter (GPC). Although this technique reduced the combinational delay of our circuits, when synthesized onto Altera Stratix-II FPGAs, by 27% on average; however, the area was increased by an average 11%. To further reduce the delay and limit the increase in area, we have developed a new solution to the mapping problem based on integer linear programming. This new approach reduced the delay of the compressor tree by 32% on average and reduced the area by 3% compared to an adder tree.",
Beamforming in wireless relay networks,"This paper is on relay beamforming in wireless networks, in which the receiver has perfect information of all channels and each relay knows its own channels. Instead of the commonly used total power constraint on relays and the transmitter, we use a more practical assumption that every node in the network has its own power constraint. A two-step amplify-and-forward protocol with beamforming is used, in which the transmitter and relays are allowed to adaptively adjust their transmit power and directions according to available channel information. The optimal beamforming problem is solved analytically. The complexity of finding the exact solution is linear in the number of relays. Our results show that the transmitter should always use its maximal power and the optimal power used at a relay is not a binary function. It can take any value between zero and its maximum transmit power. Also, interestingly, this value depends on the quality of all other channels in addition to the relay’s own ones. Despite this coupling fact, distributive strategies are proposed in which, with the aid of a low-rate broadcast from the receiver, a relay needs only its own channel information to implement the optimal power control. Simulated performance shows that network beamforming achieves full diversity and outperforms other existing schemes.","Relays,
Array signal processing,
Transmitters,
Receivers,
Signal to noise ratio,
Power control,
Protocols"
Balloon Focus: a Seamless Multi-Focus+Context Method for Treemaps,"The treemap is one of the most popular methods for visualizing hierarchical data. When a treemap contains a large number of items, inspecting or comparing a few selected items in a greater level of detail becomes very challenging. In this paper, we present a seamless multi-focus and context technique, called Balloon Focus, that allows the user to smoothly enlarge multiple treemap items served as the foci, while maintaining a stable treemap layout as the context. Our method has several desirable features. First, this method is quite general and can be used with different treemap layout algorithms. Second, as the foci are enlarged, the relative positions among all items are preserved. Third, the foci are placed in a way that the remaining space is evenly distributed back to the non-focus treemap items. When Balloon Focus enlarges the focus items to a maximum degree, the above features ensure that the treemap will maintain a consistent appearance and avoid any abrupt layout changes. In our algorithm, a DAG (Directed Acyclic Graph) is used to maintain the positional constraints, and an elastic model is employed to govern the placement of the treemap items. We demonstrate a treemap visualization system that integrates data query, manual focus selection, and our novel multi-focus+context technique, Balloon Focus, together. A user study was conducted. Results show that with Balloon Focus, users can better perform the tasks of comparing the values and the distribution of the foci.","Data visualization,
Displays,
Tree graphs,
Computer science,
Manuals,
Large-scale systems,
File systems,
Telecommunication traffic,
Navigation,
Computer interfaces"
Compact Mobile RFID Antenna Design and Analysis Using Photonic-assisted Vector Near-field Characterization,"We present the design and analysis of a small planar loop antenna for applications in mobile RFID readers within the cell phone band. Compactness of the loop antenna, crucial to size reduction in antennas designed for mobile handsets, is realized by bending a folded-dipole in a circular fashion. Furthermore, capacitive loading, through a parasitic patch, is created inside the loop antenna and has an effect of increasing the electrical length of the structure. This enhanced length provides tunability of the resonance frequencies and a secondary dominant radiation port in the center of the antenna. A short planar dipole is transformed to a twice folded dipole and a loop antenna to produce a larger input resistance. We also analyze the performance of the RFID antenna by exploring the current-induced near field radiation patterns using a non-invasive photonic-assisted dielectric field mapping system. The evolution from the near to the far field is also discussed.","Radiofrequency identification,
Mobile antennas,
Dipole antennas,
Cellular phones,
Mobile handsets,
Loaded antennas,
Resonance,
Resonant frequency,
Electric resistance,
Pattern analysis"
Rule-Based Maintenance of Post-Requirements Traceability Relations,"An accurate set of traceability relations between software development artifacts is desirable to support evolutionary development. However, even where an initial set of traceability relations has been established, their maintenance during subsequent development activities is time consuming and error prone, which results in traceability decay. This paper focuses solely on the problem of maintaining a set of traceability relations in the face of evolutionary change, irrespective of whether generated manually or via automated techniques, and it limits its scope to UML-driven development activities post-requirements specification. The paper proposes an approach for the automated update of existing traceability relations after changes have been made to UML analysis and design models. The update is based upon predefined rules that recognize elementary change events as constituent steps of broader development activities. A prototype traceMaintainer has been developed to demonstrate the approach. Currently, traceMaintainer can be used with two commercial software development tools to maintain their traceability relations. The prototype has been used in two experiments. The results are discussed and our ongoing work is summarized.","Software systems,
Unified modeling language,
Prototypes,
Programming,
Software prototyping,
Information retrieval,
Computer science,
USA Councils,
Computer errors,
Testing"
A Multi-criteria Design Optimization Framework for Haptic Interfaces,"This paper presents a general framework for optimization of haptic interfaces, in particular for haptic interfaces with closed kinematic chains, with respect to multiple design objectives, namely kinematic and dynamic criteria. Both performance measures are discussed and optimization problems for a haptic interface with best worst-case kinematic and dynamic performance are formulated. Non-convex single objective optimization problems are solved with a branch-and-bound type (culling) algorithm. Pareto methods characterizing the trade-off between multiple design criteria are advocated for multi-criteria optimization over widely used scalarization approaches and Normal Boundary Intersection method is applied to efficiently obtain the Pareto-front hyper-surface. The framework is applied to a sample parallel mechanism (five-bar mechanism) and the results are compared with the results of previously published methods in the literature. Finally, dimensional synthesis of a high performance haptic interface utilizing its Pareto-front curve is demonstrated.","Design optimization,
Haptic interfaces,
Kinematics,
Manipulator dynamics,
Humans,
Rendering (computer graphics),
Impedance,
Computer interfaces,
Physics computing,
Displays"
The Importance of Small Pupils: A Study of How Pupil Dilation Affects Iris Biometrics,"This work studies the effect of pupil dilation on the accuracy of iris biometrics. We find that when matching enrollment and recognition images of the same person, larger differences in pupil dilation yield higher template dissimilarities, and so a greater chance of a false non-match. Another experimental result is that even when the degree of dilation is similar at enrollment and recognition, comparisons involving highly dilated pupils result in worse recognition performance than comparisons involving constricted pupils. We find that when the matched images have similarly highly dilated pupils, the mean Hamming distance of the match distribution increases and the mean Hamming distance of the non-match distribution decreases, bringing the distributions closer together from both directions. We recommend that a measure of pupil dilation be kept as meta-data for every iris code. Also, the absolute dilation of the two images, and the dilation difference between them, should factor into a confidence measure for an iris match.","Iris,
Biometrics,
Rubber,
Hamming distance,
Muscles,
Image recognition,
Size control,
Lighting control,
Control systems,
Robustness"
Profile-based face recognition,"In this paper, we introduce a new system for profilebased face recognition. The specific scenario involves a driver entering a gated area and using his/her side-view image (the driver remains seated in the vehicle) as identification. The system has two modes: enrollment and identification. In the enrollment mode, 3D face models of subjects are acquired and profiles extracted under different poses and stored to form a gallery database. In the identification mode, 2D images are acquired and the corresponding planar profiles are extracted and used as probes. Then, probes are matched to the gallery profiles to determine identity. The matching is accomplished using implicit shape registration via the vector distance functions. In our experiments, the approach using implicit registration exhibited higher accuracy than the iterative closest point methodology due to the use of more general transformations. The performance of our system is illustrated using a variety of databases.",
Testing a Collaborative DDoS Defense In a Red Team/Blue Team Exercise,"Testing security systems is challenging because a system's authors have to play the double role of attackers and defenders. Red team/blue team exercises are an invaluable mechanism for security testing. They partition researchers into two competing teams of attackers and defenders, enabling them to create challenging and realistic test scenarios. While such exercises provide valuable insight into vulnerabilities of security systems, they are very expensive and thus rarely performed. In this paper we describe a red team/blue team exercise, sponsored by DARPA's FTN program, and performed October 2002 --- May 2003. The goal of the exercise was to evaluate a collaborative DDoS defense, comprised of a distributed system, COSSACK, and a stand-alone defense, D-WARD. The role of the blue team was played by developers of the tested systems from USC/ISI and UCLA, the red team included researchers from Sandia National Laboratory, and all the coordination, experiment execution, result collection and analysis was performed by the white team from BBN Technologies. This exercise was of immense value to all involved --- it uncovered significant vulnerabilities in tested systems, pointed out desirable characteristics in DDoS defense systems (e.g., avoiding reliance on timing mechanisms), and taught us many lessons about testing of DDoS defenses.","security of data,
groupware,
program testing"
Automatic bearing fault pattern recognition using vibration signal analysis,"This paper presents vibration analysis techniques for fault detection in rotating machines. Rolling-element bearing defects inside a motor pump are the object of study. A dynamic model of the faults usually found in this context is presented. Initially a graphic simulation is used to produce the signals. Signal processing techniques, like frequency filters, Hilbert transform and spectral analysis are then used to extract features that will later be used as a base to classify the states of the studied process. After that real data from a centrifugal pump is submitted to the developed methods.","Pattern recognition,
Signal analysis,
Fault detection,
Rotating machines,
Pumps,
Context modeling,
Graphics,
Signal processing,
Frequency,
Filters"
Globally Optimized Robust Systems to Overcome Scaled CMOS Reliability Challenges,"Future system design methodologies must accept the fact that the underlying hardware will be imperfect, and enable design of robust systems that are resilient to hardware imperfections. Three techniques that can enable a sea change in robust system design are: 1. built-in soft error resilience (BISER), 2. circuit failure prediction, and 3. concurrent autonomous self-test using stored patterns (CASP). Global optimization across multiple abstraction layers is essential for cost-effective robust system design using these techniques.",
A Linear Programming Driven Genetic Algorithm for Meta-Scheduling on Utility Grids,"The user-level brokers in grids consider individual application QoS requirements and minimize their cost without considering demands from other users. This results in contention for resources and sub-optimal schedules. Meta-scheduling in grids aims to address this scheduling problem, which is NP hard due to its combinatorial nature. Thus, many heuristic-based solutions using Genetic Algorithm (GA) have been proposed, apart from traditional algorithms such as Greedy and FCFS. We propose a Linear Programming/Integer Programming model (LP/IP) for scheduling these applications to multiple resources. We also propose a novel algorithm LPGA (Linear programming driven Genetic Algorithm) which combines the capabilities of LP and GA. The aim of this algorithm is to obtain the best meta-schedule for utility grids which minimize combined cost of all users in a coordinated manner. Simulation results show that our proposed integrated algorithm offers the best schedule having the minimum processing cost with negligible time",
Tree based energy efficient and Congestion aware Routing Protocol for Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) have inherent and unique characteristics. One of the most important issues is their energy constraint. Energy aware routing protocol is very important in WSN, but routing protocol which only considers energy has not efficient performance. Congestion management can affect routing protocol performance. Congestion occurrence in network nodes leads to increasing packet loss and energy consumption. Another parameter which affects routing protocol efficiency is performing fairness in nodes energy consumption. When fairness is not considered in routing process, network will be partitioned very soon and then the network performance will be decreased. In this paper a Hierarchical Tree based Energy efficient and Congestion aware Routing Protocol (HTECRP) is proposed. The proposed protocol is an energy efficient routing one which tries to manage congestion and perform fairness in network. Simulation results shown in this paper imply that the HTECRP has achieved its goals.","Energy efficiency,
Routing protocols,
Wireless sensor networks,
Energy consumption,
Computer networks,
Power engineering and energy,
Temperature sensors,
Computer science,
Energy management,
Relays"
An Efficient RSA Public Key Encryption Scheme,"In this paper, we propose an efficient RSA public key encryption scheme, which is an improved version of original RSA scheme. The proposed RSA encryption scheme is based on linear group over the ring of integer mod a composite modulus  which is the product of two distinct prime numbers. In the proposed scheme the original message and the encrypted message are  square matrices with entities in zn indicated via l(h,z,n). Since the original RSA Scheme is a block cipher in which the original message and cipher message are integer in the interval [0, n-1]  for some integer modulus n.Therefore, in this paper, we generalize RSA encryption scheme in order to be implemented in the general linear group on the ring of integer mod n. Furthermore, the suggested encryption scheme has no restriction in encryption and decryption order and is claimed to be efficient, scalable and dynamic.","Public key,
Public key cryptography,
Digital signatures,
Security,
Information technology,
Computer science,
Matrix converters,
Communication channels"
Home-based cognitive monitoring using embedded measures of verbal fluency in a computer word game,"Verbal fluency is a standard neuropsychological test that measures the ease with which a person can produce words. It is commonly used in the diagnosis and characterization of cognitive disorders. In this paper, we describe a method for measuring a proxy for verbal fluency from embedded metrics within a computer word game. We evaluated our ability to monitor verbal fluency metrics over a period of one year in 30 elderly subjects by analyzing their computer game play. We found good correspondence between these computer metrics and the scores from standard verbal fluency assessments.",
Efficient system design space exploration using machine learning techniques,"Computer manufacturers spend a huge amount of time, resources, and money in designing new systems and newer configurations, and their ability to reduce costs, charge competitive prices and gain market share depends on how good these systems perform. In this work, we develop predictive models for estimating the performance of systems by using performance numbers from only a small fraction of the overall design space. Specifically, we first develop three models, two based on artificial neural networks and another based on linear regression. Using these models, we analyze the published Standard Performance Evaluation Corporation (SPEC) benchmark results and show that by using the performance numbers of only 2% and 5% of the machines in the design space, we can estimate the performance of all the systems within 9.1% and 4.6% on average, respectively. Then, we show that the performance of future systems can be estimated with less than 2.2% error rate on average by using the data of systems from a previous year. We believe that these tools can accelerate the design space exploration significantly and aid in reducing the corresponding research/development cost and time- to-market.","Space exploration,
Machine learning,
Computer aided manufacturing,
Costs,
Time sharing computer systems,
Performance gain,
Predictive models,
Artificial neural networks,
Linear regression,
Performance analysis"
A One-to-One Code and Its Anti-Redundancy,"One-to-one codes are ldquoone-shotrdquo codes that assign a distinct codeword to source symbols and are not necessarily prefix codes (more generally, uniquely decodable). Interestingly, as Wyner proved in 1972, for such codes the average code length can be smaller than the source entropy. By how much? We call this difference the anti-redundancy. Various authors over the years have shown that the anti-redundancy can be as big as minus the logarithm of the source entropy. However, to the best of our knowledge precise estimates do not exist. In this note, we consider a block code of length n generated for a binary memoryless source, and prove that the average anti-redundancy is -1/2 log2 n +C +F (n)+o (1) where C is a constant and either F (n) = 0 if log2(1-p)/p is irrational (where p is the probability of generating a ldquo0rdquo) or F(n) is a fluctuating function as the code length increases. This relatively simple finding requires a combination of analytic tools such as precise evaluation of Bernoulli sums, the saddle point method, and theory of distribution of sequences modulo 1.","Entropy,
Decoding,
Source coding,
Block codes,
Code standards,
Information theory,
Australia,
Computer science"
A New Kind of RFID Reader Anti-Collision Algorithm,"Reader collision problem is the most important problem in RFID reader network and which must be solved in order to make sure that RFID reader network works normally. Methods which are used to solve the reader collision problem are called as reader anti-collision algorithms. The goods and bads of the reader anti-collision algorithms play a vital role to the efficiency of the whole network system. In this paper, the reader collision problem is researched in detail and its different styles are summarized. On the basis of researching the existing algorithms, a new kind of reader anti-collision algorithm is proposed. By the simulation analysis, we can get the result that the proposed algorithm has better performance compared with others.","Radiofrequency identification,
Interference,
Frequency,
Paper technology,
Educational institutions,
Computer science,
Agricultural engineering,
Agriculture,
Electronic mail,
Analytical models"
Crossmodal content binding in information-processing architectures,"Operating in a physical context, an intelligent robot faces two fundamental problems. First, it needs to combine information from its different sensors to form a representation of the environment that is more complete than any representation a single sensor could provide. Second, it needs to combine high-level representations (such as those for planning and dialogue) with sensory information, to ensure that the interpretations of these symbolic representations are grounded in the situated context. Previous approaches to this problem have used techniques such as (low-level) information fusion, ontological reasoning, and (highlevel) concept learning. This paper presents a framework in which these, and related approaches, can be used to form a shared representation of the current state of the robot in relation to its environment and other agents. Preliminary results from an implemented system are presented to illustrate how the framework supports behaviours commonly required of an intelligent robot.","Robots,
Visualization,
Monitoring,
Humans,
Grounding,
Planning,
Cognition"
On a new class of error control codes and symmetric functions,"A general key equation based on elementary symmetric functions is developed for decoding some binary error control codes. Here, the syndrome is obtained by computing the elementary symmetric functions (instead of the power-sums) of the received word. A new class of codes is introduced in this paper which can correct up to t0 0 rarr 1 errors and, simultaneously, up to t1 1 rarr 0 errors. The new key equation can be used to decode this new class of codes and some known codes such as some t-asymmetric error correcting (t-AEC) codes, the t-symmetric error correcting (t-SEC) BCH codes and Goppa codes. Some generalizations to the non binary case are also given.",
Handling Constraints for Search Based Software Test Data Generation,"A major issue in software testing is the automatic gen- eration of the inputs to be applied to the programme un- der test. To solve this problem, a number of approaches based on search methods have been developed in the last few years, offering promising results for adequacy criteria like, for instance, branch coverage. We devise branch cov- erage as the satisfaction of a number of constraints. This al- lows to formulate the test data generation as a constrained optimisation problem or as a constraint satisfaction prob- lem. Then, we can see that many of the generators so far have followed the same particular approach. Furthermore, this constraint-handling point of view overcomes this limi- tation and opens the door to new designs and search strate- gies that, to the best of our knowledge, have not been con- sidered yet. As a case study, we develop test data generators employing different penalty objective functions or multiob- jective optimisation. The results of the conducted prelimi- nary experiments suggest these generators can improve the performance of classical approaches.","Software testing,
Automatic testing,
Search methods,
System testing,
Evolutionary computation,
Computer science,
Constraint optimization,
Software systems,
Automation,
Probability distribution"
A high flexible Early-Late Gate bit synchronizer in FPGA-based software defined radios,"The more increasing necessity of integration inside digital systems together with the advantages in terms of portability, reduced time-to-market, better flexibility and versatility, lead towards integrated all-digital FPGA based communication systems. Bit synchronization is a fundamental operation required for the best symbol detection. A high flexible Early-Late Gate implementation is proposed, it is optimized for low resource consumption in FPGA implementations.","synchronisation,
field programmable gate arrays,
software radio"
Accurately measuring collective operations at massive scale,"Accurate, reproducible and comparable measurement of collective operations is a complicated task. Although Different measurement schemes are implemented in well-known benchmarks, many of these schemes introduce different systematic errors in their measurements. We characterize these errors and select a window-based approach as the most accurate method. However, this approach complicates measurements significantly and introduces a clock synchronization as a new source of systematic errors. We analyze approaches to avoid or correct those errors and develop a scalable synchronization scheme to conduct benchmarks on massively parallel systems. Our results are compared to the window-based scheme implemented in the SKaMPI benchmarks and show a reduction of the synchronization overhead by a factor of 16 on 128 processes.","Synchronization,
Delay,
Predictive models,
Computer errors,
Concurrent computing,
Open systems,
Laboratories,
Computer science,
Clocks,
Error analysis"
The RoboCup Nanogram League: An Opportunity for Problem-Based Undergraduate Education in Microsystems,"A problem-based learning approach was chosen for a new senior elective in microsystems. The problem posed to the students was to design microrobots suitable for the new ldquonanogram leaguerdquo of the international RoboCup competition, which challenges teams of students and researchers to construct microscopic untethered robots that will compete against each other in soccer-related agility drills on a 2.5 mm by 2.5 mm playing field. The approach was shown to increase student interest and motivation. The course was considered a success and will be repeated with some modifications to increase the breadth of the course coverage.","Robots,
Turning,
Materials,
Metals,
Education,
Springs,
Fires"
The Ethernet Frame Payload Size and Its Effect on IPv4 and IPv6 Traffic,"The IPv4 traffic characteristics are deeply related to the 1500 byte limit, a consequence of the payload capacity of the Ethernet frame. This paper describes how this limit may impact future IPv6 traffic and tests some aspects of traffic behavior for new values. To assess this, real IPv4 traffic was transformed to IPv6 traffic, with several simple assumptions. It is showed that in some circumstances, the change from IPv4 to IPv6 causes an overhead in terms of number of transmitted bytes that is irrelevant when compared to the overhead of the number of transmitted packets. Conclusions point to the possible existence of increased routing and switching effort resulting from the shift to the IPv6 protocol. The paper also suggests that a wider limit could decrease significantly the number of generated packets.","Ethernet networks,
Payloads,
Telecommunication traffic,
Internet,
Computer science,
Testing,
Routing protocols,
TCPIP,
Local area networks,
Traffic control"
Design and Analyses of Efficient Chaotic Generators for Crypto-systems,"In this paper, we design and implement under Matlab/Simulink some efficient digital chaotic generators for data encryption/decryption process. Some of these generators (Logistic, PWLCM, Frey) are known, the others xcos(x), xexp[cos(x)], 2-D Tmap) are proposed. A number of designed generators contain cascaded layer to improve the statistical properties of the generated sequences. We introduce and demonstrate the importance of the perturbing orbit technique to avoid the dynamical degradation caused by the 2N-dimensional finite state space. This technique increases also the orbit cycle length. Finally, to quantify the security level of the proposed generators, we analyze their global dynamical properties using system and signal processing tools (Lyapunov exponents, bifurcation diagrams, distribution, pseudo phase space, auto¬correlation, cross correlation, NIST tests “National Institute of Standards and Technology “). Experimental and theoretical analyses show that the proposed generators have good cryptographic properties.","Chaos,
Cryptography,
Space technology,
Logistics,
Degradation,
State-space methods,
Security,
Signal generators,
Signal analysis,
Signal processing"
Fusing Received Signal Strength from Multiple Access Points for WLAN User Location Estimation,"WLAN received signal strength based location estimation techniques are promising for their low cost and ease of deployment. Existing techniques use received signal strength from different access points in an equal way. However, signals sent from the far access point experience more influence of the environmental noises, and more uncertainties are added in the signal strength. As detectable access points have different distance to the user, their contributions to the final estimation should be considered discriminatingly. In this paper, a novel location estimation approach is proposed using the Dempster-Shafer evidence theory to fuse received signal strength from multiple access points with different belief. We elaborate on three key problems during applying the Dempster-Shafter evidence theory. The approach has been implemented in a practical wireless network and experimental results indicate that it is feasible in both accuracy and locating time. An average location estimation error below 2 meters can be obtained.","Wireless LAN,
Working environment noise,
Wireless networks,
Global Positioning System,
Costs,
Uncertainty,
Spaceborne radar,
Space technology,
Internet,
Computer science"
Detection of misbehavior in cooperative diversity,"Cooperative diversity envisages to provide improved quality of service at the physical layer, with the assumption that wireless nodes are always willing to cooperate. This assumption may be valid in networks where cooperation is confined to nodes whose intentions are known a priori. However, in commercial wireless networks where nodes may misbehave for malicious or selfish intentions, it is difficult to maintain stable cooperation without a mechanism to detect and mitigate effects of misbehavior. In this paper we present a distributed mechanism to detect misbehaving nodes in a cooperative network which is characterized by a minimum frame success rate requirement and hybrid automatic repeat request (HARQ) is implemented to ensure reliable communication. The detection mechanism is developed by observing a number of first transmission attempts to learn how many frames can be transmitted successfully with zero retransmissions (first HARQ phase). Probability of successful transmission for the first transmission attempts is then estimated and its percentage deviation from the minimum frame success rate is measured to detect absence or presence of misbehavior. We show that the proposed mechanism can effectively detect selfish and malicious nodes. The detection mechanism can also be integrated into a reputation based scheme where a source node inform other nodes about the behavior of its partner.","Peer to peer computing,
Wireless networks,
Physical layer,
Costs,
Physics computing,
Quality of service,
Automatic repeat request,
Telecommunication network reliability,
Phase detection,
Communication system control"
Dependence Anti Patterns,"A dependence anti pattern is a dependence structure that may indicate potential problems for on-going software maintenance and evolution. Dependence anti patterns are not structures that must always be avoided. Rather, they denote warnings that should be investigated. This paper defines a set of dependence anti patterns and presents a series of case studies that show how these patterns can be identified using techniques for dependence analysis and visualization. The paper reports the results of this analysis on six real world programs, two of which are open source and four of which are part of production code in use with Daimler.",
An Evaluation Approach of Subjective Trust Based on Cloud Model,"As online trade and interactions on the internet are on the rise, a key issue is how to use simple and effective evaluation methods to accomplish trust decision-making for customers. It is well known that subjective trust holds uncertainty like randomness and fuzziness. However, existing approaches which are commonly based on probability or fuzzy set theory can not attach enough importance to uncertainty. To remedy this problem, a new quantifiable subjective trust evaluation approach is proposed based on the cloud model. Subjective trust is modeled with cloud model in the evaluation approach, and expected value and hyper-entropy of the subjective cloud is used to evaluate the reputation of trust objects. Our experimental data shows that the method can effectively support subjective trust decisions and provide a helpful exploitation for subjective trust evaluation.","Clouds,
Decision making,
Uncertainty,
Internet,
Software engineering,
Fuzzy set theory,
IP networks,
Electronic commerce,
Authorization,
Computer science"
The Service Adaptation Machine,"The reuse of software services often requires the introduction of adapters. In the case of coarse-grained services, and especially services that engage in long-running conversations, these adapters must deal not only with mismatches at the level of individual interactions, but also across interdependent interactions. Existingtechniques support the synthesis of adapters at design-time by comparing pairs of service interfaces. However, these techniques only work under certain restrictions. This paper explores a runtime approach to service interface adaptation. The paper proposes an adaptation machine that sits between pairs of services and manipulates the exchanged messages according to a repository of mapping rules. The paper formulates an operational semantics for the adaptation machine, including algorithms to compute rule firing sequences and criteria for detecting deadlocks and information loss. The adaptation machine has been implemented as a prototype andtested on common business processes.","Runtime,
System recovery,
Prototypes,
Web services,
Testing,
Service oriented architecture,
Protocols,
Computer science,
Australia,
Software prototyping"
Isolated malay speech recognition using Hidden Markov Models,"The study aims to develop an automated isolated word speech recognition for Malay language that relies heavily on the well known and widely used statistical method in characterizing the speech pattern, the Hidden Markov Model (HMM). This paper discusses the development and implementation of an isolated Malay word speech recognition system using HMM as the acoustic model. This research focuses on isolated 5 phonemes word structure such as empat (four), lapan (eight), rekod (record), tidak (no), tujuh (seven) and tutup (close). The proposed system is relatively successful where it can identify spoken word at 88% recognition rate which is an acceptable rate of accuracy for speech recognition.","Speech recognition,
Hidden Markov models,
Feature extraction,
Natural languages,
Automatic speech recognition,
Mel frequency cepstral coefficient,
Viterbi algorithm,
Data mining,
Software engineering,
Computer science"
Functional safety and system security in automation systems - a life cycle model,"Industrial and building automation systems are more and more important in industry and buildings. New services and novel fields of application call for dependable systems. Two very important properties of such a system are functional safety and system security. In the opposite of today’s development where safety and security are treated separately, investigating security together with safety leads to a reduction of effort in the different phases of system life. That is because they have some similar objectives, but realized by different measures. The intention of the paper is to present a way of developing a safe and secure system as well as to show the associated benefit with special focus on building automation.","Safety,
Security,
Automation,
Maintenance engineering,
Authorization,
Authentication,
Buildings"
Online and markerless motion retargeting with kinematic constraints,"Transferring motion from a human demonstrator to a humanoid robot is an important step toward developing robots that are easily programmable and that can replicate or learn from observed human motion. The so called motion retargeting problem has been well studied and several off-line solutions exist based on optimization approaches that rely on pre-recorded human motion data collected from a marker-based motion capture system. From the perspective of human robot interaction, there is a growing interest in online and marker-less motion transfer. Such requirements have placed stringent demands on retargeting algorithms and limited the potential use of off-line and pre-recorded methods. To address these limitations, we present an online task space control theoretic retargeting formulation to generate robot joint motions that adhere to the robotpsilas joint limit constraints, self-collision constraints, and balance constraints. The inputs to the proposed method include low dimensional normalized human motion descriptors, detected and tracked using a vision based feature detection and tracking algorithm. The proposed vision algorithm does not rely on markers placed on anatomical landmarks, nor does it require special instrumentation or calibration. The current implementation requires a depth image sequence, which is collected from a single time of flight imaging device. We present online experimental results of the entire pipeline on the Honda humanoid robot - ASIMO.","Joints,
Robots,
Feature extraction,
Humans,
Aerospace electronics,
Collision avoidance,
Jacobian matrices"
Manifold Alignment via Local Tangent Space Alignment,"Manifold alignment is about mapping several datasets into a global space, and is of great importance in learning the shared latent structure, data fusion and multicue data matching. In this paper, we propose an algorithm to solve this problem via Local Tangent Space Alignment (LTSA). LTSA is used here as a method to find the inner manifold constraint of each dataset. A cost function to measure the quality of alignment is given by combining the inner manifold constraints of each dataset and the matching points constraints among different datasets. The effectiveness of our algorithm is validated by applying it to the problem of image sequences alignment.","Computer science,
Space technology,
Cost function,
Pixel,
Software engineering,
Manifolds,
Automation,
Image sequences,
Pattern matching,
Pattern recognition"
Kinematics and tip-over stability analysis for a mobile humanoid robot moving on a slope,"When a mobile robot performs some special tasks, such as carrying a heavy load, moving on a slope or rough terrain, particularly contacting with the environment, it may become unstable even overturn. First, the paper presents a mobile humanoid robot whose upper human-like body is mounted on the mobile platform supported by three wheels (two driving wheels and one caster wheel) and subjected to nonholonomic kinematic constraints. Then the kinematics of the mobile platform and the upper human-like body are studied and tip-over stability of the motion on a slope is analyzed. Following that, two cases are studied with different postures of the upper body and two arms in accordance with the motion on a slope. Computer simulations are carried out and the influences of different robotic postures on the tip-over stability are analyzed through simulation results.",
Specification and Verification of Soft Error Performance in Reliable Internet Core Routers,This paper presents a methodology for developing a specification for soft error performance of an integrated hardware/software system that must achieve highly reliable operation. The methodology enables tradeoffs between reliability and cost to be made during the early silicon design and SW architecture phase. An accelerated measurement technique using neutron beam irradiation is also described that ties the final system performance to the reliability model and specification. The methodology is illustrated for the design of a line card for an internet core router.,
An Efficient Hardware-Based Multi-hash Scheme for High Speed IP Lookup,"The increasingly more stringent performance and power requirements of Internet routers call for scalable IP lookup strategies that go beyond the currently viable TCAM- and trie-based solutions. This paper describes a new hash-based IP lookup scheme that is both storage efficient and high performance. In order to achieve high storage efficiency, we take a multi-hashing approach and employ an advanced hashing technique that effectively resolves hashing collisions by dynamically migrating IP prefixes that are already in the lookup table as new prefixes are inserted. To obtain high lookup throughput, the multiple hash tables are accessed in parallel (using different hash functions) or in a pipelined manner. We evaluate the proposed scheme using up-to-date core routing tables and discuss how its key design parameters can be determined. When compared with state-of-the-art TCAM designs, our scheme reduces area and power requirements by 60% and 80% respectively, while achieving competitive lookup rates. We expect that the proposed scheme scales well with the anticipated routing table sizes and technologies in the future.","IP networks,
Indexes,
Routing,
Entropy,
Throughput,
Hardware,
Titanium"
Neural network output feedback control of a quadrotor UAV,"A neural network (NN) based output feedback controller for a quadrotor unmanned aerial vehicle (UAV) is proposed. The NNs are utilized in the observer and for generating virtual and actual control inputs, respectively, where the NNs learn the nonlinear dynamics of the UAV online including uncertain nonlinear terms like aerodynamic friction and blade flapping. It is shown using Lyapunov theory that the position, orientation, and velocity tracking errors, the virtual control and observer estimation errors, and the NN weight estimation errors for each NN are all semi-globally uniformly ultimately bounded (SGUUB) in the presence of bounded disturbances and NN functional reconstruction errors while simultaneously relaxing the separation principle.","Neural networks,
Output feedback,
Unmanned aerial vehicles,
Aerodynamics,
Error correction,
Estimation error,
Vehicle dynamics,
Friction,
Blades,
Velocity control"
Generation of Service Wrapper Protocols from Choreography Specifications,"Choreography description languages specify interactions among a set of services from a global point of view. From this description, it is possible to generate either an orchestrator (centralized interactions), or a set of peers or wrappers (distributed interactions). In this paper, we present first a model of service protocols with value passing, and an abstract choreography language to describe their composition and adaptation. Adaptation is useful while composing services to correct existing mismatches which might exist between their interfaces. Given abstract descriptions of services and their choreography, we propose techniques based on encodings into process algebra to generate an orchestrator and a set of wrapper protocols. Generation of wrappers is particularly tackled in this paper because this enables the system deployment in the context of distributed systems, and keeps at the same time a full parallelism of the system execution. Our approach is completely automated by a prototype tool we implemented.",
Scan Architecture With Align-Encode,"Scan architectures that provide compression capabilities have become mandatory due to the unbearable test costs imposed by high test data volume and prolonged test application. To alleviate these test costs, a stimulus decompressor and a response compactor block are inserted between the tester channels and the scan chains. As a result, a few tester channels drive a larger number of scan chains. In such an architecture, whether a particular test pattern can be delivered depends on the care bit distribution of that pattern. In this paper, we introduce a hardware block to be utilized in conjunction with a combinational stimulus decompressor block. This block, namely, Align-Encode, provides a deterministic per pattern control over care bit distribution of test vectors, improving pattern deliverability, and thus, the effectiveness of the particular stimulus decompressor. Align-Encode is reconfigured on a per pattern basis to delay the shift-in operations in selected scan chains. The number of cycles that a chain may be delayed can be between zero and the maximum allowable value, in order to align the scan slices in such a way that originally undeliverable test vectors become encodable. The reconfigurability of Align-Encode provides a test pattern independent solution, wherein any given set of test vectors can be analyzed to compute the proper delay information. We present efficient techniques for computing the scan chain delay values that lead to pattern encodability. Experimental results also justify the test pattern encodability enhancements that Align-Encode delivers, enabling significant test quality improvements and/or test cost reductions.","Circuit testing,
Costs,
Delay,
Test data compression,
Hardware,
Information analysis,
Pattern analysis,
Compaction,
Mathematics,
Computer science"
A probabilistic representation of LiDAR range data for efficient 3D object detection,"We present a novel approach to 3D object detection in scenes scanned by LiDAR sensors, based on a probabilistic representation of free, occupied, and hidden space that extends the concept of occupancy grids from robot mapping algorithms. This scene representation naturally handles LiDAR sampling issues, can be used to fuse multiple LiDAR data sets, and captures the inherent uncertainty of the data due to occlusions and clutter. Using this model, we formulate a hypothesis testing methodology to determine the probability that given 3D objects are present in the scene. By propagating uncertainty in the original sample points, we are able to measure confidence in the detection results in a principled way. We demonstrate the approach in examples of detecting objects that are partially occluded by scene clutter such as camouflage netting.","Laser radar,
Object detection,
Layout,
Uncertainty,
Sampling methods,
Optical sensors,
Computer science,
Robot sensing systems,
Orbital robotics,
Testing"
Recognizing hand gestures using dynamic Bayesian network,"In this paper, we describe a dynamic Bayesian network or DBN based approach to both two-hand gestures and one-hand gestures. Unlike wired glove-based approaches, the success of camera-based methods depends greatly on image processing and feature extraction results. So the proposed method of DBN-based inference is preceded by fail-safe steps of motion tracking. Then a new gesture recognition model for a set of both one-hand and two-hand gestures is proposed based on the dynamic Bayesian network framework which makes it easy to represent the relationship among features and incorporate new information to the model. In an experiment with ten isolated gestures, we obtained a recognition rate upwards of 99.59% with cross validation. The proposed model is believed to have a strong potential for successful applications to other related problems such as sign languages.","Bayesian methods,
Hidden Markov models,
Nonlinear optics,
Image motion analysis,
Speech recognition,
Target tracking,
Silicon compounds,
Computer science,
Computer networks,
Image processing"
Wearable augmented reality system using gaze interaction,"Undisturbed interaction is essential to provide immersive AR environments. There have been a lot of approaches to interact with VEs (virtual environments) so far, especially in hand metaphor. When the user’s hands are being used for hand-based work such as maintenance and repair, necessity of alternative interaction technique has arisen. In recent research, hands-free gaze information is adopted to AR to perform original actions in concurrence with interaction. [3, 4]. There has been little progress on that research, still at a pilot study in a laboratory setting. In this paper, we introduce such a simple WARS(wearable augmented reality system) equipped with an HMD, scene camera, eye tracker. We propose ‘Aging’ technique improving traditional dwell-time selection, demonstrate AR gallery — dynamic exhibition space with wearable system.","Aging,
Augmented reality,
Maintenance engineering,
Virtual environment,
Integrated optics,
Cameras,
Art"
Topology characterization of high density airspace aeronautical ad hoc networks,"Aeronautical ad hoc networks represent a special type of ad hoc wireless networks, given their significantly larger scale and the distinct characteristics of their mobile nodes. Aircraft populate the international airspace very heterogeneously. Some regions experience highly dense air traffic, with aircraft headings being largely uncorrelated. Other regions remain only very sparsely populated, with aircraft typically flying parallel to each other. Moreover, the number of airborne aircraft in a given region changes significantly throughout the day. In this paper, we focus on the densely populated European airspace, and investigate the topological behavior of multihop ad hoc wireless networks formed by air-air links of varying communications range. We derive analytical expressions for various topological aspects, such as the lifetime of inter-aircraft links, and the projected hop length using greedy forwarding. These results are in good agreement with the behavior observed in our simulations of the European air traffic scenario. In addition, we assess the performance of greedy forwarding in the aero-nautical environment and show that, under moderate connectivity, this technique delivers almost all packets to their destinations with a minimum hop count.",
A review of feature selection techniques via gene expression profiles,"The invention of DNA microarray technology has modernized the approach of biology research in such a way that scientists can now measure the expression levels of thousands of genes simultaneously in a single experiment. Although this technology has shifted a new era in molecular classification, interpreting microarray data still remain a challenging issue due to their innate nature of “high dimensional low sample size”. Therefore, robust and accurate feature selection methods are required to identify differentially expressed genes across varied samples for example between cancerous and normal cells. Successful of feature selection techniques will assist to correctly classify different cancer types and consequently led to a better understanding of genetic signatures in cancers and would improve treatment strategies. This paper presents a review of feature selection techniques that have been employed in microarray data analysis. Moreover, other problems associated with microarray data analysis also addressed. In addition, several trends were noted including highly reliance on filter techniques compared to wrapper and embedded, a growing direction towards ensemble feature selection techniques and future extension to apply feature selection in combination of heterogeneous data sources.",
Multivariable integral control of resonant structures,"Integral resonant control (IRC) is a feedback control technique used for damping active structures with a collocated sensor/actuator pair. This paper extends this control technique to structures having several collocated sensor/ actuator pairs. Conditions for the closed loop stability are derived, and the set of such stabilizing IRC controllers is shown to be a convex set. An experimental implementation of an IRC controller on an active structure (Cantilever beam) with two pairs of bonded collocated piezoelectric sensors/actuators is also presented.","Resonance,
Actuators,
Stability,
Flexible structures,
Velocity control,
Frequency,
Control systems,
MIMO,
Damping,
Vibrations"
Multiclass voluntary facial expression classification based on Filter Bank Common Spatial Pattern,"This paper investigates the classification of voluntary facial expressions from electroencephalogram (EEG) and electromyogram (EMG) signals using the Filter Bank Common Spatial Pattern (FBCSP) algorithm. The FBCSP algorithm is an autonomous and effective machine learning approach for classifying two classes of EEG measurements in motor imagery-based Brain Computer Interface (BCI). However, the problem of facial expression recognition typically involves more than just two classes of measurements. Hence, this paper proposes an extension of FBCSP to the multiclass paradigm using a decision threshold-based classifier for classifying facial expressions from EEG and EMG measurements. A study is conducted using the proposed Multiclass FBCSP on 4 subjects who performed 6 different facial expressions. The results show that the Multiclass FBCSP is effective in classifying multiple facial expressions from the EEG and EMG measurements.",
Automatic Compilation of Data-Driven Circuits,"This paper describes a method of synthesising asynchronous circuits based on the Handshake Circuit paradigm but employing a data-driven, rather than the control-driven, style. This approach attempts to combine the performance advantages of data-driven asynchronous design styles with the handshake circuit style of construction. The integration into the existing Balsa design flow of a compiler for descriptions written in a new datadriven language is described. The method is demonstrated using a significant design example — a 32 bit microprocessor. This example shows that the datadriven circuit style provides better performance than conventional control-driven Balsa circuits.","Circuit synthesis,
Asynchronous circuits,
Flexible printed circuits,
Computer science,
Control system synthesis,
Automatic control,
Microprocessors,
Communication system control,
Protocols,
Large-scale systems"
Designing Fault Tolerant Web Services Using BPEL,"The web services technology provides an approach for developing distributed applications by using simple and well defined interfaces. Due to the flexibility of this architecture, it is possible to compose business processes integrating services from different domains. This paper presents an approach, which uses the specification of services orchestration, in order to create a fault tolerant model combining active and passive replication technique. This model support fault of crash. The characteristics and the results obtained by implementing this model are described along this paper.","Fault tolerance,
Web services,
Engines,
XML,
Logic,
Application software,
Web and internet services,
Transport protocols,
Companies,
Computer interfaces"
Please Permit Me: Stateless Delegated Authorization in Mashups,"Mashups have emerged as a Web 2.0 phenomenon, connecting disjoint applications together to provide unified services. However, scalable access control for mashups is difficult. To enable a mashup to gather data from legacy applications and services, users must give the mashup their login names and passwords for those services. This all-or-nothing approach violates the principle of least privilege and leaves users vulnerable to misuse of their credentials by malicious mashups. In this paper, we introduce delegation permits—a stateless approach to access rights delegation in mashups—and describe our complete implementation of a permit-based authorization delegation service. Our protocol and implementation enable fine grained, flexible, and stateless access control and authorization for distributed delegated authorization in mashups, while minimizing attackers' ability to capture and exploit users' authentication credentials.","Authorization,
Mashups,
Access protocols,
Authentication,
Application software,
Access control,
Permission,
Calendars,
Computer security,
Computer science"
Latent semantic retrieval of spoken documents over position specific posterior lattices,"This paper presents a new approach of latent semantic retrieval of spoken documents over Position Specific Posterior Lattices(PSPL). This approach performs concept matching instead of literal term matching during retrieval based on the Probabilistic Latent Semantic Analysis (PLSA), so as to solve the problem of term mismatch between the query and the desired spoken documents. This approach is performed over PSPL to consider the multiple hypotheses generated by ASR process, as well as the position information for these hypotheses, so as to alleviate the problem of relatively poor ASR accuracy. We establish a framework to evaluate semantic relevance between terms and the relevance score between a query and a PSPL, both based on the latent topic information from PLSA. Preliminary experiments on Chinese broadcast news segments showed significant improvements can be obtained with the proposed approach.","Lattices,
Information retrieval,
Automatic speech recognition,
Information analysis,
Multimedia communication,
Material storage,
Content based retrieval,
Computer science,
Performance analysis,
IP networks"
Bandwidth allocation games under budget and access constraints,"We study bandwidth allocation at base stations of a wireless network as a non-cooperative game between different wireless users. A user demands bandwidth from the base stations it has access to by submitting bids. Once the bidding process is complete, a base station distributes its bandwidth to the users in proportion to their bids. Users are assumed to be budget constrained and price anticipating, and split their wealth across base stations so as to maximize their individual bandwidths. In this paper, we study the properties of Nash Equilibrium (NE) for this bandwidth allocation game. For the special case where each user can access all base stations, we argue that there exists a unique NE, at which the bandwidth obtained by any user is proportional to its wealth. For a more general scenario where a user may be able to access only a subset of all base stations, we compare the NE of the game to the max-min fair bandwidth allocation. We show that although the NE may not be max-min fair in general, the bandwidth allocation at NE becomes arbitrarily close to max-min fair allocation as the number of users is increased, while keeping the number of base stations fixed.","Channel allocation,
Base stations,
Bandwidth,
Resource management,
Nash equilibrium,
Femtocell networks,
Systems engineering and theory,
Wireless networks,
Fading,
Cellular networks"
Scalable AOI-Cast for Peer-to-Peer Networked Virtual Environments,"Networked virtual environments (NVEs) are computer-generated virtual worlds where users interact by exchanging messages via network connections. Each NVE user often pays attention to only a limited visibility sphere called area of interest (AOI) where interactions occur. The dissemination of messages to other users within the AOI (i.e., the AOI neighbors) thus is a fundamental NVE operation referred to as AOI-cast. Existing studies on NVE scalability have focused on system scalability, or the ability for the system to handle a growing number of total users, by using multicast or peer-to-peer (P2P) architectures. However, another overlooked, yet important form of scalability relates to the handling of a growing number of users within the AOI (or AOI scalability). In this paper, we propose two AOI-cast schemes, called VoroCast and FiboCast, to improve the AOI scalability of P2P-based NVEs. VoroCast constructs a spanning tree across all AOI neighbors based on Voronoi diagrams, while FiboCast dynamically adjusts the messaging range by a Fibonacci sequence, so that AOI neighbors would receive updates at frequencies based on their hop counts from the message originator. Simulations show that the two schemes provide better AOI scalability than existing approaches.","virtual reality,
computational geometry,
multicast communication,
peer-to-peer computing,
sequences,
trees (mathematics)"
Facial expression recognition based on fusion of Gabor and LBP features,"A new person-independent facial expression recognition algorithm based on fusion of Gabor and LBP Features with OLPP is presented. Aiming at the deficiency of common Gabor feature extraction method, a new Gabor feature extraction method is proposed. Each Gabor wavelet representation of an image is divided into small subblocks, then the mean value and standard deviation in each subblock are calculated, and the statistics of all Gabor wavelet representations are connected as the feature vector. Taking into account the outstanding performance of LBP to extract local texture, we combine local statistic features of Gabor wavelets with the LBP features as feature vector. After using OLPP to reduce the feature dimension of fusion features, the facial expression image is classified by nearest neighbor method. Experimental results on two databases indicate the proposed method has higher recognition rate compared with single feature method and other methods.","Feature extraction,
Databases,
Face recognition,
Face,
Histograms,
Kernel,
Convolution"
Reliability of wireless on-chip interconnects based on carbon nanotube antennas,"Design technologies for integrated systems beyond the current CMOS era will present unprecedented advantages such as very high device densities and challenges such as soaring power dissipation issues. Most of the research effort in the emerging area of nanoelectronics has revolved around creating novel devices to replace the traditional CMOS transistor. The development of higher-level communication architectures necessary for integrating such devices into high performance systems has not received the same level of attention so far. With the current trend of CMOS scaling, traditional planar metal-based on-chip interconnect schemes are projected to be the principal bottleneck in meeting the performance needs and specifications of Systems on Chip (SoCs). Three-dimensional integration and on-chip optical and RF communication links have been envisioned as promising alternatives. In this paper we explore the possibility of having an on-chip wireless communication infrastructure using carbon nanotube antennas operating in optical frequencies, and the effect of variations in nanotube properties on the communication behavior.","System-on-a-chip,
Receivers,
Wireless communication,
Optical transmitters,
Receiving antennas,
Antennas,
Transmitting antennas"
A Robust QRS Complex Detection Algorithm Using Dynamic Thresholds,"Automatic QRS Complex detection is important in ECG signal analysis. QRS detection methods are affected by the quality of the ECG recordings and the abnormalities in the ECG signals. In this paper, a generic algorithm is introduced to improve the detection of QRS complexes in Arrhythmia ECG Signals that suffer from: 1) non-stationary effects, 2) low signal-to-noise ratio, 3) negative QRS polarities, 4) low QRS amplitudes, and 5) ventricular ectopics. We compared the algorithm to the method described by Chouhan et al. [16] by applying both algorithms to 19 records of the MIT-BIH database. It was shown that the new algorithm achieves significantly better detection rates resulting in an overall 97.5% sensitivity and 99.9% positive predictivity.","Databases,
Electrocardiography,
Signal to noise ratio,
Heuristic algorithms,
Prediction algorithms,
Algorithm design and analysis,
Sensitivity"
Pseudo virtual passive dynamic walking and effect of upper body as counterweight,"This paper investigates the effect of an upper body on efficient dynamic bipedal walking utilizing its natural dynamics. We introduce an upper body as a one-link torso and add it to a simple biped model by means of a bisecting hip mechanism (BHM). We first mathematically analyze the effect of the upper body with the BHM as a counterweight and discuss how it affects natural swinging motion of the swing leg. Second, we propose a simple method for generating efficient dynamic bipedal gait imitating the property of virtual passive dynamic walking, and numerically analyze the gait efficiency. Simulation results show that the walking system exhibits period-doubling bifurcation, and we discuss how the efficiency changes in the multiple-period gait.","Legged locomotion,
Robots,
Mechanical energy,
Leg,
Hip,
Torso,
Joints"
On Modeling the Lifetime Reliability of Homogeneous Manycore Systems,"Advancements in technology enable integration of a large number of cores on a single silicon die. At the same time, aggressive technology scaling has an ever-increasing adverse impact on the lifetime reliability of such large integrated circuits. In this work, we model the lifetime reliability of homogeneous manycore systems using a load-sharing nonrepairable k-out-of-n:G system with general failure distributions for embedded cores. In manycore systems, an embedded core can be in operational, cold standby, or warm standby state depending on system redundancy schemes and their workloads. We then use the proposed model to analyze the impact of different redundant schemes and configurations on the lifetime reliability of manycore systems.","Integrated circuit technology,
Silicon,
Integrated circuit reliability,
Failure analysis,
Temperature dependence,
Circuit faults,
CMOS technology,
Laboratories,
Computer science,
Reliability engineering"
On the optimality of the proper orthogonal decomposition and balanced truncation,"The proper orthogonal decomposition (POD), also known as Karhunen-Loève decomposition or principal component analysis, and balanced truncation, are shown to be optimal in the sense of distance minimizations in spaces of Hilbert-Schmidt or trace-class 2 integral operators. Both POD and balanced truncation are shown to be optimal approximations by finite rank operators in the Hilbert-Schmidt norm. Optimality of balanced truncation seems to have been overlooked in the literature, and in fact, it is commonly thought to be non-optimal in any sense. The role of POD and balanced truncation in minimizing different n-widths of specific compact operators is discussed. The n-widths quantify inherent and representation errors due to lack of data or inaccurate measurements and loss of information.","Hilbert space,
Reduced order systems,
Loss measurement,
Optimal control,
Distributed parameter systems,
Linear systems,
Constraint optimization,
Energy capture,
Computer errors,
Information processing"
Lumen detection for capsule endoscopy,"In this paper, two visual cues are proposed, to be exploited for the navigation of active endoscopic capsules within the gastrointestinal (GI) tract. These cues consist of the detection and tracking of the lumen and of an illumination highlight in capsule endoscopy (CE) images. The proposed approach aims at developing vision algorithms which are robust with respect to the challenging imaging conditions encountered in the GI tract and the great variability of the acquired images. Cases where no or more than one lumens exists, are also detected. The proposed approach extends the state-of-the-art in lumen detection, and is demonstrated for in-vivo video sequences acquired from endoscopic capsules.","Pixel,
Kernel,
Navigation,
Endoscopes,
Colon,
Iris,
Image color analysis"
Assigning Sensors to Competing Missions,"When a sensor network is deployed in the field, it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this paper, we propose centralized and distributed schemes to assign sensors to missions. We also adapt our distributed scheme to make it energy-aware to extend network lifetime. Finally, we show simulation results comparing these solutions. We find that our greedy algorithm frequently performs near-optimally and that the distributed schemes usually perform nearly as well.","Intelligent sensors,
Computer science,
Greedy algorithms,
Temperature sensors,
Wireless sensor networks,
Proposals,
Target tracking,
Energy resolution,
Temperature distribution,
Anisotropic magnetoresistance"
On the Impossibility of Basing Identity Based Encryption on Trapdoor Permutations,"We ask whether an Identity Based Encryption (IBE) system can be built from simpler public-key primitives. We show that there is no black-box construction of IBE from Trapdoor Permutations (TDP) or even from Chosen Ciphertext Secure Public Key Encryption (CCA-PKE). These black-box separation results are based on an essential property of IBE, namely that an IBE system is able to compress exponentially many public-keys into a short public parameters string.","Identity-based encryption,
Public key,
National security,
Public key cryptography,
Power system security,
Power system modeling,
Computer science,
Lattices,
Terrorism,
Polynomials"
The Behavior Analysis of Flash-Memory Storage Systems,"Performance and reliability are two major design concerns of flash-memory storage systems, especially for low-cost products. Although various excellent flashmemory management schemes are proposed, there is little work done on how to evaluate the designs or implementations of flash-memory storage systems. Many of the existing evaluation workloads for flash-memory storage systems still rely on those based on hard disks. This work aims at the needs of behavior analysis of flash-memory storage systems and their evaluations. In particular, a set of evaluation metrics and their corresponding access patterns are proposed. The behaviors of flash memory are also analyzed in terms of performance and reliability issues.",
Gathering Precise Patient Medical History with an Ontology-Driven Adaptive Questionnaire,"A thorough documentation of a patient's medical history is widely recognised as providing good indicators of potential intraoperative and postoperative complications. As~preoperative assessment can be time consuming, computer-based Information Collection Systems (ICS) can help free up precious and limited resources, leaving clinicians with more time to fulfil their primary mission of administrating medical care. In addition, medical histories collected by ICSs have proved to be more accurate than traditional pen-and-paper questionnaires or face-to-face interviews. A~challenge remains however in designing questionnaires which are general enough to suit a majority of patients, while at the same time, being able to capture critical individual information. In this paper, we propose a solution to this dilemma with a context-sensitive adaptive information collection system. The proposed method permits to iteratively capture finer-grained information with each successive step, should this information be relevant according to a questionnaire ontology. We argue that the method is robust, scalable and highly configurable. It results in questionnaires which are coherent and well structured and are able to capture enhanced patients' medical histories.","History,
Ontologies,
Adaptive systems,
Robustness,
Search engines,
Computer science,
Computational Intelligence Society,
Biomedical informatics,
Documentation,
Surgery"
Automatic Service Composition Using AND/OR Graph,"As SOC and Web service technology become more widely used, large amounts of services need to be efficiently and effectively composed to meet complex businesses. In this paper, we proposed an approach to resolve the composition problem over large-scale services. We used an inverted table as index for a quick service discovery, and applied a Service Dependency Graph (SDG) and an AND/OR graph as the algorithm basis for parallel compostion. Considering the semantic information described in Web service, our approach also recognizes and transmits the semantic relationships described in Web Ontology Language (OWL).","Web services,
OWL,
Large-scale systems,
Computer science,
Software design,
Web and internet services,
Ontologies"
Register File Power Reduction Using Bypass Sensitive Compiler,"This paper explores, develops, and investigates several bypass-sensitive compilation techniques to reduce the register file power by reducing the access frequency to the register file. We study the effectiveness of our techniques on the Intel XScale processor, which is based on the previously proposed ldquoon-demand register fetch readrdquo architectural feature. Furthermore, we show that our bypass-sensitive compilation technique is effective on various partial bypass configurations.","Registers,
Energy consumption,
Computer science,
Temperature,
Microwave integrated circuits,
Information technology,
Frequency,
Microprocessors,
Resumes,
Research and development"
The Applications and Simulation of Adaptive Filter in Noise Canceling,"In practical application, the statistical characteristics of signal and noise are usually unknown or can't have been learned so that we hardly design fix coefficient digital filter. In allusion to this problem, the theory of the adaptive filter and adaptive noise cancellation are researched deeply in the thesis. According to the LMS and the RLS algorithms realize the design and simulation of adaptive algorithms in noise canceling, and compare and analyze the result then prove the advantage and disadvantage of two algorithms .We simulates the adaptive filter with MATLAB, the results prove its performance is better than the use of a fixed filter designed by conventional methods.",
"Multiplicative kernels: Object detection, segmentation and pose estimation","Object detection is challenging when the object class exhibits large within-class variations. In this work, we show that foreground-background classification (detection) and within-class classification of the foreground class (pose estimation) can be jointly learned in a multiplicative form of two kernel functions. One kernel measures similarity for foreground-background classification. The other kernel accounts for latent factors that control within-class variation and implicitly enables feature sharing among foreground training samples. Detector training can be accomplished via standard SVM learning. The resulting detectors are tuned to specific variations in the foreground class. They also serve to evaluate hypotheses of the foreground state. When the foreground parameters are provided in training, the detectors can also produce parameter estimate. When the foreground object masks are provided in training, the detectors can also produce object segmentation. The advantages of our method over past methods are demonstrated on data sets of human hands and vehicles.","Kernel,
Object detection,
Detectors,
Humans,
Parameter estimation,
Support vector machines,
Biological system modeling,
Image recognition,
Vehicle detection,
Training data"
Two Query PCP with Sub-Constant Error,"We show that the NP-Complete language 3Sat has a PCPverifier that makes two queries to a proof of almost-linear size and achieves sub-constant probability of error o(1). The verifier performs only projection tests, meaning that the answer to the first query determines at most one accepting answer to the second query.Previously, by the parallel repetition theorem, there were PCP Theorems with two-query projection tests, but only (arbitrarily small) constant error and polynomial size.There were also PCP Theorems with sub-constant error andalmost-linear size, but a constant number of queries that is larger than 2.As a corollary, we obtain a host of new results. In particular, our theorem improves many of the hardness of approximation results that are proved using the parallel repetition theorem. A partial list includes the following:(1) 3Sat cannot be efficiently approximated to withina factor of 7/8+o(1), unless P = NP. This holds even under almost-linear reductions. Previously, the best knownNP-hardness factor was 7/8+epsilon for any constant epsilon≫0, under polynomial reductions.(2) 3Lin cannot be efficiently approximated to withina factor of 1/2+o(1), unless P = NP. This holdseven under almost-linear reductions. Previously, the best known NP-hardness factor was 1/2+epsilon for any constant epsilon≫0, under polynomial reductions.(3) A PCP Theorem with amortized query complexity 1 + o(1)and amortized free bit complexity o(1). Previously, the best known amortized query complexity and free bit complexity were 1+epsilon and epsilon, respectively, for any constant epsilon ≫ 0.One of the new ideas that we use is a new technique for doing the composition step in the (classical) proof of the PCP Theorem, without increasing the number of queries to the proof. We formalize this as a composition of new objects that we call Locally Decode/Reject Codes (LDRC). The notion of LDRC was implicit in several previous works, and we make it explicit in this work. We believe that the formulation of LDRCs and their construction are of independent interest.","Polynomials,
Computer science,
Computer errors,
Testing,
Radio access networks,
Mathematics,
Performance evaluation,
Decoding,
Upper bound"
1/f Noise Reduction and Image Enhancement on CMOS Image Sensors by Autocorrelation Based on Adaptive Algorithm,"In CMOS image sensor, 1/f noise is determined by width and length of gate at the circuit. Reducing the 1/f noise, a correlative multi sampling and an autocorrelation method have been used, mainly. Nevertheless, a correlative multi sampling has a practical problem of limitation which is the band of applied frequency. Because of the problem, an autocorrelation method is used for the reducing of 1/f noise better than correlative multi sampling. General noise reduction filters along with the smoothing effect. In this study, the autocorrelation method based on adaptive algorithm is proposed to reduce the smoothing effect at the edge of image.","Noise reduction,
Image enhancement,
CMOS image sensors,
Autocorrelation,
Adaptive algorithm,
Image sampling,
Circuit noise,
Smoothing methods,
Frequency,
Filters"
The modifying generalized moving least squares approximation meshless method and application on plane piezoelectric problem,"Meshless method is a new numerical method on problem for determining solution of differential equation. The moving least squares approximation makes only require least squares approximation with regard to functional value at all nodes. It makes no require for the residual of derivative approximation. However, the generalized moving least squares approximation makes require least squares approximation with regard to functional and its derivative value at all nodes. For the sake of decrease the computing time, modifying generalized moving least squares approximation is constructed under adding the residual of high orders derivative only at the portion nodes. The modifying generalized moving least squares approximation has high accuracy not only for function value but also one or higher orders derivative. This method is well applied to plane piezoelectric problem.","Least squares approximation,
Multilevel systems,
Application software,
Rail transportation,
Computer applications,
Differential equations"
An Operational Framework for Service Oriented Architecture Network Security,"This study proposes a new operational framework of a network administrator for service oriented architecture (SOA) network security. It seeks to characterize the current state of practices in SOA network security by gathering information regarding known threats and defenses for SOA deployments. It works towards the practical implementation of SOA designs by creating training and testing scenarios for those preparing to work in this area. Finally, it frames these and other SOA security efforts with respect to a classic theoretical model of information security. The resulting synthesis includes recommendations on how best to process the XML network traffic typical of SOA applications. The proposed approach is Filtering to Inspect XML (FIX) at the network's perimeter. This framework contributes to the understanding of secure SOA designs by clarifying the responsibilities of both network managers and software engineers in orchestrating XML-based services.","Service oriented architecture,
Information security,
XML,
Testing,
Semiconductor optical amplifiers,
Network synthesis,
Telecommunication traffic,
Traffic control,
Application software,
Filtering"
Butteries in the Mesh: Lightweight Localized Wireless Network Coding,"In this paper, BFLY-a lightweight localized network coding protocol for wireless mesh networks-is proposed. To supplement forwarding packets in classical networks, intermediate wireless nodes code packets from different sources, so that each transmission's information content is increased by a factor of more than one. Prior work allowed intermediate nodes to code (i.e, XOR) packets such that the recipient of that coded message must decode the message before forwarding. BFLY, however, allows intermediate recipients to, in addition to XOR-ing, forward coded packets; and thus further exploits network coding opportunities in multihop wireless networks. BFLY utilizes knowledge of the local topologies and source route information in the packet headers. We have developed network coding modules in ns-2 that facilitate simulation with large networks. Simulation studies show that BFLY can increase overall network throughput by a factor of 1.2 - 2 and reduce packet end-to-end latency. Finally, jointly coding with BFLY and COPE always yields more gain than the individual approaches.","Wireless mesh networks,
Network coding,
Wireless networks,
Spread spectrum communication,
Unicast,
Decoding,
Telecommunication traffic,
Broadcasting,
Computer science,
Wireless application protocol"
Configuration Management for Large-Scale Scientific Computing at the UK Met Office,"The UK Met Office's flexible configuration management (FCM) system uses existing open source tools, adapted for use with high-performance scientific Fortran code, to help manage evolving code in its large-scale climate simulation and weather forecasting models. FCM has simplified the development process, improved team coordination, and reduced release cycles.","Large-scale systems,
Scientific computing,
Computational modeling,
Predictive models,
Weather forecasting,
Productivity,
Software tools,
Programming profession,
Software debugging,
Software engineering"
Transformation of BPMN to YAWL,"Model transformations are frequently applied in business process modeling to bridge between languages on a different level of abstraction and formality. In this paper, we define a transformation between BPMN which is developed to enable business user to develop readily understandable graphical representations of business processes and YAWL, a formal workflow language that is able to capture all of the 20 workflow patterns reported.   We illustrate the transformation challenges and present a suitable transformation algorithm. The benefit of the transformation is threefold. Firstly, it clarifies the semantics of BPMN via a mapping to YAWL. Secondly, the deployment of BPMN business process models is simplified. Thirdly, BPMN models can be analyzed with YAWL verification tools.","Connectors,
Computer science,
Mathematics,
Software engineering,
Sun,
Mathematical model,
Bridges,
XML,
Unified modeling language,
Web services"
Method for optical-flow-based precision missile guidance,"A new precision guidance law is presented for three-dimensional intercepts against a moving target. In contrast to previously published guidance laws, it does not require knowledge of the range to the target. This makes it appropriate for use on platforms which have an imaging device, such as a video camera, as a primary sensor. We prove that with idealized dynamic model, the guidance law results in zero miss distance, and a formula is given for impact angle error which tends to zero as does target speed, making this method particularly suitable against slow moving targets. Computer simulations are used to test the law with a more realistic model, with a video camera and optical-flow algorithm providing target information. It is shown to perform well compared with another law from the literature, despite requiring less information.","Missiles,
Optical sensors,
Cameras,
Australia,
Navigation,
Iterative algorithms,
Optical imaging,
Image sensors,
Computer errors,
Computer simulation"
Enabling Automated Traceability Maintenance by Recognizing Development Activities Applied to Models,"For anything but the simplest of software systems, the ease and costs associated with change management can become critical to the success of a project. Establishing traceability initially can demand questionable effort, but sustaining this traceability as changes occur can be a neglected matter altogether. Without conscious effort, traceability relations become increasingly inaccurate and irrelevant as the artifacts they associate evolve. Based upon the observation that there are finite types of development activity that appear to impact traceability when software development proceeds through the construction and refinement of UML models, we have developed an approach to automate traceability maintenance in such contexts. Within this paper, we describe the technical details behind the recognition of these development activities, a task upon which our automated approach depends, and we discuss how we have validated this aspect of the work to date.",
Designing fuzzy rule-based classifiers that can visually explain their classification results to human users,"In various application areas of fuzzy rule-based systems, human users want to know why a particular reasoning result is obtained. That is, fuzzy rule-based systems are required to have high explanation ability. In this paper, we propose an approach to the design of fuzzy rule-based classifiers that can visually explain their classification results to human users. That is, our fuzzy rule-based classifiers can explain to human users why an input pattern is classified as a particular class in an understandable manner. The proposed approach consists of a rule selection method and a visualization interface. Our idea is to design fuzzy rule-based classifiers using fuzzy rules with only two antecedent conditions. A genetic algorithm is employed to construct a compact fuzzy rule-based classifier by choosing only a small number of fuzzy rules. In the classification phase, we use a single winner rule-based method for classifying an input pattern. The classification result of the input pattern is visually explained in a two-dimensional space where the two antecedent conditions of the winner rule are defined. Our approach is compared with feature selection by computational experiments.","Humans,
Fuzzy systems,
Knowledge based systems,
Fuzzy sets,
Fuzzy reasoning,
Visualization,
Genetic algorithms,
Design optimization,
Conferences,
Fuzzy neural networks"
Group learning using contrast NMF : Application to functional and structural MRI of schizophrenia,"Non-negative Matrix factorization (NMF) has increasingly been used as a tool in signal processing in the last couple of years. NMF, like independent component analysis (ICA) is useful for decomposing high dimensional data sets into a lower dimensional space. Here, we use NMF to learn the features of both structural and functional magnetic resonance imaging (sMRI/fMRI) data. NMF can be applied to perform group analysis of imaging data and we apply it to learn the spatial patterns which linearly covary among subjects for both sMRI and fMRI. We add an additional contrast term to NMF (called co-NMF) to identify features distinctive between two groups. We apply our approach to a dataset consisting of schizophrenia patients and healthy controls. The results from co-NMF make sense in light of expectations and are improved compared to the NMF results. Our method is general and may prove to be a useful tool for identifying differences between multiple groups.",
Flexible Address Configurations for Tree-Based ZigBee/IEEE 802.15.4 Wireless Networks,"A number of IEEE 802.15.4 devices can be connected by a tree topology as proposed by ZigBee specification. Address configuration in tree-based ZigBee networks needs to assign every device a network address that uniquely identifies it from others, and such addressing should also assist routing. The addressing method recommended by the specification forces a static assignment that is coupled with node's location in the tree, resulting in an inflexibility in allocating addresses. This property may significantly decrease the ratio of addressable devices and cause routing detour. To alleviate the problem, this paper considers three alternatives that manage address space with flexibility but require additional storage in ZigBee routers. Performance evaluations indicate that proposed approaches provide different levels of tradeoff between the ratio of addressable devices and storage costs in ZigBee routers.","ZigBee,
Wireless networks,
Routing,
Zirconium,
Network topology,
Costs,
Shape control,
Application software,
Computer science,
Communication standards"
Blindly separating mixtures of multiple layers with spatial shifts,"We address the problem of blindly separating mixtures of multiple layer images with unknown spatial shifts and mixing coefficients. Our proposed method can handle the over-determined, determined and under-determined cases where mixtures are more than, as many as and fewer than layers, respectively. The method is fast in over-determined and determined cases, with the same complexity as the fast Fourier transform (FFT), and can separate more layers from fewer mixtures in the under-determined case. It consists of two main steps. First, a novel sparse blind separation algorithm is applied, to estimate the spatial shifts, the mixing coefficients and the edge image of each layer. Second, all layers are reconstructed, by large scale linear programming in the under-determined case, or by least-squares solutions in other cases. The effectiveness of this technology is shown in the experiments on two simulated mixtures of four layers with spatial shifts, real mixture photos containing transparency and reflections, and real mixture images in a dissolve from a video.","Windows,
Layout,
Laboratories,
Space technology,
Fast Fourier transforms,
Image reconstruction,
Linear programming,
Optical reflection,
Intelligent systems,
Information science"
Computer aided diagnosis of fatty liver ultrasonic images based on support vector machine,"B-scan ultrasound is the primary means for the diagnosis of fatty liver. However, due to use of various ultrasound equipments, poor quality of ultrasonic images and physical differences of patients, fatty liver diagnosis is mainly qualitative, and often depends on the subjective judgment of technicians and doctors. Therefore, computer-aided feature extraction and quantitative analysis of liver B-scan ultrasonic images will help to improve clinical diagnostic accuracy, repeatability and efficiency, and could provide a measure for severity of hepatic steatosis. This paper proposed a novel method of fatty liver diagnosis based on liver B-mode ultrasonic images using support vector machine (SVM). Fatty liver diagnosis was transformed into a pattern recognition problem of liver ultrasound image features. According to the different characteristics of fatty liver and healthy liver, important image features were extracted and selected to distinguish between the two categories. These features could be represented by near-field light-spot density, near-far-field grayscale ratio, grayscale co-occurrence matrix, and neighborhood gray-tone difference matrix (NGTDM). A SVM classifier was modeled and trained using the clinical ultrasound images of both fatty liver and normal liver. It was then exploited to classify normal and fatty livers, achieving a high recognition rate. The diagnostic results are satisfactorily consistent with those made by doctors. This method could be used for computer-aided diagnosis of fatty liver, and help doctors identify the fatty liver ultrasonic images rapidly, objectively and accurately.","feature extraction,
fatty liver,
B-scan ultrasound,
grayscale co-occurrence matrix,
SVM"
Software Cost Estimation using Fuzzy Decision Trees,"This paper addresses the issue of software cost estimation through fuzzy decision trees, aiming at acquiring accurate and reliable effort estimates for project resource allocation and control. Two algorithms, namely CHAID and CART, are applied on empirical software cost data recorded in the ISBSG repository. Approximately 1000 project data records are selected for analysis and experimentation, with fuzzy decision trees instances being generated and evaluated based on prediction accuracy. The set of association rules extracted is used for providing mean effort value ranges. The experimental results suggest that the proposed approach may provide accurate cost predictions in terms of effort. In addition, there is strong evidence that the fuzzy transformation of cost drivers contribute to enhancing the estimation process.","Software,
Estimation,
Decision trees,
Classification algorithms,
Data mining,
Driver circuits,
Programming"
Insertable stereoscopic 3D surgical imaging device with pan and tilt,"In this paper, we present an insertable stereoscopic 3D imaging system for minimally invasive surgery. It has been designed and developed toward the goal of single port surgery. The device is fully inserted into the body cavity and affixed to the abdominal wall. It contains pan and tilt axes to move the camera under simple and intuitive joystick control. A polarization-based stereoscopic display is used to view the images in 3D. The camera’s mechanical design is based upon a single camera prototype we have previously built. We have run calibration tests on the camera and used it to track surgical tools in 3D in real-time. We have also used it in a number of live animal tests that included surgical procedures such as appendectomy, running the bowel, suturing, and nephrectomy. The experiments suggest that the device may be easier to use than a normal laparoscope since there is no special training needed for operators. The Pan/Tilt functions provide a large imaging volume that is not restricted by the fulcrum point of a standard laparoscope. Finally, the 3-D imaging system significantly improves the visualization and depth perception of the surgeon.","Cameras,
Minimally invasive surgery,
Testing,
Laparoscopes,
Abdomen,
Polarization,
Three dimensional displays,
Prototypes,
Calibration,
Animals"
Semantic device descriptions based on standard Semantic Web technologies,"Established device descriptions for automation devices lack a machine understandable semantics, thus inhibiting desired tasks like a semantic retrieval of devices, their automatic parameterization and interoperability analysis or the automated design of automation systems. To overcome these limitations, an approach for a semantic specification of automation devices is presented, which covers the hardware and particularly the applications of the devices. The semantic device descriptions are based on semantic Web technologies, especially OWL. Several application scenarios are shown, which demonstrate the potential of the introduced approach. Examples reach from a variable distribution of information over a semantic retrieval of devices up to performing intelligent reasoning based on logical inference.","semantic Web,
inference mechanisms,
information retrieval"
An analytical model describing the relationships between logic architecture and FPGA density,"This paper describes an analytical model, based principally on Rent’s Rule, that relates logic architectural parameters to the area efficiency of an FPGA. In particular, the model relates the lookup-table size, the cluster size, and the number of inputs per cluster to the amount of logic that can be packed into each lookup-table and cluster, and the number of used inputs per cluster. Comparison to experimental results show that our models are accurate. This accuracy combined with the simple form of the equations make them a powerful tool for FPGA architects to better understand and guide the development of future FPGA architectures.","Solid modeling,
Mathematical model,
Integrated circuit modeling,
Field programmable gate arrays,
Equations,
Table lookup,
Analytical models"
A new approach to the study of surface discharge on the oil-pressboard interface,"The materials used as the dielectric media of choice in large transformers continue to be a combination of pressboard and mineral oil. These materials offer a cost effective solution and are backed by a historical knowledge base of performance data and lifetime characteristics. A proven method to prevent inter-phase flashover is to use successive layers of pressboard and oil gaps. However, a number of failures in the field have been attributed to surface discharge or creeping discharge over the pressboard surface. This paper reports on an experiment to study surface discharge on pressboard in oil. A new technique for the generation of repeatable surface discharge is outlined. The paper explains the differences between surface tracking, and surface flash-over. The results indicate that the tracking is the result of surface discharge and that surface discharge and surface flash-over are two distinct phenomena.","Surface discharges,
Partial discharges,
Discharges,
Distance measurement,
Fault location,
Insulation life,
Degradation"
A LabVIEW based measure system for pulse wave transit time,"Pulse wave transit time (PWTT) is used as a non-invasive and cuffless method for blood pressure estimation. In this paper, we design a system that can measure PWTT by monitoring ECG and pulse wave continuously. The system includes analog signal sampling in PCB, signal display and data processing in computer. We measure pulse wave by the photo-plethysmograph (PPG) device in finger which includes an infrared LED transmitting light, photodiode in OPT101 as detector, amplifier and filters. We measure ECG by the sensor on limb. We design amplifier, 0.01Hz high pass filter, 75Hz low pass filter, and 50Hz notch filter. After filtering and amplification, ECG and PPG are sampled by MCU and then the data are transmitted to computer. LabVIEW is used to receive, display and process these data, and finally figure out the PWTT. Based on PWTT value, we coarsely calculate the SBP.","Pulse measurements,
Time measurement,
Low pass filters,
Pulse amplifiers,
Electrocardiography,
Computer displays,
Infrared detectors,
Blood pressure,
Biomedical monitoring,
Computerized monitoring"
Development of an extended reset controller and its experimental demonstration,"Reset control aims at enhanced performance that cannot be obtained by linear controllers. The conventional reset control is simple for implementation by resetting some of its controller states to zero when its input meets a threshold. However, it is found that in some cases the enhanced performance of conventional reset control is still limited such as with only partial reduction of the overshoot in a step reference response. Thus, the stability analysis and design of the reset control system are extended, where the reset time instances are prespecified and the controller states are reset to certain non-zero values, which are calculated online in terms of the system states for optimal performance. Experimental results on a piezoelectric positioning stage demonstrate that the extended reset control can further reduce the overshoot and thus achieve shorter settling time than the conventional reset control. Moreover, robustness tests against various step levels, disturbance and sensor noise are presented.",
Improving reliability and energy efficiency of disk systems via utilization control,"As disk drives become increasingly sophisticated and processing power increases, one of the most critical issues of designing modern disk systems is data reliability. Although numerous energy saving techniques are available for disk systems, most of energy conservation techniques are not effective in reliability critical environments due to their limitation of ignoring the reliability issue. A wide range of factors affect the reliability of disk systems; the most important factors - disk utilization and ages — are the focus of this study. We build a model to quantify the relationship among the disk age, utilization, and failure probabilities. Observing that the reliability of a disk heavily relies on both disk utilization and age, we propose a novel concept of safe utilization zone, where energy of the disk can be conserved without degrading reliability. We investigate an approach to improving both reliability and energy efficiency of disk systems via utilization control, where disk drives are operated in safe utilization zones to minimize the probability of disk failure. In this study, we integrate an existing energy consumption technique that operates the disks at different power modes with our proposed reliability approach. Experimental results show that our approach can significantly improve reliable while achieving high energy efficiency for disk systems.","Reliability,
Load management,
Energy efficiency,
Disk drives,
Energy consumption,
Energy conservation,
Correlation"
Adaptive data compression for high-performance low-power on-chip networks,"With the recent design shift towards increasing the number of processing elements in a chip, high-bandwidth support in on-chip interconnect is essential for low-latency communication. Much of the previous work has focused on router architectures and network topologies using wide/long channels. However, such solutions may result in a complicated router design and a high interconnect cost. In this paper, we exploit a table-based data compression technique, relying on value patterns in cache traffic. Compressing a large packet into a small one can increase the effective bandwidth of routers and links, while saving power due to reduced operations. The main challenges are providing a scalable implementation of tables and minimizing overhead of the compression latency. First, we propose a shared table scheme that needs one encoding and one decoding tables for each processing element, and a management protocol that does not require in-order delivery. Next, we present streamlined encoding that combines flit injection and encoding in a pipeline. Furthermore, data compression can be selectively applied to communication on congested paths only if compression improves performance. Simulation results in a 16-core CMP show that our compression method improves the packet latency by up to 44% with an average of 36% and reduces the network power consumption by 36% on average.","Data compression,
Network-on-a-chip,
Delay,
Network topology,
Costs,
Telecommunication traffic,
Bandwidth,
Decoding,
Protocols,
Pipelines"
Utilizing Model Structure for Efficient Simultaneous Localization and Mapping for a UAV Application,"This contribution aims at unifying two recent trends in applied particle filtering (PF). The first trend is the major impact in simultaneous localization and mapping (SLAM) applications, utilizing the FastSLAM algorithm. The second one is the implications of the marginalized particle filter (MPF) or the Rao-Blackwellized particle filter (RBPF) in positioning and tracking applications. Using the standard FastSLAM algorithm, only low-dimensional vehicle models are computationally feasible. In this work, an algorithm is introduced which merges FastSLAM and MPF, and the result is an algorithm for SLAM applications, where state vectors of higher dimensions can be used. Results using experimental data from a UAV (helicopter) are presented. The algorithm fuses measurements from on-board inertial sensors (accelerometer and gyro) and vision in order to solve the SLAM problem, i.e., enable navigation over a long period of time.","Simultaneous localization and mapping,
Unmanned aerial vehicles,
Particle filters,
Filtering,
Particle tracking,
Computational modeling,
Helicopters,
Fuses,
Time measurement,
Sensor fusion"
Correction for Resolution Nonuniformities Caused by Anode Angulation in Computed Tomography,"Most X-ray tubes comprise a rotating anode that is bombarded with electrons to produce X-rays. A substantial amount of heat is generated, and to increase the area of the anode exposed to the electrons, without increasing the apparent size of the focal spot, the focal track of the anode is generally beveled with a very shallow angle (typically 5deg-7deg in a computed tomography (CT) tube). Due to the line focus principle, this allows a fairly large area of the focal track to be exposed to electrons while retaining a fairly small effective projected focal spot. One side effect of anode angulation is that the focal spot appears different from different positions in the detector array; the effective focal spot size at a constant distance from the tube will be larger for a peripheral detector channel than for a central one. These differences in the effective size of the focal spot across the fleld-of-view lead to worse resolution in the periphery than in the center of reconstructed images. In this work we describe a method for achieving more uniform resolution in fanbeam CT images by correcting for these focal spot angulation effects. We do so by modeling the effects as a series of local blurrings in the space of transmitted CT intensities and determining the effective coefficients of the corresponding discrete convolutions. The effect of these blurrings can then be compensated for in the sinogram domain through the use of a penalized-likelihood sinogram restoration model we have recently developed.","Anodes,
Computed tomography,
Electron tubes,
Detectors,
Image resolution,
X-ray imaging,
Focusing,
Sensor arrays,
Image reconstruction,
Image restoration"
CompactRIO embedded system in Power Quality Analysis,"Electrical measurement department of VSB-Technical University has been involved for more than 14 years in research and development of Power Quality Analyzer built on Virtual Instrumentation Technology. PC-based power quality analyzer with National Instruments data acquisition board was designed and developed in this time frame. National Instruments LabVIEW is used as the development environment for all parts of power quality analyzer software running under MS Windows OS. Proved PC-based firmware was ported to new hardware platform for virtual instrumentation - National Instruments CompactRIO at the end of 2007. Platform change from PC to CompactRIO is not just code recompilation, but it brings up many needs for specific software redesigns. Paper describes how the monolithic executable for PC-based instruments was divided into three software layers to be ported on CompactRIO platform. The code for different parts of CompactRIO instrument is developed in a unified development environment no matter if the code is intended for FPGA, real-time processor of PC running Windows OS.",
Trajectory tracking of underactuated skid-steering robot,This paper considers problem of approximation of admissible trajectory for skid-steering mobile robot at kinematic level. Nonholonomic constraints at kinematic and dynamic level are taken into account. The trajectory tracking control problem is solved using practical stabilizer using tunable oscillator with novel method of tuning. The stability result is proved using Lyapunov analysis and takes into account uncertainty of kinematics. In order to ensure stable motion of the robot the scaling method is used. Theoretical considerations are illustrated by simulation results.,
Security in networks: A game-theoretic approach,"This paper explores network security as a game between attacker and defender. In this game, the attacker and defender both anticipate each other’s best strategy. Thus, instead of focusing on the best response to an attack, the paper analyzes the Nash equilibrium for the joint strategies. The paper studies two types of problem. The first type concerns networks where the data can be modified by an intruder. Given the probability that such an intruder exists, the network user decides whether to trust the data he observes. When present, the intruder chooses how to corrupt the data. The second type models virus attacks. The virus designer decides how aggressive the virus should be and the defender chooses a mechanism to detect the virus. If the virus is too aggressive, it is easy to detect. Accordingly, there is an optimum level of aggressiveness.","Intrusion detection,
Costs,
Computer security,
Computer worms,
Bayesian methods,
Information security,
Internet,
Computer viruses,
Computer networks,
Nash equilibrium"
Energy trapping in power transmission through an elastic plate by finite piezoelectric transducers,We study transmission of electric energy through an elastic plate by acoustic wave propagation and piezoelectric transducers. Our mechanics model consists of an elastic plate with finite piezoelectric patches on both sides of the plate. A theoretical analysis using the equations of elasticity and piezoelectricity is performed. Energy trapping that describes the confinement and localization of the vibration energy is examined.,"Power transmission,
Piezoelectric transducers,
Laplace equations,
Capacitive sensors,
Electrodes,
Scholarships,
Civil engineering,
Electronic mail,
Materials science and technology,
Stress"
Two-Dimensional Active Learning for image classification,"In this paper, we propose a two-dimensional active learning scheme and show its application in image classification. Traditional active learning methods select samples only along the sample dimension. While this is the right strategy in binary classification, it is sub-optimal for multi-label classification. In multi-label classification, we argue that, for each selected sample, only a part of more effective labels are necessary to be annotated while others can be inferred by exploring the correlations among the labels. The reason is that the contributions of different labels to minimizing the classification error are different due to the inherent label correlations. To this end, we propose to select sample-label pairs, rather than only samples, to minimize a multi-label Bayesian classification error bound. This new active learning strategy not only considers the sample dimension but also the label dimension, and we call it Two-Dimensional Active Learning (2DAL). We also show that the traditional active learning formulation is a special case of 2DAL when there is only one label. Extensive experiments conducted on two real-world applications show that the 2DAL significantly outperforms the best existing approaches which did not take label correlation into account.","Image classification,
Labeling,
Learning systems,
Bayesian methods,
Humans,
Costs,
Iterative algorithms,
Redundancy,
Uncertainty,
Laboratories"
Capturing performance knowledge for automated analysis,"Automating the process of parallel performance experimentation, analysis, and problem diagnosis can enhance environments for performance-directed application development, compilation, and execution. This is especially true when parametric studies, modeling, and optimization strategies require large amounts of data to be collected and processed for knowledge synthesis and reuse. This paper describes the integration of the PerfExplorer performance data mining framework with the OpenUH compiler infrastructure. OpenUH provides auto-instrumentation of source code for performance experimentation and PerfExplorer provides automated and reusable analysis of the performance data through a scripting interface. More importantly, PerfExplorer inference rules have been developed to recognize and diagnose performance characteristics important for optimization strategies and modeling. Three case studies are presented which show our success with automation in OpenMP and MPI code tuning, parametric characterization, and power modeling. The paper discusses how the integration supports performance knowledge engineering across applications and feedback-based compiler optimization in general.","Performance analysis,
Automation,
Performance loss,
Application software,
Data mining,
Knowledge engineering,
Information analysis,
Computer science,
Parametric study,
Character recognition"
Real-time Detection of Abnormal Vehicle Events with Multi-Feature over Highway Surveillance Video,"This paper introduces a framework of real-time abnormal vehicle event detection with multi-feature over highway high-definition surveillance video. The framework is composed of two parts: multi-feature extraction and abnormity detection. In multi-feature extraction, a fast constrained Delaunay triangulation (CDT) algorithm based on constrained-edge priority is presented to instead of complicated segmentation algorithms. After calibrating manually to extract the actual driveways from surveillance video sequence, localizing vehicle regions and tracking via detection of vehicle regions to extract static features and motional features in monitor area, multi-feature vectors are created for each vehicle. In abnormity detection, a method of adaptive detection modeling of vehicle events (ADMVE) is introduced. A Semi-supervised Mixture of Gaussian Hidden Markov Model is trained with the multi-feature vectors for each video segment. The normal model is trained by supervised mode with manual labeling, and becomes more accurate via adaptation iteration. The abnormal models are trained through the adapted Bayesian learning with unsupervised mode. Finally, experiments using real video sequence are performed to verify the proposed method.","Vehicle detection,
Event detection,
Road vehicles,
Road transportation,
Surveillance,
Hidden Markov models,
Vehicle driving,
Video sequences,
High definition video,
Tracking"
A Web Usage Mining Approach Based on LCS Algorithm in Online Predicting Recommendation Systems,"The Internet is one of the fastest growing areas of intelligence gathering. During their navigation web users leave many records of their activity. This huge amount of data can be a useful source of knowledge. Advanced mining processes are needed for this knowledge to be extracted, understood and used. Web Usage Mining (WUM) systems are specifically designed to carry out this task by analyzing the data representing usage data about a particular Web Site. WUM can model user behavior and, therefore, to forecast their future movements. Online prediction is one web usage mining application. However, the accuracy of the prediction and classification in the current architecture of predicting users' future requests systems can not still satisfy users especially in Huge Web sites. To provide online prediction efficiently, we advance an architecture for online predicting in Web Usage Mining system and propose a novel approach based on LCS algorithm for classifying user navigation patterns for predicting users' future requests. The Excremental results show that the approach can improve accuracy of classification in the architecture.","Web sites,
data mining,
information retrieval,
Internet,
knowledge representation,
pattern classification"
An Improved ARQ Scheme in Underwater Acoustic Sensor Networks,"In underwater acoustic communications, using the property of long propagation delay, a concurrent packet transmission between two nodes could be made feasible without any collision. In this paper, utilizing the channel-sharing property, an efficient ARQ scheme is proposed. By controlling packet size such a way that transmission time becomes smaller than propagation delay, and by scheduling such packets properly, the collision-free transmission between multiple nodes is achieved. In addition, during a packet-relay through multiple hops, the typical acknowledgement (ACK) signal is replaced with overhearing data packet returned back from the next hop. The usage of overhearing as an ACK not only save power consumption but also significantly reduces overhead and transmission latency. Through the mathematical analysis and simulations, we evaluate proposed scheme in terms of the latency by comparing with an existing Stop & Wait ARQ.","Automatic repeat request,
Underwater acoustics,
Acoustic sensors,
Propagation delay,
Underwater communication,
Size control,
Communication system control,
Energy consumption,
Mathematical analysis,
Analytical models"
A Case Study of Software Process Improvement in a Chinese Small Company,"This paper presents a software process improvement (SPI) project at a small software company in China, who aims to transit quality system from ISO9000 based to CMMI based. One of the main challenges is how to combine flexibility and control without impeding a small company's innovative nature. Therefore many SPI practices were implemented, mainly including process modeling, process automation, and process measurement. These experiences with the SPI initiatives offered several lessons about how small companies can more successfully manage SPI.","Software quality,
Quality management,
Computer science,
Software engineering,
Impedance,
Automation,
Contracts,
Outsourcing,
Companies,
Automatic control"
Latent Dirichlet Allocation and Singular Value Decomposition Based Multi-document Summarization,"Multi-Document Summarization deals with computing a summary for a set of related articles such that they give the user a general view about the events. One of the objectives is that the sentences should cover the different events in the documents with the information covered in as few sentences as possible. Latent Dirichlet Allocation can breakdown these documents into different topics or events. However to reduce the common information content the sentences of the summary need to be orthogonal to each other since orthogonal vectors have the lowest possible similarity and correlation between them. Singular Value Decompositions used to get the orthogonal representations of vectors and representing sentences as vectors, we can get the sentences that are orthogonal to each other in the LDA mixture model weighted term domain. Thus using LDA we find the different topics in the documents and using SVD we find the sentences that best represent these topics. Finally we present the evaluation of the algorithms on the DUC2002 Corpus multi-document summarization tasks using the ROUGE evaluator to evaluate the summaries. Compared to DUC 2002 winners, our algorithms gave significantly better ROUGE-1 recall measures.","Singular value decomposition,
Linear discriminant analysis,
Computer science,
Data engineering,
Bayesian methods,
Probability distribution,
Frequency,
Data mining,
Context modeling,
Joining processes"
Research on Combination Kernel Function of Support Vector Machine,"The kernel function and parameters selection is a key problem in the research of support vector machine. After discussing the influence of support vector machine on kernel parameters and error penalty factors, a new kernel function CombKer was proposed and constructed. The CombKer kernel function is a kind of combination kernel function, which combines the Gaussian RBF kernel function that has the local characteristic, with the linear kernel function that has the global characteristic. Finally, some experiments on different domains data in the support vector machine constructed by the CombKer kernel function were done, and the results showed the better ability on prediction of this kind of support vector machine and proved the validation of the CombKer kernel function.","Kernel,
Support vector machines,
Support vector machine classification,
Computer science,
Polynomials,
Neural networks,
Machine learning,
Virtual colonoscopy,
Pattern analysis,
Time series analysis"
Symbolic model checking of hierarchical UML state machines,"A compact symbolic encoding is described for the transition relation of systems modeled with asynchronously executing, hierarchical UML state machines that communicate through message passing and attribute access. This enables the analysis of such systems by symbolic model checking techniques, such as BDD-based model checking and SAT-based bounded model checking. Message reception, completion events, and run-to-completion steps are handled in accordance with the UML specification. The size of the encoding for state machine control logic is linear in the size of the state machine even in the presence of composite states, orthogonal regions, and message deferring. The encoding is implemented for the NuSMV model checker, and preliminary experimental results are presented.","Unified modeling language,
Encoding,
Computer science,
Message passing,
Data structures,
Logic,
Boolean functions,
Model driven engineering,
Machine control,
Hardware"
Early experience with out-of-core applications on the cray XMT,"This paper describes our early experiences with a preproduction Cray XMT system that implements a scalable shared memory architecture with hardware support for multithreading. Unlike its predecessor, the Cray MTA-2 that had very limited I/O capability, the Cray XMT offers Lustre, a scalable high-performance parallel filesystem. Therefore it enables development of out-of-core applications that can deal with very large data sets that otherwise would not fit in the system main memory. Our application performs statistically-based anomaly detection for categorical data that can be used for analysis of Internet traffic data. Experimental results indicate that the preproduction version of the machine is able to achieve good performance and scalability for the in- and out-of-core versions of the application.","Application software,
Yarn,
Delay,
Laboratories,
Mathematics,
Hardware,
Performance analysis,
Internet,
Mathematical model,
Computer architecture"
Learning weighted distances for relevance feedback in image retrieval,"We present a new method for relevance feedback in image retrieval and a scheme to learn weighted distances which can be used in combination with different relevance feedback methods. User feedback is a crucial step in image retrieval to maximise retrieval performance as was shown in recent image retrieval evaluations. Machine learning is expected to be able to learn how to rank images according to users needs. Most image retrieval systems incorporate user feedback using rather heuristic means and only few groups have formally investigated how to maximise the benefit from it using machine learning techniques. We incorporate our distance-learning method into our new relevance feedback scheme and into two different approaches from the literature. The methods are compared on two publicly available databases, one which is purely content-based and one which uses additional textual information. It is shown that the new relevance feedback scheme outperforms the other methods and that all methods benefit from weighted distance learning.","Image retrieval,
Information retrieval,
Image databases,
Computer aided instruction,
Machine learning,
Content based retrieval,
Negative feedback,
Computer science,
Support vector machines,
Support vector machine classification"
Fuzzy multiple attributes group decision-making based on the extension of TOPSIS method and interval type-2 fuzzy sets,"Type-2 fuzzy sets involve more uncertainties than type-1 fuzzy sets. They provide us additional degrees of freedom to represent the uncertainty and the fuzziness of the real-world. Type-2 fuzzy sets can be regarded as an extension of type-1 fuzzy sets. In this paper, we extend the TOPSIS method to propose a new method for handling fuzzy multiple attributes group decision-making problems based on the ranking values of interval type-2 fuzzy sets. The proposed method provides us a useful way to handle fuzzy multiple attributes group decision-making problems in a more flexible and more intelligent manner.","Fuzzy sets,
Decision making,
Uncertainty,
Machine learning,
Cybernetics,
Computer science,
Electronic mail"
Fundamental performance constraints in horizontal fusion of in-order cores,"A conceptually appealing approach to supporting a broad range of workloads is a system comprising many small cores that can be fused, on demand, into larger cores. We demonstrate that using in-order cores for this purpose, even under idealized assumptions about fusion-related overheads, would introduce fundamental obstacles to achieving good performance — obstacles that are not present when out-of-order cores are used. Matching the performance of modern dynamically-scheduled designs demands that a fused machine be able to simultaneously manage a large number of active dataflow chains, many more than the amount of ILP typically extracted from the code. When it is in-order cores that are fused, this requirement, in turn, demands either that the active dataflow chains be carefully interleaved among the available issue queues, or that enough cores be provided for them to reside at distinct queues. Using an abstract model for reasoning about the performance of these machines, we show that the former option is fundamentally hard, in the sense that it necessitates instruction steering hardware that would be too complex to build. The latter option would demand so many cores that the machine would be overwhelmed by fusion-related overheads. In short, if the goal is to match the performance of modern dynamically-scheduled machines, fusion of in-order cores is not a very compelling approach; either a fundamentally new method for fusing cores is needed, or some form of out-of-order capability must be provided at the constituent cores.","Magnetic cores,
Out of order,
Benchmark testing,
Fuses,
Hardware,
Bandwidth,
Global communication"
Incremental Latent Semantic Indexing for Automatic Traceability Link Evolution Management,"Maintaining traceability links among software artifacts is particularly important for many software engineering tasks. Even though automatic traceability link recovery tools are successful in identifying the semantic connections among software artifacts produced during software development, no existing traceability link management approach can effectively and automatically deal with software evolution. We propose a technique to automatically manage traceability link evolution and update the links in evolving software. Our novel technique, called incremental latent semantic indexing (iLSI), allows for the fast and low-cost LSI computation for the update of traceability links by analyzing the changes to software artifacts and by reusing the result from the previous LSI computation before the changes. We present our iLSI technique, and describe a complete automatic traceability link evolution management tool, TLEM, that is capable of interactively and quickly updating traceability links in the presence of evolving software artifacts. We report on our empirical evaluation with various experimental studies to assess the performance and usefulness of our approach.","Software,
Large scale integration,
Evolution (biology),
Matrix decomposition,
Documentation,
Software systems,
Complexity theory"
Self turn-on loss of MOSFET as synchronous rectifier in DC/DC buck converter - in case of a low driving impedance -,"In this paper we present a kind of losses on MOSFET as a synchronous rectifier which is very difficult to remove in compare with the other losses. For high performance, downsizing and lightweight of digital equipment as a personal computer, digital ICs are developed with high speed and densely integrated. The supply voltage for these ICs becomes lower and the supply current becomes higher. A buck converter is widely used as a DC/DC converter for this purpose. In the buck converter, a synchronous rectifier of MOSFET with ultra low On-resistance is widely used as a low-side switch for higher conversion efficiency instead of a Schottky barrier diode as a free wheel diode. When the high-side switch is turned on as the low-side switch is off, the drain-source voltage of the low-side switch rises rapidly and the gate-source voltage rises simultaneously through the drain-gate capacity. As a result, the gate-source voltage becomes up to the gate threshold voltage and the low-side switch becomes active state and the drain current flows. We named this phenomenon ‘Self Turn-on’ where the loss occurs in the low-side switch.[1][2] In this paper a precise analysis of the Self Turn-on phenomenon is presented and the experiments for verification are also presented.","Logic gates,
Switches,
Rectifiers,
MOSFET circuits,
Integrated circuit modeling,
Threshold voltage,
Impedance"
Extracting Prime Business Rules from Large Legacy System,"Business rules are operational rules that business organizations follow to perform various activities. Over time, business rules evolve and the software that implemented them is also changed. As the encompassing software becomes large and aged, the business rules embedded are substantial and difficult to extract. Furthermore, the encompassing software is changed without changing the corresponding documents, and thus often the organizations trust the code more than any other documents. It is possible to use a generic tool to extract all business rules of system however this may be an expensive exercise due to thousands upon thousands code. This paper proposes a tailored solution approach to the rule extraction problem, which consists of prime program slicing, prime domain variable identifying and data analysis, rules validation. The proposed approach has been implemented as a system and successfully experimented in a large complex financial system.","Data mining,
Software maintenance,
Educational institutions,
Data analysis,
Programming profession,
Computer science,
Software engineering,
Physics computing,
Information technology,
Organizational aspects"
Representing Fuzzy Information by Using XML Schema,"Issues related to fuzzy data have been investigated in the classical database research field, and in the last years are becoming interesting topics also in the XML data context. In this work we propose a general XML Schema definition for representing fuzzy information.","XML,
Meteorology,
Databases,
Proposals,
Fuzzy sets,
Relational databases,
Data models"
Lossless Data Hiding Based on Histogram Modification for Image Authentication,"Lossless data hiding enables the embedding of messages in a host image without any loss of content. In this paper, we present a lossless data hiding technique based on histogram modification for image authentication that is lossless in the sense that if the marked image is deemed authentic, the embedding distortion can be completely removed from the marked image after the embedded message has been extracted. This technique uses characteristics of the pixel difference to embed more data than other histogram-based lossless data hiding algorithms. We also present a histogram shifting technique to prevent overflow and underflow problems. Performance comparisons with other existing lossless data hiding schemes are provided to demonstrate the superiority of the proposed scheme.","Data encapsulation,
Histograms,
Authentication,
Data mining,
Computer science,
Performance loss,
Digital images,
Biomedical imaging,
Ubiquitous computing,
Data engineering"
Robust Runtime Optimization of Data Transfer in Queries over Web Services,"Self-managing solutions have recently attracted a lot of interest from the database community. The need for self-* properties is more evident in distributed applications comprising heterogeneous and autonomous databases and functionality providers. Such resources are typically exposed as Web Services (WSs), which encapsulate remote DBMSs and functions called from within database queries. In this setting, database queries are over WSs, and the data transfer cost becomes the main bottleneck. To reduce this cost, data is shipped to and from WSs in chunks; however the optimum chunk size is volatile, depending on both the resources' runtime properties and the query. In this paper we propose a robust control theoretical solution to the problem of optimizing the data transfer in queries over WSs, by continuously tuning at runtime the block size and thus tracking the optimum point. Also, we develop online system identification mechanisms that are capable of estimating the optimum block size analytically. Both contributions are evaluated via both empirical experimentation in a real environment and simulations, and have been proved to be more effective and efficient than static solutions.","Robustness,
Runtime,
Web services,
Costs,
Data communication,
Distributed databases,
Computer science,
Performance gain,
Automation,
Application software"
MSVM-kNN: Combining SVM and k-NN for Multi-class Text Classification,"Text categorization is the process of assigning documents to a set of previously fixed categories. It is widely used in many data-oriented management applications. Many popular algorithms for text categorization have been proposed, such as Naïve Bayes, k-Nearest Neighbor (k-NN), Support Vector Machine (SVM). However, those classification approaches do not perform well in every case, for example, SVM can not identify categories of documents correctly when the texts are in cross zones of multi-categories, k-NN cannot effectively solve the problem of overlapped categories borders. In this paper, we propose an approach named as Multi-class SVM-kNN (MSVM-kNN) which is the combination of SVM and k-NN. In the approach, SVM is first used to identify category borders, then k-NN classifies documents among borders. MSVM-kNN can overcome the shortcomings of SVM and k-NN and improve the performance of multi-class text classification. The experimental results show MSVM-kNN performs better than SVM or kNN.","text analysis,
classification,
support vector machines"
Design and implementation of a universal appliance controller based on selective interaction modes,"In next generation of smart computing environment, myriads of networked resources are pervasive and embedded into everyday consumer products and appliances. Still, most controller systems provide separate control user interfaces for different uses when users inevitably face an increased number of interfaces for control and interaction tasks in such environment. To simplify burdens of user and reduce complexities in control tasks, we propose a universal appliance controller based on mobile device with two selective interaction modes. First mode is a network scanning-based indirect interaction mode which provides fast and reliable connection to the appliances with specific filters in the networked environment. Second mode is a camera-based direct interaction mode which provides an intuitive interface as an input device. These two modes are interchangeable for different tasks and according to user's preference. Moreover, control user interface is personalized with preference described in user profile to reflect the user of the system better. To verify usefulness of our approach, we present a prototype in a smart home test bed. Through an initial user study, we demonstrate usability of our prototype through a set of mobile interaction tasks. Moreover, we note usability issues, limitations of our work and analyze the participants' feedback.",
Accomplishing reuse with a simulation conceptual model,"Reuse has been very difficult or in some cases impossible in the Modeling and Simulation (M&S) discipline. This paper focuses on how reuse can be accomplished by using a conceptual model (CM) in a community of interest (COI). We address the issue of reuse in a multifaceted manner covering many areas (types) of M&S such as discrete, continuous, Monte Carlo, system dynamics, gaming-based, and agent-based. M&S is commonly employed and reuse is critically needed by many COIs such as air traffic control, automobile manufacturing, ballistic missile defense, business process reengineering, emergency response management, military training, network-centric operations and warfare, supply chain management, telecommunications, and transportation. We present how a CM developed for a COI can assist in reuse for the design of any type of large-scale complex M&S application in that COI. A CM becomes an asset for a COI and offers significant economic benefits through its effective reuse.","Management training,
Supply chain management,
Monte Carlo methods,
Vehicle dynamics,
Air traffic control,
Automobile manufacture,
Missiles,
Business process re-engineering,
Disaster management,
Telecommunication network management"
Comparison of Particle Swarm Optimization and Genetic Algorithm for HMM training,"Hidden Markov Model (HMM) is the dominant technology in speech recognition. The problem of optimizing model parameters is of great interest to the researchers in this area. The Baum-Welch (BW) algorithm is a popular estimation method due to its reliability and efficiency. However, it is easily trapped in local optimum. Recently, Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) have attracted considerable attention among various modern heuristic optimization techniques. Since the two approaches are supposed to find a solution to a given objective function but employ different strategies and computational effort, it is appropriate to compare their performance. This paper presents the application and performance comparison of PSO and GA for continuous HMM optimization in continuous speech recognition. The experimental results demonstrate that PSO is superior to GA in respect of the recognition performance.","Particle swarm optimization,
Genetic algorithms,
Hidden Markov models,
Educational institutions,
Speech recognition,
Sun,
Biological cells,
Genetic mutations,
Statistical analysis,
Convergence"
Overcoming Impediments to Cell Phone Forensics,"Cell phones are an emerging but rapidly growing area of computer forensics. While cell phones are becoming more like desktop computers functionally, their organization and operation are quite different in certain areas. For example, most cell phones do not contain a hard drive and rely instead on flash memory for persistent storage. Cell phones are also designed more as special-purpose appliances that perform a set of predefined tasks using proprietary embedded software, rather than general-purpose extensible systems that run common operating system software. Such differences make the application of classical computer forensic techniques difficult. Also complicating the situation is the state of the art of present day cell phone forensic tools themselves and the way in which tools are applied. This paper identifies factors that impede cell phone forensics and describes techniques to address two resulting problems in particular: the limited coverage of available phone models by forensic tools, and the inadequate means for validating the correct functioning of forensic tools.","Impedance,
Cellular phones,
Forensics,
Embedded software,
Flash memory,
Home appliances,
Operating systems,
Software performance,
Software systems,
Application software"
Using k-Pricing for Penalty Calculation in Grid Market,"To distribute risk in Grid, the design of service level agreements (SLAs) plays an important role, since these contracts determine the price for a service at an agreed quality level as well as the penalties in case of SLA violation. This paper proposes a price function over the quality of service(QoS) on the basis of the agreements negotiated upon price and quality objective. This function defines fair prices for every possible quality of a service, which are in line with the business of the customer and incentivize the provider to supply welfare-maximizing quality. Therewith, penalties can be calculated for every possible quality level as the difference between the agreed price and the output of the price function for the effectively met quality. A price function according to the k-pricing scheme is presented for a single service scenario and for a scenario with multiple interdependent services.","Management information systems,
Conference management,
Information management,
Quality management,
Risk management,
Computer science,
Contracts,
Quality of service,
Grid computing,
Measurement standards"
iDEAL: Inter-router Dual-Function Energy and Area-Efficient Links for Network-on-Chip (NoC) Architectures,"Network-on-Chip (NoC) architectures have been adopted by a growing number of multi-core designs as a flexible and scalable solution to the increasing wire delay constraints in the deep sub-micron regime. However, the shrinking feature size limits the performance of NoCs due to power and area constraints. Research into the optimization of NoCs has shown that a reduction in the number of buffers in the NoC routers reduces the power and area overhead but degrades the network performance. In this paper, we propose iDEAL, a low-power area-efficient NoC architecture by reducing the number of buffers within the router. To overcome the performance degradation caused by the reduced buffer size, we propose to use adaptive dual-function links capable of data transmission as well as data storage when required. Simulation results for the proposed architecture show that reducing the router buffer size in half and using the adaptive dual-function links achieves nearly 40% savings in buffer power, 30% savings in overall network power and about 41% savings in the router area, with only a marginal 1-3% drop in performance. Moreover, the performance in iDEAL can be further improved by aggressive and speculative flow control techniques.","Network-on-a-chip,
Computer architecture,
Computer networks,
Degradation,
Energy consumption,
Wire,
Very large scale integration,
Design optimization,
Computer science,
Power engineering and energy"
A Distributed Area-Based Guiding Navigation Protocol for Wireless Sensor Networks,"One of the major applications of wireless sensor networks is guiding navigation service whose goal is to find a way to guide moving objects across a hazardous region covered by sensors. Sensor networks maintain safe paths by which the moving objects can be guided safely to exits. In this paper, we propose a distributed guiding navigation protocol for constructing area-to-area optimal guiding paths that do not traverse through the hazardous areas and to guide the moving objects to escape from this area safely and quickly. Our protocol also allows multiple exits and multiple emergency events in the sensor networks. Moreover, we propose a load-dispersion algorithm including an additional AP layer for moving objects registration and path assignment in multi-exits scenario. Hence, the moving objects can be properly dispersed to multiple paths leading to multiple exits to avoid congestion in the same exit. Simulation results show that our protocol can guide moving objects along shorter paths to reach nearest exits and operate in different scenarios or environments.","Navigation,
Wireless application protocol,
Wireless sensor networks,
Computer science,
Monitoring,
Distributed algorithms,
Application software,
Hazardous areas,
Manufacturing,
Indoor environments"
Advanced phase-based segmentation of multiple cells from brightfield microscopy images,"Segmenting transparent phase objects, such as biological cells from brightfield microscope images, is a difficult problem due to the lack of observable intensity contrast and noise. Previous image analysis solutions have used excessive defocusing or physical models to obtain the underlying phase properties. Here, an improved cell boundary detection algorithm is proposed to accurately segment multiple cells within the level set framework. This uses a novel speed term based on local phase and local orientation derived from the monogenic signal, which renders the algorithm invariant to intensity, making it ideal for these images. The new method can robustly handle noise and local minima, and distinguish touching cells. Validation is shown against manual expert segmentations.","Image segmentation,
Microscopy,
Biological cells,
Phase noise,
Image analysis,
Focusing,
Biological system modeling,
Detection algorithms,
Level set,
Rendering (computer graphics)"
"Universal multilevel dc-dc converter with variable conversion ratio, high compactness factor and limited isolation feature","A multilevel dc-dc converter with programmable conversion ratio (CR) is presented in this paper. This converter is a modified version of the MMCCC converter. A universal version of the MMCCC is developed in this paper, and the CR can be easily changed within a wide range. The MMCCC converter is based on capacitor-clamped topology, and the conversion ratio of the circuit depends on the number of active modules. However, like any other capacitor-clamped circuit, the MMCCC circuit requires a large number of transistors and capacitors to attain a high conversion ratio (CR). In this paper, a new circuit module will be introduced that can be connected in a cascade pattern to form the new converter. By using the new modular cell, it is possible to attain very high conversion ratio using a limited number of components, and thus more compactness compared to the predecessor MMCCC circuit can be achieved.",
Spatio-temporal patches for night background modeling by subspace learning,"In this paper, a novel background model on spatio-temporal patches is introduced for video surveillance, especially for night outdoor scene, where extreme lighting conditions often cause troubles. The spatio-temporal patch, called brick, is presented to simultaneously capture spatio-temporal information in surveillance video. The set of bricks of a given background patch, under all possible lighting conditions, lies in a low-dimensional subspace, which can be learned by online subspace learning. The proposed method can efficiently model the background and detect the appearance and motion variance caused by foreground. Experimental results on real data show that the proposed method is insensitive to dramatic lighting changes and achieves superior performance to two classical methods.","Layout,
Eigenvalues and eigenfunctions,
Image motion analysis,
Lighting,
Video surveillance,
Laboratories,
Information technology,
Computer science,
Partial response channels,
Motion detection"
A wavelet packet based pulse waveform analysis for cholecystitis and nephrotic syndrome diagnosis,"Traditional Chinese Pulse Diagnosis (TCPD), one of the four diagnostic methods of Traditional Chinese Medicine (TCM), had been proved to be clinically valid in Chinese Medicine history. Different from most previous work which focused on the diagnosis of cardiovascular diseases, this paper further investigated the possibility of diagnosing cholecystitis and nephrotic syndrome using the pulse waveform data. After the pre-processing, the pulse waveform signals were decomposed into a given level by Wavelet Packet Transform and the best basis was picked out by Shannon entropy criterion. Then, subband energies contained in the best basis were extracted as features and the support vector machine classifiers were trained. Experimental results indicated that the proposed method can effectively discriminate these two kinds of diseases.","Wavelet packets,
Support vector machines,
Feature extraction,
Wavelet analysis,
Diseases,
Pattern recognition,
Wavelet transforms"
Modeling and analysis of IEEE 802.16 PKM Protocols using CasperFDR,"IEEE 802.16 is the standard for broadband wireless access. The security sublayer is provided within IEEE 802.16 MAC layer for privacy and access control, in which the Privacy and Key Management (PKM) protocols are specified. This paper models the PKM protocols using Casper and analyzes the CSP output with FDR, which are formal analysis tools based on the model checker. Later versions of PKM protocols are also modeled and analyzed. Attacks are found in each version and the results are discussed.","Access protocols,
Authentication,
Body sensor networks,
Logic,
Privacy,
Wireless application protocol,
Communication system security,
Power system modeling,
WiMAX,
Data security"
Face shape recovery from a single image using CCA mapping between tensor spaces,"In this paper, we propose a new approach for face shape recovery from a single image. A single near infrared (NIR) image is used as the input, and a mapping from the NIR tensor space to 3D tensor space, learned by using statistical learning, is used for the shape recovery. In the learning phase, the two tensor models are constructed for NIR and 3D images respectively, and a canonical correlation analysis (CCA) based multi-variate mapping from NIR to 3D faces is learned from a given training set of NIR-3D face pairs. In the reconstruction phase, given an NIR face image, the depth map is computed directly using the learned mapping with the help of tensor models. Experimental results are provided to evaluate the accuracy and speed of the method. The work provides a practical solution for reliable and fast shape recovery and modeling of 3D objects.","Shape,
Tensile stress,
Image reconstruction,
Humans,
Surface reconstruction,
Face detection,
Surface fitting,
Magnetic force microscopy,
Lighting,
Infrared imaging"
A Formal Approach to Devising a Practical Method for Modeling Reusable Services,"Service-Oriented Architecture (SOA) is an effective approach to developing applications by utilizing reusable services. Service providers publish reusable services and service consumers reuse appropriate services for their applications. Services with high reusability would yield high applicability and high returns on investment. Hence, reusability is considered as a key quality attribute for services, and designing such reusable services is an essential design goal in SOA. Early approaches to service-oriented analysis and design consider the essence of service reusability, but do not provide effective methodological supports for reuse engineering. Moreover, their methods are not treated formally, leaving a room for ambiguity and incompleteness. Therefore, there is a great demand for methods to identify and design reusable services in a formal but practical manner. In this paper, we propose a systematic process for identifying and designing reusable services. The process consists of five activities and each activity is given a formal treatment and practical instructions. To show the applicability of our approach, we demonstrate the result of our case study for the domain of Flight Ticket Management Services.",
The Most Cited Intelligent Systems Articles,"In a study of its most cited articles, IEEE Intelligent Systems achieved an H-Index rating of 30 using citation information from ISI's Web of Knowledge and Google Scholar. The author also found that computer science departments' H-Index is strongly related to their ranking in Newsweek's computer science department ranking.","Intelligent systems,
Intersymbol interference,
Citation analysis,
Books,
Intelligent structures"
Learning and practising supply chain management strategies from a business simulation game: A comprehensive supply chain simulation,"An Internet based supply chain simulation game (ISCS) is introduced and demonstrated in this paper. Different from other games and extended from the Beer Game, a comprehensive set of supply chain (SC) management strategies can be tested in the game, and these strategies can be evaluated and appraised based on the built-in Management Information System (MIS). The key functionalities of ISCS are designed to increase players’ SC awareness, facilitate understanding on various SC strategies and challenges, foster collaboration between partners, and improve problem solving skills. It is concluded that an ISCS can be used as an efficient and effective teaching tool as well as a research tool in operations research and management science. Problems and obstacles have been observed while engaging in the SC business scenario game. The actions proposed and implemented to solve these problems have resulted in improved SC performance.","Supply chain management,
Supply chains,
Internet,
System testing,
Appraisal,
Management information systems,
Collaboration,
Problem-solving,
Education,
Operations research"
Analyzing High-Density ECG Signals Using ICA,"The analysis of ECG signals is of fundamental importance for cardiac diagnosis. Conventional ECG recordings, however, use a limited number of channels (12) and each records a mixture of activities generated in different parts of the heart. Therefore, direct observation of the ECG signals collected on the body surface is likely an inefficient way to study and diagnose cardiac abnormalities. This study describes new experimental and analytical methods to capture more meaningful ECG component signals, each representing more directly a physical cardiac source. This study first describes a simply applied method for collecting high-density ECG signals. The recorded signals are then separated by independent component analysis (ICA) to obtain spatially fixed and temporally independent component activations. Results from five subjects show that P-, QRS-, and T-waves can be clearly separated from the recordings, suggesting ICA might be an effective and useful tool for high-density ECG analysis, interpretation, and diagnosis.",
Mobile Services Acceptance Model,"Along with the rapid development of information and communication technology, more and more mobile commerce applications are available. However, what is the user's perception and reaction of those new applications? It is critical to study the factors that influence the user adoption of mobile applications. In this research work, we propose an extended technology acceptance model, which is called mobile services acceptance model, with a consideration of trust, context, and personal initiatives and characteristics factors in addition to perceived usefulness and perceived ease of use. Furthermore, in order to demonstrate the applicability of the mobile services acceptance model, one case study with respect to the user adoption of the mobile application---FindMyFriends is presented.","Mobile communication,
Business,
Technological innovation,
Mobile handsets,
Context modeling,
Complexity theory,
Information systems"
A SiGe BiCMOS instrumentation channel for extreme environment applications,"A instrumentation channel has been designed, implemented and tested in a 0.5-μm SiGe BiCMOS process. The circuit features a reconfigurable Wheatstone bridge network that interfaces a range of external sensors to signal processing circuits. Also, analog sampling has been implemented in the channel using a flying capacitor configuration. Measurement results show the instrumentation channel supports input signals up to 200Hz.","Bridge circuits,
Capacitors,
Sensors,
Temperature sensors,
Operational amplifiers,
Gain,
Temperature measurement"
"Channel Estimation, Equalization and Phase Correction for Single Carrier Underwater Acoustic Communications","In this paper, we employ a time-domain channel estimation, equalization and phase correction scheme for single carrier single input multiple output (SIMO) underwater acoustic communications. In this scheme, Doppler shift, which is caused by relative motion between transducer (source) and hydrophones (receiver), is estimated and compensated in the received baseband signals. Then the channel is estimated using a small training block at the front of a transmitted data package, in which the data is artificially partitioned into consecutive data blocks. The estimated channel is utilized to equalize a block of received data, then the equalized data is processed by a group-wise phase correction before data detection. At the end of the detected data block, a small portion of the detected data is utilized to update channel estimation, and the re-estimated channel is employed for channel equalization for next data block. This block-wise channel estimation, equalization and phase correction process is repeated until the entire data package is processed. The receiver scheme is tested with experimental data measured at Saint Margaret's Bay, Nova Scotia, Canada, in May 2006. The results show that it can be applied not only to the scenario of fixed source to fixed receiver, but also to the moving source to fixed receiver case. The achievable uncoded bit error rate (BER) is on the order of 10-4 for moving-to-fixed transmissions, and on the order of 10-5 for fixed-to-fixed transmissions.","Channel estimation,
Underwater acoustics,
Underwater communication,
Packaging,
Bit error rate,
Time domain analysis,
Doppler shift,
Acoustic transducers,
Sonar equipment,
Motion estimation"
Efficient Fully Implicit Time Integration Methods for Modeling Cardiac Dynamics,"Implicit methods are well known to have greater stability than explicit methods for stiff systems, but they often are not used in practice due to perceived computational complexity. This paper applies the backward Euler (BE) method and a second-order one-step two-stage composite backward differentiation formula (C-BDF2) for the monodomain equations arising from mathematically modeling the electrical activity of the heart. The C-BDF2 scheme is an L-stable implicit time integration method and easily implementable. It uses the simplest forward Euler and BE methods as fundamental building blocks. The nonlinear system resulting from application of the BE method for the monodomain equations is solved for the first time by a nonlinear elimination method, which eliminates local and nonsymmetric components by using a Jacobian-free Newton solver, called Newton--Krylov solver. Unlike other fully implicit methods proposed for the monodomain equations in the literature, the Jacobian of the global system after the nonlinear elimination has much smaller size, is symmetric and possibly positive definite, which can be solved efficiently by standard optimal solvers. Numerical results are presented demonstrating that the C-BDF2 scheme can yield accurate results with less CPU times than explicit methods for both a single patch and spatially extended domains.",
The Use of a Meta-Model to Support Multi-Project Process Measurement,"In today's environment, software companies are engaged in multiple projects delivered on heterogeneous platforms for a wide class of applications in disparate application domains. They are increasingly engaged in the co-development of software systems through joint software development projects including staff from partners and customers as well as their own. As a result, they must support multiple software development processes while trying to guarantee uniform levels of process enactment, and product quality across all projects. Our approach is capable of providing process measurement in a joint-project, multi-process model business environment. It is based on a simple meta-model for computing across-process, multiple-project metrics designed to permit monitoring of CMMI compliance. The open source tool Spago4Q has been developed to support our approach and is capable of producing the measurements needed for monitoring of a set  of large-scale development projects using different process models, in a real industrial setting in Europe. The results support the view that that it will not always be possible to aggregate the same set of metrics across disparate process models.","Monitoring,
Application software,
Programming,
Project management,
Large-scale systems,
Maintenance,
Software engineering,
Software measurement,
Information technology,
Computer science"
Matching non-rigidly deformable shapes across images: A globally optimal solution,"While global methods for matching shapes to images have recently been proposed, so far research has focused on small deformations of a fixed template.","Image segmentation,
Shape measurement,
Partitioning algorithms,
Pixel,
Animals,
Computer vision,
Legged locomotion,
Computer Society,
Pattern recognition,
Computer science"
An Environment to Support Large Scale Experimentation in Software Engineering,"Experimental studies have been used as a mechanism to acquire knowledge through a scientific approach based on measurement of phenomena in different areas. However it is hard to run such studies when they require models (simulation), produce large amount of information, and explore science in large scale. In this case, a computerized infrastructure is necessary and constitutes a complex system to be built. In this paper we discuss an experimentation environment that has being built to support large scale experimentation and scientific knowledge management in Software Engineering.","Large-scale systems,
Software engineering,
Knowledge engineering,
Knowledge management,
Distributed computing,
Engineering management,
Software systems,
Software quality,
Area measurement,
Software measurement"
Requirement Model-Based Mutation Testing for Web Service,"Web services present a new promising software technology. However, some new issues and challenges in testing of them come out due to their characteristics of distribution, source code invisibility etc. This paper discusses the traditional mutation testing and then a new methodology of OWL-S requirement model-based web service mutation testing is brought forward. The traits of this methodology are as follows. Firstly, requirements are used effectively to reduce the magnitude of mutants. Secondly, mutants are generated by AOP technology conveniently and promptly. Thirdly, to reducing testing cost, using business logic implied in OWL-S requirement model as assistant of the process of killing the mutants. Fourthly, two sufficient measurement criteria are employed to evaluate the testing process. Finally, our empirical results have shown the usefulness of this testing method.","Genetic mutations,
Web services,
Programming profession,
Software testing,
System testing,
Computer science,
Software systems,
Logic testing,
Fault detection,
Software quality"
Scheduling of Fault-Tolerant Embedded Systems with Soft and Hard Timing Constraints,"In this paper we present an approach to the synthesis of fault-tolerant schedules for embedded applications with soft and hard real-time constraints. We are interested to guarantee the deadlines for the hard processes even in the case of faults, while maximizing the overall utility. We use time/utility functions to capture the utility of soft processes. Process re-execution is employed to recover from multiple faults. A single static schedule computed off-line is not fault tolerant and is pessimistic in terms of utility, while a purely online approach, which computes a new schedule every time a process fails or completes, incurs an unacceptable overhead. Thus, we use a quasi-static scheduling strategy, where a set of schedules is synthesized off-line and, at run time, the scheduler will select the right schedule based on the occurrence of faults and the actual execution times of processes. The proposed schedule synthesis heuristics have been evaluated using extensive experiments.","Fault tolerant systems,
Embedded system,
Timing,
Processor scheduling,
Real time systems,
Fault tolerance,
Electromagnetic transients,
Embedded computing,
Information science,
Informatics"
A User-Oriented Approach to Automated Service Composition,"In the past a few years, the Web has undergone a tremendous change towards a highly user-centric environment. Millions of users can participate and collaborate for their own interests and benefits. Service Oriented Computing and web services have created great potential opportunities for the users to build their own applications. Then, it is a pressing issue that, the users can compose services without too complex tasks and efforts. In this paper, we introduce a user-oriented approach which aims to simplify service composition. We leverage the plentiful information residing in service tags, both from service descriptions (such as WSDL) and the annotations tagged by users. Employing some mining algorithms, a Direct Acyclic Graph is built up to represent potential composition opportunities. With a simple and intuitive search, it allows users to explore the space of potentially composable services and achieve service composition in a heuristic manner. We have developed a composition advisor to provide recommendations guiding and assisting the users. It also lets the users discover and make use of services without having to understand too many details of individual candidate services. To enable the users to accomplish service composition in a more interactive access channel, we finally provide a user-friendly prototype based on web browsers. It undoubtedly reduces the complexity and lowers the entry barrier for the users, and makes them better play their role in the Service-Oriented Web Environment.","Web services,
Space exploration,
Web and internet services,
Collaboration,
Pressing,
Prototypes,
Usability,
Cities and towns,
Computer science,
Educational technology"
Unity Power Factor control of permanent magnet motor drive system,"The Permanent Magnet Synchronous motors (PMSMs) have gained an increasing interest recently. The wide variety of applications of PMSM drives makes it necessary to achieve fast and reliable drive control system design. Vector control of PMSM can achieve fast dynamic response with less complexity and parameter-independent controller, prevent demagnetization of the motor and allow maximum efficiency operation. In this paper, a novel Unity Power Factor (UPF) control drive for PMSMs is presented. The drive is performed with constraint on the (PF) such that its steady-state value is unity. This feature provides an extension of the constant torque region, resulting in higher output power of the PMSM drive, which is desirable in many applications requiring extended speed range at rated motor torque. However, this drive is not optimal in terms of efficiency which will be less than that obtained from conventional decoupled vector control drive for the same torque. Therefore, it is concluded that before reaching the rated speed, the conventional decoupled vector control is preferable, whereas, the UPF control is optimal to have a wider range of speed operation (above the base speed of the conventional vector control) and hence, extension of the constant torque region. Above this extended base speed, the PMSM drive can be operated in constant power mode using the conventional field-weakening technique having constant supply voltage and current. The drive system is built using MATLAB-SIMULINK software. The validity is evaluated in both steady-state condition and transient response using computer simulation.","Reactive power,
Control systems,
Permanent magnet motors,
Machine vector control,
Synchronous motors,
Steady-state,
Torque,
Power system reliability,
Demagnetization,
Power generation"
"A self-referential childlike model to acquire phones, syllables and words from acoustic speech","Speech understanding requires the ability to parse spoken utterances into words. But this ability is not innate and needs to be developed by infants within the first years of their life. So far almost all computational speech processing systems neglected this bootstrapping process. Here we propose a model for early infant word learning embedded into a layered architecture comprising phone, phonotactics and syllable learning. Our model uses raw acoustic speech as input and aims to learn the structure of speech unsupervised on different levels of granularity. We present first experiments which evaluate our model on speech corpora that have some of the properties of infant-directed speech. To further motivate our approach we outline how the proposed model integrates into an embodied multimodal learning and interaction framework running on Honda’s ASIMO robot.","Speech,
Hidden Markov models,
Training,
Computational modeling,
Stability analysis,
Acoustics,
Pediatrics"
A 60 GHz high-Q tapered transmission line resonator in 90nm CMOS,"This paper presents an integrated high quality factor tapered transmission line resonator for 60 GHz applications, in a 90nm digital CMOS process. The resonator takes advantage of the standing wave properties of shorted quarter wavelength transmission lines to enhance the resonant quality factor by trading off between resistive and conductive losses. The tapered resonator achieves over 70% quality factor improvement over an optimal uniform resonator and can provide even higher gains on lower loss substrates.","Impedance,
Shape,
Optimization,
Power transmission lines,
Transmission line measurements,
Q factor,
Capacitors"
A Framework for Semantic Group Formation,"Collaboration has long been considered an effective approach to learning. However, forming optimal groups can be a time consuming and complex task. Different approaches have been developed to assist teachers allocate students to groups based on a set of constraints. However, existing tools often fail to assign some students to groups creating a problem well known as “orphan students”. In this paper we propose a framework for learner group formation, based upon satisfying the constraints of the person forming the groups by reasoning over semantic data about the potential participants.  The use of both Semantic Web technologies and Logic programming proved to increase the satisfaction of the constraints and overcome the orphans’ problem.","Computer science,
Semantic Web,
Logic programming,
Software engineering,
International collaboration,
Education,
Collaborative work,
Constraint theory,
Equal opportunities,
Information analysis"
Image Watermarking Scheme Using Singular Value Decomposition and Micro-genetic Algorithm,"In this paper, we introduce a novel image watermarking scheme using singular value decomposition (SVD) and micro-genetic algorithm (micro-GA). In an SVD-based watermarking scheme, the singular values of the cover image are modified by multiple scaling factors to embed the watermark image. The proper values of scaling factors are optimized and obtained efficiently by means of the micro-GA. Experimental results are provided to illustrate the feasibility of the proposed approach.","Watermarking,
Genetics,
Multimedia communication,
Gallium,
Signal processing algorithms,
Algorithm design and analysis,
Biological cells"
Evolutionary Clustering by Hierarchical Dirichlet Process with Hidden Markov State,"This paper studies evolutionary clustering, which is a recently hot topic with many important applications, noticeably in social network analysis. In this paper, based on the recent literature on Hierarchical Dirichlet Process (HDP) and Hidden Markov Model (HMM), we have developed a statistical model HDP-HTM that combines HDP with a Hierarchical Transition Matrix (HTM) based on the proposed Infinite Hierarchical Hidden Markov State model (iH2MS) as an effective solution to this problem. The HDP-HTM model substantially advances the literature on evolutionary clustering in the sense that not only it performs better than the existing literature, but more importantly it is capable of automatically learning the cluster numbers and structures and at the same time explicitly addresses the correspondence issue during the evolution. Extensive evaluations have demonstrated the effectiveness and promise of this solution against the state-of-the-art literature.","Hidden Markov models,
Data mining,
Computer science,
USA Councils,
Social network services,
Application software,
Information services,
Web sites,
Internet,
Time sharing computer systems"
Design and Implementation of Wireless Sensor Network for Ubiquitous Glass Houses,"Recently, there are many researches to enable future ubiquitous environments based on Wireless Sensor Network (WSN). Various sensing devices deployed in wireless sensor network collect meaningful data from physical environments and the data are delivered to neighbor node through radio interfaces for further processing. The representative application in WSN is a data collecting system, which monitors physical phenomena and gathers data around our life. In this system, sensor nodes can be considered as a kind of database and hosts connected to the Internet, in most cases, initiate some queries destined to WSN and collect query responses from WSN. In this study, the efficient use of limited energy efficiently for the WSN based large scale glass houses was researched. We designed an integration method which was based on WSN Gateway which integrated two different networks efficiently. To realize the proposed design, we implemented an efficient routing method and WSN Gateway in the wired and wireless integrated networks in ubiquitous glass houses.","Wireless sensor networks,
Glass,
Sensor phenomena and characterization,
Sensor systems,
Databases,
Internet,
Energy efficiency,
Large-scale systems,
Design methodology,
Routing"
Fast lane detection with Randomized Hough Transform,"Lane detection is an essential component of autonomous mobile robot applications. Any lane detection method has to deal with the varying conditions of the lane and surrounding that the robot would encounter while moving. Lane detection procedure can provide estimates for the position and orientation of the robot within the lane and also can provide a reference system for locating other obstacles in the path of the robot. In this paper we present a method for lane detection in video frames of a camera mounted on top of the mobile robot. Given video input from the camera, the gradient of the current lane in the near field of view are automatically detected. Randomized Hough Transform is used for extracting parametric curves from the images acquired. A priori knowledge of the lane position is assumed for better accuracy of lane detection.",
Security analysis of routing protocol for MANET based on extended Rubin logic,"One of the greatest obstacles to wide-spread deployment of MANET is security. Security of routing protocols for MANET is the emphasis of MANET security. However, it is difficult to design protocols that are immune to malicious attack, because good analysis techniques are lacking. Rubin logic is the first technique for specifying and analyzing nonmonotonic cryptographic protocols, though it is inadequate to analyze non-repudiation of protocols. Considered the property of routing protocols for MANET, extensions to Rubin logic are presented in this paper. A new local set as well as new actions and inference rules are introduced. The extended Rubin logic can be used to analyze non-repudiation of protocols. Taken example of ARAN, the security of routing protocol is analyzed by using the extended Rubin logic, which proves the method is valid.",
Overlay protection against link failures using network coding,"This paper introduces a network coding-based protection scheme against single and multiple link failures. The proposed strategy makes sure that in a connection, each node receives two copies of the same data unit: one copy on the working circuit, and a second copy that can be extracted from linear combinations of data units transmitted on a shared protection path. This guarantees instantaneous recovery of data units upon the failure of a working circuit. The strategy can be implemented at an overlay layer, which makes its deployment simple and scalable. The proposed strategy is an extension of the scheme presented in [1]. The new scheme is simpler, less expensive, and does not require the synchronization required by the original scheme. The sharing of the protection circuit by a number of connections is the key to the reduction of the cost of protection. A preliminary comparison of the cost of the proposed scheme to the 1+1 protection strategy is conducted, and establishes the benefits of our strategy.",
Association Action Rules,"Action rules describe possible transitions of objects from one state to another with respect to a distinguished attribute. Previous research on action rule discovery usually required the extraction of classification rules before constructing any action rule. This paper gives anew approach for generating association-type action rules. The notion of frequent action sets and Apriori-like strategy generating them is proposed. We introduce the notion of a representative action rules and give an algorithm to construct them directly from frequent action sets. Finally, we introduce the notion of a simple association action rule, the cost of association action rule, and give a strategy to construct simple association action rules of a lowest cost.","Costs,
Data mining,
Computer science,
USA Councils,
Conferences,
Educational technology,
Computer science education,
Remuneration,
Search problems,
Information systems"
Leakage power reduction for coarse grained dynamically reconfigurable processor arrays with fine grained Power Gating technique,"One of the benefits of coarse grained dynamically reconfigurable processor array(DRPA) is its low dynamic power consumption by operating a number of processing elements(PE) in parallel with low clock frequency. However, in the future advanced processes, leakage power will occupy a considerable part of the total power consumption, and it may degrade the advantage of DRPAs. In order to reduce the leakage power, a fine grained Power Gating(PG) is applied to a DRPA, MuCCRA-2.32b, and leakage power and area overhead are measured. We evaluated the effect of two control modes; Pair and Unit Individual based on layout design and real applications. It appears that by applying PG for ALUs and SMUs in PEs individually, 48% of leakage power can be reduced with 9.0% of area overhead",
An adaptive fast multiple reference frames selection algorithm for H.264/AVC,"To make full use of the temporal correlation of video sequences, H.264/AVC adopts multiple reference frames to enhance the coding quality. While the performance being improved, the complexity of computation has been increased linearly, too. In this paper, we study motion characteristic of video sequences and spatial correlation in video frames first, and then we propose an adaptive fast multiple reference frames selection algorithm. It can decrease the number of reference frames for motion compensation, and reduce the complexity of coding adaptively according to the features of video sequences. The results show that our algorithm achieves 45% coding time saving on average with unnoticeable quality loss.",
Reasoning about Channel Passing in Choreography,"Web services choreography describes global models of service interactions among a set of participants. For an interaction to be executed, the participants must know the required channel(s) used in the interaction, otherwise the execution will get stuck. Because of dynamic composition, the initial channel set on each participant is often insufficient to meet the requirements. It is the  responsibility of the participants to pass required channels owned (known) by one to some others. Since a choreography may involve many participants and complex channel constraints, it is hard for designers to specify channel passing in a choreography exactly as required. In this paper, we address the problem of checking whether a choreography lacks channels or has redundant channels, and how to automatically generate channel passing based on interaction flows of the choreography in the case of channel absence. Concretely, we propose a small language Chorc named for a channel interaction sub-language for modeling the channel passing aspect of choreography. Based on the formal operational semantics of Chorc, the algorithms for static checking choreography and generating channel passing are studied as well.","Web services,
Protocols,
Software engineering,
Chaos,
Informatics,
Process control,
Application software,
Laboratories,
Computer science,
Collaborative work"
Tracking and Repairing Damaged Databases Using Before Image Table,"Traditional damage assessment approaches can only locate damage caused by reading corrupted data in a post-intrusion database. This paper indicates another kind of damage spreading pattern characterized by omitting maliciously deleted data, which was not considered in previous studies. An extended recovery model arms at this problem is presented and a novel approach based on the model is proposed to recover a damaged database from malicious attacks. This approach can track the damage spreading more completely by maintaining before image tables (BI tables) in databases and analyzing transactions’ potential-read to the BI tables. BI tables are also used for damage repair without accessing database logs. Experimental evaluation of the overhead of this method based upon TPC-C benchmark is also presented.","Image databases,
Transaction databases,
Investments,
Bismuth,
Database systems,
Computer science,
Arm,
Data analysis,
Image analysis,
Data security"
Face Recognition Using Scale Invariant Feature Transform and Support Vector Machine,"Face recognition has received significant attention in the last decades for many potential applications. Recently, the Scale Invariant Feature Transform (SIFT) becomes an interesting technique for the task of object recognition. This paper investigated the application of the SIFT approach to the face recognition and proposed a new method based on SIFT and Support Vector Machine (SVM) for the face recognition problem. First the SIFT features are generated and then SVM is used for the classification. The presented method has been tested with the ORL database and the Yale face database, and the recognition results demonstrate its robust performance under different expression conditions.","Face recognition,
Support vector machines,
Support vector machine classification,
Face detection,
Application software,
Object recognition,
Spatial databases,
Robustness,
Computer science,
Educational institutions"
Evaluation of OPC UA secure communication in web browser applications,"OPC UA XML Web services mapping offers a Web service interface to access process data. Web services use XML technology for data exchange. Present-day Web browsers include XML functionality already as a standard feature, they are therefore very promising candidates for the implementation of monitoring and operating functions for industrial processes. However, the acceptance of Web services in industrial automation depends on adequate security realizations. For this purpose, the Web services security stack provides several specifications to meet the requirements for secure message exchange. The OPC UA XML Web services mapping refers to these specifications. The application of Web browsers for monitoring and operating of technical processes using OPC UA Web services demand the computation of cryptographic algorithms within the scripting engine of the Web browser. However, available scripting languages are not designed to compute complex mathematical, i.e., cryptographic, algorithms. Therefore, at the Institute of Automation of the Technische Universitat Dresden the suitability of a native Web browser for monitoring and operating of industrial processes with OPC UA based secure communication was analyzed. The paper shows representative measured computing times of cryptographic algorithms in JavaScript. The security specification XML signature - which is mandatory for OPC UA Web services mapping - requires about 700 ms to create a signature. Finally, the paper discusses methods to improve the performance.","Web services,
cryptography,
online front-ends,
telecommunication security"
A novel fuzzy background subtraction method based on cellular automata for urban traffic applications,"Computational structure of cellular automata has attracted researchers and vastly been used in various fields of science. They are especially suitable for modeling natural systems that can be described as massive collections of simple objects interacting locally with each other, such as motion detection in image processing. On the other hand, extraction of moving objects from an image sequence is a fundamental problem in dynamic image analysis Nowadays background modeling and subtraction algorithms are commonly used in real-time urban traffic applications for detecting and tracking vehicles and monitoring streets. In this paper by the use of cellular automata, a novel fuzzy approach for background subtraction with a particular interest to the problem of vehicle detection is presented. Our experimental results demonstrate that fuzzy-cellular system is much more efficient, robust and accurate than classical approaches.","Vehicle detection,
Traffic control,
Computer applications,
Motion detection,
Image processing,
Image sequences,
Vehicle dynamics,
Image motion analysis,
Image sequence analysis,
Monitoring"
Riemannian manifold optimisation for non-rigid structure from motion,"This paper address the problem of automatically extracting the 3D configurations of deformable objects from 2D features. Our focus in this work is to build on the observation that the subspace spanned by the motion parameters is a subset of a smooth manifold, and therefore we hunt for the solution in this space, rather than use heuristics (as previously attempted earlier). We succeed in this by attaching a canonical Riemannian metric, and using a variant of the non-rigid factorisation algorithm for Structure from Motion. We qualitatively and quantitatively show that our algorithm produces better results when compared to the state of art.","Shape,
Cameras,
Optimization methods,
Manifolds,
Computer science,
Focusing,
Joining processes,
Art,
Encoding,
Noise shaping"
Saving 200kW and $200 K/year by power-aware job/machine scheduling,"This paper reports our 3.75-year empirical study on power-aware operations of Kyoto University’s supercomputer system. The supercomputer system of 10 TFlops had required about 540 kW on average in its first fiscal year 2004. After that and one-year try-and-error of power efficient operation, we implemented a simple but effective scheduler of jobs and machine powering to improve the perload power efficiency by up to 39% and to save 200 kW and $200,000 electric charge in the fiscal year 2006. The power-aware scheduler tries to minimize the number of active nodes in the system of eleven nodes keeping sufficient computational power for given loads. Thus the power-aware scheduler has not degraded, but has significantly improved, the service quality in terms of the average job-waiting time.","Supercomputers,
Processor scheduling,
Energy consumption,
Power engineering computing,
High performance computing,
Hardware,
Degradation,
Computer science,
Power engineering and energy,
Space charge"
Everybody Share: The Challenge of Data-Sharing Systems,"Data sharing is increasingly important in modern society, yet researchers typically focus on a single technology, such as Web services, or explore only one aspect, such as semantic integration. The authors propose a technology-neutral framework for characterizing data sharing problems and solutions and discuss open research challenges.","Tellurium,
Terrorism,
Read only memory,
PROM"
Schema Versioning in Multi-temporal XML Databases,"Schema evolution keeps only the current data and the schema version after applying schema changes. On the contrary, schema versioning creates new schema versions and preserves old schema versions and their corresponding data. Much research work has recently focused on the problem of schema evolution in XML databases, but less attention has been devoted to schema versioning in such databases. In this paper, we present an approach for schema versioning in multi-temporal XML databases. This approach is based on the XML Schema language for describing XML schema, and is database consistency-preserving.","XML,
Relational databases,
Object oriented databases,
Object oriented modeling,
Spatial databases,
Information science,
Laboratories,
Conference management,
Context modeling,
Data models"
Visualizing Transport Structures of Time-Dependent Flow Fields,"This article focuses on the transport characteristics of physical properties in fluids-in particular, visualizing the finite-time transport structure of property advection. Applied to a well-chosen set of property fields, the proposed approach yields structures giving insights into the underlying flow's dynamic processes.",
Pushing using learned manipulation maps,"Robot haptics ultimately consists of a set of models which interpret and predict a robot’s physical interaction with the world. In this paper, we describe one approach to modeling support friction within a two-dimensional environment consisting of a single robot finger pushing objects on a table. Instead of explicitly modeling the friction distribution between the object and the table, we learn the mapping between pushes and the motion of the object using an online, memory-based model using local regression. The resulting manipulation map implicitly describes the support friction without a complex model. We also describe methods of acquiring object shape and localizing the object using a proximity sensor. Results are presented for objects with different friction distributions.",
UAV Autopilot Integration and Testing,The development of an Unmanned Aerial Vehicle (UAV) platform and the integration of avionics for a search and rescue UAV is examined. The project follows the guidelines for the UAV Challenge - Outback Rescue which is an international aerospace competition. The selection process for a commercial autopilot and avionics package is described. The selected system is integrated into a standard hobby remote control aircraft and configured for autonomous flight and navigation. The autopilot system must be tuned to the aircraft platform and flight characteristics. Flight tests are described for a GPS-based grid search pattern.,"Unmanned aerial vehicles,
Testing,
Aerospace electronics,
Aircraft navigation,
Packaging,
Aerospace control,
Australia,
Satellite ground stations,
Aerospace engineering,
Control systems"
A new fuzzy interpolative reasoning method based on interval type-2 fuzzy sets,"Fuzzy rule interpolation plays an important role in sparse fuzzy rule-based systems. In this paper, we present a new method for handling fuzzy rule interpolation in sparse fuzzy rule-based systems based on interval type-2 fuzzy sets. The proposed method handles fuzzy rule interpolation based on the principle membership functions and the uncertainty grade functions of interval type-2 fuzzy sets. The proposed method can handle fuzzy rule interpolation with polygonal interval type-2 fuzzy sets. It also can handle fuzzy rule interpolation with multiple antecedent variables and can generate reasonable fuzzy interpolative reasoning results in sparse fuzzy rule-based systems based on interval type-2 fuzzy sets.","Fuzzy sets,
Fuzzy reasoning,
Fuzzy systems,
Interpolation,
Knowledge based systems,
Uncertainty,
Computer science,
Fuzzy logic"
Shot Boundary Detection: An Information Saliency Approach,"This paper proposes a new approach for shot boundary detection using information saliency. Both temporal and spatial saliency are considered to generate an information saliency map (ISM). The shot detection (both abrupt changes and gradual transitions) is then based on the change of saliency, Six publicly available video databases are used for evaluation and the results are encouraging. The overall performance of the proposed method outperforms two commercial software, namely VideoAnnex and VCM.","Gunshot detection systems,
Lighting,
Cameras,
Information theory,
Signal processing,
Computer science,
Sun,
Spatial databases,
Software performance,
Histograms"
Variations on the theme of the Witsenhausen counterexample,"This is a semi-tutorial paper that places Witsenhausen’s celebrated 1968 counterexample within a broad class of dynamic decision problems with nonclassical information, which includes stochastic linear-quadratic Gaussian (LQG) teams as well as LQG zero-sum stochastic games. For a fixed (nonclassical) information structure, there are instances (depending on the structure of the objective function) when linear policies are optimal and other instances (including Witsenhausen’s counterexample) when the optimal policies are nonlinear. The paper discusses these instances, optimality as well as saddle-point property (in the case of zero-sum games) of linear policies, and implications of these results for general multi-stage decision problems with specific information structures. It also discusses possible extensions to nonzero-sum stochastic dynamic games where the solution concept is Nash equilibrium.","Stochastic processes,
Control systems,
Shape control,
Cost function,
Random variables,
Shape measurement,
Optimal control,
Probability distribution,
Stochastic systems,
Nash equilibrium"
Computing the Tutte Polynomial in Vertex-Exponential Time,"The deletion–contraction algorithm is perhapsthe most popular method for computing a host of fundamental graph invariants such as the chromatic, flow, and reliability polynomials in graph theory, the Jones polynomial of an alternating link in knot theory, and the partition functions of the models of Ising, Potts, and Fortuin–Kasteleyn in statistical physics. Prior to this work, deletion–contraction  was also the fastest known general-purpose algorithm for these invariants, running in time roughly proportional to the number of spanning trees in the input graph.Here, we give a substantially faster algorithm that computes the Tutte polynomial—and hence, all the aforementioned invariants and more—of an arbitrary graph in time within a polynomial factor of the number of connected vertex sets. The algorithm actually evaluates a multivariate generalization of the Tutte polynomial by making use of an identity due to Fortuin and Kasteleyn. We also provide a polynomial-space variant of the algorithm and give an analogous result for Chung and Graham's cover polynomial.","Polynomials,
Computer science,
Partitioning algorithms,
Physics computing,
Graph theory,
Tree graphs,
Quantum computing,
Approximation algorithms,
Reliability theory,
Information technology"
Region Sampling: Continuous Adaptive Sampling on Sensor Networks,"Satisfying energy constraints while meeting performance requirements is a primary concern when a sensor network is being deployed. Many recent proposed techniques offer error bounding solutions for aggregate approximation but cannot guarantee energy spending. Inversely, our goal is to bound the energy consumption while minimizing the approximation error. In this paper, we propose an online algorithm, Region Sampling, for computing approximate aggregates while satisfying a pre-defined energy budget. Our algorithm is distinguished by segmenting a sensor network into partitions of non-overlapping regions and performing sampling and local aggregation for each region. The sampling energy cost rate and sampling statistics are collected and analyzed to predict the optimal sampling plan. Comprehensive experiments on real-world data sets indicate that our approach is at a minimum of 10% more accurate compared with the previously proposed solutions.","Sampling methods,
Aggregates,
Computer science,
Power engineering and energy,
Energy consumption,
Partitioning algorithms,
Buildings,
Condition monitoring,
Technical Activities Guide -TAG,
Adaptive systems"
System-Level Early Power Estimation for Memory Subsystem in Embedded Systems,"Early power estimation is important to guide architectural design, especially for embedded systems. Since the power consumption of memory subsystem dominates, while DRAM and NAND Flash are the two main storage mediums nowadays, we analyze the power model of DRAM and propose a power model for NAND Flash, considering its system-level behaviors. Experimental results show that the accuracy of model proposed can be up to 95% .","Embedded system,
Random access memory,
Power system modeling,
Read-write memory,
Energy consumption,
Costs,
Process control,
Embedded computing,
Chaos,
Computer science"
Towards Formalizing UML Activity Diagrams in CSP,"The UML Activity diagrams (ADs), are lack of formal semantics in UML official specifications and therefore they cannot be performed formal system behavior analysis. This paper firstly employs the Hoare's CSP (Communicating Sequential Processes) to formalize the behaviors of UML ADs and hence it can provide an approach to model checking UML ADs during software analysis or design stage since CSP is supported by model-checkers such as FDR.","Unified modeling language,
Computer science,
Erbium,
Performance analysis,
Software design,
Formal languages,
System recovery"
From WebEx to NavEx: Interactive Access to Annotated Program Examples,"This paper reviews our work on providing students interactive access to annotated program examples. We review our experience with WebEx, the system that allows students to explore examples line by line. After that we present NavEx, an adaptive environment for accessing interactive programming examples. NavEx enhances WebEx with a specific kind of adaptive navigation support known as adaptive annotation. The classroom study of NavEx discovered that adaptive navigation support can visibly increase student motivation to work with nonmandatory educational content. NavEx boosted the overall amount of work done and the average length of a session. In addition, various features of NavEx were highly regarded by the students.","Programming profession,
Navigation,
Problem-solving,
Computer languages,
Adaptive systems,
Computer science education,
Educational technology,
Hypertext systems,
Feedback,
Technological innovation"
A fast Automatic Gain Control scheme for IEEE 802.15.4 receiver,"The requirements for IC core based on IEEE 802.15.4 are fast computing and low complexity which challenge the traditional Automatic Gain Control (AGC) design. This paper presents a fast AGC scheme for IEEE 802.15.4 receiver, which can quickly accomplish gain adjustment in less than 1.5 symbols. Relating to numeric constant parameters determining the gain convergence speed and the number of samples beyond given thresholds, exponential non-linear adjustment functions are employed. For signals with large dynamic range, the proposed AGC scheme can adjust signals quickly and accurately. By achieving the full scale of Analog-to-Digital (A/D) converter, the proposed scheme can work well in low Signal-to-Noise Ratio (SNR) environment with low complexity.","IEEE 802.15.4 receiver,
AGC,
exponential adjustment function"
Selection of Bit-Rate for Wireless Network Coding,"Network coding is known to improve throughput by mixing information from different flows and conveying more information in each transmission. Recently some proposals have demonstrated the benefits of applying network coding to wireless networks with broadcast transmissions. It is expected that the opportunities for coding and the corresponding gains depend on the bit-rate chosen for determining routes and transmitting packets. However, the previous work on wireless network coding assumed a fixed rate and did not explicitly account for the interaction between rate selection and coding gain. In this paper, we define a new metric, expected coded time (ECT), that measures the total time needed by a node to deliver two packets to their receivers given the bit-rate for transmitting coded packets. We then investigate how the optimal bit-rate for coded packets differs from that for transmission of native packets individually. We also study the performance of network coding under different fixed bit-rates for the whole network. Our evaluation shows that 11 Mbps is the best default fixed rate for MIT Roofnet and 5.5 Mbps is mostly the optimal rate to transmit coded packets when the ideal individual bit-rate for each receiver is different.","Wireless networks,
Network coding,
Throughput,
Broadcasting,
Bit rate,
Signal to noise ratio,
Computer science,
Proposals,
Electrical capacitance tomography,
Time measurement"
Intermediate checkpointing with conflicting access prediction in transactional memory systems,"Transactional memory systems promise to reduce the burden of exposing thread-level parallelism in programs by relieving programmers from analyzing complex inter-thread dependences in detail. By encapsulating large program code blocks and executing them as atomic blocks, dependence checking is deferred to run-time at which point one of many conflicting transactions will be committed whereas the others will have to roll-back and re-execute. In current proposals, a checkpoint is taken at the beginning of the atomic block and all execution can be wasted even if the conflicting access happens at the end of the atomic block In this paper, we propose a novel scheme that (1) predicts when the first conflicting access occurs and (2) inserts a checkpoint before it is executed. When the prediction is correct, the only execution discarded is the one that has to be re-done. When the prediction is incorrect, the whole transaction has to be re-executed just as before. Overall, we find that our scheme manages to maintain high prediction accuracy and leads to a quite significant reduction in the number of lost cycles due to roll-backs; the geometric mean speedup across five applications is 16%.","Checkpointing,
Proposals,
Yarn,
Hardware,
Access protocols,
Computer science,
Parallel processing,
Programming profession,
Runtime,
Accuracy"
Short Proofs May Be Spacious: An Optimal Separation of Space and Length in Resolution,"A number of works have looked at the relationship between length and space of resolution proofs. A notorious question has been whether the existence of a short proof implies the existence of a proof that can be verified using limited space.In this paper we resolve the question by answering it negatively in the strongest possible way. We show that there are families of 6-CNF formulas of size n, for arbitrarily large n, that have resolution proofs of length O(n) but for which any proof requires space Omega(n / log n).  This is the strongest asymptotic separation possible since any proof of length O(n) can always be transformed into a proof in space O(n / log n).Our result follows by reducing the space complexity of so called pebbling formulas over a directed acyclic graph to the black-white pebbling price of the graph.The proof is somewhat simpler than previous results (in particular, those reported in [Nordstrom 2006, Nordstrom and Hastad 2008]) as it uses a slightly different flavor of pebbling formulas which allows for a rather straightforward reduction of proof space to standard black-white pebbling price.","Space technology,
Computer science,
Extraterrestrial measurements,
Length measurement,
Size measurement,
Polynomials"
Body Area Networks: Radio channel modelling and propagation characteristics,"Many current and future wireless devices are wearable and use the human body as a carrier. This has made the body an important part of the transmission channel of these wireless devices. Inclusion of the body as a transmission channel will see future wireless networks rely heavily on Body Area Networks (BAN). BAN will be used not only in medical applications but also in personal area network applications. In order to build BAN devices, it is imperative to model the channel accurately. Channel measurements are important, however, a closer look on the body channel can only be attained through Electromagnetic (EM) propagation modelling. This paper presents an analytical EM channel model for BAN. Specifically, the dyadic Green's function for a cylindrical human body model is used to propose a simple channel model. Four possible cases are considered, where the transmitter and receiver are either inside or outside the body. An exact analytical expression is derived for the case where both the transmitter and receiver are outside the body. This case is then used to show how the received signal power varies around the body, with the receiver at a constant radial distance from the cylindrical axis of the body.","Body area networks,
Biological system modeling,
Body sensor networks,
Humans,
Radio transmitters,
Wireless personal area networks,
Medical services,
Biomedical equipment,
Personal area networks,
Electromagnetic measurements"
Quality Induced Fingerprint Identification using Extended Feature Set,"Automatic fingerprint identification systems use level-1 and level-2 features for fingerprint identification. However, forensic examiners utilize inherent level-3 details along with level-2 features. Existing level-3 feature extraction algorithms are computationally expensive to be used for identification. This paper presents a novel algorithm for fast level-3 feature extraction and identification. The algorithm starts with computing local image quality score using redundant discrete wavelet transform. A fast curve evolution algorithm is then used to extract four level-3 features namely, pores, ridge contours, dots, and incipient ridges. Along with level-1 and level-2 features, these level-3 features are used in a Delaunay triangulation based indexing algorithm. Finally, quality-based likelihood ratio is used to further improve the identification performance. Experiments conducted on a high resolution fingerprint database containing rolled, slap and latent images indicate that the algorithm offers significant benefits for fast fingerprint identification.","Fingerprint recognition,
Feature extraction,
Image matching,
Image quality,
Discrete wavelet transforms,
Image resolution,
Forensics,
Indexing,
Image databases,
Spatial databases"
Joint coding for flash memory storage,"Flash memory is an electronic non-volatile memory with wide applications. Due to the substantial impact of block erasure operations on the speed, reliability and longevity of flash memories, writing schemes that enable data to be modified numerous times without incurring the block erasure is desirable. This requirement is addressed by floating codes, a coding scheme that jointly stores and rewrites data and maximizes the rewriting capability of flash memories. In this paper, we present several new floating code constructions. They include both codes with specific parameters and general code constructions that are asymptotically optimal. We also present bounds to the performance of floating codes.","Construction industry,
Indexes,
Encoding,
Flash memory,
Indexing,
Error correction codes,
Nonvolatile memory"
Elucidating Factors that Can Facilitate Veridical Spatial Perception in Immersive Virtual Environments,"Ensuring veridical spatial perception in immersive virtual environments (IVEs) is an important yet elusive goal. In this paper, we present the results of two experiments that seek further insight into this problem. In the first of these experiments, initially reported in Interrante, Ries, Lindquist, and Anderson (2007), we seek to disambiguate two alternative hypotheses that could explain our recent finding (Interrante, Anderson, and Ries, 2006a) that participants appear not to significantly underestimate egocentric distances in HMD-based IVEs, relative to in the real world, in the special case that they unambiguously know, through first-hand observation, that the presented virtual environment is a high-fidelity 3D model of their concurrently occupied real environment. Specifically, we seek to determine whether people are able to make similarly veridical judgments of egocentric distances in these matched real and virtual environments because (1) they are able to use metric information gleaned from their exposure to the real environment to calibrate their judgments of sizes and distances in the matched virtual environment, or because (2) their prior exposure to the real environment enabled them to achieve a heightened sense of presence in the matched virtual environment, which leads them to act on the visual stimulus provided through the HMD as if they were interpreting it as a computer-mediated view of an actual real environment, rather than just as a computer-generated picture, with all of the uncertainties that that would imply. In our second experiment, we seek to investigate the extent to which augmenting a virtual environment model with faithfully-modeled replicas of familiar objects might enhance people's ability to make accurate judgments of egocentric distances in that environment.",
Tech-note: Dynamic Dragging for Input of 3D Trajectories,"We present Dynamic Dragging, a virtual reality (VR) technique for input of smooth 3D trajectories with varying curvature. Users ""drag"" a virtual pen behind a hand-held tracked stylus to sweep out curving 3D paths in the air. Previous explorations of dragging-style input have established its utility for producing controlled, smooth inputs relative to freehand alternatives. However, a limitation of previous techniques is the reliance on a fixed-length drag line, biasing input toward trajectories of a particular curvature range. Dynamic Dragging explores the design space of techniques utilizing an adaptive drag line that adjusts length dynamically based on the local properties of the input, such as curvature and drawing speed. Finding the right mapping from these local properties to drag line length proves to be critical and challenging. Three potential mappings have been explored, and results of informal evaluations are reported. Initial findings indicate that Dynamic Dragging makes input of many styles of 3D curves easier than traditional drag-style input, allowing drag techniques to approach the flexibility for varied input of more sophisticated and much harder to learn techniques, such as two-handed tape drawing.","Virtual reality,
Computer graphics,
Space exploration,
Filtering,
Shape control,
Computer science,
Dynamic range,
Smoothing methods,
Trajectory,
Path planning"
A discriminant color space method for face representation and verification on a large-scale database,"In a wide range of color-related computer vision applications, researchers tried to select one of the conventional color spaces as the optimum one. This paper, however, addresses the problem of how to learn an optimum color space from the given training sample set. We seek a set of optimal coefficients to combine the R, G and B components based on a discriminant criterion and then gain one discriminant color component for representing color image for recognition purposes. Further, we can obtain three sets of optimal combination coefficients and use them to generate a three-dimensional discriminant color space (DCS). The proposed DCS method was assessed on Experiment 4 of the Face Recognition Grand Challenge (FRGC) database and the experimental results show the proposed discriminant color space significantly outperforms the RGB and Ig(r-g) color spaces.",
A Hybrid Genetic Learning Algorithm for Pi-Sigma Neural Network and the Analysis of Its Convergence,"This paper uses a hybrid genetic learning algorithm to train Pi-sigma neural network and this algorithm was once applied to resolve a function optimizing problem. The hybrid genetic  learning algorithm incorporates the stronger global search of genetic algorithm into the stronger local search of flexible polyhedron method, and can search out the global optimum faster than standard genetic algorithm. The experiments show that the hybrid genetic algorithm can achieve better performance. At last, the hybrid genetic algorithm is proved converge to the global optimum with the probability of 1.","Neural networks,
Algorithm design and analysis,
Convergence,
Genetic algorithms,
Genetic mutations,
Biological cells,
Displays,
Computer networks,
Educational institutions,
Computer science"
The American Sign Language Lexicon Video Dataset,"The lack of a written representation for American Sign Language (ASL) makes it difficult to do something as commonplace as looking up an unknown word in a dictionary. The majority of printed dictionaries organize ASL signs (represented in drawings or pictures) based on their nearest English translation; so unless one already knows the meaning of a sign, dictionary look-up is not a simple proposition. In this paper we introduce the ASL Lexicon Video Dataset, a large and expanding public dataset containing video sequences of thousands of distinct ASL signs, as well as annotations of those sequences, including start/end frames and class label of every sign. This dataset is being created as part of a project to develop a computer vision system that allows users to look up the meaning of an ASL sign. At the same time, the dataset can be useful for benchmarking a variety of computer vision and machine learning methods designed for learning and/or indexing a large number of visual classes, and especially approaches for analyzing gestures and human communication.","Handicapped aids,
Dictionaries,
Video sequences,
Vocabulary,
Computer science,
Computer vision,
Data engineering,
Learning systems,
Design methodology,
Machine learning"
Traffic sign classification using invariant features and Support Vector Machines,This paper presents a novel approach to recognize traffic signs using invariant features and support vector machines (SVM). Images of traffic signs are collected by a digital camera mounted in a vehicle. They are color segmented and all objects which represent signs are extracted and normalized to 36 x 36 pixels images. Invariant features of sign rims and speed-limit sign interiors of 350 and 250 images are computed and the SVM classifier is trained with these features. Two stages of SVM are trained; the first stage determines the shape of sign rim and the second determines the pictogram of the sign. Training and testing of both SVM classifiers are done using still images. The best performance achieved is 98% for sign rims and 93% for speed limit signs.,
Harmonic Detection in Electric Power System Based on Wavelet Multi-resolution Analysis,"Because of widespread uses of nonlinear load, electric power system network is injected with much harmonic current, which does great harm to consumer equipment. In order to prevent harmonic current from influencing safety of system¿s operation, we should know well how much distorted harmonic wave contained and take corresponding measurement to control or compensate it. Inthis paper, based on a kind of multi-resolution wavelet method, we use seven levels of restructuring through using Daubechies wavelet (db24) to decompose the electric current signal into fundamental wave and higher harmonic. By experiment analysis of simulation software MATLAB, separated fundamental wave error is less than 1%, which realizes harmonic effective tracking and gives accuracy compensation by making use of total distorted component.","Power system harmonics,
Wavelet analysis,
Harmonic analysis,
Safety,
Harmonic distortion,
Distortion measurement,
Current measurement,
Control systems,
Analytical models,
MATLAB"
A Cooperative Retransmission Scheme for IR-UWB networks,"We design a cooperative retransmission scheme in the MAC layer which utilizes the UWB unique properties such as fine ranging and immunity to small scale fading in order to exploit the multiuser diversity in UWB networks. We analyze the optimal cooperation strategy to maximize the system throughput in proactive and reactive settings. We also perform simulations to show that the proposed UWB-based Coperative Retransmission Scheme, (UCoRS), achieves a considerable diversity gain in spite of its implementation simplicity. UCoRS also minimizes the number of control packets that is required for cooperation in order to provide energy efficiency in the UWB receivers.",
Rescheduling co-allocation requests based on flexible advance reservations and processor remapping,"Large-scale computing environments, such as TeraGrid, Distributed ASCI Supercomputer (DAS), and Grid’5000, have been using resource co-allocation to execute applications on multiple sites. Their schedulers work with requests that contain imprecise estimations provided by users. This lack of accuracy generates fragments inside the scheduling queues that can be filled by rescheduling both local and multi-site requests. Current resource co-allocation solutions rely on advance reservations to ensure that users can access all the resources at the same time. These coallocation requests cannot be rescheduled if they are based on rigid advance reservations. In this work, we investigate the impact of rescheduling co-allocation requests based on flexible advance reservations and processor remapping. The metascheduler can modify the start time of each job component and remap the number of processors they use in each site. The experimental results show that local jobs may not fill all the fragments in the scheduling queues and hence rescheduling co-allocation requests reduces response time of both local and multi-site jobs. Moreover, we have observed in some scenarios that processor remapping increases the chances of placing the tasks of multi-site jobs into a single cluster, thus eliminating the inter-cluster network overhead.","Time factors,
Estimation,
Runtime,
Schedules,
Resource management,
Processor scheduling,
Supercomputers"
Using cluster computing to support automatic and dynamic database clustering,"Query response time is the number one metrics when it comes to database performance. Because of data proliferation, efficient access methods and data storage techniques have become increasingly critical to maintain an acceptable query response time. Retrieving data from disk is several orders of magnitude slower than retrieving it from memory, it is easy to see the direct correlation between query response time and the number of disk I/Os. One of the common ways to reduce disk I/Os and therefore improve query response time is database clustering, which is a process that partitions the database vertically (attribute clustering) and/or horizontally (record clustering). A clustering is optimized for a given set of queries. However in dynamic systems the queries change with time, the clustering in place becomes obsolete, and the database needs to be re-clustered dynamically. This paper presents an efficient algorithm for attribute clustering that dynamically and automatically generates attribute clusters based on closed item sets mined from the attributes sets found in the queries running against the database. The paper then discusses how this algorithm can be implemented using the cluster computing paradigm to reduce query response time even further through parallelism and data redundancy.","Clustering algorithms,
Databases,
Partitioning algorithms,
Data mining,
Time factors,
Memory,
Humans"
Conditional entropy of non-binary LDPC codes over the BEC,"We consider transmission over the binary erasure channel (BEC) using non-binary LDPC codes. We generalize the concept of stopping sets to non-binary LDPC codes. We give a combinatorial characterization of decoding failures for non-binary LDPC codes decoded via Belief Propagation (BP). Using the density evolution analysis, we compute the asymptotic residual degree distribution for non-binary LDPC codes. In order to show that asymptotically almost every code in the non-binary LDPC ensemble has a rate equal to the design rate, we generalize the arguments of Measson, Montanari, and Urbanke to the non-binary setting. This generalization enables us to compute the conditional entropy of non-binary LDPC codes. We observe that the Maxwell construction of Measson, Montanari, and Urbanke relating the performance of MAP and BP decoding, holds in the setting of non-binary LDPC codes.","Decoding,
Parity check codes,
Integrated circuits,
Entropy,
Iterative decoding,
Evolution (biology),
Construction industry"
Investigating the separability of features from different views for gait based gender classification,"In this paper, we investigate the efficiency of different view angles when classifying gender with gait biometrics for the first time. A gait database is built for this purpose in which walking videos are recorded at seven different views for each subject. Then, we employ a robust gait representation method to extract gait features. The class separability of these features from different view angles are analyzed and compared. A set of experiments are designed to evaluate the performance of gait based gender classification along with the changes of view angle. The experimental results show that 0° and 180° are the worst view angles in this two-category case and the 90° view dose not perform the best, unlike it takes the best performance in gait recognition.","Humans,
Legged locomotion,
Spatial databases,
Biometrics,
Robustness,
Feature extraction,
Data analysis,
Data mining,
Image recognition,
Image processing"
A Model for Differentiated Service Support in Wireless Multimedia Sensor Networks,"Different network applications need different Quality of Service (QoS) requirements such as packet delay, packet loss, bandwidth and availability. It is important to develop a network architecture which is able to guaranty quality of service requirements for high priority traffic. In Wireless Multimedia Sensor Networks (WMSNs), a sensor node may have different kinds of sensor which gather different types of data, with differing levels of importance. We argue that the sensor networks should be willing to spend more resources in disseminating packets that carry more important information. Some applications of WMSNs need to send real time traffic toward the sink node. This real time traffic requires low latency and high reliability so that immediate remedial and defensive actions can be taken, where necessary. Similar to wired networks, service differentiation in wireless sensor networks is also very important. In this paper we propose a differentiated service model for WMSNs. The proposed model can provide requested quality of service for high priority real time classes. In the proposed model, we distinguish high priority real time traffic from the low priority non-real time traffic, and input traffic streams are then serviced based on their priorities. Simulation results confirm the efficiency of the proposed model.",
Integration Starts on Day One in Global Software Development Projects,"Since 2005, Pace University, Delhi University and the Institute of Technology of Cambodia have been partnering to offer students the opportunity to work on globally distributed software development projects. The innovative collaborative model has evolved towards an emphasis on technology mashups for development and communication, mentoring and auditing for assuring quality, and team and software integration for right-sourcing. This paper describes a project where students working in sub-teams were required to integrate their sub-components as a single system for a Cambodian environment. Furthermore, a well-defined design sub-component was subject to a competitive bidding process in an attempt to enhance quality though design diversity. The paper reports on our findings and summarizes the dos and don'ts associated with integration. Both team and software integration needs careful attention from day one on a project, a finding that has repercussions for educational and industrial practice.","Software,
Programming,
Databases,
Lead,
Education,
Computer science,
Planing"
Efficient photometric stereo on glossy surfaces with wide specular lobes,"This paper presents a new photometric stereo method aiming to efficiently estimate BRDF and reconstruct glossy surfaces. Rough specular surfaces exhibit wide specular lobes under different lightings. They are ubiquitous and usually bring difficulties to both specular pixel removal and surface normal recovery. In our approach, we do not apply unreliable highlight separation and specularity estimation. Instead, an important visual cue, i.e. the cast shadow silhouette of the object, is employed to optimally recover global BRDF parameters. These parameter estimates are then taken into a reflectance model for robustly computing the surface normals and other local parameters using an iterative optimization. Within the unified framework, our method can also be used to reconstruct object surfaces assembled with multiple materials.","Photometry,
Surface reconstruction,
Rough surfaces,
Surface roughness,
Reflectivity,
Image reconstruction,
Brain modeling,
Robustness,
Optical reflection,
Stereo image processing"
One-Time Key Authentication Protocol for PMIPv6,"We are now going to the 4G network and in the 4G network environment, there are so many devices connected to the Internet while they move. We have protocol that can support movement of communicating node without any disruption of their connection status named Mobile IP (MIP). But, the major problem of this MIP is too heaviness of the protocol for small mobile nodes. So, IETF now propose Proxy MIP to solve this problem. But, there is no way to authenticate the mobile node in PMIP. In this paper, we propose new authentication protocol for PMIPv6 and show the results of analysis. With our proposed protocol, we can give a lot of securing features to current PMIPv6.","Authentication,
Protocols,
Home automation,
Information technology,
Computer science,
IP networks,
Terminology,
Security,
Routing,
Mobile radio mobility management"
A new edge-based text verification approach for video,"In this paper, we propose a new edge-based text verification approach for video. Based on the investigation of the relation between candidate blocks and their neighbor areas, the proposed approach first detects background edges in candidate blocks, then erases them by an edge tracking technique, and finally the candidate blocks containing too few remaining edges are eliminated as false alarms. Three measures for text detection evaluation in video were used to assess the performance of the proposed text verification approach. Experimental results on 50 broadcast news video clips demonstrate the validity of our approach.",
A Detailed Comparison of Probabilistic Approaches for Coping with Unfair Ratings in Trust and Reputation Systems,"The unfair rating problem exists when a buying agent models the trustworthiness of selling agents by also relying on ratings of the sellers from other buyers. Different probabilistic approaches have been proposed to cope with this issue. In this paper, we first summarize these approaches and provide a detailed categorization of them. This includes our own ""personalized"" approach for addressing this problem. Based on the implication of such analysis, we then focus on experimental comparison of our approach with two key models in a framework that simulates a dynamic electronic marketplace environment. We specifically examine different scenarios, including ones where the majority of buyers are dishonest, buyers lack personal experience with sellers, sellers may vary their behavior, and buyers may provide a large number of ratings. Our study provides the basis for deciding which approach is most appropriate to employ, in which scenario.","Bayesian methods,
Probabilistic logic,
Probability density function,
Prediction algorithms,
Floods,
Servers,
Consumer electronics"
Extraction of Building Heights from VHR SAR Imagery using an Iterative Simulation and Match Procedure,"The new spaceborne very high resolution (VHR) SAR sensors onboard the TerraSAR-X and COSMO-SkyMED satellites have a spatial resolution of up to 1 meter. In VHR SAR data, features from individual urban structures (like buildings) can be identified in their characteristic settings in urban settlement patterns. In this paper, we present a novel methodology for the height estimation for generic man made structures from single power SAR data. The proposed approach is based on the definition of a hypothesis on the height of the building and on the simulation of a SAR image for that hypothesis. Then a matching procedure is applied between the estimated and the actual SAR images in order to validate the height assumption. The process is iterated for different initial height assumptions until the matching function is satisfied and thus the building height is estimated. The efficiency and the properties of the proposed method are demonstrated for the height estimation of flat- and gable roof buildings from a VHR airborne SAR scene and for a pyramid in Giza (Egypt) from VHR TerraSAR-X SPOT beam data.","Buildings,
Spatial resolution,
Sensor phenomena and characterization,
Data mining,
Image sensors,
Layout,
Image reconstruction,
Computational modeling,
Computer simulation,
Protection"
Spread Spectrum Watermark for Color Image Based on Wavelet Tree Structure,"This paper proposes a spread spectrum watermark for color image based on wavelet tree structure. It embeds a pseudo-random sequence representing one bit of the original watermark into a four-fork tree in the DWT of Y component of YUV. This distributes the information of one bit into several frequency sub-bands, such as low, intermediate and high frequency sub-band, which improves the robustness of watermark against different attacks. In addition, we apply denoising method to watermark image extracted from the attacked watermarked image to remove the salt-and-pepper noise introduced by the attacks to host image, which improves the NC of watermark. Experimental results prove that the algorithm is very robust to image processing attacks such as JPEG compression, Gaussian noise, filtering, cropping and contrast enhancement, and it has a good transparency.","Spread spectrum communication,
Watermarking,
Color,
Tree data structures,
Frequency,
Noise robustness,
Discrete wavelet transforms,
Noise reduction,
Data mining,
Filtering algorithms"
Arterial Pulse Rate Variability analysis for diagnoses,"Heart rate variability (HRV) provides an estimate of sympathetic and parasympathetic influences on the heart rate. Although HRV has been extensively studied, sustained clinical use is still outstanding. The noninvasive, convenient, and inexpensive arterial pulse originate from heartbeats, but has not been studied in a systematic fashion except in rudimentary ways. In this paper, we present Pulse Rate Variability (PRV) as an alternative to HRV. We give evidence for the detection of disorders in patients using PRV, paving the way for future clinical use.","Heart rate variability,
Frequency estimation,
Frequency conversion,
Hafnium,
Measurement standards,
Heart rate,
Frequency measurement,
Pulse measurements,
Density measurement,
Power measurement"
Attributing events to individuals in multi-inhabitant environments,"Intelligent environment research has resulted in many useful tools such as activity recognition, prediction, and automation. However, most of these techniques have been applied in the context of a single resident. A current looming issue for intelligent environment systems is performing these same techniques when multiple residents are present in the environment. In this paper we investigate the problem of attributing sensor events to individuals in a multi-resident intelligent environment. Specifically, we use a naive Bayesian classifier to identify the resident responsible for a unique sensor event. We present results of experimental validation in a real intelligent workplace testbed and discuss the unique issues that arise in addressing this challenging problem.","pattern classification,
Bayes methods,
home automation,
intelligent sensors,
learning (artificial intelligence)"
Early experiences in application level I/O tracing on blue gene systems,"On todays massively parallel processing (MPP) supercomputers, it is increasingly important to understand I/O performance of an application both to guide scalable application development and to tune its performance. These two critical steps are often enabled by performance analysis tools to obtain performance data on thousands of processors in an MPP system. To this end, we present the design, implementation, and early experiences of an application level I/O tracing library and the corresponding tool for analyzing and optimizing I/O performance on Blue Gene (BG) MPP systems. This effort was a part of IBM HPC Toolkit for BG systems. To our knowledge, this is the first comprehensive application-level I/O monitoring, playback, and optimizing tool available on BG systems. The preliminary experiments on popular NPB BTIO benchmark show that the tool is much useful on facilitating detailed I/O performance analysis.",
Dynamic Deployment of Custom Execution Environments in Grids,"One of the most important obstacles when porting an application to the Grid is its highly heterogeneous nature. This heterogeneity usually means an increaseof the cost of both the application porting cycle and the operational cost of the infrastructure. Moreover, the effective number of resources available to a user are also limited by this heterogeneity. In this paper we presents two approaches to tackle these problems: (i) an straightforward deployment of custom virtual machines  to support the application execution; (ii) and a new architecture to provision computing elements that allows to dynamically adapt them to changing VO demands. Experimental results for both approaches on prototyped testbed are discussed. In particular, the on-demandprovision of computing elements show less than a 11\% overall performance loss including the hypervisor overhead.",
Automating Privacy Compliance with ExPDT,"Today, personalized services are lucrative for service providers and their customers. With their increasing pervasiveness and interconnection, however, customers show concerns about their privacy. If customers were to refuse the processing of their personal data in general, the economic potential of personalized services could not be realized. We claim that such scepticism is a direct consequence of incomplete control mechanisms. To be in line with laws and to help users to control the usage of their personal data, we propose the Extended Privacy Definition Tool (ExPDT) that not only provides customers as well as service providers with a formal language to specify and to compare different policies, but also providers with a monitor tool to enforce the policies within their services.","Data privacy,
Business,
Automation,
Costs,
Computer science,
Telematics,
Waste materials,
Formal languages,
Monitoring,
Contracts"
Dense specular shape from multiple specular flows,"The inference of specular (mirror-like) shape is a particularly difficult problem because an image of a specular object is nothing but a distortion of the surrounding environment. Consequently, when the environment is unknown, such an image would seem to convey little information about the shape itself. It has recently been suggested (Adato et al., ICCV 2007) that observations of relative motion between a specular object and its environment can dramatically simplify the inference problem and allow one to recover shape without explicit knowledge of the environment content. However, this approach requires solving a non-linear PDE (the ‘shape from specular flow equation’) and analytic solutions are only known to exist for very constrained motions. In this paper, we consider the recovery of shape from specular flow under general motions. We show that while the ‘shape from specular flow’ PDE for a single motion is non-linear, we can combine observations of multiple specular flows from distinct relative motions to yield a linear set of equations. We derive necessary conditions for this procedure, discuss several numerical issues with their solution, and validate our results quantitatively using image data.","Shape,
Nonlinear equations,
Motion analysis,
Nonlinear distortion,
Optical reflection,
Surface reconstruction,
Image reconstruction,
Computer science,
Art,
Mirrors"
Coherent ray tracing via stream filtering,"We introduce an approach to coherent ray tracing based on a new stream filtering algorithm. This algorithm, which is motivated by breadth-first ray traversal and elimination of inactive ray elements, exploits the coherence exhibited by processing arbitrarily-sized groups of rays in SIMD fashion. These groups are processed by a series of filters that partition rays into active and inactive subsets throughout the various stages of the rendering process. We present results obtained with a detailed cycle-accurate simulation of a hardware architecture that supports wider-than-four SIMD processing and efficient scatter/gather memory and stream partitioning operations. In this context, stream filtering achieves frame rates of 15–25 fps for scenes of high geometric complexity rendered with path tracing and a variety of advanced visual effects.","Ray tracing,
Filtering,
Coherence,
Hardware,
Computational modeling,
Algorithm design and analysis,
Image generation"
ALPACAS: A Large-Scale Privacy-Aware Collaborative Anti-Spam System,"While the concept of collaboration provides a natural defense against massive spam emails directed at large numbers of recipients, designing effective collaborative anti-spam systems raises several important research challenges. First and foremost, since emails may contain confidential information, any collaborative anti-spam approach has to guarantee strong privacy protection to the participating entities. Second, the continuously evolving nature of spam demands the collaborative techniques to be resilient to various kinds of camouflage attacks. Third, the collaboration has to be lightweight, efficient, and scalable. Towards addressing these challenges, this paper presents ALPACAS - a privacy-aware framework for collaborative spam filtering. In designing the ALPACAS framework, we make two unique contributions. The first is a feature-preserving message transformation technique that is highly resilient against the latest kinds of spam attacks. The second is a privacy-preserving protocol that provides enhanced privacy guarantees to the participating entities. Our experimental results conducted on a real email dataset shows that the proposed framework provides a 10 fold improvement in the false negative rate over the Bayesian-based Bogofilter when faced with one of the recent kinds of spam attacks. Further, the privacy breaches are extremely rare. This demonstrates the strong privacy protection provided by the ALPACAS system.","Large-scale systems,
Collaboration,
Privacy,
Fingerprint recognition,
Information filtering,
Information filters,
Protection,
Collaborative work,
Computer science,
Bayesian methods"
Low-power Vehicle Speed Estimation Algorithm based on WSN,"According to the characteristics of actual traffic stream, an on-road speed estimation model and algorithm based on wireless magnetic sensor networks was researched. In this model, we used 3 sensor nodes working together to estimate the speed of passing vehicle. To achieve long-life of the model, the mode was designed as a hierarchy architecture to reduce the power consumption. Furthermore, we presented a power consumed scheme, Duty-cycling-V, which could take the speed of vehicle queue as parameter for dynamically adjusting the working cycle of the sensor node. And we used it to the key sensor node of the model, which could farther reduce the power consumption. And the results of the emulation and on-road experiments are demonstrated that the vehicle speed captured by the 3 nodes model is more precise and better power efficiency than the 2 nodes detection model.","Wireless sensor networks,
Power system modeling,
Energy consumption,
Traffic control,
Vehicle detection,
Telecommunication traffic,
Infrared detectors,
Intelligent transportation systems,
Magnetic sensors,
Radiation detectors"
Analysis of human attractiveness using manifold kernel regression,"This paper uses a recently introduced manifold kernel regression technique to explore the relationship between facial shape and attractiveness on a heterogeneous dataset of over three thousand images gathered from the Web. Using the concept of the Fréchet mean of images under a diffeomorphic transformation model, we evolve the average face as a function of attractiveness ratings. Examining these averages and associated deformation maps enables us to discern aggregate shape change trends for male and female faces.",
Common Influence Join: A Natural Join Operation for Spatial Pointsets,"We identify and formalize a novel join operator for two spatial pointsets P and Q. The common influence join (CIJ) returns the pairs of points (p, q), pP, qQ, such that there exists a location in space, being closer to p than to any other point in P and at the same time closer to q than to any other point in Q. In contrast to existing join operators between pointsets (i.e., -distance joins and k-closest pairs), CIJ is parameter-free, providing a natural join result that finds application in marketing and decision support. We propose algorithms for the efficient evaluation of CIJ, for pointsets indexed by hierarchical multi-dimensional indexes. We validate the effectiveness and the efficiency of these methods via experimentation with synthetic and real spatial datasets. The experimental results show that a non-blocking algorithm, which computes intersecting pairs of Voronoi cells on-demand, is very efficient in practice, incurring only slightly higher I/O cost than the theoretical lower bound cost for the problem.","Computer science,
Costs,
Data analysis,
Collaboration,
Motion pictures,
Lungs,
Informatics"
Formalizing Service-Oriented Architectures,"Service-oriented architecture (SOA) is defined as a paradigm for organizing and using distributed capabilities that might be under the control of different ownership domains. SOA is also known as a methodology for achieving application interoperability and reuse of IT assets in distributed computing environments characterized as transformable by the visibility, interaction, and effect dimensions.",
A Comparative Evaluation of State-of-the-Art Approaches for Web Service Composition,"In today's Web environment, many enterprises decide to implement and publish their applications on the Internet using Web services technology. In many cases, a single service is not sufficient to fulfill the user's request. To solve this problem, services should be combined together. Therefore, composition of Web services is one of the recent critical issues. A number of approaches have been presented, to tackle this problem. In this paper, we categorize these approaches into four categories (Workflow-based, AI-planning based, Syntactic-based, and Semantic-based). Then, we compare these approaches based on some criteria (like QoS, scalability, and correctness). Investigation of that classification will help researchers who are working on service composition to deliver more applicable solutions.",
Novel Index for Objective Evaluation of Road Detection Algorithms,"Road detection is a relevant task within vision-based systems devoted to assist the driver. Although they have been improved during the last decade, these algorithms are usually validated using qualitative results. Nonetheless, quantitative evaluation is necessary either to enable the comparison between different algorithms or to achieve the optimal performance of a given one. In this paper we present a composite index to quantitatively assess the performance of road detection algorithms. The measure is based on a weighted combination of different evaluations which use a trade-off between precision and recall scores. Obtaining a single index score is a major benefit. It can be used to easily compare algorithms or to properly set their parameters. Moreover, innovatively our proposal includes a human perception criterion to improve its usefulness. Experiments on real-world data corroborate the usefulness of the proposed index.","Detection algorithms,
Image edge detection,
Layout,
Intelligent transportation systems,
Proposals,
Humans,
Cameras,
Object detection,
Road transportation,
Safety"
An efficient adaptive distributed space time coding scheme for cooperative relaying,"A non-regenerative dual-hop wireless system based on a distributed space-time-coding strategy is considered. It is assumed that each relay retransmits an appropriately scaled space-time coded version of received signals. The main goal of this paper is to investigate a power allocation strategy in relay stations using analytical and simulation arguments to satisfy the quality of service requirements. In the high signal-to-noise ratio regime for the relay-destination link, it is shown that the optimum power allocation strategy in each relay which minimizes the outage probability is to remain silent, if its channel gain with the source is less than a prespecified threshold level. The Monte-Carlo simulations show that the near-optimal power allocation scheme in each relay in order to minimize the outage probability or the frame-error rate is the threshold-based on-off power scheme. Also, the numerical results demonstrate a dramatic improvement in the system performance by using this scheme compared to the case that the relay stations forward their received signals with full power. Finally, a hybrid amplify-and-forward/detect-and-forward scheme is numerically evaluated.","Power system relaying,
Frame relay,
Wireless networks,
Fading,
Rayleigh channels,
Quality of service,
Signal to noise ratio,
Degradation,
Mobile antennas,
Bit error rate"
"Morphology, thermal, mechanical and electrical properties of propylene-based materials for cable applications","Crosslinked polyethylene (XLPE) remains the material of choice for modern high voltage extruded cables possessing good thermal stability and excellent electrical properties. However, increasing pressure to limit the environmental effects of human activity and to promote recycling has forced researchers to look to new materials to replace XLPE in the medium to long term. Recently, a number of propylene based systems have been developed through copolymerization with novel catalyst systems which may offer novel alternatives to polyethylene. Such systems offer higher operating temperatures together with potentially desirable thermal, electrical and mechanical properties. In this paper we report on a range of propylene based materials and assess their suitability for cable applications through the use of thermal, mechanical and electrical testing. We supplement these tests with morphological investigations.","Morphology,
Mechanical factors,
Mechanical cables,
Polyethylene,
Voltage,
Thermal stability,
Humans,
Recycling,
Temperature,
Materials testing"
Spike-based acoustic signal processing chips for detection and localization,"Voltage spikes are ubiquitous in biological nervous systems. How spikes can be used to encode signals, facilitate communication, and implement important computations is an important question of contemporary neuroscience. Acoustic processing tasks provide a rich range of applications for this encoding scheme. As a summary of the Ph.D. research of the first author, we present two analog VLSI spike-based example systems that process acoustic information using spikes: a model of the neural signal processing involved in bat echolocation, and a low-power, time-domain acoustic periodicity detector.","Acoustic signal processing,
Acoustic signal detection,
Voltage,
Biological information theory,
Nervous system,
Biology computing,
Pervasive computing,
Neuroscience,
Acoustic applications,
Encoding"
Contact Location Estimation from a Nonlinear Array of Pressure Sensors,"In physical medicine and rehabilitation, it is important to be able to collect information regarding a patient's behavior and range ofmobility throughout their daily activities. Grab bars are used widely in the homes of individuals with mobility impairments so their usage while performing physical tasks can provide valuable information as to the individual's physical status. This paper explores the extraction of location information for forces applied to a grab bar embedded with a nonlinear pressure sensor array of low spatial resolution. Itfirst describes the instrumentation of the grab bar and the calibration procedure. It then investigates three methods ofestimating the contact location; a simple centroid, a percentage-based lookup table and an artificial neural network. Results of the three methods are reported based on data collectedfrom differentforces and contact locations applied along the bar The artificial neural network proves to be the most successful method of estimating the points of contact, by most accurately modeling the nonlinearities in the system.",
Importance of Software Architecture during Release Planning,"Release planning is the process of deciding what to include in future release(s) of a product. In this paper we look at how software architects are involved during release planning in industry today, and how architectural issues are considered during this phase.",
A Memory-Efficient Hashing by Multi-Predicate Bloom Filters for Packet Classification,"Hash tables (HTs) are poorly designed for multiple off-chip memory accesses during packet classification and critically affect throughput in high-speed routers. Therefore, an HT with fast on-chip memory and high-capacity off-chip memory for predictable lookup-throughput is desirable. Both a legacy HT (LHT) and a recently proposed fast HT (FHT) have the disadvantage of memory overhead due to pointers and duplicate items in linked lists. Also, memory usage for an FHT did not consider the bits in counters for fair comparison with an LHT. In this paper, we propose a novel hash architecture called a Multi-predicate Bloom-filtered HT (MBHT) using parallel Bloom filters and generating off-chip memory addresses in the base- 2x number system, xisin{1,2,hellip}, which removes the overhead of pointers. Using a larger base of number system, an MBHT reduces on-chip memory size by a factor of log2 b2/ log2 b1 where b1 and b2 are bases of number system (b2>b1). Compared to an FHT, the MBHT is approximately x(log2 n + 4)/(2 log2 n) times more efficient for on-chip memory, where n is the number of keys. This results in a significant reduction in the number of off- chip memory accesses. A simulation with a dataset of packets from NLANR shows the on-chip memory reductions by 1.7 and 2 times over an LHT and an FHT are made. Besides, an MBHT of base-16 needs less off-chip memory accesses by 2117 in total URL queries of NLANR, compared to an FHT.","Filters,
Uniform resource locators,
Large-scale systems,
Costs,
Communications Society,
Computer science,
Throughput,
Counting circuits,
System-on-a-chip,
Access control"
Low data rate ultra wideband ECG monitoring system,"This paper presents a successfully implemented wireless electrocardiograph monitoring using low data rate ultra wideband (UWB) transmission. Low data rate ultra wideband is currently under consideration for the newly formed wireless body area network (WBAN) group (IEEE802.15.6) to develop a standard for wireless vital sign monitoring. Maximizing the transmission power of the transmitter and reducing the stringent requirements and complexity of the receiver have always been the key considerations for an UWB transceiver. Multiple pulses per bit has been sent in our low data rate UWB prototype system to increase the transmitter power, to reduce the complexity of the receiver and to ease the requirement on the receiver's analog to digital converter. Non-coherent technique has been used for the demodulation of UWB signals at the receiver that reduces the receiver complexity further.",
Evolution Induced Secondary Immunity: An Artificial Immune System Based Intrusion Detection System,"The analogy between Immune Systems and Intrusion Detection Systems encourage the use of Artificial Immune Systems for anomaly detection in computer networks. This paper describes a technique of applying Artificial Immune System along with Genetic algorithm to develop an Intrusion Detection System. Far from developing Primary Immune Response, as most of the related works do, it attempts to evolve this Primary Immune Response to a Secondary Immune Response using the concept of memory cells prevalent in Natural Immune Systems. A Genetic Algorithm using genetic operators- selection, cloning, crossover and mutation- facilitates this. Memory cells formed enable faster detection of already encountered attacks. These memory cells, being highly random in nature, are dependent on the evolution of the detectors and guarantee greater immunity from anomalies and attacks. The fact that the whole procedure is enveloped in the concepts of Approximate Binding and Memory Cells of lightweight of Natural Immune Systems makes this system reliable, robust and quick responding.","Artificial immune systems,
Intrusion detection,
Immune system,
Detectors,
Genetic algorithms,
Computer science,
Computer networks,
Computer network reliability,
Gene expression,
Phase detection"
An FPGA architecture for CABAC decoding in manycore systems,"Arithmetic coding is an efficient entropy compression method that achieves results close to the entropy limit and it is used in modern standards such as JPEG-2000 and H.264. Arithmetic decoding (AD) in H.264 video coding standard is a sequential task that takes a significant part of computing time. In present and future multicore and manycore systems, AD becomes a bottleneck as it cannot be parallelized, limiting the concurrent execution of other tasks. In this paper, an FPGA-based accelerator is proposed to speed-up AD in H.264 and enable parallel decoding at macroblock and frame levels scaling up to tens or hundreds of cores.","Decoding,
Encoding,
Pipeline processing,
Field programmable gate arrays,
Table lookup,
Complexity theory,
Parallel processing"
Two Novel Technologies for Accessible Math and Science Education,"We are using the results of the study to improve the design of both programs. We plan to repeat these evaluations several times as development of both programs progresses. Evaluation with kindergarten and elementary school deaf children and their teachers will be done in collaboration with the Indiana School for the Deaf and will start in the fall of 2009. We will report the results in a future article. A strong need exists for solutions that allow deaf users to communicate and interact in an environment free of prejudice, stigma, technological barrier, or other obstacles. The fact that all children were able to engage with and complete the tasks in both test systems is encouraging.","Educational technology,
Deafness,
Animation,
Handicapped aids,
Motion control,
Education,
Auditory system,
Speech,
Natural languages,
Character generation"
IPSViz: An After-Action Review Tool for Human-Virtual Human Experiences,"This paper proposes after-action review (AAR) with human-virtual human (H-VH) experiences. H-VH experiences are seeing increased use in training for real-world, H-H experiences. To improve training, the users of H-VH experiences need to review, evaluate, and get feedback on them. AAR enables users to review their H- VH interaction, evaluate their actions, and receive feedback on how to improve future real-world, H-H experiences. The Interpersonal Scenario Visualizer (IPSViz), an AAR tool for H-VH experiences, is presented. IPSViz allows medical students to review their interactions with VH patients. To enable review, IPSViz generates spatial, temporal, and social visualizations of H- VH interactions. Visualizations are generated by treating the interaction as a set of signals. Interaction signals are captured, logged, and processed to generate visualizations for review, evaluation and feedback. In a study (N=27), reviewing the visualizations helped students become self-aware of their actions with a virtual human and gain insight into how to improve interactions with real humans.",
Toward a Comprehensive Model in Internet Auction Fraud Detection,"Fraud detection has become a common concern of the online auction Web sites. Fraudsters often manipulate reputation systems and commit nondelivery fraud. To deal with fraud in group behavior we consider network level features, such as users' beliefs of other users. In this paper we use the loopy belief propagation algorithm and apply it to network level fraud detection, classifying fraudsters, accomplices, as well as honest users. Our method shows good classification accuracy using real data.","Internet,
Marketing and sales,
Feedback,
Face detection,
Machine learning,
Belief propagation,
Social network services,
Computer vision,
Electronic commerce,
Merchandise"
Target tracking in heterogeneous sensor networks using audio and video sensor fusion,"Heterogeneous sensor networks (HSNs) with multiple sensing modalities are gaining popularity in diverse fields. Tracking is an application that can benefit from multiple sensing modalities. If a moving target emits sound then both audio and video sensors can be utilized. These modalities can complement each other in the presence of high background noise that impairs the audio or visual clutter affecting the video. Audio-video tracking can also provide cues for the other modality for actuation. In this paper, we describe an approach for target tracking in urban environments utilizing an HSN of mote class devices equipped with acoustic sensor boards and embedded PCs equipped with web cameras. Our system employs a Markov Chain Monte Carlo Data Association algorithm for tracking vehicles emitting engine noise. Experimental results from a deployment in an urban environment are used to demonstrate our approach.","video signal processing,
audio signals,
clutter,
distributed sensors,
Markov processes,
Monte Carlo methods,
sensor fusion,
target tracking"
The Quest for a Logic Capturing PTIME,"The question of whether there is a logic that captures polynomial time is thecentral open problem in descriptive complexity theory. In my talk, I will review the question and the early, mostly negative results that were obtained until the mid 1990s, and then move on to positive results about capturing polynomial time on specific classes of graphs. This will include recent results on definability in fixed-point logic and graph structure theory. Finally, I will dicuss stronger logics and propose directions for further research.The purpose of this accompanying note is to give the basic definitions in detail, state the main results, mention some open problems, and give a list of references.","Logic,
Polynomials,
Vocabulary,
Complexity theory,
Relational databases,
Database languages,
Terminology,
Computer science,
Robustness,
Mathematical model"
Approximate dynamic programming using support vector regression,"This paper presents a new approximate policy iteration algorithm based on support vector regression (SVR). It provides an overview of commonly used cost approximation architectures in approximate dynamic programming problems, explains some difficulties encountered by these architectures, and argues that SVR-based architectures can avoid some of these difficulties. A key contribution of this paper is to present an extension of the SVR problem to carry out approximate policy iteration by forcing the Bellman error to zero at selected states. The algorithm does not require trajectory simulations to be performed and is able to utilize a rich set of basis functions in a computationally efficient way. Computational results for an example problem are shown.","Dynamic programming,
Function approximation,
Computer architecture,
Costs,
Neural networks,
Space technology,
Computational modeling,
Decision making,
Uncertainty,
Finance"
A wearable device for repetitive hand therapy,"Intensive task-oriented repetitive physical therapies provided by individualized interaction between the patient and a rehabilitation specialist can improve hand motor performance in patients survived from stroke and traumatic brain injury. However, the therapy process is long and expensive and difficult to evaluate quantitatively and objectively. The goal of this research is to develop a novel wearable device for robotic assisted hand repetitive therapy. We designed a pneumatic muscle (PM) driven therapeutic device that is wearable and provides assistive forces required for grasping and release movements. The robot has two distinct degrees of freedom at the thumb and the fingers. The embedded sensors feedback position and force information for robot control and quantitative evaluation of task performance. It has the potential of providing supplemental at-home therapy in addition to in the clinic treatment.",
A fast high quality pseudo random number generator for graphics processing units,Limited numerical precision of nVidia GeForce 8800 GTX and other GPUs requires careful implementation of PRNGs. The Park-Miller PRNG is programmed using G80’s native Value4f floating point in RapidMind C++. Speed up is more than 40. Code is available via ftp cs.ucl.ac.uk genetic/gp-code/random-numbers/gpu_park-miller.tar.gz,"Generators,
Program processors,
Random access memory,
Evolutionary computation,
Arrays,
Software,
Linux"
Modelling and analysis of a three-phase quadrature phase shifter with a hybrid transformer,"This paper deals with a quadrature phase shifter based on a hybrid transformer (HT) for power flow control and for transient stability improvement in AC transmission systems. This paper presents an operational description, a modelling and a theoretical analysis of the properties of the proposed solution. The steady-state analysis is based on the averaged state-space method and four terminal descriptions, and is verified both by means of the simulation and experimental investigations.","state-space methods,
flexible AC transmission systems,
load flow control,
phase control,
phase shifters,
power transformers,
stability"
Efficient hierarchical content distribution using P2P technology,"CDN (Content Delivery Network) and P2P (Peer-to-Peer) are two dominant technologies in large-scale content distribution. However, they have their own advantages and disadvantages. In this paper, an efficient hierarchical content distribution scheme is proposed, integrating the traditional CDN and centralized P2P. In our approach, content distribution is divided into two stages. In the CDN-level core network, the content is strategically placed from central server to a set of edge servers, and edge servers can exchange content between each other to enhance efficiency. In the P2P-level logical access network, the content is distributed from edge server to user nodes, and the user nodes can concurrently download from both the edge server and other peer user nodes. Analytical results are given to demonstrate the performance of HCDN (Hierarchical Content Distribution Network).","Network servers,
Web server,
Peer to peer computing,
Quality of service,
Scalability,
Costs,
Computer architecture,
Large-scale systems,
Performance analysis,
Internet"
Rhythmic similarity of music based on dynamic periodicity warping,"This paper introduces a new way to measure rhythmic similarity between two musical pieces using periodicity spectra. In order to detect similarity for pieces of different tempi, the linearity of the warping path between their spectra serves as a measure of their rhythmic similarity. Using a modified kNN classification approach on two datasets, the proposed measure provides comparable classification accuracy (82.1%) to the best of widely used measures (85.5%) for the first dataset; For the second dataset, which is characterized by a large variance of tempi, the proposed measure outperforms all reference measures, reaching an accuracy of 69.0%, while the best of the other measures reaches 53.8%. Moreover, the presented technique works fully automatically, and no information regarding tempo is needed.","Rhythm,
Computer science,
Shape measurement,
Multiple signal classification,
Informatics,
Linearity,
Music information retrieval,
Fourier transforms,
Bars,
Timing"
Dynamic Spectrum Allocation MAC Protocol Based on Cognitive Radio for QoS Support,"To address inefficiency of the spectrum utilization and the limited available spectrum band, Cognitive Radio has emerged as a significant solution. In this paper, we propose a novel MAC protocol in multi-channel wireless networks by using Dynamic Spectrum Allocation (DSA) based on Cognitive Radio for QoS support. According to the user request, variable spectrum isallocated for quality of service (QoS) guarantee through DSA mechanism. Dynamic spectrum allocation is implemented with the procedure of FRQ/FRP/ACK-hello over a control channel, DATA/ACK over allocated data channels. For cooperative detection, a hello message is exchanged periodically to enhance the spectrum sharing in our MAC protocol. In addition, the results of performance evaluation demonstrate that the proposed DSA-MAC protocol improves the throughput significantly, as compared to the IEEE 802.11 MAC.","Media Access Protocol,
Cognitive radio,
Quality of service,
Access protocols,
Wireless application protocol,
Wireless networks,
FCC,
Educational institutions,
Throughput,
Chromium"
Detecting and Repairing Inconsistencies across Heterogeneous Models,"With the advent of Domain Specific Languages for Model Engineering, detecting inconsistencies between models is becoming increasingly challenging. Nowadays, it is not uncommon for models participating in the same development process to be captured using different modelling languages and even different modelling technologies. We present a classification of the types of relationships that can arise between models participating in a software development process and outline the types of inconsistencies each relationship can suffer from. From this classification we identify a set of requirements for a generic inconsistency detection and reconciliation mechanism and use a case study to demonstrate how those requirements are implemented in the Epsilon Validation Language (EVL), a task-specific language developed in the context of the Epsilon GMT component.","Context modeling,
Domain specific languages,
Software testing,
Computer science,
Programming,
Prototypes,
Project management,
Technology management,
Engineering management,
Maintenance engineering"
An evaluation of the benefits of look-ahead in Pac-Man,"The immensely popular video game Pac-Man has challenged players for nearly 30 years, with the very best human competitors striking a highly honed balance between the games two key factors; the ‘chomping’ of pills (or pac-dots) throughout the level whilst avoiding the ghosts that haunt the maze trying to capture the titular hero. We believe that in order to achieve this it is important for an agent to plan-ahead in creating paths in the maze while utilising a reactive control to escape the clutches of the ghosts. In this paper we evaluate the effectiveness of such a look-ahead against greedy and random behaviours. Results indicate that a competent agent, on par with novice human players can be constructed using a simple framework.","Games,
Humans,
Fires,
Switches,
History,
Earth,
Cloning,
Cognitive science,
Marine vehicles,
Navigation"
Non-circular pupil localization in iris images,"Iris localization is the most important step in iris recognition systems, due to the fact that all the subsequent steps, iris normalization, feature extraction and matching, depends on its accuracy. Iris localization means segmentation of iris from the other parts of eye like pupil, sclera, eyelids and eyelashes in the image of an eye. A new method of exact pupil localization is proposed in this paper. Pupil is localized by finding a point in the pupil and then center is obtained using centroid of the pupil region whereas radius for further processing is calculated from the binary image of the region. Non-circular boundary of the pupil is ciphered by divide and conquer rule. Circular boundary of pupil is divided into a specified number of points. These points are repositioned with respect to the maximum gradient and then joined together to obtained exact boundary of the pupil. Iris outer boundary and eyelids are determined using intensity gradient. Experimental results show that the proposed method of iris localization is quite effective.","Image segmentation,
Eyelids,
Biometrics,
Image edge detection,
Iris recognition,
Eyelashes,
Detectors,
Humans,
Educational institutions,
Mechanical engineering"
Zero-forcing-based two-phase relaying with multiple mobile stations,"It is well known that relay stations improve the link performance between the base and mobile stations and thereby improve the total system throughput. Nonetheless, the additional resource consumption to deploy the relay station reduces the system throughput significantly. The zero-forcing (ZF)-based two-phase relaying scheme that requires only two phases to communicate a frame was suggested for the system where a single mobile station is considered [1]. The performance bottle-necks of the conventional ZF-based two-phase relaying are channel asymmetry and an ill-conditioned channel that reduces the effective channel gain. We propose a novel two-phase relaying scheme to support multiple users simultaneously for a given channel resource. The proposed relaying scheme includes the multiuser scheduling at the relay station and the additional precoding at the base station to make the effective channels from the base and mobile stations equal to each other. We evaluate the proposed relaying scheme by numerically comparing its sum rate to those of the conventional relaying schemes.",
The W-Model for Testing Software Product Lines,"Testing is one of the important means of software quality assurance. Each software development paradigm requires an appropriate test model. Software product line engineering emerges as a new software development method. This paper puts forward a new test model for software product lines, the W-model, describes two separate and closely related sub-models of domain test and application test. Key issues and activities specific to testing software product lines such as variability test and regression test are addressed and explained with an example.","Software testing,
Application software,
Programming,
System testing,
Computer architecture,
Software quality,
Mobile handsets,
Educational institutions,
Software engineering,
Design engineering"
Development method of simulation and test system for vehicle body CAN bus based on CANoe,"The CAN bus which has been widely used in Electric Vehicle Control System have many characteristics such as high transmission efficiency, high reliability, good real-time feature and so on. The CANoe which is the product of German company Vector is a practical and powerful tool for system design and analysis. Firstly, A design scheme of simulation and test system for vehicle body CAN bus network is brought forward, which mainly includes the topology of network, the hierarchical model of network and the selection of bus baud rate. Secondly, a method of how to use CANoe to construct the simulation and test environment for vehicle body CAN bus system is introduced Finally, the simulation and test system for vehicle body CAN bus is completed by CANoe. Experiments demonstrate that the development method of simulation and test system is feasible.","Vehicles,
Protocols,
Control systems,
Analytical models,
Data models,
Instruments,
Network topology"
Adaptive codebook for beamforming in limited feedback MIMO systems,"We propose a new scheme for limited feedback in MIMO systems. We consider transmit beamforming and receiver maximal ratio combining as a base for our work, and propose a novel beamforming codebook to exploit the inherent correlation of the channel. This novel beamforming codebook, unlike the conventional beamforming codebooks, adaptively changes with the channel matrix. Moreover, the adaptive approach is independent of the channel model and can be applied to any general MIMO channel with temporal and spatial correlations. Simulation results show that compared to previously known beamforming schemes, this technique significantly improves the BER performance in spatio-temporally correlated channels.","Array signal processing,
Feedback,
MIMO,
Transmitters,
Fading,
Bit error rate,
Diversity reception,
Robustness,
WiMAX,
Character generation"
Nonintrusive Driver Fatigue Detection,"Driver fatigue is an important factor in many transportation accidents. Therefore, detecting driver fatigue is extremely important to improving transportation safety. When a driver fatigues, he will take many special visual cues on his face. In this paper, we combine visual cues from mouths and eyes systematically which characterize eye closed and yawning to infer the fatigue level of a driver. AdaBoost is used to extract the most discriminative features from the local binary pattern (LBP) features of eye areas and constructs a highly accurate classifier to get the eye visual cue. Yawning is an important evidence of driver fatigue. We detect driver's left and right mouth corners by gray projection, and extract texture features of driver's mouth corners using Gabor wavelets, and finally the mouth visual cue is extracted by using LDA to classify Gabor features. A probabilistic method based on Bayesian networks (BN) is used to fuse the two visual cues at the confidence level for fatigue detection. The proposed method has been tested on wide range of human subjects under real-life fatigue conditions of different genders, poses and illumination conditions. It yields a much more robust, reliable and accurate fatigue detection than using a single visual cue. The test data includes 4800 images from thirty people's videos, and the average recognition rate of the proposed method is 96.79%.","Fatigue,
Mouth,
Transportation,
Testing,
Accidents,
Safety,
Face detection,
Eyes,
Feature extraction,
Linear discriminant analysis"
The Need for Verifiable Visualization,"Visualization is often employed as part of the simulation science pipeline, it's the window through which scientists examine their data for deriving new science, and the lens used to view modeling and discretization interactions within their simulations. We advocate that as a component of the simulation science pipeline, visualization must be explicitly considered as part of the validation and verification (V&V) process. In this article, the authors define V&V in the context of computational science, discuss the role of V&V in the scientific process, and present arguments for the need for verifiable visualization.","Reliability engineering,
Computer errors,
Pipelines,
Mathematical model,
Biology computing,
Computational modeling,
Physics computing,
Computational fluid dynamics,
Data visualization,
Uncertainty"
Sonar Grid Map Based Localization for Autonomous Mobile Robots,"A mobile robot must be able to build a reliable map of surroundings and estimate its position. We have developed a technique for a grid-based localization of a mobile robot with ultrasonic sensors using Extended Kalman Filter (EKF). For this, we used grids themselves as landmarks of the environment. The grid-based localization can minimize the use of computer resources for localization because this approach does not rely on exact geometric representation of a landmark. Experiments were performed in a real environment to verify the methodology developed in this study, and the results indicate that the grid-based localization can be useful for a practical application.","Mobile robots,
Orbital robotics,
Sensor phenomena and characterization,
Robot sensing systems,
Sonar applications,
Vehicles,
Grid computing,
Application software,
Minimization methods,
Laser modes"
Fast log-based concurrent writing of checkpoints,"This report describes how a file system level log-based technique can improve the write performance of many-to-one write checkpoint workload typical for high performance computations. It is shown that a simple log-based organization can provide for substantial improvements in the write performance while retaining the convenience of a single flat file abstraction. The improvement of the write performance comes at the cost of degraded read performance however. Techniques to alleviate the read performance penalty, such as file reconstruction on the first read, are discussed.","Writing,
Application software,
Concurrent computing,
File systems,
Image reconstruction,
Delay,
Computer science,
High performance computing,
Costs,
Degradation"
Build to order linear algebra kernels,"The performance bottleneck for many scientific applications is the cost of memory access inside linear algebra kernels. Tuning such kernels for memory efficiency is a complex task that reduces the productivity of computational scientists. Software libraries such as the Basic Linear Algebra Subprograms (BLAS) ameliorate this problem by providing a standard interface for which computer scientists and hardware vendors have created highly-tuned implementations. Scientific applications often require a sequence of BLAS operations, which presents further opportunities for memory optimization. However, because BLAS are tuned in isolation they do not take advantage of these opportunities. This phenomenon motivated the recent addition to the BLAS of several routines that perform sequences of operations. Unfortunately, the exact sequence of operations needed in a given situation is highly application dependent, so many more routines are needed. In this paper we present preliminary work on a domain-specific compiler that generates implementations for arbitrary sequences of basic linear algebra operations and tunes them for memory efficiency. We report experimental results for dense kernels and show speedups of 25% to 120% relative to sequences of calls to GotoBLAS and vendor-tuned BLAS on Intel Xeon and IBM PowerPC platforms.","Linear algebra,
Kernel,
Application software,
Computer interfaces,
Costs,
Read-write memory,
Optimizing compilers,
Computer science,
Productivity,
Software libraries"
Dynamic logic of phenomena and cognition,"Modeling of complex phenomena such as the mind presents tremendous computational complexity challenges. The neural modeling fields theory (NMF) addresses these challenges in a non-traditional way. The main idea behind success of NMF is matching the levels of uncertainty of the problem/model and the levels of uncertainty of the evaluation criterion used to identify the model. When a model becomes more certain then the evaluation criterion is also adjusted dynamically to match the adjusted model. This process is called dynamic logic (DL) of model construction, which mimics processes of the mind and natural evolution. This paper provides a formal description of Phenomena Dynamic Logic (P-DL) and outlines its extension to the Cognitive Dynamic Logic (C-DL). P-DL is presented with its syntactic, reasoning, and semantic parts. Computational complexity issues that motivate this paper are presented using an example of polynomial models.",
Distributed Sequence Alignment Applications for the Public Computing Architecture,"The public computer architecture shows promise as a platform for solving fundamental problems in bioinformatics such as global gene sequence alignment and data mining with tools such as the basic local alignment search tool (BLAST). Our implementation of these two problems on the Berkeley open infrastructure for network computing (BOINC) platform demonstrates a runtime reduction factor of 1.15 for sequence alignment and 16.76 for BLAST. While the runtime reduction factor of the global gene sequence alignment application is modest, this value is based on a theoretical sequential runtime extrapolated from the calculation of a smaller problem. Because this runtime is extrapolated from running the calculation in memory, the theoretical sequential runtime would require 37.3 GB of memory on a single system. With this in mind, the BOINC implementation not only offers the reduced runtime, but also the aggregation of the available memory of all participant nodes. If an actual sequential run of the problem were compared, a more drastic reduction in the runtime would be seen due to an additional secondary storage I/O overhead for a practical system. Despite the limitations of the public computer architecture, most notably in communication overhead, it represents a practical platform for grid- and cluster-scale bioinformatics computations today and shows great potential for future implementations.","Distributed computing,
Computer architecture,
Runtime,
Bioinformatics,
Computer networks,
Application software,
Concurrent computing,
Computer science,
Data mining,
Grid computing"
Extracting age information from local spatially flexible patches,"Motivated by the fact that age information can often be observed from local evidence on the human face, we contribute to the age estimation problem in two aspects. On the one hand, we present a new feature descriptor, called spatially flexible patch (SFP), which encodes the local appearance and position information simultaneously. SFP has the potential to alleviate the problem of insufficient samples owing to that SFPs similar in appearance yet slightly different in position can still provide similar confidence for age estimation. One the other hand, the SFP associated with age label is modeled with Gaussian Mixture Model, and then age estimation is conducted by maximizing the sum of likelihoods from all the SFPs associated with the hypothetic age. Experiments are conducted on the YAMAHA database with 8,000 face images and ages ranging from 0 to 93. Compared with the latest reported results, our new algorithm brings encouraging reduction in mean absolute error for age estimation.","Data mining,
Humans,
Face,
Image databases,
Spatial databases,
Neural networks,
Aging,
Uncertainty,
Pixel,
Estimation error"
Front-end amplifier of low-noise and tunable BW/gain for portable biomedical signal acquisition,"We proposed a novel analog circuit design which is suitable for various biomedical signal acquisitions. In addition to the consideration of low power and low noise, the analog front-end integrated circuit (AFEIC) is presented with design of high common-mode rejection ratio (CMRR) and high power supply ripple rejection ratio (PSRR). It has not only reduced the number of outer components, and enhances a better signal-to-noise ratio (SNR). The chip includes a current-balancing instrumentation amplifier, switched-capacitor filter, non-overlapping clock generator, and a programmable gain amplifier (PGA). It was fabricated by TSMC 0.35 μm CMOS 2P4M standard process, with CMRR 155 dB CMRR, 131 dB of PSRR+, and 127 dB of PSRR- at 50 Hz. The power consumption is about 142.4 μW under ±1.5V supply.","Low-noise amplifiers,
Signal to noise ratio,
Integrated circuit noise,
Tunable circuits and devices,
Analog circuits,
Signal design,
Analog integrated circuits,
Power supplies,
Instruments,
Filters"
Verification of Access Control Requirements in Web Services Choreography,"Web services choreography is used to design peer-to-peer applications where each peer is potentially a Web service. It defines the required behavior of participating Web services along with their interactions through message exchanges. Implementing a complex system described by a choreography requires selecting actual Web services whose individual behaviors are compatible with the overall behavior described by the choreography. Although the selected Web services implement the specified behavior, they may not be able to interact due to the policies they enforce to protect their resources. A Web service' resource can be an operation or a credential type to be submitted to be able to invoke an operation. In this paper, we propose a novel approach to determine at design time whether a choreography can be implemented by a set of Web services based on their access control policies and the disclosure policies regulating the release of their credentials. We model both Web services and Web services choreography as transition systems and represent Web services credential disclosure policies as directed graphs. We then verify that all possible conversations of the Web services choreography can be implemented by matching credential disclosure policies of the invoker Web service with the access control policy of the Web services being invoked. We propose a resource release graph to enable this verification.","Web services,
authorisation,
formal verification,
peer-to-peer computing"
Forward error correction for high-speed I/O,"Modern state-of-the-art high-speed (Gb/s) I/O links today rely exclusively upon an equalization-based transceiver to achieve a bit error-rate (BER) of 10−15. This paper explores the potential of applying forward error-correction (FEC) in such links to reduce power and BER. The FEC coding gain can be employed to lower the power consumed in the analog components (e.g., transmit driver, clock recovery unit (CRU)) since these do not scale with process technology. A BER improvement of six orders-of-magnitude and ten orders-of-magnitude is demonstrated for a 20″ FR4 channel operating at 10 Gb/s with a LE and a DFE, respectively, using a BCH code. Savings in the encoder-decoder power overhead of up to 50% is demonstrated for a (63, 36, 11) BCH code using a novel gated decoder architecture.","Forward error correction,
Bit error rate,
Tin,
Transceivers,
Filters,
Clocks,
Decoding,
Intersymbol interference,
Energy consumption,
Equalizers"
Support for Educating Software Engineers Through Humanitarian Open Source Projects,"The net generation of students have characteristics which make them well-suited for participating in open source projects including being comfortable with information technologies, using IT as a form of communication, desiring to work in groups, a desire to do social good, and being fascinated by new technologies. The nature of open source projects where communities of developers from around the world collaborate to create useful applications are a natural fit for this generation of students. Humanitarian open source projects can serve as a solid foundation for providing software engineering education to the net generation of students. This paper discusses the initial stages of SoftHum, a project for developing course materials to support undergraduate involvement in humanitarian open source projects.",
"Kinematics analysis, design, and control of an Isoglide3 Parallel Robot (IG3PR)","The paper presents a novel structure of the Isoglide3 Parallel Robot (IG3PR), as an effective robotic device with three degrees of freedom manipulation. The IG3PR manipulator offers the characteristics, advantageous relative to the other parallel manipulators (light weight construction), while on the other hand alleviates some of the traditional weaknesses of parallel manipulators, (extensive use of spherical joints and coupling of the platform orientation and position). The presented IG3PR robot employs only revolute (rotary) and prismatic (sliding) joints to achieve the translational motion of the moving platform. The pivotal advantages of the presented parallel manipulator are the following: all of the actuators can be attached directly to the base; closed-form solutions are available for the forward and inverse kinematics; and the moving platform maintains the same orientation throughout the entire workspace. In addition to these comparative improvements, the paper presents an innovative user interface for high-level control of the Isoglide3 parallel robot. The novel IG3PR was verified and tested, and results in MATLAB, Simulink, and SimMechanics were presented.",
Handoff decision scheme with guaranteed QoS in heterogeneous network,"According to the development of the wireless communication technologies. The wireless communication network should consist of several wireless network technologies in the future. The mobility management could assure of the connection when user is roaming in the heterogeneous wireless networks. Handoff is a very important function for the mobile communication network. It is a procedure of transferring the connection from old base station to new base station which is called vertical handoff in heterogeneous networks. A good handoff decision could avoid the redundant handoffs and reduce the packet lost or communication latency. In this paper, we proposed a handoff decision scheme for heterogeneous networks. We make the decision according to the user’s communicating types and the performance of the networks. The results of simulations show that the scheme reduces the frequency of vertical handoff and enhances the performance of the whole network.","Wireless communication,
Wireless networks,
Mobile communication,
Wireless LAN,
Real time systems,
Communications technology,
Base stations,
Bandwidth,
Mobile radio mobility management,
Delay"
An Integrated Hydrologic Modeling and Data Assimilation Framework,"The Land Information System (LIS) is a multiscale hydrologic modeling and data assimilation framework that integrates the use of satellite and ground-based observational data products with advanced land-surface modeling tools to aid several application areas, including water resources management, numerical weather prediction, agricultural management, air quality, and military mobility assessment.",
Knowledge reduction based on incremental algorithms on attribute space,"Information System is a kind of important form of knowledge representation System, and most of the existing incremental algorithm investigators focus on adding objects to IS. In this paper, the change laws of the core and the reductions with increasing any attributes to a information system are discussed based on the concepts defined, then the a single attribute incremental algorithm and the many attributes incremental algorithm are presented. The examples show that the efficiency of computing of the core and the reductions of the extension information system based on the incremental algorithms may be improved.",
Monocular range sensing: A non-parametric learning approach,"Mobile robots rely on the ability to sense the geometry of their local environment in order to avoid obstacles or to explore the surroundings. For this task, dedicated proximity sensors such as laser range finders or sonars are typically employed. Cameras are a cheap and lightweight alternative to such sensors, but do not directly offer proximity information. In this paper, we present a novel approach to learning the relationship between range measurements and visual features extracted from a single monocular camera image. As the learning engine, we apply Gaussian processes, a non-parametric learning technique that not only yields the most likely range prediction corresponding to a certain visual input but also the predictive uncertainty. This information, in turn, can be utilized in an extended grid-based mapping scheme to more accurately update the map. In practical experiments carried out in different environments with a mobile robot equipped with an omnidirectional camera system, we demonstrate that our system is able to produce proximity estimates with an accuracy comparable to that of dedicated sensors such as sonars or infrared range finders.","Cameras,
Mobile robots,
Robot vision systems,
Infrared sensors,
Geometrical optics,
Computational geometry,
Sonar measurements,
Feature extraction,
Data mining,
Engines"
An energy detection receiver robust to multi-user interference for IEEE 802.15.4a networks,"Energy-detection receivers are appealing to IEEE 802.15.4a low data-rate networks because of their low complexity. With a reasonable energy consumption, these receivers can exploit the ranging capabilities and multipath resistance of impulse-radio UWB (IR-UWB). However, the performance of energy-detection receivers can be severely degraded by multi-user interference (MUI). One solution may be to coordinate access to the physical layer with an exclusion protocol. Unfortunately, this cannot prevent MUI due to uncontrolled activities in neighboring networks (e.g., several IEEE 802.15.4a piconets running in parallel). Hence, interference must be taken into account already in the design of the physical layer. In this paper, we present an IR-UWB receiver robust to MUI for IEEE 802.15.4a networks. Its architecture is based on energy detection. We also take into full account the different signaling structure between the preamble and the payload of IEEE 802.15.4a packets. In certain scenarios with MUI we found the packet error rate to be up to two orders of magnitude lower when compared to a traditional energy detection receiver that neglects MUI. Further, this significant performance improvement entails only a moderate increase in complexity.",
Scalable Techniques for Transparent Privatization in Software Transactional Memory,"We address the recently recognized privatization problem  in software transactional memory (STM) runtimes, and introduce the notion of partially visible reads (PVRs) to heuristically reduce the overhead of transparent privatization.  Specifically, PVRs avoid the need for a ""privatization fence"" in the absence of conflict with concurrent readers.  We present several techniques to trade off the cost of enforcing partial visibility with the precision of conflict detection.  We also consider certain special-case variants of our approach, e.g., for predominantly read-only workloads.  We compare our implementations to prior techniques on a multicore Niagara1 system using a variety of artificial workloads.  Our results suggest that while no one technique performs best in all cases, a dynamic hybrid of PVRs and strict in-order commits is stable and reasonably fast across a wide range of load parameters.  At the same time, the remaining overheads are high enough to suggest the need for programming model or architectural support.","Privatization,
Optimization,
Throughput,
Data structures,
Clocks,
Benchmark testing,
Runtime"
A Spatial Median Filter for noise removal in digital images,"In this paper, six different image filtering algorithms are compared based on their ability to reconstruct noise-affected images. The purpose of these algorithms is to remove noise from a signal that might occur through the transmission of an image. A new algorithm, the Spatial Median Filter, is introduced and compared with current image smoothing techniques. Experimental results demonstrate that the proposed algorithm is comparable to these techniques. A modification to this algorithm is introduced to achieve more accurate reconstructions over other popular techniques.","Digital filters,
Digital images,
Nonlinear filters,
Smoothing methods,
Signal processing algorithms,
Filtering algorithms,
Image reconstruction,
Digital cameras,
Digital photography,
Color"
Digital Ecosystems: Optimisation by a distributed intelligence,"Can intelligence optimise Digital Ecosystems? How could a distributed intelligence interact with the ecosystem dynamics? Can the software components that are part of genetic selection be intelligent in themselves, as in an adaptive technology? We consider the effect of a distributed intelligence mechanism on the evolutionary and ecological dynamics of our Digital Ecosystem, which is the digital counterpart of a biological ecosystem for evolving software services in a distributed network. We investigate Neural Networks (NNs) and Support Vector Machines (SVM) for the learning based pattern recognition functionality of our distributed intelligence. Simulation results imply that the Digital Ecosystem performs better with the application of a distributed intelligence, marginally more effectively when powered by SVM than NNs. These results suggest that a distributed intelligence can contribute to optimising the operation of our Digital Ecosystem.","Ecosystems,
Support vector machines,
Artificial neural networks,
Training,
Optimization,
Classification algorithms,
Biological system modeling"
Distributed Kalman filter via Gaussian Belief Propagation,"Recent result shows how to compute distributively and efficiently the linear MMSE for the multiuser detection problem, using the Gaussian BP algorithm. In the current work, we extend this construction, and show that operating this algorithm twice on the matching inputs, has several interesting interpretations. First, we show equivalence to computing one iteration of the Kalman filter. Second, we show that the Kalman filter is a special case of the Gaussian information bottleneck algorithm, when the weight parameter β = 1. Third, we discuss the relation to the Affine-scaling interior-point method and show it is a special case of Kalman filter. Besides of the theoretical interest of this linking estimation, compression/clustering and optimization, we allow a single distributed implementation of those algorithms, which is a highly practical and important task in sensor and mobile ad-hoc networks. Application to numerous problem domains includes collaborative signal processing and distributed allocation of resources in a communication network.",
Technology and learning-centered education: Research-based support for how the tablet PC embodies the Seven Principles of Good Practice in Undergraduate Education,"Student learning improves when faculty use learning-centered teaching practices, and a symbiotic relationship exists between technology and learning-centered education. One technological tool, the Tablet PC, offers university faculty a powerful way to enhance student learning. The Seven Principles for Good Practice in Undergraduate Education offer a framework for learning-centered education, and this paper illustrates the Seven Principles through research data focused on innovative and pedagogically appropriate uses of Tablet PCs. Examples include assessment research data from MIT, DePauw, Rose-Hulman Institute of Technology, University of Washington, Pace University, University of Michigan and Virginia Tech","Educational technology,
Computer science education,
Feedback,
Personal communication networks,
Collaborative work,
Symbiosis,
Problem-solving,
Frequency,
Springs,
Ink"
Unsupervised learning from local features for video-based face recognition,"This paper presents an unsupervised learning approach to video-based face recognition that does not make any assumptions about the pose, expressions or prior localization of landmarks on the faces. The proposed algorithm exploits spatiotemporal information obtained from local features that are extracted from arbitrary keypoints on faces as opposed to pre-defined landmarks. The algorithm is inherently robust to large scale occlusions as it relies on local features. During unsupervised learning, faces from a video sequence are automatically clustered based on the similarity of their local features and a voting-based algorithm is employed to pick the representative features of each cluster. During recognition, video frames of a probe are sequentially matched to the clusters of all individuals in the gallery and its identity is decided on the basis of best temporally cohesive cluster matches. The proposed algorithms can also detect sudden identity changes in video by utilizing the temporal dimension. The algorithm was tested on the Honda/UCSD video database and a maximum of 99.5% recognition rate was achieved.",
Neuromorphic implementation of active gaze and vergence control,"We present an active stereo system with gaze and vergence control driven by model of the disparity selective neurons in the mammalian visual cortex. The hardware consists of a mobile stereo camera and three Multimap boards. The Multimap boards compute multiple cortical maps responding to target locations, orientations and disparities, and generate movement commands to track target in space. Each board can compute more than 10 cortical maps at 320*240 pixel resolution and 25 frames per second, and consumes 3.5W.","Neuromorphics,
Neurons,
Cameras,
Target tracking,
Eyes,
Stereo vision,
Brain modeling,
Hardware,
Head,
Signal generators"
Analyzing medical processes,"This paper shows how software engineering technologies used to define and analyze complex software systems can also be effective in detecting defects in human-intensive processes used to administer healthcare. The work described here builds upon earlier work demonstrating that healthcare processes can be defined precisely. This paper describes how finite-state verification can be used to help find defects in such processes as well as find errors in the process definitions and property specifications. The paper includes a detailed example, based upon a real-world process for transfusing blood, where the process defects that were found led to improvements in the process.","Medical services,
Computer science,
Software engineering,
Blood,
Hospitals,
Permission,
Software systems,
Engineering management,
Human factors,
Natural languages"
Facial expression recognition using encoded dynamic features,"In this paper, we propose a novel framework for video-based facial expression recognition, which can handle the data with various time resolution including a single frame. We first use the haar-like features to represent facial appearance, due to their simplicity and effectiveness. Then we perform K-Means clustering on the facial appearance features to explore the intrinsic temporal patterns of each expression. Based on the temporal pattern models, we further map the facial appearance variations into dynamic binary patterns. Finally, boosting learning is performed to construct the expression classifiers. Compared to previous work, the dynamic binary patterns encode the intrinsic dynamics of expression, and our method makes no assumption on the time resolution of the data. Extensive experiments carried on the Cohn-Kanade database show the promising performance of the proposed method.","Face recognition,
Pattern recognition,
Cameras,
Computer science,
Laboratories,
Boosting,
Databases,
Psychology,
Active shape model,
Multi-stage noise shaping"
From Cells to Cell Processors: The Integration of Health and Video Games,"Healthcare offers special opportunities for the application of game research and technology. This was evident in the presentations at the 2007 Games for Health Conference.One of the most important sectors seeing the impact of games is health. In 2004, with support from the Lounsberry Foundation and the Woodrow Wilson International Center for Scholars, the Serious Games Initiative started the Games for Health Project. The Serious Games Initiative focuses on all sectors, but after surveying many usage areas for games, we felt that health and healthcare had special opportunities for games.","Medical services,
Economic indicators,
Game theory,
Toy industry,
History,
Virtual reality,
Biomarkers,
Moore's Law,
Computer networks,
Computer science"
Research on Multi-project Scheduling Problem Based on Hybrid Genetic Algorithm,"A simulated annealing genetic algorithm was put forward to solve the resource-constrained multi-project scheduling problem. The ordinary genetic algorithm and simulated annealing algorithm used in this method were improved separately firstly. Then the simulated annealing operations which can overcome the defects of genetic algorithm easy to fall into the local optimal solution were used in the genetic algorithm. The method also inherited the rapid convergence characteristic of genetic algorithm. It was proved by a practical example that this hybrid algorithm improved the deficiencies of genetic algorithm and simulated annealing and can effectively shorten the implementation time of projects. Compared with other heuristic and intelligent methods, this algorithm performs better than them.","Genetic algorithms,
Simulated annealing,
Software engineering,
Processor scheduling,
Computational modeling,
Resource management,
Computer science,
Geology,
Laboratories,
Constraint optimization"
Retroactivity attenuation in transcriptional networks: Design and analysis of an insulation device,"Retroactivity is a phenomenon that changes the desired input/output response of a system when it is connected to “downstream” systems. Transcriptional networks are not immune to this phenomenon. In this paper, we propose a phosphorylation-based design for a bio-molecular system that acts as an insulator between its upstream systems and its downstream ones in a transcriptional network. Performing singular perturbation analysis, we mathematically show that such a design attenuates retroactivity. Stochastic simulations are run to analyze the robustness of the proposed device to biological noise and to highlight design tradeoffs.","Attenuation,
Insulation,
Biological system modeling,
Biological systems,
Control systems,
Stochastic resonance,
Immune system,
Performance analysis,
Analytical models,
Noise robustness"
Trust evaluation in health information on the World Wide Web,"The impact of health information on the web is mounting and with the Health 2.0 revolution around the corner, online health promotion and management is becoming a reality. User-generated content is at the core of this revolution and brings to the fore the essential question of trust evaluation, a pertinent problem for health applications in particular. Evolving Web 2.0 health applications provide abundant opportunities for research. We identify these applications, discuss the challenges for trust assessment, characterize conceivable variables, list potential techniques for analysis, and provide a vision for future research.",
Robotic wireless network connection of civilians for emergency response operations,"Mobile robots equipped with wireless devices can prove very useful during emergency response operations. We envision such robots that locate trapped civilians and initiate an ad hoc network connection between them and the rescuers, so that the latter can better assess the situation and plan the rescue operation accordingly. We present a centralised formulation for the novel problem of optimally allocating robots so that they connect as many civilians as possible, while maintaining their multi-hop connection with a static wireless sink. This formulation stems from a combination of characteristics typically found in assignment and network flow optimisation problems. We have also developed a distributed heuristic with which the robots start from the location of the sink and move autonomously trying to connect the civilians while maintaining connectivity. We evaluate our distributed heuristic using a building evacuation simulator and compare it with the centralised approach.","Wireless networks,
Mobile robots,
Ad hoc networks,
Collaborative work,
Network topology,
Collaboration,
Wireless sensor networks,
Robot sensing systems,
Humans,
Hazardous materials"
Automatic Dynamic Task Distribution between CPU and GPU for Real-Time Systems,"The increase of computational power of programmable GPU (Graphics Processing Unit) brings new concepts for using these devices for generic processing. Hence, with the use of the CPU and the GPU for data processing come new ideas that deals with distribution of tasks among CPU and GPU, such as automatic distribution. The importance of the automatic distribution of tasks between CPU and GPU lies in three facts. First, automatic task distribution enables the applications to use the best of both processors. Second, the developer does not have to decide which processor will do the work, allowing the automatic task distribution system to choose the best option for the moment. And third, sometimes, the application can be slowed down by other processes if the CPU or GPU is already overloaded. Based on these facts, this paper presents new schemes for efficient automatic task distribution between CPU and GPU. This paper also includes tests and results of implementing those schemes with a test case and with a real-time system.","parallel processing,
computer graphics"
Combination of Particle Swarm Optimization and Stochastic Local Search for Multimodal Function Optimization,"In this paper we present a combinatorial optimization method based on particle swarm optimization and stochastic local search concept. Under this method, in order to balance between exploration and exploitation, at each iteration step a local exploration performed around particles. The stochastic local search encourages the particle to explore local region beyond that defined by the search algorithm to achieve better solutions. The proposed method is assessed using a set of multimodal functions. Experimental results show that the proposed method outperforms other algorithms.","Particle swarm optimization,
Stochastic processes,
Topology,
Optimization methods,
Acceleration,
Conferences,
Computational intelligence,
Computer industry,
Application software,
Computer science"
Improving the modeling of the noise part in the harmonic plus noise model of speech,"Harmonic + Noise model (HNM) is a hybrid model of speech with a harmonic component and a noise component. While the harmonic part describes efficiently the periodicities in speech signals (voiced parts), modeling of the noise part introduces artifacts primarily because of the specific time-domain characteristics of noise in voiced speech. In this paper, we concentrated on the modeling of noise in voiced frames. To model the temporal characteristics of noise, we study three time envelopes in the context of HNM; Triangular envelope, Hilbert envelope and Energy envelope. Listening tests showed a clear preference for the Energy envelope and Hilbert envelope for male voices and to a lesser extent the same conclusions can be drawn for female voices.",
A hybrid camera for motion deblurring and depth map super-resolution,We present a hybrid camera that combines the advantages of a high resolution camera and a high speed camera. Our hybrid camera consists of a pair of low-resolution high-speed (LRHS) cameras and a single high-resolution low-speed (HRLS) camera. The LRHS cameras are able to capture fast-motion with little motion blur. They also form a stereo pair and provide a low-resolution depth map. The HRLS camera provides a high spatial resolution but also introduces severe motion blur when capturing fast moving objects. We develop efficient algorithms to simultaneously motion-deblur the HRLS image and reconstruct a high resolution depth map. Our method estimates the motion flow in the LRHS pair and then warps the flow field to the HRLS camera to estimate the point spread function (PSF).We then deblur the HRLS image and use the resulting image to enhance the low-resolution depth map using joint bilateral filters. We demonstrate the hybrid camera in depth map super-resolution and motion deblurring with spatially varying kernels. Experiments show that our framework is robust and highly effective.,"Cameras,
Image resolution,
Spatial resolution,
Kernel,
Image reconstruction,
Motion estimation,
Filters,
Signal resolution,
Image restoration,
Image sensors"
Survival multipath routing for MANETs,"Many efforts have been made to develop security solutions for MANETs. These networks are vulnerable to diverse types of attacks, where routing is a critical operation. Current secure routing protocols are based on preventive or reactive security mechanisms, monitoring node behavior or controlling the access to networks and information. These mechanisms are insufficient to put all attacks and intrusions off, motivating the use of techniques that can tolerate them as multipath routing. This work proposes a new scheme based on fuzzy logic to select better paths and to increase network survivability. Our approach considers different selection criteria, where some of them represent network status and others are issued by preventive, reactive and tolerant defense lines. Simulations have been carried out comparing AODV and AOMDV with a modified AOMDV protocol. Results show the survivability of our approach under different conditions.",
"Follow Me, Follow You - Spatiotemporal Community Context Modeling and Adaptation for Mobile Information Systems","Nowadays various mobile platforms from PDA, smart phones to iPhones can deliver road warriors or trippers much mobile information. However, most mobile applications are lacking the ""social intelligence"" of real  companionship. One reason is that community context and spatiotemporal context information are not well  taken into consideration. In this paper we propose an ontology-based context model using OWL/RDF. With the enhanced interoperability substantial context information can be expressed and reasoned across systems.  Based on the context model and the middleware Context-Aware Adaptation Service (CAAS), we introduce a  mobile tourist guide as proof-of-concept that provides users context-aware information. We employ the  MPEG-7 and MPEG-21 metadata standards to realize multimedia adaptation to device preferences. The  evaluation result proves the feasibility of the context model and shows the easy extensibility of CAAS.",
Design and FPGA implementation of radix-10 algorithm for division with limited precision primitives,"We present a radix-10 digit-recurrence algorithm for division using limited-precision multipliers, adders, and table-lookups. We describe the algorithm, a design, and its FPGA implementation. The proposed scheme is implemented on the Xilinx Virtex-5 FPGA device and we obtained the following characteristics: for n = 7, delay is ≈ 105ns and the cost is 782 LUTs. For n = 14, the implementation has a delay of ≈ 197ns and the cost of 1263 LUTs. The proposed scheme uses short operators which may have an advantage at the layout level and in power optimization.","Algorithm design and analysis,
Field programmable gate arrays,
Computer science,
Delay,
Costs,
Table lookup,
Convolution,
Application specific integrated circuits,
Bandwidth,
Error compensation"
Feasibility and performance analysis of a high power drive based on four synchro-converters supplying a twelve-phase synchronous motor,"In this paper a high-power electric drive based on a quadruple-three-phase synchronous motor and four Load Commutated Inverters (LCI) is investigated to evaluate its feasibility and performance. Based on accurate simulation analyses, the advantages of the proposed solution compared to the technologically-proven dual three-phase scheme are highlighted in terms of torque quality and motor efficiency improvement.",
Global EDF-Based Scheduling with Efficient Priority Promotion,"This paper presents an algorithm, called Earliest Deadline Critical Laxity (EDCL), for the efficient scheduling of sporadic real-time tasks on multiprocessors systems. EDCL is a derivative of the Earliest Deadline Zero Laxity (EDZL) algorithm in that the priority of a job reaching certain laxity is imperiously promoted to the top, but it differs in that the occurrence of priority promotion is confined to at the release time or the completion time of a job. This modification enables EDCL to bound the number of scheduler invocations and to relax the implementation complexity of scheduler, while the schedulability is still competitive with EDZL. The schedulability test of EDCL is designed through theoretical analysis. In addition, an error in the traditional schedulability test of EDZL is corrected. Simulation studies demonstrate the effectiveness of EDCL in terms of guaranteed schedulability and exhaustive schedulability by comparing with traditional efficient scheduling algorithms.",
A Meta-scheduler with Auction Based Resource Allocation for Global Grids,"As users increasingly require better quality of service from Grids, resource management and scheduling mechanisms have to evolve in order to satisfy competing demands on limited resources. Traditional schedulers for Grids are system centric and favour system performance over increasing user’s utility. On the other hand market oriented schedulers are price-based systems that favour users but are based solely on user valuations. This paper proposes a novel meta-scheduler that unifies the advantages of both the systems for benefiting both users and resources. In order to do that, we design a valuation metric for user’s applications and computational resources based on multi-criteria requirements of users and resource load. The meta-scheduler maps user applications to suitable distributed resources using a Continuous Double Auction (CDA). Through simulation, we compare our scheduling mechanism against other common mechanisms used by current meta-schedulers. The results show that our meta-scheduler mechanism can satisfy more users than the others while still meeting traditional system-centric performance criteria such as average load and deadline of applications",
Social Modeling as an Interdisciplinary Research Practice,"Social modeling applies computational methods and techniques to the analysis of social processes and human behavior. It's expected to provide conceptual and technological tools for supporting analysis and decision making in areas related to national and public security, political stability, law and order, and sociocultural changes. Modeling social and cultural processes must draw on the knowledge obtained within social sciences, including conceptual models, cultural insights, and empirical data. However, how to best integrate social scientific knowledge into modeling remains an open research problem. The author presents the perspective of a social scientist to describe why modeling can be useful for social research on political violence, social conflicts, and cultural changes. She develops an interactionist approach to interdisciplinary research practice and discusses how this approach can help identify the problems related to the integration of social scientific knowledge in modeling. The discussion focuses upon research on political violence and related sociocultural processes.","Ontologies,
Cultural differences,
Lenses,
Equations,
Context modeling,
Computational modeling,
Humans,
Decision making,
Data security,
National security"
Stable electrodes and ultrathin passivation coatings for high temperature sensors in harsh environments,"Sensor operation in harsh environments up to 1000°C requires robust packages including stable electrodes and protective coatings. We have developed nanostructured ultra-thin (≪ 100 nm) Pt-10%Rh / ZrO2 electrode structures grown by e-beam co-evaporation that operate at temperatures approaching 1000°C. X-ray diffraction (XRD), resistivity, and electron microscopy (EM) studies indicate incorporation of ZrO2 within the film delays recrystallization, maintaining a stable morphology. We have also developed ultra-thin (≪ 50 nm) SiAlON passivation coatings that mechanically protect the sensor surfaces, yet allow interaction with the environment. Different SiAlON stoichiometeries were produced by rf magnetron sputtering of Al and Si targets in O2/N2/Ar mixtures. The SiAlON films are amorphous and extremely smooth (≪ 1 nm rms) and remain so even after extended annealing at 1000°C. Our results are applicable to a wide range of high temperature sensor configurations.",
Adaptation of the MCDS broadcasting protocol to VANET safety applications,"Broadcast mechanisms are widely used in self-organizing wireless networks as support for other network layer protocols, in this paper we investigate the broadcast techniques proposed in literature for vehicular ad hoc network (VANET). For the safety applications the broadcast protocol has to guaranty the performance and the reliability. In this paper, we propose an implementation the minimum connected dominating set (MCDS), taking into count the safety applications constraint and the specifics of the VANET context. Simulation results demonstrate the good performances and the robustness of such protocol compared to other ones.","Broadcasting,
Wireless application protocol,
Ad hoc networks,
Road safety,
Road vehicles,
Vehicle safety,
Mobile ad hoc networks,
Network topology,
Application software,
Computer networks"
Security Implications of Cooperative Communications in Wireless Networks,"Cooperative communications is an innovative technique that is expected to change the behavior of wireless networks in the near feature. In the MAC layer, this technique defines new protocols by enabling additional collaboration from stations that otherwise will not directly participate in the transmission. A typical example of such a protocol is CoopMAC [1], a cooperative MAC protocol that involves an intermediate station or helper in the communication between a transmitter and a receiver. Under this scheme, the transmitter sends its packets to the receiver by forwarding them through the helper. In this way the protocol takes advantage of spatial diversity and faster two-hop transmission, significantly improving the performance of the network. In such an environment, where the sender relies on an intermediate helper to forward its packets to the original destination, numerous security issues may arise. The present security schemes need to be adapted in order to support end-to-end security in the source-helper-destination communication model. In this paper we discuss the potential security issues that cooperation may raise and propose two new security schemes to address those concerns. To evaluate the feasibility of the proposed algorithms, we implement them using open source drivers platform, which is explained in the paper in detail. Moreover, the paper also discusses the design challenges encounterd and share the experience and insights gained during implementation. Our implementations of the suggested techniques allow the WEP, WPA and WPA2 (802.11i) security protocols to successfully operate in the new cooperative environment.","Wireless networks,
Media Access Protocol,
Collaboration,
Transmitters,
Wireless application protocol,
Linux,
Computer security,
Computer science,
Laboratories,
Electronic mail"
LHT: A Low-Maintenance Indexing Scheme over DHTs,"DHT is a widely-used building block in P2P systems, and complex queries are gaining popularity in P2P applications. To support efficient query processing over DHTs, effective indexing structures are essential. Recently, a number of indexing schemes have been proposed. However, these schemes have focused on improving query efficiency, and as a trade-off, sacrificed maintenance efficiency — an important performance measure in the P2P context, where frequent data updating and high peer dynamism are typically incurred. In this paper, we propose LHT, a Low maintenance Hash Tree, for efficient data indexing over DHTs. LHT employs a novel naming function and a tree summarization strategy to gracefully distribute its index structure. It is adaptable to any DHT substrates, and is easy to be implemented and deployed. Experiments show that in comparison with the state-of-the-art indexing technique, LHT saves up to 75% (at least 50%) maintenance cost, and achieves better performance for exact-match queries and range queries.","Indexing,
Indexes,
Peer to peer computing,
Maintenance engineering,
Substrates,
Query processing,
Data structures"
A Novel Approach to Service Discovery in Mobile Adhoc Network,"Mobile Adhoc Network (MANET) is a network of a number of mobile routers and associated hosts, organized in a random fashion via wireless links [16]. During recent years MANET has gained enormous amount of attention and has been widely used for not only military purposes but for search-and-rescue operations, intelligent transportation system, data collection, virtual classrooms and ubiquitous computing [14]. Service Discovery is one of the most important issues in MANET. It is defined as the process of facilitating service providers to advertise their services in a dynamic way and to allow consumers to discover and access those services in an efficient and scalable manner [20]. In this paper, we are proposing a flexible and efficient approach to service discovery for MANET by extending the work of [5]. Most of the service discovery protocols proposed in literature don't provide an appropriate route from consumer to service provider. Hence after services are discovered, a route request needs to be initiated in order to access the service. In this paper, we are proposing a robust and flexible approach to service discovery for MANET that not only discovers a service provider in the vicinity of a node, but at the same time, it also provides a route to access the service. We have extended the approach proposed in [5] for efficiency by adding the push capabilities to service discovery","Mobile ad hoc networks,
Ubiquitous computing,
Vehicle dynamics,
Mobile computing,
Military computing,
Pervasive computing,
Intelligent transportation systems,
Robustness,
Ad hoc networks,
Protocols"
Alert Fusion Based on Cluster and Correlation Analysis,"For the purpose of reducing redundant alerts and false alerts as well as recognizing complicated attack scenarios, a multilevel model of alert fusion is presented. This model fuses alerts layer upon layer through primary alert reduction, alert verification, alert clustering and alert correlation. In order to construct accurate and complete attack sensors, in the phase of alert clustering, this paper proposes alert correlation method based on the similarity between alert attributes as well as based on prerequisites and consequences of attacks. The experimental results show that the model is effective and efficient in fusing large numbers of alerts.","IP networks,
Correlation,
Clustering algorithms,
Algorithm design and analysis,
Protocols,
Classification algorithms,
Knowledge based systems"
Firefly-Inspired Synchronization for Improved Dynamic Pricing in Online Markets,"We consider the problem of dynamic pricing by sellers in an online market economy using software agents called price bots. In previous research on dynamic pricing algorithms, each seller's pricebot employs either heuristics-based or learning-based techniques to determine and update the profit maximizing price for itself at certain intervals in response to changes in market dynamics. In these dynamic pricing techniques, each seller's pricebot uses only its private information such as past prices and profits to update its price in successive intervals. In this paper, we posit that the profits obtained by a pricebot can be improved if each pricebot incorporates its competitors' pricing information along with its private price and profit information in its price-update calculations. However, incorporating competitors' pricing information accurately into a pricebot's dynamic pricing algorithm is a challenging problem because competing sellers (pricebots) update their prices asynchronously and by an amount determined by each seller's private pricing strategy. Our contribution in this paper is a novel dynamic pricing algorithm that uses a distributed synchronization model observed in nature to align each seller's price with its competitors' prices. Our analytical and simulation results show that the combination of a heuristics-based pricing mechanism that uses only a seller's private information and the synchronization-based mechanism that aligns its prices with its competitors, enables a seller's pricebot to improve its profits by as much as 78% as compared to previous dynamic pricing algorithms.","Pricing,
Heuristic algorithms,
Software agents,
Algorithm design and analysis,
Computational modeling,
Computer science,
USA Councils,
Information analysis,
Analytical models,
Large-scale systems"
An Efficient Frequent Patterns Mining Algorithm Based on Apriori Algorithm and the FP-Tree Structure,"Association rule mining is to find association relationships among large data sets. Mining frequent patterns is an important aspect in association rule mining. In this paper, an efficient algorithm named Apriori-Growth based on Apriori algorithm and the FP-tree structure is presented to mine frequent patterns. The advantage of the Apriori-Growth algorithm is that it doesn't need to generate conditional pattern bases and sub- conditional pattern tree recursively. Computational results show the Apriori-Growth algorithm performs faster than Apriori algorithm, and it is almost as fast as FP-Growth, but it needs smaller memory.",
Strictly Non-Blocking Conditions for the Central-Stage Buffered Clos-Network,"We consider using the Clos-network to scale high performance routers, especially the space-memory-space (SMS) packet switches. In circuit switching, the Clos-network is responsible for pure connections and the internal links are the only blocking sources. In packet switching, however, the buffers cause additional blockings. In this letter, we first propose a scalable packet switch architecture that we call the central-stage buffered Clos-network (CBC). Then, we analyze the memory requirements for the CBC to be strictly non-blocking, especially for emulating an output-queuing packet switch. Results show that even with the additional memory blockings the CBC still inherits advantages from the Clos-network, e.g., modular design and cost efficiency.","Switches,
Packet switching,
Costs,
Scheduling algorithm,
Space technology,
Switching circuits,
Algorithm design and analysis,
Quality of service,
Computer science,
Fabrication"
Dynamic On-Line Allocation of Independent Task onto Heterogeneous Computing Systems to Maximize Load Balancing,"Heterogeneous computing (HC) Systems use different types of machines, networks, and interfaces to coordinate the execution of various task components which have different computational requirements. This variation in tasks requirements as well as machine capabilities has created a very strong need for developing Mapping techniques to decide on which task should be moved to where and when, to optimize some system performance criteria. The existing dynamic heuristics for mapping tasks in HC systems works either on-line (immediate) or in Batch mode. In batch mode, tasks are collected into a set that is examined for mapping at prescheduled times called mapping events. On contrast, on-line mode algorithms map a task onto a machine as soon as it arrives at the mapper. In this paper, we propose an on-line mapping algorithm which is called the Maximum Load Balance, or for short the MLB. It tries to minimize the makespan by maximizing the load balancing of the target system. At each task arrival, the MLB algorithm examines all the machines in the HC suite one by one looking for the one that gives the maximum system balance among all possible mappings. In contrast with the opportunistic load balancing (OLB) heuristic; which assigns a task to the machine that becomes ready next, the MLB takes into consideration both the availability of the machine as well as the execution time of the task on that machine.",
Efficient and Robust Local Mutual Exclusion in Mobile Ad Hoc Networks,"This paper presents two algorithms for the local mutual exclusion problem, an extension of the dining philosophers problem for mobile ad hoc networks. A solution to this problem allows nodes that are currently geographically close to obtain exclusive access to a resource. The algorithms exhibit different tradeoffs between response time and failure locality (the size of the neighborhood adversely affected by a node crash). The first algorithm has two variations, one of which has response time that depends very weakly on the number of nodes in the entire system and is polynomial in the maximum number of neighboring nodes; the failure locality, although not optimal, is small and grows very slowly with system size. The second algorithm has optimal failure locality and response time that is quadratic in the number of nodes. A pleasing aspect of this algorithm is that, when run in a system with no node movement, it has linear response time, improving on previous results for static algorithms with optimal failure locality.","Color,
Time factors,
Ad hoc networks,
Mobile computing,
Algorithm design and analysis,
Mobile communication,
Protocols"
Likelihood ratio in a SVM framework: Fusing linear and non-linear face classifiers,"The performance of score-level fusion algorithms is often affected by conflicting decisions generated by the constituent matchers/classifiers. This paper describes a fusion algorithm that incorporates the likelihood ratio test statistic in a support vector machine (SVM) framework in order to classify match scores originating from multiple matchers. The proposed approach also takes into account the precision and uncertainties of individual matchers. The resulting fusion algorithm is used to mitigate the effect of covariate factors in face recognition by combining the match scores of linear appearance-based face recognition algorithms with their non-linear counterparts. Experimental results on a heterogeneous face database of 910 subjects suggest that the proposed fusion algorithm can significantly improve the verification performance of a face recognition system. Thus, the contribution of this work is two-fold: (a) the design of a novel fusion technique that incorporates the likelihood ratio test-statistic in a SVM fusion framework; and (b) the application of the technique to face recognition in order to mitigate the effect of covariate factors.",
Hiding Local State in Direct Style: A Higher-Order Anti-Frame Rule,"Separation logic involves two dual forms of modularity: local reasoning makes part of the store invisible within a static scope, whereas hiding local state makes part of the store invisible outside a static scope.  In the recent literature, both idioms are explained in terms of a higher-order frame rule. I point out that this approach to hiding local state imposes continuation-passing style, which is impractical. Instead, I introduce a higher-order anti-frame rule, which permits hiding local state in directstyle. I formalize this rule in the setting of a type system, equipped with linear capabilities, for an ML-like programming language, and prove type soundness via a syntactic argument.  Several applications illustrate the expressive power of the new rule.",
Soft Errors in SRAM-FPGAs: A Comparison of Two Complementary Approaches,"As SRAM-based field-programmable gate arrays (FPGAs) are introduced in safety- or mission-critical applications, the availability of suitable Electronic Design Automation (EDA) tools for predicting systems dependability becomes mandatory for designers. Nowadays designers can opt either for workload-independent EDA tools, which provide information about system's dependability disregarding the workload the system is supposed to elaborate when deployed in the mission, or workload-dependent approaches. In this paper, we compare two tools for predicting the effects of soft errors in circuits implemented using SRAM-based FPGAs, a workload-independent one (STAR) and a workload-dependent one (FLIPPER). Experimental results show that the two tools are complementary and can be used fruitfully for obtaining accurate predictions.",
Hecataeus: A What-If Analysis Tool for Database Schema Evolution,"Databases are continuously evolving environments, where design constructs are added, removed or updated rather often. Small changes in the database configurations might impact a large number of applications and data stores around the system: queries and data entry forms can be invalidated, application programs might crash. HECATAEUS is a tool, which represents the database schema along with its dependent workload, mainly queries and views, as a uniform directed graph. The tool enables the user to create hypothetical evolution events and examine their impact over the overall graph as well as to define rules so that both syntactical and semantic correctness of the affected workload is retained.","Data analysis,
EMP radiation effects,
Computer crashes,
Spatial databases,
Database systems,
Computer science,
Information systems,
Web server,
Performance analysis,
Predictive models"
Tangible User Interfaces Compensate for Low Spatial Cognition,"This research investigates how interacting with tangible user interfaces (TUIs) affects spatial cognition. To study the impact of TUIs, a between subjects study was conducted (n=60) in which students learned about the operation of an anesthesia machine. A TUI was compared to two other interfaces commonly used in anesthesia education: (1) a Graphical User Interface (a 2D abstract simulation model of an anesthesia machine) and (2) a Physical User Interface (a real world anesthesia machine). Overall, the TUI was found to significantly compensate for low user spatial cognition in the domain of anesthesia machine training.",
Interactive Lessons for Pre-University Power Education,"A key need facing the electric power industry is the ongoing requirement to develop its future workforce. While university education is a crucial step in this process, studies have shown that many promising students are unaware of possible careers in the power industry. Many also lose interest in math and science during their high school and even middle school years. This paper presents lesson plans and associated applets designed to help address these needs, developed as a collaboration between electric power researchers and education specialists. Thus far, two units have been developed to engage pre-university students in the power area. The first unit, Power and Energy in the Home, serves as an introduction to the concepts of power and energy and provides many sample loads to illustrate the impacts of running different appliances. Special attention is paid to environmental issues by the inclusion of Energy Star appliances along with incandescent and compact fluorescent lighting. The second unit, titled The Power Grid, aims to inform students about the macroscopic picture of how energy gets from generators to loads. Many different generation technologies are included, along with external system connections to demonstrate how power is imported and exported. Discussion of line overloading, and how networks can be both beneficial and detrimental depending on circumstances, are facilitated by features built into the applet and provided in the lesson plans. The materials have been distributed to students and educators, many of whom have provided valuable feedback.",
WS-DREAM: A distributed reliability assessment Mechanism for Web Services,"It is critical to guarantee the reliability of service-oriented applications. This is because they may employ remote Web Services as components, which may easily become unavailable in the unpredictable Internet environment. This practical experience report presents a Distribute REliability Assessment Mechanism for Web Services (WS-DREAM), allowing users to carry out Web Services reliability assessment in a collaborative manner. With WS-DREAM, users in different geography locations help each other to carry out testing, and share test cases under the coordination of a centralized server. Based on this collaborative mechanism, reliability assessment for Web Services in real environment from different locations of the world becomes seamless. To illustrate the advantage of this mechanism, a prototype is implemented and a case study is carried out. Users from five locations all over the world perform reliability assessment to Web Services distributed in six countries. Over 1,000,000 test cases are executed in a collaborative manner and detailed results are provided.","Web services,
distributed processing,
groupware,
software reliability"
Searching Musical Audio Using Symbolic Queries,"Finding a piece of music based on its content is a key problem in music in for music information retrieval . For example, a user may be interested in finding music based on knowledge of only a small fragment of the overall tune. In this paper, we consider the searching of musical audio using symbolic queries. We first propose a relative pitch approach for representing queries and pieces. Experiments show that this technique, while effective, works best when the whole tune is used as a query. We then present an algorithm for matching based on a pitch classes approach, using the longest common subsequence between a query and target. Experimental evaluation shows that our technique is highly effective, with a mean average precision of 0.77 on a collection of 1808 recordings. Significantly, our technique is robust for truncated queries, being able to maintain effectiveness and to retrieve correct answers whether the query fragment is taken from the beginning, middle, or end of a piece. This represents a significant reduction in the burden placed on users when formulating queries.","Music information retrieval,
Instruments,
Dynamic programming,
Content based retrieval,
Robustness,
Multiple signal classification,
Timbre,
Computer science,
Information technology"
