Title,Abstract,Keywords
A Fast Learning Algorithm for Deep Belief Nets,"We show how to use “complementary priors” to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.",
A Random Linear Network Coding Approach to Multicast,"We present a distributed random linear network coding approach for transmission and compression of information in general multisource multicast networks. Network nodes independently and randomly select linear mappings from inputs onto output links over some field. We show that this achieves capacity with probability exponentially approaching 1 with the code length. We also demonstrate that random linear coding performs compression when necessary in a network, generalizing error exponents for linear Slepian-Wolf coding in a natural way. Benefits of this approach are decentralized operation and robustness to network changes or link failures. We show that this approach can take advantage of redundant network capacity for improved success probability and robustness. We illustrate some potential advantages of random linear network coding over routing in two examples of practical scenarios: distributed network operation and networks with dynamically varying connections. Our derivation of these results also yields a new bound on required field size for centralized network coding on general multicast networks",
Self-Adapting Control Parameters in Differential Evolution: A Comparative Study on Numerical Benchmark Problems,"We describe an efficient technique for adapting control parameter settings associated with differential evolution (DE). The DE algorithm has been used in many practical cases and has demonstrated good convergence properties. It has only a few control parameters, which are kept fixed throughout the entire evolutionary process. However, it is not an easy task to properly set control parameters in DE. We present an algorithm-a new version of the DE algorithm-for obtaining self-adaptive control parameter settings that show good performance on numerical benchmark problems. The results show that our algorithm with self-adaptive control parameter settings is better than, or at least comparable to, the standard DE algorithm and evolutionary algorithms from literature when considering the quality of the solutions obtained",
MaxProp: Routing for Vehicle-Based Disruption-Tolerant Networks,,
Cooperative Sensing among Cognitive Radios,"Cognitive Radios have been advanced as a technology for the opportunistic use of under-utilized spectrum since they are able to sense the spectrum and use frequency bands if no Primary user is detected. However, the required sensitivity is very demanding since any individual radio might face a deep fade. We propose light-weight cooperation in sensing based on hard decisions to mitigate the sensitivity requirements on individual radios. We show that the ""link budget"" that system designers have to reserve for fading is a significant function of the required probability of detection. Even a few cooperating users (~10-20) facing independent fades are enough to achieve practical threshold levels by drastically reducing individual detection requirements. Hard decisions perform almost as well as soft decisions in achieving these gains. Cooperative gains in a environment where shadowing is correlated, is limited by the cooperation footprint (area in which users cooperate). In essence, a few independent users are more robust than many correlated users. Unfortunately, cooperative gain is very sensitive to adversarial/failing Cognitive Radios. Radios that fail in a known way (always report the presence/absence of a Primary user) can be compensated for by censoring them. On the other hand, radios that fail in unmodeled ways or may be malicious, introduce a bound on achievable sensitivity reductions. As a rule of thumb, if we believe that 1/N users can fail in an unknown way, then the cooperation gains are limited to what is possible with N trusted users.",
The Semantic Web Revisited,"The article included many scenarios in which intelligent agents and bots undertook tasks on behalf of their human or corporate owners. Of course, shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks: they have little ability to interact with heterogeneous data and information types. Because we haven't yet delivered large-scale, agent-based mediation, some commentators argue that the semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years",
Deploying a wireless sensor network on an active volcano,"Augmenting heavy and power-hungry data collection equipment with lighten smaller wireless sensor network nodes leads to faster, larger deployments. Arrays comprising dozens of wireless sensor nodes are now possible, allowing scientific studies that aren't feasible with traditional instrumentation. Designing sensor networks to support volcanic studies requires addressing the high data rates and high data fidelity these studies demand. The authors' sensor-network application for volcanic data collection relies on triggered event detection and reliable data retrieval to meet bandwidth and data-quality demands.",
L-diversity: privacy beyond k-anonymity,"Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called \kappa-anonymity has gained popularity. In a \kappa-anonymized dataset, each record is indistinguishable from at least k—1 other records with respect to certain ""identifying"" attributes. In this paper we show with two simple attacks that a \kappa-anonymized dataset has some subtle, but severe privacy problems. First, we show that an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. Second, attackers often have background knowledge, and we show that \kappa-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks and we propose a novel and powerful privacy definition called \ell-diversity. In addition to building a formal foundation for \ell-diversity, we show in an experimental evaluation that \ell-diversity is practical and can be implemented efficiently.",
SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition,"We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(±0.56%) at 15 training images per class, and 66.23%(±0.48%) at 30 training images.",
Secrecy Capacity of Wireless Channels,"We consider the transmission of confidential data over wireless channels with multiple communicating parties. Based on an information-theoretic problem formulation in which two legitimate partners communicate over a quasi-static fading channel and an eavesdropper observes their transmission through another independent quasi-static fading channel, we define the secrecy capacity in terms of outage probability and provide a complete characterization of the maximum transmission rate at which the eavesdropper is unable to decode any information. In sharp contrast with known results for Gaussian wiretap channels (without feedback), our contribution shows that in the presence of fading information-theoretic security is achievable even when the eavesdropper has a better average signal-to-noise ratio (SNR) than the legitimate receiver - fading thus turns out to be a friend and not a foe",
MILES: Multiple-Instance Learning via Embedded Instance Selection,"Multiple-instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple-instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (multiple-instance learning via embedded instance selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm SVM is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, MILES demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty","Drugs,
Application software,
Computer vision,
Learning systems,
Supervised learning,
Support vector machines,
Support vector machine classification,
Robustness,
Labeling,
Uncertainty"
A Theory of Network Localization,"In this paper, we provide a theoretical foundation for the problem of network localization in which some nodes know their locations and other nodes determine their locations by measuring the distances to their neighbors. We construct grounded graphs to model network localization and apply graph rigidity theory to test the conditions for unique localizability and to construct uniquely localizable networks. We further study the computational complexity of network localization and investigate a subclass of grounded graphs where localization can be computed efficiently. We conclude with a discussion of localization in sensor networks where the sensors are placed randomly",
Locating and tracking multiple dynamic optima by a particle swarm model using speciation,"This paper proposes an improved particle swarm optimizer using the notion of species to determine its neighborhood best values for solving multimodal optimization problems and for tracking multiple optima in a dynamic environment. In the proposed species-based particle swam optimization (SPSO), the swarm population is divided into species subpopulations based on their similarity. Each species is grouped around a dominating particle called the species seed. At each iteration step, species seeds are identified from the entire population, and then adopted as neighborhood bests for these individual species groups separately. Species are formed adaptively at each step based on the feedback obtained from the multimodal fitness landscape. Over successive iterations, species are able to simultaneously optimize toward multiple optima, regardless of whether they are global or local optima. Our experiments on using the SPSO to locate multiple optima in a static environment and a dynamic SPSO (DSPSO) to track multiple changing optima in a dynamic environment have demonstrated that SPSO is very effective in dealing with multimodal optimization functions in both environments","Particle tracking,
Particle swarm optimization,
Feedback,
Computer science,
Information technology,
Australia,
Shape"
Using Multiple Segmentations to Discover Objects and their Extent in Image Collections,"Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe.","Image segmentation,
Layout,
Image recognition,
Text analysis,
Artificial intelligence,
Laboratories,
Computer science,
Object recognition,
Concrete,
Information retrieval"
MORPH: a longitudinal image database of normal adult age-progression,"This paper details MORPH a longitudinal face database developed for researchers investigating all facets of adult age-progression, e.g. face modeling, photo-realistic animation, face recognition, etc. This database contributes to several active research areas, most notably face recognition, by providing: the largest set of publicly available longitudinal images; longitudinal spans from a few months to over twenty years; and, the inclusion of key physical parameters that affect aging appearance. The direct contribution of this data corpus for face recognition is highlighted in the evaluation of a standard face recognition algorithm, which illustrates the impact that age-progression, has on recognition rates. Assessment of the efficacy of this algorithm is evaluated against the variables of gender and racial origin. This work further concludes that the problem of age-progression on face recognition (FR) is not unique to the algorithm used in this work",
Gossip-based ad hoc routing,"Many ad hoc routing protocols are based on some variant of flooding. Despite various optimizations of flooding, many routing messages are propagated unnecessarily. We propose a gossiping-based approach, where each node forwards a message with some probability, to reduce the overhead of the routing protocols. Gossiping exhibits bimodal behavior in sufficiently large networks: in some executions, the gossip dies out quickly and hardly any node gets the message; in the remaining executions, a substantial fraction of the nodes gets the message. The fraction of executions in which most nodes get the message depends on the gossiping probability and the topology of the network. In the networks we have considered, using gossiping probability between 0.6 and 0.8 suffices to ensure that almost every node gets the message in almost every execution. For large networks, this simple gossiping protocol uses up to 35% fewer messages than flooding, with improved performance. Gossiping can also be combined with various optimizations of flooding to yield further benefits. Simulations show that adding gossiping to AODV results in significant performance improvement, even in networks as small as 150 nodes. Our results suggest that the improvement should be even more significant in larger networks","Routing protocols,
Floods,
Ad hoc networks,
Mobile communication,
Computer science,
Global Positioning System,
Network topology,
Spread spectrum communication,
Power supplies,
Wireless sensor networks"
An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol,,
Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions,"We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice","Nearest neighbor searches,
Data structures,
Image databases,
Spatial databases,
Approximation algorithms,
Probes,
Decoding,
Lattices,
Data compression,
Data mining"
Raptor codes on binary memoryless symmetric channels,"In this paper, we will investigate the performance of Raptor codes on arbitrary binary input memoryless symmetric channels (BIMSCs). In doing so, we generalize some of the results that were proved before for the erasure channel. We will generalize the stability condition to the class of Raptor codes. This generalization gives a lower bound on the fraction of output nodes of degree 2 of a Raptor code if the error probability of the belief-propagation decoder converges to zero. Using information-theoretic arguments, we will show that if a sequence of output degree distributions is to achieve the capacity of the underlying channel, then the fraction of nodes of degree 2 in these degree distributions has to converge to a certain quantity depending on the channel. For the class of erasure channels this quantity is independent of the erasure probability of the channel, but for many other classes of BIMSCs, this fraction depends on the particular channel chosen. This result has implications on the ""universality"" of Raptor codes for classes other than the class of erasure channels, in a sense that will be made more precise in the paper. We will also investigate the performance of specific Raptor codes which are optimized using a more exact version of the Gaussian approximation technique.","Iterative decoding,
Iterative algorithms,
Parity check codes,
Information theory,
Computer science,
Algorithm design and analysis,
Stability,
Error probability,
Gaussian approximation,
Bipartite graph"
A large-scale study of failures in high-performance computing systems,"Designing highly dependable systems requires a good understanding of failure characteristics. Unfortunately, little raw data on failures in large IT installations is publicly available. This paper analyzes failure data recently made publicly available by one of the largest high-performance computing sites. The data has been collected over the past 9 years at Los Alamos National Laboratory and includes 23000 failures recorded on more than 20 different systems, mostly large clusters of SMP and NUMA nodes. We study the statistics of the data, including the root cause of failures, the mean time between failures, and the mean time to repair. We find for example that average failure rates differ wildly across systems, ranging from 20-1000 failures per year, and that time between failures is modeled well by a Weibull distribution with decreasing hazard rate. From one system to another, mean repair time varies from less than an hour to more than a day, and repair times are well modeled by a lognormal distribution","Large-scale systems,
Read only memory,
Failure analysis,
Testing,
Computer science,
Statistics,
Reactive power,
Electronic switching systems,
Resource management,
Availability"
Optimal Surface Segmentation in Volumetric Images-A Graph-Theoretic Approach,"Efficient segmentation of globally optimal surfaces representing object boundaries in volumetric data sets is important and challenging in many medical image analysis applications. We have developed an optimal surface detection method capable of simultaneously detecting multiple interacting surfaces, in which the optimality is controlled by the cost functions designed for individual surfaces and by several geometric constraints defining the surface smoothness and interrelations. The method solves the surface segmentation problem by transforming it into computing a minimum s{\hbox{-}} t cut in a derived arc-weighted directed graph. The proposed algorithm has a low-order polynomial time complexity and is computationally efficient. It has been extensively validated on more than 300 computer-synthetic volumetric images, 72 CT-scanned data sets of different-sized plexiglas tubes, and tens of medical images spanning various imaging modalities. In all cases, the approach yielded highly accurate results. Our approach can be readily extended to higher-dimensional image segmentation.","Image segmentation,
Biomedical imaging,
Image sequence analysis,
Polynomials,
Image analysis,
Solid modeling,
Optimal control,
Cost function,
Information analysis"
Computer analysis of computed tomography scans of the lung: a survey,"Current computed tomography (CT) technology allows for near isotropic, submillimeter resolution acquisition of the complete chest in a single breath hold. These thin-slice chest scans have become indispensable in thoracic radiology, but have also substantially increased the data load for radiologists. Automating the analysis of such data is, therefore, a necessity and this has created a rapidly developing research area in medical imaging. This paper presents a review of the literature on computer analysis of the lungs in CT scans and addresses segmentation of various pulmonary structures, registration of chest scans, and applications aimed at detection, classification and quantification of chest abnormalities. In addition, research trends and challenges are identified and directions for future research are discussed.",
Extracting a Mobility Model from Real User Traces,,
Cross-Level Sensor Network Simulation with COOJA,"Simulators for wireless sensor networks are a valuable tool for system development. However, current simulators can only simulate a single level of a system at once. This makes system development and evolution difficult since developers cannot use the same simulator for both high-level algorithm development and low-level development such as device-driver implementations. We propose cross-level simulation, a novel type of wireless sensor network simulation that enables holistic simultaneous simulation at different levels. We present an implementation of such a simulator, COOJA, a simulator for the Contiki sensor node operating system. COOJA allows for simultaneous simulation at the network level, the operating system level, and the machine code instruction set level. With COOJA, we show the feasibility of the cross-level simulation approach","Hardware,
Java,
Operating systems,
Sensor systems,
Wireless sensor networks,
Computational modeling,
Computer simulation,
Programming,
Computer science,
Software systems"
Towards analytic modeling for CSMA/CA based MAC protocol in wireless multi-hop networks - A simulation study,"This paper reports on research into the relevance and prevalence of different events and conditions occurring at the Medium Access Control (MAC) layer in a multi-hop wireless network scenario. The purpose of this research is to determine which events and conditions are essential to an analytic model, and which can be ignored for the sake of simplicity, because simplicity and accuracy are generally competing factors in the design of a model but complimentary factors in the usefulness of a model. The research focuses on one important class of MAC protocols, Carrier Sense Multiple Access schemes with Collision Avoidance (CSMA/CA) as implemented in IEEE 802.11 type networks. The behavior of these protocols in the multi-hop scenario is significantly different from their behavior in the more commonly studied single-hop scenario. The research identifies what events are possible in the multi-hop scenario and uses a detailed simulator to evaluate their frequency and effect in different topologies and with different protocol parameters. Unfairness, different types of collisions, throughput, and other factors are considered. The observations are used to formulate directions for future analytic modeling approaches.",
Multiple Nose Region Matching for 3D Face Recognition under Varying Facial Expression,"An algorithm is proposed for 3D face recognition in the presence of varied facial expressions. It is based on combining the match scores from matching multiple overlapping regions around the nose. Experimental results are presented using the largest database employed to date in 3D face recognition studies, over 4,000 scans of 449 subjects. Results show substantial improvement over matching the shape of a single larger frontal face region. This is the first approach to use multiple overlapping regions around the nose to handle the problem of expression variation",
Networked PID control,"PID controllers are the most widely used control scheme in industry. Traditionally these controllers have been implemented in analog or digital form on specifically dedicated communication links. However, there has been significant recent interest into deploying control over general purpose communication channels, which allow one to transmit data, voice and control signals. Naturally the successful design of such a networked control system necessitates a blend of techniques which reflect both control and communication aspects. The present work examines the effect of channel noise (or, equivalently, of channel capacity) on closed loop system behaviour. It is shown, and experimentally verified, how performance can be optimized via signal coding",
Design Guidelines for Maximizing Lifetime and Avoiding Energy Holes in Sensor Networks with Uniform Distribution and Uniform Reporting,,
Penalized weighted least-squares approach to sinogram noise reduction and image reconstruction for low-dose X-ray computed tomography,"Reconstructing low-dose X-ray computed tomography (CT) images is a noise problem. This work investigated a penalized weighted least-squares (PWLS) approach to address this problem in two dimensions, where the WLS considers first- and second-order noise moments and the penalty models signal spatial correlations. Three different implementations were studied for the PWLS minimization. One utilizes a Markov random field (MRF) Gibbs functional to consider spatial correlations among nearby detector bins and projection views in sinogram space and minimizes the PWLS cost function by iterative Gauss-Seidel algorithm. Another employs Karhunen-Loeve (KL) transform to de-correlate data signals among nearby views and minimizes the PWLS adaptively to each KL component by analytical calculation, where the spatial correlation among nearby bins is modeled by the same Gibbs functional. The third one models the spatial correlations among image pixels in image domain also by a MRF Gibbs functional and minimizes the PWLS by iterative successive over-relaxation algorithm. In these three implementations, a quadratic functional regularization was chosen for the MRF model. Phantom experiments showed a comparable performance of these three PWLS-based methods in terms of suppressing noise-induced streak artifacts and preserving resolution in the reconstructed images. Computer simulations concurred with the phantom experiments in terms of noise-resolution tradeoff and detectability in low contrast environment. The KL-PWLS implementation may have the advantage in terms of computation for high-resolution dynamic low-dose CT imaging",
"An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks","Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this understanding and how software development environments might be involved, a study was performed in which developers were given an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however, they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers seek, relate, and collect information in a more effective and explicit manner",
Predictive Modeling of the NBTI Effect for Reliable Design,"This paper presents a predictive model for the negative bias temperature instability (NBTI) of PMOS under both short term and long term operation. Based on the reaction-diffusion (R-D) mechanism, this model accurately captures the dependence of NBTI on the oxide thickness (tox), the diffusing species (H or H2) and other key transistor and design parameters. In addition, a closed form expression was derived for the threshold voltage change (DeltaVth ) under multiple cycle dynamic operation. Model accuracy and efficiency were verified with 90-nm experimental and simulation data. The impact of NBTI was further investigated on representative digital circuits",
"Multiclass Object Recognition with Sparse, Localized Features","We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.",
Making Sense of Sensemaking 1: Alternative Perspectives,Sensemaking has become an umbrella term for efforts at building intelligent systems. This essay examines sensemaking from various perspectives to see if we can separate the things that are doable from the things that seem more like pie-in-the-sky,
Improving Traffic Locality in BitTorrent via Biased Neighbor Selection,"Peer-to-peer (P2P) applications such as BitTorrent ignore traffic costs at ISPs and generate a large amount of cross-ISP traffic. As a result, ISPs often throttle BitTorrent traffic to control the cost. In this paper, we examine a new approach to enhance BitTorrent traffic locality, biased neighbor selection, in which a peer chooses the majority, but not all, of its neighbors from peers within the same ISP. Using simulations, we show that biased neighbor selection maintains the nearly optimal performance of Bit- Torrent in a variety of environments, and fundamentally reduces the cross-ISP traffic by eliminating the traffic’s linear growth with the number of peers. Key to its performance is the rarest first piece replication algorithm used by Bit- Torrent clients. Compared with existing locality-enhancing approaches such as bandwidth limiting, gateway peers, and caching, biased neighbor selection requires no dedicated servers and scales to a large number of BitTorrent networks.",
Activity Recognition of Assembly Tasks Using Body-Worn Microphones and Accelerometers,"In order to provide relevant information to mobile users, such as workers engaging in the manual tasks of maintenance and assembly, a wearable computer requires information about the user's specific activities. This work focuses on the recognition of activities that are characterized by a hand motion and an accompanying sound. Suitable activities can be found in assembly and maintenance work. Here, we provide an initial exploration into the problem domain of continuous activity recognition using on-body sensing. We use a mock ""wood workshop"" assembly task to ground our investigation. We describe a method for the continuous recognition of activities (sawing, hammering, filing, drilling, grinding, sanding, opening a drawer, tightening a vise, and turning a screwdriver) using microphones and three-axis accelerometers mounted at two positions on the user's arms. Potentially ""interesting"" activities are segmented from continuous streams of data using an analysis of the sound intensity detected at the two different locations. Activity classification is then performed on these detected segments using linear discriminant analysis (LDA) on the sound channel and hidden Markov models (HMMs) on the acceleration data. Four different methods at classifier fusion are compared for improving these classifications. Using user-dependent training, we obtain continuous average recall and precision rates (for positive activities) of 78 percent and 74 percent, respectively. Using user-independent training (leave-one-out across five users), we obtain recall rates of 66 percent and precision rates of 63 percent. In isolation, these activities were recognized with accuracies of 98 percent, 87 percent, and 95 percent for the user-dependent, user-independent, and user-adapted cases, respectively",
Random Multispace Quantization as an Analytic Mechanism for BioHashing of Biometric and Random Identity Inputs,"Biometric analysis for identity verification is becoming a widespread reality. Such implementations necessitate large-scale capture and storage of biometric data, which raises serious issues in terms of data privacy and (if such data is compromised) identity theft. These problems stem from the essential permanence of biometric data, which (unlike secret passwords or physical tokens) cannot be refreshed or reissued if compromised. Our previously presented biometric-hash framework prescribes the integration of external (password or token-derived) randomness with user-specific biometrics, resulting in bitstring outputs with security characteristics (i.e., noninvertibility) comparable to cryptographic ciphers or hashes. The resultant BioHashes are hence cancellable, i.e., straightforwardly revoked and reissued (via refreshed password or reissued token) if compromised. BioHashing furthermore enhances recognition effectiveness, which is explained in this paper as arising from the random multispace quantization (RMQ) of biometric and external random inputs",
ARMAR-III: An Integrated Humanoid Platform for Sensory-Motor Control,"In this paper, we present a new humanoid robot currently being developed for applications in human-centered environments. In order for humanoid robots to enter human-centered environments, it is indispensable to equip them with manipulative, perceptive and communicative skills necessary for real-time interaction with the environment and humans. The goal of our work is to provide reliable and highly integrated humanoid platforms which on the one hand allow the implementation and tests of various research activities and on the other hand the realization of service tasks in a household scenario. We introduce the different subsystems of the robot. We present the kinematics, sensors, and the hardware and software architecture. We propose a hierarchically organized architecture and introduce the mapping of the functional features in this architecture into hardware and software modules. We also describe different skills related to real-time object localization and motor control, which have been realized and integrated into the entire control architecture",
A Multiobjective Optimization-Based Evolutionary Algorithm for Constrained Optimization,"A considerable number of constrained optimization evolutionary algorithms (COEAs) have been proposed due to increasing interest in solving constrained optimization problems (COPs) by evolutionary algorithms (EAs). In this paper, we first review existing COEAs. Then, a novel EA for constrained optimization is presented. In the process of population evolution, our algorithm is based on multiobjective optimization techniques, i.e., an individual in the parent population may be replaced if it is dominated by a nondominated individual in the offspring population. In addition, three models of a population-based algorithm-generator and an infeasible solution archiving and replacement mechanism are introduced. Furthermore, the simplex crossover is used as a recombination operator to enrich the exploration and exploitation abilities of the approach proposed. The new approach is tested on 13 well-known benchmark functions, and the empirical evidence suggests that it is robust, efficient, and generic when handling linear/nonlinear equality/inequality constraints. Compared with some other state-of-the-art algorithms, our algorithm remarkably outperforms them in terms of the best, mean, and worst objective function values and the standard deviations. It is noteworthy that our algorithm does not require the transformation of equality constraints into inequality constraints",
Constrained band selection for hyperspectral imagery,"Constrained energy minimization (CEM) has shown effective in hyperspectral target detection. It linearly constrains a desired target signature while minimizing interfering effects caused by other unknown signatures. This paper explores this idea for band selection and develops a new approach to band selection, referred to as constrained band selection (CBS) for hyperspectral imagery. It interprets a band image as a desired target signature vector while considering other band images as unknown signature vectors. As a result, the proposed CBS using the concept of the CEM to linearly constrain a band image, while also minimizing band correlation or dependence provided by other band images, is referred to as CEM-CBS. Four different criteria referred to as Band Correlation Minimization (BCM), Band Correlation Constraint (BCC), Band Dependence Constraint (BDC), and Band Dependence Minimization (BDM) are derived for CEM-CBS.. Since dimensionality resulting from conversion of a band image to a vector may be huge, the CEM-CBS is further reinterpreted as linearly constrained minimum variance (LCMV)-based CBS by constraining a band image as a matrix where the same four criteria, BCM, BCC, BDC, and BDM, can be also used for LCMV-CBS. In order to determine the number of bands required to select p, a recently developed concept, called virtual dimensionality, is used to estimate the p. Once the p is determined, a set of p desired bands can be selected by the CEM/LCMV-CBS. Finally, experiments are conducted to substantiate the proposed CEM/LCMV-CBS four criteria, BCM, BCC, BDC, and BDM, in comparison with variance-based band selection, information divergence-based band selection, and uniform band selection.",
Activity recognition and monitoring using multiple sensors on different body positions,"The design of an activity recognition and monitoring system based on the eWatch, multi-sensor platform worn on different body positions, is presented in this paper. The system identifies the user's activity in realtime using multiple sensors and records the classification results during a day. We compare multiple time domain feature sets and sampling rates, and analyze the tradeoff between recognition accuracy and computational complexity. The classification accuracy on different body positions used for wearing electronic devices was evaluated","Wearable sensors,
Sensor systems,
Accelerometers,
Hardware,
Sensor arrays,
Flash memory,
Computerized monitoring,
Computer science,
Wrist,
Cellular phones"
Generalized Overlap Measures for Evaluation and Validation in Medical Image Analysis,"Measures of overlap of labelled regions of images, such as the Dice and Tanimoto coefficients, have been extensively used to evaluate image registration and segmentation algorithms. Modern studies can include multiple labels defined on multiple images yet most evaluation schemes report one overlap per labelled region, simply averaged over multiple images. In this paper, common overlap measures are generalized to measure the total overlap of ensembles of labels defined on multiple test images and account for fractional labels using fuzzy set theory. This framework allows a single ""figure-of-merit"" to be reported which summarises the results of a complex experiment by image pair, by label or overall. A complementary measure of error, the overlap distance, is defined which captures the spatial extent of the nonoverlapping part and is related to the Hausdorff distance computed on grey level images. The generalized overlap measures are validated on synthetic images for which the overlap can be computed analytically and used as similarity measures in nonrigid registration of three-dimensional magnetic resonance imaging (MRI) brain images. Finally, a pragmatic segmentation ground truth is constructed by registering a magnetic resonance atlas brain to 20 individual scans, and used with the overlap measures to evaluate publicly available brain segmentation algorithms",
Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria,"The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then; compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment",
"Retinal Vessel Centerline Extraction Using Multiscale Matched Filters, Confidence and Edge Measures","Motivated by the goals of improving detection of low-contrast and narrow vessels and eliminating false detections at nonvascular structures, a new technique is presented for extracting vessels in retinal images. The core of the technique is a new likelihood ratio test that combines matched-filter responses, confidence measures and vessel boundary measures. Matched filter responses are derived in scale-space to extract vessels of widely varying widths. A vessel confidence measure is defined as a projection of a vector formed from a normalized pixel neighborhood onto a normalized ideal vessel profile. Vessel boundary measures and associated confidences are computed at potential vessel boundaries. Combined, these responses form a six-dimensional measurement vector at each pixel. A training technique is used to develop a mapping of this vector to a likelihood ratio that measures the ""vesselness"" at each pixel. Results comparing this vesselness measure to matched filters alone and to measures based on the Hessian of intensities show substantial improvements, both qualitatively and quantitatively. The Hessian can be used in place of the matched filter to obtain similar but less-substantial improvements or to steer the matched filter by preselecting kernel orientations. Finally, the new vesselness likelihood ratio is embedded into a vessel tracing framework, resulting in an efficient and effective vessel centerline extraction algorithm",
rDCF: A Relay-Enabled Medium Access Control Protocol for Wireless Ad Hoc Networks,"It is well known that IEEE 802.11 provides a physical layer multirate capability and, hence, MAC layer mechanisms are needed to exploit this capability. Several solutions have been proposed to achieve this goal. However, these solutions only consider how to exploit good channel quality for the direct link between the sender and the receiver. Since IEEE 802.11 supports multiple transmission rates in response to different channel conditions, data packets may be delivered faster through a relay node than through the direct link if the direct link has low quality and low rate. In this paper, we propose a novel MAC layer relay-enabled distributed coordination function (DCF) protocol, called rDCF, to further exploit the physical layer multirate capability. We design a protocol to assist the sender, the relay node, and the receiver to reach an agreement on which data rate to use and whether to transmit the data through a relay node. Considering various issues, such as, bandwidth utilization, channel errors, and security, we propose techniques to further improve the performance of rDCF. Simulation results show that rDCF can significantly reduce the packet delay, improve the system throughput, and alleviate the impact of channel errors on fairness",
A visual analytics agenda,"Researchers have made significant progress in disciplines such as scientific and information visualization, statistically based exploratory and confirmatory analysis, data and knowledge representations, and perceptual and cognitive sciences. Although some research is being done in this area, the pace at which new technologies and technical talents are becoming available is far too slow to meet the urgent need. National Visualization and Analytics Center's goal is to advance the state of the science to enable analysts to detect the expected and discover the unexpected from massive and dynamic information streams and databases consisting of data of multiple types and from multiple sources, even though the data are often conflicting and incomplete. Visual analytics is a multidisciplinary field that includes the following focus areas: (i) analytical reasoning techniques, (ii) visual representations and interaction techniques, (iii) data representations and transformations, (iv) techniques to support production, presentation, and dissemination of analytical results. The R&D agenda for visual analytics addresses technical needs for each of these focus areas, as well as recommendations for speeding the movement of promising technologies into practice. This article provides only the concise summary of the R&D agenda. We encourage reading, discussion, and debate as well as active innovation toward the agenda for visual analysis.",
Stability and Convergence Properties of Dynamic Average Consensus Estimators,"We analyze two different estimation algorithms for dynamic average consensus in sensing and communication networks, a proportional algorithm and a proportional-integral algorithm. We investigate the stability properties of these estimators under changing inputs and network topologies as well as their convergence properties under constant or slowly-varying inputs. In doing so, we discover that the more complex proportional-integral algorithm has performance benefits over the simpler proportional algorithm",
AnySee: Peer-to-Peer Live Streaming,,
A faster algorithm for calculating hypervolume,"We present an algorithm for calculating hypervolume exactly, the Hypervolume by Slicing Objectives (HSO) algorithm, that is faster than any that has previously been published. HSO processes objectives instead of points, an idea that has been considered before but that has never been properly evaluated in the literature. We show that both previously studied exact hypervolume algorithms are exponential in at least the number of objectives and that although HSO is also exponential in the number of objectives in the worst case, it runs in significantly less time, i.e., two to three orders of magnitude less for randomly generated and benchmark data in three to eight objectives. Thus, HSO increases the utility of hypervolume, both as a metric for general optimization algorithms and as a diversity mechanism for evolutionary algorithms.",
3D People Tracking with Gaussian Process Dynamical Models,"We advocate the use of Gaussian Process Dynamical Models (GPDMs) for learning human pose and motion priors for 3D people tracking. A GPDM provides a lowdimensional embedding of human motion data, with a density function that gives higher probability to poses and motions close to the training data. With Bayesian model averaging a GPDM can be learned from relatively small amounts of data, and it generalizes gracefully to motions outside the training set. Here we modify the GPDM to permit learning from motions with significant stylistic variation. The resulting priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and significant occlusions.",
Single- and multiobjective evolutionary optimization assisted by Gaussian random field metamodels,"This paper presents and analyzes in detail an efficient search method based on evolutionary algorithms (EA) assisted by local Gaussian random field metamodels (GRFM). It is created for the use in optimization problems with one (or many) computationally expensive evaluation function(s). The role of GRFM is to predict objective function values for new candidate solutions by exploiting information recorded during previous evaluations. Moreover, GRFM are able to provide estimates of the confidence of their predictions. Predictions and their confidence intervals predicted by GRFM are used by the metamodel assisted EA. It selects the promising members in each generation and carries out exact, costly evaluations only for them. The extensive use of the uncertainty information of predictions for screening the candidate solutions makes it possible to significantly reduce the computational cost of singleand multiobjective EA. This is adequately demonstrated in this paper by means of mathematical test cases and a multipoint airfoil design in aerodynamics",
Supervised Learning of Edges and Object Boundaries,"Edge detection is one of the most studied problems in computer vision, yet it remains a very challenging task. It is difficult since often the decision for an edge cannot be made purely based on low level cues such as gradient, instead we need to engage all levels of information, low, middle, and high, in order to decide where to put edges. In this paper we propose a novel supervised learning algorithm for edge and object boundary detection which we refer to as Boosted Edge Learning or BEL for short. A decision of an edge point is made independently at each location in the image; a very large aperture is used providing significant context for each decision. In the learning stage, the algorithm selects and combines a large number of features across different scales in order to learn a discriminative model using an extended version of the Probabilistic Boosting Tree classification algorithm. The learning based framework is highly adaptive and there are no parameters to tune. We show applications for edge detection in a number of specific image domains as well as on natural images. We test on various datasets including the Berkeley dataset and the results obtained are very good.",
Nonbinary Stabilizer Codes Over Finite Fields,"One formidable difficulty in quantum communication and computation is to protect information-carrying quantum states against undesired interactions with the environment. To address this difficulty, many good quantum error-correcting codes have been derived as binary stabilizer codes. Fault-tolerant quantum computation prompted the study of nonbinary quantum codes, but the theory of such codes is not as advanced as that of binary quantum codes. This paper describes the basic theory of stabilizer codes over finite fields. The relation between stabilizer codes and general quantum codes is clarified by introducing a Galois theory for these objects. A characterization of nonbinary stabilizer codes over Fq in terms of classical codes over Fq 2 is provided that generalizes the well-known notion of additive codes over F4 of the binary case. This paper also derives lower and upper bounds on the minimum distance of stabilizer codes, gives several code constructions, and derives numerous families of stabilizer codes, including quantum Hamming codes, quadratic residue codes, quantum Melas codes, quantum Bose-Chaudhuri-Hocquenghem (BCH) codes, and quantum character codes. The puncturing theory by Rains is generalized to additive codes that are not necessarily pure. Bounds on the maximal length of maximum distance separable stabilizer codes are given. A discussion of open problems concludes this paper",
Counting Crowded Moving Objects,"In its full generality, motion analysis of crowded objects necessitates recognition and segmentation of each moving entity. The difficulty of these tasks increases considerably with occlusions and therefore with crowding. When the objects are constrained to be of the same kind, however, partitioning of densely crowded semi-rigid objects can be accomplished by means of clustering tracked feature points. We base our approach on a highly parallelized version of the KLT tracker in order to process the video into a set of feature trajectories. While such a set of trajectories provides a substrate for motion analysis, their unequal lengths and fragmented nature present difficulties for subsequent processing. To address this, we propose a simple means of spatially and temporally conditioning the trajectories. Given this representation, we integrate it with a learned object descriptor to achieve a segmentation of the constituent motions. We present experimental results for the problem of estimating the number of moving objects in a dense crowd as a function of time.","Computer vision,
Motion segmentation,
Motion analysis,
Karhunen-Loeve transforms,
Trajectory,
Image motion analysis,
Optical computing,
Computer science,
Humans,
Animals"
Morphological segmentation and partial volume analysis for volumetry of solid pulmonary lesions in thoracic CT scans,"Volumetric growth assessment of pulmonary lesions is crucial to both lung cancer screening and oncological therapy monitoring. While several methods for small pulmonary nodules have previously been presented, the segmentation of larger tumors that appear frequently in oncological patients and are more likely to be complexly interconnected with lung morphology has not yet received much attention. We present a fast, automated segmentation method that is based on morphological processing and is suitable for both small and large lesions. In addition, the proposed approach addresses clinical challenges to volume assessment such as variations in imaging protocol or inspiration state by introducing a method of segmentation-based partial volume analysis (SPVA) that follows on the segmentation procedure. Accuracy and reproducibility studies were performed to evaluate the new algorithms. In vivo interobserver and interscan studies on low-dose data from eight clinical metastasis patients revealed that clinically significant volume change can be detected reliably and with negligible computation time by the presented methods. In addition, phantom studies were conducted. Based on the segmentation performed with the proposed method, the performance of the SPVA volumetry method was compared with the conventional technique on a phantom that was scanned with different dosages and reconstructed with varying parameters. Both systematic and absolute errors were shown to be reduced substantially by the SPVA method. The method was especially successful in accounting for slice thickness and reconstruction kernel variations, where the median error was more than halved in comparison to the conventional approach.","Solids,
Lesions,
Computed tomography,
Image segmentation,
Lungs,
Imaging phantoms,
Image reconstruction,
Cancer,
Medical treatment,
Patient monitoring"
Power-efficient access-point selection for indoor location estimation,"An important goal of indoor location estimation systems is to increase the estimation accuracy while reducing the power consumption. In this paper, we present a novel algorithm known as CaDet for power-efficient location estimation by intelligently selecting the number of access points (APs) used for location estimation. We show that by employing machine learning techniques, CaDet is able to use a small subset of the APs in the environment to detect a client's location with high accuracy. CaDet uses a combination of information theory, clustering analysis, and a decision tree algorithm. By collecting data and testing our algorithms in a realistic WLAN environment in the computer science department area of the Hong Kong University of Science and Technology, we show that CaDet (clustering and decision tree-based method) can be much higher in accuracy as compared to other methods. We also show through experiments that, by intelligently selecting APs, we are able to save the power on the client device while achieving the same level of accuracy.","Machine learning algorithms,
Decision trees,
Clustering algorithms,
Energy consumption,
Learning systems,
Machine learning,
Information theory,
Information analysis,
Algorithm design and analysis,
Testing"
Free-riding and whitewashing in peer-to-peer systems,"We devise a model to study the phenomenon of free-riding and free-identities in peer-to-peer systems. At the heart of our model is a user of a certain type, an intrinsic and private parameter that reflects the user's willingness to contribute resources to the system. A user decides whether to contribute or free-ride based on how the current contribution cost in the system compares to her type. We study the impact of mechanisms that exclude low type users or, more realistically, penalize free-riders with degraded service. We also consider dynamic scenarios with arrivals and departures of users, and with whitewashers -users who leave the system and rejoin with new identities to avoid reputational penalties. We find that imposing penalty on all users that join the system is effective under many scenarios. In particular, system performance degrades significantly only when the turnover rate among users is high. Finally, we show that the optimal exclusion or penalty level differs significantly from the level that optimizes the performance of contributors only for a limited range of societal generosity levels.","Peer to peer computing,
Costs,
Heart,
Degradation,
System performance,
Information management,
Computer science,
Engineering profession,
Environmental economics"
Head-worn displays: a review,"Head-worn display design is inherently an interdisciplinary subject fusing optical engineering, optical materials, optical coatings, electronics, manufacturing techniques, user interface design, computer science, human perception, and physiology for assessing these displays. This paper summarizes the state-of-the-art in head-worn display design (HWD) and development. This review is focused on the optical engineering aspects, divided into different sections to explore principles and applications. Building on the guiding fundamentals of optical design and engineering, the principles section includes a summary of microdisplay or laser sources, the Lagrange invariant for understanding the trade-offs in optical design of HWDs, modes of image presentation (i.e., monocular, biocular, and stereo) and operational modes such as optical and video see-through. A brief summary of the human visual system pertinent to the design of HWDs is provided. Two optical design forms, namely, pupil forming and non-pupil forming are discussed. We summarize the results from previous design work using aspheric, diffractive, or holographic elements to achieve compact and lightweight systems. The applications section is organized in terms of field of view requirements and presents a reasonable collection of past designs",
The Eta Pairing Revisited,"In this paper, we simplify and extend the Eta pairing, originally discovered in the setting of supersingular curves by Barreto , to ordinary curves. Furthermore, we show that by swapping the arguments of the Eta pairing, one obtains a very efficient algorithm resulting in a speed-up of a factor of around six over the usual Tate pairing, in the case of curves that have large security parameters, complex multiplication by an order of Qopf (radic-3), and when the trace of Frobenius is chosen to be suitably small. Other, more minor savings are obtained for more general curves",
Incentive and Service Differentiation in P2P Networks: A Game Theoretic Approach,"Conventional peer-to-peer (P2P) networks do not provide service differentiation and incentive for users. Therefore, users can easily obtain information without themselves contributing any information or service to a P2P community. This leads to the well known free-riding problem. Consequently, most of the information requests are directed towards a small number of P2P nodes which are willing to share information or provide service, causing the ""tragedy of the commons."" The aim of this paper is to provide service differentiation in a P2P network based on the amount of services each node has provided to the network community. Since the differentiation is based on nodes' prior contributions, the nodes are encouraged to share information/services with each other. We first introduce a resource distribution mechanism for all the information sharing nodes. The mechanism is distributed in nature, has linear time complexity, and guarantees Pareto-optimal resource allocation. Second, we model the whole resource request/distribution process as a competition game between the competing nodes. We show that this game has a Nash equilibrium. To realize the game, we propose a protocol in which the competing nodes can interact with the information providing node to reach Nash equilibrium efficiently and dynamically. We also present a generalized incentive mechanism for nodes having heterogeneous utility functions. Convergence analysis of the competition game is carried out. Examples are used to illustrate that the incentive protocol provides service differentiation and can induce productive resource sharing by rational network nodes. Lastly, the incentive protocol is adaptive to node arrival and departure events, and to different forms of network congestion",
BiToS: Enhancing BitTorrent for Supporting Streaming Applications,"BitTorrent (BT) in the last years has been one of the most effective mechanisms for P2P content distribution. Although BT was created for distribution of time insensitive content, in this work we try to identify what are the minimal changes needed in the BT's mechanisms in order to support streaming. The importance of this capability is that the peer will now have the ability to start enjoying the video before the complete download of the video file. This ability is particularly important in highly polluted environments, since the peer can evaluate the quality of the video content early and thus preserve its valuable resources. In a nutshell, our approach gives higher download priority to pieces that are close to be reproduced by the player. This comes in contrast to the original BT protocol, where pieces are downloaded in an out-of-order manner based solely on their rareness. In particular, our approach tries to strike the balance between downloading pieces in: (a) playing order, enabling smooth playback, and (b) the rarest first order, enabling the use of parallel downloading of pieces. In this work, we introduce three different Piece Selection mechanisms and we evaluate them through simulations based on how well they deliver streaming services to the peers.",
Evaluation of Stability of k-Means Cluster Ensembles with Respect to Random Initialization,"Many clustering algorithms, including cluster ensembles, rely on a random component. Stability of the results across different runs is considered to be an asset of the algorithm. The cluster ensembles considered here are based on k-means clusterers. Each clusterer is assigned a random target number of clusters, k and is started from a random initialization. Here, we use 10 artificial and 10 real data sets to study ensemble stability with respect to random k, and random initialization. The data sets were chosen to have a small number of clusters (two to seven) and a moderate number of data points (up to a few hundred). Pairwise stability is defined as the adjusted Rand index between pairs of clusterers in the ensemble, averaged across all pairs. Nonpairwise stability is defined as the entropy of the consensus matrix of the ensemble. An experimental comparison with the stability of the standard k-means algorithm was carried out for k from 2 to 20. The results revealed that ensembles are generally more stable, markedly so for larger k. To establish whether stability can serve as a cluster validity index, we first looked at the relationship between stability and accuracy with respect to the number of clusters, k. We found that such a relationship strongly depends on the data set, varying from almost perfect positive correlation (0.97, for the glass data) to almost perfect negative correlation (-0.93, for the crabs data). We propose a new combined stability index to be the sum of the pairwise individual and ensemble stabilities. This index was found to correlate better with the ensemble accuracy. Following the hypothesis that a point of stability of a clustering algorithm corresponds to a structure found in the data, we used the stability measures to pick the number of clusters. The combined stability index gave best results","Stability,
Clustering algorithms,
Entropy,
Glass,
Shape,
Voting,
Clustering methods,
Protocols,
Partitioning algorithms"
Matrix embedding for large payloads,"Matrix embedding is a previously introduced coding method that is used in steganography to improve the embedding efficiency (increase the number of bits embedded per embedding change). Higher embedding efficiency translates into better steganographic security. This gain is more important for long messages than for shorter ones because longer messages are, in general, easier to detect. In this paper, we present two new approaches to matrix embedding for large payloads suitable for practical steganographic schemes-one based on a family of codes constructed from simplex codes and the second one based on random linear codes of small dimension. The embedding efficiency of the proposed methods is evaluated with respect to theoretically achievable bounds","Payloads,
Steganography,
Linear code,
Government,
Random media,
Data security,
Change detection algorithms,
Computer science,
Terminology"
Integral Invariants for Shape Matching,"For shapes represented as closed planar contours, we introduce a class of functionals which are invariant with respect to the Euclidean group and which are obtained by performing integral operations. While such integral invariants enjoy some of the desirable properties of their differential counterparts, such as locality of computation (which allows matching under occlusions) and uniqueness of representation (asymptotically), they do not exhibit the noise sensitivity associated with differential quantities and, therefore, do not require presmoothing of the input shape. Our formulation allows the analysis of shapes at multiple scales. Based on integral invariants, we define a notion of distance between shapes. The proposed distance measure can be computed efficiently and allows warping the shape boundaries onto each other; its computation results in optimal point correspondence as an intermediate step. Numerical results on shape matching demonstrate that this framework can match shapes despite the deformation of subparts, missing parts and noise. As a quantitative analysis, we report matching scores for shape retrieval from a database",
Fuzzy interpolative reasoning via scale and move transformations,"Interpolative reasoning does not only help reduce the complexity of fuzzy models but also makes inference in sparse rule-based systems possible. This paper presents an interpolative reasoning method by means of scale and move transformations. It can be used to interpolate fuzzy rules involving complex polygon, Gaussian or other bell-shaped fuzzy membership functions. The method works by first constructing a new inference rule via manipulating two given adjacent rules, and then by using scale and move transformations to convert the intermediate inference results into the final derived conclusions. This method has three advantages thanks to the proposed transformations: 1) it can handle interpolation of multiple antecedent variables with simple computation; 2) it guarantees the uniqueness as well as normality and convexity of the resulting interpolated fuzzy sets; and 3) it suggests a variety of definitions for representative values, providing a degree of freedom to meet different requirements. Comparative experimental studies are provided to demonstrate the potential of this method",
MDCT-based 3-D texture classification of emphysema and early smoking related lung pathologies,"Our goal is to enhance the ability to differentiate normal lung from subtle pathologies via multidetector row CT (MDCT) by extending a two-dimensional (2-D) texturebased tissue classification [adaptive multiple feature method (AMFM)] to use three-dimensional (3-D) texture features. We performed MDCT on 34 humans and classified volumes of interest (VOIs) in the MDCT images into five categories: EC, emphysema in severe chronic obstructive pulmonary disease (COPD); MC, mild emphysema in mild COPD; NC, normal appearing lung in mild COPD; NN, normal appearing lung in normal nonsmokers; and NS, normal appearing lung in normal smokers. COPD severity was based upon pulmonary function tests (PFTs). Airways and vessels were excluded from VOIs; 24 3-D texture features were calculated; and a Bayesian classifier was used for discrimination. A leave-one-out method was employed for validation. Sensitivity of the four-class classification in the form of 3-D/2-D was: EC: 85%/71%, MC: 90%/82%; NC: 88%/50%; NN: 100%/60%. Sensitivity and specificity for NN using a two-class classification of NN and NS in the form of 3-D/2-D were: 99%/72% and 100%/75%, respectively. We conclude that 3-D AMFM analysis of lung parenchyma improves discrimination compared to 2-D AMFM of the same VOIs. Furthermore, our results suggest that the 3-D AMFM may provide a means of discriminating subtle differences between smokers and nonsmokers both with normal PFTs.","Lungs,
Pathology,
Neural networks,
Computed tomography,
Two dimensional displays,
Humans,
Diseases,
Testing,
Bayesian methods,
Sensitivity and specificity"
Security Enhancement on a New Authentication Scheme With Anonymity for Wireless Environments,"In a paper recently published in the IEEE Transactions on Consumer Electronics, Zhu and Ma proposed a new authentication scheme with anonymity for wireless environments. However, this paper shows that Zhu and Ma's scheme has some security weaknesses. Therefore, in this paper, a slight modification to their scheme is proposed to improve their shortcomings. As a result, the scheme proposed in this paper can enhance the security of Zhu and Ma's scheme. Finally, the performance of this scheme is analyzed. Compared with the Zhu-Ma scheme, this scheme is also simple and efficient","Authentication,
Communication system security,
Information security,
Privacy,
Wireless networks,
Wireless communication,
Smart cards,
Computer science,
Consumer electronics,
Performance analysis"
Index Coding with Side Information,"Motivated by a problem of transmitting data over broadcast channels (BirkandKol, INFOCOM1998), we study the following coding problem: a sender communicates with n receivers Rl,.., Rn. He holds an input x isin {0, 1}n and wishes to broadcast a single message so that each receiver Ri can recover the bit xi. Each Ri has prior side information about x, induced by a directed graph G on n nodes; Ri knows the bits of x in the positions {j | (i, j) is anedge of G}. We call encoding schemes that achieve this goal INDEX codes for {0, 1} n with side information graph G. In this paper we identify a measure on graphs, the minrank, which we conjecture to exactly characterize the minimum length of INDEX codes. We resolve the conjecture for certain natural classes of graphs. For arbitrary graphs, we show that the minrank bound is tight for both linear codes and certain classes of non-linear codes. For the general problem, we obtain a (weaker) lower bound that the length of an INDEX code for any graph G is at least the size of the maximum acyclic induced subgraph of G",
Image Denoising Via Learned Dictionaries and Sparse representation,"We address the image denoising problem, where zeromean white and homogeneous Gaussian additive noise should be removed from a given image. The approach taken is based on sparse and redundant representations over a trained dictionary. The proposed algorithm denoises the image, while simultaneously trainining a dictionary on its (corrupted) content using the K-SVD algorithm. As the dictionary training algorithm is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm, with state-of-the-art performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.","Image denoising,
Dictionaries,
Noise reduction,
Additive noise,
Bayesian methods,
Computer science,
Noise measurement,
Gaussian noise,
Measurement standards,
Algorithm design and analysis"
An Integrated Model of Top-Down and Bottom-Up Attention for Optimizing Detection Speed,"Integration of goal-driven, top-down attention and image-driven, bottom-up attention is crucial for visual search. Yet, previous research has mostly focused on models that are purely top-down or bottom-up. Here, we propose a new model that combines both. The bottom-up component computes the visual salience of scene locations in different feature maps extracted at multiple spatial scales. The topdown component uses accumulated statistical knowledge of the visual features of the desired search target and background clutter, to optimally tune the bottom-up maps such that target detection speed is maximized. Testing on 750 artificial and natural scenes shows that the model’s predictions are consistent with a large body of available literature on human psychophysics of visual search. These results suggest that our model may provide good approximation of how humans combine bottom-up and top-down cues such as to optimize target detection speed.","Object detection,
Layout,
Biological system modeling,
Humans,
Face detection,
Acceleration,
Robots,
Navigation,
Surveillance,
Computer science"
A fast iterative algorithm for implementation of pixel purity index,"The pixel purity index (PPI) has been widely used in hyperspectral image analysis for endmember extraction due to its publicity and availability in the Environment for Visualizing Images (ENVI) software. Unfortunately, its detailed implementation has never been made available in the literature. This paper investigates the PPI based on limited published results and proposes a fast iterative algorithm to implement the PPI, referred to as fast iterative PPI (FIPPI). It improves the PPI in several aspects. Instead of using randomly generated vectors as initial endmembers, the FIPPI produces an appropriate initial set of endmembers to speed up its process. Additionally, it estimates the number of endmembers required to be generated by a recently developed concept, virtual dimensionality (VD) which is one of the most crucial issues in the implementation of PPI. Furthermore, it is an iterative algorithm, where an iterative rule is developed to improve each of the iterations until it reaches a final set of endmembers. Most importantly, it is an unsupervised algorithm as opposed to the PPI, which requires human intervention to manually select a final set of endmembers. The experiments show that both the FIPPI and the PPI produce very close results, but the FIPPI converges very rapidly with significant savings in computation.","Iterative algorithms,
Hyperspectral imaging,
Indexes,
Pixel,
Image analysis,
Humans,
Hyperspectral sensors,
Computer science,
Availability,
Visualization"
Lung motion correction on respiratory gated 3-D PET/CT images,"Motion is a source of degradation in positron emission tomography (PET)/computed tomography (CT) images. As the PET images represent the sum of information over the whole respiratory cycle, attenuation correction with the help of CT images may lead to false staging or quantification of the radioactive uptake especially in the case of small tumors. We present an approach avoiding these difficulties by respiratory-gating the PET data and correcting it for motion with optical flow algorithms. The resulting dataset contains all the PET information and minimal motion and, thus, allows more accurate attenuation correction and quantification.","Lungs,
Positron emission tomography,
Computed tomography,
Optical attenuators,
Neoplasms,
Heart,
Attenuation,
Degradation,
Image motion analysis,
Nuclear medicine"
Radiation-Induced Soft Error Rates of Advanced CMOS Bulk Devices,"This work provides a comprehensive summary of radiation-induced soft error rate (SER) scaling trends of key CMOS bulk devices. Specifically we analyzed the SER per bit scaling trends of SRAMs, sequentials and static combinational logic. Our results show that for SRAMs the single-bit soft error rate continues to decrease whereas the multi-bit SER increases dramatically. While the total soft error rate of logic devices (sequentials and static combinational devices) has not changed significantly, a substantial increase in the susceptibility to alpha particles is observed. Finally, a novel methodology to extract one-dimensional cross sections of the collected charge distributions from measured multi-bit statistics is introduced",
Location-Aware Combinatorial Key Management Scheme for Clustered Sensor Networks,"Recent advances in wireless sensor networks (WSNs) are fueling the interest in their application in a wide variety of sensitive settings such as battlefield surveillance, border control, and infrastructure protection. Data confidentiality and authenticity are critical in these settings. However, the wireless connectivity, the absence of physical protection, the close interaction between WSNs and their physical environment, and the unattended deployment of WSNs make them highly vulnerable to node capture as well as a wide range of network-level attacks. Moreover, the constrained energy, memory, and computational capabilities of the employed sensor nodes limit the adoption of security solutions designed for wire-line and wireless networks. In this paper, we focus on the management of encryption keys in large-scale clustered WSNs. We propose a novel distributed key management scheme based on exclusion basis systems (EBS); a combinatorial formulation of the group key management problem. Our scheme is termed SHELL because it is scalable, hierarchical, efficient, location-aware, and light-weight. Unlike most existing key management schemes for WSNs, SHELL supports rekeying and, thus, enhances network security and survivability against node capture. SHELL distributes key management functionality among multiple nodes and minimizes the memory and energy consumption through trading off the number of keys and rekeying messages. In addition, SHELL employs a novel key assignment scheme that reduces the potential of collusion among compromised sensor nodes by factoring the geographic location of nodes in key assignment. Simulation results demonstrate that SHELL significantly boosts the network resilience to attacks while conservatively consuming nodes' resources","Wireless sensor networks,
Protection,
Surveillance,
Computer networks,
Cryptography,
Large-scale systems,
Energy management,
Memory management,
Energy consumption,
Resilience"
Network Visualization by Semantic Substrates,"Networks have remained a challenge for information visualization designers because of the complex issues of node and link layout coupled with the rich set of tasks that users present. This paper offers a strategy based on two principles: (1) layouts are based on user-defined semantic substrates, which are non-overlapping regions in which node placement is based on node attributes, (2) users interactively adjust sliders to control link visibility to limit clutter and thus ensure comprehensibility of source and destination. Scalability is further facilitated by user control of which nodes are visible. We illustrate our semantic substrates approach as implemented in NVSS 1.0 with legal precedent data for up to 1122 court cases in three regions with 7645 legal citations",
Volumetric breast density estimation from full-field digital mammograms,"A method is presented for estimation of dense breast tissue volume from mammograms obtained with full-field digital mammography (FFDM). The thickness of dense tissue mapping to a pixel is determined by using a physical model of image acquisition. This model is based on the assumption that the breast is composed of two types of tissue, fat and parenchyma. Effective linear attenuation coefficients of these tissues are derived from empirical data as a function of tube voltage (kVp), anode material, filtration, and compressed breast thickness. By employing these, tissue composition at a given pixel is computed after performing breast thickness compensation, using a reference value for fatty tissue determined by the maximum pixel value in the breast tissue projection. Validation has been performed using 22 FFDM cases acquired with a GE Senographe 2000D by comparing the volume estimates with volumes obtained by semi-automatic segmentation of breast magnetic resonance imaging (MRI) data. The correlation between MRI and mammography volumes was 0.94 on a per image basis and 0.97 on a per patient basis. Using the dense tissue volumes from MRI data as the gold standard, the average relative error of the volume estimates was 13.6%.",
On measuring the change in size of pulmonary nodules,"The pulmonary nodule is the most common manifestation of lung cancer, the most deadly of all cancers. Most small pulmonary nodules are benign, however, and currently the growth rate of the nodule provides for one of the most accurate noninvasive methods of determining malignancy. In this paper, we present methods for measuring the change in nodule size from two computed tomography image scans recorded at different times; from this size change the growth rate may be established. The impact of partial voxels for small nodules is evaluated and isotropic resampling is shown to improve measurement accuracy. Methods for nodule location and sizing, pleural segmentation, adaptive thresholding, image registration, and knowledge-based shape matching are presented. The latter three techniques provide for a significant improvement in volume change measurement accuracy by considering both image scans simultaneously. Improvements in segmentation are evaluated by measuring volume changes in benign or slow growing nodules. In the analysis of 50 nodules, the variance in percent volume change was reduced from 11.54% to 9.35% (p=0.03) through the use of registration, adaptive thresholding, and knowledge-based shape matching.","Size measurement,
Computed tomography,
Cancer,
Lungs,
Image segmentation,
Shape,
Radiography,
Image registration,
Volume measurement,
Biopsy"
"Concealed Data Aggregation for Reverse Multicast Traffic in Sensor Networks: Encryption, Key Distribution, and Routing Adaptation","Routing in wireless sensor networks is different from that in commonsense mobile ad-hoc networks. It mainly needs to support reverse multicast traffic to one particular destination in a multihop manner. For such a communication pattern, end-to-end encryption is a challenging problem. To save the overall energy resources of the network, sensed data needs to be consolidated and aggregated on its way to the final destination. We present an approach that 1) conceals sensed data end-to-end by 2) still providing efficient and flexible in-network data aggregation. The aggregating intermediate nodes are not required to operate on the sensed plaintext data. We apply a particular class of encryption transformations and discuss techniques for computing the aggregation functions ""average"" and ""movement detection."" We show that the approach is feasible for the class of ""going down"" routing protocols. We consider the risk of corrupted sensor nodes by proposing a key predistribution algorithm that limits an attacker's gain and show how key predistribution and a key-ID sensitive ""going down"" routing protocol help increase the robustness and reliability of the connected backbone","Telecommunication traffic,
Intelligent networks,
Cryptography,
Wireless sensor networks,
Spine,
Biomedical monitoring,
Ad hoc networks,
Routing protocols,
Robustness,
Mobile communication"
Local Graph Partitioning using PageRank Vectors,"A local graph partitioning algorithm finds a cut near a specified starting vertex, with a running time that depends largely on the size of the small side of the cut, rather than the size of the input graph. In this paper, we present a local partitioning algorithm using a variation of PageRank with a specified starting distribution. We derive a mixing result for PageRank vectors similar to that for random walks, and show that the ordering of the vertices produced by a PageRank vector reveals a cut with small conductance. In particular, we show that for any set C with conductance Phi and volume k, a PageRank vector with a certain starting distribution can be used to produce a set with conductance (O(radic(Phi log k)). We present an improved algorithm for computing approximate PageRank vectors, which allows us to find such a set in time proportional to its size. In particular, we can find a cut with conductance at most oslash, whose small side has volume at least 2b in time O(2 log m/(2b log2 m/oslash2) where m is the number of edges in the graph. By combining small sets found by this local partitioning algorithm, we obtain a cut with conductance oslash and approximately optimal balance in time O(m log4 m/oslash)",
Deformable segmentation of 3-D ultrasound prostate images using statistical texture matching method,"This paper presents a novel deformable model for automatic segmentation of prostates from three-dimensional ultrasound images, by statistical matching of both shape and texture. A set of Gabor-support vector machines (G-SVMs) are positioned on different patches of the model surface, and trained to adaptively capture texture priors of ultrasound images for differentiation of prostate and nonprostate tissues in different zones around prostate boundary. Each G-SVM consists of a Gabor filter bank for extraction of rotation-invariant texture features and a kernel support vector machine for robust differentiation of textures. In the deformable segmentation procedure, these pretrained G-SVMs are used to tentatively label voxels around the surface of deformable model as prostate or nonprostate tissues by a statistical texture matching. Subsequently, the surface of deformable model is driven to the boundary between the tentatively labeled prostate and nonprostate tissues. Since the step of tissue labeling and the step of label-based surface deformation are dependent on each other, these two steps are repeated until they converge. Experimental results by using both synthesized and real data show the good performance of the proposed model in segmenting prostates from ultrasound images.",
Statistical Debugging: A Hypothesis Testing-Based Approach,"Manual debugging is tedious, as well as costly. The high cost has motivated the development of fault localization techniques, which help developers search for fault locations. In this paper, we propose a new statistical method, called SOBER, which automatically localizes software faults without any prior knowledge of the program semantics. Unlike existing statistical approaches that select predicates correlated with program failures, SOBER models the predicate evaluation in both correct and incorrect executions and regards a predicate as fault-relevant if its evaluation pattern in incorrect executions significantly diverges from that in correct ones. Featuring a rationale similar to that of hypothesis testing, SOBER quantifies the fault relevance of each predicate in a principled way. We systematically evaluate SOBER under the same setting as previous studies. The result clearly demonstrates the effectiveness: SOBER could help developers locate 68 out of the 130 faults in the Siemens suite by examining no more than 10 percent of the code, whereas the cause transition approach proposed by Holger et al. [2005] and the statistical approach by Liblit et al. [2005] locate 34 and 52 faults, respectively. Moreover, the effectiveness of SOBER is also evaluated in an ""imperfect world"", where the test suite is either inadequate or only partially labeled. The experiments indicate that SOBER could achieve competitive quality under these harsh circumstances. Two case studies with grep 2.2 and bc 1.06 are reported, which shed light on the applicability of SOBER on reasonably large programs",
Three-Dimensional Imaging and Processing Using Computational Holographic Imaging,"Digital holography is a technique that permits digital capture of holograms and subsequent processing on a digital computer. This paper reviews various applications of this technique. The presented applications cover three-dimensional (3-D) imaging as well as several associated problems. For the case of 3-D imaging, optical and digital methods to reconstruct and visualize the recorded objects are described. In addition, techniques to compress and encrypt 3-D information in the form of digital holograms are presented. Lastly, 3-D pattern recognition applications of digital holography are discussed. The described techniques constitute a comprehensive approach to 3-D imaging and processing.",
Data Replication for Improving Data Accessibility in Ad Hoc Networks,"In ad hoc networks, due to frequent network partition, data accessibility is lower than that in conventional fixed networks. In this paper, we solve this problem by replicating data items on mobile hosts. First, we propose three replica allocation methods assuming that each data item is not updated. In these three methods, we take into account the access frequency from mobile hosts to each data item and the status of the network connection. Then, we extend the proposed methods by considering aperiodic updates and integrating user profiles consisting of mobile users' schedules, access behavior, and read/write patterns. We also show the results of simulation experiments regarding the performance evaluation of our proposed methods",
Distributed Multirobot Exploration and Mapping,"Efficient exploration of unknown environments is a fundamental problem in mobile robotics. We present an approach to distributed multirobot mapping and exploration. Our system enables teams of robots to efficiently explore environments from different, unknown locations. In order to ensure consistency when combining their data into shared maps, the robots actively seek to verify their relative locations. Using shared maps, they coordinate their exploration strategies to maximize the efficiency of exploration. This system was evaluated under extremely realistic real-world conditions. An outside evaluation team found the system to be highly efficient and robust. The maps generated by our approach are consistently more accurate than those generated by manually measuring the locations and extensions of rooms and objects",
Heuristics for QoS-aware Web Service Composition,"This paper discusses the quality of service (QoS)-aware composition of Web services. The work is based on the assumption that for each task in a workflow a set of alternative Web services with similar functionality is available and that these Web services have different QoS parameters and costs. This leads to the general optimization problem of how to select Web services for each task so that the overall QoS and cost requirements of the composition are satisfied. Current proposals use exact algorithms or complex heuristics (e.g. genetic algorithms) to solve this problem. An actual implementation of a workflow engine (like our WSQoSX architecture), however, has to be able to solve these optimization problems in real-time and under heavy load. Therefore, we present a heuristic that performs extremely well while providing excellent (almost optimal) solutions. Using simulations, we show that in most cases our heuristic is able to calculate solutions that come as close as 99% to the optimal solution while taking less than 2% of the time of a standard exact algorithm. Further, we also investigate how much and under which circumstances the solution obtained by our heuristic can be further improved by other heuristics","Web services,
Quality of service,
Runtime,
Linear programming,
Cost function,
Computer science,
Proposals,
Heuristic algorithms,
Genetic algorithms,
Engines"
On Delay Performance Gains From Network Coding,"This paper analyzes the gains in delay performance resulting from network coding. We consider a model of file transmission to multiple receivers from a single base station. Using this model, we show that gains in delay performance from network coding with or without channel side information can be substantial compared to conventional scheduling methods for downlink transmission.","Performance gain,
Network coding,
Optimal scheduling,
Performance analysis,
Base stations,
Downlink,
Delay,
Information analysis,
Wireless networks,
Wireless communication"
"Managing Distributed, Shared L2 Caches through OS-Level Page Allocation","This paper presents and studies a distributed L2 cache management approach through OS-level page allocation for future many-core processors. L2 cache management is a crucial multicore processor design aspect to overcome non-uniform cache access latency for good program performance and to reduce on-chip network traffic and related power consumption. Unlike previously studied hardware-based private and shared cache designs implementing a ""fixed"" caching policy, the proposed OS-micro architecture approach is flexible; it can easily implement a wide spectrum of L2 caching policies without complex hardware support. Furthermore, our approach can provide differentiated execution environment to running programs by dynamically controlling data placement and cache sharing degrees. We discuss key design issues of the proposed approach and present preliminary experimental results showing the promise of our approach",
Handling forecasting problems based on two-factors high-order fuzzy time series,"In our daily life, people often use forecasting techniques to predict weather, economy, population growth, stock, etc. However, in the real world, an event can be affected by many factors. Therefore, if we consider more factors for prediction, then we can get better forecasting results. In recent years, many researchers used fuzzy time series to handle prediction problems. In this paper, we present a new method to predict temperature and the Taiwan Futures Exchange (TAIFEX), based on the two-factors high-order fuzzy time series. The proposed method constructs two-factors high-order fuzzy logical relationships based on the historical data to increase the forecasting accuracy rate. The proposed method gets a higher forecasting accuracy rate than the existing methods.","Fuzzy logic,
Temperature,
Weather forecasting,
Economic forecasting,
Fuzzy sets,
Predictive models,
Fuzzy set theory,
Fuzzy reasoning,
Councils,
Computer science"
An electromechanical model of the heart for image analysis and simulation,"This paper presents a new three-dimensional electromechanical model of the two cardiac ventricles designed both for the simulation of their electrical and mechanical activity, and for the segmentation of time series of medical images. First, we present the volumetric biomechanical models built. Then the transmembrane potential propagation is simulated, based on FitzHugh-Nagumo reaction-diffusion equations. The myocardium contraction is modeled through a constitutive law including an electromechanical coupling. Simulation of a cardiac cycle, with boundary conditions representing blood pressure and volume constraints, leads to the correct estimation of global and local parameters of the cardiac function. This model enables the introduction of pathologies and the simulation of electrophysiology interventions. Moreover, it can be used for cardiac image analysis. A new proactive deformable model of the heart is introduced to segment the two ventricles in time series of cardiac images. Preliminary results indicate that this proactive model, which integrates a priori knowledge on the cardiac anatomy and on its dynamical behavior, can improve the accuracy and robustness of the extraction of functional parameters from cardiac images even in the presence of noisy or sparse data. Such a model also allows the simulation of cardiovascular pathologies in order to test therapy strategies and to plan interventions.","Heart,
Image analysis,
Analytical models,
Medical simulation,
Image segmentation,
Pathology,
Biomedical imaging,
Equations,
Myocardium,
Boundary conditions"
DOMINO: Detecting MAC Layer Greedy Behavior in IEEE 802.11 Hotspots,"IEEE 802.11 works properly only if the stations respect the MAC protocol. We show in this paper that a greedy user can substantially increase his share of bandwidth, at the expense of the other users, by slightly modifying the driver of his network adapter. We explain how easily this can be performed, in particular, with the new generation of adapters. We then present DOMINO (detection of greedy behavior in the MAC layer of IEEE 802.11 public networks), a piece of software to be installed in or near the access point. DOMINO can detect and identify greedy stations without requiring any modification of the standard protocol. We illustrate these concepts by simulation results and by the description of a prototype that we have recently implemented",
A New Distributed Time Synchronization Protocol for Multihop Wireless Networks,"A distributed algorithm to achieve accurate time synchronization in large multihop wireless networks is presented. The central idea is to exploit the large number of global constraints that have to be satisfied by a common notion of time in a multihop network. If, at a certain instant, Oij is the clock offset between two neighboring nodes i and j, then for any loop i1, i2, i3 , ..., in, in + 1 - i1 in the multihop network, these offsets must satisfy the global constraint Sigma k = 1 nOik, ik + 1 = 0. Noisy estimates Ocirc ij of Oij are usually arrived at by bilateral exchanges of timestamped messages or local broadcasts. By imposing the large number of global constraints for all the loops in the multihop network, these estimates can be smoothed and made more accurate. A fully distributed and asynchronous algorithm which functions by simple local broadcasts is designed. Changing the time reference node for synchronization is also easy, consisting simply of one node switching on adaptation, and another switching it off. Implementation results on a forty node network, and comparative evaluation against a leading algorithm, are presented","Wireless application protocol,
Spread spectrum communication,
Wireless networks,
Synchronization,
Clocks,
Broadcasting,
Distributed algorithms,
Tree graphs,
Testing,
Ad hoc networks"
Utility-based Intelligent Network Selection in Beyond 3G Systems,Development in wireless access technologies and multihomed personal user devices is driving the way towards a heterogeneous wireless access network environment. Success in this arena will be reliant on the ability to offer an enhanced user experience. Users will plan to take advantage of the competition and always connect to the network which can best service their preferences for the current application. They will rely on intelligent network selection decision strategies to aid them in their choice. The contribution of this paper is to propose an intelligent utility-based strategy for network selection in this multi-access network scenario. A number of utility functions are examined which explore different user attitudes to risk for money and delay preferences related to their current application. For example we show that risk takers who are willing to pay more money get a better service.,"Intelligent networks,
Wireless networks,
Radio access networks,
Wireless LAN,
Informatics,
Computer science,
Educational institutions,
User centered design,
Delay,
Uncertainty"
Atlas-driven lung lobe segmentation in volumetric X-ray CT images,"High-resolution X-ray computed tomography (CT) imaging is routinely used for clinical pulmonary applications. Since lung function varies regionally and because pulmonary disease is usually not uniformly distributed in the lungs, it is useful to study the lungs on a lobe-by-lobe basis. Thus, it is important to segment not only the lungs, but the lobar fissures as well. In this paper, we demonstrate the use of an anatomic pulmonary atlas, encoded with a priori information on the pulmonary anatomy, to automatically segment the oblique lobar fissures. Sixteen volumetric CT scans from 16 subjects are used to construct the pulmonary atlas. A ridgeness measure is applied to the original CT images to enhance the fissure contrast. Fissure detection is accomplished in two stages: an initial fissure search and a final fissure search. A fuzzy reasoning system is used in the fissure search to analyze information from three sources: the image intensity, an anatomic smoothness constraint, and the atlas-based search initialization. Our method has been tested on 22 volumetric thin-slice CT scans from 12 subjects, and the results are compared to manual tracings. Averaged across all 22 data sets, the RMS error between the automatically segmented and manually segmented fissures is 1.96/spl plusmn/0.71 mm and the mean of the similarity indices between the manually defined and computer-defined lobe regions is 0.988. The results indicate a strong agreement between the automatic and manual lobe segmentations.",
"Ultralow-voltage, minimum-energy CMOS","Energy efficiency has become a ubiquitous design requirement for digital circuits. Aggressive supply-voltage scaling has emerged as the most effective way to reduce energy use. In this work, we review circuit behavior at low voltages, specifically in the subthreshold (Vdd < Vth) regime, and suggest new strategies for energy-efficient design. We begin with a study at the device level, and we show that extreme sensitivity to the supply and threshold voltages complicates subthreshold design. The effects of this sensitivity can be minimized through simple device modifications and new device geometries. At the circuit level, we review the energy characteristics of subthreshold logic and SRAM circuits, and demonstrate that energy efficiency relies on the balance between dynamic and leakage energies, with process variability playing a key role in both energy efficiency and robustness. We continue the study of energy-efficient design by broadening our scope to the architectural level. We discuss the energy benefits of techniques such as multiple-threshold CMOS (MTCMOS) and adaptive body biasing (ABB), and we also consider the performance benefits of multiprocessor design at ultralow supply voltages.",
"Zero-Configuration, Robust Indoor Localization: Theory and Experimentation",,
Sensory-Updated Residual Life Distributions for Components With Exponential Degradation Patterns,"Research on interpreting data communicated by smart sensors and distributed sensor networks, and utilizing these data streams in making critical decisions stands to provide significant advancements across a wide range of application domains such as maintenance management. In this paper, a stochastic degradation modeling framework is developed for computing and continuously updating residual life distributions of partially degraded components. The proposed degradation methodology combines population-specific degradation characteristics with component-specific sensory data acquired through condition monitoring in order to compute and continuously update remaining life distributions of partially degraded components. Two sensory updating procedures are developed and validated using real-world vibration-based degradation information acquired from rolling element thrust bearings. The results are compared with two benchmark policies and illustrate the benefits of the sensory updated degradation models proposed in this paper. Note for Practitioners-The proposed degradation-based prognostic methodology provides a comprehensive assessment of the current and future degradation states of partially degraded components by combining population-specific degradation or reliability information with real-time sensory health monitoring data. It is specifically beneficial for cases where degradation occurs in a cumulative manner and the degradation signal can be approximated by an exponential functional form. To implement this methodology, it is necessary: 1) to identify the physical phenomena associated with the evolution of the degradation process (spalling and wear herein); 2) choose the appropriate condition monitoring technology to monitor this phenomena (accelerometers); 3) identify a characteristic pattern in the sensory information to help develop a degradation signal (exponential growth); and 4) identify a failure threshold associated with the degradation signal. The first step in implementing this prognostic methodology is to obtain prior information related to stochastic parameters f the exponential model. This may require fitting some sample degradation signals with an exponential functional form and noting the values of the exponential parameters, or using subjective prior distributions. The second step is to acquire sensory information and begin updating the prior distribution. The updating frequency will dictate which expressions are used to compute the posterior distributions. Once the posterior means, variances, and correlation are computed, the truncated CDF of the residual life can be evaluated using (10) and (11). Note that the truncation is necessary to preclude negative values of the remaining life. Practitioners can implement this methodology using a simple spreadsheet. Since the residual life distributions are skewed, it is reasonable to utilize the median as a measure of the central tendency and, hence, an alternative estimate for the expected value of the remaining life","Degradation,
Distributed computing,
Condition monitoring,
Intelligent sensors,
Stochastic processes,
Signal processing,
Computer network management,
Appropriate technology,
Accelerometers,
Fitting"
Capacity Enhancement using Throwboxes in DTNs,"Disruption tolerant networks (DTNs) are designed to overcome limitations in connectivity due to conditions such as mobility, poor infrastructure, and short range radios. DTNs rely on the inherent mobility in the network to deliver packets around frequent and extended network partitions using a store-carry-and-forward paradigm. However, missed contact opportunities decrease throughput and increase delay in the network. We propose the use of throwboxes in mobile DTNs to create a greater number of contact opportunities, consequently improving the performance of the network. Throwboxes are wireless nodes that act as relays, creating additional contact opportunities in the DTN. We propose algorithms to deploy stationary throwboxes in the network that simultaneously consider routing as well as placement. We also present placement algorithms that use more limited knowledge about the network structure. We perform an extensive evaluation of our algorithms by varying both the underlying routing and mobility models. Our results suggest several findings to guide the design and operation of throwbox-augmented DTNs",
Coverage by randomly deployed wireless sensor networks,"One of the main applications of wireless sensor networks is to provide proper coverage of their deployment regions. A wireless sensor network k-covers its deployment region if every point in its deployment region is within the coverage ranges of at least k sensors. In this paper, we assume that the sensors are deployed as either a Poisson point process or a uniform point process in a square or disk region, and study how the probability of the k-coverage changes with the sensing radius or the number of sensors. Our results take the complicated boundary effect into account, rather than avoiding it by assuming the toroidal metric as done in the literature.",
Detecting Phishing Web Pages with Visual Similarity Assessment Based on Earth Mover's Distance (EMD),"An effective approach to phishing Web page detection is proposed, which uses Earth mover's distance (EMD) to measure Web page visual similarity. We first convert the involved Web pages into low resolution images and then use color and coordinate features to represent the image signatures. We use EMD to calculate the signature distances of the images of the Web pages. We train an EMD threshold vector for classifying a Web page as a phishing or a normal one. Large-scale experiments with 10,281 suspected Web pages are carried out to show high classification precision, phishing recall, and applicable time performance for online enterprise solution. We also compare our method with two others to manifest its advantage. We also built up a real system which is already used online and it has caught many real phishing cases","Web pages,
Earth,
Protection,
Internet,
Image converters,
Large-scale systems,
Electronic mail,
Image resolution,
Credit cards,
Computer crime"
Dynamic Characterization of Cluster Structures for Robust and Inductive Support Vector Clustering,"A topological and dynamical characterization of the cluster structures described by the support vector clustering is developed. It is shown that each cluster can be decomposed into its constituent basin level cells and can be naturally extended to an enlarged clustered domain, which serves as a basis for inductive clustering. A simplified weighted graph preserving the topological structure of the clusters is also constructed and is employed to develop a robust and inductive clustering algorithm. Simulation results are given to illustrate the robustness and effectiveness of the proposed method",
On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",
Image-processing technique for suppressing ribs in chest radiographs by means of massive training artificial neural network (MTANN),"When lung nodules overlap with ribs or clavicles in chest radiographs, it can be difficult for radiologists as well as computer-aided diagnostic (CAD) schemes to detect these nodules. In this paper, we developed an image-processing technique for suppressing the contrast of ribs and clavicles in chest radiographs by means of a multiresolution massive training artificial neural network (MTANN). An MTANN is a highly nonlinear filter that can be trained by use of input chest radiographs and the corresponding ""teaching"" images. We employed ""bone"" images obtained by use of a dual-energy subtraction technique as the teaching images. For effective suppression of ribs having various spatial frequencies, we developed a multiresolution MTANN consisting of multiresolution decomposition/composition techniques and three MTANNs for three different-resolution images. After training with input chest radiographs and the corresponding dual-energy bone images, the multiresolution MTANN was able to provide ""bone-image-like"" images which were similar to the teaching bone images. By subtracting the bone-image-like images from the corresponding chest radiographs, we were able to produce ""soft-tissue-image-like"" images where ribs and clavicles were substantially suppressed. We used a validation test database consisting of 118 chest radiographs with pulmonary nodules and an independent test database consisting of 136 digitized screen-film chest radiographs with 136 solitary pulmonary nodules collected from 14 medical institutions in this study. When our technique was applied to nontraining chest radiographs, ribs and clavicles in the chest radiographs were suppressed substantially, while the visibility of nodules and lung vessels was maintained. Thus, our image-processing technique for rib suppression by means of a multiresolution MTANN would be potentially useful for radiologists as well as for CAD schemes in detection of lung nodules on chest radiographs.",
Generalized Core Vector Machines,"Kernel methods, such as the support vector machine (SVM), are often formulated as quadratic programming (QP) problems. However, given m training patterns, a naive implementation of the QP solver takes O(m 3) training time and at least O(m2) space. Hence, scaling up these QPs is a major stumbling block in applying kernel methods on very large data sets, and a replacement of the naive method for finding the QP solutions is highly desirable. Recently, by using approximation algorithms for the minimum enclosing ball (MEB) problem, we proposed the core vector machine (CVM) algorithm that is much faster and can handle much larger data sets than existing SVM implementations. However, the CVM can only be used with certain kernel functions and kernel methods. For example, the very popular support vector regression (SVR) cannot be used with the CVM. In this paper, we introduce the center-constrained MEB problem and subsequently extend the CVM algorithm. The generalized CVM algorithm can now be used with any linear/nonlinear kernel and can also be applied to kernel methods such as SVR and the ranking SVM. Moreover, like the original CVM, its asymptotic time complexity is again linear in m and its space complexity is independent of m. Experiments show that the generalized CVM has comparable performance with state-of-the-art SVM and SVR implementations, but is faster and produces fewer support vectors on very large data sets",
Making a Long Video Short: Dynamic Video Synopsis,"The power of video over still images is the ability to represent dynamic activities. But video browsing and retrieval are inconvenient due to inherent spatio-temporal redundancies, where some time intervals may have no activity, or have activities that occur in a small image region. Video synopsis aims to provide a compact video representation, while preserving the essential activities of the original video. We present dynamic video synopsis, where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times. For example, we can create a ""stroboscopic movie"", where multiple dynamic instances of a moving object are played simultaneously. This is an extension of the still stroboscopic picture. Previous approaches for video abstraction addressed mostly the temporal redundancy by selecting representative key-frames or time intervals. In dynamic video synopsis the activity is shifted into a significantly shorter period, in which the activity is much denser. Video examples can be found online in http://www.vision.huji.ac.il/synopsis",
Impact of Initialization on Design of Endmember Extraction Algorithms,"Many endmember extraction algorithms (EEAs) have been developed to find endmembers that are assumed to be pure signatures in hyperspectral data. However, two issues arising in EEAs have not been addressed: one is the knowledge of the number of endmembers that must be provided a priori, and the other is the initialization of EEAs, where most EEAs initialize their endmember-searching processes by using randomly generated endmembers, which generally result in inconsistent final selected endmembers. Unfortunately, there has been no previous work reported on how to address these two issues, i.e., how to select a set of appropriate initial endmembers and how to determine the number of endmembers p. This paper takes up these two issues and describes two-stage processes to improve EEAs. First, a recently developed concept of virtual dimensionality (VD) is used to determine how many endmembers are needed to be generated for an EEA. Experiments show that the VD is an adequate measure for estimating p. Second, since EEAs are sensitive to initial endmembers, a properly selected set of initial endmembers can make significant improvements on the searching process. In doing so, a new concept of endmember initialization algorithm (EIA) is thus proposed, and four different algorithms are suggested for this purpose. It is surprisingly found that many EIA-generated initial endmembers turn out to be the final desired endmembers. A further objective is to demonstrate that EEAs implemented in conjunction with EIA-generated initial endmembers can significantly reduce the number of endmember replacements as well as the computing time during endmember search",
Visual odometry on the Mars exploration rovers - a tool to ensure accurate driving and science imaging,"In this paper, visual odometry is presented as an approach to position estimation to find features in a stereo image pair and track them from one frame to the next. Visual odometry has been a highly effective tool for maintaining vehicle safety while driving near obstacles on slopes, achieving difficult drive approaches in fewer sols, and ensuring accurate science imaging. Although it requires active pointing by human drivers in feature-poor terrain, the improved position knowledge enables more autonomous capability and better science return during planetary operations",
Twenty New Digital Brain Phantoms for Creation of Validation Image Data Bases,"Simulations provide a way of generating data where ground truth is known, enabling quantitative testing of image processing methods. In this paper, we present the construction of 20 realistic digital brain phantoms that can be used to simulate medical imaging data. The phantoms are made from 20 normal adults to take into account intersubject anatomical variabilities. Each digital brain phantom was created by registering and averaging four T1, T2, and proton density (PD)-weighted magnetic resonance imaging (MRI) scans from each subject. A fuzzy minimum distance classification was used to classify voxel intensities from T1, T2, and PD average volumes into grey-matter, white matter, cerebro-spinal fluid, and fat. Automatically generated mask volumes were required to separate brain from nonbrain structures and ten fuzzy tissue volumes were created: grey matter, white matter, cerebro-spinal fluid, skull, marrow within the bone, dura, fat, tissue around the fat, muscles, and skin/muscles. A fuzzy vessel class was also obtained from the segmentation of the magnetic resonance angiography scan of the subject. These eleven fuzzy volumes that describe the spatial distribution of anatomical tissues define the digital phantom, where voxel intensity is proportional to the fraction of tissue within the voxel. These fuzzy volumes can be used to drive simulators for different modalities including MRI, PET, or SPECT. These phantoms were used to construct 20 simulated T1-weighted MR scans. To evaluate the realism of these simulations, we propose two approaches to compare them to real data acquired with the same acquisition parameters. The first approach consists of comparing the intensities within the segmented classes in both real and simulated data. In the second approach, a whole brain voxel-wise comparison between simulations and real T1-weighted data is performed. The first comparison underlines that segmented classes appear to properly represent the anatomy on average, and that inside these classes, the simulated and real intensity values are quite similar. The second comparison enables the study of the regional variations with no a priori class. The experiments demonstrate that these variations are small when real data are corrected for intensity nonuniformity",
"""Cloning Considered Harmful"" Considered Harmful","Current literature on the topic of duplicated (cloned) code in software systems often considers duplication harmful to the system quality and the reasons commonly cited for duplicating code often have a negative connotation. While these positions are sometimes correct, during our case studies we have found that this is not universally true, and we have found several situations where code duplication seems to be a reasonable or even beneficial design option. For example, a method of introducing experimental changes to core subsystems is to duplicate the subsystem and introduce changes there in a kind of sandbox testbed. As features mature and become stable within the experimental subsystem, they can then be introduced gradually into the stable code base. In this way risk of introducing instabilities in the stable version is minimized. This paper describes several patterns of cloning that we have encountered in our case studies and discusses the advantages and disadvantages associated with using them",
E-service design using i* and e/sup 3/ value modeling,"Two requirements engineering techniques, i* and e3 value, work together to explore commercial e-services from a strategic-goal and profitability perspective. We demonstrate our approach using a case study on Internet radio",
Bayesian Population Decoding of Motor Cortical Activity Using a Kalman Filter,"Effective neural motor prostheses require a method for decoding neural activity representing desired movement. In particular, the accurate reconstruction of a continuous motion signal is necessary for the control of devices such as computer cursors, robots, or a patient's own paralyzed limbs. For such applications, we developed a real-time system that uses Bayesian inference techniques to estimate hand motion from the firing rates of multiple neurons. In this study, we used recordings that were previously made in the arm area of primary motor cortex in awake behaving monkeys using a chronically implanted multielectrode microarray. Bayesian inference involves computing the posterior probability of the hand motion conditioned on a sequence of observed firing rates; this is formulated in terms of the product of a likelihood and a prior. The likelihood term models the probability of firing rates given a particular hand motion. We found that a linear gaussian model could be used to approximate this likelihood and could be readily learned from a small amount of training data. The prior term defines a probabilistic model of hand kinematics and was also taken to be a linear gaussian model. Decoding was performed using a Kalman filter, which gives an efficient recursive method for Bayesian inference when the likelihood and prior are linear and gaussian.In off-line experiments, the Kalman filter reconstructions of hand trajectory were more accurate than previously reported results.The resulting decoding algorithm provides a principled probabilistic model of motor-cortical coding, decodes hand motion in real time, provides an estimate of uncertainty, and is straightforward to implement. Additionally the formulation unifies and extends previous models of neural coding while providing insights into the motor-cortical code.",
Reconstruction of coronary arteries from a single rotational X-ray projection sequence,"Cardiovascular diseases remain the primary cause of death in developed countries. In most cases, exploration of possibly underlying coronary artery pathologies is performed using X-ray coronary angiography. Current clinical routine in coronary angiography is directly conducted in two-dimensional projection images from several static viewing angles. However, for diagnosis and treatment purposes, coronary artery reconstruction is highly suitable. The purpose of this study is to provide physicians with a three-dimensional (3-D) model of coronary arteries, e.g., for absolute 3-D measures for lesion assessment, instead of direct projective measures deduced from the images, which are highly dependent on the viewing angle. In this paper, we propose a novel method to reconstruct coronary arteries from one single rotational X-ray projection sequence. As a side result, we also obtain an estimation of the coronary artery motion. Our method consists of three main consecutive steps: 1) 3-D reconstruction of coronary artery centerlines, including respiratory motion compensation; 2) coronary artery four-dimensional motion computation; 3) 3-D tomographic reconstruction of coronary arteries, involving compensation for respiratory and cardiac motions. We present some experiments on clinical datasets, and the feasibility of a true 3-D Quantitative Coronary Analysis is demonstrated.",
Balancing Systematic and Flexible Exploration of Social Networks,"Social network analysis (SNA) has emerged as a powerful method for understanding the importance of relationships in networks. However, interactive exploration of networks is currently challenging because: (1) it is difficult to find patterns and comprehend the structure of networks with many nodes and links, and (2) current systems are often a medley of statistical methods and overwhelming visual output which leaves many analysts uncertain about how to explore in an orderly manner. This results in exploration that is largely opportunistic. Our contributions are techniques to help structural analysts understand social networks more effectively. We present SocialAction, a system that uses attribute ranking and coordinated views to help users systematically examine numerous SNA measures. Users can (1) flexibly iterate through visualizations of measures to gain an overview, filter nodes, and find outliers, (2) aggregate networks using link structure, find cohesive subgroups, and focus on communities of interest, and (3) untangle networks by viewing different link types separately, or find patterns across different link types using a matrix overview. For each operation, a stable node layout is maintained in the network visualization so users can make comparisons. SocialAction offers analysts a strategy beyond opportunism, as it provides systematic, yet flexible, techniques for exploring social networks",
Towards Regulatory Compliance: Extracting Rights and Obligations to Align Requirements with Regulations,"In the United States, federal and state regulations prescribe stakeholder rights and obligations that must be satisfied by the requirements for software systems. These regulations are typically wrought with ambiguities, making the process of deriving system requirements ad hoc and error prone. In highly regulated domains such as healthcare, there is a need for more comprehensive standards that can be used to assure that system requirements conform to regulations. To address this need, we expound upon a process called semantic parameterization previously used to derive rights and obligations from privacy goals. In this work, we apply the process to the privacy rule from the U.S. Health Insurance Portability and Accountability Act (HIPAA). We present our methodology for extracting and prioritizing rights and obligations from regulations and show how semantic models can be used to clarify ambiguities through focused elicitation and to balance rights with obligations. The results of our analysis can aid requirements engineers, standards organizations, compliance officers, and stakeholders in assuring systems conform to policy and satisfy requirements",
An RSSI-based scheme for sybil attack detection in wireless sensor networks,"A sybil node impersonates other nodes by broadcasting messages with multiple node identifiers (ID). In contrast to existing solutions which are based on sharing encryption keys, we present a robust and lightweight solution for sybil attack problem based on received signal strength indicator (RSSI) readings of messages. Our solution is robust since it detects all sybil attack cases with 100% completeness and less than a few percent false positives. Our solution is lightweight in the sense that alongside the receiver we need the collaboration of one other node (i.e., only one message communication) for our protocol. We show through experiments that even though RSSI is time-varying and unreliable in general and radio transmission is non-isotropic, using ratio of RSSIs from multiple receivers it is feasible to overcome these problems",
A planning system based on Markov decision processes to guide people with dementia through activities of daily living,Older adults with dementia often cannot remember how to complete activities of daily living and require a caregiver to aid them through the steps involved. The use of a computerized guidance system could potentially reduce the reliance on a caregiver. This paper examines the design and preliminary evaluation of a planning system that uses Markov decision processes (MDPs) to determine when and how to provide prompts to a user with dementia for guidance through the activity of handwashing. Results from the study suggest that MDPs can be applied effectively to this type of guidance problem. Considerations for the development of future guidance systems are presented,
"Measure Locally, Reason Globally: Occlusion-sensitive Articulated Pose Estimation","Part-based tree-structured models have been widely used for 2D articulated human pose-estimation. These approaches admit efficient inference algorithms while capturing the important kinematic constraints of the human body as a graphical model. These methods often fail however when multiple body parts fit the same image region resulting in global pose estimates that poorly explain the overall image evidence. Attempts to solve this problem have focused on the use of strong prior models that are limited to learned activities such as walking. We argue that the problem actually lies with the image observations and not with the prior. In particular, image evidence for each body part is estimated independently of other parts without regard to self-occlusion. To address this we introduce occlusion-sensitive local likelihoods that approximate the global image likelihood using per-pixel hidden binary variables that encode the occlusion relationships between parts. This occlusion reasoning introduces interactions between non-adjacent body parts creating loops in the underlying graphical model. We deal with this using an extension of an approximate belief propagation algorithm (PAMPAS). The algorithm recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods.",
Fully Automatic Registration of 3D Point Clouds,"We propose a novel technique for the registration of 3D point clouds which makes very few assumptions: we avoid any manual rough alignment or the use of landmarks, displacement can be arbitrarily large, and the two point sets can have very little overlap. Crude alignment is achieved by estimation of the 3D-rotation from two Extended Gaussian Images even when the data sets inducing them have partial overlap. The technique is based on the correlation of the two EGIs in the Fourier domain and makes use of the spherical and rotational harmonic transforms. For pairs with low overlap which fail a critical verification step, the rotational alignment can be obtained by the alignment of constellation images generated from the EGIs. Rotationally aligned sets are matched by correlation using the Fourier transform of volumetric functions. A fine alignment is acquired in the final step by running Iterative Closest Points with just few iterations.",
"Ultra-Narrow Silicon Nanowire Gate-All-Around CMOS Devices: Impact of Diameter, Channel-Orientation and Low Temperature on Device Performance","Fully CMOS compatible silicon-nanowire (SiNW) gate-all-around (GAA) n- and p-MOS transistors are fabricated with nanowire channel in different crystal orientations and characterized at various temperatures down to 5K. SiNW width is controlled in 1 nm steps and varied from 3 to 6 nm. Devices show high drive current (2.4 mA/mum for n-FET, 1.3 mA/mum for p-FET), excellent gate control, and reduced sensitivity to temperature. Strong evidences of carrier confinement are noticed in term of Id-Vg oscillations and shift in threshold voltage with SiNW diameter. Orientation impact has been investigated as well",
Multimodal function optimization based on particle swarm optimization,"In this paper, a new algorithm for the multimodal function optimization is proposed, based on the particle swarm optimization (PSO). A new method, named the multigrouped particle swarm optimization (MGPSO), keeps basic concepts of the PSO, and, thus, shows a more straightforward convergence compared to conventional hybrid type approaches. Moreover, the MGPSO has a unique advantage in that one can search N superior peaks of a multimodal function when the number of groups is N. The usefulness of the proposed algorithm was verified by the application to various case studies, including a practical electromagnetic optimization problem",
Modeling Crowd and Trained Leader Behavior during Building Evacuation,"This article considers animating evacuation in complex buildings by crowds who might not know the structure's connectivity, or who find routes accidentally blocked. It takes into account simulated crowd behavior under two conditions: where agents communicate building route knowledge, and where agents take different roles such as trained personnel, leaders, and followers",
Implementation and Evaluation of On-Chip Network Architectures,"Driven by the need for higher bandwidth and complexity reduction, off-chip interconnect has evolved from proprietary busses to networked architectures. A similar evolution is occurring in on-chip interconnect. This paper presents the design, implementation and evaluation of one such on-chip network, the TRIPS OCN. The OCN is a wormhole routed, 4x10, 2D mesh network with four virtual channels. It provides a high bandwidth, low latency interconnect between the TRIPS processors, L2 cache banks and I/O units. We discuss the tradeoffs made in the design of the OCN, in particular why area and complexity were traded off against latency. We then evaluate the OCN using synthetic as well as realistic loads. We found that synthetic benchmarks do not provide sufficient indication of the behavior of realistic loads on this network. Finally, we examine the effect of link bandwidth and router FIFO depth on overall performance.",
Infrastructure for engineered emergence on sensor/actuator networks,"The study of self-organizing systems has now reached the tool-building phase, in which a new discipline of self-managing systems engineering can begin to emerge. The next step is to refine the principles of self-organization into a system of composable parts suitable for engineering - much as components such as capacitors, transistors, and resistors capture electromagnetism principles for electronic engineering. We've begun the process of transforming the science into an engineering discipline in the domain of sensor/actuator network applications, observing that in many applications the deployed network approximates a physical space and that the space, rather than the network, is being programmed. This observation lets us use the amorphous medium abstraction to decouple self-management problems. So, global behavior descriptions in our Proto language can be compiled automatically into locally executed code that produces emergent phenomena matching the global description. We've experimentally verified our code both in simulation and (for small programs) on a network of sensor/actuator nodes called Mica2 motes.",
A probabilistic framework for modeling and real-time monitoring human fatigue,"A probabilistic framework based on the Bayesian networks for modeling and real-time inferring human fatigue by integrating information from various sensory data and certain relevant contextual information is introduced. A static fatigue model that captures the static relationships between fatigue, significant factors that cause fatigue, and various sensory observations that typically result from fatigue is first presented. Such a model provides mathematically coherent and sound basis for systematically aggregating uncertain evidences from different sources, augmented with relevant contextual information. The static model, however, fails to capture the dynamic aspect of fatigue. Fatigue is a cognitive state that is developed over time. To account for the temporal aspect of human fatigue, the static fatigue model is extended based on dynamic Bayesian networks. The dynamic fatigue model allows to integrate fatigue evidences not only spatially but also temporally, therefore, leading to a more robust and accurate fatigue modeling and inference. A real-time nonintrusive fatigue monitor was built based on integrating the proposed fatigue model with a computer vision system developed for extracting various visual cues typically related to fatigue. Performance evaluation of the fatigue monitor using both synthetic and real data demonstrates the validity of the proposed fatigue model in both modeling and real-time inference of fatigue",
Resilient multicast using overlays,"We introduce Probabilistic Resilient Multicast (PRM): a multicast data recovery scheme that improves data delivery ratios while maintaining low end-to-end latencies. PRM has both a proactive and a reactive components; in this paper we describe how PRM can be used to improve the performance of application-layer multicast protocols especially when there are high packet losses and host failures. Through detailed analysis in this paper, we show that this loss recovery technique has efficient scaling properties-the overheads at each overlay node asymptotically decrease to zero with increasing group sizes. As a detailed case study, we show how PRM can be applied to the NICE application-layer multicast protocol. We present detailed simulations of the PRM-enhanced NICE protocol for 10 000 node Internet-like topologies. Simulations show that PRM achieves a high delivery ratio (>97%) with a low latency bound (600 ms) for environments with high end-to-end network losses (1%-5%) and high topology change rates (5 changes per second) while incurring very low overheads (<5%).",
Multidimensional Vector Regression for Accurate and Low-Cost Location Estimation in Pervasive Computing,"In this paper, we present an algorithm for multidimensional vector regression on data that are highly uncertain and nonlinear, and then apply it to the problem of indoor location estimation in a wireless local area network (WLAN). Our aim is to obtain an accurate mapping between the signal space and the physical space without requiring too much human calibration effort. This location estimation problem has traditionally been tackled through probabilistic models trained on manually labeled data, which are expensive to obtain. In contrast, our algorithm adopts kernel canonical correlation analysis (KCCA) to build a nonlinear mapping between the signal-vector space and the physical location space by transforming data in both spaces into their canonical features. This allows the pairwise similarity of samples in both spaces to be maximally correlated using kernels. We use a Gaussian kernel to adapt to the noisy characteristics of signal strengths and a Matern kernel to sense the changes in physical locations. By using real data collected in an 802.11 wireless LAN environment, we achieve accurate location estimation for pervasive computing while requiring a much smaller set of labeled training data than previous methods",
Seamless image stitching by minimizing false edges,"Various applications such as mosaicing and object insertion require stitching of image parts. The stitching quality is measured visually by the similarity of the stitched image to each of the input images, and by the visibility of the seam between the stitched images. In order to define and get the best possible stitching, we introduce several formal cost functions for the evaluation of the stitching quality. In these cost functions the similarity to the input images and the visibility of the seam are defined in the gradient domain, minimizing the disturbing edges along the seam. A good image stitching will optimize these cost functions, overcoming both photometric inconsistencies and geometric misalignments between the stitched images. We study the cost functions and compare their performance for different scenarios both theoretically and practically. Our approach is demonstrated in various applications including generation of panoramic images, object blending and removal of compression artifacts. Comparisons with existing methods show the benefits of optimizing the measures in the gradient domain.","Cost function,
Image generation,
Photometry,
Image coding,
Layout,
Computer science,
Optimization methods,
Image resolution,
Cameras,
Lighting"
Monomial and quadratic bent functions over the finite fields of odd characteristic,"Considered are p-ary bent functions having the form f(x)=Tr/sub n/(/spl sigma//sub i=0//sup s/a/sub i/x/sup di/). A new class of ternary monomial regular bent function with the Dillon exponent is discovered. The existence of Dillon bent functions in the general case is an open problem of deciding whether a certain Kloosterman sum can take on the value -1. Also described is the general Gold-like form of a bent function that covers all the previously known monomial quadratic cases. The (weak) regularity of the new as well as of known monomial bent functions is discussed and the first example of a not weakly regular bent function is given. Finally, some criteria for an arbitrary quadratic function to be bent are proven.",
Intravascular ultrasound image segmentation: a three-dimensional fast-marching method based on gray level distributions,"Intravascular ultrasound (IVUS) is a catheter based medical imaging technique particularly useful for studying atherosclerotic disease. It produces cross-sectional images of blood vessels that provide quantitative assessment of the vascular wall, information about the nature of atherosclerotic lesions as well as plaque shape and size. Automatic processing of large IVUS data sets represents an important challenge due to ultrasound speckle, catheter artifacts or calcification shadows. A new three-dimensional (3-D) IVUS segmentation model, that is based on the fast-marching method and uses gray level probability density functions (PDFs) of the vessel wall structures, was developed. The gray level distribution of the whole IVUS pullback was modeled with a mixture of Rayleigh PDFs. With multiple interface fast-marching segmentation, the lumen, intima plus plaque structure, and media layers of the vessel wall were computed simultaneously. The PDF-based fast-marching was applied to 9 in vivo IVUS pullbacks of superficial femoral arteries and to a simulated IVUS pullback. Accurate results were obtained on simulated data with average point to point distances between detected vessel wall borders and ground truth <0.072 mm. On in vivo IVUS, a good overall performance was obtained with average distance between segmentation results and manually traced contours <0.16 mm. Moreover, the worst point to point variation between detected and manually traced contours stayed low with Hausdorff distances <0.40 mm, indicating a good performance in regions lacking information or containing artifacts. In conclusion, segmentation results demonstrated the potential of gray level PDF and fast-marching methods in 3-D IVUS image processing.",
Evaluation of Four Probability Distribution Models for Speckle in Clinical Cardiac Ultrasound Images,"Segmenting cardiac ultrasound images requires a model for the statistics of speckle in the images. Although the statistics of speckle are well understood for the raw transducer signal, the statistics of speckle in the image are not. This paper evaluates simple empirical models for first-order statistics for the distribution of gray levels in speckle. The models are created by analyzing over 100 images obtained from commercial ultrasound machines in clinical settings. The data in the images suggests a unimodal scalable family of distributions as a plausible model. Four families of distributions (Gamma, Weibull, Normal, and Log-normal) are compared with the data using goodness-of-fit and misclassification tests. Attention is devoted to the analysis of artifacts in images and to the choice of goodness-of-fit and misclassification tests. The distribution of parameters of one of the models is investigated and priors for the distribution are suggested",
The weight distribution of a class of linear codes from perfect nonlinear functions,"In this correspondence, the weight distribution of a class of linear codes based on perfect nonlinear functions (also called planar functions) is determined. The class of linear codes under study are either optimal or among the best codes known, and have nice applications in cryptography.","Linear code,
Error correction codes,
Cryptography,
Frequency,
Councils,
Computer science"
A Joinless Approach for Mining Spatial Colocation Patterns,"Spatial colocations represent the subsets of features which are frequently located together in geographic space. Colocation pattern discovery presents challenges since spatial objects are embedded in a continuous space, whereas classical data is often discrete. A large fraction of the computation time is devoted to identifying the instances of colocation patterns. We propose a novel joinless approach for efficient colocation pattern mining. The jotnless colocation mining algorithm uses an instance-lookup scheme instead of an expensive spatial or instance join operation for identifying colocation instances. We prove the joinless algorithm is correct and complete in finding colocation rules. We also describe a partial join approach for spatial data which are clustered in neighborhood areas. We provide the algebraic cost models to characterize the performance dominance zones of the joinless method and the partial join method with a current join-based colocation mining method, and compare their computational complexities. In the experimental evaluation, using synthetic and real-world data sets, our methods performed more efficiently than the join-based method and show more scalability in dense data",
Smart Gossip: An Adaptive Gossip-based Broadcasting Service for Sensor Networks,"A network-wide broadcast service is often used for information dissemination in sensor networks. Sensor networks are typically energy-constrained and prone to failures. In view of these constraints, the broadcast service should minimize energy consumption by reducing redundant transmissions, and be tolerant to frequent node and link failures. We propose ""smart gossip"", a probabilistic protocol that offers a broadcast service with low overheads. Smart gossip automatically and dynamically adapts transmission probabilities based on the underlying network topology. The protocol is capable of coping with wireless losses and unpredictable node failures that affect network connectivity over time. The resulting protocol is completely decentralized. We present thorough experimental results to evaluate our ""smart gossip"" proposal, and demonstrate its benefits over existing protocols",
Distributed Anomaly Detection in Wireless Sensor Networks,"Identifying misbehaviors is an important challenge for monitoring, fault diagnosis and intrusion detection in wireless sensor networks. A key problem is how to minimize the communication overhead and energy consumption in the network when identifying misbehaviors. Our approach to this problem is based on a distributed, cluster-based anomaly detection algorithm. We minimize the communication overhead by clustering the sensor measurements and merging clusters before sending a description of the clusters to the other nodes. In order to evaluate our distributed scheme, we implemented our algorithm in a simulation based on the sensor data gathered from the Great Duck Island project. We demonstrate that our scheme achieves comparable accuracy compared to a centralized scheme with a significant reduction in communication overhead",
An Adaptive Memoryless Protocol for RFID Tag Collision Arbitration,"A radio frequency identification (RFID) reader recognizes objects through wireless communications with RFID tags. Tag collision arbitration for passive tags is a significant issue for fast tag identification due to communication over a shared wireless channel. This paper presents an adaptive memoryless protocol, which is an improvement on the query tree protocol. Memoryless means that tags need not have additional memory except ID for identification. To reduce collisions and identify tags promptly, we use information obtained from the last process of tag identification at a reader. Our performance evaluation shows that the adaptive memoryless protocol causes fewer collisions and takes shorter delay for recognizing all tags while preserving lower communication overhead than other tree based tag anticollision protocols",
Deformation-based mapping of volume change from serial brain MRI in the presence of local tissue contrast change,"This paper is motivated by the analysis of serial structural magnetic resonance imaging (MRI) data of the brain to map patterns of local tissue volume loss or gain over time, using registration-based deformation tensor morphometry. Specifically, we address the important confound of local tissue contrast changes which can be induced by neurodegenerative or neurodevelopmental processes. These not only modify apparent tissue volume, but also modify tissue integrity and its resulting MRI contrast parameters. In order to address this confound we derive an approach to the voxel-wise optimization of regional mutual information (RMI) and use this to drive a viscous fluid deformation model between images in a symmetric registration process. A quantitative evaluation of the method when compared to earlier approaches is included using both synthetic data and clinical imaging data. Results show a significant reduction in errors when tissue contrast changes locally between acquisitions. Finally, examples of applying the technique to map different patterns of atrophy rate in different neurodegenerative conditions is included.",
Secret sharing schemes from three classes of linear codes,"Secret sharing has been a subject of study for over 20 years, and has had a number of real-world applications. There are several approaches to the construction of secret sharing schemes. One of them is based on coding theory. In principle, every linear code can be used to construct secret sharing schemes. But determining the access structure is very hard as this requires the complete characterization of the minimal codewords of the underlying linear code, which is a difficult problem in general. In this paper, a sufficient condition for all nonzero codewords of a linear code to be minimal is derived from exponential sums. Some linear codes whose covering structure can be determined are constructed, and then used to construct secret sharing schemes with nice access structures.",
Graphical Models and Point Pattern Matching,"This paper describes a novel solution to the rigid point pattern matching problem in Euclidean spaces of any dimension. Although we assume rigid motion, jitter is allowed. We present a noniterative, polynomial time algorithm that is guaranteed to find an optimal solution for the noiseless case. First, we model point pattern matching as a weighted graph matching problem, where weights correspond to Euclidean distances between nodes. We then formulate graph matching as a problem of finding a maximum probability configuration in a graphical model. By using graph rigidity arguments, we prove that a sparse graphical model yields equivalent results to the fully connected model in the noiseless case. This allows us to obtain an algorithm that runs in polynomial time and is provably optimal for exact matching between noiseless point sets. For inexact matching, we can still apply the same algorithm to find approximately optimal solutions. Experimental results obtained by our approach show improvements in accuracy over current methods, particularly when matching patterns of different sizes",
Routers with Very Small Buffers,,
Sensor-Based Fast Thermal Evaluation Model For Energy Efficient High-Performance Datacenters,"In this work, we propose an abstract heat flow model which uses temperature information from onboard and ambient sensors, characterizes hot air recirculation based on these information, and accelerates the thermal evaluation process for high performance datacenters. This is critical to minimize energy costs, optimize computing resources, and maximize computation capability of the datacenters. Given a workload and thermal profile, obtained from various distributed sensors, we predict the resulting temperature distribution in a fast and accurate manner taking into account the recirculation characterization of a datacenter topology. Simulation results confirm our hypothesis that heat recirculation can be characterized as cross interference in our abstract heat flow model. Moreover, fast thermal evaluation based on cross interference can be used in online thermal management to predict temperature distribution in real-time.",
MineBench: A Benchmark Suite for Data Mining Workloads,"Data mining constitutes an important class of scientific and commercial applications. Recent advances in data extraction techniques have created vast data sets, which require increasingly complex data mining algorithms to sift through them to generate meaningful information. The disproportionately slower rate of growth of computer systems has led to a sizeable performance gap between data mining systems and algorithms. The first step in closing this gap is to analyze these algorithms and understand their bottlenecks. With this knowledge, current computer architectures can be optimized for data mining applications. In this paper, we present MineBench, a publicly available benchmark suite containing fifteen representative data mining applications belonging to various categories such as clustering, classification, and association rule mining. We believe that MineBench will be of use to those looking to characterize and accelerate data mining workloads","Data mining,
Classification tree analysis,
Application software,
Clustering algorithms,
Association rules,
Computer architecture,
Databases,
Fuzzy logic,
DNA,
Sequences"
Modified Differential Evolution for Constrained Optimization,"In this paper, we present a Differential-Evolution based approach to solve constrained optimization problems. The aim of the approach is to increase the probability of each parent to generate a better offspring. This is done by allowing each solution to generate more than one offspring but using a different mutation operator which combines information of the best solution in the population and also information of the current parent to find new search directions. Three selection criteria based on feasibility are used to deal with the constraints of the problem and also a diversity mechanism is added to maintain infeasible solutions located in promising areas of the search space. The approach is tested in a set of test problems proposed for the special session on Constrained Real Parameter Optimization. The results obtained are discussed and some conclusions are established.",
A project-based learning approach to design electronic systems curricula,"This paper presents an approach to design Electronic Systems Curricula for making electronics more appealing to students. Since electronics is an important grounding for other disciplines (computer science, signal processing, and communications), this approach proposes the development of multidisciplinary projects using the project-based learning (PBL) strategy for increasing the attractiveness of the curriculum. The proposed curriculum structure consists of eight courses: four theoretical courses and four PBL courses (including a compulsory Master's thesis). In PBL courses, the students, working together in groups, develop multidisciplinary systems, which become progressively more complex. To address this complexity, the Department of Electronic Engineering has invested in the last five years in many resources for developing software tools and a common hardware. This curriculum has been evaluated successfully for the last four academic years: the students have increased their interest in electronics and have given the courses an average grade of more than 71% for all PBL course evaluations (data extracted from students surveys). The students have also acquired new skills and obtained very good academic results: the average grade was more than 74% for all PBL courses. An important result is that all students have developed more complex and sophisticated electronic systems, while considering that the results are worth the effort invested",Electronics engineering education
A Novel Approach for Phase-Type Fitting with the EM Algorithm,"The representation of general distributions or measured data by phase-type distributions is an important and nontrivial task in analytical modeling. Although a large number of different methods for fitting parameters of phase-type distributions to data traces exist, many approaches lack efficiency and numerical stability. In this paper, a novel approach is presented that fits a restricted class of phase-type distributions, namely, mixtures of Erlang distributions, to trace data. For the parameter fitting, an algorithm of the expectation maximization type is developed. This paper shows that these choices result in a very efficient and numerically stable approach which yields phase-type approximations for a wide range of data traces that are as good or better than approximations computed with other less efficient and less stable fitting methods. To illustrate the effectiveness of the proposed fitting algorithm, we present comparative results for our approach and two other methods using six benchmark traces and two real traffic traces as well as quantitative results from queuing analysis","Telecommunication traffic,
Traffic control,
Phase measurement,
Algorithm design and analysis,
Parameter estimation,
Analytical models,
Numerical stability,
Queueing analysis,
Process design,
Markov processes"
Reduced PWM harmonic distortion for multilevel inverters operating over a wide modulation range,"It is known that the optimal carrier based approach for modulating a multilevel converter is to use a phase disposition (PD) carrier arrangement with a common mode offset added to the reference waveforms to centre the implicitly selected space vectors. However, this strategy does not fully utilize all available voltage levels at lower modulation depths, with an odd level system only using odd voltage levels and an even level system only using even voltage levels as the modulation depth varies. Recent work has suggested that this is not the harmonically optimal approach at reduced modulation depths. This paper shows how up to a 40% reduction in harmonic distortion can be achieved if all available voltage levels are used throughout the linear modulation range. The improvement is achieved by adding a simple (1/2) carrier magnitude common mode dc offset in key modulation regions, which allows the converter to use all available voltage levels. The paper uses analytical spectral decomposition and harmonic flux trajectory analysis to propose a theoretical basis for this improvement, and to determine the precise points at which the (1/2) carrier magnitude offset should be added to achieve the harmonic improvement.",
Visual Analysis of Large Heterogeneous Social Networks by Semantic and Structural Abstraction,"Social network analysis is an active area of study beyond sociology. It uncovers the invisible relationships between actors in a network and provides understanding of social processes and behaviors. It has become an important technique in a variety of application areas such as the Web, organizational studies, and homeland security. This paper presents a visual analytics tool, OntoVis, for understanding large, heterogeneous social networks, in which nodes and links could represent different concepts and relations, respectively. These concepts and relations are related through an ontology (also known as a schema). OntoVis is named such because it uses information in the ontology associated with a social network to semantically prune a large, heterogeneous network. In addition to semantic abstraction, OntoVis also allows users to do structural abstraction and importance filtering to make large networks manageable and to facilitate analytic reasoning. All these unique capabilities of OntoVis are illustrated with several case studies",
Spectral Occupancy and Interference Studies in support of Cognitive Radio Technology Deployment,"This paper describes the high value of cognitive radio technology and characterizes the opportunity space in four distinct classes. A Chicago-based spectrum occupancy study illustrates the opportunity showing that 82.6% of the spectral capacity is unused. A set of spectral signatures is presented for common devices in the unlicensed frequency band with the view that this technique can be widely deployed across the spectrum. The limitations of current network simulation tools in an interference environment are identified. Finally, the paper briefly discusses several of the non-technology related issues that impact the deployment of cognitive radio techniques.",
Eclipse Attacks on Overlay Networks: Threats and Defenses,,"Peer to peer computing,
Application software,
Cathode ray tubes,
Computer science,
Software systems,
Robustness,
Control systems,
Operating systems,
Computer crime,
Advertising"
A Cross-Layer Protocol for Wireless Sensor Networks,"Severe energy constraints of battery-powered sensor nodes necessitate energy-efficient communication protocols in order to fulfill application objectives of wireless sensor networks (WSN). However, the vast majority of the existing solutions are based on classical layered protocols approach. It is much more resource-efficient to have a unified scheme which melts common protocol layer functionalities into a cross-layer module resource-constrained sensor nodes. To the best of our knowledge, to date, there is no unified cross-layer communication protocol for efficient and reliable event communication which considers transport, routing, medium access functionalities with physical layer (wireless channel) effects for WSNs. In this paper, a unified cross-layer protocol is developed, which replaces the entire traditional layered protocol architecture that has been used so far in WSNs. Our design principle is complete unified cross-layering such that both the information and the functionalities of traditional communication layers are melted in a single protocol. The objective of the proposed cross-layer protocol is highly reliable communication decisions and local congestion avoidance. To this end, the protocol operation is governed by the new concept of initiative determination. Based on this concept, the cross-layer protocol performs received based contention, local congestion control, and distributed duty cycle operation in order to realize efficient and reliable communication in WSN. Performance evaluation results show that the proposed cross-layer protocol significantly improves the communication efficiency and outperforms the traditional layered protocol architectures.","Wireless application protocol,
Wireless sensor networks,
Access protocols,
Sensor phenomena and characterization,
Media Access Protocol,
Energy efficiency,
Routing protocols,
Transport protocols,
Physical layer,
Collaboration"
The Index Poisoning Attack in P2P File Sharing Systems,,"Peer to peer computing,
Information science,
Environmentally friendly manufacturing techniques,
Industrial pollution,
TV,
Pollution measurement,
Application software,
Internet,
Digital audio players,
Motion pictures"
Tutorial on Agent-Based Modeling and Simulation PART 2: How to Model with Agents,"Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of interacting autonomous agents. ABMS promises to have far-reaching effects on the way that businesses use computers to support decision-making and researchers use electronic laboratories to do research. Some have gone so far as to contend that ABMS is a new way of doing science. Computational advances make possible a growing number of agent-based applications across many fields. Applications range from modeling agent behavior in the stock market and supply chains, to predicting the spread of epidemics and the threat of bio-warfare, from modeling the growth and decline of ancient civilizations to modeling the complexities of the human immune system, and many more. This tutorial describes the foundations of ABMS, identifies ABMS toolkits and development methods illustrated through a supply chain example, and provides thoughts on the appropriate contexts for ABMS versus conventional modeling techniques","Tutorial,
Predictive models,
Computational modeling,
Application software,
Supply chains,
Autonomous agents,
Decision making,
Consumer electronics,
Laboratories,
Stock markets"
ScalaBLAST: A Scalable Implementation of BLAST for High-Performance Data-Intensive Bioinformatics Analysis,"Genes in an organism's DNA (genome) have embedded in them information about proteins, which are the molecules that do most of a cell's work. A typical bacterial genome contains on the order of 5,000 genes. Mammalian genomes can contain tens of thousands of genes. For each genome sequenced, the challenge is to identify protein components (proteome) being actively used for a given set of conditions. Fundamentally, sequence alignment is a sequence matching problem focused on unlocking protein information embedded in the genetic code, making it possible to assemble a ""tree of life"" by comparing new sequences against all sequences from known organisms. But, the memory footprint of sequence data is growing more rapidly than per-node core memory. Despite years of research and development, high-performance sequence alignment applications either do not scale well, cannot accommodate very large databases in core, or require special hardware. We have developed a high-performance sequence alignment application, ScalaBLAST, which accommodates very large databases and which scales linearly to as many as thousands of processors on both distributed memory and shared memory architectures, representing a substantial improvement over the current state-of-the-art in high-performance sequence alignment with scaling and portability. ScalaBLAST relies on a collection of techniques - distributing the target database over available memory, multilevel parallelism to exploit concurrency, parallel I/O, and latency hiding through data prefetching - to achieve high-performance and scalability. This demonstrated approach of database sharing combined with effective task scheduling should have broad ranging applications to other informatics-driven sciences",
Multi-robot SLAM with Unknown Initial Correspondence: The Robot Rendezvous Case,"This paper presents a new approach to the multi-robot map-alignment problem that enables teams of robots to build joint maps without initial knowledge of their relative poses. The key contribution of this work is an optimal algorithm for merging (not necessarily overlapping) maps that are created by different robots independently. Relative pose measurements between pairs of robots are processed to compute the coordinate transformation between any two maps. Noise in the robot-to-robot observations, propagated through the map-alignment process, increases the error in the position estimates of the transformed landmarks, and reduces the overall accuracy of the merged map. When there is overlap between the two maps, landmarks that appear twice provide additional information, in the form of constraints, which increases the alignment accuracy. Landmark duplicates are identified through a fast nearest-neighbor matching algorithm. In order to reduce the computational complexity of this search process, a kd-tree is used to represent the landmarks in the original map. The criterion employed for matching any two landmarks is the Mahalanobis distance. As a means of validation, we present experimental results obtained from two robots mapping an area of 4,800 m2",
A low-power 22-bit incremental ADC,"This paper describes a low-power 22-bit incremental ADC, including an on-chip digital filter and a low-noise/low-drift oscillator, realized in a 0.6-mum CMOS process. It incorporates a novel offset-cancellation scheme based on fractal sequences, a novel high-accuracy gain control circuit, and a novel reduced-complexity realization for the on-chip sinc filter. The measured output noise was 0.25 ppm (2.5 muVRMS), the DC offset 2 muV, the gain error 2 ppm, and the INL 4 ppm. The chip operates with a single 2.7-5 V supply, and draws only 120 muA current during conversion","Digital filters,
Noise measurement,
Gain control,
Gain measurement,
Computer science,
CMOS process,
Fractals,
Integrated circuit measurements,
Integrated circuit noise,
Switching converters"
Feedback Control Architecture and Design Methodology for Service Delay Guarantees in Web Servers,"This paper presents the design and implementation of an adaptive Web server architecture to provide relative and absolute connection delay guarantees for different service classes. The first contribution of this paper is an adaptive architecture based on feedback control loops that enforce desired connection delays via dynamic connection scheduling and process reallocation. The second contribution is the use of control theoretic techniques to model and design the feedback loops with desired dynamic performance. In contrast to heuristics-based approaches that rely on laborious hand-tuning and testing iteration, the control theoretic approach enables systematic design of an adaptive Web server with established analytical methods. The adaptive architecture has been implemented by modifying an Apache server. Experimental results demonstrate that the adaptive server provides robust delay guarantees even when workload varies significantly",
A budget constrained scheduling of workflow applications on utility Grids using genetic algorithms,"Over the last few years, grid technologies have progressed towards a service-oriented paradigm that enables a new way of service provisioning based on utility computing models. Users consume these services based on their QoS (quality of service) requirements. In such ldquopay-per-userdquo grids, workflow execution cost must be considered during scheduling based on users' QoS constraints. In this paper, we propose a budget constraint based scheduling, which minimizes execution time while meeting a specified budget for delivering results. A new type of genetic algorithm is developed to solve the scheduling optimization problem and we test the scheduling algorithm in a simulated grid testbed.","Genetic algorithms,
Silicon,
Photovoltaic effects,
Optical interconnections,
Microactuators,
Electrostatics,
Electrodes,
Mirrors,
Photovoltaic systems,
Solar power generation"
Anytime path planning and replanning in dynamic environments,"We present an efficient, anytime method for path planning in dynamic environments. Current approaches to planning in such domains either assume that the environment is static and replan when changes are observed, or assume that the dynamics of the environment are perfectly known a priori. Our approach takes into account all prior information about both the static and dynamic elements of the environment, and efficiently updates the solution when changes to either are observed. As a result, it is well suited to robotic path planning in known or unknown environments in which there are mobile objects, agents or adversaries",
Self-Adaptive Differential Evolution Algorithm in Constrained Real-Parameter Optimization,"Differential Evolution (DE) has been shown to be a powerful evolutionary algorithm for global optimization in many real problems. Self-adaptation has been found to be high beneficial for adjusting control parameters during evolutionary process, especially when done without any user interaction. In this paper we investigate a self-adaptive differential evolution algorithm where more DE strategies are used and control parameters F and CR are self-adapted. The performance of the self-adaptive differential evolution algorithm is evaluated on the set of 24 benchmark functions provided for the CEC2006 special session on constrained real parameter optimization.",
Superposition coding for side-information channels,"We present simple, practical codes designed for the binary and Gaussian dirty-paper channels. We show that the dirty-paper decoding problem can be transformed into an equivalent multiple-access decoding problem, for which we apply superposition coding. Our concept is a generalization of the nested lattices approach of Zamir, Shamai, and Erez. In a theoretical setting, our constructions are capable of achieving capacity using random component codes and maximum-likelihood decoding. We also present practical implementations of the constructions, and simulation results for both dirty-paper channels. Our results for the Gaussian dirty-paper channel are on par with the best known results for nested lattices. We discuss the binary dirty- tape channel, for which we present a simple, effective coding technique. Finally, we propose a framework for extending our approach to general Gel'fand-Pinsker channels.",
Local Histograms for Design of Transfer Functions in Direct Volume Rendering,"Direct volume rendering (DVR) is of increasing diagnostic value in the analysis of data sets captured using the latest medical imaging modalities. The deployment of DVR in everyday clinical work, however, has so far been limited. One contributing factor is that current transfer function (TF) models can encode only a small fraction of the user's domain knowledge. In this paper, we use histograms of local neighborhoods to capture tissue characteristics. This allows domain knowledge on spatial relations in the data set to be integrated into the TF. As a first example, we introduce partial range histograms in an automatic tissue detection scheme and present its effectiveness in a clinical evaluation. We then use local histogram analysis to perform a classification where the tissue-type certainty is treated as a second TF dimension. The result is an enhanced rendering where tissues with overlapping intensity ranges can be discerned without requiring the user to explicitly define a complex, multidimensional TF","Histograms,
Transfer functions,
Biomedical imaging,
Data visualization,
Rendering (computer graphics),
Shape,
Data analysis,
Image analysis,
Performance analysis,
Multidimensional systems"
Software Design Patterns for Information Visualization,"Despite a diversity of software architectures supporting information visualization, it is often difficult to identify, evaluate, and re-apply the design solutions implemented within such frameworks. One popular and effective approach for addressing such difficulties is to capture successful solutions in design patterns, abstract descriptions of interacting software components that can be customized to solve design problems within a particular context. Based upon a review of existing frameworks and our own experiences building visualization software, we present a series of design patterns for the domain of information visualization. We discuss the structure, context of use, and interrelations of patterns spanning data representation, graphics, and interaction. By representing design knowledge in a reusable form, these patterns can be used to facilitate software design, implementation, and evaluation, and improve developer education and communication",
On the Energy Hole Problem of Nonuniform Node Distribution in Wireless Sensor Networks,"In this paper, we investigate the theoretical aspects of the nonuniform node distribution strategy in wireless sensor networks, which aims to avoid the energy hole around the sink. We find that in a circular sensor network with a nonuniform node distribution and constant data reporting, the unbalanced energy depletion among the nodes in the whole network is unavoidable. This is because although all the inner nodes have used up their energy simultaneously, the outmost part of the network may still have energy left. In spite of this fact, a suboptimal energy efficiency among the inner parts of the network is possible if the number of nodes increases with geometric proportion from the outer parts to the inner ones. In our proposed nonuniform node distribution strategy, the ratio between the node densities of the adjacent (i+1)th corona and the ith corona is equal to (2i-1)/q(2i+1), where q is the geometric proportion mentioned above. We also present a routing algorithm with this node distribution strategy. Simulation experiments demonstrate that when the network lifetime has ended, the nodes in the inner parts of the network achieve nearly balanced energy depletion, and only less than 10% of the total energy is wasted","Wireless sensor networks,
Distribution strategy,
Routing,
Corona,
Sensor phenomena and characterization,
Telecommunication traffic,
Energy efficiency,
Temperature sensors,
Temperature measurement,
Monitoring"
Low-cost Virtual Rehabilitation of the Hand for Patients Post-Stroke,"We are witnessing the convergence of game technology (both software and hardware) with rehabilitation science to form a second-generation virtual rehabilitation framework. This is fortunate in view of the need to reduce system costs and thus facilitate adoption in clinical practice. This paper presents an Xbox-based physical rehabilitation system currently under development at Rutgers University. Unlike its high-end precursor aimed at hand training for patients post-stroke, the experimental system described here uses an inexpensive P5 game glove and Java 3D simulations. This results in significant cost savings, albeit with some tradeoff in functionality","Games,
Costs,
Java,
Virtual reality,
Medical treatment,
Computational modeling,
Computer simulation,
Wrist,
Pediatrics,
Brain modeling"
Image Completion Using Global Optimization,"A new exemplar-based framework unifying image completion, texture synthesis and image inpainting is presented in this work. Contrary to existing greedy techniques, these tasks are posed in the form of a discrete global optimization problem with a well defined objective function. For solving this problem a novel optimization scheme, called Priority- BP, is proposed which carries two very important extensions over standard belief propagation (BP): ""prioritybased message scheduling"" and ""dynamic label pruning"". These two extensions work in cooperation to deal with the intolerable computational cost of BP caused by the huge number of existing labels. Moreover, both extensions are generic and can therefore be applied to any MRF energy function as well. The effectiveness of our method is demonstrated on a wide variety of image completion examples.",
Path planning for deformable linear objects,"We present a new approach to path planning for deformable linear (one-dimensional) objects such as flexible wires. We introduce a method for efficiently computing stable configurations of a wire subject to manipulation constraints. These configurations correspond to minimal-energy curves. By restricting the planner to minimal-energy curves, the execution of a path becomes easier. Our curve representation is adaptive in the sense that the number of parameters automatically varies with the complexity of the underlying curve. We introduce a planner that computes paths from one minimal-energy curve to another such that all intermediate curves are also minimal-energy curves. This planner can be used as a powerful local planner in a sampling-based roadmap method. This makes it possible to compute a roadmap of the entire ""shape space,"" which is not possible with previous approaches. Using a simplified model for obstacles, we can find minimal-energy curves of fixed length that pass through specified tangents at given control points. Our work has applications in cable routing, and motion planning for surgical suturing and snake-like robots","Path planning,
Wire,
Computational modeling,
Motion planning,
Robots,
Shape,
Routing,
Surgery,
Deformable models,
Computer science"
Animals on the Web,"We demonstrate a method for identifying images containing categories of animals. The images we classify depict animals in a wide range of aspects, configurations and appearances. In addition, the images typically portray multiple species that differ in appearance (e.g. ukari’s, vervet monkeys, spider monkeys, rhesus monkeys, etc.). Our method is accurate despite this variation and relies on four simple cues: text, color, shape and texture. Visual cues are evaluated by a voting method that compares local image phenomena with a number of visual exemplars for the category. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required involves identifying which clusters of exemplars refer to which sense of a term (for example, ""monkey"" can refer to an animal or a bandmember). Because our method is applied to web pages with free text, the word cue is extremely noisy. We show unequivocal evidence that visual information improves performance for our task. Our method allows us to produce large, accurate and challenging visual datasets mostly automatically.",
LION: Layered Overlay Multicast With Network Coding,"Recent advances in information theory show that the throughput of a multicast session can be improved using network coding. In overlay networks, the available bandwidth between sender and different receivers are different. In this paper, we propose a solution to improve the throughput of an overlay multicast session with heterogeneous receivers by organizing the receivers into layered data distribution meshes and sending substreams to each mesh using layered coding. Our solutions utilize alternative paths and network coding in each mesh. We first formulate the problem into a mathematical programming, whose optimal solution requires global information. We therefore present a distributed heuristic algorithm. The heuristic progressively organizes the receivers into layered meshes. Each receiver can subscribe to a proper number of meshes to maximize its throughput by fully utilizing its available bandwidth. The benefits of organizing the topology into layered mesh and using network coding are demonstrated through extensive simulations. Numerical results indicate that the average throughput of a multicast session is significantly improved (up to 50% to 60%) with only slightly higher delay and network resource consumption","Network coding,
Throughput,
Bandwidth,
Organizing,
Computer science,
Information theory,
Mathematical programming,
Heuristic algorithms,
Network topology,
Peer to peer computing"
Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications,"In the past few years, all manner of storage applications, ranging from disk array systems to distributed and wide-area systems, have started to grapple with the reality of tolerating multiple simultaneous failures of storage nodes. Unlike the single failure case, which is optimally handled with RAID level-5 parity, the multiple failure case is more difficult because optimal general purpose strategies are not yet known. Erasure coding is the field of research that deals with these strategies, and this field has blossomed in recent years. Despite this research, the decades-old Reed-Solomon erasure code remains the only space-optimal (MDS) code for all but the smallest storage systems. The best performing implementations of Reed-Solomon coding employ a variant called Cauchy Reed-Solomon coding, developed in the mid 1990's. In this paper, we present an improvement to Cauchy Reed-Solomon coding that is based on optimizing the Cauchy distribution matrix. We detail an algorithm for generating good matrices and then evaluate the performance of encoding using all implementations Reed-Solomon codes, plus the best MDS codes from the literature. The improvements over the original Cauchy Reed-Solomon codes are as much as 83% in realistic scenarios, and average roughly 10% over all cases that we tested","Reed-Solomon codes,
Fault tolerance,
Peer to peer computing,
Fault tolerant systems,
Application software,
Encoding,
Computer science,
Decoding,
Testing,
Data structures"
Modeling the amplitude statistics of ultrasonic images,"In this paper, a new statistical model for representing the amplitude statistics of ultrasonic images is presented. The model is called the Rician inverse Gaussian (RiIG) distribution, due to the fact that it is constructed as a mixture of the Rice distribution and the Inverse Gaussian distribution. The probability density function (pdf) of the RiIG model is given in closed form as a function of three parameters. Some theoretical background on this new model is discussed, and an iterative algorithm for estimating its parameters from data is given. Then, the appropriateness of the RiIG distribution as a model for the amplitude statistics of medical ultrasound images is experimentally studied. It is shown that the new distribution can fit to the various shapes of local histograms of linearly scaled ultrasound data better than existing models. A log-likelihood cross-validation comparison of the predictive performance of the RiIG, the K, and the generalized Nakagami models turns out in favor of the new model. Furthermore, a maximum a posteriori (MAP) filter is developed based on the RiIG distribution. Experimental studies show that the RiIG MAP filter has excellent filtering performance in the sense that it smooths homogeneous regions, and at the same time preserves details.",
Large margin hidden Markov models for speech recognition,"In this paper, motivated by large margin classifiers in machine learning, we propose a novel method to estimate continuous-density hidden Markov model (CDHMM) for speech recognition according to the principle of maximizing the minimum multiclass separation margin. The approach is named large margin HMM. First, we show this type of large margin HMM estimation problem can be formulated as a constrained minimax optimization problem. Second, we propose to solve this constrained minimax optimization problem by using a penalized gradient descent algorithm, where the original objective function, i.e., minimum margin, is approximated by a differentiable function and the constraints are cast as penalty terms in the objective function. The new training method is evaluated in the speaker-independent isolated E-set recognition and the TIDIGITS connected digit string recognition tasks. Experimental results clearly show that the large margin HMMs consistently outperform the conventional HMM training methods. It has been consistently observed that the large margin training method yields significant recognition error rate reduction even on top of some popular discriminative training methods","Hidden Markov models,
Speech recognition,
Automatic speech recognition,
Training data,
Minimax techniques,
Maximum likelihood estimation,
Computer science,
Mutual information,
Optimization methods,
Constraint optimization"
A combined timing and frequency synchronization and channel estimation for OFDM,"This letter addresses training-signal-based combined timing and frequency synchronization and channel estimation for orthogonal frequency-division multiplexing systems. The proposed scheme consists of two stages. At the first stage, coarse timing and frequency-offset estimates are obtained. Based on these estimates, a (coarse) channel response estimate is obtained. The timing and frequency-offset estimates at the second stage are obtained by maximum-likelihood (ML) realization based on a sliding observation vector. Then ML channel estimation is performed. A means of complexity reduction by an adaptive scheme is also presented. The simulation results show that the proposed combined approach performs quite well, and circumvents the problem of mismatch among individual synchronization tasks.",
A hierarchical characterization of a live streaming media workload,"We present a thorough characterization of what we believe to be the first significant live Internet streaming media workload in the scientific literature. Our characterization of over 3.5 million requests spanning a 28-day period is done at three increasingly granular levels, corresponding to clients, sessions, and transfers. Our findings support two important conclusions. First, we show that the nature of interactions between users and objects is fundamentally different for live versus stored objects. Access to stored objects is user driven, whereas access to live objects is object driven. This reversal of active/passive roles of users and objects leads to interesting dualities. For instance, our analysis underscores a Zipf-like profile for user interest in a given object, which is in contrast to the classic Zipf-like popularity of objects for a given user. Also, our analysis reveals that transfer lengths are highly variable and that this variability is due to client stickiness to a particular live object, as opposed to structural (size) properties of objects. Second, by contrasting two live streaming workloads from two radically different applications, we conjecture that some characteristics of live media access workloads are likely to be highly dependent on the nature of the live content being accessed. This dependence is clear from the strong temporal correlation observed in the traces, which we attribute to the impact of synchronous access to live content. Based on our analysis, we present a model for live media workload generation that incorporates many of our findings, and which we implement in GISMO.",
Displacement and velocity of the coronary arteries: cardiac and respiratory motion,"This paper presents measurements of three-dimensional (3-D) displacements and velocities of the coronary arteries due to the myocardial beating motion and due to breathing. Data were acquired by reconstructing the coronary arteries and their motion from biplane angiograms in 10 patients. A parametric motion model was used to separate the cardiac and breathing motion fields. The arteries move consistently toward the left, inferior, and anterior during a cardiac contraction. The displacement and velocity of the right coronary artery during a cardiac contraction was larger than measured for the left coronary tree. Cardiac motion dominates the respiratory motion of the coronary arteries during spontaneous breathing. On inspiration, the arteries move caudally, but the motion in the left-right and anterior-posterior axes was variable. Spatial variation in respiratory displacement and velocity of the coronary arteries indicates that the breathing motion of the heart is more complex than a 3-D translation.",
An Introduction to Algebraic Multigrid Computing,"Algebraic multigrid (AMG) solves linear systems based on multigrid principles, but in a way that depends only on the coefficients in the underlying matrix","Interpolation,
Equations,
Linear systems,
Smoothing methods,
Multigrid methods,
Error correction,
Parallel machines,
Concurrent computing,
Distributed computing,
Large-scale systems"
Particle Video: Long-Range Motion Estimation using Point Trajectories,"This paper describes a new approach to motion estimation in video. We represent video motion using a set of particles. Each particle is an image point sample with a longduration trajectory and other properties. To optimize these particles, we measure point-based matching along the particle trajectories and distortion between the particles. The resulting motion representation is useful for a variety of applications and cannot be directly obtained using existing methods such as optical flow or feature tracking. We demonstrate the algorithm on challenging real-world videos that include complex scene geometry, multiple types of occlusion, regions with low texture, and non-rigid deformations.",
Bitmap Algorithms for Counting Active Flows on High-Speed Links,"This paper presents a family of bitmap algorithms that address the problem of counting the number of distinct header patterns (flows) seen on a high-speed link. Such counting can be used to detect DoS attacks and port scans and to solve measurement problems. Counting is especially hard when processing must be done within a packet arrival time (8 ns at OC-768 speeds) and, hence, may perform only a small number of accesses to limited, fast memory. A naive solution that maintains a hash table requires several megabytes because the number of flows can be above a million. By contrast, our new probabilistic algorithms use little memory and are fast. The reduction in memory is particularly important for applications that run multiple concurrent counting instances. For example, we replaced the port-scan detection component of the popular intrusion detection system Snort with one of our new algorithms. This reduced memory usage on a ten minute trace from 50 to 5.6 MB while maintaining a 99.77% probability of alarming on a scan within 6 s of when the large-memory algorithm would. The best known prior algorithm (probabilistic counting) takes four times more memory on port scan detection and eight times more on a measurement application. This is possible because our algorithms can be customized to take advantage of special features such as a large number of instances that have very small counts or prior knowledge of the likely range of the count",
Distributed Selfish Replication,"A commonly employed abstraction for studying the object placement problem for the purpose of Internet content distribution is that of a distributed replication group. In this work, the initial model of the distributed replication group of Leff et al. [CHECK END OF SENTENCE] is extended to the case that individual nodes act selfishly, i.e., cater to the optimization of their individual local utilities. Our main contribution is the derivation of equilibrium object placement strategies that 1) can guarantee improved local utilities for all nodes concurrently as compared to the corresponding local utilities under greedy local object placement, 2) do not suffer from potential mistreatment problems, inherent to centralized strategies that aim at optimizing the social utility, and 3) do not require the existence of complete information at all nodes. We develop a baseline computationally efficient algorithm for obtaining the aforementioned equilibrium strategies and then extend it to improve its performance with respect to fairness. Both algorithms are realizable, in practice, through a distributed protocol that requires only a limited exchange of information.",
Different Aspects of Social Network Analysis,"A social network is a set of people (or organizations or other social entities) connected by a set of social relationships, such as friendship, co-working or information exchange. Social network analysis focuses on the analysis of pattern of relationships among people, organizations, states and such social entities. Social network analysis provides both a visual and a mathematical analysis of human relationships. Web can also be considered as a social network. Social networks are formed between Web pages by hyperlinking to other Web pages. In this paper a state of the art survey of the works done on social network analysis ranging from pure mathematical analyses in graphs to analysing the social networks in semantic Web is given. The main goal is to provide a road map for researchers working on different aspects of social network analysis",
Adaptive Data Fusion for Energy Efficient Routing in Wireless Sensor Networks,"While in-network data fusion can reduce data redundancy and, hence, curtail network load, the fusion process itself may introduce significant energy consumption for emerging wireless sensor networks with vectorial data and/or security requirements. Therefore, fusion-driven routing protocols for sensor networks cannot optimize over communication cost only - fusion cost must also be accounted for. In our prior work, while a randomized algorithm termed MFST is devised toward this end, it assumes that fusion shall be performed at any intersection node whenever data streams encounter. In this paper, we design a novel routing algorithm, called adaptive fusion Steiner tree (AFST), for energy efficient data gathering. Not only does AFST jointly optimize over the costs for both data transmission and fusion, but also AFST evaluates the benefit and cost of data fusion along information routes and adaptively adjusts whether fusion shall be performed at a particular node. Analytically and experimentally, we show that AFST achieves better performance than existing algorithms, including SLT, SPT, and MFST",
Congestion Adaptive Routing in Mobile Ad Hoc Networks,"Mobility, channel error, and congestion are the main causes for packet loss in mobile ad hoc networks. Reducing packet loss typically involves congestion control operating on top of a mobility and failure adaptive routing protocol at the network layer. In the current designs, routing is not congestion-adaptive. Routing may let a congestion happen which is detected by congestion control, but dealing with congestion in this reactive manner results in longer delay and unnecessary packet loss and requires significant overhead if a new route is needed. This problem becomes more visible especially in large-scale transmission of heavy traffic such as multimedia data, where congestion is more probable and the negative impact of packet loss on the service quality is of more significance. We argue that routing should not only be aware of, but also be adaptive to, network congestion. Hence, we propose a routing protocol (CRP) with such properties. Our ns-2 simulation results confirm that CRP improves the packet loss rate and end-to-end delay while enjoying significantly smaller protocol overhead and higher energy efficiency as compared to AODV and DSR",
Hidden Conditional Random Fields for Gesture Recognition,"We introduce a discriminative hidden-state approach for the recognition of human gestures. Gesture sequences often have a complex underlying structure, and models that can incorporate hidden structures have proven to be advantageous for recognition tasks. Most existing approaches to gesture recognition with hidden states employ a Hidden Markov Model or suitable variant (e.g., a factored or coupled state model) to model gesture streams; a significant limitation of these models is the requirement of conditional independence of observations. In addition, hidden states in a generative model are selected to maximize the likelihood of generating all the examples of a given gesture class, which is not necessarily optimal for discriminating the gesture class against other gestures. Previous discriminative approaches to gesture sequence recognition have shown promising results, but have not incorporated hidden states nor addressed the problem of predicting the label of an entire sequence. In this paper, we derive a discriminative sequence model with a hidden state structure, and demonstrate its utility both in a detection and in a multi-way classification formulation. We evaluate our method on the task of recognizing human arm and head gestures, and compare the performance of our method to both generative hidden state and discriminative fully-observable models.","Hidden Markov models,
Pattern recognition,
Humans,
Computer vision,
State estimation,
Computer science,
Artificial intelligence,
Laboratories,
Application software,
Power generation"
Rain Removal in Video by Combining Temporal and Chromatic Properties,"Removal of rain streaks in video is a challenging problem due to the random spatial distribution and fast motion of rain. This paper presents a new rain removal algorithm that incorporates both temporal and chromatic properties of rain in video. The temporal property states that an image pixel is never always covered by rain throughout the entire video. The chromatic property states that the changes of R, G, and B values of rain-affected pixels are approximately the same. By using both properties, the algorithm can detect and remove rain streaks in both stationary and dynamic scenes taken by stationary cameras. To handle videos taken by moving cameras, the video can be stabilized for rain removal, and destabilized to restore camera motion after rain removal. It can handle both light rain and heavy rain conditions. Experimental results show that the algorithm performs better than existing algorithms",
Evaluation of Queueing Policies and Forwarding Strategies for Routing in Intermittently Connected Networks,"Delay tolerant networking (DTN), and more specifically the subset known as intermittently connected networking, is emerging as a solution for supporting asynchronous data transfers in challenging environments where a fully connected end-to-end path between a source and destination may never exist. Message delivery in such networks is enabled via scheduled or opportunistic communication based on transitive local connectivity among nodes influenced by factors such as node mobility. Given the inherently store-and-forward and opportunistic nature of the DTN architecture, the choice of buffer management policies and message forwarding strategies can have a major impact on system performance. In this paper, we propose and evaluate different combinations of queueing policies and forwarding strategies for intermittently connected networks. We show that a probabilistic routing approach along with the correct choice of buffer management policy and forwarding strategy can result in much performance improvements in terms of message delivery, overhead and end-to-end delay",
Efficient Clustering of Uncertain Data,"We study the problem of clustering data objects whose locations are uncertain. A data object is represented by an uncertainty region over which a probability density function (pdf) is defined. One method to cluster uncertain objects of this sort is to apply the UK-means algorithm, which is based on the traditional K-means algorithm. In UK-means, an object is assigned to the cluster whose representative has the smallest expected distance to the object. For arbitrary pdf, calculating the expected distance between an object and a cluster representative requires expensive integration computation. We study various pruning methods to avoid such expensive expected distance calculation.",
Unsupervised Learning of Categories from Sets of Partially Matching Image Features,"We present a method to automatically learn object categories from unlabeled images. Each image is represented by an unordered set of local features, and all sets are embedded into a space where they cluster according to their partial-match feature correspondences. After efficiently computing the pairwise affinities between the input images in this space, a spectral clustering technique is used to recover the primary groupings among the images. We introduce an efficient means of refining these groupings according to intra-cluster statistics over the subsets of features selected by the partial matches between the images, and based on an optional, variable amount of user supervision. We compute the consistent subsets of feature correspondences within a grouping to infer category feature masks. The output of the algorithm is a partition of the data into a set of learned categories, and a set of classifiers trained from these ranked partitions that can recognize the categories in novel images.",
Information Dissemination in Power-Constrained Wireless Networks,,
The Design of High-Level Features for Photo Quality Assessment,"We propose a principled method for designing high level features forphoto quality assessment. Our resulting system can classify between high quality professional photos and low quality snapshots. Instead of using the bag of low-level features approach, we first determine the perceptual factors that distinguish between professional photos and snapshots. Then, we design high level semantic features to measure the perceptual differences. We test our features on a large and diverse dataset and our system is able to achieve a classification rate of 72% on this difficult task. Since our system is able to achieve a precision of over 90% in low recall scenarios, we show excellent results in a web image search application.","Quality assessment,
Asia,
Art,
Computer science,
Design methodology,
System testing,
Application software,
Search engines,
Quality management,
Software quality"
Construction and use of linear regression models for processor performance analysis,"Processor architects have a challenging task of evaluating a large design space consisting of several interacting parameters and optimizations. In order to assist architects in making crucial design decisions, we build linear regression models that relate processor performance to micro-architectural parameters, using simulation based experiments. We obtain good approximate models using an iterative process in which Akaike's information criteria is used to extract a good linear model from a small set of simulations, and limited further simulation is guided by the model using D-optimal experimental designs. The iterative process is repeated until desired error bounds are achieved. We used this procedure to establish the relationship of the CPI performance response to 26 key micro-architectural parameters using a detailed cycle-by-cycle superscalar processor simulator. The resulting models provide a significance ordering on all micro-architectural parameters and their interactions, and explain the performance variations of micro-architectural techniques.",
DRBTS: Distributed Reputation-based Beacon Trust System,"Wireless sensor networks (WSNs) have critical applications in diverse domains like environmental monitoring and military operations where accurate location of sensors is vital. One common method of location discovery uses a set of specialty nodes known as beacon nodes (BNs) that assist other sensor nodes (SNs) to determine their location. This paper proposes a novel reputation-based scheme called distributed reputation-based beacon trust system (DRBTS) for excluding malicious BNs that provide false location information. To the best of our knowledge, DRBTS is the first model to use the concept of reputation for excluding BNs. In DRBTS, every BN monitors its 1-hop neighborhood for misbehaving BNs and accordingly updates the reputation of the corresponding BN in the neighbor-reputation-table (NRT). The BNs then publish their NRT in their 1-hop neighborhood. BNs use this second-hand information published in NRT for updating the reputation of their neighbors after it qualifies a deviation test. On the other hand, the SNs use the NRT information to determine whether or not to use a given beacon's location information, based on a simple majority voting scheme","Wireless sensor networks,
Ad hoc networks,
Voting,
Information security,
Sensor systems,
Robustness,
Distributed computing,
Iterative algorithms,
Computer science,
Application software"
Multilevel algorithms for partitioning power-law graphs,"Graph partitioning is an enabling technology for parallel processing as it allows for the effective decomposition of unstructured computations whose data dependencies correspond to a large sparse and irregular graph. Even though the problem of computing high-quality partitionings of graphs arising in scientific computations is to a large extent well-understood, this is far from being true for emerging HPC applications whose underlying computation involves graphs whose degree distribution follows a power-law curve. This paper presents new multilevel graph partitioning algorithms that are specifically designed for partitioning such graphs. It presents new clustering-based coarsening schemes that identify and collapse together groups of vertices that are highly connected. An experimental evaluation of these schemes on 10 different graphs show that the proposed algorithms consistently and significantly outperform existing state-of-the-art approaches.",
A Storytelling Robot: Modeling and Evaluation of Human-like Gaze Behavior,"Engaging storytelling is a necessary skill for humanoid robots if they are to be used in education and entertainment applications. Storytelling requires that the humanoid robot be aware of its audience and able to direct its gaze in a natural way. In this paper, we explore how human gaze can be modeled and implemented on a humanoid robot to create a natural, human-like behavior for storytelling. Our gaze model integrates data collected from a human storyteller and a discourse structure model developed by Cassell and her colleagues for human-like conversational agents (1994). We used this model to direct the gaze of a humanoid robot, Honda's ASIMO, as he recited a Japanese fairy tale using a pre-recorded human voice. We assessed the efficacy of this gaze algorithm by manipulating the frequency of ASIMO's gaze between two participants and used pre and post questionnaires to assess whether participants evaluated the robot more positively and did better on a recall task when ASIMO looked at them more. We found that participants performed significantly better in recalling ASIMO's story when the robot looked at them more. Our results also showed significant differences in how men and women evaluated ASIMO based on the frequency of gaze they received from the robot. Our study adds to the growing evidence that there are many commonalities between human-human communication and human-robot communication",
MobiEyes: A Distributed Location Monitoring Service Using Moving Location Queries,"With the growing popularity and availability of mobile communications, our ability to stay connected while on the move is becoming a reality instead of science fiction as it was just a decade ago. An important research challenge for modern location-based services is the scalable processing of location monitoring requests on a large collection of mobile objects. The centralized architecture, though studied extensively in literature, would create intolerable performance problems as the number of mobile objects grows significantly. This paper presents a distributed architecture and a suite of optimization techniques for scalable processing of continuously moving location queries. Moving location queries can be viewed as standing location tracking requests that continuously monitor the locations of mobile objects of interest and return a subset of mobile objects when certain conditions are met. We describe the design of MobiEyes, a distributed real time location monitoring system in a mobile environment. The main idea behind the MobiEyes' distributed architecture is to promote a careful partition of a real time location monitoring task into an optimal coordination of server-side processing and client-side processing. Such a partition allows evaluating moving location queries with a high degree of precision using a small number of location updates, thus providing highly scalable location monitoring services. A set of optimization techniques are used to limit the amount of computation to be handled by the mobile objects and enhance the overall performance and system utilization of MobiEyes. Important metrics to validate the proposed architecture and optimizations include messaging cost, server load, and amount of computation at individual mobile objects. We evaluate the scalability of the MobiEyes location monitoring approach using a simulation model based on a mobile setup. Our experimental results show that MobiEyes can lead to significant savings in terms of server load and messaging cost when compared to solutions relying on central processing of location information",
Reversible Steganography for VQ-Compressed Images Using Side Matching and Relocation,"The reversible steganographic method allows an original image to be completely reconstructed from the stegoimage after the extraction of the embedded data. The traditional reversible embedding schemes are not suitable for images compressed using vector quantization (VQ) and usually require the use of the location map for reversibility. In this paper, we propose a reversible embedding scheme for VQ-compressed images that is based on side matching and relocation. The new method achieves reversibility without using the location map. The experimental results show that the proposed method is practical for VQ-compressed images and provides high image quality and embedding capacity",
Continuous Medial Representation for Anatomical Structures,"The m-rep approach pioneered by Pizer (2003) is a powerful morphological tool that makes it possible to employ features derived from medial loci (skeletons) in shape analysis. This paper extends the medial representation paradigm into the continuous realm, modeling skeletons and boundaries of three-dimensional objects as continuous parametric manifolds, while also maintaining the proper geometric relationship between these manifolds. The parametric representation of the boundary-medial relationship makes it possible to fit shape-based coordinate systems to the interiors of objects, providing a framework for combined statistical analysis of shape and appearance. Our approach leverages the idea of inverse skeletonization, where the skeleton of an object is defined first and the object's boundary is derived analytically from the skeleton. This paper derives a set of sufficient conditions ensuring that inverse skeletonization is well-posed for single-manifold skeletons and formulates a partial differential equation whose solutions satisfy the sufficient conditions. An efficient variational algorithm for deformable template modeling using the continuous medial representation is described and used to fit a template to the hippocampus in 87 subjects from a schizophrenia study with sub-voxel accuracy and 95% mean overlap",
A Sampling-Based Approach to Optimizing Top-k Queries in Sensor Networks,"Wireless sensor networks generate a vast amount of data. This data, however, must be sparingly extracted to conserve energy, usually the most precious resource in battery-powered sensors. When approximation is acceptable, a model-driven approach to query processing is effective in saving energy by avoiding contacting nodes whose values can be predicted or are unlikely to be in the result set. To optimize queries such as top-k, however, reasoning directly with models of joint probability distributions can be prohibitively expensive. Instead of using models explicitly, we propose to use samples of past sensor readings. Not only are such samples simple to maintain, but they are also computationally efficient to use in query optimization. With these samples, we can formulate the problem of optimizing approximate top-k queries under an energy constraint as a linear program. We demonstrate the power and flexibility of our sampling-based approach by developing a series of topk query planning algorithms with linear programming, which are capable of efficiently producing plans with better performance and novel features. We show that our approach is both theoretically sound and practically effective on simulated and real-world datasets.",
,,
An Investigation into Mutation Operators for Particle Swarm Optimization,"The Particle Swarm Optimization (PSO) technique can be augmented with an additional mutation operator that helps prevent premature convergence on local optima. In this paper, different mutation operators for PSO are empirically investigated and compared. A review of previous mutation approaches is given and key factors concerning how mutation operators can be applied to PSO are identified. A PSO algorithm incorporating different mutation operators is applied to both mathematical and constrained optimization problems. Results shown that the addition of a mutation operator to PSO can enhance optimisation performance and insight is gained into how to design mutation operators dependent on the nature of the problem being optimized.",
CAD-guided automated nanoassembly using atomic force microscopy-based nonrobotics,"Nanoassembly using atomic force microscopy (AFM) is a promising technique for nanomanufacturing. Most AFM-based nanoassembly schemes are implemented either manually using haptic devices or in an interactive way between the users and the atomic force microscope images. These schemes are time consuming and inefficient. Therefore, the computer-aided design (CAD)-guided automated nanoassembly using AFM is desirable for nanomanufacturing. In this paper, a general framework for CAD-guided automated nanoassembly using AFM is developed. Based on the CAD model of a nanostructure, the manipulation paths for both nanoparticles and nanorods are generated automatically. A local scanning method is developed to compensate for the random drift that may cause the failure of the nanoassembly. The experimental results demonstrate that the developed general framework can be employed to manufacture nanostructures efficiently. The research work opens a door to the CAD-guided automated nanomanufacturing using AFM. Note to Practitioners-Atomic force microscope (AFM)-based nanoassembly will lead to potential breakthroughs in manufacturing new revolutionary industrial products because many potential nanostructures and nanodevices are asymmetric, which cannot be manufactured using self-assembly only. In order to increase the efficiency and accuracy of AFM-based nanoassembly, automated computer-aided-design (CAD)-guided nanoassembly is desirable to manufacture nanostructures and nanodevices. Based on the CAD model, the environment model and the model of the nanoobjects, collision-free paths are generated to control the AFM tip to manipulate nanoobjects. A local scanning method is developed to obtain the actual position of each nanoobject to compensate for the random drift. Since the building materials of nanostructures and nanodevices may include nanoparticles, nanorods, nanowires, nanotubes, etc., automated path planning algorithms are developed for both nanoparticles and nanorods. The experimental results show that the developed general framework can be used to manufacture nanostructures more efficiently.","Atomic force microscopy,
Nanoparticles,
Design automation,
Computer aided manufacturing,
Nanoscale devices,
Haptic interfaces,
Manufacturing industries,
Self-assembly,
Manufacturing automation,
Automatic generation control"
A generalized likelihood ratio test for impropriety of complex signals,"A complex random vector is called improper if it is correlated with its complex conjugate. We present a hypothesis test for impropriety based on a generalized likelihood ratio (GLR). This GLR is invariant to linear transformations on the data, including rotation and scaling, because propriety is preserved by linear transformations. More specifically, we show that the GLR is a function of the squared canonical correlations between the data and their complex conjugate. These canonical correlations make up a complete, or maximal, set of invariants for the Hermitian and complementary covariance matrices under linear, but not widely linear, transformation","Testing,
Vectors,
Covariance matrix,
Statistics,
Binary phase shift keying,
Maximum likelihood estimation,
Phase modulation,
Performance gain,
Computer science,
Australia"
Wavelet-based reconstruction for limited-angle X-ray tomography,"The aim of X-ray tomography is to reconstruct an unknown physical body from a collection of projection images. When the projection images are only available from a limited angle of view, the reconstruction problem is a severely ill-posed inverse problem. Statistical inversion allows stable solution of the limited-angle tomography problem by complementing the measurement data by a priori information. In this work, the unknown attenuation distribution inside the body is represented as a wavelet expansion, and a Besov space prior distribution together with positivity constraint is used. The wavelet expansion is thresholded before reconstruction to reduce the dimension of the computational problem. Feasibility of the method is demonstrated by numerical examples using in vitro data from mammography and dental radiology.",
Bayesian Restoration Using a New Nonstationary Edge-Preserving Image Prior,"In this paper, we propose a class of image restoration algorithms based on the Bayesian approach and a new hierarchical spatially adaptive image prior. The proposed prior has the following two desirable features. First, it models the local image discontinuities in different directions with a model which is continuous valued. Thus, it preserves edges and generalizes the on/off (binary) line process idea used in previous image priors within the context of Markov random fields (MRFs). Second, it is Gaussian in nature and provides estimates that are easy to compute. Using this new hierarchical prior, two restoration algorithms are derived. The first is based on the maximum a posteriori principle and the second on the Bayesian methodology. Numerical experiments are presented that compare the proposed algorithms among themselves and with previous stationary and non stationary MRF-based with line process algorithms. These experiments demonstrate the advantages of the proposed prior",
"Adaptive, Transparent Frequency and Voltage Scaling of Communication Phases in MPI Programs","Although users of high-performance computing are most interested in raw performance, both energy and power consumption have become critical concerns. Some microprocessors allow frequency and voltage scaling, which enables a system to reduce CPU performance and power when the CPU is not on the critical path. When properly directed, such dynamic frequency and voltage scaling can produce significant energy savings with little performance penalty. This paper presents an MPI runtime system that dynamically reduces CPU performance during communication phases in MPI programs. It dynamically identifies such phases and, without profiling or training, selects the CPU frequency in order to minimize energy-delay product. All analysis and subsequent frequency and voltage scaling is within MPI and so is entirely transparent to the application. This means that the large number of existing MPI programs, as well as new ones being developed, can use our system without modification. Results show that the average reduction in energy-delay product over the NAS benchmark suite is 10% - the average energy reduction is 12% while the average execution time increase is only 2.1%",
Relay Placement for Higher Order Connectivity in Wireless Sensor Networks,,
Public Key Based Cryptoschemes for Data Concealment in Wireless Sensor Networks,"In-network data aggregation is a popular technique for reducing the energy consumption tied to data transmission in a multi-hop wireless sensor network. However, data aggregation in untrusted or even hostile environments becomes problematic when end-to-end privacy between sensors and the sink is desired. In this paper we revisit and investigate the applicability of additively homomorphic public-key encryption algorithms for certain classes of wireless sensor networks. Finally, we provide recommendations for selecting the most suitable public key schemes for different topologies and wireless sensor network scenarios.",
Entity Resolution with Markov Logic,"Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. Interest in it has grown rapidly, and many approaches have been proposed. However, they tend to address only isolated aspects of the problem, and are often ad hoc. This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous approaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two citation databases show the utility of this approach, and evaluate the contribution of the different components.",
Privacy Protection: p-Sensitive k-Anonymity Property,"In this paper, we introduce a new privacy protection property called p-sensitive k-anonymity. The existing kanonymity property protects against identity disclosure, but it fails to protect against attribute disclosure. The new introduced privacy model avoids this shortcoming. Two necessary conditions to achieve p-sensitive kanonymity property are presented, and used in developing algorithms to create masked microdata with p-sensitive k-anonymity property using generalization and suppression.","Privacy,
Protection,
Medical services,
Databases,
Data mining,
Computer science,
Biomedical imaging,
Medical diagnostic imaging,
History,
Statistical analysis"
Fast Byzantine Consensus,"We present the first protocol that reaches asynchronous Byzantine consensus in two communication steps in the common case. We prove that our protocol is optimal in terms of both number of communication steps and number of processes for two-step consensus. The protocol can be used to build a replicated state machine that requires only three communication steps per request in the common case. Further, we show a parameterized version of the protocol that is safe despite f Byzantine failures and, in the common case, guarantees two-step execution despite some number t of failures (t les f). We show that this parameterized two-step consensus protocol is also optimal in terms of both number of communication steps and number of processes",
LITMUS^RT : A Testbed for Empirically Comparing Real-Time Multiprocessor Schedulers,"We present a real-time, Linux-based testbed called LITMUS, which we have developed for empirically evaluating multiprocessor real-time scheduling algorithms. We also present the results from such an evaluation, in which partitioned earliest-deadline-first (EDF) scheduling, preemptive and nonpreemptive global EDF scheduling, and two variants of the global PD2 Pfair algorithm were considered. The tested algorithms were compared based on both raw performance and schedulability (with real overheads considered) assuming either hard- or soft-real-time constraints. To our knowledge, this paper is the first attempt by anyone to compare partitioned and global real-time scheduling approaches using empirical data",
Learning-based deformable registration of MR brain images,"This paper presents a learning-based method for deformable registration of magnetic resonance (MR) brain images. There are two novelties in the proposed registration method. First, a set of best-scale geometric features are selected for each point in the brain, in order to facilitate correspondence detection during the registration procedure. This is achieved by optimizing an energy function that requires each point to have its best-scale geometric features consistent over the corresponding points in the training samples, and at the same time distinctive from those of nearby points in the neighborhood. Second, the active points used to drive the brain registration are hierarchically selected during the registration procedure, based on their saliency and consistency measures. That is, the image points with salient and consistent features (across different individuals) are considered for the initial registration of two images, while other less salient and consistent points join the registration procedure later. By incorporating these two novel strategies into the framework of the HAMMER registration algorithm, the registration accuracy has been improved according to the results on simulated brain data, and also visible improvement is observed particularly in the cortical regions of real brain data",
Recovering Facial Shape Using a Statistical Model of Surface Normal Direction,"In this paper, we show how a statistical model of facial shape can be embedded within a shape-from-shading algorithm. We describe how facial shape can be captured using a statistical model of variations in surface normal direction. To construct this model, we make use of the azimuthal equidistant projection to map the distribution of surface normals from the polar representation on a unit sphere to Cartesian points on a local tangent plane. The distribution of surface normal directions is captured using the covariance matrix for the projected point positions. The eigenvectors of the covariance matrix define the modes of shape-variation in the fields of transformed surface normals. We show how this model can be trained using surface normal data acquired from range images and how to fit the model to intensity images of faces using constraints on the surface normal direction provided by Lambert's law. We demonstrate that the combination of a global statistical constraint and local irradiance constraint yields an efficient and accurate approach to facial shape recovery and is capable of recovering fine local surface details. We assess the accuracy of the technique on a variety of images with ground truth and real-world images",
Towards Urban 3D Reconstruction from Video,"The paper introduces a data collection system and a processing pipeline for automatic geo-registered 3D reconstruction of urban scenes from video. The system collects multiple video streams, as well as GPS and INS measurements in order to place the reconstructed models in geo- registered coordinates. Besides high quality in terms of both geometry and appearance, we aim at real-time performance. Even though our processing pipeline is currently far from being real-time, we select techniques and we design processing modules that can achieve fast performance on multiple CPUs and GPUs aiming at real-time performance in the near future. We present the main considerations in designing the system and the steps of the processing pipeline. We show results on real video sequences captured by our system.",
COLA: A Coverage and Latency Aware Actor Placement for Wireless Sensor and Actor Networks,"In addition to the sensors, wireless sensor and actor networks (WSANs) employ significantly more capable actor nodes that can perform application specific actions. In these setups responsiveness to serious events is of utmost importance and thus requires minimal latency in both data gathering and action completion. In addition, since these actions are often taken at or close to where events are detected, which can be any spot within the monitored area, the actors should strive to provide maximal coverage of the area. In this paper, we propose COLA, an actor placement mechanism that considers both the delay requirements of data collection and the coverage. COLA first evenly distributes the actors in the region for maximized coverage. Actors then collaboratively partition the sensors, forming clusters. Each individual actor then repositions itself at a location that enables minimal latency in collecting data. The effectiveness of COLA is evaluated by extensive simulations.",
Feature Reduction via Generalized Uncorrelated Linear Discriminant Analysis,"High-dimensional data appear in many applications of data mining, machine learning, and bioinformatics. Feature reduction is commonly applied as a preprocessing step to overcome the curse of dimensionality. Uncorrelated linear discriminant analysis (ULDA) was recently proposed for feature reduction. The extracted features via ULDA were shown to be statistically Uncorrelated, which is desirable for many applications. In this paper, an algorithm called ULDA/QR is proposed to simplify the previous implementation of ULDA. Then, the ULDA/GSVD algorithm is proposed, based on a novel optimization criterion, to address the singularity problem which occurs in undersampled problems, where the data dimension is larger than the sample size. The criterion used is the regularized version of the one in ULDA/QR. Surprisingly, our theoretical result shows that the solution to ULDA/GSVD is independent of the value of the regularization parameter. Experimental results on various types of data sets are reported to show the effectiveness of the proposed algorithm and to compare it with other commonly used feature reduction algorithms",
On selfish routing in Internet-like environments,"A recent trend in routing research is to avoid inefficiencies in network-level routing by allowing hosts to either choose routes themselves (e.g., source routing) or use overlay routing networks (e.g., Detour or RON). Such approaches result in selfish routing, because routing decisions are no longer based on system-wide criteria but are instead designed to optimize host-based or overlay-based metrics. A series of theoretical results showing that selfish routing can result in suboptimal system behavior have cast doubts on this approach. In this paper, we use a game-theoretic approach to investigate the performance of selfish routing in Internet-like environments based on realistic topologies and traffic demands in our simulations. We show that in contrast to theoretical worst cases, selfish routing achieves close to optimal average latency in such environments. However, such performance benefits come at the expense of significantly increased congestion on certain links. Moreover, the adaptive nature of selfish overlays can significantly reduce the effectiveness of traffic engineering by making network traffic less predictable",
Modeling Heterogeneous User Churn and Local Resilience of Unstructured P2P Networks,"Previous analytical results on the resilience of unstructured P2P systems have not explicitly modeled heterogeneity of user churn (i.e., difference in online behavior) or the impact of in-degree on system resilience. To overcome these limitations, we introduce a generic model of heterogeneous user churn, derive the distribution of the various metrics observed in prior experimental studies (e.g., lifetime distribution of joining users, joint distribution of session time of alive peers, and residual lifetime of a randomly selected user), derive several closed-form results on the transient behavior of in-degree, and eventually obtain the joint in/out degree isolation probability as a simple extension of the out-degree model.",
Improving Lookup Performance Over a Widely-Deployed DHT,,
Extended Dominating Set and Its Applications in Ad Hoc Networks Using Cooperative Communication,"We propose a notion of an extended dominating set where each node in an ad hoc network is covered by either a dominating neighbor or several 2-hop dominating neighbors. This work is motivated by cooperative communication in ad hoc networks whereby transmitting independent copies of a packet generates diversity and combats the effects of fading. We first show the NP-completeness of the minimum extended dominating set problem. Then, several heuristic algorithms, global and local, for constructing a small extended dominating set are proposed. These are nontrivial extensions of the existing algorithms for the regular dominating set problem. The application of the extended dominating set in efficient broadcasting is also discussed. The performance analysis includes an analytical study in terms of approximation ratio and a simulation study of the average size of the extended dominating set derived from the proposed algorithms",
Combining Probabilistic Ranking and Latent Semantic Indexing for Feature Identification,"The paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The main contribution consists in the combination of two existing techniques for feature location in source code. Both techniques provide a set of ranked facts from the software, as result to the feature identification problem. One of the techniques is based on a scenario based probabilistic ranking of events observed while executing a program under given scenarios. The other technique is defined as an information retrieval task, based on the latent semantic indexing of the source code. We show the viability and effectiveness of the combined technique with two case studies. A first case study is a replication of feature identification in Mozilla, which allows us to directly compare the results with previously published data. The other case study is a bug location problem in Mozilla. The results show that the combined technique improves feature identification significantly with respect to each technique used independently",
"Comments on ""Dynamical Optimal Training for Interval Type-2 Fuzzy Neural Network (T2FNN)","In this comment, it will be shown that the backpropagation (BP) equations by Wang are not correct. These BP equations were used to tune the parameters of the antecedent type-2 membership functions as well as the consequent part of the interval type-2 fuzzy neural networks (T2FNNs). These incorrect equations would have led to erroneous results, and hence this might affect the comparisons and findings presented by Wang This comment will highlight the correct BP tuning equations for the T2FNN","Fuzzy neural networks,
Equations,
Fuzzy systems,
Fuzzy logic,
Heuristic algorithms,
Backpropagation algorithms,
Fuzzy sets,
Uncertainty,
Genetic algorithms,
Computer science"
Transmission and Reception by Ultra-Wideband (UWB) Antennas,"Broadband antennas are very useful in many applications because they operate over a wide range of frequencies. The objective of this paper is to study the transient responses of various well-known antennas over broad frequency ranges. As such, the phase responses of these antennas as a function of frequency are of great interest. In the ensuing analysis, each antenna is excited by a monocycle pulse. Many antennas show resonant properties, and numerous reflections exist in the antennas' outputs. The first part of this paper deals with ways of converting various resonating antennas to traveling-wave antennas by using resistive loading. Appropriate loading increases the bandwidth of operation of the antennas. However, the drawback is the additional loss in the load applied to the antenna structure, leading to a loss of efficiency to around fifty percent. However, some of the antennas are inherently broadband, up to a 100:1 bandwidth. Hence, the transient responses of these antennas can be used to determine their suitability for wideband applications with a low cutoff frequency. The second part of the paper illustrates the radiation and reception properties of various conventional ultra-wideband (UWB) antennas in the time domain. An antenna's transient response can be used to determine the suitability of the antenna in wideband applications",
Hybrid segmentation of colon filled with air and opacified fluid for CT colonography,"Reliable segmentation of the colon is a requirement for three-dimensional visualization programs and automatic detection of polyps on computed tomography (CT) colonography. There is an evolving clinical consensus that giving patients positive oral contrast to tag out remnants of stool and residual fluids is mandatory. The presence of positive oral contrast in the colon adds an additional challenge for colonic segmentation but ultimately is beneficial to the patient because the enhanced fluid helps reveal polyps in otherwise hidden areas. Therefore, we developed a new segmentation procedure which can handle both air- and fluid-filled parts of the colon. The procedure organizes individual air- and fluid-filled regions into a graph that enables identification and removal of undesired leakage outside the colon. In addition, the procedure provides a risk assessment of possible leakage to assist the user prior to the tedious task of visual verification. The proposed hybrid algorithm uses modified region growing, fuzzy connectedness and level set segmentation. We tested our algorithm on 160 CT colonography scans containing 183 known polyps. All 183 polyps were in segmented regions. In addition, visual inspection of 24 CT colonography scans demonstrated good performance of our procedure: the reconstructed colonic wall appeared smooth even at the interface between air and fluid and there were no leaked regions.",
Local noise weighted filtering for emphysema scoring of low-dose CT images,"Computed tomography (CT) has become the new reference standard for quantification of emphysema. The most popular measure of emphysema derived from CT is the pixel index (PI), which expresses the fraction of the lung volume with abnormally low intensity values. As PI is calculated from a single, fixed threshold on intensity, this measure is strongly influenced by noise. This effect shows up clearly when comparing the PI score of a high-dose scan to the PI score of a low-dose (i.e., noisy) scan of the same subject. In this paper, the noise variance (NOVA) filter is presented: a general framework for (iterative) nonlinear filtering, which uses an estimate of the spatially dependent noise variance in an image. The NOVA filter iteratively estimates the local image noise and filters the image. For the specific purpose of emphysema quantification of low-dose CT images, a dedicated, noniterative NOVA filter is constructed by using prior knowledge of the data to obtain a good estimate of the spatially dependent noise in an image. The performance of the NOVA filter is assessed by comparing characteristics of pairs of high-dose and low-dose scans. The compared characteristics are the PI scores for different thresholds and the size distributions of emphysema bullae. After filtering, the PI scores of high-dose and low-dose images agree to within 2%-3%points. The reproducibility of the high-dose bullae size distribution is also strongly improved. NOVA filtering of a CT image of typically 400/spl times/512/spl times/512 voxels takes only a couple of minutes which makes it suitable for routine use in clinical practice.",
Video scene segmentation using Markov chain Monte Carlo,"Videos are composed of many shots that are caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to the same subject, which could be a particular physical setting, an ongoing action or a theme. In this paper, we present a general framework for temporal scene segmentation in various video domains. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo (MCMC) technique to determine the boundaries between video scenes. In this approach, a set of arbitrary scene boundaries are initialized at random locations and are automatically updated using two types of updates: diffusion and jumps. Diffusion is the process of updating the boundaries between adjacent scenes. Jumps consist of two reversible operations: the merging of two scenes and the splitting of an existing scene. The posterior probability of the target distribution of the number of scenes and their corresponding boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the MCMC process, and the samples are collected to generate the final scene boundaries. The major advantage of the proposed framework is two-fold: 1) it is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold; 2) it can be applied to different video domains. We have tested the proposed method on two video domains: home videos and feature films, and accurate results have been obtained",
Self-calibrating 3D-ultrasound-based bone registration for minimally invasive orthopedic surgery,"Intraoperative freehand three-dimensional (3-D) ultrasound (3D-US) has been proposed as a noninvasive method for registering bones to a preoperative computed tomography image or computer-generated bone model during computer-aided orthopedic surgery (CAOS). In this technique, an US probe is tracked by a 3-D position sensor and acts as a percutaneous device for localizing the bone surface. However, variations in the acoustic properties of soft tissue, such as the average speed of sound, can introduce significant errors in the bone depth estimated from US images, which limits registration accuracy. We describe a new self-calibrating approach to US-based bone registration that addresses this problem, and demonstrate its application within a standard registration scheme. Using realistic US image data acquired from 6 femurs and 3 pelves of intact human cadavers, and accurate Gold Standard registration transformations calculated using bone-implanted fiducial markers, we show that self-calibrating registration is significantly more accurate than a standard method, yielding an average root mean squared target registration error of 1.6 mm. We conclude that self-calibrating registration results in significant improvements in registration accuracy for CAOS applications over conventional approaches where calibration parameters of the 3D-US system remain fixed to values determined using a preoperative phantom-based calibration.","Bones,
Minimally invasive surgery,
Orthopedic surgery,
Calibration,
Ultrasonic imaging,
Computed tomography,
Probes,
Acoustic sensors,
Acoustic devices,
Biological tissues"
Security and Privacy Issues with Health Care Information Technology,"The face of health care is changing as new technologies are being incorporated into the existing infrastructure. Electronic patient records and sensor networks for in-home patient monitoring are at the current forefront of new technologies. Paper-based patient records are being put in electronic format enabling patients to access their records via the Internet. Remote patient monitoring is becoming more feasible as specialized sensors can be placed inside homes. The combination of these technologies will improve the quality of health care by making it more personalized and reducing costs and medical errors. While there are benefits to technologies, associated privacy and security issues need to be analyzed to make these systems socially acceptable. In this paper we explore the privacy and security implications of these next-generation health care technologies. We describe existing methods for handling issues as well as discussing which issues need further consideration",
The Monte Carlo method in science and engineering,"Since 1953, researchers have applied the Monte Carlo method to a wide range of areas. Specialized algorithms have also been developed to extend the method's applicability and efficiency. The author describes some of the algorithms that have been developed to perform Monte Carlo simulations in science and engineering",
Peer-Assisted File Distribution: The Minimum Distribution Time,"With the emergence of BitTorrent, Swarm-cast, and CDNs, peer-assisted file distribution has become a prominent Internet application, both in terms of user popularity and traffic volumes. We consider the following fundamental problem for peer-assisted file distribution. There are seed nodes, each of which has a copy of the file, and leecher nodes, each of which wants a copy the file. The goal is to distribute the file to all the leechers - with the assistance of the upload capacity of the leechers - in order to minimize the time to get the file to all the leechers (the distribution time). We obtain explicit expressions for the minimum distribution time of a general heterogeneous peer-assisted file distribution system. Derived with fluid-flow arguments, the expressions are in terms of the file size, the seeds' upload rates and the leechers' upload and download rates. We demonstrate the utility of the result by comparing the optimal distribution time with the measured distribution time when BitTorrent is used to distribute a file from a seed to ten leechers",
A Fast Algorithm for Vision-Based Hand Gesture Recognition for Robot Control,"We propose a fast algorithm for automatically recognizing a limited set of gestures from hand images for a robot control application. Hand gesture recognition is a challenging problem in its general form. We consider a fixed set of manual commands and a reasonably structured environment, and develop a simple, yet effective, procedure for gesture recognition. Our approach contains steps for segmenting the hand region, locating the fingers, and finally classifying the gesture. The algorithm is invariant to translation, rotation, and scale of the hand. We demonstrate the effectiveness of the technique on real imagery",
The Spatially Distributed Dynamic Transmembrane Voltage of Cells and Organelles due to 10 ns Pulses: Meshed Transport Networks,"The authors describe two versions of a two-dimensional (2-D) cell model that contains three circular membranes representing the plasma membrane (PM) and single bilayer approximations to both the nuclear envelope and the mitochondrial membrane. The first version uses a Cartesian transport network, which respects topology but approximates geometry. The second version uses a meshed transport network, which respects both. The asymptotic electroporation model is assigned to all local membrane sites in order to assess the electrical response of the membranes. The predictions of these two models are presented for 10-ns trapezoidal pulses with 1.5 ns rise and fall times. The applied field magnitudes range from 1 to 100 kV/cm, corresponding to recent experiments. The spatially distributed electroporation models exhibit a supraelectroporation for the larger pulses with a maximum transmembrane voltage of Um~1.5 V for both the PM and organelle membranes. For the larger fields, the PM and organelle membranes electroporate essentially simultaneously. The meshed version of the transport network eliminates numerical artifacts and is more computationally efficient",
MODEST: A Compositional Modeling Formalism for Hard and Softly Timed Systems,"This paper presents MODEST (modeling and description language for stochastic timed systems), a formalism that is intended to support 1) the modular description of reactive systems' behavior while covering both 2) functional and 3) nonfunctional system aspects such as timing and quality-of-service constraints in a single specification. The language contains, features such as simple and structured data types, structuring mechanisms like parallel composition and abstraction, means to control the granularity of assignments, exception handling, and nondeterministic and random branching and timing. MODEST can be viewed as an overarching notation for a wide spectrum of models, ranging from labeled transition systems to timed automata (and probabilistic variants thereof), as well as prominent stochastic processes such as (generalized semi-) Markov chains and decision processes. The paper describes the design rationales and details of the syntax and semantics",
Multi-Target Tracking - Linking Identities using Bayesian Network Inference,"Multi-target tracking requires locating the targets and labeling their identities. The latter is a challenge when many targets, with indistinct appearances, frequently occlude one another, as in football and surveillance tracking. We present an approach to solving this labeling problem. When isolated, a target can be tracked and its identity maintained. While, if targets interact this is not always the case. This paper assumes a track graph exists, denoting when targets are isolated and describing how they interact. Measures of similarity between isolated tracks are defined. The goal is to associate the identities of the isolated tracks, by exploiting the graph constraints and similarity measures. We formulate this as a Bayesian network inference problem, allowing us to use standard message propagation to find the most probable set of paths in an efficient way. The high complexity inevitable in large problems is gracefully reduced by removing dependency links between tracks. We apply the method to a 10 min sequence of an international football game and compare results to ground truth.",
Tree-Based Data Broadcast in IEEE 802.15.4 and ZigBee Networks,"This paper studies efficient and simple data broadcast in IEEE 802.15.4-based ad hoc networks (e.g., ZigBee). Since finding the minimum number of rebroadcast nodes in general ad hoc networks is NP-hard, current broadcast protocols either employ heuristic algorithms or assume extra knowledge such as position or two-hop neighbor table. However, the ZigBee network is characterized as low data rate and low cost. It cannot provide position or two-hop neighbor information, but it still requires an efficient broadcast algorithm that can reduce the number of rebroadcast nodes with limited computation complexity and storage space. To this end, this paper proposes self-pruning and forward node selection algorithms that exploit the hierarchical address space in ZigBee networks. Only one-hop neighbor information is needed; a partial list of two-hop neighbors is derived without exchanging messages between neighboring nodes. The ZigBee forward node selection algorithm finds the minimum rebroadcast nodes set with polynomial computation time and memory space. Using the proposed localized algorithms, it is proven that the entire network is covered. Simulations are conducted to evaluate the performance improvement in terms of the number of rebroadcast nodes, number of duplicated receivings, coverage time, and communication overhead",
Continuous Skyline Queries for Moving Objects,"The literature on skyline algorithms has so far dealt mainly with queries of static query points over static data sets. With the increasing number of mobile service applications and users, however, the need for continuous skyline query processing has become more pressing. A continuous skyline query involves not only static dimensions, but also the dynamic one. In this paper, we examine the spatiotemporal coherence of the problem and propose a continuous skyline query processing strategy for moving query points. First, we distinguish the data points that are permanently in the skyline and use them to derive a search bound. Second, we investigate the connection between the spatial positions of data points and their dominance relationship, which provides an indication of where to find changes in the skyline and how to maintain the skyline continuously. Based on the analysis, we propose a kinetic-based data structure and an efficient skyline query processing algorithm. We concisely analyze the space and time costs of the proposed method and conduct an extensive experiment to evaluate the method. To the best of our knowledge, this is the first work on continuous skyline query processing",
Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure,"A shared distributed infrastructure is formed by federating computation resources from multiple domains. Such shared infrastructures are increasing in popularity and are providing massive amounts of aggregated computation resources to large numbers of users. Meanwhile, virtualization technologies, at machine and network levels, are maturing and enabling mutually isolated virtual computation environments for executing arbitrary parallel/distributed applications on top of such a shared physical infrastructure. In this paper; we go one step further by supporting autonomic adaptation of virtual computation environments as active, integrated entities. More specifically, driven by both dynamic availability of infrastructure resources and dynamic application resource demand, a virtual computation environment is able to automatically relocate itself across the infrastructure and scale its share of infrastructural resources. Such autonomic adaptation is transparent to both users of virtual environments and administrators of infrastructures, maintaining the look and feel of a stable, dedicated environment for the user As our proof-of-concept, we present the design, implementation and evaluation of a system called VIOLIN, which is composed of a virtual network of virtual machines capable of live migration across a multi-domain physical infrastructure.",
Distributed Navigation Algorithms for Sensor Networks,,
Settling the Complexity of Two-Player Nash Equilibrium,"Even though many people thought the problem of finding Nash equilibria is hard in general, and it has been proven so for games among three or more players recently, it's not clear whether the two-player case can be shown in the same class of PPAD-complete problems. We prove that the problem of finding a Nash equilibrium in a two-player game is PPAD-complete",
A Comparison of Personal Name Matching: Techniques and Practical Issues,"Finding and matching personal names is at the core of an increasing number of applications: from text and Web mining, search engines, to information extraction, deduplication and data linkage systems. Variations and errors in names make exact string matching problematic, and approximate matching techniques have to be applied. When compared to general text, however, personal names have different characteristics that need to be considered. In this paper, we discuss the characteristics of personal names and present potential sources of variations and errors. We then overview a comprehensive number of commonly used, as well as some recently developed name matching techniques. Experimental comparisons using four large name data sets indicate that there is no clear best matching technique",
Nascap-2k Spacecraft Charging Code Overview,"Nascap-2k is a modern spacecraft charging code, replacing the older codes NASA Charging Analyzer Program for GEosynchronous Orbit (NASCAP/GEO), NASA Charging Analyzer Program for Low-Earth Orbit (NASCAP/LEO), Potentials Of Large objects in the Auroral Region (POLAR), and Dynamic Plasma Analysis Code (DynaPAC). The code builds on the physical principles, mathematical algorithms, and user experience developed over three decades of spacecraft charging research. Capabilities include surface charging in geosynchronous and interplanetary orbits, sheath, and wake structure, and current collection in low-Earth orbits, and auroral charging. External potential structure and particle trajectories are computed using a finite element method on a nested grid structure and may be visualized within the Nascap-2k interface. Space charge can be treated either analytically, self-consistently with particle trajectories, or consistent with imported plume densities. Particle-in-cell (PIC) capabilities are available to study dynamic plasma effects. Auxiliary programs to Nascap-2k include Object Toolkit (for developing spacecraft surface models) and GridTool (for constructing nested grid structures around spacecraft models). The capabilities of the code are illustrated by way of four examples: charging of a geostationary satellite, self-consistent potentials for a negative probe in a low-Earth orbit spacecraft wake, potentials associated with thruster plumes, and PIC calculations of plasma effects on a very low frequency (about 1 to 20 kHz) antenna",
Image and Texture Segmentation Using Local Spectral Histograms,"We present a method for segmenting images consisting of texture and nontexture regions based on local spectral histograms. Defined as a vector consisting of marginal distributions of chosen filter responses, local spectral histograms provide a feature statistic for both types of regions. Using local spectral histograms of homogeneous regions, we decompose the segmentation process into three stages. The first is the initial classification stage, where probability models for homogeneous texture and nontexture regions are derived and an initial segmentation result is obtained by classifying local windows. In the second stage, we give an algorithm that iteratively updates the segmentation using the derived probability models. The third is the boundary localization stage, where region boundaries are localized by building refined probability models that are sensitive to spatial patterns in segmented regions. We present segmentation results on texture as well as nontexture images. Our comparison with other methods shows that the proposed method produces more accurate segmentation results",
Personal Learning Environments,Personal learning environments (PLEs) are attracting increasing interest in the e-learning domain. PLEs may be characterised in a multidimensional space. Examples of PLEs are discussed,
A modular haptic rendering algorithm for stable and transparent 6-DOF manipulation,"This paper presents a modular algorithm for six-degree-of-freedom (6-DOF) haptic rendering. The algorithm is aimed to provide transparent manipulation of rigid models with a high polygon count. On the one hand, enabling a stable display is simplified by exploiting the concept of virtual coupling and employing passive implicit integration methods for the simulation of the virtual tool. On the other hand, transparency is enhanced by maximizing the update rate of the simulation of the virtual tool, and thereby the coupling impedance, and allowing for stable simulation with small mass values. The combination of a linearized contact model that frees the simulation from the computational bottleneck of collision detection, with penalty-based collision response well suited for fixed time-stepping, guarantees that the motion of the virtual tool is simulated at the same high rate as the synthesis of feedback force and torque. Moreover, sensation-preserving multiresolution collision detection ensures a fast update of the linearized contact model in complex contact scenarios, and a novel contact clustering technique alleviates possible instability problems induced by penalty-based collision response",
The Conceptual Coupling Metrics for Object-Oriented Systems,"Coupling in software has been linked with maintainability and existing metrics are used as predictors of external software quality attributes such as fault-proneness, impact analysis, ripple effects of changes, changeability, etc. Many coupling measures for object-oriented (OO) software have been proposed, each of them capturing specific dimensions of coupling. This paper presents a new set of coupling measures for OO systems - named conceptual coupling, based on the semantic information obtained from the source code, encoded in identifiers and comments. A case study on open source software systems is performed to compare the new measures with existing structural coupling measures. The case study shows that the conceptual coupling captures new dimensions of coupling, which are not captured by existing coupling measures; hence it can be used to complement the existing metrics",
On the adaptive detection of blood vessels in retinal images,"This paper proposes an automated blood vessel detection scheme based on adaptive contrast enhancement, feature extraction, and tracing. Feature extraction of small blood vessels is performed by using the standard deviation of Gabor filter responses. Tracing of vessels is done via forward detection, bifurcation identification, and backward verification. Tests over twenty images show that for normal images, the true positive rate (TPR) ranges from 80% to 91%, and their corresponding false positive rates (FPR) range from 2.8% to 5.5%. For abnormal images, the TPR ranges from 73.8% to 86.5% and the FPR ranges from 2.1% to 5.3%, respectively. In comparison with two published solution schemes that were also based on the STARE database, our scheme has lower FPR for the reported TPR measure.","Blood vessels,
Biomedical imaging,
Retina,
Gabor filters,
Feature extraction,
Standards development,
Filtering,
Computer science,
Bifurcation,
Testing"
On the informatics laws and deductive semantics of software,"A fundamental finding in computer science is that software, an artifact of human creativity, is not constrained by the laws and properties known in the physical world. Thus, a natural question we have to ask is ""what are the constraints that software obeys?"" This paper attempts to demonstrate that software obeys the laws of informatics and mathematics. This paper explores a comprehensive set of informatics and semantic properties and laws of software as well as their mathematical models. In order to provide a rigorous mathematical treatment of both the abstract and concrete semantics of software, a new type of formal semantics known as the deductive semantics is developed. The deductive models of semantics, semantic function, and semantic environment at various composing levels of programs are formally described. The findings of this paper can be applied to perceive the basic characteristics of software and the development of fundamental theories that deal with the informatics and semantic properties of software.",
Monocular 3D Head Tracking to Detect Falls of Elderly People,"Faced with the growing population of seniors, Western societies need to think about new technologies to ensure the safety of elderly people at home. Computer vision provides a good solution for healthcare systems because it allows a specific analysis of people behavior. Moreover, a system based on video surveillance is particularly well adapted to detect falls. We present a new method to detect falls using a single camera. Our approach is based on the 3D trajectory of the head, which allows us to distinguish falls from normal activities using 3D velocities",
Why Machine Ethics?,"Machine ethics isn't merely science fiction; it's a topic that requires serious consideration, given the rapid emergence of increasingly complex autonomous software agents and robots. Machine ethics is an emerging field that seeks to implement moral decision-making faculties in computers and robots. We already have semiautonomous robots and software agents that violate ethical standards as a matter of course. In the case of AI and robotics, fearful scenarios range from the future takeover of humanity by a superior form of AI to the havoc created by endlessly reproducing nanobots","Ethics,
Robots,
Artificial intelligence,
Switches,
Software agents,
Foot,
Bridges,
Airports,
Decision making,
Rails"
"Distance Perception in Immersive Virtual Environments, Revisited","Numerous previous studies have suggested that distances appear to be compressed in immersive virtual environments presented via head mounted display systems, relative to in the real world. However, the principal factors that are responsible for this phenomenon have remained largely unidentified. In this paper we shed some new light on this intriguing problem by reporting the results of two recent experiments in which we assess egocentric distance perception in a high fidelity, low latency, immersive virtual environment that represents an exact virtual replica of the participant’s concurrently occupied real environment. Under these novel conditions, we make the startling discovery that distance perception appears not to be significantly compressed in the immersive virtual environment, relative to in the real world.",
Automatic Derivation of Loop Bounds and Infeasible Paths for WCET Analysis Using Abstract Execution,"Static worst-case execution time (WCET) analysis is a technique to derive upper bounds for the execution times of programs. Such bounds are crucial when designing and verifying real-time systems. A key component for statically deriving safe and tight WCET bounds is information on the possible program flow through the program. Such flow information can be provided manually by user annotations, or automatically by a flow analysis. To make WCET analysis as simple and safe as possible, it should preferably be automatically derived, with no or very limited user interaction. In this paper we present a method for deriving such flow information called abstract execution. This method can automatically calculate loop bounds, bounds for including nested loops, as well as many types of infeasible paths. Our evaluations show that it can calculate WCET estimates automatically, without any user annotations, for a range of benchmark programs, and that our techniques for nested loops and infeasible paths sometimes can give substantially better WCET estimates than using loop bounds analysis only",
Attribute-Based Access Control with Hidden Policies and Hidden Credentials,"In an open environment such as the Internet, the decision to collaborate with a stranger (e.g., by granting access to a resource) is often based on the characteristics (rather than the identity) of the requester, via digital credentials: access is granted if Alice's credentials satisfy Bob's access policy. The literature contains many scenarios in which it is desirable to carry out such trust negotiations in a privacy-preserving manner, i.e., so as minimize the disclosure of credentials and/or of access policies. Elegant solutions were proposed for achieving various degrees of privacy-preservation through minimal disclosure. In this paper, we present protocols that protect both sensitive credentials and sensitive policies. That is, Alice gets the resource only if she satisfies the policy, Bob does not learn anything about Alice's credentials (not even whether Alice got access), and Alice learns neither Bob's policy structure nor which credentials caused her to gain access. Our protocols are efficient in terms of communication and in rounds of interaction",
How are Real Grids Used? The Analysis of Four Grid Traces and Its Implications,"The grid computing vision promises to provide the needed platform for a new and more demanding range of applications. For this promise to become true, a number of hurdles, including the design and deployment of adequate resource management and information services, need to be overcome. In this context, understanding the characteristics of real grid workloads is a crucial step for improving the quality of existing grid services, and in guiding the design of new solutions. Towards this goal, in this work we present the characteristics of traces of four real grid environments, namely LCG, Grid3, and TeraGrid, which are among the largest production grids currently deployed, and the DAS, which is a research grid. We focus our analysis on virtual organizations, on users, and on individual jobs characteristics. We further attempt to quantify the evolution and the performance of the grid systems from which our traces originate. Finally, given the scarcity of the information available for analysis purposes, we discuss the requirements of a new format for grid traces, and we propose the establishment of a virtual center for workload-based grid benchmarking data: the grid workloads archive",
Real-Time Scheduling on Multicore Platforms,"Multicore architectures, which have multiple processing units on a single chip, are widely viewed as a way to achieve higher processor performance, given that thermal and power problems impose limits on the performance of single-core designs. Accordingly, several chip manufacturers have already released, or will soon release, chips with dual cores, and it is predicted that chips with up to 32 cores will be available within a decade. To effectively use the available processing resources on multicore platforms, software designs should avoid co-executing applications or threads that can worsen the performance of shared caches, if not thrash them. While cache-aware scheduling techniques for such platforms have been proposed for throughput-oriented applications, to the best of our knowledge, no such work has targeted real-time applications. In this paper, we propose and evaluate a cache-aware Pfair-based scheduling scheme for real-time tasks on multicore platforms",
A Trust Based Framework for Secure Data Aggregation in Wireless Sensor Networks,"In unattended and hostile environments, node compromise can become a disastrous threat to wireless sensor networks and introduce uncertainty in the aggregation results. A compromised node often tends to completely reveal its secrets to the adversary which in turn renders purely cryptography-based approaches vulnerable. How to secure the information aggregation process against compromised-node attacks and quantify the uncertainty existing in the aggregation results has become an important research issue. In this paper, we address this problem by proposing a trust based framework, which is rooted in sound statistics and some other distinct and yet closely coupled techniques. The trustworthiness (reputation) of each individual sensor node is evaluated by using an information theoretic concept, Kullback-Leibler (KL) distance, to identify the compromised nodes through an unsupervised learning algorithm. Upon aggregating, an opinion, a metric of the degree of belief, is generated to represent the uncertainty in the aggregation result. As the result is being disseminated and assembled through the routes to the sink, this opinion will be propagated and regulated by Josang's belief model. Following this model, the uncertainty within the data and aggregation results can be effectively quantified throughout the network. Simulation results demonstrate that our trust based framework provides a powerful mechanism for detecting compromised nodes and reasoning about the uncertainty in the network. It further can purge false data to accomplish robust aggregation in the presence of multiple compromised nodes",
QMDD: A Decision Diagram Structure for Reversible and Quantum Circuits,"In this paper, we present a novel structure, QuantumMultiple- valued Decision Diagrams (QMDD), specifically designed to represent and manipulate the matrices encountered in the specification of reversible and quantum gates and circuits, both binary and multiple-valued. QMDD use many common decision diagram techniques, ideas introduced in QuIDDPro and novel techniques introduced here. Building the QMDD for the matrices for individual gates and the subsequent construction of the QMDD for the matrix describing a circuit are discussed. A prototype C implementation is described and experimental results are given that show the new structure is a promising and compact representation for reversible and quantum logic circuits.",
Performance of Secondary Radios in Spectrum Sharing with Prioritized Primary Access,"Research and discussion into more efficient utilization of frequency spectrum has led to a paradigm shift in future allocation and utilization of spectral resources in the frequency band of licensed (primary) systems. Secondary usage of licensed spectrum is being considered as a viable solution to the problem of inefficient usage of spectral resources. In this paper, we model and analyze the performance of the secondary system in dynamic spectrum access by secondary users employing an overlay approach of sharing spectral resources with a primary system. Our model assumes the operation of primary system is completely unaffected by the introduction of a secondary system, i.e., primary users have priority in transmission over secondary users. Each primary user is randomly assigned an available frequency slot for transmission by the primary system controller. Secondary users are required to sense across the same spectrum to identify any unoccupied frequency slots for transmission. During its transmission, if it detects a primary user accessing the same frequency slot, the connection will be forced to drop. Using a three-dimension Markov chain model, this paper presents an analytical approach to study the secondary system performance under a given primary traffic. Two important parameters that are indicative of the grade-of-service (GoS) of the secondary system, namely, secondary call blocking probability and secondary call dropping probability are computed using the model presented. The analysis provides an insightful study about how the secondary system capacity is determined by its GoS requirement. We use the same analytical approach to show that when the secondary call dropping probability is the cause of low capacity, by suitably reserving a number of frequency slots for primary users' access, the secondary system capacity can be improved significantly",
Seamless Image Stitching of Scenes with Large Motions and Exposure Differences,"This paper presents a technique to automatically stitch multiple images at varying orientations and exposures to create a composite panorama that preserves the angular extent and dynamic range of the inputs. The main contribution of our method is that it allows for large exposure differences, large scene motion or other misregistrations between frames and requires no extra camera hardware. To do this, we introduce a two-step graph cut approach. The purpose of the first step is to fix the positions of moving objects in the scene. In the second step, we fill in the entire available dynamic range. We introduce data costs that encourage consistency and higher signal-to-noise ratios, and seam costs that encourage smooth transitions. Our method is simple to implement and effective. We demonstrate the effectiveness of our approach on several input sets with varying exposures and camera orientations.",
Predictability of WLAN Mobility and Its Effects on Bandwidth Provisioning,,
Robust Computation of Aggregates in Wireless Sensor Networks: Distributed Randomized Algorithms and Analysis,"A wireless sensor network consists of a large number of small, resource-constrained devices and usually operates in hostile environments that are prone to link and node failures. Computing aggregates such as average, minimum, maximum and sum is fundamental to various primitive functions of a sensor network, such as system monitoring, data querying, and collaborative information processing. In this paper, we present and analyze a suite of randomized distributed algorithms to efficiently and robustly compute aggregates. Our distributed random grouping (DRG) algorithm is simple and natural and uses probabilistic grouping to progressively converge to the aggregate value. DRG is local and randomized and is naturally robust against dynamic topology changes from link/node failures. Although our algorithm is natural and simple, it is nontrivial to show that it converges to the correct aggregate value and to bound the time needed for convergence. Our analysis uses the eigenstructure of the underlying graph in a novel way to show convergence and to bound the running time of our algorithms. We also present simulation results of our algorithm and compare its performance to various other known distributed algorithms. Simulations show that DRG needs far fewer transmissions than other distributed localized schemes","Robustness,
Computer networks,
Distributed computing,
Aggregates,
Wireless sensor networks,
Algorithm design and analysis,
Distributed algorithms,
Convergence,
Sensor systems,
Condition monitoring"
Routing Correlated Data with Fusion Cost in Wireless Sensor Networks,"In this paper, we propose a routing algorithm called minimum fusion Steiner tree (MFST) for energy efficient data gathering with aggregation (fusion) in wireless sensor networks. Different from existing schemes, MFST not only optimizes over the data transmission cost, but also incorporates the cost for data fusion, which can be significant for emerging sensor networks with vectorial data and/or security requirements. By employing a randomized algorithm that allows fusion points to be chosen according to the nodes' data amounts, MFST achieves an approximation ratio of 5/4log(k + 1), where k denotes the number of source nodes, to the optimal solution for extremely general system setups, provided that fusion cost and data aggregation are nondecreasing against the total input data. Consequently, in contrast to algorithms that only excel in full or nonaggregation scenarios without considering fusion cost, MFST can thrive in a wide range of applications",
Vibration-based Terrain Classification Using Support Vector Machines,"In outdoor environments, there is a variety of different types of ground surfaces. If some of them are slippery or bumpy, for example, the ground surface itself is a possible hazard for an autonomous mobile vehicle traversing the surface. Therefore, it is beneficial if the vehicle is able to estimate, which terrain it is currently traversing. Using this estimation, the vehicle can adapt its driving style to the terrain. In this paper, we present a method for terrain classification based on vibration induced in the vehicle's body. An accelerometer mounted on the vehicle measures the vibration perpendicular to the ground surface. We experimentally compare representations of the data based on the fast Fourier transform (FFT) and on the power spectral density (PSD). Additionally, we suggest a simpler and more compact representation based on features calculated from the raw data vectors and a combination of this representation with the PSD. We train and classify the data with a support vector machine (SVM). Experiments on a large real-world dataset containing seven different terrain types evaluate our approach",
Wireless sensor networks: to cluster or not to cluster?,"The key challenge in the design and operation of wireless sensor networks (WSNs) is the maximization of system lifetime. Node clustering is commonly considered as one of the most promising techniques for dealing with the given challenge, and as such has been referred to by many researchers. It is interesting to observe, however, that very few, if any, published research works provide explicit analysis of node clustering in WSNs and/or manage to prove its actual effectiveness. In this paper we take a closer analytical look at WSNs of clustered organization. We prove that these networks do not necessarily outperform non-clustered WSNs. The condition that ensures superior performance of clustered WSNs, with absolute certainty, is that the formed clusters lie within the isoclusters of the monitored phenomenon. We also show that in clustered WSNs which satisfy the given condition, cluster sizes do not need to match the sizes of their respective underlying isoclusters. Instead, simple 5-hop clusters can provide near-optimal network performance under a wide range of cluster-to-sink and cluster-to-isocluster spatial arrangements",
VoIP Intrusion Detection Through Interacting Protocol State Machines,"Being a fast-growing Internet application, voice over Internet protocol (VoIP) shares the network resources with the regular Internet traffic, and is susceptible to the existing security holes of the Internet. Moreover, given that voice communication is time sensitive and uses a suite of interacting protocols, VoIP exposes new forms of vulnerabilities to malicious attacks. In this paper, we propose a highly-needed VoIP intrusion detection system. Our approach is novel in that, it utilizes not only the state machines of network protocols but also the interaction among them for intrusion detection. This detection approach is particularly suited for protecting VoIP applications, in which a melange of protocols are involved to provide IP telephony services. Based on tracking deviations from interacting protocol state machines, our solution shows promising detection characteristics and low runtime impact on the perceived quality of voice streams",
Partial Network Coding: Theory and Application for Continuous Sensor Data Collection,"Wireless sensor networks have been widely used for surveillance in harsh environments. In many such applications, the environmental data are continuously sensed, and data collection by a server is only performed occasionally. Hence, the sensor nodes have to temporarily store the data, and provide easy and on-hand access for the most updated data when the server approaches. Given the expensive server-to-sensor communications, the large amount of sensors and the limited storage space at each tiny sensor, continuous data collection becomes a challenging problem. In this paper, we present partial network coding (PNC) as a generic tool for the above applications. PNC generalizes the existing network coding (NC) paradigm, an elegant solution for ubiquitous data distribution and collection. Yet, PNC enables efficient storage replacement for continuous data, which is a major deficiency of the conventional NC. We prove that the performance of PNC is quite close to NC, except for a sub-linear overhead on storage and communications. We then address a set of practical concerns toward PNC-based continuous data collection in sensor networks. Its feasibility and superiority are further demonstrated through simulation results",
An efficient algorithm for optimal linear estimation fusion in distributed multisensor systems,"Under the assumption of independent observation noises across sensors, Bar-Shalom and Campo proposed a distributed fusion formula for two-sensor systems, whose main calculation is the inverse of submatrices of the error covariance of two local estimates instead of the inverse of the error covariance itself. However, the corresponding simple estimation fusion formula is absent in a general distributed multisensor system. In this paper, an efficient iterative algorithm for distributed multisensor estimation fusion without any restrictive assumption on the noise covariance (i.e., the assumption of independent observation noises across sensors and the two-sensor system, and the direct computation of the Moore-Penrose generalized inverse of the joint error covariance of local estimates are not necessary) is presented. At each iteration, only the inverse or generalized inverse of a matrix having the same dimension as the error covariance of a single-sensor estimate is required. In fact, the proposed algorithm is a generalization of Bar-Shalom and Campo's fusion formula and reduces the computational complexity significantly since the number of iterative steps is less than the number of sensors. An example of a three-sensor system shows how to implement the specific iterative steps and reduce the computational complexities",
Distributed collaborative key agreement and authentication protocols for dynamic peer Groups,"We consider several distributed collaborative key agreement and authentication protocols for dynamic peer groups. There are several important characteristics which make this problem different from traditional secure group communication. They are: 1) distributed nature in which there is no centralized key server; 2) collaborative nature in which the group key is contributory (i.e., each group member will collaboratively contribute its part to the global group key); and 3) dynamic nature in which existing members may leave the group while new members may join. Instead of performing individual rekeying operations, i.e., recomputing the group key after every join or leave request, we discuss an interval-based approach of rekeying. We consider three interval-based distributed rekeying algorithms, or interval-based algorithms for short, for updating the group key: 1) the Rebuild algorithm; 2) the Batch algorithm; and 3) the Queue-batch algorithm. Performance of these three interval-based algorithms under different settings, such as different join and leave probabilities,is analyzed. We show that the interval-based algorithms significantly outperform the individual rekeying approach and that the Queue-batch algorithm performs the best among the three interval-based algorithms. More importantly, the Queue-batch algorithm can substantially reduce the computation and communication workload in a highly dynamic environment. We further enhance the interval-based algorithms in two aspects: authentication and implementation. Authentication focuses on the security improvement, while implementation realizes the interval-based algorithms in real network settings. Our work provides a fundamental understanding about establishing a group key via a distributed and collaborative approach for a dynamic peer group.",
Nonparallel training for voice conversion based on a parameter adaptation approach,"The objective of voice conversion algorithms is to modify the speech by a particular source speaker so that it sounds as if spoken by a different target speaker. Current conversion algorithms employ a training procedure, during which the same utterances spoken by both the source and target speakers are needed for deriving the desired conversion parameters. Such a (parallel) corpus, is often difficult or impossible to collect. Here, we propose an algorithm that relaxes this constraint, i.e., the training corpus does not necessarily contain the same utterances from both speakers. The proposed algorithm is based on speaker adaptation techniques, adapting the conversion parameters derived for a particular pair of speakers to a different pair, for which only a nonparallel corpus is available. We show that adaptation reduces the error obtained when simply applying the conversion parameters of one pair of speakers to another by a factor that can reach 30%. A speaker identification measure is also employed that more insightfully portrays the importance of adaptation, while listening tests confirm the success of our method. Both the objective and subjective tests employed, demonstrate that the proposed algorithm achieves comparable results with the ideal case when a parallel corpus is available.",
PET reconstruction with system matrix derived from point source measurements,"The quality of images reconstructed by statistical iterative methods depends on an accurate model of the relationship between image space and projection space through the system matrix. The elements of a system matrix on the HiRez scanner from CPS Innovations were acquired by positioning a point source at different positions in the scanner field of view. Then, a whole system matrix was derived by processing the responses in projection space. Such responses included both geometrical and detection physics components of the system matrix. The response was parameterized to correct for point source location, smooth projection noise and the whole system matrix was derived. The model accounts for axial compression (span) used on the scanner. The projection operator for iterative reconstruction was constructed using the estimated response parameters. Computer-generated and acquired data were used to compare reconstruction obtained by the HiRez standard software and that produced by better modeling. Results showed that better resolution and noise property can be achieved with the modeled system matrix.",
On Heterogeneous Overlay Construction and Random Node Selection in Unstructured P2P Networks,,
Full Body Virtual Autopsies using a State-of-the-art Volume Rendering Pipeline,"This paper presents a procedure for virtual autopsies based on interactive 3D visualizations of large scale, high resolution data from CT-scans of human cadavers. The procedure is described using examples from forensic medicine and the added value and future potential of virtual autopsies is shown from a medical and forensic perspective. Based on the technical demands of the procedure state-of-the-art volume rendering techniques are applied and refined to enable real-time, full body virtual autopsies involving gigabyte sized data on standard GPUs. The techniques applied include transfer function based data reduction using level-of-detail selection and multi-resolution rendering techniques. The paper also describes a data management component for large, out-of-core data sets and an extension to the GPU-based raycaster for efficient dual TF rendering. Detailed benchmarks of the pipeline are presented using data sets from forensic cases",
Estimating Intrinsic Component Images using Non-Linear Regression,"Images can be represented as the composition of multiple intrinsic component images, such as shading, albedo, and noise images. In this paper, we present a method for estimating intrinsic component images from a single image, which we apply to the problems of estimating shading and albedo images and image denoising. Our method is based on learning estimators that predict filtered versions of the desired image. Unlike previous approaches, our method does not require unnatural discretizations of the problem. We also demonstrate how to learn a weighting function that properly weights the local estimates when constructing the estimated image. For shading estimation, we introduce a new training set of real-world images. The accuracy of our method is measured both qualitatively and quantitatively, showing better performance on the shading/albedo separation problem than previous approaches. The performance on denoising is competitive with the current state of the art.",
Internet 3.0: Ten Problems with Current Internet Architecture and Solutions for the Next Generation,"The basic ideas of the Internet architecture were developed 30+ years ago. In these 30 years, we have learnt a lot about networking and packet switching. Is this the way we would design the Internet if we were to start it now? This paper is an attempt to answer this question raised by US National Science Foundation, which has embarked on the design of the next generation Internet called GENI. In this paper, we point out key problems with the current Internet architecture and propose directions for the solutions. We propose a general architectural framework for the next generation Internet, which we call Internet 3.0. The next generation Internet should be secure. It should allow business to set their boundaries and enforce their policies inside their boundaries. It should allow governments to set rules that protect their citizens on the Internet the same way they protect them on other means of transports. It should allow receivers to set policies for how and where they receive their information. They should have freedom to select their names, IDs and addresses with as little centralized control as possible. The architecture should be general enough to allow different governments to have different rules. Information transport architecture should provide at least as much control and freedom as the goods transport networks provide. We propose the framework of an architecture that supports all these requirements","Internet,
Government,
Protection,
Computer architecture,
Proposals,
Packet switching,
Intrusion detection,
Centralized control,
Computer science,
Investments"
Face Recognition Using 3-D Models: Pose and Illumination,"Unconstrained illumination and pose variation lead to significant variation in the photographs of faces and constitute a major hurdle preventing the widespread use of face recognition systems. The challenge is to generalize from a limited number of images of an individual to a broad range of conditions. Recently, advances in modeling the effects of illumination and pose have been accomplished using three-dimensional (3-D) shape information coupled with reflectance models. Notable developments in understanding the effects of illumination include the nonexistence of illumination invariants, a characterization of the set of images of objects in fixed pose under variable illumination (the illumination cone), and the introduction of spherical harmonics and low-dimensional linear subspaces for modeling illumination. To generalize to novel conditions, either multiple images must be available to reconstruct 3-D shape or, if only a single image is accessible, prior information about the 3-D shape and appearance of faces in general must be used. The 3-D Morphable Model was introduced as a generative model to predict the appearances of an individual while using a statistical prior on shape and texture allowing its parameters to be estimated from single image. Based on these new understandings, face recognition algorithms have been developed to address the joint challenges of pose and lighting. In this paper, we review these developments and provide a brief survey of the resulting face recognition algorithms and their performance",
Mode conversion in a magnetron with axial extraction of radiation,We demonstrate the ability to form simple radiation patterns from a relativistic magnetron with axial extraction. This is achieved by tapering onto a conical antenna only those cavities of the anode block that correspond to the symmetry of the radiated modes. The efficiency of mode conversion of the operating pi-mode into a radiated mode using this method is demonstrated using computer simulations of a six-cavity magnetron,
Bayesian Methods for Pharmacokinetic Models in Dynamic Contrast-Enhanced Magnetic Resonance Imaging,This paper proposes a new method for estimating kinetic parameters of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) based on adaptive Gaussian Markov random fields. Kinetic parameter estimates using neighboring voxels reduce the observed variability in local tumor regions while preserving sharp transitions between heterogeneous tissue boundaries. Asymptotic results for standard errors from likelihood-based nonlinear regression are compared with those derived from the posterior distribution using Bayesian estimation with and without neighborhood information. Application of the method to the analysis of breast tumors based on kinetic parameters has shown that the use of Bayesian analysis combined with adaptive Gaussian Markov random fields provides improved convergence behavior and more consistent morphological and functional statistics,
Characterization of Scientific Workloads on Systems with Multi-Core Processors,"Multi-core processors are planned for virtually all next-generation HPC systems. In a preliminary evaluation of AMD Opteron Dual-Core processor systems, we investigated the scaling behavior of a set of micro-benchmarks, kernels, and applications. In addition, we evaluated a number of processor affinity techniques for managing memory placement on these multi-core systems. We discovered that an appropriate selection of MPI task and memory placement schemes can result in over 25% performance improvement for key scientific calculations. We collected detailed performance data for several large-scale scientific applications. Analyses of the application performance results confirmed our micro-benchmark and scaling results",
Voice conversion using duration-embedded bi-HMMs for expressive speech synthesis,"This paper presents an expressive voice conversion model (DeBi-HMM) as the post processing of a text-to-speech (TTS) system for expressive speech synthesis. DeBi-HMM is named for its duration-embedded characteristic of the two HMMs for modeling the source and target speech signals, respectively. Joint estimation of source and target HMMs is exploited for spectrum conversion from neutral to expressive speech. Gamma distribution is embedded as the duration model for each state in source and target HMMs. The expressive style-dependent decision trees achieve prosodic conversion. The STRAIGHT algorithm is adopted for the analysis and synthesis process. A set of small-sized speech databases for each expressive style is designed and collected to train the DeBi-HMM voice conversion models. Several experiments with statistical hypothesis testing are conducted to evaluate the quality of synthetic speech as perceived by human subjects. Compared with previous voice conversion methods, the proposed method exhibits encouraging potential in expressive speech synthesis",
Relevance feedback for CBIR: a new approach based on probabilistic feature weighting with positive and negative examples,"In content-based image retrieval, understanding the user's needs is a challenging task that requires integrating him in the process of retrieval. Relevance feedback (RF) has proven to be an effective tool for taking the user's judgement into account. In this paper, we present a new RF framework based on a feature selection algorithm that nicely combines the advantages of a probabilistic formulation with those of using both the positive example (PE) and the negative example (NE). Through interaction with the user, our algorithm learns the importance he assigns to image features, and then applies the results obtained to define similarity measures that correspond better to his judgement. The use of the NE allows images undesired by the user to be discarded, thereby improving retrieval accuracy. As for the probabilistic formulation of the problem, it presents a multitude of advantages and opens the door to more modeling possibilities that achieve a good feature selection. It makes it possible to cluster the query data into classes, choose the probability law that best models each class, model missing data, and support queries with multiple PE and/or NE classes. The basic principle of our algorithm is to assign more importance to features with a high likelihood and those which distinguish well between PE classes and NE classes. The proposed algorithm was validated separately and in image retrieval context, and the experiments show that it performs a good feature selection and contributes to improving retrieval effectiveness.",
A new TCP for persistent packet reordering,"Most standard implementations of TCP perform poorly when packets are reordered. In this paper, we propose a new version of TCP that maintains high throughput when reordering occurs and yet, when packet reordering does not occur, is friendly to other versions of TCP. The proposed TCP variant, or TCP-PR, does not rely on duplicate acknowledgments to detect a packet loss. Instead, timers are maintained to keep track of how long ago a packet was transmitted. In case the corresponding acknowledgment has not yet arrived and the elapsed time since the packet was sent is larger than a given threshold, the packet is assumed lost. Because TCP-PR does not rely on duplicate acknowledgments, packet reordering (including out-or-order acknowledgments) has no effect on TCP-PR's performance. Through extensive simulations, we show that TCP-PR performs consistently better than existing mechanisms that try to make TCP more robust to packet reordering. In the case that packets are not reordered, we verify that TCP-PR maintains the same throughput as typical implementations of TCP (specifically, TCP-SACK) and shares network resources fairly. Furthermore, TCP-PR only requires changes to the TCP sender side making it easier to deploy.",
Active Graph Cuts,"This paper adds a number of novel concepts into global s/t cut methods improving their efficiency and making them relevant for a wider class of applications in vision where algorithms should ideally run in real-time. Our new Active Cuts (AC) method can effectively use a good approximate solution (initial cut) that is often available in dynamic, hierarchical, and multi-label optimization problems in vision. In many problems AC works faster than the state-of-the-art max-flow methods [2] even if initial cut is far from the optimal one. Moreover, empirical speed improves several folds when initial cut is spatially close to the optima. Before converging to a global minima, Active Cuts outputs a multitude of intermediate solutions (intermediate cuts) that, for example, can be used be accelerate iterative learning-based methods or to improve visual perception of graph cuts realtime performance when large volumetric data is segmented. Finally, it can also be combined with many previous methods for accelerating graph cuts.","Application software,
Acceleration,
Visual perception,
Costs,
Image segmentation,
Computer vision,
Testing,
Computer science,
Optimization methods,
Iterative methods"
Image Based Source Camera Identification using Demosaicking,"We represent the spatially periodic inter-pixel correlation due to color filter array interpolation in a quadratic form. Based on this, a coefficient matrix is obtained for each color channel, whose principal components are extracted and fed to feed-forward back propagation networks for source camera identification. Experiments demonstrate the efficacy, sensitivity and robustness of our method",
On the channel capacity in Rician and Hoyt fading environments with MRC diversity,Expressions for the capacity of Rician and Hoyt (Nakagami-q) fading channels with maximal ratio combining diversity are available in the literature in the form of an infinite series. We derive lower and upper bounds on the errors resulting from truncating the infinite series in those capacity expressions. We also present numerical examples for illustration purposes.,
Distributed Fountain Codes for Networked Storage,"We investigate the problem of constructing fountain codes for distributed storage in sensor networks. Specifically, we assume that there are n storage nodes with limited memory and k < n data nodes generating the data by sensing the environment. We want a data collector who can appear anywhere in the network, to query any k + epsi storage nodes and be able to retrieve almost all the data packets. We demonstrate how it is possible to solve this problem by using a specific kind of fountain code that requires only linear communication and decoding complexity. Further, for a grid topology, we propose a randomized algorithm that constructs the fountain code over a network using only geographical knowledge and local decisions. A key step in the analysis of our algorithm is a novel result concerning random walks on finite grids with traps",
Tramcar: A Context-Aware Cross-Layer Architecture for Next Generation Heterogeneous Wireless Networks,"Major research challenges in the next generation (4G) of wireless networks include the provisioning of worldwide seamless mobility across heterogeneous wireless networks, the improvement of end-to-end Quality of Service (QoS) and enabling users to specify their personal preferences. Under this motivation, we design a novel cross-layer architecture that provides context-awareness, smart handoff and mobility control in heterogeneous wireless IP networks. We develop a Transport and Application Layer Architecture for vertical Mobility with Context-awareness (Tramcar). Tramcar is tailored for a variety of different network technologies with different characteristics and has the ability of adapting to changing environment conditions and unpredictable background traffic. Furthermore, Tramcar allows users to identify and prioritize their preferences. Simulation results demonstrate that Tramcar increases user satisfaction levels and network throughput under rough network conditions and reduces overall handoff latencies.",
Reducing Congestion Effects in Wireless Networks by Multipath Routing,"We propose a solution to improve fairness and increase throughput in wireless networks with location information. Our approach consists of a multipath routing protocol, biased geographical routing (BGR), and two congestion control algorithms, in-network packet scatter (IPS) and end-to-end packet scatter (EPS), which leverage BGR to avoid the congested areas of the network. BGR achieves good performance while incurring a communication overhead of just 1 byte per data packet, and has a computational complexity similar to greedy geographic routing. IPS alleviates transient congestion by splitting traffic immediately before the congested areas. In contrast, EPS alleviates long term congestion by splitting the flow at the source, and performing rate control. EPS selects the paths dynamically, and uses a less aggressive congestion control mechanism on non-greedy paths to improve energy efficiency. Simulation and experimental results show that our solution achieves its objectives. Extensive ns-2 simulations show that our solution improves both fairness and throughput as compared to single path greedy routing. Our solution reduces the variance of throughput across all flows by 35%, reduction which is mainly achieved by increasing throughput of long-range flows with around 70%. Furthermore, overall network throughput increases by approximately 10% Experimental results on a 50- node testbed are consistent with our simulation results, suggesting that BGR is effective in practice.",
Bayesian Gaussian Process Classification with the EM-EP Algorithm,"Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a certain class at an input location is monotonically related to the value of some latent function at that location. Starting from a Gaussian process prior over this latent function, data are used to infer both the posterior over the latent function and the values of hyperparameters to determine various aspects of the function. Recently, the expectation propagation (EP) approach has been proposed to infer the posterior over the latent function. Based on this work, we present an approximate EM algorithm, the EM-EP algorithm, to learn both the latent function and the hyperparameters. This algorithm is found to converge in practice and provides an efficient Bayesian framework for learning hyperparameters of the kernel. A multiclass extension of the EM-EP algorithm for GPCs is also derived. In the experimental results, the EM-EP algorithms are as good or better than other methods for GPCs or support vector machines (SVMs) with cross-validation",
Efficiency Centric Communication Model for Wireless Sensor Networks,,
WITS: A Wireless Sensor Network for Intelligent Transportation System,"Transportation information collection and communication plays a key role in intelligent transportation system (ITS). Unfortunately, most conventional ITSs can only detect the vehicle in a fixed position, and their communication cables and power cables elevate the cost of construction and maintenance. Because of the advantages of the wireless sensor network (WSN) such as low power consumption, wireless distribution, and flexibility without cable restrictions., the usage of WSN in ITSs is expected to be able to overcome the above difficulties. This paper proposes a WSN-based transportation information collection and communication system. Hardware and software WSN modules are designed and prototyped","Intelligent transportation systems,
Wireless sensor networks,
Intelligent networks,
Intelligent sensors,
Vehicle detection,
Power cables,
Communication cables,
Costs,
Energy consumption,
Hardware"
Design of 3D Swim Patterns for Autonomous Robotic Fish,"To realise fish-like swim patterns by a robotic system poses tremendous challenges. This requires fully understanding of fish biomechanics and the way to mimic it. This paper presents our research toward the sensor-based control of autonomous robotic fish that can swim in a 3D unstructured environment, based on observations of fish swim behaviours. Our robotic fish has a tail with three or four degrees of freedom (DOF) and is controlled by 4 onboard computers (a powerful Gumstix Linux PC and 3 PICs) and over 10 embedded sensors. Both simulated and the real fish experiments are conducted to show the feasibility and performance of the proposed approach",
Imitation Learning of Dual-Arm Manipulation Tasks in Humanoid Robots,"In this paper, we deal with imitation learning of arm movements in humanoid robots. Hidden Markov models (HMM) are used to generalize movements demonstrated to a robot multiple times. They are trained with the characteristic features (key points) of each demonstration. Using the same HMM, key points that are common to all demonstrations are identified; only those are considered when reproducing a movement. We also show how HMM can be used to detect temporal dependencies between both arms in dual-arm tasks. We created a model of the human upper body to simulate the reproduction of dual-arm movements and generate natural-looking joint configurations from tracked hand paths. Results are presented and discussed",
A Fully Automated Method for Lung Nodule Detection From Postero-Anterior Chest Radiographs,"In the past decades, a great deal of research work has been devoted to the development of systems that could improve radiologists' accuracy in detecting lung nodules. Despite the great efforts, the problem is still open. In this paper, we present a fully automated system processing digital postero-anterior (PA) chest radiographs, that starts by producing an accurate segmentation of the lung field area. The segmented lung area includes even those parts of the lungs hidden behind the heart, the spine, and the diaphragm, which are usually excluded from the methods presented in the literature. This decision is motivated by the fact that lung nodules may be found also in these areas. The segmented area is processed with a simple multiscale method that enhances the visibility of the nodules, and an extraction scheme is then applied to select potential nodules. To reduce the high number of false positives extracted, cost-sensitive support vector machines (SVMs) are trained to recognize the true nodules. Different learning experiments were performed on two different data sets, created by means of feature selection, and employing Gaussian and polynomial SVMs trained with different parameters; the results are reported and compared. With the best SVM models, we obtain about 1.5 false positives per image (fp/image) when sensitivity is approximately equal to 0.71; this number increases to about 2.5 and 4 fp/image when sensitivity is ap0.78 and ap0.85, respectively. For the highest sensitivity (ap0.92 and 1.0), we get 7 or 8 fp/image",
Method to correct intensity inhomogeneity in MR images for atherosclerosis characterization,"We are developing methods to characterize atherosclerotic disease in human carotid arteries using multiple MR images having different contrast mechanisms (T1W, T2W, PDW). To enable the use of voxel gray values for interpretation of disease, we created a new method, local entropy minimization with a bicubic spline model (LEMS), to correct the severe (/spl ap/80%) intensity inhomogeneity that arises from the surface coil array. This entropy-based method does not require classification and robustly addresses some problems that are more severe than those found in brain imaging, including noise, steep bias field, sensitivity of artery wall voxels to edge artifacts, and signal voids near the artery wall. Validation studies were performed on a synthetic digital phantom with realistic intensity inhomogeneity, a physical phantom roughly mimicking the neck, and patient carotid artery images. We compared LEMS to a modified fuzzy c-means segmentation based method (mAFCM), and a linear filtering method (LINF). Following LEMS correction, skeletal muscles in patient images were relatively isointense across the field of view. In the physical phantom, LEMS reduced the variation in the image to 1.9 % and across the vessel wall region to 2.5 %, a value which should be sufficient to distinguish plaque tissue types, based on literature measurements. In conclusion, we believe that the correction method shows promise for aiding human and computerized tissue classification from MR signal intensities.",
A Massively Parallel Multigrid Method for Finite Elements,"The hierarchical hybrid grid framework supports the parallel implementation of multigrid solvers for finite element problems. Specifically, it generates extremely fine meshes by using a structured refinement of an unstructured base mesh. For special problems with piecewise uniform material parameters, this leads to the possibility of stencil-based operations, which save substantial memory and permit a very efficient implementation of the multigrid method",
A Visual Interface for Multivariate Temporal Data: Finding Patterns of Events across Multiple Histories,"Finding patterns of events over time is important in searching patient histories, Web logs, news stories, and criminal activities. This paper presents PatternFinder, an integrated interface for query and result-set visualization for search and discovery of temporal patterns within multivariate and categorical data sets. We define temporal patterns as sequences of events with inter-event time spans. PatternFinder allows users to specify the attributes of events and time spans to produce powerful pattern queries that are difficult to express with other formalisms. We characterize the range of queries PatternFinder supports as users vary the specificity at which events and time spans are defined. Pattern Finder's query capabilities together with coupled ball-and-chain and tabular visualizations enable users to effectively query, explore and analyze event patterns both within and across data entities (e.g. patient histories, terrorist groups, Web logs, etc.)",
A hybrid SoC interconnect with dynamic TDMA-based transaction-less buses and on-chip networks,"The two dominant architectural choices for implementing efficient communication fabrics for SoC's have been transaction-based buses and packet-based networks-on-chip (NoC). Both implementations have some inherent disadvantages - the former resulting from poor scalability and the transactional character of their operation, and the latter from inconsistent access times and deterioration of performance at high injection rates. In this paper, we propose a transaction-less, time-division-based bus architecture, which dynamically allocates timeslots on-the-fly - the dTDMA bus. This architecture addresses the contention issues of current bus architectures, while avoiding the multi-hop overhead of NoC's. It is compared to traditional bus architectures and NoC's and shown to outperform both for configurations with fewer than 10 PE's. In order to exploit the advantages of the dTDMA bus for smaller configurations, and the scalability of NoC's, we propose a new hybrid SoC interconnect combining the two, showing significant improvement in both latency and power consumption.",
Affine Invariance Revisited,This paper proposes a Riemannian geometric framework to compute averages and distributions of point configurations so that different configurations up to affine transformations are considered to be the same. The algorithms are fast and proven to be robust both theoretically and empirically. The utility of this framework is shown in a number of affine invariant clustering algorithms on image point data.,
Landmark-Based Information Storage and Retrieval in Sensor Networks,,
The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering,"The nonnegative matrix factorization (NMF) has been shown recently to be useful for clustering and various extensions and variations of NMF have been proposed recently. Despite significant research progress in this area, few attempts have been made to establish the connections between various factorization methods while highlighting their differences. In this paper we aim to provide a comprehensive study on matrix factorization for clustering. In particular, we present an overview and summary on various matrix factorization algorithms and theoretically analyze the relationships among them. Experiments are also conducted to empirically evaluate and compare various factorization methods. In addition, our study also answers several previously unaddressed yet important questions for matrix factorizations including the interpretation and normalization of cluster posterior and the benefits and evaluation of simultaneous clustering. We expect our study would provide good insights on matrix factorization research for clustering.",
Dynamically formed heterogeneous robot teams performing tightly-coordinated tasks,"As we progress towards a world where robots play an integral role in society, a critical problem that remains to be solved is the pickup team challenge; that is, dynamically formed heterogeneous robot teams executing coordinated tasks where little information is known a priori about the tasks, the robots, and the environments in which they would operate. Successful solutions to forming pickup teams would enable researchers to experiment with larger numbers of robots and enable industry to efficiently and cost-effectively integrate new robot technology with existing legacy teams. In this paper, we define the challenge of pickup teams and propose the treasure hunt domain for evaluating the performance of pickup teams. Additionally, we describe a basic implementation of a pickup team that can search and discover treasure in a previously unknown environment. We build on prior approaches in market-based task allocation and plays for synchronized task execution, to allocate roles amongst robots in the pickup team, and to execute synchronized team actions to accomplish the treasure hunt task",
High-Dimensional Visual Analytics: Interactive Exploration Guided by Pairwise Views of Point Distributions,"We introduce a method for organizing multivariate displays and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional Euclidean space. These characterizations include such measures as density, skewness, shape, outliers, and texture. Statistical analysis of these measures leads to ways for 1) organizing 2D scatterplots of points for coherent viewing, 2) locating unusual (outlying) marginal 2D distributions of points for anomaly detection and 3) sorting multivariate displays based on high-dimensional data, such as trees, parallel coordinates, and glyphs",
Tomographic reconstruction using an adaptive tetrahedral mesh defined by a point cloud,"Medical images in nuclear medicine are commonly represented in three dimensions as a stack of two-dimensional images that are reconstructed from tomographic projections. Although natural and straightforward, this may not be an optimal visual representation for performing various diagnostic tasks. A method for three-dimensional (3-D) tomographic reconstruction is developed using a point cloud image representation. A point cloud is a set of points (nodes) in space, where each node of the point cloud is characterized by its position and intensity. The density of the nodes determines the local resolution allowing for the modeling of different parts of the image with different resolution. The reconstructed volume, which in general could be of any resolution, size, shape, and topology, is represented by a set of nonoverlapping tetrahedra defined by the nodes. The intensity at any point within the volume is defined by linearly interpolating inside a tetrahedron from the values at the four nodes that define the tetrahedron. This approach creates a continuous piecewise linear intensity over the reconstruction domain. The reconstruction provides a distinct multiresolution representation, which is designed to accurately and efficiently represent the 3-D image. The method is applicable to the acquisition of any tomographic geometry, such as parallel-, fan-, and cone-beam; and the reconstruction procedure can also model the physics of the image detection process. An efficient method for evaluating the system projection matrix is presented. The system matrix is used in an iterative algorithm to reconstruct both the intensity and location of the distribution of points in the point cloud. Examples of the reconstruction of projection data generated by computer simulations and projection data experimentally acquired using a Jaszczak cardiac torso phantom are presented. This work creates a framework for voxel-less multiresolution representation of images in nuclear medicine",
Scaling System-Level Science: Scientific Exploration and IT Implications,"The study and creation of the infrastructure required to enable system-level science--the integration of diverse sources of knowledge about the constituent parts of a complex system with the goal of obtaining an understanding of the system's properties as a whole--is becoming increasingly important, spawning new knowledge in variety of fields at a rapid pace.",
Maximizing the Lifetime of Wireless Sensor Networks through Optimal Single-Session Flow Routing,"Wireless sensor networks are becoming increasingly important in recent years due to their ability to detect and convey real-time, in-situ information for many civilian and military applications. A fundamental challenge for such networks lies in energy constraint, which poses a performance limit on the achievable network lifetime. We consider a two-tier wireless sensor network and address the network lifetime problem for upper-tier aggregation and forwarding nodes (AFNs). Existing flow routing solutions proposed for maximizing network lifetime require AFNs to split flows to different paths during transmission, which we call multisession flow routing solutions. If an AFN is equipped with a single transmitter/receiver pair, a multisession flow routing solution requires a packet-level power control at the AFN so as to conserve energy, which calls for considerable overhead in synchronization among the AFNs. In this paper, we show that it is possible to achieve the same optimal network lifetime by power control on a much larger timescale with the so-called single-session flow routing solutions, under which the packet-level power control and, thus, strict requirement on synchronization are not necessary. We also show how to perform optimal single-session flow routing when the bit-rate of composite flows generated by AFNs is time-varying, as long as the average bit-rate can be estimated",
"Multi-variate, Time Varying, and Comparative Visualization with Contextual Cues","Time-varying, multi-variate, and comparative data sets are not easily visualized due to the amount of data that is presented to the user at once. By combining several volumes together with different operators into one visualized volume, the user is able to compare values from different data sets in space over time, run, or field without having to mentally switch between different renderings of individual data sets. In this paper, we propose using a volume shader where the user is given the ability to easily select and operate on many data volumes to create comparison relationships. The user specifies an expression with set and numerical operations and her data to see relationships between data fields. Furthermore, we render the contextual information of the volume shader by converting it to a volume tree. We visualize the different levels and nodes of the volume tree so that the user can see the results of suboperations. This gives the user a deeper understanding of the final visualization, by seeing how the parts of the whole are operationally constructed",
Computing over Multiple-Access Channels with Connections to Wireless Network Coding,"We study the problem of multicasting over a network of multiple-access channels (MACs). The separation-based solution to this problem is to reduce each MAC to a set of noiseless bit pipes via a channel code and then employ network coding. Sometimes, however, the physical-layer structure of the MAC can be exploited more advantageously. In many cases of interest, the MAC output is a (deterministic) function of its inputs, corrupted by noise. We develop structured codes to exploit the natural function of a MAC to reliably compute functions as part of a network code and show that in many scenarios of interest our scheme outperforms the separation-based solution. If each MAC can be written as a sum over some finite field plus noise, then our achievable rate coincides with the max-flow min-cut bound",
Smart Cars on Smart Roads: An IEEE Intelligent Transportation Systems Society Update,"To promote tighter collaboration between the IEEE Intelligent Transportation Systems Society and the pervasive computing research community, the authors introduce the ITS Society and present several pervasive computing-related research topics that ITS Society researchers are working on. This department is part of a special issue on Intelligent Transportation.","intelligent transportation systems,
ITSS"
An association rule mining-based methodology for automated detection of ischemic ECG beats,"Currently, an automated methodology based on association rules is presented for the detection of ischemic beats in long duration electrocardiographic (ECG) recordings. The proposed approach consists of three stages. 1) Preprocessing: Noise is removed and all the necessary ECG features are extracted. 2) Discretization: The continuous valued features are transformed to categorical. 3) Classification: An association rule extraction algorithm is utilized and a rule-based classification model is created. According to the proposed methodology, electrocardiogram (ECG) features extracted from the ST segment and the T-wave, as well as the patient's age, were used as inputs. The output was the classification of the beat as ischemic or not. Various algorithms were tested both for discretization and for classification using association rules. To evaluate the methodology, a cardiac beat dataset was constructed using several recordings of the European Society of Cardiology ST-T database. The obtained sensitivity (Se) and specificity (Sp) was 87% and 93%, respectively. The proposed methodology combines high accuracy with the ability to provide interpretation for the decisions made, since it is based on a set of association rules",
Optimal Worst-Case Coverage of Directional Field-of-View Sensor Networks,"Sensor coverage is a fundamental sensor networking design and use issue that in general tries to answer the questions about the quality of sensing (surveillance) that a particular sensor network provides. Although isotropic sensor models and coverage formulations have been studied and analyzed in great depth recently, the obtained results do not easily extend to, and address the coverage of directional and field-of-view sensors such as imagers and video cameras. In this paper, we present an optimal polynomial time algorithm for computing the worst-case breach coverage in sensor networks that are comprised of directional ""field-of-view"" (FOV) sensors. Given a region covered by video cameras, a direct application of the presented algorithm is to compute ""breach"", which is defined as the maximal distance that any hostile target can maintain from the sensors while traversing through the region. Breach translates to ""worst-case coverage"" by assuming that in general, targets are more likely to be detected and observed when they are closer to the sensors (while in the field of view). The approach is amenable to the inclusion of any sensor detection model that is either independent of, or inversely proportional to distance from the targets. Although for the sake of discussion we mainly focus on square fields and model the sensor FOV as an isosceles triangle, we also discuss how the algorithm can trivially be extended to deal with arbitrary polygonal field boundaries and sensor FOVs, even in the presence of rigid obstacles. We also present several simulation-based studies of the scaling issues in such coverage problems and analyze the statistical properties of breach and its sensitivity to node density, locations, and orientations. A simple grid-based approximation approach is also analyzed for comparison and validation of the implementation","Image sensors,
Cameras,
Art,
Image analysis,
Communications Society,
Jacobian matrices,
Computer science,
Surveillance,
Polynomials,
Computer networks"
Feature Aligned Volume Manipulation for Illustration and Visualization,"In this paper we describe a GPU-based technique for creating illustrative visualization through interactive manipulation of volumetric models. It is partly inspired by medical illustrations, where it is common to depict cuts and deformation in order to provide a better understanding of anatomical and biological structures or surgical processes, and partly motivated by the need for a real-time solution that supports the specification and visualization of such illustrative manipulation. We propose two new feature aligned techniques, namely surface alignment and segment alignment, and compare them with the axis-aligned techniques which were reported in previous work on volume manipulation. We also present a mechanism for defining features using texture volumes, and methods for computing correct normals for the deformed volume in respect to different alignments. We describe a GPU-based implementation to achieve real-time performance of the techniques and a collection of manipulation operators including peelers, retractors, pliers and dilators which are adaptations of the metaphors and tools used in surgical procedures and medical illustrations. Our approach is directly applicable in medical and biological illustration, and we demonstrate how it works as an interactive tool for focus+context visualization, as well as a generic technique for volume graphics",
Optimizing bandwidth limited problems using one-sided communication and overlap,"This paper demonstrates the one-sided communication used in languages like UPC can provide a significant performance advantage for bandwidth-limited applications. This is shown through communication microbenchmarks and a case-study of UPC and MPI implementations of the NAS FT benchmark. Our optimizations rely on aggressively overlapping communication with computation, alleviating bottlenecks that typically occur when communication is isolated in a single phase. The new algorithms send more and smaller messages, yet the one-sided versions achieve > 1.9times speedup over the base Fortran/MPI. Our one-sided versions show an average 15% improvement over the two-sided versions, due to the lower software overhead of onesided communication, whose semantics are fundamentally lighter-weight than message passing. Our UPC results use Berkeley UPC with GASNet and demonstrate the scalability of that system, with performance approaching 0.5 TFlop/s on the FT benchmark with 512 processors",
A systematic approach to design and operation of disassembly lines,"This paper presents a systematic approach to the disassembly line (DL) design in meeting the requirement of variant orders for multiple used parts with different due dates. An extended disassembly Petri net model is proposed for the hierarchical modeling in order to derive the disassembly path with the maximal benefit in the presence of some defective components. An algorithm for balancing DLs to maximize the productivity of a disassembly system is presented. The results of simulation runs of the proposed methodology and algorithms applied to a simplified personal computer disassembly are provided. This work lays a foundation for designing efficient industrial automatic and semiautomatic disassembly systems. Note to Practitioners-Disassembly is rapidly growing in importance as manufacturers face increasing pressure to deal with obsolete products in an environmentally responsible and economically sound manner. This process can be performed at a single workstation or on a disassembly line (DL) that is organized as a sequence of workstations, each with one or more machines/operators to handle a certain type of disassembly task. Compared to a single workstation, DL provides higher productivity and greater potential for disassembly automation. However, it still faces serious scheduling and inventory problems because of a high degree of uncertainty in discarded products and disparity between demands for certain parts and their yield from disassembly. To address this challenge, this paper proposes a two-level systematic approach, aiming to maximize system throughput and system revenue by dynamically configuring the disassembly system into many DLs, while considering line balance, different process flows, and meeting different order due dates. The research results can help engineers build better disassembly systems.",
Improving the performance of switched diversity with post-examining selection,"Switched diversity offers one of the lowest complexity solution for fading mitigation. The receiver employing switched combining schemes. seeks and uses an acceptable diversity path for data reception. However, when no acceptable path is found after all paths have been examined, the receiver ends up randomly choosing an unacceptable path. In this paper, we consider the performance improvement of switched combining schemes. In particular, we modify conventional multi-branch switch and examine combining (SEC) schemes in a way that the best path is selected when no acceptable path is found after all paths have been examined. We carry out a thorough performance versus complexity tradeoff study on the resulting scheme: SEC with post-examining selection and illustrate the study with some selected numerical examples.","Diversity reception,
Switches,
Fading,
Communication switching,
Performance analysis,
Wireless communication,
Signal to noise ratio,
Computer science education,
Cities and towns,
Diversity methods"
Coordinated Multiagent Teams and Sliding Autonomy for Large-Scale Assembly,"Recent research in human-robot interaction has investigated the concept of Sliding, or Adjustable, Autonomy, a mode of operation bridging the gap between explicit teleoperation and complete robot autonomy. This work has largely been in single-agent domains-involving only one human and one robot-and has not examined the issues that arise in multiagent domains. We discuss the issues involved in adapting Sliding Autonomy concepts to coordinated multiagent teams. In our approach, remote human operators have the ability to join, or leave, the team at will to assist the autonomous agents with their tasks (or aspects of their tasks) while not disrupting the team's coordination. Agents model their own and the human operator's performance on subtasks to enable them to determine when to request help from the operator. To validate our approach, we present the results of two experiments. The first evaluates the human/multirobot team's performance under four different collaboration strategies including complete teleoperation, pure autonomy, and two distinct versions of Sliding Autonomy. The second experiment compares a variety of user interface configurations to investigate how quickly a human operator can attain situational awareness when asked to help. The results of these studies support our belief that by incorporating a remote human operator into multiagent teams, the team as a whole becomes more robust and efficient","Large-scale systems,
Robotic assembly,
Robot kinematics,
Computer science,
Robustness,
Human robot interaction,
Robot sensing systems,
Orbital robotics,
Solar energy,
Autonomous agents"
"Facial Expression Recognition using Advanced Local Binary Patterns, Tsallis Entropies and Global Appearance Features","This paper proposes a novel facial expression recognition approach based on two sets of features extracted from the face images: texture features and global appearance features. The first set is obtained by using the extended local binary patterns in both intensity and gradient maps and computing the Tsallis entropy of the Gabor filtered responses. The second set of features is obtained by performing null-space based linear discriminant analysis on the training face images. The proposed method is evaluated by extensive experiments on the JAFFE database, and compared with two widely used facial expression recognition approaches. Experimental results show that the proposed approach maintains high recognition rate in a wide range of resolution levels and outperforms the other alternative methods.","Face recognition,
Pattern recognition,
Entropy,
Image recognition,
Feature extraction,
Data mining,
Linear discriminant analysis,
Image databases,
Spatial databases,
Image resolution"
Integrating Semantic and Syntactic Descriptions to Chain Geographic Services,"Integrating multiple geographic services from different information communities and spatiolinguistic regions is challenging because of its inherent complexity and heterogeneity. A geographic information systems workflow approach can use semantic and syntactic service descriptions to form service chains that can integrate service discovery, composition, and reuse. Service chaining links remote geographic services to help expert users form complex geoprocessing services and perform timely analysis of geodata. This method facilitates the use of XML-based service description languages to build a geoservice-reuse architecture based on common ontologies and shared service descriptions","Geographic Information Systems,
Concrete,
Data visualization,
Joining processes,
Geoscience,
Information analysis,
Performance analysis,
Ontologies,
Web services,
Production facilities"
Non-Rigid Metric Shape and Motion Recovery from Uncalibrated Images Using Priors,In this paper we focus on the estimation of the 3D Euclidean shape and motion of a non-rigid object which is moving rigidly while deforming and is observed by a perspective camera. Our method exploits the fact that it is often a reasonable assumption that some of the points are deforming throughout the sequence while others remain rigid. First we use an automatic segmentation algorithm to identify the set of rigid points which in turn is used to estimate the internal camera calibration parameters and the overall rigid motion. Finally we formalise the problem of non-rigid shape estimation as a constrained non-linear minimization adding priors on the degree of deformability of each point. We perform experiments on synthetic and real data which show firstly that even when using a minimal set of rigid points it is possible to obtain reliable metric information and secondly that the shape priors help to disambiguate the contribution to the image motion caused by the deformation and the perspective distortion.,
Allocating non-real-time and soft real-time jobs in multiclusters,"This paper addresses workload allocation techniques for two types of sequential jobs that might be found in multicluster systems, namely, non-real-time jobs and soft real-time jobs. Two workload allocation strategies, the optimized mean response time (ORT) and the optimized mean miss rate (OMR), are developed by establishing and numerically solving two optimization equation sets. The ORT strategy achieves an optimized mean response time for non-real-time jobs, while the OMR strategy obtains an optimized mean miss rate for soft real-time jobs over multiple clusters. Both strategies take into account average system behaviors (such as the mean arrival rate of jobs) in calculating the workload proportions for individual clusters and the workload allocation is updated dynamically when the change in the mean arrival rate reaches a certain threshold. The effectiveness of both strategies is demonstrated through theoretical analysis. These strategies are also evaluated through extensive experimental studies and the results show that when compared with traditional strategies, the proposed workload allocation schemes significantly improve the performance of job scheduling in multiclusters, both in terms of the mean response time (for non-real-time jobs) and the mean miss rate (for soft real-time jobs).",
The partitioned scheduling of sporadic tasks according to static-priorities,"A polynomial-time algorithm is presented for partitioning a collection of sporadic tasks among the processors of an identical multiprocessor platform with static-priority scheduling on each individual processor. Since the partitioning problem is easily seen to be NP-hard in the strong sense, this algorithm is not optimal. A quantitative characterization of its worst-case performance is provided in terms of sufficient conditions and resource augmentation approximation bounds. The partitioning algorithm is also evaluated over randomly generated task systems",
Flexible Design of Frequency Reuse Factor in OFDMA Cellular Networks,"The OFDMA systems are emerging for future cellular networks. Creating multiple data channels, they can support the flexible frequency reuse factor (FRF). Although FRF 1 is the best choice in terms of cell throughput, it causes intercell interference at the cell boundary, thereby being unable to serve the whole cell area. Therefore it was proposed to use the FRF of greater than 3. In this paper, we develop a flexible FRF design mechanism that provides an intermediate value between 1 and 3 while the conventional schemes are dedicated to use some integer numbers only such as 3, 4, or 7. In our design, if the number of shared channels between any neighboring cells is given, we implement it simply according to a difference set. Simulation results show that a FRF of 7/4 achieves better throughput than FRF 3 and overcomes the intercell interference problem of FRF 1, so it can replace the conventional FRF such as 3. We expect that our new FRFs have the advantage in supporting smooth handoff because there are always some common channels between two neighboring cells.",
"Non-cooperative, semi-cooperative, and cooperative games-based grid resource allocation","In this paper we consider, compare and analyze three game theoretical grid resource allocation mechanisms. Namely, 1) the non-cooperative sealed-bid method where tasks are auctioned off to the highest bidder, 2) the semi-cooperative n-round sealed-bid method in which each site delegate its work to others if it cannot perform the work itself, and 3) the cooperative method in which all of the sites deliberate with one another to execute all the tasks as efficiently as possible. To experimentally evaluate the above mentioned techniques, we perform extensive simulation studies that effectively encapsulate the task and machine heterogeneity. The tasks are assumed to be independent and bear multiple execution time deadlines. The simulation model is built around a hierarchical grid infrastructure where machines are abstracted into larger computing centers labeled ""federations"", each of which are responsible for managing their own resources independently. These federations are then linked together with a primary portal to which grid tasks would be submitted. To measure the effectiveness of these game theoretical techniques, the recorded performance is evaluated against a conventional baseline method in which tasks are randomly assigned to the sites without any task execution guarantee",
Minimum Bounded Degree Spanning Trees,"We consider the minimum cost spanning tree problem under the restriction that all degrees must be at most a given value k. We show that we can efficiently find a spanning tree of maximum degree at most k+2 whose cost is at most the cost of the optimum spanning tree of maximum degree at most k. This is almost best possible. The approach uses a sequence of simple algebraic, polyhedral and combinatorial arguments. It illustrates many techniques and ideas in combinatorial optimization as it involves polyhedral characterizations, uncrossing, matroid intersection, and graph orientations (or packing of spanning trees). The result generalizes to the setting where every vertex has both upper and lower bounds and gives then a spanning tree which violates the bounds by at most two units and whose cost is at most the cost of the optimum tree. It also gives a better understanding of the subtour relaxation for both the symmetric and asymmetric traveling salesman problems. The generalization to l-edge-connected subgraphs is briefly discussed",
Scheduling multiple DAGs onto heterogeneous systems,"The problem of scheduling a single DAG onto heterogeneous systems has been studied extensively. In this paper, we focus on the problem of scheduling more than one DAG at the same time onto a set of heterogeneous resources. The aim is not only to optimize the overall makespan, but also to achieve fairness, defined on the basis of the slowdown that each DAG would experience as a result of competing for resources with other DAGs. Two policies particularly focussing to deliver fairness are presented and evaluated along with another four policies that can be used to schedule multiple DAGs.",
RFID Application in Hospitals: A Case Study on a Demonstration RFID Project in a Taiwan Hospital,"After manufacturing and retail marketing, healthcare is considered the next home for Radio Frequency Identification (RFID). Although in its infancy, RFID technology has great potential in healthcare to significantly reduce cost, and improve patient safety and medical services. However, the challenge will be to incorporate RFID into medical practice, especially when relevant experience is limited. To explore this issue, we conducted a case study that demonstrated RFID integration into the medical world at one Taiwan hospital. The project that was studied, the Location-based Medicare Service project, was partially subsidized by the Taiwan government and was aimed at containing SARS, a dangerous disease that struck Taiwan in 2003. Innovative and productive of several significant results, the project established an infrastructure and platform allowing for other applications. In this paper we describe the development strategy, design, and implementation of the project. We discuss our findings pertaining to the collaborative development strategy, device management, data management, and value generation. These findings have important implications for developing RFID applications in healthcare organizations.",
Improving the Face Recognition Grand Challenge Baseline Performance using Color Configurations Across Color Spaces,"This paper presents a method that applies color information to improve face recognition performance of the face recognition grand challenge (FRGC) baseline algorithm, also known as the biometric experimentation environment (BEE) baseline algorithm. In particular, we empirically assess the face recognition performance of the BEE baseline algorithm by applying color configurations in the YIQ and the YCbCr color spaces. The color configuration is defined as an individual or a combination of color component images. Experimental results using an FRGC ver1.0 dateset containing 1,126 images demonstrate that the YQCr color configuration improves the rank-one face recognition rate of the BEE baseline algorithm from 37% to 70%; when experimenting with an FRGC ver2.0 dataset consisting of 30,702 images, the YQCr color configuration achieves 65% verification rate comparing to the FRGC baseline performance of 12%.",
Mining Aspects from Version History,"Aspect raining identifies cross-culling concerns in a program, to help migrating it to an aspect-oriented design. Such concerns may not exist from the beginning, but emerge over time. By analysing where developers add code to a program, our history-based aspect mining (BAM) identifies and ranks cross-cutting concerns. We evaluated the effectiveness of our approach with the history of three open-source projects. BAM scales up to Industrial-sized projects: for example, we were able to identify a locking concern that cross-cuts 1284 methods in Eclipse. Additionally, the precision of HAM increases with project size and history: for Eclipse, it reaches 90% for the top-10 candidates",
Jellyfish: A conceptual model for the as Internet topology,"Several novel concepts and tools have revolutionized our understanding of the Internet topology. Most of the existing efforts attempt to develop accurate analytical models. In this paper, our goal is to develop an effective conceptual model: A model that can be easily drawn by hand, while at the same time, it captures significant macroscopic properties. We build the foundation for our model with two thrusts: a) We identify new topological properties and b) we provide metrics to quantify the topological importance of a node. We propose the jellyfish as a model for the inter-domain Internet topology. We show that our model captures and represents the most significant topological properties. Furthermore, we observe that the jellyfish has lasting value: It describes the topology for more than six years.","Topology,
Internet topology,
Measurement,
Network topology,
Correlation,
Generators"
A New Approach to Persian/Arabic Text Steganography,"Conveying information secretly and establishing hidden relationship has been of interest since long past. Text documents have been widely used since very long time ago. Therefore, we have witnessed different method of hiding information in texts (text steganography) since past to the present. In this paper we introduce a new approach for steganography in Persian and Arabic texts. Considering the existence of too many points in Persian and Arabic phrases, in this approach, by vertical displacement of the points, we hide information in the texts. This approach can be categorized under feature coding methods. This method can be used for Persian/Arabic watermarking. Our method has been implemented by Java programming language","Steganography,
Information security,
Image coding,
Watermarking,
Cryptography,
Computer science,
Java,
Computer languages,
Image processing,
Pattern recognition"
Min-Cost Selfish Multicast With Network Coding,"The single-source min-cost multicast problem, which can be framed as a convex optimization problem with the use of network codes and convex increasing edge costs is considered. A decentralized approach to this problem is presented by Lun, Ratnakar for the case where all users cooperate to reach the global minimum. Further, the cost for the scenario where each of the multicast receivers greedily routes its flows is analyzed and the existence of a Nash equilibrium is proved. An allocation rule by which edge cost at each edge is allocated to flows through that edge is presented. We prove that under our pricing rule, the flow cost at user equilibrium is the same as the min-cost. This leads to the construction of a selfish flow-steering algorithm for each receiver, which is also globally optimal. Further, the algorithm is extended for completely distributed flow adaptation at nodes in the network to achieve globally minimal cost in steady state. Analogous results are also presented for the case of multiple multicast sessions",
Symmetry Compensation using a H-Bridge Multilevel STATCOM with Zero Sequence Injection,"This paper examines the use of a H-bridge cascade multilevel STATCOM for symmetry compensation. One of the particular problems H-bridge based STATCOMs have when used in these applications is maintaining correct voltages on the H-bridge capacitors for each of the individual phases of the STATCOM. This difficulty is the result of average real power flowing in or out of the individual phase legs. A solution has already been published for the delta connected STATCOM, but has not, until this paper, been solved for the wye connected topology. This paper uses a new approach to solve the phase leg power balance problem using zero sequence current and voltage injection. It allows the individual phase voltages for both the delta and wye connected cascaded H-bridge STATCOM to be controlled. Furthermore, when implemented for the delta connected STATCOM it leads to a more elegant and parameter independent control system architecture",
STEP-NC and function blocks for interoperable manufacturing,"Interoperable manufacturing systems help manufacturing companies stay competitive in the environment of frequent and unpredictable market changes. An important part of a manufacturing system is computer numerically controlled (CNC) machine tools. Over the years, G-codes have been extensively used by CNC machine tools and are now considered as a bottleneck for making these machines adaptable and interoperable. Two new technologies emerged in recent years: Standard for the Exchange of Product data for Numerical Control (STEP-NC) and function blocks. The STEP-NC data model represents a common standard for NC programming, making the goal of a generic NC code generation facility a reality. Function blocks are an emerging IEC standard for distributed industrial processes and control systems. They can be used for CNC controls to encapsulate machining data, such as machining features and their needed algorithms. This paper introduces the above two new standards and the technologies that are developed based on the standards. The main body is devoted to analyze the standards from the functionality viewpoint. These functionalities include, bidirectional information flow in computer-aided design/computer-aided manufacturing, data sharing over the Internet, the use of feature-based machining concept, modularity and reusability, intelligent and autonomous CNC, and portability among resources. Some implementations are also presented to showcase how the standards are used to develop technologies for interoperable machining. Note to Practitioners-Modern computer numerically controlled (CNC) machine tools are limited in functions because their controllers rely on G-codes for communications. G-code is considered a ""dumb"" language as it only documents instructional and procedural data, leaving most of the design information behind. G-code programs are also hardware dependent, denying modern CNC machine tools desired interoperability and portability. In recent years, two new standards emerged, STEP-NC and function blocks. They may hold the key to empowering CNC machine tools with richer information which, in turn, gives CNC machine tools the ability to ""think"" intelligently and to be interoperable. This paper introduces these two standards, the technologies that have been developed based on the standards and some prototype systems using the standards and technologies. The intention is not to highlight any achieved research outcome. Instead, the focus is on informing the research and practical world about these new standards, analyzing them from the viewpoint of supporting interoperable CNC machine tools, and offering some futuristic views about these standards and technologies. While these standards are still in their infancy, research activities and prototype systems are already coming thick and fast. There seems to be a ""healthy"" mixture of participants working in the field. They range from the manufacturers of all systems related to the data interface (i.e., CAM systems, controls, and machine tools), to the users and academic institutions.","Computer numerical control,
Machine tools,
Standards development,
Machining,
Computer aided manufacturing,
Control systems,
Manufacturing systems,
Machine intelligence,
Communication system control,
Prototypes"
Fault-Tolerance in Sensor Networks: A New Evaluation Metric,,
Estimation of Velocity Vectors in Synthetic Aperture Ultrasound Imaging,"A method for determining both velocity magnitude and angle in a synthetic aperture ultrasound system is described. The approach uses directional beamforming along the flow direction and cross correlation to determine velocity magnitude. The angle of the flow is determined from the maximum normalized correlation calculated as a function of angle. This assumes the flow direction is within the imaging plane. Simulations of the angle estimation method show both biases and standard deviations of the flow angle estimates below 3deg for flow angles from 20deg to 90deg (transverse flow). The method is also investigated using data measured by an experimental ultrasound scanner from a flow rig. A commercial 128 element 7-MHz linear array transducer is used, and data are measured for flow angles of 60deg and 90deg. Data are acquired using the RASMUS experimental ultrasound scanner, which samples 64 channels simultaneously. A 20-mus chirp was used during emission and eight virtual transmit sources were created behind the transducer using 11 transmitting elements. Data from the eight transmissions are beamformed and coherently summed to create high-resolution lines at different angles for a set of points within the region of flow. The velocity magnitude is determined with a precision of 0.36% (60deg) and 1.2% (90deg), respectively. The 60deg angle is estimated with a bias of 0.54deg and a standard deviation of 2.1deg. For 90deg the bias is 0.0003deg and standard deviation 1.32deg. A parameter study with regard to correlation length and number of emissions is performed to reveal the accuracy of the method. Real time data covering 2.2 s of the carotid artery of a healthy 30-year-old male volunteer is acquired and then processed offline using a computer cluster. The direction of flow is estimated using the above mentioned method. It is compared to the flow angle of 106deg with respect to the axial direction, determined visually from the B-mode image. For a point in the center of the common carotid artery, 76% of the flow angle estimates over the 2.2 s were within 10deg of the visually determined flow angle. The standard deviation of these estimates was below 2.7deg. Full color flow maps from different parts of the cardiac cycle are presented, including vector arrows indicating both estimated flow direction and velocity magnitude",
Analysis of Long-Running Replicated Systems,,
Data-Driven Multithreading Using Conventional Microprocessors,"This paper describes the data-driven multithreading (DDM) model and how it may be implemented using off-the-shelf microprocessors. Data-driven multithreading is a nonblocking multithreading execution model that tolerates internode latency by scheduling threads for execution based on data availability. Scheduling based on data availability can be used to exploit cache management policies that reduce significantly cache misses. Such policies include firing a thread for execution only if its data is already placed in the cache. We call this cache management policy the CacheFlow policy. The core of the DDM implementation presented is a memory mapped hardware module that is attached directly to the processor's bus. This module is responsible for thread scheduling and is known as the thread synchronization unit (TSU). The evaluation of DDM was performed using simulation of the data-driven network of workstations (D2NOW). D2NOW is a DDM implementation built out of regular workstations augmented with the TSU. The simulation was performed for nine scientific applications, seven of which belong to the SPLASH-2 suite. The results show that DDM can tolerate well both the communication and synchronization latency. Overall, for 16 and 32-node D2NOW machines the speedup observed was 14.4 and 26.0, respectively",
Control Plane for Advance Bandwidth Scheduling in Ultra High-Speed Networks,"A control-plane architecture for supporting advance reservation of dedicated bandwidth channels on a switched network infrastructure is described including the front-end web interface, user and token management scheme, bandwidth scheduler, and signaling daemon. A path computation algorithm for bandwidth scheduling is proposed based on an extension of Bellman-Ford algorithm to an algebraic structure on sequences of disjoint non-negative real intervals. An implementation of this architecture for UltraScience Net is briefly described.","Bandwidth,
High-speed networks,
Processor scheduling,
Scheduling algorithm,
Multiprotocol label switching,
Computer science,
Computer architecture,
Supercomputers,
Remote monitoring,
Circuits"
Segment-based streaming media proxy: modeling and optimization,"Researchers often use segment-based proxy caching strategies to deliver streaming media by partially caching media objects. The existing strategies mainly consider increasing the byte hit ratio and/or reducing the client perceived startup latency (denoted by the metric delayed startup ratio). However, these efforts do not guarantee continuous media delivery because the to-be-viewed object segments may not be cached in the proxy when they are demanded. The potential consequence is playback jitter at the client side due to proxy delay in fetching the uncached segments, which we call proxy jitter. Thus, for the best interests of clients, a correct model for streaming proxy system design should aim to minimize proxy jitter subject to reducing the delayed startup ratio and increasing the byte hit ratio. However, we have observed two major pairs of conflicting interests inherent in this model: (1) one between improving the byte hit ratio and reducing proxy jitter, and (2) the other between improving the byte hit ratio and reducing the delayed startup ratio. In this study, we first propose and analyze prefetching methods for in-time prefetching of uncached segments, which provides insights into the first pair of conflicting interests. Second, to address the second pair of the conflicting interests, we build a general model to analyze the performance tradeoff between the second pair of conflicting performance objectives. Finally, considering our main objective of minimizing proxy jitter and optimizing the two tradeoffs, we propose a new streaming proxy system called Hyper Proxy. Synthetic and real workloads are used to evaluate our system. The performance results show that Hyper Proxy generates minimum proxy jitter with a low delayed startup ratio and a small decrease of byte hit ratio compared with existing schemes.",
Human Carrying Status in Visual Surveillance,"A person’s gait changes when he or she is carrying an object such as a bag, suitcase or rucksack. As a result, human identification and tracking are made more difficult because the averaged gait image is too simple to represent the carrying status. Therefore, in this paper we first introduce a set of Gabor based human gait appearance models, because Gabor functions are similar to the receptive field profiles in the mammalian cortical simple cells. The very high dimensionality of the feature space makes training difficult. In order to solve this problem we propose a general tensor discriminant analysis (GTDA), which seamlessly incorporates the object (Gabor based human gait appearance model) structure information as a natural constraint. GTDA differs from the previous tensor based discriminant analysis methods in that the training converges. Existing methods fail to converge in the training stage. This makes them unsuitable for practical tasks. Experiments are carried out on the USF baseline data set to recognize a human’s ID from the gait silhouette. The proposed Gabor gait incorporated with GTDA is demonstrated to significantly outperform the existing appearance-based methods.","Humans,
Surveillance,
Tensile stress,
Computer vision,
Linear discriminant analysis,
Computer science,
Information analysis,
Signal processing algorithms,
Pattern recognition,
Information systems"
Bounding Preemption Delay within Data Cache Reference Patterns for Real-Time Tasks,"Caches have become invaluable for higher-end architectures to hide, in part, the increasing gap between processor speed and memory access times. While the effect of caches on timing predictability of single real-time tasks has been the focus of much research, bounding the overhead of cache warm-ups after preemptions remains a challenging problem, particularly for data caches. In this paper, we bound the penalty of cache interference for real-time tasks by providing accurate predictions of the data cache behavior across preemptions. For every task, we derive data cache reference patterns for all scalar and non-scalar references. Partial timing of a task is performed up to a preemption point using these patterns. The effects of cache interference are then analyzed using a settheoretic approach, which identifies the number and location of additional misses due to preemption. A feedback mechanism provides the means to interact with the timing analyzer, which subsequently times another interval of a task bounded by the next preemption. Our experimental results demonstrate that it is sufficient to consider the n most expensive preemption points, where n is the maximum possible number of preemptions. Further, it is shown that such accurate modeling of data cache behavior in preemptive systems significantly improves the WCET predictions for a task. To the best of our knowledge, our work of bounding preemption delay for data caches is unprecedented.","Timing,
Interference,
Delay effects,
Upper bound,
Data analysis,
Equations,
Real time systems,
Computer science,
Embedded system,
Computer architecture"
3-D/2-D registration by integrating 2-D information in 3-D,"In image-guided therapy, high-quality preoperative images serve for planning and simulation, and intraoperatively as ""background"", onto which models of surgical instruments or radiation beams are projected. The link between a preoperative image and intraoperative physical space of the patient is established by image-to-patient registration. In this paper, we present a novel 3-D/2-D registration method. First, a 3-D image is reconstructed from a few 2-D X-ray images and next, the preoperative 3-D image is brought into the best possible spatial correspondence with the reconstructed image by optimizing a similarity measure (SM). Because the quality of the reconstructed image is generally low, we introduce a novel SM, which is able to cope with low image quality as well as with different imaging modalities. The novel 3-D/2-D registration method has been evaluated and compared to the gradient-based method (GBM) using standardized evaluation methodology and publicly available 3-D computed tomography (CT), 3-D rotational X-ray (3DRX), and magnetic resonance (MR) and 2-D X-ray images of two spine phantoms, for which gold standard registrations were known. For each of the 3DRX, CT, or MR images and each set of X-ray images, 1600 registrations were performed from starting positions, defined as the mean target registration error (mTRE), randomly generated and uniformly distributed in the interval of 0-20 mm around the gold standard. The capture range was defined as the distance from gold standard for which the final TRE was less than 2 mm in at least 95% of all cases. In terms of success rate, as the function of initial misalignment and capture range the proposed method outperformed the GBM. TREs of the novel method and the GBM were approximately the same. For the registration of 3DRX and CT images to X-ray images as few as 2-3 X-ray views were sufficient to obtain approximately 0.4 mm TREs, 7-9 mm capture range, and 80%-90% of successful registrations. To obtain similar results for MR to X-ray registrations, an image, reconstructed from at least 11 X-ray images was required. Reconstructions from more than 11 images had no effect on the registration results.","X-ray imaging,
Image reconstruction,
Computed tomography,
Gold,
Samarium,
Medical treatment,
Surgical instruments,
Image quality,
Optical imaging,
Magnetic resonance imaging"
The Delicate Tradeoffs in BitTorrent-like File Sharing Protocol Design,"The BitTorrent (BT) file sharing protocol is popular due to its scalability property and the incentive mechanism to reduce free-riding. However, in designing such P2P file sharing protocols, there is a fundamental ""tussle"" between keeping peers, specially the more resourceful ones, in the system for as long as possible to help the system achieve better performance and allowing peers finish their download as quickly as possible. The current BT protocol represents only ""one"" possible implementation in this whole design spectrum. In this paper, we characterize the ""complete"" design space of BT-like protocols. We use fairness index to measure the fairness that incorporates the contribution peers make. We show that there is a wide range of design choices, ranging from optimizing the performance of file download, to optimizing the fairness measure. More importantly, we show that there is a simple and easily implementable design knob which can be used to choose a particular operating point in the design space. We then discuss different algorithms (centralized versus distributed) in realizing the design knob. We also carry out performance evaluation to quantify the merits and properties of the BT-like file sharing protocols.","Peer to peer computing,
Computer science,
Design optimization,
Algorithm design and analysis,
Access protocols,
Performance analysis,
Scalability,
Mechanical factors,
Internet,
Throughput"
CASA and LEAD: adaptive cyberinfrastructure for real-time multiscale weather forecasting,"Two closely linked projects aim to dramatically improve storm forecasting speed and accuracy. CASA is creating a distributed, collaborative, adaptive sensor network of low-power, high-resolution radars that respond to user needs. LEAD offers dynamic workflow orchestration and data management in a Web services framework designed to support on-demand, real-time, dynamically adaptive systems",
Simulation of tissue atrophy using a topology preserving transformation model,"We propose a method to simulate atrophy and other similar volumetric change effects on medical images. Given a desired level of atrophy, we find a dense warping deformation that produces the corresponding levels of volumetric loss on the labeled tissue using an energy minimization strategy. Simulated results on a real brain image indicate that the method generates realistic images of tissue loss. The method does not make assumptions regarding the mechanics of tissue deformation, and provides a framework where a pre-specified pattern of atrophy can readily be simulated. Furthermore, it provides exact correspondences between images prior and posterior to the atrophy that can be used to evaluate provisional image registration and atrophy quantification algorithms.",
Image Denoising with Shrinkage and Redundant Representations,"Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signal’s representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives.",
Semantic Interoperability of Web Services - Challenges and Experiences,"With the rising popularity of Web services, both academia and industry have invested considerably in Web service description standards, discovery, and composition techniques. The standards based approach utilized by Web services has supported interoperability at the syntax level. However, issues of structural and semantic heterogeneity between messages exchanged by Web services are far more complex and crucial to interoperability. It is for these reasons that we recognize the value that schema/data mappings bring to Web service descriptions. In this paper, we examine challenges to interoperability; classify the types of heterogeneities that can occur between interacting services and present a possible solution for data mediation using the mapping support provided by WSDL-S, the extensibility features of WSDL and the popular SOAP engine, Axis 2","Web services,
Mediation,
Databases,
Service oriented architecture,
Semantic Web,
Computer science,
Computer industry,
Simple object access protocol,
Engines,
Impedance"
Towards SET Mitigation in RF Digital PLLs: From Error Characterization to Radiation Hardening Considerations,"In this work, the characteristics of single-event transient (SET) generation and propagation are analyzed in a digital phase-locked loop (DPLL) circuit, designed to achieve speeds applicable to mixed-signal RF operations. The analysis shows that the sensitivity of a DPLL system is strongly dependent on which of its modules is subjected to ionizing radiation. Computer simulations of single-event transients on the phase-frequency detector module and the voltage-controlled oscillator module indicate that their radiation responses have a negligible impact on the DPLL normal operations. More importantly, our findings identify the sensitivity of the charge pump module as the dominant contributor to the radiation vulnerability of the DPLL system. The charge pump incorporates a design element that is favorable to achieving high-speed operations, but simultaneously introduces a circuit configuration that can be easily perturbed by the impact of a heavy-ion. Hardening by design techniques that could be used to mitigate this issue are also discussed",
Pattern Mining in Frequent Dynamic Subgraphs,"Graph-structured data is becoming increasingly abundant in many application domains. Graph mining aims at finding interesting patterns within this data that represent novel knowledge. While current data mining deals with static graphs that do not change over time, coming years will see the advent of an increasing number of time series of graphs. In this article, we investigate how pattern mining on static graphs can be extended to time series of graphs. In particular, we are considering dynamic graphs with edge insertions and edge deletions over time. We define frequency in this setting and provide algorithmic solutions for finding frequent dynamic subgraph patterns. Existing subgraph mining algorithms can be easily integrated into our framework to make them handle dynamic graphs. Experimental results on real-world data confirm the practical feasibility of our approach.",
You Are Who You Talk To: Detecting Roles in Usenet Newsgroups,"Understanding the social roles of the members a group can help to understand the social context of the group. We present a method of applying social network analysis to support the task of characterizing authors in Usenet newsgroups. We compute and visualize networks created by patterns of replies for each author in selected newsgroups and find that second-degree ego-centric networks give us clear distinctions between different types of authors and newsgroups. Results show that newsgroups vary in terms of the populations of participants and the roles that they play. Newsgroups can be characterized by populations that include question and answer newsgroups, conversational newsgroups, social support newsgroups, and flame newsgroups. This approach has applications for both researchers seeking to characterize different types of social cyberspaces as well as participants seeking to distinguish interaction partners and content authors.",
Rough Set Method Based on Multi-Granulations,"The original rough set model is concerned primarily with the approximation of sets described by single binary relation on universe. In the view of granular computing, classical rough set theory is researched by single granulation (static granulation). The article extends the Pawlak rough set model to rough set model based on multi-granulations MGRS, where the set approximations are defined by using multi-equivalences on the universe. Mathematical properties of MGRS are investigated. It is shown that some properties of Pawlak rough set are special instances of MGRS, approximation measure of set described by using multi-granulations is always better than by using single granulation, which is suitable for describing more accurately the concept and solving problem according to user requirement",
Dense Photometric Stereo: A Markov Random Field Approach,"We address the problem of robust normal reconstruction by dense photometric stereo, in the presence of complex geometry, shadows, highlight, transparencies, variable attenuation in light intensities, and inaccurate estimation in light directions. The input is a dense set of noisy photometric images, conveniently captured by using a very simple set-up consisting of a digital video camera, a reflective mirror sphere, and a handheld spotlight. We formulate the dense photometric stereo problem as a Markov network and investigate two important inference algorithms for Markov random fields (MRFs) - graph cuts and belief propagation - to optimize for the most likely setting for each node in the network. In the graph cut algorithm, the MRF formulation is translated into one of energy minimization. A discontinuity-preserving metric is introduced as the compatibility function, which allows a-expansion to efficiently perform the maximum a posteriori (MAP) estimation. Using the identical dense input and the same MRF formulation, our tensor belief propagation algorithm recovers faithful normal directions, preserves underlying discontinuities, improves the normal estimation from one of discrete to continuous, and drastically reduces the storage requirement and running time. Both algorithms produce comparable and very faithful normals for complex scenes. Although the discontinuity-preserving metric in graph cuts permits efficient inference of optimal discrete labels with a theoretical guarantee, our estimation algorithm using tensor belief propagation converges to comparable results, but runs faster because very compact messages are passed and combined. We present very encouraging results on normal reconstruction. A simple algorithm is proposed to reconstruct a surface from a normal map recovered by our method. With the reconstructed surface, an inverse process, known as relighting in computer graphics, is proposed to synthesize novel images of the given scene under user-specified light source and direction. The synthesis is made to run in real time by exploiting the state-of-the-art graphics processing unit (GPU). Our method offers many unique advantages over previous relighting methods and can handle a wide range of novel light sources and directions","Photometry,
Markov random fields,
Inference algorithms,
Image reconstruction,
Belief propagation,
Tensile stress,
Layout,
Surface reconstruction,
Computer graphics,
Light sources"
Visualizing the underwater behavior of humpback whales,"A new collaboration between visualization experts, engineers, and marine biologists has changed. For the first time, we can see and study the foraging behavior of humpback whales. Our study's primary objective was furthering the science of marine mammal ethology. We also had a second objective: field testing GeoZui4D, an innovative test-bench for investigate effective ways of navigating through time-varying geospatial data","Visualization,
Whales,
Boats,
Navigation,
Mice,
Animals,
Layout,
Watches,
Digital recording,
Underwater acoustics"
Continuous MOSFET performance increase with device scaling: The role of strain and channel material innovations,"A simple model that links MOSFET performance, in the form of intrinsic switch delay, to effective carrier velocity in the channel is developed and fitted to historical data. It is shown that nearly continuous carrier velocity increase, most recently via the introduction of process-induced strain, has been responsible for the device performance increase commensurately with dimensional scaling. The paper further examines channel material innovations that will be required in order to maintain continued commensurate scaling beyond what can be achieved with process-induced strain, and discusses some of the technological tradeoffs that will have to be faced for their introduction.",
Evolving robust and specialized car racing skills,"Neural network-based controllers arc evolved for racing simulated R/C cars around several tracks of varying difficulty. The transferability of driving skills acquired when evolving for a single track is evaluated, and different ways of evolving controllers able to perform well on many different tracks are investigated, ft is further shown that such generally proficient controllers can reliably be developed into specialized controllers for individual tracks. Evolution of sensor parameters together with network weights is shown to lead to higher final fitness, but only if turned on after a general controller is developed, otherwise it hinders evolution, ft is argued that simulated car racing is a scalable and relevant testbed for evolutionary robotics research, and that the results of this research can be useful for commercial computer games.","Robustness,
Robot sensing systems,
Robotics and automation,
Computational modeling,
Computer simulation,
Automatic control,
Computer science,
Path planning,
Intelligent robots,
Evolutionary computation"
Investigating Spatial Relationships in Human-Robot Interaction,"Co-presence and embodied interaction are two fundamental characteristics of the command and control situation for service robots. This paper presents a study of spatial distances and orientation of a robot with respect to a human user in an experimental setting. Relevant concepts of spatiality from social interaction studies are introduced and related to human-robot interaction (HRI). A Wizard-of-Oz study quantifies the observed spatial distances and spatial formations encountered. However, it is claimed that a simplistic parameterization and measurement of spatial interaction misses the dynamic character and might be counter-productive in the design of socially appropriate robots",
Generation and visualization of four-dimensional MR angiography data using an undersampled 3-D projection trajectory,"Time-resolved contrast-enhanced magnetic resonance (MR) angiography (CE-MRA) has gained in popularity relative to X-ray Digital Subtraction Angiography because it provides three-dimensional (3-D) spatial resolution and it is less invasive. We have previously presented methods that improve temporal resolution in CE-MRA while providing high spatial resolution by employing an undersampled 3-D projection (3D PR) trajectory. The increased coverage and isotropic resolution of the 3D PR acquisition simplify visualization of the vasculature from any perspective. We present a new algorithm to develop a set of time-resolved 3-D image volumes by preferentially weighting the 3D PR data according to its acquisition time. An iterative algorithm computes a series of density compensation functions for a regridding reconstruction, one for each time frame, that exploit the variable sampling density in 3D PR. The iterative weighting procedure simplifies the calculation of appropriate density compensation for arbitrary sampling patterns, which improve sampling efficiency and, thus, signal-to-noise ratio and contrast-to-noise ratio, since it is does not require a closed-form calculation based on geometry. Current medical workstations can display these large four-dimensional studies, however, interactive cine animation of the data is only possible at significantly degraded resolution. Therefore, we also present a method for interactive visualization using powerful graphics cards and distributed processing. Results from volunteer and patient studies demonstrate the advantages of dynamic imaging with high spatial resolution.",
Elastic registration for retinal images based on reconstructed vascular trees,"The vascular tree of the retina is likely the most representative and stable feature for eye fundus images in registration. Based on the reconstructed vascular tree, we propose an elastic matching algorithm to register pairs of fundus images. The identified vessels are thinned and approximated using short line segments of equal length that results a set of elements. The set of elements corresponding to one vascular tree are elastically deformed to optimally match the set of elements of another vascular tree, with the guide of an energy function to finally establish pixel relationship between both vascular trees. The mapped positions of pixels in the transformed retinal image are computed to be the sum of their original locations and corresponding displacement vectors. For the purpose of performance comparison, a weak affine model based fast chamfer matching technique is proposed and implemented. Experiment results validated the effectiveness of the elastic matching algorithm and its advantage over the weak affine model for registration of retinal fundus images.",
On using game theory to optimize the rate control in video coding,"This paper presents a game theory based technique for optimizing the bit rate control in video coding. Game theory, by virtue of its enormous potential for solving constrained optimization problems, has been effectively utilized in several branches of natural and social sciences. But this paper is the first attempt in using game theory for video compression. The objective is to optimize the perceptual quality while guaranteeing ""fairness"" in bit allocation among macroblocks (MBs). The proposed technique is a dual-level rate control algorithm: At the first level, the algorithm allocates the target bits to frames based on their coding complexity; a method to estimate the coding complexity of the remaining frames is proposed. At the second level, MBs of a frame play cooperative games such that each MB competes for a fair share of resources (bits) to optimize its quantization scale while considering the human visual system (HVS) perceptual property. We formulate the rate control problem by defining players, strategies and objective function. Since the whole frame is an entity perceived by viewers, MBs compete cooperatively under a global objective of achieving the best quality with the given bit constraint. The major advantage of the proposed approach is that the cooperative game leads to an optimal and fair bit allocation strategy based on the Nash bargaining solution. Another advantage is that it allows multi-objective optimization with multiple decision makers (e.g., MBs) in order to achieve accurate bit rate with good perceptual quality while maintaining a stable buffer level. Several extensions of the work are possible.","Game theory,
Video coding,
Bit rate,
Quantization,
Video compression,
Humans,
Visual system,
Discrete cosine transforms,
Computer science,
Constraint theory"
Media hash-dependent image watermarking resilient against both geometric attacks and estimation attacks based on false positive-oriented detection,"The major disadvantage of existing watermarking methods is their limited resistance to extensive geometric attacks. In addition, we have found that the weakness of multiple watermark embedding methods that were initially designed to resist geometric attacks is their inability to withstand the watermark-estimation attacks (WEAs), leading to reduce resistance to geometric attacks. In view of these facts, this paper proposes a robust image watermarking scheme that can withstand geometric distortions and WEAs simultaneously. Our scheme is mainly composed of three components: 1) robust mesh generation and mesh-based watermarking to resist geometric distortions; 2) construction of media hash-based content-dependent watermark to resist WEAs; and 3) a mechanism of false positive-oriented watermark detection, which can be used to determine the existence of a watermark so as to achieve a tradeoff between correct detection and false detection. Furthermore, extensive experimental results obtained using the standard benchmark (i.e., Stirmark) and WEAs, and comparisons with relevant watermarking methods confirm the excellent performance of our method in improving robustness. To our knowledge, such a thorough evaluation has not been reported in the literature before","Watermarking,
Robustness,
Resists,
Sun,
Information science,
Fourier transforms,
Chaos,
Mesh generation,
Copyright protection,
Data security"
Convergence of gradient method with momentum for two-Layer feedforward neural networks,"A gradient method with momentum for two-layer feedforward neural networks is considered. The learning rate is set to be a constant and the momentum factor an adaptive variable. Both the weak and strong convergence results are proved, as well as the convergence rates for the error function and for the weight. Compared to the existing convergence results, our results are more general since we do not require the error function to be quadratic.",
OACerts: Oblivious Attribute Certificates,"We propose oblivious attribute certificates (OACerts), an attribute certificate scheme in which a certificate holder can select which attributes to use and how to use them. In particular, a user can use attribute values stored in an OACert obliviously, i.e., the user obtains a service if and only if the attribute values satisfy the policy of the service provider, yet the service provider learns nothing about these attribute values. This way, the service provider's access control policy is enforced in an oblivious fashion. To enable the oblivious access control using OACerts, we propose a new cryptographic primitive called oblivious commitment-based envelope (OCBE). In an OCBE scheme, Bob has an attribute value committed to Alice and Alice runs a protocol with Bob to send an envelope (encrypted message) to Bob such that: 1) Bob can open the envelope if and only if his committed attribute value satisfies a predicate chosen by Alice and 2) Alice learns nothing about Bob's attribute value. We develop provably secure and efficient OCBE protocols for the Pedersen commitment scheme and comparison predicates as well as logical combinations of them","Access control,
Licenses,
Access protocols,
Senior citizens,
Privacy,
Public key cryptography,
Cryptographic protocols,
Protection,
Public key,
Credit cards"
Performance Study of Power Saving Classes of Type I and II in IEEE 802.16e,"The IEEE 802.16e standard introduces two types of sleep modes which perform very differently under different traffic types: power saving classes of type I based on binary-increasing sleep window size and power saving of type II using constant sleep window size. This paper provides simple but accurate analytical models capable of calculating the power efficiency and packet access delay for the two power saving types. In addition, a comparison of the energy efficiency and delay performance between the two power saving types is reported. By means of the proposed model and evaluation, we point out the trade-off between these two types and suggest a power switching scheme to obtain optimal power efficiency under different traffic conditions. Numerical and simulation results are provided for validation of our models",
Breaking a remote user authentication scheme for multi-server architecture,"Lin et al., (2003) proposed a remote user authentication scheme for multi-server architecture. In this paper, we breaks this scheme by giving an attack. Our attack allows an adversary to impersonate any user in the system, as long as a single authentication message of that user is observed","Authentication,
Tin,
Digital signatures,
Equations,
Environmental management,
Public key,
Computer science,
Random number generation,
Bismuth,
Computer architecture"
On Model-Checking Trees Generated by Higher-Order Recursion Schemes,"We prove that the modal mu-calculus model-checking problem for (ranked and ordered) node-labelled trees that are generated by order-n recursion schemes (whether safe or not, and whether homogeneously typed or not) is n-EXPTIME complete, for every nges0. It follows that the monadic second-order theories of these trees are decidable. There are three major ingredients. The first is a certain transference principle from the tree generated by the scheme - the value tree - to an auxiliary computation tree, which is itself a tree generated by a related order-0 recursion scheme (equivalently, a regular tree). Using innocent game semantics in the sense of Hyland and Ong, we establish a strong correspondence between paths in the value tree and traversals in the computation tree. This allows us to prove that a given alternating parity tree automaton (APT) has an (accepting) run-tree over the value tree iff it has an (accepting) traversal-tree over the computation tree. The second ingredient is the simulation of an (accepting) traversal-tree by a certain set of annotated paths over the computation tree; we introduce traversal-simulating APT as a recognising device for the latter. Finally, for the complexity result, we prove that traversal-simulating APT enjoy a succinctness property: for deciding acceptance, it is enough to consider run-trees that have a reduced branching factor. The desired bound is then obtained by analysing the complexity of solving an associated (finite) acceptance parity game",
Toward trustworthy software systems,"Organizations such as Microsoft's Trusted Computing Group and Sun Microsystems' Liberty Alliance are currently leading the debate on ""trustworthy computing."" However, these and other initiatives primarily focus on security, and trustworthiness depends on many other attributes. To address this problem, the University of Oldenburg's TrustSoft Graduate School aims to provide a holistic view of trustworthiness in software - one that considers system construction, evaluation/analysis, and certification - in an interdisciplinary setting. Component technology is the foundation of our research program. The choice of a component architecture greatly influences the resulting software systems' nonfunctional properties. We are developing new methods for the rigorous design of trustworthy software systems with predictable, provable, and ultimately legally certifiable system properties. We are well aware that it is impossible to build completely error-free complex software systems. We therefore complement fault-prevention and fault-removal techniques with fault-tolerance methods that introduce redundancy and diversity into software systems. Quantifiable attributes such as availability, reliability, and performance call for analytical prediction models, which require empirical studies for calibration and validation. To consider the legal aspects of software certification and liability, TrustSoft integrates the disciplines of computer science and computer law.","Software systems,
Certification,
Law,
Sun,
Security,
Component architectures,
Software design,
Fault tolerant systems,
Redundancy,
Availability"
Views on Visualization,"The field of visualization is maturing. Many problems have been solved and new directions are sought. In order to make good choices, an understanding of the purpose and meaning of visualization is needed. In this paper, visualization is considered from multiple points of view. First, a technological viewpoint is adopted, where the value of visualization is measured based on effectiveness and efficiency. An economic model of visualization is presented and benefits and costs are established. Next, consequences and limitations of visualization are discussed (including the use of alternative methods, high initial costs, subjectiveness, and the role of interaction). Example uses of the model for the judgment of existing classes of methods are given to understand why they are or are not used in practice. However, such an economic view is too restrictive. Alternative views on visualization are presented and discussed: visualization as an art, visualization as design and, finally, visualization as a scientific discipline.",
Where am I? Scene Recognition for Mobile Robots using Audio Features,"Automatic recognition of unstructured environments is an important problem for mobile robots. We focus on using audio features to recognize different auditory environments, where they are characterized by different types of sounds. The use of audio information provides a complementary means of scene recognition that can effectively augment visual information. In particular, audio can be used toward both the analysis and characterization of the environment at a higher level of abstraction. We begin our investigation of recognizing different auditory environments with the audio information. In this paper, we utilize low-level audio features from a mobile robot and investigate using high-level features based on spectral analysis for scene characterization, and a recognition system was built to discriminate between different environments based on these audio features found",
Model-Based Face De-Identification,"Advances in camera and computing equipment hardware in recent years have made it increasingly simple to capture and store extensive amounts of video data. This, among other things, creates ample opportunities for the sharing of video sequences. In order to protect the privacy of subjects visible in the scene, automated methods to de-identify the images, particularly the face region, are necessary. So far the majority of privacy protection schemes currently used in practice rely on ad-hoc methods such as pixelation or blurring of the face. In this paper we show in extensive experiments that pixelation and blurring offers very poor privacy protection while significantly distorting the data. We then introduce a novel framework for de-identifying facial images. Our algorithm combines a model-based face image parameterization with a formal privacy protection model. In experiments on two large-scale data sets we demonstrate privacy protection and preservation of data utility.",
Improvement of the output characteristics of magnetrons using the transparent cathode,"The output characteristics of magnetrons can be significantly improved by using a cathode that is transparent to the synchronous electromagnetic fields thereby providing improved conditions for fast conversion of the electrons' potential energy into electromagnetic energy. The transparent cathode consists of separate longitudinal metal strips, arranged to form a cylindrical surface, that act as individual electron emitters. Favorable prebunching of electrons to excite the desired operating mode is provided by a suitable choice of the number and azimuthal position of the cathode strips. The strong azimuthal wave field in the cathode region rapidly captures the prebunched electrons into rotating spokes forming the anode current. This process provides faster start-of-oscillations than priming using a solid cathode. The strong wave field in the electron sheath over the cathode, for any sheath thickness, gives the possibility of improving the efficiency by concomitantly increasing the applied voltage and magnetic field. Computer simulations of the A6 magnetron driven both by a solid and transparent cathode demonstrate the advantages of this approach","Magnetrons,
Cathodes,
Electromagnetic fields,
Strips,
Solids,
Potential energy,
Electron guns,
Anodes,
Voltage,
Magnetic fields"
Validating a Biometric Authentication System: Sample Size Requirements,"Authentication systems based on biometric features (e.g., fingerprint impressions, iris scans, human face images, etc.) are increasingly gaining widespread use and popularity. Often, vendors and owners of these commercial biometric systems claim impressive performance that is estimated based on some proprietary data. In such situations, there is a need to independently validate the claimed performance levels. System performance is typically evaluated by collecting biometric templates from n different subjects, and for convenience, acquiring multiple instances of the biometric for each of the n subjects. Very little work has been done in 1) constructing confidence regions based on the ROC curve for validating the claimed performance levels and 2) determining the required number of biometric samples needed to establish confidence regions of prespecified width for the ROC curve. To simplify the analysis that addresses these two problems, several previous studies have assumed that multiple acquisitions of the biometric entity are statistically independent. This assumption is too restrictive and is generally not valid. We have developed a validation technique based on multivariate copula models for correlated biometric acquisitions. Based on the same model, we also determine the minimum number of samples required to achieve confidence bands of desired width for the ROC curve. We illustrate the estimation of the confidence bands as well as the required number of biometric samples using a fingerprint matching system that is applied on samples collected from a small population","Biometrics,
Authentication,
Fingerprint recognition,
Error analysis,
Testing,
Iris,
Humans,
Face,
System performance"
On-Chip Electrochemical Analysis System Using Nanoelectrodes and Bioelectronic CMOS Chip,"An electrochemical microanalysis system for reversible redox species determination has been presented. This system consists of a nanoelectrochemical sensor and a complimentary metal-oxide-semiconductor (CMOS) chip for sensor signal interfacing and conditioning. The nanoelectrochemical sensor is an interdigitated array nanoelectrode fabricated using e-beam lithography and ultraviolet lithography. The CMOS chip is designed and fabricated to realize the electrochemical analysis method of sensing, which is a switch-based method. Using this switch-based method, the potential dynamics of the nanoelectrodes is recorded to evaluate the species concentration. Experimental results have shown that the detection limit of this microanalysis system on reversible redox species is in the range of 1-10 nM","System-on-a-chip,
Electrodes,
Biosensors,
Sensor arrays,
Laboratories,
Circuits,
Sensor systems,
Lithography,
Testing,
Computer science"
Efficient synchronization under global EDF scheduling on multiprocessors,"We consider coordinating accesses to shared data structures in multiprocessor real-time systems scheduled under preemptive global EDF. To our knowledge, prior work on global EDF has focused only on systems of independent tasks. We take an initial step here towards a generic resource-sharing framework by considering simple shared objects, such as queues, stacks, and linked lists. In many applications, the predominate use of synchronization constructs is for sharing such simple objects. We analyze two synchronization methods for such objects, one based on queue-based spin locks and a second based on lock-free algorithms","Processor scheduling,
Real time systems,
Data structures,
Spinning,
Queueing analysis,
Scheduling algorithm,
Kernel,
Computer science,
Algorithm design and analysis,
Hardware"
A Wall Climbing Robot for Oil Tank Inspection,Thousands of storage tanks in oil refineries have to be inspected manually to prevent leakage and/or any other potential catastrophe. A wall climbing robot with permanent magnet adhesion mechanism equipped with nondestructive sensor has been designed. The robot can be operated autonomously or manually. In autonomous mode the robot uses an ingenious coverage algorithm based on distance transform function to navigate itself over the tank surface in a back and forth motion to scan the external wall for the possible faults using sensors without any human intervention. In manual mode the robot can be navigated wirelessly from the ground station to any location of interest. Preliminary experiment has been carried out to test the prototype.,"Climbing robots,
Fuel storage,
Inspection,
Robot sensing systems,
Navigation,
Oil refineries,
Permanent magnets,
Adhesives,
Magnetic sensors,
Humans"
Skyline Queries Against Mobile Lightweight Devices in MANETs,"Skyline queries are well suited when retrieving data according to multiple criteria. While most previous work has assumed a centralized setting this paper considers skyline querying in a mobile and distributed setting, where each mobile device is capable of holding only a portion of the whole dataset; where devices communicate through mobile ad hoc networks; and where a query issued by a mobile user is interested only in the user’s local area, although a query generally involves data stored on many mobile devices due to the storage limitations. We present techniques that aim to reduce the costs of communication among mobile devices and reduce the execution time on each single mobile device. For the former, skyline query requests are forwarded among mobile devices in a deliberate way, such that the amount of data to be transferred is reduced. For the latter, specific optimization measures are proposed for resource-constrained mobile devices. We conduct extensive experiments to show that our proposal performs efficiently in real mobile devices and simulated wireless ad hoc networks.",
Complexity of two-level logic minimization,"The complexity of two-level logic minimization is a topic of interest to both computer-aided design (CAD) specialists and computer science theoreticians. In the logic synthesis community, two-level logic minimization forms the foundation for more complex optimization procedures that have significant real-world impact. At the same time, the computational complexity of two-level logic minimization has posed challenges since the beginning of the field in the 1960s; indeed, some central questions have been resolved only within the last few years, and others remain open. This recent activity has classified some logic optimization problems of high practical relevance, such as finding the minimal sum-of-products (SOP) form and maximal term expansion and reduction. This paper surveys progress in the field with self-contained expositions of fundamental early results, an account of the recent advances, and some new classifications. It includes an introduction to the relevant concepts and terminology from computational complexity, as well a discussion of the major remaining open problems in the complexity of logic minimization","Minimization methods,
Programmable logic arrays,
Logic design,
Computational complexity,
Multivalued logic,
Design automation,
Data structures,
Boolean functions,
Computer science,
History"
Network Coding for Joint Storage and Transmission with Minimum Cost,"Network coding provides elegant solutions to many data transmission problems. The usage of coding for distributed data storage has also been explored. In this work, we study a joint storage and transmission problem, where a source transmits a file to storage nodes whenever the file is updated, and clients read the file by retrieving data from the storage nodes. The cost includes the transmission cost for file update and file read, as well as the storage cost. We show that such a problem can be transformed into a pure flow problem and is solvable in polynomial time using linear programming. Coding is often necessary for obtaining the optimal solution with the minimum cost. However, we prove that for networks of generalized tree structures, where adjacent nodes can have asymmetric links between them, file splitting instead of coding - is sufficient for achieving optimality. In particular, if there is no constraint on the numbers of bits that can be stored in storage nodes, there exists an optimal solution that always transmits and stores the file as a whole. The proof is accompanied by an algorithm that optimally assigns file segments to storage nodes","Network coding,
Costs,
Data communication,
Memory,
Information retrieval,
Polynomials,
Linear programming,
Tree data structures,
Interleaved codes,
Computer science"
Self-Adaptive SLA-Driven Capacity Management for Internet Services,"This work considers the problem of hosting multiple third-party Internet services in a cost-effective manner so as to maximize a provider's business objective. For this purpose, we present a dynamic capacity management framework based on an optimization model, which links a cost model based on SLA contracts with an analytical queuing-based performance model, in an attempt to adapt the platform to changing capacity needs in real time. In addition, we propose a two-level SLA specification for different operation modes, namely, normal and surge, which allows for per-use service accounting with respect to requirements of throughput and tail distribution response time. The cost model proposed is based on penalties, incurred by the provider due to SLA violation, and rewards, received when the service level expectations are exceeded. Finally, we evaluate approximations for predicting the performance of the hosted services under two different scheduling disciplines, namely FCFS and processor sharing. Through simulation, we assess the effectiveness of the proposed approach as well as the level of accuracy resulting from the performance model approximations",
Comparison of Heart Rate Variability Signal Features Derived from Electrocardiography and Photoplethysmography in Healthy Individuals,"The heart rate variability (HRV) signal is indicative of autonomic regulation of the heart rate (HR). It could be used as a noninvasive marker in monitoring the physiological state of an individual. Currently, the primary method of deriving the HRV signal is to acquire the electrocardiogram (ECG) signal, apply appropriate QRS detection algorithms to locate the R wave and its peak, find the RR intervals, and perform suitable interpolation and resampling to produce a uniformly sampled tachogram. This process could sometimes result in errors in the HRV signal due to drift, electromagnetic and biologic interference, and the complex morphology of the ECG signal. The photoplethysmographic (PPG) signal has the potential to eliminate the problems with the ECG signal to derive the HRV signal. To investigate this point, a PDA-based system was developed to simultaneously record ECG and PPG signals to facilitate accurately controlled sampling and recording durations. Two healthy young volunteers participated in this pilot study to evaluate the applicability of our approach. To improve data quality, ECG and PPG recordings were acquired three times/subject. A comparison between different features of the HRV signals derived from both methods was performed to test the validity of using PPG signals in HRV analysis. We used autoregressive (AR) modeling, Poincare' plots, cross correlation, standard deviation, arithmetic mean, skewness, kurtosis, and approximate entropy (ApEn) to derive and compare different measures from both ECG and PPG signals. This study demonstrated that our PDA-based system was a convenient and reliable means for acquisition of PPG-derived and ECG-derived HRV signals. The excellent agreement between different measures of HRV signals acquired from both methods provides potential support for the idea of using PPGs instead of ECGs in HRV signal derivation and analysis in ambulatory cardiac monitoring of healthy individuals","Heart rate variability,
Electrocardiography,
Heart rate,
Signal processing,
Signal analysis,
Biomedical monitoring,
Detection algorithms,
Interpolation,
Electromagnetic interference,
Morphology"
Multiobjective hypergraph-partitioning algorithms for cut and maximum subdomain-degree minimization,"In this paper, we present a family of multiobjective hypergraph-partitioning algorithms based on the multilevel paradigm, which are capable of producing solutions in which both the cut and the maximum subdomain degree are simultaneously minimized. This type of partitionings are critical for existing and emerging applications in very large scale integration (VLSI) computer-aided design (CAD) as they allow to both minimize and evenly distribute the interconnects across the physical devices. Our experimental evaluation on the International Symposium on Physical Design (ISPD98) benchmark show that our algorithms produce solutions, which when compared against those produced by hM/sub E/T/sub I/S have a maximum subdomain degree that is reduced by up to 36% while achieving comparable quality in terms of cut.","Minimization methods,
Partitioning algorithms,
Very large scale integration,
Design automation,
Iterative algorithms,
Application software,
Computer science,
Algorithm design and analysis,
Databases,
Information retrieval"
Power Allocation and Asymptotic Achievable Sum-Rates in Single-Hop Wireless Networks,"A network of n communication links operating over a shared wireless channel is considered. Power management is crucial to such interference-limited networks to improve the aggregate throughput. We consider sum-rate maximization of the network by optimum power allocation when conventional linear receivers (without interference cancellation) are utilized. It is shown that in the case of n=2 links, the optimum power allocation strategy is such that either both links use their maximum power or one of them uses its maximum power and the other keeps silent. An asymptotic analysis for large n is carried out to show that in a Rayleigh fading channel the average sum-rate scales at least as log(n). This is obtained by deriving an on-off power allocation strategy. The same scaling law is obtained in the work of Gowaikar et al., where the number of links, their end-points (source-destination pairs), and the relay nodes are optimally chosen all by a central controller. However, our proposed strategy can be implemented in a decentralized fashion for any number of links, arbitrary transmitter-receiver pairs, and without any relay nodes. It is shown that the proposed power allocation scheme is optimum among all on-off power allocation strategies in the sense that no other strategies can achieve an average sum-rate of higher order.",
RANSAC for (Quasi-)Degenerate data (QDEGSAC),"The computation of relations from a number of potential matches is a major task in computer vision. Often RANSAC is employed for the robust computation of relations such as the fundamental matrix. For (quasi-)degenerate data however, it often fails to compute the correct relation. The computed relation is always consistent with the data but RANSAC does not verify that it is unique. The paper proposes a framework that estimates the correct relation with the same robustness as RANSAC even for (quasi-)degenerate data. The approach is based on a hierarchical RANSAC over the number of constraints provided by the data. In contrast to all previously presented algorithms for (quasi-)degenerate data our technique does not require problem specific tests or models to deal with degenerate configurations. Accordingly it can be applied for the estimation of any relation on any data and is not limited to a special type of relation as previous approaches. The results are equivalent to the results achieved by state of the art approaches that employ knowledge about degeneracies.",
Network loss tomography using striped unicast probes,"In this paper, we explore the use of end-to-end unicast traffic as measurement probes to infer link-level loss rates. We leverage off of earlier work that produced efficient estimates for link-level loss rates based on end-to-end multicast traffic measurements. We design experiments based on the notion of transmitting stripes of packets (with no delay between transmission of successive packets within a stripe) to two or more receivers. The purpose of these stripes is to ensure that the correlation in receiver observations matches as closely as possible what would have been observed if a multicast probe followed the same path to the receivers. Measurements provide good evidence that a packet pair to distinct receivers introduces considerable correlation which can be further increased by simply considering longer stripes. Using an M/M/1/K model for a link, we theoretically confirm this benefit for stripes. We also use simulation to explore how well these stripes translate into accurate link-level loss estimates. We observe good accuracy with packet pairs, with a typical error of about 1%, which significantly decreases as stripe length is increased","Tomography,
Unicast,
Probes,
Loss measurement,
Telecommunication traffic,
Delay,
Internet,
Computer science,
Size measurement,
Performance loss"
ODAR: On-Demand Anonymous Routing in Ad Hoc Networks,"Routing in wireless ad hoc networks are vulnerable to traffic analysis, spoofing and denial of service attacks due to open wireless medium communications. Anonymity mechanisms in ad hoc networks are critical security measures used to mitigate these problems by concealing identification information, such as those of nodes, links, traffic flows, paths and network topology information from harmful attackers. We propose ODAR, an On-Demand Anonymous Routing protocol for wireless ad hoc networks to enable complete anonymity of nodes, links and source-routing paths/trees using Bloom filters. We simulate ODAR using J-Sim, and compare its performance with AODV in certain ad hoc network scenarios","Routing,
Ad hoc networks,
Mobile ad hoc networks,
Telecommunication traffic,
Computer crime,
Communication system security,
Wireless communication,
Information security,
Traffic control,
Network topology"
An SPC monitoring system for cycle-based waveform signals using haar transform,"Due to the rapid development of computer and sensing technology, many measurements of process variables are readily available in manufacturing processes. These measurements carry a large amount of information about process conditions. It is highly desirable to develop a process monitoring and diagnosis methodology that can utilize this information. In this paper, a statistical process control monitoring system is developed for a class of commonly available process measurements-cycle-based waveform signals. This system integrates the statistical process control technology and the Haar wavelet transform. With it, one can not only detect a process change, but also identify the location and estimate the magnitude of the process mean shift within the signal. A case study involving a stamping process demonstrates the effectiveness of the proposed methodology on the monitoring of the profile-type data. Note to Practitioners-Cycle-based signal refers to an analog or digital signal that is obtained through automatic sensing during each operation cycle of a manufacturing process. The cycle-based signal is very common in various manufacturing processes (e.g., forming force in stamping processes, the holding force, and the current signals in spot welding processes, the insertion force in the engine assembly process). In general, cycle-based signals contain rich process information. In this paper, cycle-based signal monitoring will be accomplished by monitoring the wavelet transformation of the signal, instead of monitoring the raw observations themselves. Further, a decision-making technique is developed using the SPC monitoring system to locate where the mean shift occurred and to estimate magnitudes of mean shifts. Thus, this paper presents a generic framework for the enhanced statistical process control technique of cycle-based signals.","Signal processing,
Computerized monitoring,
Manufacturing processes,
Process control,
Computer aided manufacturing,
Wavelet transforms,
Spot welding,
Engines,
Assembly,
Decision making"
Lightweight I/O for Scientific Applications,"Today's high-end massively parallel processing (MPP) machines have thousands to tens of thousands of processors, with next-generation systems planned to have in excess of one hundred thousand processors. For systems of such scale, efficient I/O is a significant challenge that cannot be solved using traditional approaches. In particular, general purpose parallel file systems that limit applications to standard interfaces and access policies do not scale and will likely be a performance bottleneck for many scientific applications. In this paper, we investigate the use of a ""lightweight"" approach to I/O that requires the application or I/O-library developer to extend a core set of critical I/O functionality with the minimum set of features and services required by its target applications. We argue that this approach allows the development of I/O libraries that are both scalable and secure. We support our claims with preliminary results for a lightweight checkpoint operation on a development cluster at Sandia","File systems,
Application software,
Libraries,
Laboratories,
Prefetching,
Pattern matching,
Kernel,
Computer science,
Parallel processing,
Hardware"
Fault-aware scheduling for Bag-of-Tasks applications on Desktop Grids,"Desktop grids have proved to be a suitable platform for the execution of bag-of-tasks applications but, being characterized by a high resource volatility, require the availability of scheduling techniques able to effectively deal with resource failures and/or unplanned periods of unavailability. In this paper we present a set of fault-aware scheduling policies that, rather than just tolerating faults as done by traditional fault-tolerant schedulers, exploit the information concerning resource availability to improve application performance. The performance of these strategies have been compared via simulation with those attained by traditional fault-tolerant schedulers. Our results, obtained by considering a set of realistic scenarios modeled after real desktop grids, show that our approach results in better application performance and resource utilization",
Asymptotic Mean-Square Optimality of Belief Propagation for Sparse Linear Systems,"This paper studies the estimation of a high-dimensional vector signal where the observation is a known ""sparse"" linear transformation of the signal corrupted by additive Gaussian noise. A paradigm of such a linear system is code-division multiple access (CDMA) channel with sparse spreading matrix. Assuming a ""semi-regular"" ensemble of sparse matrix linear transformations, where the bi-partite graph describing the system is asymptotically cycle-free, it is shown that belief propagation (BP) achieves the minimum mean-square error (MMSE) in estimating the transformation of the input vector in the large-system limit. The result holds regardless of the the distribution and power of the input symbols. Furthermore, the mean squared error of estimating each symbol of the input vector using BP is proved to be equal to the MMSE of estimating the same symbol through a scalar Gaussian channel with some degradation in the signal-to-noise ratio (SNR). The degradation, called the efficiency, is determined from a fixed-point equation due to Guo and Verdu, which is a generalization of Tanaka's formula to arbitrary prior distributions","Belief propagation,
Linear systems,
Vectors,
Multiaccess communication,
Sparse matrices,
Degradation,
Additive noise,
Gaussian noise,
Gaussian channels,
Signal to noise ratio"
A Parallel Approach to XML Parsing,"A language for semi-structured documents, XML has emerged as the core of the Web services architecture, and is playing crucial roles in messaging systems, databases, and document processing. However, the processing of XML documents has a reputation for poor performance, and a number of optimizations have been developed to address this performance problem from different perspectives, none of which have been entirely satisfactory. In this paper, we present a seemingly quixotic, but novel approach: parallel XML parsing. Parallel XML parsing leverages the growing prevalence of multicore architectures in all sectors of the computer market, and yields significant performance improvements. This paper presents our design and implementation of parallel XML parsing. Our design consists of an initial preparsing phase to determine the structure of the XML document, followed by a full, parallel parse. The results of the preparsing phase are used to help partition the XML document for data parallel processing. Our parallel parsing phase is a modification of the libxml2 in Veillard, D. (2004) XML parser, which shows that our approach applies to real-world, production quality parsers. Our empirical study shows our parallel XML parsing algorithm can improved the XML parsing performance significantly and scales well","XML,
Web services,
Service oriented architecture,
Databases,
Multicore processing,
Computer architecture,
Concurrent computing,
Parallel processing,
Production,
Partitioning algorithms"
STIR: Software for Tomographic Image Reconstruction Release 2,"We present an update to STIR, an Open Source object-oriented library in C++ for 3D PET reconstruction. This library has been designed so that it can be used for many algorithms and scanner geometries, while being portable to various computing platforms. This second release enhances its flexibility and modular design, but also adds extra capabilities such as list mode reconstruction, more data formats etc.","Image reconstruction,
Positron emission tomography,
Software libraries,
Algorithm design and analysis,
Object oriented programming,
Open source software,
Portable computers,
Collaboration,
Writing,
Filters"
Adaptive path selection in OBS networks,"In this paper, the authors investigate the concept of adaptive path selection in optical burst-switched networks and its potential to reducing the overall burst drop probability. Specifically, the authors assume that each source maintains a (short) list of alternate paths to each destination and uses information regarding the recent congestion status of the network links to rank the paths; it then transmits bursts along the least congested path. The authors present a suite of path selection strategies, each utilizing a different type of information regarding the link congestion status, and evaluate them using simulation. The results demonstrate that, in general, adaptive path selection outperforms shortest path routing, and, depending on the path strategy involved, the network topology, and the traffic pattern, this improvement can be significant. A new framework for the development of hybrid (or meta) path selection strategies, which make routing decisions based on a weighted combination of the decisions taken by several independent path selection strategies, has been presented. This paper presents two instances of such hybrid strategies, i.e., 1) one that assigns static weights and 2) one that dynamically adjusts the weights based on feedback from the network; it has been shown that these strategies can further improve the overall burst drop probability in the network",
Power MOSFET Switching Loss Analysis: A New Insight,"Realistic estimation of power MOSFET switching losses is critical for predicting the maximum junction temperature and efficiency of power electronics circuits. The purpose of this paper is to investigate the internal physics of MOSFET switching processes using a physically based semiconductor device modeling approach, and subsequently examine the commonly used power loss calculation method based on the new physical insights. The widely accepted output capacitance loss term in this calculation method is found to be redundant and erroneous. In addition, the current method of approximating switching times with power MOSFET gate charge parameters grossly overestimates the switching power loss. This paper recommends a new MOSFET gate charge parameter specification and an effective switching time estimation method to compensate for the power loss calculation error introduced by the two slope voltage transition waveform of the power MOSFET","MOSFET circuits,
Power MOSFET,
Switching loss,
Power semiconductor switches,
Temperature,
Power electronics,
Physics,
Semiconductor device modeling,
Capacitance,
Voltage"
On Measuring Anonymity For Wireless Mobile Ad-hoc Networks,"We propose an evidence theory based anonymity measuring approach for wireless mobile ad-hoc networks. In our approach, an evidence is a measure of the number of detected packets within a given time period. Based on the collected evidence, we can set up basic probability assignments for all packet delivery paths and use evidence theory to quantify the anonymity in the number of bits. Our approach is more general and practical comparing to the traditional Shannon information theory based solutions where the probability assignments are predefined","Ad hoc networks,
Information theory,
Mobile computing,
Communication systems,
Protocols,
Peer to peer computing,
Computer science,
Time measurement,
Electronic mail,
Communication system traffic"
Asynchronous Distributed Calibration for Scalable and Reconfigurable Multi-Projector Displays,"Centralized techniques have been used until now when automatically calibrating (both geometrically and photometrically) large high-resolution displays created by tiling multiple projectors in a 2D array. A centralized server managed all the projectors and also the camera(s) used to calibrate the display. In this paper, we propose an asynchronous distributed calibration methodology via a display unit called the plug-and-play projector (PPP). The PPP consists of a projector, camera, computation and communication unit, thus creating a self-sufficient module that enables an asynchronous distributed architecture for multi-projector displays. We present a single-program-multiple-data (SPMD) calibration algorithm that runs on each PPP and achieves a truly scalable and reconfigurable display without any input from the user. It instruments novel capabilities like adding/removing PPPs from the display dynamically, detecting faults, and reshaping the display to a reasonable rectangular shape to react to the addition/removal/faults. To the best of our knowledge, this is the first attempt to realize a completely asynchronous and distributed calibration architecture and methodology for multi-projector displays","Calibration,
Photometry,
Two dimensional displays,
Project management,
Cameras,
Computer displays,
Distributed computing,
Computer architecture,
Instruments,
Fault detection"
Region Enhanced Scale-Invariant Saliency Detection,"Saliency measures the low-level stimuli to human vision, and serves as an alternative to semantic image understanding. This paper presents a region enhanced scale-invariant saliency detection method. Our method constructs a scale-invariant saliency map from an image, segments the image into regions, and enhances the saliency map with the region information. Compared with previous methods, our method has advantages in providing robust scale-invariant saliency, giving meaningful region information for applications, and eliminating misleading high-contrast edges",
TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams,"In this paper, we propose a new term weighting scheme called term frequency-inverse corpus frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods",
Manipulation in Human Environments,"Robots that work alongside us in our homes and workplaces could extend the time an elderly person can live at home, provide physical assistance to a worker on an assembly line, or help with household chores. In order to assist us in these ways, robots will need to successfully perform manipulation tasks within human environments. Human environments present special challenges for robot manipulation since they are complex, dynamic, uncontrolled, and difficult to perceive reliably. In this paper we present a behavior-based control system that enables a humanoid robot, Domo, to help a person place objects on a shelf. Domo is able to physically locate the shelf, socially cue a person to hand it an object, grasp the object that has been handed to it, transfer the object to the hand that is closest to the shelf, and place the object on the shelf. We use this behavior-based control system to illustrate three themes that characterize our approach to manipulation in human environments. The first theme, cooperative manipulation, refers to the advantages that can be gained by having the robot work with a person to cooperatively perform manipulation tasks. The second theme, task relevant features, emphasizes the benefits of carefully selecting the aspects of the world that are to be perceived and acted upon during a manipulation task. The third theme, let the body do the thinking, encompasses several ways in which a robot can use its body to simplify manipulation tasks.","Humans,
Control systems,
Robotic assembly,
Humanoid robots,
Senior citizens,
Employment,
Manipulator dynamics,
Mobile robots,
Books,
Computer science"
Example Based 3D Reconstruction from Single 2D Images,"We present a novel solution to the problem of depth reconstruction from a single image. Single view 3D reconstruction is an ill-posed problem. We address this problem by using an example-based synthesis approach. Our method uses a database of objects from a single class (e.g. hands, human figures) containing example patches of feasible mappings from the appearance to the depth of each object. Given an image of a novel object, we combine the known depths of patches from similar objects to produce a plausible depth estimate. This is achieved by optimizing a global target function representing the likelihood of the candidate depth. We demonstrate how the variability of 3D shapes and their poses can be handled by updating the example database on-the-fly. In addition, we show how we can employ our method for the novel task of recovering an estimate for the occluded backside of the imaged objects. Finally, we present results on a variety of object classes and a range of imaging conditions.",
Recent innovations in speech-to-text transcription at SRI-ICSI-UW,"We summarize recent progress in automatic speech-to-text transcription at SRI, ICSI, and the University of Washington. The work encompasses all components of speech modeling found in a state-of-the-art recognition system, from acoustic features, to acoustic modeling and adaptation, to language modeling. In the front end, we experimented with nonstandard features, including various measures of voicing, discriminative phone posterior features estimated by multilayer perceptrons, and a novel phone-level macro-averaging for cepstral normalization. Acoustic modeling was improved with combinations of front ends operating at multiple frame rates, as well as by modifications to the standard methods for discriminative Gaussian estimation. We show that acoustic adaptation can be improved by predicting the optimal regression class complexity for a given speaker. Language modeling innovations include the use of a syntax-motivated almost-parsing language model, as well as principled vocabulary-selection techniques. Finally, we address portability issues, such as the use of imperfect training transcripts, and language-specific adjustments required for recognition of Arabic and Mandarin","Technological innovation,
Natural languages,
Computer science,
Laboratories,
Speech recognition,
Adaptation model,
Acoustic measurements,
Multilayer perceptrons,
Cepstral analysis,
Telephony"
Streamline Predicates,"Predicates are functions that return Boolean values. They are an essential tool in computer science. A close look at flow feature definitions reveals that they can be seen as point predicates that tell if a specific feature exists at a certain point. Besides the information about features, scientists and engineers like to know the overall behavior of all streamlines in the flow, typically in the connection with the important features in their application domain. We call this a structure definition for the flow. A successful example for a structure definition is flow topology. In this paper, we present streamline predicates as functions that tell the user about the connection between streamlines and features selected by the user. This means answers to questions like: Which streamlines flow through a given vortex, separation bubble, or shock wave? It can be shown that streamline predicates may refine flow topology so that it also reveals questions about vortices in 3D","Topology,
Computer Society,
Computer science,
Shock waves,
Computer vision,
Fluid dynamics,
Temperature,
Data engineering,
Data visualization,
Feature extraction"
Random Multiresolution Representations for Arbitrary Sensor Network Graphs,"We propose a distributed multiresolution representation of sensor network data so that large-scale summaries are readily available by querying a small fraction of sensor nodes, anywhere in the network, and small-scale details are available by querying a larger number of sensors, locally in the region of interest. A global querier (such as a mobile collector or unmanned aerial vehicle) can obtain a lossy to lossless representation of the network data, according to the desired resolution. A local querier (such as a sensor node) can also obtain either large-scale trends or local details, by querying its immediate neighborhood. We want the encoding to be robust to arbitrary, even time-varying, wireless communication connectivity graphs. Thus we want to avoid cluster heads or deterministic hierarchies that are not robust to single points of failure. We propose a randomized encoding which enables both robustness, and distributed computation that does not require long distance coordination or awareness of network connectivity at individual sensors. Our distributed encoding algorithm operates on local neighborhoods of the communication graph","Robustness,
Signal resolution,
Signal processing algorithms,
Computer networks,
Encoding,
Spatial resolution,
Large-scale systems,
Distributed computing,
Wireless communication,
Clustering algorithms"
A Topology Control Approach to Using Directional Antennas in Wireless Mesh Networks,"Directional antennas in wireless mesh networks can improve spatial reuse. However, using them effectively needs specialized protocol support at the MAC layer, which is always not practical. In this work, we present a topology control approach to effectively using directional antennas with legacy MAC layer protocols such as IEEE 802.11. The idea is to use multiple directional antennas on each node and orient them appropriately to create low interference topologies while maintatining network connectivity. Our approach is based on a well-known approximation algorithm to compute minimum degree spanning trees. We show via empirical studies that this approach can reduce interference significantly without increasing stretch factors to any appreciable extent. Detailed wireless network simulations also show that this approach improves end-to-end throughput of multihop flows relative to using omni-directional antennas. Three or four directional antennas per network node with only moderate beamwidths are sufficient to improve the saturation throughput of multihop flows by a factor of 3¿4.","Network topology,
Directional antennas,
Wireless mesh networks,
Media Access Protocol,
Interference,
Throughput,
Spread spectrum communication,
Approximation algorithms,
Wireless networks,
Computational modeling"
Addressing the Need for Map-Matching Speed: Localizing Global Curve-Matching Algorithms,"With vehicle tracking data becoming an important sensor data resource for a range of applications related to traffic assessment and prediction, fast and accurate map-matching algorithms become a necessary means to ultimately utilize this data. This work proposes a fast map-matching algorithm which exploits tracking data error estimates in a provably correct way and offers a quality guarantee for the computed result trajectory. A new model for the map-matching task is introduced which takes tracking error estimates into account. The proposed adaptive clipping algorithm (i) provably solves this map-matching task and (ii) utilizes the weak Frechet distance to measure similarity between curves. The algorithm uses the error estimates in the trajectory data to reduce the search space (error-aware pruning), while offering the quality guarantee of finding a curve which minimizes the weak Frechet distance to the vehicle trajectory among all possible curves in the road network. Moreover, this work introduces an output-sensitive variant of an existing weak Frechet map-matching algorithm, which is also employed in the adaptive clipping algorithm. Output-sensitiveness paired with error-aware pruning makes adaptive clipping the first map-matching algorithm that provably solves a well-defined map-matching task. An experimental evaluation establishes further that adaptive clipping is also in a practical setting a fast algorithm that at the same time produces high-quality matching results",
Biasing Coevolutionary Search for Optimal Multiagent Behaviors,"Cooperative coevolutionary algorithms (CEAs) offer great potential for concurrent multiagent learning domains and are of special utility to domains involving teams of multiple agents. Unfortunately, they also exhibit pathologies resulting from their game-theoretic nature, and these pathologies interfere with finding solutions that correspond to optimal collaborations of interacting agents. We address this problem by biasing a cooperative CEA in such a way that the fitness of an individual is based partly on the result of interactions with other individuals (as is usual), and partly on an estimate of the best possible reward for that individual if partnered with its optimal collaborator. We justify this idea using existing theoretical models of a relevant subclass of CEAs, demonstrate how to apply biasing in a way that is robust with respect to parameterization, and provide some experimental evidence to validate the biasing approach. We show that it is possible to bias coevolutionary methods to better search for optimal multiagent behaviors","Collaboration,
Pathology,
Robustness,
Evolutionary computation,
Engineering education,
Computer science,
Laboratories,
Algorithm design and analysis"
Two-Variable Logic on Words with Data,"In a data word each position carries a label from a finite alphabet and a data value from some infinite domain. These models have been already considered in the realm of semistructured data, timed automata and extended temporal logics. It is shown that satisfiability for the two-variable first-order logic FO2(~,<,+1) is decidable over finite and over infinite data words, where ~ is a binary predicate testing the data value equality and +1,< are the usual successor and order predicates. The complexity of the problem is at least as hard as Petri net reachability. Several extensions of the logic are considered, some remain decidable while some are undecidable","Logic testing,
Automata,
Petri nets,
Databases,
Data models,
XML,
Navigation,
Upper bound"
Address Space Layout Permutation (ASLP): Towards Fine-Grained Randomization of Commodity Software,"Address space randomization is an emerging and promising method for stopping a broad range of memory corruption attacks. By randomly shifting critical memory regions at process initialization time, address space randomization converts an otherwise successful malicious attack into a benign process crash. However, existing approaches either introduce insufficient randomness, or require source code modification. While insufficient randomness allows successful brute-force attacks, as shown in recent studies, the required source code modification prevents this effective method from being used for commodity software, which is the major source of exploited vulnerabilities on the Internet. We propose address space layout permutation (ASLP) that introduces high degree of randomness (or high entropy) with minimal performance overhead. Essential to ASLP is a novel binary rewriting tool that can place the static code and data segments of a compiled executable to a randomly specified location and performs fine grained permutation of procedure bodies in the code segment as well as static data objects in the data segment. We have also modified the Linux operating system kernel to permute stack, heap, and memory mapped regions. Together, ASLP completely permutes memory regions in an application. Our security and performance evaluation shows minimal performance overhead with orders of magnitude improvement in randomness (e.g., up to 29 bits of randomness on a 32-bit architecture)",
Subtleties of Transactional Memory Atomicity Semantics,"Transactional memory has great potential for simplifying multithreaded programming by allowing programmers to specify regions of the program that must appear to execute atomically. Transactional memory implementations then optimistically execute these transactions concurrently to obtain high performance. This work shows that the same atomic guarantees that give transactions their power also have unexpected and potentially serious negative effects on programs that were written assuming narrower scopes of atomicity. We make four contributions: (1) we show that a direct translation of lock-based critical sections into transactions can introduce deadlock into otherwise correct programs, (2) we introduce the terms strong atomicity and weak atomicity to describe the interaction of transactional and non-transactional code, (3) we show that code that is correct under weak atomicity can deadlock under strong atomicity, and (4) we demonstrate that sequentially composing transactional code can also introduce deadlocks. These observations invalidate the intuition that transactions are strictly safer than lock-based critical sections, that strong atomicity is strictly safer than weak atomicity, and that transactions are always composable","System recovery,
Interleaved codes,
Programming profession,
Information science,
Hardware,
Software performance,
Interference,
Transaction databases,
Computer languages,
Law"
The Influence of a Robot's Social Abilities on Acceptance by Elderly Users,"This study examines the influence of perceived social abilities of a robot on user's attitude towards and acceptance of the robot. An interface robot with simulated conversational capabilities was used in a Wizard of Oz experiment with two conditions: a more socially communicative (the robot made use of a larger set of social abilities in interaction) and a less socially communicative interface. Participants (n=40) were observed in 5 minute interaction sessions and were asked to answer questions on perceived social abilities and technology acceptance. Results show that participants who were confronted with the more socially communicative version of the robot felt more comfortable and were more expressive in communicating with it. This suggests that the more socially communicative condition would be more likely to be accepted as a conversational partner. However, the findings did not show a significant correlation between perceived social abilities and technology acceptance","Senior citizens,
Intelligent robots,
Human robot interaction,
Mood,
Testing,
Seals,
Particle measurements,
Laboratories,
Helium,
Medical services"
Crowd Simulation for Emergency Response using BDI Agent Based on Virtual Reality,"This paper presents a novel VR (virtual reality) trained BDI (belief, desire, intention) software agent used to construct crowd simulations for emergency response. The BDI framework allows modeling of human behavior with a high degree of fidelity. The proposed simulation has been developed using AnyLogic software to mimic crowd evacuation from an area under a terrorist bomb attack. The attributes that govern the BDI characteristics of the agent are studied by conducting human in the loop experiments in VR using the CAVE (cave automatic virtual environment). To enhance generality and interoperability of the proposed crowd simulation modeling scheme, input data models have been developed to define environment attributes. Experiments are also conducted to demonstrate the effect of various parameters on key performance indicators such as crowd evacuation rate and densities","Virtual reality,
Humans,
Computational modeling,
Disaster management,
Discrete event simulation,
Terrorism,
Data models,
Management training,
Intelligent agent,
Weapons"
Information Dissemination via Network Coding,"We study distributed algorithms, also known as gossip algorithms, for information dissemination in an arbitrary connected network of nodes. Distributed algorithms have applications to peer-to-peer, sensor, and ad hoc networks, in which nodes operate under limited computational, communication, and energy resources. These constraints naturally give rise to ""gossip"" algorithms: schemes in which nodes repeatedly communicate with randomly chosen neighbors, thus distributing the computational burden across all the nodes in the network and making the computation robust against node failures. Information dissemination based on network coding was introduced by Deb and Medard. They showed the virtue of coding by analyzing a coding algorithm for a complete graph. Although their scheme generalizes to arbitrary graphs, the analysis does not. We present analysis of this algorithm for arbitrary graphs. Specifically, we find that the information dissemination time is naturally related to the spectral properties of the underlying network graph. Our results provide insight into how the graph topology affects the performance of the coding-based information dissemination algorithm","Network coding,
Peer to peer computing,
Computer networks,
Distributed computing,
Distributed algorithms,
Algorithm design and analysis,
Ad hoc networks,
Energy resources,
Robustness,
Network topology"
Adaptive Statistical Optimization Techniques for Firewall Packet Filtering,,
Geometric and shading correction for images of printed materials using boundary,"A novel technique that uses boundary interpolation to correct geometric distortion and shading artifacts present in images of printed materials is presented. Unlike existing techniques, our algorithm can simultaneously correct a variety of geometric distortions, including skew, fold distortion, binder curl, and combinations of these. In addition, the same interpolation framework can be used to estimate the intrinsic illumination component of the distorted image to correct shading artifacts. We detail our algorithm for geometric and shading correction and demonstrate its usefulness on real-world and synthetic data.","Interpolation,
Lighting,
Photometry,
Two dimensional displays,
Shape,
Books,
Simultaneous localization and mapping,
Computer science"
A Simple High-Speed Multiplier Design,"The performance of multiplication is crucial for multimedia applications such as 3D graphics and signal processing systems, which depend on the execution of large numbers of multiplications. Previously reported algorithms mainly focused on rapidly reducing the partial products rows down to final sums and carries used for the final accumulation. These techniques mostly rely on circuit optimization and minimization of the critical paths. In this paper, an algorithm to achieve fast multiplication in two's complement representation is presented. Rather than focusing on reducing the partial products rows down to final sums and carries, our approach strives to generate fewer partial products rows. In turn, this influences the speed of the multiplication, even before applying partial products reduction techniques. Fewer partial products rows are produced, thereby lowering the overall operation time. In addition to the speed improvement, our algorithm results in a true diamond-shape for the partial product tree, which is more efficient in terms of implementation. The synthesis results of our multiplication algorithm using the Artisan TSMC 0.13mum 1.2-volt standard-cell library show 13 percent improvement in speed and 14 percent improvement in power savings for 8-bit times 8-bit multiplications (10 percent and 3 percent, respectively, for 16-bit times 16-bit multiplications) when compared to conventional multiplication algorithms","circuit optimisation,
digital arithmetic,
logic design,
multiplying circuits"
Statistical strategy for anisotropic adventitia modelling in IVUS,"Vessel plaque assessment by analysis of intravascular ultrasound sequences is a useful tool for cardiac disease diagnosis and intervention. Manual detection of luminal (inner) and media-adventitia (external) vessel borders is the main activity of physicians in the process of lumen narrowing (plaque) quantification. Difficult definition of vessel border descriptors, as well as, shades, artifacts, and blurred signal response due to ultrasound physical properties trouble automated adventitia segmentation. In order to efficiently approach such a complex problem, we propose blending advanced anisotropic filtering operators and statistical classification techniques into a vessel border modelling strategy. Our systematic statistical analysis shows that the reported adventitia detection achieves an accuracy in the range of interobserver variability regardless of plaque nature, vessel geometry, and incomplete vessel borders",
Designing for social data analysis,"The NameVoyager, a Web-based visualization of historical trends in baby naming, has proven remarkably popular. We describe design decisions behind the application and lessons learned in creating an application that makes do-it-yourself data mining popular. The prime lesson, it is hypothesized, is that an information visualization tool may be fruitfully viewed not as a tool but as part of an online social environment. In other words, to design a successful exploratory data analysis tool, one good strategy is to create a system that enables ""social"" data analysis. We end by discussing the design of an extension of the NameVoyager to a more complex data set, in which the principles of social data analysis played a guiding role.",
WSN16-5: Distributed Formation of Overlapping Multi-hop Clusters in Wireless Sensor Networks,"Clustering is a standard approach for achieving efficient and scalable performance in wireless sensor networks. Most of the published clustering algorithms strive to generate the minimum number of disjoint clusters. However, we argue that guaranteeing some degree of overlap among clusters can facilitate many applications, like inter-cluster routing, topology discovery and node localization, recovery from cluster head failure, etc. We formulate the overlapping multi-hop clustering problem as an extension to the k-dominating set problem. Then we propose MOCA; a randomized distributed multi-hop clustering algorithm for organizing the sensors into overlapping clusters. We validate MOCA in a simulated environment and analyze the effect of different parameters, e.g. node density and network connectivity, on its performance. The simulation results demonstrate that MOCA is scalable, introduces low overhead and produces approximately equal-sized clusters.","Spread spectrum communication,
Wireless sensor networks,
Routing protocols,
Clustering algorithms,
Computer science,
Educational institutions,
Network topology,
Organizing,
Sensor phenomena and characterization,
Analytical models"
Video loss recovery with FEC and stream replication,"Packet loss is inevitable in video multicast. In this paper, we propose and study an effective feedback-free loss recovery scheme for layered video which combines forward error correction (FEC) and stream replication. In our scheme, the server multicasts the video in parallel with FEC packets and a number of replicated delayed (ReD) version of the stream. Receivers autonomously and dynamically join the FEC and ReD streams to repair their losses. On the server side, we analyze and optimize the number of replicated streams and FEC packets to meet a certain residual loss requirement (i.e., error after correction). On the receiver side, we analyze the optimal combination of FEC and ReD packets to minimize its loss. We also present a fast yet accurate approximation algorithm for receiver to make such decision. We show that FEC combined with merely one or two replicated streams can effectively reduce the residual error rate (by as much as 50%) as compared with pure FEC or replication alone. Both subjective and objective video measures confirm that our recovery scheme achieves much better visual quality.","Streaming media,
Forward error correction,
Delay,
Computer science,
Error correction,
Error analysis,
Automatic repeat request,
Councils,
Approximation algorithms,
Video sequences"
Analyzing multi-channel medium access control schemes with ALOHA reservation,"In order to improve the throughput performance of medium access control (MAC) schemes in wireless communication networks, some researchers proposed to divide a single shared channel into several sub-channels: one as control sub-channel and the others as data sub-channels. In this paper, we analyze and evaluate the maximum achievable throughput of a class of generic multi-channel MAC schemes that are based on the RTS/CTS (ready-to-send/clear-to-send) dialogue and on ALOHA contention resolution. We study these multi-channel MAC schemes under two split-channel scenarios: the fixed-total-bandwidth scenario and the fixed-channel-bandwidth scenario. In the fixed-total-bandwidth scenario, we show that the throughput of the multi-channel MAC schemes is inferior to that of the corresponding single-channel MAC scheme, which sends the RTS/CTS packets and DATA packets on a single shared channel. For the fixed-channel-bandwidth scenario, where CDMA or similar techniques can be applied, we derive the optimal number of the data sub-channels that maximizes the throughput. The analytical framework that we derive in this paper can also be used to evaluate other contention resolution technique, when the average contention period is known",
Comparison of Clustering Algorithms and Protocols for Wireless Sensor Networks,"One of the mechanisms used to enlarge the lifetime of wireless sensor networks (WSN) and to provide more efficient functioning procedures is clustering. By assuming roles within a cluster hierarchy, the nodes in a WSN can control the activities they performed and therefore, reduce their energy consumption. However, the election of when to act as a data provider (saving energy) and when to act as a gateway (cluster head) between the nodes and the base station is not a simple task. To make this decision it is necessary to take into account aspects like power level signal, transmission schedules and networking functioning (proactive or reactive). In this paper we study some basic concepts related to the clustering process in WSN and presenting a comparison survey between different clustering protocols",
New Results for Learning Noisy Parities and Halfspaces,"We address well-studied problems concerning the learn-ability of parities and halfspaces in the presence of classification noise. Learning of parities under the uniform distribution with random classification noise, also called the noisy parity problem is a famous open problem in computational learning. We reduce a number of basic problems regarding learning under the uniform distribution to learning of noisy parities. We show that under the uniform distribution, learning parities with adversarial classification noise reduces to learning parities with random classification noise. Together with the parity learning algorithm of Blum et al. (2003), this gives the first nontrivial algorithm for learning parities with adversarial noise. We show that learning of DNF expressions reduces to learning noisy parities of just logarithmic number of variables. We show that learning of k-juntas reduces to learning noisy parities of k variables. These reductions work even in the presence of random classification noise in the original DNF or junta. We then consider the problem of learning halfspaces over Qopfn with adversarial noise or finding a halfspace that maximizes the agreement rate with a given set of examples. We prove an essentially optimal hardness factor of 2 - epsi, improving the factor of (85/84) - epsi due to Bshouty and Burroughs (2002). Finally, we show that majorities of halfspaces are hard to PAC-learn using any representation, based on the cryptographic assumption underlying the Ajtai-Dwork cryptosystem",
Why Multigrid Methods Are So Efficient,"Originally introduced as a way to numerically solve elliptic boundary-value problems, multigrid methods, and their various multiscale descendants, have since been developed and applied to various problems in many disciplines. This introductory article provides the basic concepts and methods of analysis and outlines some of the difficulties of developing efficient multigrid algorithms",
Grid Capacity Planning with Negotiation-based Advance Reservation for Optimized QoS,"Advance reservation of grid resources can play a key role in enabling grid middleware to deliver on-demand resource provision with significantly improved quality-of-service (QoS). However, in the grid, advance reservation has been largely ignored due to the dynamic grid behavior, underutilization concerns, multi-constrained applications, and lack of support for agreement enforcement. These issues force the grid middleware to make resource allocations at run-time with reduced QoS. To remedy these, we introduce a new, 3-layered negotiation protocol for advance reservation of the grid resources. We model resource allocation as an online strip packing problem and introduce a new mechanism that optimizes resource utilization and QoS constraints while generating the contention-free solutions. The mechanism supports open reservations to deal with the dynamic grid and provides a practical solution for agreement enforcement. We have implemented a prototype and performed experiments to demonstrate the effectiveness of our approach",
Network protocol system monitoring-a formal approach with passive testing,"We study network protocol system monitoring for fault detection using a formal technique of passive testing that is a process of detecting system faults by passively observing its input/output behaviors without interrupting its normal operations. After describing a formal model of event-driven extended finite state machines, we present two algorithms for passive testing of protocol system control and data portions. Experimental results on OSPF and TCP are reported.",
Driving into the Future with ITS,"Intelligent transportation systems have integrated a broad range of AI-based technologies into both the transportation infrastructure and vehicles themselves. Although the future of ITS is promising, the field is anything but futuristic. Various ITS products and services are already at work throughout the world, significantly improving transportation safety, mobility, and productivity",
Steganalysis based on Markov Model of Thresholded Prediction-Error Image,"A steganalysis system based on 2-D Markov chain of thresholded prediction-error image is proposed in this paper. Image pixels are predicted with their neighboring pixels, and the prediction-error image is generated by subtracting the prediction value from the pixel value and then thresholded with a predefined threshold. The empirical transition matrixes of Markov chain along the horizontal, vertical and diagonal directions serve as features for steganalysis. Support vector machines (SVM) are utilized as classifier. The effectiveness of the proposed system has been demonstrated by extensive experimental investigation. The detection rate for Cox et al.'s non-blind spread spectrum (SS) data hiding method, Piva et al.'s blind SS method, and a generic QIM method (as embedding data rate being 0.1 bpp (bits per pixel)) are all above 90% over an image database consisting of approximately 4000 images. For generic LSB method (with various embedding data rates), our steganalysis system achieves a detection rate above 85% as the embedding data rate is 0.1 bpp and above",
BPEL4WS Unit Testing: Test Case Generation Using a Concurrent Path Analysis Approach,"BPEL is a language that could express complex concurrent behaviors. This paper presents a novel method of BPEL test case generation, which is based on concurrent path analysis. This method first uses an extended control flow graph (XCFG) to represent a BPEL program, and generates all the sequential test paths from XCFG. These sequential test paths are then combined to form concurrent test paths. Finally a constraint solver BoNuS is used to solve the constraints of these test paths and generate feasible test cases. Some techniques are proposed to reduce the number of combined concurrent test paths. Some test criteria derived from traditional sequential program testing are also presented to reduce the number of test cases. This method is modularized so that many test techniques such as various test criteria and complex constraint solvers can be applied. This method is tested sound and efficient in experiments. It is also applicable to the testing of other business process languages with possible extension and adaptation",
"A Survey of Large High-Resolution Display Technologies, Techniques, and Applications","Continued advances in display hardware, computing power, networking, and rendering algorithms have all converged to dramatically improve large high-resolution display capabilities. We present a survey on prior research with large high-resolution displays. In the hardware configurations section we examine systems including multi-monitor workstations, reconfigurable projector arrays, and others. Rendering and the data pipeline are addressed with an overview of current technologies. We discuss many applications for large high-resolution displays such as automotive design, scientific visualization, control centers, and others. Quantifying the effects of large high-resolution displays on human performance and other aspects is important as we look toward future advances in display technology and how it is applied in different situations. Interacting with these displays brings a different set of challenges for HCI professionals, so an overview of some of this work is provided. Finally, we present our view of the top ten greatest challenges in large highresolution displays.",
A general noise model and its effects on evolution strategy performance,"Most studies concerned with the effects of noise on the performance of optimization strategies, in general, and on evolutionary approaches, in particular, have assumed a Gaussian noise model. However, practical optimization strategies frequently face situations where the noise is not Gaussian. Noise distributions may be skew or biased, and outliers may be present. The effects of non-Gaussian noise are largely unexplored, and it is unclear whether the insights gained and the recommendations with regard to the sizing of strategy parameters that have been made under the assumption of Gaussian noise bear relevance to more general situations. In this paper, the behavior of a powerful class of recombinative evolution strategies is studied on the sphere model under the assumption of a very general noise model. A performance law is derived, its implications are studied both analytically and numerically, and comparisons with the case of Gaussian noise are drawn. It is seen that while overall, the assumption of Gaussian noise in previous studies is less severe than might have been expected, some significant differences do arise when considering noise that is of unbounded variance, skew, or biased",
CADRE: Cycle-Accurate Deterministic Replay for Hardware Debugging,"One of the main reasons for the difficulty of hardware verification is that hardware platforms are typically nondeterministic at clock-cycle granularity. Uninitialized state elements, I/O, and timing variations on high-speed buses all introduce nondeterminism that causes different behavior on different runs starting from the same initial state. To improve our ability to debug hardware, we would like to completely eliminate nondeterminism. This paper introduces the cycle-accurate deterministic replay (CADRE) architecture, which cost-effectively makes a board-level computer cycle-accurate deterministic. We characterize the sources of nondeterminism in computers and show how to address them. In particular, we introduce a novel scheme to ensure deterministic communication on source-synchronous buses that cross clock-domain boundaries. Experiments show that CADRE on a 4-way multiprocessor server enables cycle-accurate deterministic execution of one-second intervals with modest buffering requirements (around 200MB) and minimal performance loss (around 1%). Moreover, CADRE has modest hardware requirements",
Shadow Removal from a Single Image,"Shadow detection and removal in real scene images is always a challenging but yet intriguing problem. In contrast with the rapidly expanding and continuous interests on this area, it is always hard to provide a robust system to eliminate shadows in static images. This paper aimed to give a comprehensive method to remove both vague and hard shadows from a single image. First, classification is applied to the derivatives of the input image to separate the vague shadows. Then, color invariant is exploited to distinguish the hard shadow edges from the material edges. Next, we derive the illumination image via solving the standard Poisson equation. Finally, we got the shadow-free reflectance image. Experimental results showed that our method can robustly remove both vague and hard shadows appearing in the real scene images",
On the Performance of Flooding-Based Resource Discovery,"We consider flooding-based resource discovery in distributed systems. With flooding, a node searching for a resource contacts its neighbors in the network, which in turn contact their own neighbors and so on until a node possessing the requested resource is located. Flooding assumes no knowledge about the network topology or the resource distribution thus offering an attractive means for resource discovery in dynamically evolving networks such as peer-to-peer systems. We provide analytical results for the performance of a number of flooding-based approaches that differ in the set of neighbors contacted at each step. The performance metrics we are interested in are the probability of locating a resource and the average number of steps and messages for doing so. We study both uniformly random resource requests and requests in the presence of popular (hot) resources. Our analysis is also extended to take into account the fact that nodes may become unavailable either due to failures or voluntary departures from the system. Our analytical results are validated through simulation",
Enhancing security using mobility-based anomaly detection in cellular mobile networks,"Location information is an important feature in users' profiles in cellular mobile networks. In this paper, by exploiting the location history traversed by a mobile user, two domain-independent online anomaly detection schemes are designed, namely the Lempel-Ziv (LZ)-based and Markov-based detection schemes. The authors focus on the identification of a group of especially harmful internal attackers-masqueraders. For both schemes, cell IDs traversed by each mobile user are extracted as the feature value. Specifically, the mobility pattern of each user is characterized by a high-order Markov model. The LZ-based detection scheme from the well-developed data compression techniques is derived. Moreover, the technique of exponentially weighted moving average is used to modify a user's normal profile dynamically. The user profile can characterize the normal behavior of each user accurately and is sensitive to abnormal changes. For the Markov-based detection scheme, a fixed-order Markov model is used to characterize the normal behavior. Based on the constructed probability transition matrix, the probability of the user's current activity is calculated. A threshold policy is then used in both schemes to determine whether a mobile device is potentially compromised or not. Simulation results are presented to show the effectiveness of the proposed schemes. Moreover, our results show that the LZ-based detection scheme performs better than the Markov-based detection scheme, especially for low-speed mobile users",
Super-resolution registration using tissue-classified distance fields,"We present a method for registering the position and orientation of bones across multiple computed-tomography (CT) volumes of the same subject. The method is subvoxel accurate, can operate on multiple bones within a set of volumes, and registers bones that have features commensurate in size to the voxel dimension. First, a geometric object model is extracted from a reference volume image. We use then unsupervised tissue classification to generate from each volume to be registered a super-resolution distance field-a scalar field that specifies, at each point, the signed distance from the point to a material boundary. The distance fields and the geometric bone model are finally used to register an object through the sequence of CT images. In the case of multiobject structures, we infer a motion-directed hierarchy from the distance-field information that allows us to register objects that are not within each other's capture region. We describe a validation framework and evaluate the new technique in contrast with grey-value registration. Results on human wrist data show average accuracy improvements of 74% over grey-value registration. The method is of interest to any intrasubject, same-modality registration applications where subvoxel accuracy is desired.",
The Internet Dark Matter - on the Missing Links in the AS Connectivity Map,,
SOAF: An Architectural Framework for Service Definition and Realization,"Service oriented architecture (SOA) is an approach for building distributed systems that deliver application functionality as a set of self-contained business-aligned services with well-defined and discoverable interfaces. This paper presents a systematic and architecture-centric framework, named service oriented architecture framework (SOAF), to ease the definition, the design and the realization of SOA in order to achieve a better business and IT alignment. The proposed framework is business-process centric and comprises a set of structured activities grouped in five phases. It incorporates a range of techniques and guidelines for systematically identifying services, deciding service granularity and modeling services while integrating existing operational/legacy systems. The results from a pilot validation of SOAF for SOA enablement of a realistic securities trading application are presented. Best practices and lessons learned are also discussed",
Normal and Pathological NCAT Image and Phantom Data Based on Physiologically Realistic Left Ventricle Finite-Element Models,"The four-dimensional (4-D) NURBS-based cardiac-torso (NCAT) phantom, which provides a realistic model of the normal human anatomy and cardiac and respiratory motions, is used in medical imaging research to evaluate and improve imaging devices and techniques, especially dynamic cardiac applications. One limitation of the phantom is that it lacks the ability to accurately simulate altered functions of the heart that result from cardiac pathologies such as coronary artery disease (CAD). The goal of this work was to enhance the 4-D NCAT phantom by incorporating a physiologically based, finite-element (FE) mechanical model of the left ventricle (LV) to simulate both normal and abnormal cardiac motions. The geometry of the FE mechanical model was based on gated high-resolution X-ray multislice computed tomography (MSCT) data of a healthy male subject. The myocardial wall was represented as a transversely isotropic hyperelastic material, with the fiber angle varying from -90deg at the epicardial surface, through 0deg at the midwall, to 90deg at the endocardial surface. A time-varying elastance model was used to simulate fiber contraction, and physiological intraventricular systolic pressure-time curves were applied to simulate the cardiac motion over the entire cardiac cycle. To demonstrate the ability of the FE mechanical model to accurately simulate the normal cardiac motion as well as the abnormal motions indicative of CAD, a normal case and two pathologic cases were simulated and analyzed. In the first pathologic model, a subendocardial anterior ischemic region was defined. A second model was created with a transmural ischemic region defined in the same location. The FE-based deformations were incorporated into the 4-D NCAT cardiac model through the control points that define the cardiac structures in the phantom which were set to move according to the predictions of the mechanical model. A simulation study was performed using the FE-NCAT combination to investigate how the differences in contractile function between the subendocardial and transmural infarcts manifest themselves in myocardial Single photon emission computed tomography (SPECT) images. The normal FE model produced strain distributions that were consistent with those reported in the literature and a motion consistent with that defined in the normal 4-D NCAT beating heart model based on tagged magnetic resonance imaging (MRI) data. The addition of a subendocardial ischemic region changed the average transmural circumferential strain from a contractile value of -0.09 to a tensile value of 0.02. The addition of a transmural ischemic region changed average circumferential strain to a value of 0.13, which is consistent with data reported in the literature. Model results demonstrated differences in contractile function between subendocardial and transmural infarcts and how these differences in function are documented in simulated myocardial SPECT images produced using the 4-D NCAT phantom. Compared with the original NCAT beating heart model, the FE mechanical model produced a more accurate simulation for the cardiac motion abnormalities. Such a model, when incorporated into the 4-D NCAT phantom, has great potential for use in cardiac imaging research. With its enhanced physiologically based cardiac model, the 4-D NCAT phantom can be used to simulate realistic, predictive imaging data of a patient population with varying whole-body anatomy and with varying healthy and diseased states of the heart that will provide a known truth from which to evaluate and improve existing and emerging 4-D imaging techniques used in the diagnosis of cardiac disease","Pathology,
Imaging phantoms,
Finite element methods,
Iron,
Heart,
Predictive models,
Computational modeling,
Myocardium,
Magnetic field induced strain,
Coronary arteriosclerosis"
Search-based software maintenance,"The high cost of software maintenance could potentially be greatly reduced by the automatic refactoring of object-oriented programs to increase their understandability, adaptability and extensibility. This paper describes a novel approach in providing automated refactoring support for software maintenance; the formulation of the task as a search problem in the space of alternative designs. Such a search is guided by a quality evaluation function that must accurately reflect refactoring goals. We have constructed a search-based software maintenance tool and report here the results of experimental refactoring of two Java programs, which yielded improvements in terms of the quality functions used. We also discuss the comparative merits of the three quality functions employed and the actual effect on program design that resulted from their use",
Conductivity image reconstruction from defective data in MREIT: numerical Simulation and animal experiment,"Magnetic resonance electrical impedance tomography (MREIT) is designed to produce high resolution conductivity images of an electrically conducting subject by injecting current and measuring the longitudinal component, B/sub z/, of the induced magnetic flux density B = (B/sub x/,B/sub y/,B/sub z/). In MREIT, accurate measurements of B/sub z/ are essential in producing correct conductivity images. However, the measured B/sub z/ data may contain fundamental defects in local regions where MR magnitude image data are small. These defective B/sub z/ data result in completely wrong conductivity values there and also affect the overall accuracy of reconstructed conductivity images. Hence, these defects should be appropriately recovered in order to carry out any MREIT image reconstruction algorithm. This paper proposes a new method of recovering B/sub z/ data in defective regions based on its physical properties and neighboring information of B/sub z/. The technique will be indispensable for conductivity imaging in MREIT from animal or human subjects including defective regions such as lungs, bones, and any gas-filled internal organs.",
Exploiting parallelism and structure to accelerate the simulation of chip multi-processors,"Simulation is an important means of evaluating new microarchitectures. Current trends toward chip multiprocessors (CMPs) try the ability of designers to develop efficient simulators. CMP simulation speed can be improved by exploiting parallelism in the CMP simulation model. This may be done by either running the simulation on multiple processors or by integrating multiple processors into the simulation to replace simulated processors. Doing so usually requires tedious manual parallelization or re-design to encapsulate processors. This paper presents techniques to perform automated simulator parallelization and hardware integration for CMP structural models. We show that automated parallelization can achieve an 7.60 speedup for a 16-processor CMP model on a conventional 4-processor shared-memory multiprocessor. We demonstrate the power of hardware integration by integrating eight hardware PowerPC cores into a CMP model, achieving a speedup of up to 5.82.",
Spherical Wavelet Descriptors for Content-based 3D Model Retrieval,"The description of 3D shapes with features that possess descriptive power and invariant under similarity transformations is one of the most challenging issues in content based 3D model retrieval. Spherical harmonics-based descriptors have been proposed for obtaining rotation invariant representations. However, spherical harmonic analysis is based on latitude-longitude parameterization of a sphere which has singularities at each pole. Consequently, features near the two poles are over represented while features at the equator are under-sampled, and variations of the north pole affects significantly the shape function. In this paper we discuss these issues and propose the usage of spherical wavelet transform as a tool for the analysis of 3D shapes represented by functions on the unit sphere. We introduce three new descriptors extracted from the wavelet coefficients, namely: (1) a subset of the spherical wavelet coefficients, (2) the L1 and, (3) the L2 energies of the spherical wavelet sub-bands. The advantage of this tool is three fold; first, it takes into account feature localization and local orientations. Second, the energies of the wavelet transform are rotation invariant. Third, shape features are uniformly represented which makes the descriptors more efficient. Spherical wavelet descriptors are natural extension of 3D Zernike moments and spherical harmonics. We evaluate, on the Princeton shape benchmark, the proposed descriptors regarding computational aspects and shape retrieval performance",
Multirobot Simultaneous Localization and Mapping Using Manifold Representations,"This paper describes a novel representation for two-dimensional maps, and shows how this representation may be applied to the problem of multirobot simultaneous localization and mapping. We are inspired by the notion of a manifold, which takes maps out of the two-dimensional plane and onto a surface embedded in a higher-dimensional space. The key advantage of the manifold representation is self-consistency: when closing loops, manifold maps do not suffer from the ""cross over"" problem exhibited in planar maps. This self-consistency, in turn, facilitates a number of important capabilities, including autonomous exploration, search, and retro-traverse. It also supports a very robust form of loop closure, in which pairs of robots act collectively to confirm or reject possible correspondence points. In this paper, we develop the basic formalism of the manifold representation, show how this may be applied to the multirobot simultaneous localization and mapping problem, and present experimental results obtained from teams of up to four robots in environments ranging in size from 400 to 900 m2",
Approximation algorithms for allocation problems: Improving the factor of 1 - 1/e,"Combinatorial allocation problems require allocating items to players in a way that maximizes the total utility. Two such problems received attention recently, and were addressed using the same linear programming (LP) relaxation. In the maximum submodular welfare (SMW) problem, utility functions of players are submodular, and for this case Dobzinski and Schapira [SODA 2006] showed an approximation ratio of 1 - 1/e. In the generalized assignment problem (GAP) utility functions are linear but players also have capacity constraints. GAP admits a (1 - 1/e)-approximation as well, as shown by Fleischer, Goemans, Mirrokni and Sviridenko [SODA 2006]. In both cases, the approximation ratio was in fact shown for a more general version of the problem, for which improving 1 - 1/e is NP-hard. In this paper, we show how to improve the 1 - 1/e approximation ratio, both for SMW and for GAP. A common theme in both improvements is the use of a new and optimal fair contention resolution technique. However, each of the improvements involves a different rounding procedure for the above mentioned LP. In addition, we prove APX-hardness results for SMW (such results were known for GAP). An important feature of our hardness results is that they apply even in very restricted settings, e.g. when every player has nonzero utility only for a constant number of items",
Learning sparse features in granular space for multi-view face detection,"In this paper, a novel sparse feature set is introduced into the Adaboost learning framework for multi-view face detection (MVFD), and a learning algorithm based on heuristic search is developed to select sparse features in granular space. Compared with Haar-like features, sparse features are more generic and powerful to characterize multi-view face pattern that is more diverse and asymmetric than frontal face pattern. In order to cut down search space to a manageable size, we propose a multi-scaled search algorithm that is about 6 times faster than brute-force search. With this method, a MVFD system is implemented that covers face pose changes over +/-45deg rotation in plane (RIP) and +/-90deg rotation off plane (ROP). Experiments over well-know test set are reported to show its high performance in both accuracy and speed",
Coercion-resistance and receipt-freeness in electronic voting,"In this paper we formally study important properties of electronic voting protocols. In particular we are interested in coercion-resistance and receipt-freeness. Intuitively, an election protocol is coercion-resistant if a voter A cannot prove to a potential coercer C that she voted in a particular way. We assume that A cooperates with C in an interactive fashion. Receipt-freeness is a weaker property, for which we assume that A and C cannot interact during the protocol: to break receipt-freeness, A later provides evidence (the receipt) of how she voted. While receipt-freeness can be expressed using observational equivalence from the applied pi calculus, we need to introduce a new relation to capture coercion-resistance. Our formalization of coercion-resistance and receipt-freeness are quite different. Nevertheless, we show in accordance with intuition that coercion-resistance implies receipt-freeness, which implies privacy, the basic anonymity property of voting protocols, as defined in previous work. Finally we illustrate the definitions on a simplified version of the Lee et al. voting protocol",
A formal methods approach to medical device review,"With software playing an increasingly important role in medical devices, regulatory agencies such as the US Food and Drug Administration need effective means for assuring that this software is safe and reliable. The FDA has been striving for a more rigorous engineering-based review strategy to provide this assurance. The use of mathematics-based techniques in the development of software might help accomplish this. However, the lack of standard architectures for medical device software and integrated engineering-tool support for software analysis make a science-based software review process more difficult. The research presented here applies formal modeling methods and static analysis techniques to improve the review process. Regulation of medical device software encompasses reviews of device designs (premarket review) and device performance (postmarket surveillance). The FDA's Center for Devices and Radiological Health performs the premarket review on a device to evaluate its safety and effectiveness. As part of this process, the agency reviews software development life-cycle artifacts for appropriate quality-assurance attributes, which tend to reveal little about the device software integrity.",
Software Carpentry: Getting Scientists to Write Better Code by Making Them More Productive,"For the past years, my colleagues and I have developed a one-semester course that teaches scientists and engineers the ""common core"" of modern software development. Our experience shows that an investment of 150 hours-25 of lectures and the rest of practical work-can improve productivity by roughly 20 percent. That's one day a week, one less semester in a master's degree, or one less year for a typical PhD. The course is called software carpentry, rather than software engineering, to emphasize the fact that it focuses on small-scale and immediately practical issues. All of the material is freely available under an open-source license at www.swc.scipy.org and can be used both for self-study and in the classroom. This article describes what the course contains, and why","Programming profession,
Debugging,
World Wide Web,
Open source software,
Java,
Computer science,
Physics,
Ethics,
Teamwork,
Portable computers"
Slack matching quasi delay-insensitive circuits,Slack matching is an optimization that determines the amount of buffering that must be added to each channel of a slack elastic asynchronous system in order to reduce its cycle time to a specified target. We present two methods of expressing the slack matching problem as a mixed integer linear programming problem. The first method is applicable to systems composed of either full-buffers or half-buffers but not both. The second method is applicable to systems composed of any combination of full-buffers and half-buffers,
Adaptive Privacy-Preserving Authentication in Vehicular Networks,"Vehicular networks have attracted extensive attentions in recent years for their promises in improving safety and enabling other value-added services. Most previous work focuses on designing the media access and physical layer protocols. Privacy issues in vehicular systems have not been well addressed. We argue that privacy is a user-specific concept, and a good privacy protection mechanism should allow users to select the degrees of privacy they wish to have. To address this requirement, we propose an adaptive privacy-preserving authentication mechanism that can trade off the privacy degree with computational and communication overheads (resource usage). This mechanism, to our knowledge, is the first effort on adaptive privacy-preserving authentication. We present analytical and preliminary simulation results to show that the proposed protocol is not only adaptive but also scalable.",
Data Transfers between Processes in an SMP System: Performance Study and Application to MPI,"This paper focuses on the transfer of large data in SMP systems. Achieving good performance for intranode communication is critical for developing an efficient communication system, especially in the context of SMP clusters. We evaluate the performance of five transfer mechanisms: shared-memory buffers, message queues, the Ptrace system call, kernel module-based copy, and a high-speed network. We evaluate each mechanism based on latency, bandwidth, its impact on application cache usage, and its suitability to support MPI two-sided and one-sided messages",
Symbol Recognition with Kernel Density Matching,We propose a novel approach to similarity assessment for graphic symbols. Symbols are represented as 2D kernel densities and their similarity is measured by the Kullback-Leibler divergence. Symbol orientation is found by gradient-based angle searching or independent component analysis. Experimental results show the outstanding performance of this approach in various situations,
History of the plasma display panel,"The 40 year development of the plasma display panel from the first single pixel device, invented at the University of Illinois in 1964, to the 80-in diagonal high-definition television full color products found in the department stores today is presented using photos of the many displays developed during this period. The sources of the critical ideas and inventions used in today's plasma TV products are examined in terms of the physical and electrical principles exploited.",
A Model-Based Approach to the Analysis of Patterns of Length of Stay in Institutional Long-Term Care,"Understanding the pattern of length of stay in institutional long-term care has important practical implications in the management of long-term care. Furthermore, residents' attributes are believed to have significant effects on these patterns. In this paper, we present a model-based approach to extract, from a routinely gathered administrative social care dataset, high-level length-of-stay patterns of residents in long-term care. This approach extends previous work by the authors to incorporate residents' features. Two applications using data provided by a local authority in England are presented to demonstrate the potential use of this approach",
Estimating Road Traffic Congestion using Vehicle Velocity,"This study investigates an alternative way to estimate degrees of road traffic congestion based on routine GPS measurements from main roads in urban areas of Bangkok, Thailand. We classify three levels of traffic congestion according to the weighted exponential moving averages of measured GPS speed. Human perception is used to obtain classification thresholds and evaluated the performance of the proposed method. The benefits of our proposed method over existing techniques is that it is simple, easy to understand, and compatible with existing traffic report systems in Bangkok",
Registering fNIR Data to Brain Surface Image using MRI templates,"Functional near-infrared spectroscopy (fNIR) measures changes in the relative levels of oxygenated and deoxygenated hemoglobin and has increasingly been used to assess neural functioning in the brain. In addition to the ongoing technological developments, investigators have also been conducting studies on functional mapping and refinement of data analytic strategies in order to better understand the relationship between the fNIR signal and brain activity. However, since fNIR is a relatively new functional brain imaging modality as compared to positron emission tomography (PET) and functional magnetic resonance imaging (fMRI), it still lacks brain-mapping tools designed to allow researchers and clinicians to easily interact with their data. The aim of this study is to develop a registration technique for the fNIR measurements using anatomical landmarks and structural magnetic resonance imaging (MRI) templates in order to visualize the brain activation when and where it happens. The proposed registration technique utilizes chain-code algorithm and depicts activations over respective locations based on sensor geometry. Furthermore, registered data locations have been used to create spatiotemporal visualization of fNIR measurements",
"A Cross Validation Study of Deep Brain Stimulation Targeting: From Experts to Atlas-Based, Segmentation-Based and Automatic Registration Algorithms","Validation of image registration algorithms is a difficult task and open-ended problem, usually application-dependent. In this paper, we focus on deep brain stimulation (DBS) targeting for the treatment of movement disorders like Parkinson's disease and essential tremor. DBS involves implantation of an electrode deep inside the brain to electrically stimulate specific areas shutting down the disease's symptoms. The subthalamic nucleus (STN) has turned out to be the optimal target for this kind of surgery. Unfortunately, the STN is in general not clearly distinguishable in common medical imaging modalities. Usual techniques to infer its location are the use of anatomical atlases and visible surrounding landmarks. Surgeons have to adjust the electrode intraoperatively using electrophysiological recordings and macrostimulation tests. We constructed a ground truth derived from specific patients whose STNs are clearly visible on magnetic resonance (MR) T2-weighted images. A patient is chosen as atlas both for the right and left sides. Then, by registering each patient with the atlas using different methods, several estimations of the STN location are obtained. Two studies are driven using our proposed validation scheme. First, a comparison between different atlas-based and nonrigid registration algorithms with a evaluation of their performance and usability to locate the STN automatically. Second, a study of which visible surrounding structures influence the STN location. The two studies are cross validated between them and against expert's variability. Using this scheme, we evaluated the expert's ability against the estimation error provided by the tested algorithms and we demonstrated that automatic STN targeting is possible and as accurate as the expert-driven techniques currently used. We also show which structures have to be taken into account to accurately estimate the STN location","Brain stimulation,
Satellite broadcasting,
Biomedical electrodes,
Surgery,
Image segmentation,
Image registration,
Parkinson's disease,
Biomedical imaging,
Magnetic recording,
Testing"
Dependability analysis of nano-scale FinFET circuits,"FinFET technology has been proposed as a promising alternative for deep sub-micro bulk CMOS technology, because of its better scalability. Previous works have studied the performance or power advantages of FinFET circuits over bulk CMOS circuits. This paper provides the dependability analysis of FinFET circuits, studying the soft error vulnerability of FinFET circuits and the impact of process variation. Our experiments compare FinFET circuits against bulk CMOS circuits in both 32 nm and 45 nm technologies, showing that FinFET circuits have better dependability and scalability, which is indicated by better soft error immunity and less impact of process variation. It is concluded that FinFET-based circuit design is more robust than the bulk CMOS based circuit design","FinFETs,
CMOS technology,
Random access memory,
Scalability,
Circuit analysis,
CMOS process,
Circuit synthesis,
Very large scale integration,
Logic devices,
Computer science"
Biventricular myocardial strains via nonrigid registration of AnFigatomical NURBS models,"We present research in which both left and right ventricular deformation is estimated from tagged cardiac magnetic resonance imaging using volumetric deformable models constructed from nonuniform rational B-splines (NURBS). The four model types considered and compared for the left ventricle include two Cartesian NURBS models-one with a cylindrical parameter assignment and one with a prolate spheroidal parameter assignment. The remaining two are non-Cartesian, i.e., prolate spheroidal and cylindrical each with their respective prolate spheroidal and cylindrical parameter assignment regimes. These choices were made based on the typical shape of the left ventricle. For each frame starting with end-diastole, a NURBS model is constructed by fitting two surfaces with the same parameterization to the corresponding set of epicardial and endocardial contours from which a volumetric model is created. Using normal displacements of the three sets of orthogonal tag planes as well as displacements of contour/tag line intersection points and tag plane intersection points, one can solve for the optimal homogeneous coordinates, in a weighted least squares sense, of the control points of the deformed NURBS model at end-diastole using quadratic programming. This allows for subsequent nonrigid registration of the biventricular model at end-diastole to all later time frames. After registration of the model to all later time points, the registered NURBS models are temporally lofted in order to create a comprehensive four-dimensional NURBS model. From the lofted model, we can extract three-dimensional myocardial deformation fields and corresponding Lagrangian and Eulerian strain maps which are local measures of nonrigid deformation. The results show that, in the case of simulated data, the quadratic Cartesian NURBS models with the cylindrical and prolate spheroidal parameter assignments outperform their counterparts in predicting normal strain. The decreased complexity associated with the Cartesian model with the cylindrical parameter assignment prompted its use for subsequent calculations. Lagrangian strains in three canine data, a normal human, and a patient with history of myocardial infarction are presented. Eulerian strains for the normal human data are also included.","Myocardium,
Capacitive sensors,
Spline,
Surface topography,
Surface reconstruction,
Deformable models,
Surface fitting,
Lagrangian functions,
Strain measurement,
Predictive models"
Towards Effective Content-Based Music Retrieval With Multiple Acoustic Feature Combination,"In this paper, we present a new approach to constructing music descriptors to support efficient content-based music retrieval and classification. The system applies multiple musical properties combined with a hybrid architecture based on principal component analysis (PCA) and a multilayer perceptron neural network. This architecture enables straightforward incorporation of multiple musical feature vectors, based on properties such as timbral texture, pitch, and rhythm structure, into a single low-dimensioned vector that is more effective for classification than the larger individual feature vectors. The use of supervised training enables incorporation of human musical perception that further enhances the classification process. We compare our approach with state of the art techniques and demonstrate its effectiveness on content-based music retrieval. In addition, extensive experimental study illustrates its effectiveness and robustness against various kinds of audio alteration","Music information retrieval,
Content based retrieval,
Humans,
Principal component analysis,
Multiple signal classification,
Neural networks,
Rhythm,
Robustness,
Computer science,
Multilayer perceptrons"
Modular nonblocking verification using conflict equivalence,"This paper proposes a modular approach to verifying whether a large discrete event system is nonconflicting. The new approach avoids computing the synchronous product of a large set of finite-state machines. Instead, the synchronous product is computed gradually, and intermediate results are simplified using conflict-preserving abstractions based on process-algebraic results about fair testing. Heuristics are used to choose between different possible abstractions. Experimental results show that the method is applicable to finite-state machine models of industrial scale and brings considerable improvements in performance over other methods",
"Multiscale modeling: physiome project standards, tools, and databases","The Physiome Project's markup languages and associated tools leverage the CellML and FieldML model databases published in peer-reviewed journals. As these tools mature, researchers can check models for conformance to underlying physics laws, using them to develop complex physiological models from separately validated components",
From Nondeterministic Buchi and Streett Automata to Deterministic Parity Automata,"In this paper we revisit Safra's determinization constructions. We show how to construct deterministic automata with fewer states and, most importantly, parity acceptance conditions. Specifically, starting from a nondeterministic Buchi automaton with n states our construction yields a deterministic parity automaton with n2n+2 states and index 2n (instead of a Rabin automaton with (12)nn2n states and n pairs). Starting from a nondeterministic Streett automaton with n states and k pairs our construction yields a deterministic parity automaton with nn(k+2)+2(k+1)2n(K+1) states and index 2n(k+1) (instead of a Rabin automaton with (12)n(k+1)n n(k+2)(k+1)2n(k+1) states and n(k+1) pairs). The parity condition is much simpler than the Rabin condition. In applications such as solving games and emptiness of tree automata handling the Rabin condition involves an additional multiplier of n2n!(or(n(k+1))2(n(k+1))! in the case of Streett) which is saved using our construction",
Smart Regenerative Relays for Link-Adaptive Cooperative Communications,"Without being necessary to pack multiple antennas per terminal, cooperation among distributed single-antenna nodes offers resilience to shadowing and can, in principle, enhance the performance of wireless communication networks by exploiting the available space diversity. Enabling the latter however, calls for practically implementable protocols to cope with errors at relay nodes so that simple receiver processing can collect the diversity at the destination. To this end, we derive in this paper a class of strategies whereby decoded bits at relay nodes are scaled in power before being forwarded to the destination. The scale is adapted to the signal-to-noise-ratio (SNR) of the source-relay and the intended relay-destination links. With maximum ratio combining (MRC) at the destination, we prove that such link-adaptive regeneration (LAR) strategies effect the maximum possible diversity while requiring simple channel state information that can be pragmatically available at the relay. In addition, LAR exhibits robustness to quantization and feedback errors and leads to efficient use of power both at relay as well as destination nodes. Analysis and corroborating simulations demonstrate that LAR relays are attractive across the practical SNR range; they are universally applicable to multi-branch and multi-hop uncoded or coded settings regardless of the underlying constellation; and outperform existing alternatives.","Relays,
Diversity reception,
Resilience,
Shadow mapping,
Wireless communication,
Protocols,
Decoding,
Signal to noise ratio,
Channel state information,
Robustness"
Rule generation for protein secondary structure prediction with support vector machines and decision tree,"Support vector machines (SVMs) have shown strong generalization ability in a number of application areas, including protein structure prediction. However, the poor comprehensibility hinders the success of the SVM for protein structure prediction. The explanation of how a decision made is important for accepting the machine learning technology, especially for applications such as bioinformatics. The reasonable interpretation is not only useful to guide the ""wet experiments,"" but also the extracted rules are helpful to integrate computational intelligence with symbolic AI systems for advanced deduction. On the other hand, a decision tree has good comprehensibility. In this paper, a novel approach to rule generation for protein secondary structure prediction by integrating merits of both the SVM and decision tree is presented. This approach combines the SVM with decision tree into a new algorithm called SVM_DT, which proceeds in three steps. This algorithm first trains an SVM. Then, a new training set is generated through careful selection from the output of the SVM. Finally, the obtained training set is used to train a decision tree learning system and to extract the corresponding rule sets. The results of the experiments of protein secondary structure prediction on RS126 data set show that the comprehensibility of SVM_DT is much better than that of the SVM. Moreover, the generalization ability of SVM_DT is better than that of C4.5 decision trees and is similar to that of the SVM. Hence, SVM_DT can be used not only for prediction, but also for guiding biological experiments","Proteins,
Support vector machines,
Decision trees,
Support vector machine classification,
Machine learning,
Bioinformatics,
Computer science,
Scholarships,
Pattern recognition"
An Electronic Pillbox for Continuous Monitoring of Medication Adherence,"We have developed an instrumented pillbox, called a MedTracker, which allows monitoring of medication adherence on a continuous basis. This device improves on existing systems by providing mobility, frequent and automatic data collection, more detailed information about non-adherence and medication errors, and the familiar interface of a 7-day drug store pillbox. We report on the design of the MedTracker, and on the results of a field trial in 39 homes to evaluate the device",
Unwrapping of MR phase images using a Markov random field model,"Phase unwrapping is an important problem in many magnetic resonance imaging applications, such as field mapping and flow imaging. The challenge in two-dimensional phase unwrapping lies in distinguishing jumps due to phase wrapping from those due to noise and/or abrupt variations in the actual function. This paper addresses this problem using a Markov random field to model the true phase function, whose parameters are determined by maximizing the a posteriori probability. To reduce the computational complexity of the optimization procedure, an efficient algorithm is also proposed for parameter estimation using a series of dynamic programming connected by the iterated conditional modes. The proposed method has been tested with both simulated and experimental data, yielding better results than some of the state-of-the-art method (e.g., the popular least-squares method) in handling noisy phase images with rapid phase variations.",
WOW: Self-Organizing Wide Area Overlay Networks of Virtual Workstations,"This paper describes WOW, a distributed system that combines virtual machine, overlay networking and peer-to-peer techniques to create scalable wide-area networks of virtual workstations for high-throughput computing. The system is architected to: facilitate the addition of nodes to a pool of resources through the use of system virtual machines (VMs) and self-organizing virtual network links; to maintain IP connectivity even if VMs migrate across network domains; and to present to end-users and applications an environment that is functionally identical to a local-area network or cluster of workstations. We describe a novel, extensible user-level decentralized technique to discover, establish and maintain overlay links to tunnel IP packets over different transports (including UDP and TCP) and across firewalls. We also report on several experiments conducted on a testbed WOW deployment with 118 P2P router nodes over PlanetLab and 33 VMware-based VM nodes distributed across six firewalled domains. Experiments show that the latency in joining a WOW network is of the order of seconds: in a set of 300 trials, 90% of the nodes self-configured P2P routes within 10 seconds, and more than 99% established direct connections to other nodes within 200 seconds. Experiments also show that the testbed delivers good performance for two unmodified, representative benchmarks drawn from the life-sciences domain. The testbed WOW achieves an overall throughput of 53 jobs/minute for PBS-scheduled executions of the MEME application (with average single-job sequential running time of 24.1s) and a parallel speedup of 13.5 for the PVM-based fastDNAml application. Experiments also demonstrate that the system is capable of seamlessly maintaining connectivity at the virtual IP layer for typical client/server applications (NFS, SSH, PBS) when VMs migrate across a WAN",
Modeling Non-Functional Aspects in Service Oriented Architecture,"Service oriented architecture (SOA) is an architectural style to reuse and integrate subsystems in existing systems for designing new applications. Each application is designed in an implementation independent manner using abstract concepts: network services and connections between network services. In SOA, the non-functional aspects of services and connections should be described separately from their functional aspects because different applications use services and connections in different non-functional contexts. This paper proposes a UML profile to graphically design the non-functional aspects in SOA and maintain them in an implementation independent manner. This paper presents the design of the proposed UML profile and describes how it is used in service-oriented application development",
Access Control and Authorization Constraints for WS-BPEL,"Computerized workflow systems have attracted considerable research interest. More recently, there have been several XML-based languages proposed for specifying and orchestrating business processes, culminating in WS-BPEL. A significant omission from WS-BPEL is the ability to specify authorization information associating users with activities in the business process and authorization constraints on the execution of activities such as separation of duty. In this paper, we address these deficiencies by developing the RBAC-WS-BPEL and BPCL languages. The first of these provides for the specification of authorization information associated with a business process specified in WS-BPEL, while BPCL provides for the articulation of authorization constraints",
Fourier-based forward and back-projectors in iterative fan-beam tomographic image reconstruction,"Fourier-based forward and back-projection methods can reduce computation in iterative tomographic image reconstruction. Recently, an optimized nonuniform fast Fourier transform (NUFFT) approach was shown to yield accurate parallel-beam projections. In this paper, we extend the NUFFT approach to describe an O(N/sup 2/ log N) projector/backprojector pair for fan-beam transmission tomography. Simulations and experiments with real CT data show that fan-beam Fourier-based forward and back-projection methods can reduce computation for iterative reconstruction while still providing accuracy comparable to their O(N/sup 3/) space-based counterparts.",
New model for rigorous analysis of LT-codes,"We present a new model for LT codes which simplifies the analysis of the error probability of decoding by belief propagation. For any given degree distribution, we provide the first rigorous expression for the limiting bit-error probability as the length of the code goes to infinity via recent results in random hypergraphs by Darling and Norris, Ann. Appl. Probab., 2005. For a code of finite length, we provide an algorithm for computing the probability of block-error of the decoder. This algorithm improves by a linear factor the algorithm of Karp, Luby, and Shokrollahi, Proc. of ISIT, 2004",
Degrees of Freedom for the MIMO Interference Channel,"We explore the available degrees of freedom (DoF) for the two user MIMO interference channel, and find a general inner bound and a genie aided outer bound that give us the exact # of DoF in many cases. We also study a share-and-transmit scheme and show how the gains of transmitter cooperation are entirely offset by the cost of enabling that cooperation so that the available DoF are not increased",
Criteria and Strategies of Ubiquitous Learning,"Recent progress of wireless technologies has initiated a new trend in learning environments, called ubiquitous learning (u-learning), which is able to sense the situation of the learners, and hence provide more adaptive supports. Many researchers have investigated the development of such learning environments; nevertheless, the criteria of establishing a u-learning environment have not been clearly defined, not to mention the strategies for conducting effective learning activities. To cope with these problems, we shall present in this paper the basic criteria of constructing a u-learning environment, such that subsequent researchers can identify and check the necessary items while establishing the learning environment. Moreover, several strategies for conducting u-learning activities are proposed, based on the given criteria",
Supporting Integrated MAC and PHY Software Development for the USRP SDR,"Software Defined Radios (SDR) offer great runtime flexibility both at the physical and MAC layer. This makes them an attractive platform for the development of cognitive radios that can adapt to changes in channel conditions, traffic load, and user requirements. However, to realize this goal, we need a software framework that supports both MAC protocol and PHY layer development in an integrated fashion. In this paper we report on our experience in using two different software frameworks for integrated PHY-MAC development for SDRs: GNU Radio, which was originally designed to support PHY layer development, and Click, a framework for protocol development. We also discuss a number of broader system considerations, such as what functionality should be offloaded to the SDR device.",
Gait recognition using acceleration from MEMS,"This paper presents an approach on recognising individuals based on 3D acceleration data from walking, which are collected using MEMS. Unlike most other gait recognition methods, which are based on video source, our approach uses walking acceleration in three directions: vertical, backward-forward and sideways. Using gait samples from 21 individuals and applying two methods, histogram similarity and cycle length, the equal error rates of 5% and 9% are achieved, respectively.",
Extended-rtPS Algorithm for VoIP Services in IEEE 802.16 systems,"There are several scheduling algorithms for Voice over IP (VoIP) services in IEEE 802.16 systems, such as unsolicited grant service (UGS), real-time polling service (rtPS), UGS with Activity Detection (UGS-AD), and Lee's algorithm using Grant-Me bit of the generic MAC header. However, these algorithms have some problems of a waste of uplink resources, additional access delay, and MAC overhead for supporting VoIP services with variable data rates and silence suppression. To solve these problems, we propose a novel uplink scheduling algorithm (Extended-rtPS) for the VoIP services in IEEE 802.16 systems. Through the performance analysis and simulation results of resource utilization, VoIP capacity, total throughput, and packet transmission delay, we show that our proposed algorithm can solve the problems of the conventional algorithms, and has the best performance among these algorithms. In addition, with simulation results of packet transmission delay, we prove that our proposed algorithm can support more 74%, 24%, and 9% voice users compared with the UGS, rtPS, and UGS-AD (Lee's) algorithms, respectively.",
Using Angle of Arrival (Bearing) Information in Network Localization,"In this paper, we consider using angle of arrival information (bearing) for sensor network localization. The essential property we require in this paper is that a node can infer heading information from its neighbors. We address the uniqueness of network localization solutions by the theory of globally rigid graphs. We show that while the parallel rigidity problem for formations with bearings is isomorphic to the distance case, the global rigidity of the formation is simpler (in fact identical to the simpler rigidity case) for a network with bearings, compared to formations with distances",
Flow Scheduling for End-Host Multihoming,,
Online energy-aware I/O device scheduling for hard real-time systems,"Much research has focused on power conservation for the processor, while power conservation for I/O devices has received little attention. In this paper, we analyze the problem of online energy-aware I/O scheduling for hard real-time systems based on the preemptive periodic task model. We propose an online energy-aware I/O device scheduling algorithm: energy-efficient device scheduling (EEDS). The EEDS algorithm utilizes device slack to perform device power state transitions to save energy, without jeopardizing temporal correctness. An evaluation of the approach shows that it yields significant energy savings with respect to no dynamic power management (DPM) techniques",
Fast floorplanning by look-ahead enabled recursive bipartitioning,"A new paradigm is introduced for floorplanning any combination of fixed-shape and variable-shape blocks under tight fixed-outline area constraints and a wirelength objective. Dramatic improvement over traditional floorplanning methods is achieved by the explicit construction of strictly legal layouts for every partition block at every level of a cutsize-driven top-down hierarchy. By scalably incorporating legalization into the hierarchical flow, post hoc legalization is successfully eliminated. For large floorplanning benchmarks, an implementation, called partitioning to optimize module arrangement (PATOMA), generates solutions with half the wirelength of state-of-the-art floorplanners in orders of magnitude less run time. Experiments on standard Gigascale Systems Research Center benchmarks compare PATOMA to the Capo macro placer, the Traffic floorplanner, and to both the default and high-effort modes of the Parquet 4.0 floorplanner. With all blocks hard, PATOMA's average wirelength is comparable to the high-effort mode of Parquet 4.0 floorplanner and Capo, while PATOMA runs significantly faster. With all blocks soft, PATOMA produces wirelength 9% shorter on average than that of Parquet's default mode, and PATOMA runs seven times faster. For a new set of benchmarks with a mix of 500 to 2000 hard and soft blocks, PATOMA produces results with wirelengths roughly half of Parquet's, with a speedup of almost 200times",
Routing in Networks with Low Doubling Dimension,"This paper studies compact routing schemes for networks with low doubling dimension. Two variants are explored, name-independent routing and labeled routing. The key results obtained for this model are the following. First, we provide the first name-independent solution. Specifically, we achieve constant stretch and polylogarithmic storage. Second, we obtain the first truly scale-free solutions, namely, the network’s aspect ratio is not a factor in the stretch. Scale-free schemes are given for three problem models: name-independent routing on graphs, labeled routing on metric spaces, and labeled routing on graphs. Third, we prove a lower bound requiring linear storage for stretch \gt 3 schemes. This has the important ramification of separating for the first time the name-independent problem model from the labeled model for these networks, since compact stretch-1+e labeled schemes are known to be possible.","Routing,
Intelligent networks,
Extraterrestrial measurements,
Computer science,
Silicon,
Costs,
Books,
Upper bound"
Fusion of Stereo-Camera and PMD-Camera Data for Real-Time Suited Precise 3D Environment Reconstruction,"3D environment reconstruction is a basic task, delivering the data for mapping, localization and navigation in mobile robotics. We present a new technique that combines a stereo-camera system with a PMD-camera. Both systems generate distance images of the environment but with different characteristics. It is shown that each system compensates effectively for the deficiencies of the other one. The combined system is real-time suited. Experimental data of an indoor scene including the calibration procedure are reported",
An integrated neighborhood correlation and hierarchical clustering approach of functional MRI,"Clustering analysis is a promising data-driven method for the analysis of functional magnetic resonance imaging (fMRI) time series, however, the huge computation load makes it difficult for practical use. In this paper, neighborhood correlation (NC) and hierarchical clustering (HC) methods are integrated as a new approach where fMRI data are processed first by NC to get a preliminary image of brain activations, and then by HC to remove some noises. In HC, to better use spatial and temporal information in fMRI data, a new spatio-temporal measure is introduced. A simulation study and an application to visual fMRI data show that the brain activations can be effectively detected and that different response patterns can be discriminated. These results suggest that the proposed new integrated approach could be useful in detecting weak fMRI signals.",
Update-Based Cache Access and Replacement in Wireless Data Access,"Cache has been applied for wireless data access with different replacement policies in wireless networks. Most of the current cache replacement schemes are access-based replacement policies since they are based on object access frequency/recency information. Access-based replacement policies either ignore or do not focus on update information. However, update information is extremely important since it can make access information almost useless. In this paper, we consider two fundamental and strongly consistent access algorithms: poll-per-read (PER) and call-back (CB). We propose a server-based PER (SB-PER) cache access mechanism in which the server makes replacement decisions and a client-based CB cache access mechanism in which clients make replacement decisions. Both mechanisms have been designed to be suitable for using both update frequency and access frequency. We further propose two update-based replacement policies, least access-to-update ratio (LA2U) and least access-to-update difference (LAUD). We provide a thorough performance analysis via extensive simulations for evaluating these algorithms in terms of access rate, update rate, cache size, database size, object size, etc. Our study shows that although effective hit ratio is a better metric than cache hit ratio, it is a worse metric than transmission cost, and a higher effective hit ratio does not always mean a lower cost. In addition, the proposed SB-PER mechanism is better than the original PER algorithm in terms of effective hit ratio and cost, and the update-based policies outperform access-based policies in most cases",
3D Face Recognition Using 3D Alignment for PCA,"This paper presents a 3D approach for recognizing faces based on Principal Component Analysis (PCA). The approach addresses the issue of proper 3D face alignment required by PCA for maximum data compression and good generalization performance for new untrained faces. This issue has traditionally been addressed by 2D data normalization, a step that eliminates 3D object size information important for the recognition process. We achieve correspondence of facial points by registering a 3D face to a scaled generic 3D reference face and subsequently perform a surface normal search algorithm. 3D scaling of the generic reference face is performed to enable better alignment of facial points while preserving important 3D size information in the input face. The benefits of this approach for 3D face recognition and dimensionality reduction have been demonstrated on components of the Face Recognition Grand Challenge (FRGC) database versions 1 and 2.","Face recognition,
Principal component analysis,
National security,
Image recognition,
Face detection,
Computer science,
Power engineering and energy,
Computer security,
Laboratories,
Data security"
SUBSKY: Efficient Computation of Skylines in Subspaces,"Given a set of multi-dimensional points, the skyline contains the best points according to any preference function that is monotone on all axes. In practice, applications that require skyline analysis usually provide numerous candidate attributes, and various users depending on their interests may issue queries regarding different (small) subsets of the dimensions. Formally, given a relation with a large number (e.g.,ge 10) of attributes, a query aims at finding the skyline in an arbitrary subspace with a low dimensionality (e.g., 2). The existing algorithms do not support subspace skyline retrieval efficiently because they (i) require scanning the entire database at least once, or (ii) are optimized for one particular subspace but incur significant overhead for other subspaces. In this paper, we propose a technique SUBSKY which settles the problem using a single B-tree, and can be implemented in any relational database. The core of SUBSKY is a transformation that converts multi-dimensional data to 1D values, and enables several effective pruning heuristics. Extensive experiments with real data confirm that SUBSKY outperforms alternative approaches significantly in both efficiency and scalability.",
Fractional order PID control of a DC-motor with elastic shaft: a case study,"In this paper, a fractional order PID controller is investigated for a position servomechanism control system considering actuator saturation and the shaft torsional flexibility. For actually implementation, we introduced a modified approximation method to realize the designed fractional order PID controller. Numerous simulation comparisons presented in this paper indicate that, the fractional order PID controller, if properly designed and implemented, will outperform the conventional integer order PID controller",
How to Determine a Good Multi-Programming Level for External Scheduling,"Scheduling/prioritization of DBMS transactions is important for many applications that rely on database backends. A convenient way to achieve scheduling is to limit the number of transactions within the database, maintaining most of the transactions in an external queue, which can be ordered as desired by the application. While external scheduling has many advantages in that it doesn’t require changes to internal resources, it is also difficult to get right in that its performance depends critically on the particular multiprogramming limit used (the MPL), i.e. the number of transactions allowed into the database. If the MPL is too low, throughput will suffer, since not all DBMS resources will be utilized. On the other hand, if the MPL is too high, there is insufficient control on scheduling. The question of how to adjust theMPL to achieve both goals simultaneously is an open problem, not just for databases but in system design in general. Herein we study this problem in the context of transactional workloads, both via extensive experimentation and queueing theoretic analysis. We find that the two most critical factors in adjusting the MPL are the number of resources that the workload utilizes and the variability of the transactions’ service demands. We develop a feedback based controller, augmented by queueing theoretic models for automatically adjusting the MPL. Finally, we apply our methods to the specific problem of external prioritization of transactions. We find that external prioritization can be nearly as effective as internal prioritization, without any negative consequences, when the MPL is set appropriately.","Transaction databases,
Processor scheduling,
Queueing analysis,
Quality of service,
Computer science,
Application software,
Throughput,
System analysis and design,
Feedback,
Automatic control"
Analytical IP Alias Resolution,"IP alias resolution is an important step in generating sample Internet topologies from collected path traces. Inaccuracies in IP alias resolution may significantly affect the characteristics of the resulting sample topologies. This in turn affects the accuracy of measurement results obtained using such topologies. Existing tools for alias resolution use an active probing approach. They induce significant traffic overhead into the network and critically depend on the participation of the routers. Recent studies have reported on the limited effectiveness of these approaches [1], [2]. In this paper, we present a novel approach, called Analytical Alias Resolver (AAR), for IP alias resolution. Given a set of path traces, AAR utilizes the common IP address assignment scheme to infer IP aliases from the collected path traces. AAR does not incur traffic overhead due to active probing for alias resolution. Our experimental evaluations on a set of collected Internet path traces show that, compared to existing approaches, AAR can detect significantly more number of IP aliases.",
Robust 3D scan point classification using associative Markov networks,"In this paper we present an efficient technique to learn associative Markov networks (AMNs) for the segmentation of 3D scan data. Our technique is an extension of the work recently presented by Anguelov et al. (2005), in which AMNs are applied and the learning is done using max-margin optimization. In this paper we show that by adaptively reducing the training data, the training process can be performed much more efficiently while still achieving good classification results. The reduction is obtained by utilizing kd-trees and pruning them appropriately. Our algorithm does not require any additional parameters and yields an abstraction of the training data. In experiments with real data collected from a mobile outdoor robot we demonstrate that our approach yields accurate segmentations",
Ensemble of Piecewise FDA Based on Spatial Histograms of Local (Gabor) Binary Patterns for Face Recognition,"Spatial histogram* of Local Binary Pattern (LBP) and Local Gabor Binary Pattern (LGBP) has been successfully applied to face recognition and achieved state-of-the-art performance. Both LBP and LGBP utilize traditional histogram matching method such as histogram intersection for face classification. In this paper, we propose a statistical extension for L(G)BP similarity computation by introducing Fisher Discriminant Analysis (FDA) of the L(G)BP spatial histogram ""features"". More than a simple application of FDA, we have constructed Ensemble of Piecewise FDA (EPFDA) classifiers, each of which is designed using one segment of the entire spatial histogram features. We show that this extension not only greatly reduces the feature dimension but also brings very impressive performance improvement. Especially, we have made a large step to recognizing all the faces in the standard FERET face database.",
A Vertical Handover Decision Algorithm Based on Fuzzy Control Theory,"The paper mainly deals with a vertical handover decision algorithm based on the fuzzy control theory. The algorithm takes into consider the factors of power level, cost and bandwidth. After establishing the membership functions, membership degrees of corresponding factors can be determined, which are processed by the weight vector. Finally, the fuzzy vertical handoff decision vector is derived and vertical handover decision can be made. It is shown through simulation that the algorithm realizes the optimized vertical handover by evaluating and analyzing various input parameters",
Real Time Feedback Control for Nonholonomic Mobile Robots With Obstacles,"We introduce a method for constructing smooth feedback laws for a nonholonomic robot in a 2-dimensional polygonal workspace. First, we compute a smooth feedback law in the workspace without taking the nonholonomic constraints into account. We then give a general technique for using this to construct a new smooth feedback law over the entire 3-dimensional configuration space (consisting of position and orientation). The trajectories of the resulting feedback law will be smooth and will stabilize the position of the robot in the plane, neglecting the orientation. Our method is suitable for real time implementation and can be applied to dynamic environments",
Offloading IDS Computation to the GPU,"Signature-matching intrusion detection systems can experience significant decreases in performance when the load on the IDS-host increases. We propose a solution that off-loads some of the computation performed by the IDS to the graphics processing unit (GPU). Modern GPUs are programmable, stream-processors capable of high-performance computing that in recent years have been used in non-graphical computing tasks. The major operation in a signature-matching IDS is matching values seen operation to known black-listed values, as such, our solution implements the string-matching on the GPU. The results show that as the CPU load on the IDS host system increases, PixelSnort's performance is significantly more robust and is able to outperform conventional Snort by up to 40%",
The role of physical embodiment in human-robot interaction,"Autonomous robots are agents with physical bodies that share our environment. In this work, we test the hypothesis that physical embodiment has a measurable effect on performance and perception of social interactions. Support of this hypothesis would suggest fundamental differences between virtual agents and robots from a social standpoint and have significant implications for human-robot interaction. We measure task performance and perception of a robot's social abilities in a structured but open-ended task based on the Towers of Hanoi puzzle. Our experiment compares aspects of embodiment by evaluating: (1) the difference between a physical robot and a simulated one; (2) the effect of physical presence through a co-located robot versus a remote tele-present robot. We present data from a pilot study with 12 subjects showing interesting differences in perception of remote physical robot's and simulated agent's attention to the task, and task enjoyment",
3D Touchless Fingerprints: Compatibility with Legacy Rolled Images,"Fingerprints are traditionally captured based on contact of the finger on paper or a platen surface. This often results in partial or degraded images due to improper finger placement, skin deformation, slippage and smearing, or sensor noise from wear and tear of surface coatings. A new generation of touchless live scan devices that generate 3D representation of fingerprints is appearing in the market. This new sensing technology addresses many of the problems stated above. However, 3D touchless fingerprint images need to be compatible with the legacy rolled images used in automated fingerprint identification systems (AFIS). In order to solve this interoperability issue, we propose an unwrapping algorithm that unfolds the 3D fingerprint in such a way that it resembles the effect of virtually rolling the 3D finger on a 2D plane. Our preliminary experiments show promising results in obtaining touchless fingerprint images that are of high quality and at the same time compatible with legacy rolled fingerprint images.",
Graph Partitioning by Spectral Rounding: Applications in Image Segmentation and Clustering,"We introduce a family of spectral partitioning methods. Edge separators of a graph are produced by iteratively reweighting the edges until the graph disconnects into the prescribed number of components. At each iteration a small number of eigenvectors with small eigenvalue are computed and used to determine the reweighting. In this way spectral rounding directly produces discrete solutions where as current spectral algorithms must map the continuous eigenvectors to discrete solutions by employing a heuristic geometric separator (e.g. k-means). We show that spectral rounding compares favorably to current spectral approximations on the Normalized Cut criterion (NCut). Results are given for natural image segmentation, medical image segmentation, and clustering. A practical version is shown to converge.",
PHONEY: mimicking user response to detect phishing attacks,"Phishing scams pose a serious threat to end-users and commercial institutions alike. Email continues to be the favorite vehicle to perpetrate such scams mainly due to its widespread use combined with the ability to easily spoof them. Several approaches, both generic and specialized, have been proposed to address this problem. However, phishing techniques, growing in ingenuity as well as sophistication, render these solutions weak. In this paper we propose a novel approach to detect phishing attacks using fake responses which mimic real users, essentially, reversing the role of the victim and the adversary. Our prototype implementation called PHONEY, sits between a user's mail transfer agent (MTA) and mail user agent (MUA) and processes each arriving email for phishing attacks. Using live email data collected over a period of eight months we demonstrate data that our approach is able to detect a wider range of phishing attacks than existing schemes. Also, the performance analysis study shows that the implementation overhead introduced by our tool is very negligible",
Thesaurus based automatic keyphrase indexing,We propose a new method that enhances automatic keyphrase extraction by using semantic information on terms and phrases gleaned from a domain-specific thesaurus. We evaluate the results against keyphrase sets assigned by a state-of-the-art keyphrase extraction system and those assigned by six professional indexers,
Are GSM Phones THE Solution for Localization?,"In this paper, we argue that localization solution based on cellular phone technology, specifically GSM phones, is a sufficient and attractive option in terms of coverage and accuracy for a wide range of indoor, outdoor, and placebased location-aware applications. We present preliminary results that indicate that GSM-based localization systems have the potential to detect the places that people visit in their everyday lives, and can achieve median localization accuracies of 5 and 75 meters for indoor and outdoor environments, respectively.",
Distributive High-Rate Full-Diversity Space-Frequency Codes for Asynchronous Cooperative Communications,"In user cooperative communications, relay nodes are usually asynchronous. By realizing that the processing in the frequency domain is insensitive to the errors in the time domain, Mei et. al., (2005) recently applied the OFDM technique to achieve the full cooperative diversity for asynchronous cooperative communications, where orthogonal space-time block codes, Alamouti code in particular, were used for relay nodes and the channels from nodes to nodes are assumed flat fading. In this paper, we also consider asynchronous cooperative communications, but the channels from nodes to nodes are assumed to be frequency-selective fading. We use high rate space-frequency codes to encode the correctly detected information symbols across some subcarriers at relay nodes to achieve both full cooperative diversity and full multipath diversity for asynchronous communications, which, we show, holds if the recent high rate space-frequency codes constructed in W. Zhang et al., (2005) are used",
Services Science: A New Field for Today's Economy,"Agriculture and manufacturing used to be the major elements of the modern world's economies. Now, services are a critical element, a trend also affecting the developing world. Universities throughout the world - most notably in North America, Europe, and Australia - are offering courses and graduate-level certification in services science, with the long-term goal of establishing degree programs. Services science is interested in both relatively simple service businesses such as fast-food restaurants and more sophisticated operations such as healthcare companies. Businesses could use technologies such as knowledge management and data mining to get targeted analytical information they can use to evaluate their operations. Companies can utilize technology to find patterns in the way they have successfully delivered services and interacted with customers. The companies could then repeat those patterns with multiple customers. Although technology is a key element of services science, a better understanding of human behavior is critical. The field calls on the resources of social sciences such as psychology and sociology, as well as anthropology, which could provide useful information about the way people and groups work and interact. Understanding these factors is an important aspect of services science. Businesses are also employing services science principles in their operations","Companies,
Agriculture,
Manufacturing,
Educational institutions,
North America,
Europe,
Australia,
Certification,
Medical services,
Knowledge management"
Towards robust on-line multi-robot coverage,"Area coverage is an important task for mobile robots, with many real-world applications. In many cases, the coverage has to be completed without the use of a map or any a priori knowledge about the area, a process referred-to as on-line coverage. Previous investigations of multi-robot on-line coverage focused on the improved efficiency gained from the use of multiple robots, but did not formally addressed the potential for greater robustness. We present a novel multi-robot on-line coverage algorithm, based on approximate cell decomposition. We analytically show that the algorithm is complete and robust, in that as long as a single robot is able to move, the coverage would be completed. We analyze the assumptions underlying the algorithm requirements and present a number of techniques for executing it in real robots. We show empirical coverage-time results of running the algorithm in two different environments and several group sizes",
Information Quality as a Common Ground for Key Players in e-Government Integration and Interoperability,"Whereas computer-supported sharing of information is a core issue of e-Government integration and interoperation, the key players involved hardly find any guidance how to settle for an agreement covering the various aspects related to information exchange. Hence, we argue that e-Government research needs an explicit dedication to the phenomenon of information quality (IQ). We base our analysis on key players and constituents in e-Government, and on how those key players’ and constituents’ needs and wants, roles and agendas shape the view on and the understanding of IQ. We propose detailed and practical steps for information sharing in e-Government interoperation projects and demonstrate how those steps are related to information quality.",
Deterministic packet marking based on redundant decomposition for IP traceback,"A novel deterministic packet marking scheme for IP trace back against distributed denial of service attacks is presented. Besides the hash correlation functions, our scheme has a unique technique: redundant decomposition, which plays an important role in improving the recovery performance. Theoretical analyses, the pseudo code and the experimental results are provided. The scheme is proved to be accurate and efficient and can handle large-scale DDoS attacks.","Computer crime,
Large-scale systems,
TCPIP,
Telecommunication traffic,
Intrusion detection,
Stochastic processes,
Computer science education,
Educational technology,
Computer science,
Information science"
Automatic feature extraction for multiview 3D face recognition,"Current 2D face recognition systems encounter difficulties in recognizing faces with large pose variations. Utilizing the pose-invariant features of 3D face data has the potential to handle multiview face matching. A feature extractor based on the directional maximum is proposed to estimate the nose tip location and the pose angle simultaneously. A nose profile model represented by subspaces is used to select the best candidates for the nose tip. Assisted by a statistical feature location model, a multimodal scheme is presented to extract eye and mouth corners. Using the automatic feature extractor, a fully automatic 3D face recognition system is developed. The system is evaluated on two databases, the MSU database (300 multiview test scans from 100 subjects) and the UND database (953 near frontal scans from 277 subjects). The automatic system provides recognition accuracy that is comparable to the accuracy of a system with manually labeled feature points",
"An Empirical Study into the Use of Chernoff Information for Robust, Distributed Fusion of Gaussian Mixture Models","This paper considers the problem of developing algorithms for the distributed fusion of Gaussian mixture models through the use of Chernoff information. We derive a first order approximation and show that, in a distributed tracking problem in which sensor nodes are equipped with only range-only or bearing-only sensors, it yields consistent estimates",
A sampling ADC data acquisition system for positron emission tomography,"A data acquisition system for a positron emission tomograph (PET) based on avalanche photodiode (APD) readout of lutetium oxyorthosilicate (LSO) scintillator crystals is presented. The analog data of each APD are read out by fast analog to digital converters (ADCs) and processed within field programmable gate arrays (FPGAs). The ADCs are continuously sampling with a 80 MHz, low jitter clock, which is synchronous for the whole detector. The main tasks of the FPGAs are pulse detection and extraction of signal timing information from the digitized data stream. The detected signal pulse data are therefore compared to a predefined set of pulse shapes with known phase shifts with respect to the sample clock. By searching for the best match within this calibration set, a precise start time information for the signal pulse can be determined. The calculated time values are then transmitted from the ADC cards via fiber optic links to multiplexer modules which combine the different data streams and can also perform further processing like search for coincident events. Finally, the preprocessed detector data are transmitted from the multiplexers to PCI cards in the image reconstruction computers by gigabit optical links",
Evaluation of algorithms for bearing-only SLAM,"An important milestone for building affordable robots that can become widely popular is to address robustly the simultaneous localization and mapping (SLAM) problem with inexpensive, off-the-shelf sensors, such as monocular cameras. These sensors, however, impose significant challenges on SLAM procedures because they provide only bearing data related to environmental landmarks. This paper starts by providing an extensive comparison of different techniques for bearing-only SLAM in terms of robustness under different noise models, landmark densities and robot paths. We have experimented in a simulated environment with a variety of existing online algorithms including Rao-Blackwellized particle filters (RB-PFs). Our experiments suggest that RB-PFs are more robust compared to other existing methods and run considerably faster. Nevertheless, their performance suffers in the presence of outliers. In order to overcome this limitation we proceed to propose an augmentation of RB-PFs with: (a) Gaussian sum filters for landmark initialization and (b) an online, unsupervised outlier rejection policy. This framework exhibits impressive robustness and efficiency even in the presence of outliers",
Mining Domain-Specific Thesauri from Wikipedia: A Case Study,"Domain-specific thesauri are high-cost, high-maintenance, high-value knowledge structures. We show how the classic thesaurus structure of terms and links can be mined automatically from Wikipedia. In a comparison with a professional thesaurus for agriculture we find that Wikipedia contains a substantial proportion of its concepts and semantic relations; furthermore it has impressive coverage of contemporary documents in the domain. Thesauri derived using our techniques capitalize on existing public efforts and tend to reflect contemporary language usage better than their costly, painstakingly-constructed manual counterparts",
Increasing the Awareness of Daily Activity Levels with Pervasive Computing,"Public health promotion technology should be accessible to the general public at which it is aimed. This paper explores the potential for use of an unaugmented commodity technology - the mobile phone - as a health promotion tool. We describe a prototype application that tracks the daily exercise activities of people carrying phones, using fluctuation in signal strength to estimate a user's movement. In a short-term study of the prototype that shared activity information amongst groups of friends, we found that awareness encouraged reflection on, and increased motivation for, daily activity. We describe some of the details of the pilot study, and conclude with our intended plans to develop the system further in order to carry out a longer-term clinical trial",
Source Code Exploration with Google,"The paper presents a new approach to source code exploration, which is the result of integrating the Google Desktop Search (GDS) engine into the Eclipse development environment. The resulting search engine, named Google Eclipse Search (GES), provides improved searching in Eclipse software projects. The paper advocates for a component-based approach that allows us to develop strong tools, which support various maintenance tasks, by leveraging the strengths of existing frameworks and components. The development effort for such tools is reduced, while customization and flexibility, to fully support user needs, is maintained. GES allows developers to search software projects in a manner similar to searching the Internet or their own desktops. The proposed approach takes advantages of the power of GDS for quick and accurate searching and of Eclipse's extensibility. The paper discusses usage scenarios, advantages, limitations, and possible extensions of the proposed tandem",
Locating Sensors in Concave Areas,,
Joint Allocation of Subcarriers and Transmit Powers in a Multiuser OFDM Cellular Network,"In the present paper, we consider the problem of joint bandwidth (subcarriers) and power allocation for the downlink of a multi-user multi-cell OFDM cellular network. This resource allocation problem is formulated as a power minimization problem, subject to meeting the target rates of all users in the network. We develop a distributed solution to find the globally optimal allocation which determines the subcarrier and power allocation dynamically. In addition, we investigate the impact of reducing the complexity by reducing the number of degrees of freedom available in the optimization. In particular, we consider a static bandwidth allocation scheme, and a static power allocation scheme. The numerical results show that the penalty on network performance due to the reduction in the available degrees of freedom is not significant.","OFDM,
Land mobile radio cellular systems,
Base stations,
Bandwidth,
Interference constraints,
Cellular networks,
Channel allocation,
Wireless LAN,
Downlink,
Resource management"
Traffic-Aware Firewall Optimization Strategies,"The overall performance of a firewall is crucial in enforcing and administrating security, especially when the network is under attack. The continuous growth of the Internet, coupled with the increasing sophistication of the attacks, is placing stringent demands on firewall performance. In this paper, we describe a traffic-aware optimization framework to improve the operational cost of firewalls. Based on this framework, we design a set of tools that inspect and analyze both multidimensional firewall rules and traffic logs and construct the optimal equivalent firewall rules based on the observed traffic characteristics. To the best of our knowledge, this work is the first to use traffic characteristics in firewall optimization. Furthermore, we develop a novel adaptation mechanism that dynamically detects anomalous traffic behavior and adaptively alters the firewall rules to avoid serious performance degradation due to the traffic anomaly. To evaluate the performance of our approaches, we collected a large set of firewall rules and traffic logs at tens of enterprise networks managed by a Tier-1 service provider. Our evaluation results find these approaches very effective. In particular, we achieve more than 10 fold performance improvement by using the proposed traffic-aware firewall optimization.","Telecommunication traffic,
Design optimization,
Computer security,
Protection,
Information security,
Computer science,
Bandwidth,
Hardware,
Adaptive systems,
Communication system traffic control"
Online Multicasting for Network Capacity Maximization in Energy-Constrained Ad Hoc Networks,"In this paper, we present new algorithms for online multicast routing in ad hoc networks where nodes are energy-constrained. The objective is to maximize the total amount of multicast message data routed successfully over the network without any knowledge of future multicast request arrivals and generation rates. Specifically, we first propose an online algorithm for the problem based on an exponential function of energy utilization at each node. The competitive ratio of the proposed algorithm is analyzed if admission control of multicast requests is permitted. We then provide another online algorithm for the problem, which is based on minimizing transmission energy consumption for each multicast request and guaranteeing that the local network lifetime is no less than 7 times of the optimum, where 7 is constant with 0 < gamma < 1. We finally conduct extensive experiments by simulations to analyze the performance of the proposed algorithms, in terms of network capacity, network lifetime, and transmission energy consumption for each multicast request. The experimental results clearly indicate that, for online multicast routing in ad hoc wireless networks, the network capacity is proportional to the network lifetime if the transmission energy consumption for each multicast request is at the same time minimized. This is in contrast to the implication by Kar et al. that the network lifetime is proportional to the network capacity when they considered the online unicast routing by devising an algorithm based on the exponential function of energy utilization at each node",
Guest Editorial Validation in Medical Image Processing,,
Detecting and Isolating Malicious Routers,"Network routers occupy a unique role in modern distributed systems. They are responsible for cooperatively shuttling packets amongst themselves in order to provide the illusion of a network with universal point-to-point connectivity. However, this illusion is shattered - as are implicit assumptions of availability, confidentiality, or integrity - when network routers are subverted to act in a malicious fashion. By manipulating, diverting, or dropping packets arriving at a compromised router, an attacker can trivially mount denial-of-service, surveillance, or man-in-the-middle attacks on end host systems. Consequently, Internet routers have become a choice target for would-be attackers and thousands have been subverted to these ends. In this paper, we specify this problem of detecting routers with incorrect packet forwarding behavior and we explore the design space of protocols that implement such a detector. We further present a concrete protocol that is likely inexpensive enough for practical implementation at scale. Finally, we present a prototype system, called Fatih, that implements this approach on a PC router and describe our experiences with it. We show that Fatih is able to detect and isolate a range of malicious router actions with acceptable overhead and complexity. We believe our work is an important step in being able to tolerate attacks on key network infrastructure components",
Space-Time Video Montage,"Conventional video summarization methods focus predominantly on summarizing videos along the time axis, such as building a movie trailer: The resulting video trailer tends to retain much empty space in the background of the video frames while discarding much informative video content due to size limit. In this paper we propose a novel spacetime video summarization method which we call space-time video montage. The method simultaneously analyzes both the spatial and temporal injbrmation distribution in a video sequence, and extracts the visually informative space-time portions of the input videos. The informative video porlions are represented in volumetric layers. The layers are then packrd together in a smull ouzput video volume such that the total amount of visual information in the video volume is maximized. To achieve the packing process, we develop a new algorithm based upon the first-fit and Graph cut optimization techniques. Since our method is uble to cut off spatially und temporally less informative portions, it is uble to generate much more compact yet highly informative output videos. The effecliveness of our method is validated by extensive experiments over a wide variety of videos.",
Faster Solutions of Rabin and Streett Games,"In this paper we improve the complexity of solving Rabin and Streett games to approximately the square root of previous bounds. We introduce direct Rabin and Streett ranking that are a sound and complete way to characterize the winning sets in the respective games. By computing directly and explicitly the ranking we can solve such games in time O(mnk+1kk!) and space O(nk) for Rabin and O(nkk!) for Streett where n is the number of states, m the number of transitions, and k the number of pairs in the winning condition. In order to prove completeness of the ranking method we give a recursive fixpoint characterization of the winning regions in these games. We then show that by keeping intermediate values during the fixpoint evaluation, we can solve such games symbolically in time O(nk+1k!) and space O(nk+1k!). These results improve on the current bounds of O(mn2kk!) time in the case of direct (symbolic) solution or O(m(nk2k!)k) in the case of reduction to parity games",
A Genetic Binary Particle Swarm Optimization Model,"In this paper, a genetic binary particle swarm optimization (GBPSO) model is proposed, and its performance is compared with the regular binary particle swarm optimizer (PSO), introduced by Kennedy and Eberhart. In the original model, the size of the swarm was fixed. In our model, we introduce birth and death operations in order to make the population very dynamic. Since birth and mortality rates change naturally with time, our model allows oscillations in the size of the population. Compared to the original PSO model, and genetic algorithms, our strategy proposes a more natural simulation of the social behavior of intelligent animals. The experimental results show that compared to original PSO, our GBPSO model can reach broader domains in the search space and converge faster in very high dimensional and complex environments.",
DDSOS: a dynamic distributed service-oriented simulation framework,"This paper presents the DDSOS framework developed at Arizona State University, which supports the simulation, development, and evaluation of large scale distributed systems such as network-centric and system-of-systems applications. The distinct features of the framework include automated simulation code generation from the specification, code deployment, simulation of different architectures with a template-based platform builder, service-oriented multi-agent simulation for easy reconfiguration, and dynamic analyses of results from evaluation and monitoring. The framework and the associated tools have been implemented and applied in several governmental and industrial projects.",
Privacy Preserving Nearest Neighbor Search,"Data mining is frequently obstructed by privacy concerns. In many cases data is distributed, and bringing the data together in one place for analysis is not possible due to privacy laws (e.g. HIPAA) or policies. Privacy preserving data mining techniques have been developed to address this issue by providing mechanisms to mine the data while giving certain privacy guarantees. In this work we address the issue of privacy preserving nearest neighbor search, which forms the kernel of many data mining applications. To this end, we present a novel algorithm based on secure multiparty computation primitives to compute the nearest neighbors of records in horizontally distributed data. We show how this algorithm can be used in three important data mining algorithms, namely LOF outlier detection, SNN clustering, and kNN classification",
Quantitative Cyber Risk Reduction Estimation Methodology for a Small SCADA Control System,"We propose a new methodology for obtaining a quantitative measurement of the risk reduction achieved when a control system is modified with the intent to improve cyber security defense against external attackers. The proposed methodology employs a directed graph called a compromise graph, where the nodes represent stages of a potential attack and the edges represent the expected time-to-compromise for differing attacker skill levels. Time-to-compromise is modeled as a function of known vulnerabilities and attacker skill level. The methodology was used to calculate risk reduction estimates for a specific SCADA system and for a specific set of control system security remedial actions. Despite an 86% reduction in the total number of vulnerabilities, the estimated time-to-compromise was increased only by about 3 to 30% depending on target and attacker skill level.",
Detecting selective forwarding attacks in wireless sensor networks,"Selective forwarding attacks may corrupt some mission-critical applications such as military surveillance and forest fire monitoring. In these attacks, malicious nodes behave like normal nodes in most time but selectively drop sensitive packets, such as a packet reporting the movement of the opposing forces. Such selective dropping is hard to detect. In this paper, we propose a lightweight security scheme for detecting selective forwarding attacks. The detection scheme uses a multi-hop acknowledgement technique to launch alarms by obtaining responses from intermediate nodes. This scheme is efficient and reliable in the sense that an intermediate node reports any abnormal packet loss and suspect nodes to both the base station and the source node. To the best of our knowledge, this is the first paper that presents a detailed scheme for detecting selective forwarding attacks in the environment of sensor networks. The simulation results show that even when the channel error rate is 15%, simulating very harsh radio conditions, the detection accuracy of the proposed scheme is over 95%.",
FLAW: FPGA lifetime awareness,"Aggressive scaling of technology has an adverse impact on the reliability of VLSI circuits. Apart from increasing transient error susceptibility, the circuits also become more vulnerable to permanent damage and failures due to different physical phenomenon. Such concerns have been recently demonstrated for regular micro-architectures. In this work we demonstrate the vulnerability of field programmable gate arrays (FPGA)s to two different types of hard errors, namely, time dependent dielectric breakdown (TDDB) and electro-migration. We also analyze the performance degradation of FPGAs over time caused by hot carrier effects (HCE). We also propose three novel techniques to counter such aging based failures and increase the lifetime of the device",
Inter-regional messenger scheduling in delay tolerant mobile networks,"The evolution of wireless devices along with the increase in user mobility have created new challenges such as network partitioning and intermittent connectivity. These new challenges have become apparent in many situations where the transmission of critical data is of high priority. Disaster rescue groups, for example, are equipped with numerous devices which constantly gather and transmit various forms of data. The challenge of establishing communication between groups of this type has led to an evolutionary form of networks which we consider in this paper, namely, delay tolerant mobile networks (DTMNs). Nodes in DTMNs usually form clusters that we define as regions. Nodes within each region have end-to-end paths between them. Both regions, as well as nodes within a region, can be either stationary or mobile. For such environments, we propose using a dedicated set of messengers that relay message bundles between these regions. Our goal is to understand how messenger scheduling can be used to improve network performance and connectedness. We develop several classes of messenger scheduling algorithms which can be used to achieve inter-regional communication in such environments. We use simulation to better understand the performance and tradeoffs between these algorithms",
Exploring Random Access and Handshaking Techniques in Large-Scale Underwater Wireless Acoustic Sensor Networks,"In this paper, we study the medium access control (MAC) problem in large-scale underwater wireless acoustic sensor networks. We specially explore the random access and handshaking (i.e., RTS/CTS) techniques. We first formally model the two approaches, and then conduct extensive numerical experiments to study their performance in various network conditions. Based on our results, we observe that the performance of random access and RTS/CTS are affected by many factors such as data rate, transmission range, network topology, packet size, and traffic pattern. And our results show that RTS/CTS is more suitable for dense networks with high date rate, low/medium transmission range and bursty traffic, whereas random access is preferred in sparse networks with low data rate and non-bursty traffic. We believe this work will supply useful guidelines for energy-efficient adaptive MAC design in large-scale underwater wireless acoustic sensor networks","Large-scale systems,
Wireless sensor networks,
Acoustic sensors,
Access protocols,
Media Access Protocol,
Sensor phenomena and characterization,
Multiaccess communication,
Telecommunication traffic,
Throughput,
Underwater communication"
New Design of Low-Correlation Zone Sequence Sets,"In this paper, we present several construction methods for low-correlation zone (LCZ) sequence sets. First, we propose a design scheme for binary LCZ sequence sets with parameters (2n+1-2,M,L,2). In this scheme, we can freely set the LCZ length L and the resulting LCZ sequence sets have the size M, which is almost optimal with respect to Tang, Fan, and Matsufuji bound. Second, given a q-ary LCZ sequence set with parameters (N,M,L,epsi) and even q, we construct another q-ary LCZ sequence set with parameters (2N,2M,L,2epsi) or (2N,2M,L-1,2epsi). Especially, the new set with parameters (2N,2M,L,2) can be optimal in terms of the set size if a q-ary optimal LCZ sequence set with parameters (N,M,L,1) is used",
RRT-blossom: RRT with a local flood-fill behavior,"This paper proposes a new variation of the RRT planner which demonstrates good performance on both loosely-constrained and highly-constrained environments. The key to the planner is an implicit flood-fill-like mechanism, a technique that is well suited to escaping local minima in highly constrained problems. We show sample results for a variety of problems and environments, and discuss future improvements",
Towards an integrated conceptual model of security and dependability,"It is now commonly accepted that security and dependability largely represent two different aspects of an overall meta-concept that reflects the trust that we put in a computer system. There exist a large number of models of security and dependability with various definitions and terminology. This position paper suggests a high-level conceptual model that is aimed to give a novel approach to the area. The model defines security and dependability characteristics in terms of a system's interaction with its environment via the system boundaries and attempts to clarify the relation between malicious environmental influence, e.g. attacks, and the service delivered by the system. The model is intended to help reasoning about security and dependability and to provide an overall means for finding and applying fundamental defense mechanisms. Since the model is high-level and conceptual it must be interpreted into each specific sub-area of security/dependability to be practically useful.",
Dynamic Verification of Memory Consistency in Cache-Coherent Multithreaded Computer Architectures,"Multithreaded servers with cache-coherent shared memory are the dominant type of machines used to run critical network services and database management systems. To achieve the high availability required for these tasks, it is necessary to incorporate mechanisms for error detection and recovery. Correct operation of the memory system is defined by the memory consistency model. Errors can therefore be detected by checking if the observed memory system behavior deviates from the specified consistency model. Based on recent work, we design a framework for dynamic verification of memory consistency (DVMC). The framework consists of mechanisms to verify three invariants that are proven to guarantee that a specified memory consistency model is obeyed. We describe an implementation of the framework for the SPARCv9 architecture and experimentally evaluate its performance using full-system simulation of commercial workloads",
Synthesis of System Verilog Assertions,"In recent years, assertion-based verification is being widely accepted as a key technology in the pre-silicon validation of system-on-chip (SOC) designs. The System Verilog language integrates the specification of assertions with the hardware description. In this paper we show that there are several compelling reasons for synthesizing assertions in hardware, and present an approach for synthesizing System Verilog assertions (SVA) in hardware. Our method investigates the structure of SVA properties and decomposes them into simple communicating parallel hardware units that together act as a monitor for the property. We present a tool that performs this synthesis, and also show that the chip area required by the monitors for a industry standard ABV IP for the ARMAMBA AHB protocol is quite modest",
Highly scalable and robust rule learner: performance evaluation and comparison,"Business intelligence and bioinformatics applications increasingly require the mining of datasets consisting of millions of data points, or crafting real-time enterprise-level decision support systems for large corporations and drug companies. In all cases, there needs to be an underlying data mining system, and this mining system must be highly scalable. To this end, we describe a new rule learner called DataSqueezer. The learner belongs to the family of inductive supervised rule extraction algorithms. DataSqueezer is a simple, greedy, rule builder that generates a set of production rules from labeled input data. In spite of its relative simplicity, DataSqueezer is a very effective learner. The rules generated by the algorithm are compact, comprehensible, and have accuracy comparable to rules generated by other state-of-the-art rule extraction algorithms. The main advantages of DataSqueezer are very high efficiency, and missing data resistance. DataSqueezer exhibits log-linear asymptotic complexity with the number of training examples, and it is faster than other state-of-the-art rule learners. The learner is also robust to large quantities of missing data, as verified by extensive experimental comparison with the other learners. DataSqueezer is thus well suited to modern data mining and business intelligence tasks, which commonly involve huge datasets with a large fraction of missing data.","Robustness,
Data mining,
Decision trees,
Production,
Computer science,
Bioinformatics,
Real time systems,
Decision support systems,
Drugs,
Companies"
Vision and Laser Data Fusion for Tracking People with a Mobile Robot,"In this paper we present a multi-sensor fusion system for tracking people with a mobile robot, which integrates the information provided by a laser range sensor and a PTZ camera. We introduce the algorithms used for detecting legs from laser scans and faces from video images, then we illustrate a human motion model for the estimation of people position, orientation and height. The ego-motion of the robot is also taken into account and the information fused using an implementation of the unscented Kalman filter. Finally, multiple human tracks are generated and maintained thanks to an appropriate data association procedure. The results of several experiments are illustrated, proving the effectiveness of our approach, and some considerations drawn.","Laser fusion,
Mobile robots,
Laser modes,
Face detection,
Humans,
Sensor fusion,
Sensor systems,
Robot vision systems,
Cameras,
Motion detection"
Lazy Associative Classification,"Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.",
Towards Privacy-Aware Location-Based Database Servers,"The wide spread of location-based services results in a strong market for location-detection devices (e.g., GPS-like devices, RFIDs, handheld devices, and cellular phones). Examples of location-based services include location-aware emergency service, location-based advertisement, live traffic reports, and location-based store finder. However, location-detection devices pose a major privacy threat on its users where it transmits private information (i.e., the location) to the server who may be untrustworthy. The existing model of location-based applications trades service with privacy where if a user wants to keep her private location information, she has to turn off her location-detection device, i.e., unsubscribe from the service. This paper tackles this model in a way that protects the user privacy while keeping the functionality of location-based services. The main idea is to employ a trusted third party, the Location Anonymizer, that expands the user location into a spatial region such that: (1) The exact user location can lie anywhere in the spatial region, and (2) There are k other users within the expanded spatial region so that each user is k-anonymous. The location-based database server is equipped with additional functionalities that support spatio-temporal queries based on the spatial region received from the location anonymizer rather than the exact point location received from the user.",
Testing BPEL-based Web Service Composition Using High-level Petri Nets,"This paper proposes a technique for analysis and testing BPEL-based Web service composition using high-level Petri nets. To illustrate how these compositions are verified, the relationships between BPEL-based Web service composition and high-level Petri nets is constructed. By analyzing the structure of Web service composition based on BPEL, the corresponding HPN is constructed. The dynamism and occurrence are presented in HPN with guard expression with coloured token. After translation, the equivalent HPN of the Web service composition based on BPEL can be verified on existing mature tool, and the related researches on HPN, e.g. testing coverage and reduction techniques that have been studied deeply, can be employed in testing of Web service composition based on BPEL, optimized test case can be generated based on the HPN translated. An example is provided to illustrate the translation ruled and the automatic verify progress",
Imitation Learning of Whole-Body Grasps,"A system is detailed here for using imitation learning to teach a robot to grasp objects using both hand and whole-body grasps, which use the arms and torso as well as hands. Demonstration grasp trajectories are created by teleoperating a simulated robot to pick up simulated objects, modeled as combinations of up to three aligned primitives - boxes, cylinders, and spheres. When presented with a target object, the system compares it against the objects in a stored database to pick a demonstrated grasp used on a similar object. By considering the target object to be a transformed version of the demonstration object, contact points are mapped from one object to the other. The most promising grasp candidate is chosen with the aid of a grasp quality metric. To test the success of the chosen grasp, a collision-free grasp trajectory is found and an attempt is made to execute it in simulation. The implemented system successfully picks up 92 out of 100 randomly generated test objects in simulation",
Robust Detection of Region-Duplication Forgery in Digital Image,"Region duplication forgery, in which a part of a digital image is copied and then pasted to another portion of the same image in order to conceal an important object in the scene, is one of the common image forgery techniques. In this paper, we describe an efficient and robust algorithm for detecting and localizing this type of malicious tampering. We present experimental results which show that our method is robust and can successfully detect this type of tampering for images that have been subjected to various forms of post region duplication image processing, including blurring, noise contamination, severe lossy compression, and a mixture of these processing operations",
An Assisted Living Oriented Information System Based on a Residential Wireless Sensor Network,"This paper deals with a new medical information system called Alarm Net designed for smart healthcare. Based on an advanced Wireless Sensor Network (WSN), it specifically targets assisted-living residents and others who may benefit from continuous and remote health monitoring. We present the advantages, objectives, and status of the system built at the Department of Computer Science at UVA. Early results of the prototype suggest a strong potential for WSNs to open new research perspectives for ad hoc deployment of multi-modal sensors and improved quality of medical care",
Building web services for scientific grid applications,"Web service architectures have gained popularity in recent years within the scientific grid research community. One reason for this is that web services allow software and services from various organizations to be combined easily to provide integrated and distributed applications. However, most applications developed and used by scientific communities are not web-service-oriented, and there is a growing need to integrate them into grid applications based on service-oriented architectures. In this paper, we describe a framework that allows scientists to provide a web service interface to their existing applications as web services without having to write extra code or modify their applications in any way. In addition, application providers do not need to be experts in web services standards, such as Web Services Description Language, Web Services Addressing, Web Services Security, or secure authorization, because the framework automatically generates these details. The framework also enables users to discover these application services, interact with them, and compose scientific workflows from the convenience of a grid portal.",
Replica placement design with static optimality and dynamic maintainability,We propose a static replica placement algorithm that places replicas to sites by optimizing average response time and a dynamic replica maintenance algorithm that re-allocates replicas to new candidate sites if a performance metric degrades significantly over last K time periods. Simulation results demonstrate that the dynamic maintenance algorithm with static placement decisions performs best in dynamic environments like data grids.,
Open at the top; open at the bottom; and continually (but slowly) evolving,"Systems of systems differ from traditional systems in that they are open at the top, open at the bottom, and continually (but slowly) evolving. ""Open at the top"" means that there is no pre-defined top level application. New applications may be created at any time. ""Open at the bottom"" means that the system primitives are defined functionally rather than concretely. This allows the implementation of these primitives to be modified as technology changes. ""Continually (but slowly) evolving"" means that the system's functionality is stable enough to be useful but is understood to be subject to modification. Systems with these properties tend to be environments within which other systems operate - and hence are systems of systems. It is also important to understand the larger environment within which a system of systems exists",
Strong edge coloring for channel assignment in wireless radio networks,We give efficient sequential and distributed approximation algorithms for strong edge coloring graphs modeling wireless networks. Strong edge coloring is equivalent to computing a conflict-free assignment of channels or frequencies to pairwise links between transceivers in the network,
Generalized Linear Kernels for One-Versus-All Classification: Application to Speaker Recognition,"In this paper, we examine the problem of kernel selection for one-versus-all (OVA) classification of multiclass data with support vector machines (SVMs). We focus specifically on the problem of training what we refer to as generalized linear kernels - that is, kernels of the form, k(x1,x2) = xT 1Rx2 , where R is a positive semidefinite matrix. Our approach for training k(x1, x2) involves first constructing a set of upper bounds on the rates of false positives and false negatives at a given score threshold. Under various conditions, minimizing these bounds leads to the closed-form solution, R = W-1, where W is the expected within-class covariance matrix of the data. We tested various parameterizations of R, including a diagonal parameterization that simply performs per-feature variance normalization, on the 1-conversation training condition of the SRE-2003 and SRE-2004 speaker recognition tasks. In experiments on a state-of-the-art MLLR-SVM speaker recognition system (A. Stolcke et al., 2005), the parameterization, R = W-1 s, where Ws is a smoothed estimate of W, achieves relative reductions in the minimum decision cost function (DCF) of up to 22% below the results obtained when R does per-feature variance normalization",
Particle Methods as Message Passing,"It is shown how particle methods can be viewed as message passing on factor graphs. In this setting, particle methods can readily be combined with other message-passing techniques such as the sum-product and max-product algorithm, expectation maximization, iterative conditional modes, steepest descent, Kaiman filters, etc. Generic message computation rules for particle-based representations of sum-product messages are formulated. Various existing particle methods are described as instances of those generic rules, i.e., Gibbs sampling, importance sampling, Markov-chain Monte Carlo methods (MCMC), particle filtering, and simulated annealing",
Hub-based Simulation and Graphics Hardware Accelerated Visualization for Nanotechnology Applications,"The Network for computational nanotechnology (NCN) has developed a science gateway at nanoHUB.org for nanotechnology education and research. Remote users can browse through online seminars and courses, and launch sophisticated nanotechnology simulation tools, all within their Web browser. Simulations are supported by a middleware that can route complex jobs to grid supercomputing resources. But what is truly unique about the middleware is the way that it uses hardware accelerated graphics to support both problem setup and result visualization. This paper describes the design and integration of a remote visualization framework into the nanoHUB for interactive visual analytics of nanotechnology simulations. Our services flexibly handle a variety of nanoscience simulations, render them utilizing graphics hardware acceleration in a scalable manner, and deliver them seamlessly through the middleware to the user. Rendering is done only on-demand, as needed, so each graphics hardware unit can simultaneously support many user sessions. Additionally, a novel node distribution scheme further improves our system's scalability. Our approach is not only efficient but also cost-effective. Only half-dozen render nodes are anticipated to support hundreds of active tool sessions on the nanoHUB. Moreover, this architecture and visual analytics environment provides capabilities that can serve many areas of scientific simulation and analysis beyond nanotechnology with its ability to interactively analyze and visualize multivariate scalar and vector fields",
Practical Darknet Measurement,"The Internet today is beset with constant attacks targeting users and infrastructure. One popular method of detecting these attacks and the infected hosts behind them is to monitor unused network addresses. Because many Internet threats propagate randomly, infection attempts can be captured by monitoring the unused spaces between live addresses. Sensors that monitor these unused address spaces are called darknets, network telescopes, or blackholes. They capture important information about a diverse range of threats such as Internet worms, denial of services attacks, and botnets. In this paper, we describe and analyze the important measurement issues associated with deploying darknets, evaluating the placement and service configuration of darknets, and analyzing the data collected by darknets. To support the discussion, we leverage 4 years of experience operating the Internet motion sensor (IMS), a network of distributed darknet sensors monitoring 60 distinct address blocks in 19 organizations over 3 continents.","Data analysis,
Telecommunication traffic,
Radiation monitoring,
Web and internet services,
IP networks,
Telescopes,
Computer worms,
Computer crime,
Continents,
Network topology"
Pathological Voice Assessment,"While there are number of guidelines and methods used in practice, there is no standard universally agreed upon system for assessment of pathological voices. Pathological voices are primarily labeled based on the perceptual judgments of specialists, a process that may result in different label(s) being assigned to a given voice sample. This paper focuses on the recognition of five specific pathologies. The main goal is to compare two different classification methods. The first method considers single label classification by assigning a new label (single label) to the ensembles to which they most likely belong. The second method employs all labels originally assigned to the voice samples. Our results show that the pathological voice assessment performance in the second method is improved with respect to the first method",
Layering as Optimization Decomposition: Questions and Answers,"Network protocols in layered architectures have historically been obtained on an ad-hoc basis, and much of the recent cross-layer designs are conducted through piecemeal approaches. Network protocols may instead be holistically analyzed and systematically designed as distributed solutions to some global optimization problems in the form of generalized Network Utility Maximization (NUM), providing insight on what they optimize and on the structures of network protocol stacks. In the form of 10 Questions and Answers, this paper presents a short survey of the recent efforts towards a systematic understanding of ""layering"" as ""optimization decomposition"". The overall communication network is modeled by a generalized NUM problem, each layer corresponds to a decomposed subproblem, and the interfaces among layers are quantified as functions of the optimization variables coordinating the subproblems. Furthermore, there are many alternative decompositions, each leading to a different layering architecture. Industry adoption of this unifying framework has also started. Here we summarize the current status of horizontal decomposition into distributed computation and vertical decomposition into functional modules such as congestion control, routing, scheduling, random access, power control, and coding. We also discuss under-explored future research directions in this area. More importantly than proposing any particular crosslayer design, this framework is working towards a mathematical foundation of network architectures and the design process of modularization.",
Likelihood-Based Algorithms for Linear Digital Modulation Classification in Fading Channels,"Blind modulation classification (MC) is an intermediate step between signal detection and demodulation, with both military and civilian applications. MC is a challenging task, especially in a non-cooperative environment, as no prior information on the incoming signal is available at the receiver. In this paper, we investigate classification of linear digital modulations in slowly varying flat fading channels. With unknown channel amplitude, phase and noise power at the receive-side, we derive hybrid likelihood ratio test (HLRT) and quasiHLRT (QHLRT) - based classifiers, and discuss their performance versus computational complexity. It is shown that the QHLRT algorithm provides a low computational complexity solution, yet yielding performance close to the HLRT algorithm",
A scheme for privacy-preserving data dissemination,"An adequate level of trust must be established between prospective partners before an interaction can begin. In asymmetric trust relationships, one of the interacting partners is stronger. The weaker partner can gain a higher level of trust by disclosing private information. Dissemination of sensitive data owned by the weaker partner starts at this moment. The stronger partner can propagate data to others, who may then choose to spread data further. The proposed scheme for privacy-preserving data dissemination enables control of data by their owner (such as a weaker partner). It relies on the ideas of bundling sensitive data with metadata, an apoptosis of endangered bundles, and an adaptive evaporation of bundles in suspect environments. Possible applications include interactions among patients and healthcare providers, customers and businesses, researchers, and suppliers of their raw data. They will contribute to providing privacy guarantees, which are indispensable for the realization of the promise of pervasive computing",
Nonlinear motion correction of respiratory-gated lung SPECT images,"We propose a method for correcting the motion of the lungs between different phase images obtained by respiratory-gated single photon emission computed tomography (SPECT). This method is applied to SPECT images that show a preserved activity distribution in the lungs such as 99m-Tc macro aggregated albumin (99m-Tc-MAA) perfusion images and 99m-Tc-Technegas ventilation images. In the proposed method, an objective function, which consists of both the degree of similarity between a reference image and a deformed image, and the smoothness of deformation is defined and optimized using a simulated annealing algorithm. For the degree of similarity term in the objective function, an expansion ratio, defined as the ratio of change in local volume due to deformation, is introduced to preserve the total activity during the motion correction process. This method was applied to data simulated from computer phantoms, data acquired from a physical phantom, and 17 sets of clinical data. In all cases, the motion correction between inspiration and expiration phase images was successfully achieved.",
Comparing human robot interaction scenarios using live and video based methods: towards a novel methodological approach,"This paper presents results of a pilot study that investigated whether people's perceptions from live and video HRI trials were comparable. Subjects participated in a live HRI trial and videotaped HRI trials in which the scenario for both trials was identical, and involved a robot fetching an object using different approach directions. Results of the trials indicated moderate to high levels of agreement for subjects' preferences, and opinions for both the live and video based HRI trials. This methodology is in its infancy and should not be seen as a replacement for live trials. However, our results indicate that for certain HRI scenarios videotaped trials do have potential as a technique for prototyping, testing, developing HRI scenarios, and testing methodologies for use in definitive live trials",
Distributed Output Feedback MPC for Power System Control,"In this paper, a distributed output feedback model predictive control (MPC) framework with guaranteed nominal stability and performance properties is described. Distributed state estimation strategies are developed for supporting distributed output feedback MPC of large-scale systems, such as power systems. It is shown that under certain (easily verifiable) conditions, local measurements are sufficient for observer stability. More generally, stable observers can be designed by exchanging measurements between adjacent subsystems. Both estimation strategies are suboptimal, but the estimates generated converge exponentially to the optimal estimates. A disturbance modeling framework for achieving zero-offset control in the presence of nonzero mean disturbances and modeling errors is presented. Automatic generation control (AGC) provides a practical example for contrasting the performance of centralized and distributed controllers",
Anytime Classification Using the Nearest Neighbor Algorithm with Applications to Stream Mining,"For many real world problems we must perform classification under widely varying amounts of computational resources. For example, if asked to classify an instance taken from a bursty stream, we may have from milliseconds to minutes to return a class prediction. For such problems an anytime algorithm may be especially useful. In this work we show how we can convert the ubiquitous nearest neighbor classifier into an anytime algorithm that can produce an instant classification, or if given the luxury of additional time, can utilize the extra time to increase classification accuracy. We demonstrate the utility of our approach with a comprehensive set of experiments on data from diverse domains.","Nearest neighbor searches,
Insects,
Frequency,
Manufacturing industries,
Data mining,
Application software,
Computer science,
Temperature,
Humidity,
Embedded computing"
Performance Measurement of 802.11a Wireless Links from UAV to Ground Nodes with Various Antenna Orientations,"We report measured performance of 802.11a wireless links from an unmanned aerial vehicle (UAV) to ground stations. In a set of field experiments, we record the received signal strength indicator (RSSI) and measure the raw link-layer throughput for various antenna orientations, communication distances and ground-station elevations. By comparing the performance of 32 simultaneous pairs of UAV and ground station configurations, we are able to conclude that, in order to achieve the highest throughput under a typical flyover UAV flight path, both the UAV and the ground station should use omni-directional dipole (as opposed to high-gain, narrow- beam) antennas positioned horizontally, with their respective antenna null pointing to a direction perpendicular to the UAV's flight path. In addition, a moderate amount of elevation of the ground stations can also improve performance significantly.",
Mirrored Disk Organization Reliability Analysis,"Disk mirroring or RAID level 1 (RAID1) is a popular paradigm to achieve fault tolerance and a higher disk access bandwidth for read requests. We consider four RAID1 organizations: basic mirroring, group rotate declustering, interleaved declustering, and chained declustering, where the last three organizations attain a more balanced load than basic mirroring when disk failures occur. We first obtain the number of configurations, A(n, i), which do not result in data loss when i out of n disks have failed. The probability of no data loss in this case is A(n, i)/matrix of(n, i). The reliability of each RAID1 organization is the summation over 1 les i les n/2 of A(n, i)rn-i (1 - r) i, where r denotes the reliability of each disk. A closed-form expression for A(n, i) is obtained easily for the first three organizations. We present a relatively simple derivation of the expression for A(n, i) for the chained declustering method, which includes a correctness proof. We also discuss the routing of read requests to balance disk loads, especially when there are disk failures, to maximize the attainable throughput","fault tolerant computing,
file organisation,
probability,
RAID,
resource allocation"
Camera Calibration Method for Far Range Stereovision Sensors Used in Vehicles,"This paper presents a camera calibration method for far-range stereo-vision used for driving environment perception on highways. For a high accuracy stereovision system the most critical camera parameters are the relative extrinsic parameters which are describing the geometry of the stereo-rig. Experiments proved that even a few seconds drift of the relative camera angles can lead to disastrous consequences in the whole stereo-vision process: incorrect epipolar lines and consequently lack of reconstructed 3D points. Therefore we propose an off-line method able to give a very accurate estimation of these parameters and on-line methods for monitoring their stability in time taking into account the real-world automotive conditions, which can introduce drifts to the initial parameters due vibrations, temperature variations etc",
Gender Recognition in Non Controlled Environments,"In most of the automatic face classification applications, images should be captured in natural environments, where partial occlusions or high local changes in the illumination are frequent. For this reason, face classification tasks in uncontrolled environment are still nowadays unsolved problems, given that the loss of information caused by these artifacts can easily mislead any classifier. We present in this paper a system to extract robust face features that can be applied to encode information from any zone of the face and that can be used for different face classification problems. To test this method we include the results obtained in different gender classification experiments, considering controlled and uncontrolled environments and extracting face features from internal and external face zones. The obtained rates show, on the one hand, that we can obtain significant information applying the presented feature extraction scheme and, on the other hand, that the external face zone can contribute useful information for classification purposes",
Comparison between Single-Objective and Multi-Objective Genetic Algorithms: Performance Comparison and Performance Measures,"We compare single-objective genetic algorithms (SOGAs) with multi-objective genetic algorithms (MOGAs) in their applications to multi-objective knapsack problems. First we discuss difficulties in comparing a single solution by SOGAs with a solution set by MOGAs. We also discuss difficulties in comparing several solutions from multiple runs of SOGAs with a large number of solutions from a single run of MOGAs. It is shown that existing performance measures are not necessarily suitable for such comparison. Then we compare SOGAs with MOGAs through computational experiments on multi-objective knapsack problems. Experimental results on two-objective problems show that MOGAs outperform SOGAs even when they are evaluated with respect to a scalar fitness function used in SOGAs. This is because MOGAs are more likely to escape from local optima. On the other hand, experimental results on four-objective problems show that the search ability of MOGAs is degraded by the increase in the number of objectives. Finally we suggest a framework of hybrid algorithms where a scalar fitness function in SOGAs is probabilistically used in MOGAs to improve the convergence of solutions to the Pareto front.",
Privacy-Preserving Computation of Bayesian Networks on Vertically Partitioned Data,"Traditionally, many data mining techniques have been designed in the centralized model in which all data is collected and available in one central site. However, as more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties often wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data need not be revealed. In this paper, we present privacy-preserving protocols for a particular data mining task: learning a Bayesian network from a database vertically partitioned among two parties. In this setting, two parties owning confidential databases wish to learn the Bayesian network on the combination of their databases without revealing anything else about their data to each other. We present an efficient and privacy-preserving protocol to construct a Bayesian network on the parties' joint data",
Parallel computing patterns for Grid workflows,"Whereas a consensus has been reached on defining the set of workflow patterns for business process modeling languages, no such patterns exists for workflows applied to scientific computing on the Grid. By looking at different kinds of parallelism, in this paper we identify a set of workflow patterns related to parallel and pipelined execution. The paper presents how these patterns can be represented in different Grid workflow languages and discusses their implications for the design of the underlying workflow management and execution infrastructure. A preliminary classification of these patterns is introduced by surveying how they are supported by several existing advanced scientific and Grid workflow languages.",
HIMAC: High Throughput MAC Layer Multicasting in Wireless Networks,"Efficient, scalable and robust multicasting support from the MAC layer is needed for meeting the demands of multicast based applications over WiFi and mesh networks. However, the IEEE 802.11 protocol has no specific mechanism for multicasting. It implements multicasting using broadcasting at the base transmission rate. We identify two fundamental reasons for performance limitations of this approach: (a) Channel-state indifference: irrespective of the current quality of the channel to the receivers, the transmission always uses the base transmission rate; (b) Demand ignorance: packets are transmitted by a node even if children in the multicast tree have received those packets by virtue of overhearing. We propose a solution for MAC layer multicasting called HIMAC that uses the following two mechanisms: unary channel feedback (UCF) and unary negative feedback (UNF) to respectively address the shortcomings of 802.11. Our study is supported by measurements in a testbed, and simulations. We observe that the end-to-end throughput of multicast sessions using MAODV can be increased by up to 74% while reducing the end-to-end latency by up to a factor of 56",
Improving the security of a dynamic look-up table based chaotic cryptosystem,"In this brief, we analyze the cause of vulnerability of the original dynamic look-up table based chaotic encryption scheme in detail, and then propose the corresponding enhancement measures. Theoretical analysis and computer simulation indicate that the modified scheme is more secure than the original one",
Analysis of observer performance in known-location tasks for tomographic image reconstruction,"We consider the task of detecting a statistically varying signal of known location on a statistically varying background in a reconstructed tomographic image. We analyze the performance of linear observer models in this task. We show that, if one chooses a suitable reconstruction method, a broad family of linear observers can exactly achieve the optimal detection performance attainable with any combination of a linear observer and linear reconstructor. This conclusion encompasses several well-known observer models from the literature, including models with a frequency-selective channel mechanism and certain types of internal noise. Interestingly, the ""optimal"" reconstruction methods are unregularized and in some cases quite unconventional. These results suggest that, for the purposes of designing regularized reconstruction methods that optimize lesion detectability, known-location tasks are of limited use.",
On the Sensitivity of Online Game Playing Time to Network QoS,,
Dynamic Scratch-Pad Memory Management for Irregular Array Access Patterns,"There exist many embedded applications such as those executing on set-top boxes, wireless base stations, HDTV, and mobile handsets that are structured as nested loops and benefit significantly from software managed memory. Prior work on scratchpad memories (SPMs) focused primarily on applications with regular data access patterns. Unfortunately, some embedded applications do not fit in this category and consequently conventional SPM management schemes will fail to produce the best results for them. In this work, we propose a novel compilation strategy for data SPMs for embedded applications that exhibit irregular data access patterns. Our scheme divides the task of optimization between compiler and runtime. The compiler processes each loop nest and inserts code to collect information at runtime. Then, the code is modified in such a fashion that, depending on the collected information, it dynamically chooses to use or not to use the data SPM for a given set of accesses to irregular arrays. Our results indicate that this approach is very successful with the applications that have irregular patterns and improves their execution cycles by about 54% over a state-of-the-art SPM management technique and 23% over the conventional cache memories. Also, the additional code size overhead incurred by our approach is less than 5% for all the applications tested",
Semiautomatic 3-D Prostate Segmentation from TRUS Images Using Spherical Harmonics,"Prostate brachytherapy quality assessment procedure should be performed while the patient is still on the operating table since this would enable physicians to implant additional seeds immediately into the prostate if necessary thus reducing the costs and increasing patient outcome. Seed placement procedure is readily performed under fluoroscopy and ultrasound guidance. Therefore, it has been proposed that seed locations be reconstructed from fluoroscopic images and prostate boundaries be identified in ultrasound images to perform dosimetry in the operating room. However, there is a key hurdle that needs to be overcome to perform the ultrasound and fluoroscopy-based dosimetry: it is highly time-consuming for physicians to outline prostate boundaries in ultrasound images manually, and there is no method that enables physicians to identify three-dimensional (3-D) prostate boundaries in postimplant ultrasound images in a fast and robust fashion. In this paper, we propose a new method where the segmentation is defined in an optimization framework as fitting the best surface to the underlying images under shape constraints. To derive these constraints, we modeled the shape of the prostate using spherical harmonics of degree eight and performed statistical analysis on the shape parameters. After user initialization, our algorithm identifies the prostate boundaries on the average in 2 min. For algorithm validation, we collected 30 postimplant prostate volume sets, each consisting of axial transrectal ultrasound images acquired at 1-mm increments. For each volume set, three experts outlined the prostate boundaries first manually and then using our algorithm. By treating the average of manual boundaries as the ground truth, we computed the segmentation error. The overall mean absolute distance error was 1.26plusmn0.41 mm while the percent volume overlap was 83.5plusmn4.2. We found the segmentation error to be slightly less than the clinically-observed interobserver variability",
Selecting software phase markers with code structure analysis,"Most programs are repetitive, where similar behavior can be seen at different execution times. Algorithms have been proposed that automatically group similar portions of a program's execution into phases, where samples of execution in the same phase have homogeneous behavior and similar resource requirements. In this paper, we present an automated profiling approach to identify code locations whose executions correlate with phase changes. These ''software phase markers"" can be used to easily detect phase changes across different inputs to a program without hardware support. Our approach builds a combined hierarchical procedure call and loop graph to represent a program's execution, where each edge also tracks the max, average, and standard deviation in hierarchical execution variability on paths from that edge. We search this annotated call-loop graph for instructions in the binary that accurately identify the start of unique stable behaviors across different inputs. We show that our phase markers can be used to accurately partition execution into units of repeating homogeneous behavior by counting execution cycles and data cache hits. We also compare the use of our software markers to prior work on guiding data cache reconfiguration using data-reuse markers. Finally, we show that the phase markers can be used to partition the program's execution at code transitions to pick accurately simulation points for SimPoint. When simulation points are defined in terms of phase markers, they can potentially be re-used across inputs, compiler optimizations, and different instruction set architectures for the same source code.",
Computing Nash Equilibria: Approximation and Smoothed Complexity,"The authors advance significantly beyond the recent progress on the algorithmic complexity of Nash equilibria by solving two major open problems in the approximation of Nash equilibria and in the smoothed analysis of algorithms. (1) The authors show that no algorithm with complexity poly(n, 1/epsi) can compute an epsi-approximate Nash equilibrium in a two-player game, in which each player has n pure strategies, unless PPAD sube P. In other words, the problem of computing a Nash equilibrium in a two-player game does not have a fully polynomial-time approximation scheme unless PPAD sube P. (2) The authors prove that no algorithm for computing a Nash equilibrium in a two-player game can have smoothed complexity poly(n, 1/sigma) under input perturbation of magnitude sigma, unless PPAD sube RP. In particular, the smoothed complexity of the classic Lemke-Howson algorithm is not polynomial unless PPAD sube RP. Instrumental to our proof, we introduce a new discrete fixed-point problem on a high-dimensional hypergrid with constant side-length, and show that it can host the embedding of the proof structure of any PPAD problem. We prove a key geometric lemma for finding a discrete fixed-point, a new concept defined on n + 1 vertices of a unit hypercube. This lemma enables us to overcome the curse of dimensionality in reasoning about fixed-points in high dimensions",
Wavelet Analysis of Sodium Iodide Spectra,"Wavelet analysis is a mathematical technique that was presented in the mid-1980s to solve a variety of problems in signal analysis where the signal is aperiodic, noisy, transient, etc. More recently, wavelets have been applied to other problems such as feature detection and localization, making it a very promising tool for the analysis of gamma-ray spectra. Recent results have also shown that this technique has the potential to benefit over other approaches due to the fact that the signal can simultaneously be analyzed over multiple scales by using wavelet analysis, thus eliminating potential false isotope identifications from artifacts such as the Compton edge and backscatter peaks. This implies that this peak localization algorithm is no longer a function of detector resolution, which changes with energy. We will present our results evaluating the technique of wavelet analysis for low-resolution (NaI) gamma-ray spectra. Emphasis will be placed on wavelet selection and the incorporation of a simple algorithm to the problem of isotope identification","Wavelet analysis,
Signal analysis,
Isotopes,
Transient analysis,
Computer vision,
Gamma ray detection,
Gamma ray detectors,
Signal processing,
Backscatter,
Energy resolution"
The Throughput Order of Ad Hoc Networks Employing Network Coding and Broadcasting,"Gupta and Kumar established that the per node throughput of ad hoc networks with multi-pair unicast traffic scales as lambda(n)=Theta(l/radic(nlogn)), thus indicating that network performance does not scale well with an increasing number of nodes. However, the model of Gupta and Kumar did not allow for the possibility of network coding and broadcasting, and recent work has suggested that such techniques have the potential to greatly improve network throughput. Here, for multiple unicast flows in a random topology under the protocol communication model of Gupta and Kumar, we show that for arbitrary network coding and broadcasting in a two-dimensional random topology that the throughput scales as lambda(n)=Theta(1/nr(n)), where n is the total number of nodes and r(n) is the transmission radius. When r(n) is set to ensure connectivity, lambda(n)=Theta(1/radic(nlogn)), which is of the same order as the lower bound for the throughput without network coding and broadcasting; in other words, network coding and broadcasting at most provides a constant factor improvement in the throughput. This result is also extended to one-dimensional and three-dimensional random deployment topologies, where it is shown that lambda(n)=Theta(1/n) for the one-dimensional topology and lambda(n)=Theta(1/cube root of (nlog 2n)) for three-dimensional networks",
Probabilistic Model Checking of Contention Resolution in the IEEE 802.15.4 Low-Rate Wireless Personal Area Network Protocol,"The international standard IEEE 802.15.4 defines low-rate wireless personal area networks, a central communication infrastructure of pervasive computing. In order to avoid conflicts caused by multiple devices transmitting at the same time, it uses a contention resolution algorithm based on randomised exponential backoff that is similar to the ones used in IEEE 802.3 for Ethernet and IEEE 802.11 for Wireless LAN. We model the protocol using probabilistic timed automata, a formalism in which both nondeterministic and probabilistic choice can be represented. The probabilistic timed automaton is transformed into a finite-state Markov decision process via a property-preserving integral-time semantics. Using the probabilistic model checker PRISM, we verify correctness properties, compare different operation modes of the protocol, and analyse performance and accuracy of different model abstractions.",
Performance Evaluation of Scheduling in IEEE 802.16 Based Wireless Mesh Networks,"We propose an efficient centralized scheduling algorithm in IEEE 802.16 based wireless mesh networks (WMN) to provide high qualified wireless multimedia services. Our algorithm takes special attention on the relay function of the mesh nodes in a transmission tree which is seldom studied in previous research. Some important design metrics, such as fairness, channel utilization and transmission delay are considered in this scheduling algorithm. IEEE 802.16 employs TDMA and the selection policy for scheduled links in a time slot will definitely impact the system performance. We evaluated the proposed algorithm with four selection criteria through extensive simulations and the results are instrumental for improving the performance of IEEE 802.16 based WMNs in terms of link scheduling",
A QoS multicast routing optimization algorithm based on genetic algorithm,"Most of the multimedia applications require strict quality of service (QoS) guarantee during the communication between a single source and multiple destinations. This gives rise to the need for an efficient QoS multicast routing strategy. Determination of such QoS-based optimal multicast routes basically leads to a multi-objective optimization problem, which is computationally intractable in polynomial time due to the uncertainty of resources in Internet. This paper describes a network model for researching the routing problem and proposes a new multicast tree selection algorithm based on genetic algorithms to simultaneously optimize multiple QoS parameters. The paper mainly presents a QoS multicast routing algorithm based on genetic algorithm (QMRGA). The QMRGA can also optimize the network resources such as bandwidth and delay, and can converge to the optimal or near-optimal solution within few iterations, even for the networks environment with uncertain parameters. The incremental rate of computational cost can close to polynomial and is less than exponential rate. The performance measures of the QMRGA are evaluated using simulations. The simulation results show that this approach has fast convergence speed and high reliability. It can meet the real-time requirement in multimedia communication networks.",
"A Trust-Aware, P2P-Based Overlay for Intrusion Detection","Collaborative intrusion detection systems (IDSs) have a great potential for addressing the challenges posed by the increasing aggressiveness of current Internet attacks. However, one of the major concerns with the proposed collaborative IDSs is their vulnerability to the insider threat. Malicious intruders, infiltrating such a system, could poison the collaborative detectors with false alarms, disrupting the intrusion detection functionality and placing at risk the whole system. In this paper, we propose a P2P-based overlay for intrusion detection (overlay IDS) that addresses the insider threat by means of a trust-aware engine for correlating alerts and an adaptive scheme for managing trust. We have implemented our system using JXTA framework and we have evaluated its effectiveness for preventing the spread of a real Internet worm over an emulated network. The evaluation results show that our overlay IDS significantly increases the overall survival rate of the network",
Thresholding noise-free ordered mean filter based on Dempster-Shafer theory for image restoration,"This work proposes a new decision-based filter, the thresholding noise-free ordered mean (TNOM) filter based on the Dempster-Shafer (D-S) evidence theory, to preserve more details of images than can other decision-based filters, while effectively suppressing impulse noise. The new filter mechanism is composed of an efficient D-S impulse detector and a noise filter that works by estimating the central noise-free ordered mean (CNOM) value. The D-S evidence theory provides a way to deal with the uncertainty in the evidence and information fusion. Pieces of evidence are extracted, and the mass functions defined using the local information in the filter window. Then, the decision rule is applied to determine whether noise exists, according to the final combined belief value. If a pixel is detected to be a corrupted pixel, then the proposed filter will be triggered to replace it. Otherwise, the pixel is kept unchanged. With respect to the noise suppression of noise on both fixed-valued and random-valued impulses without smearing the fine details in the image, extensive simulation results reveal that the proposed scheme significantly outperforms other decision-based filters.",
A New Multi-objective Evolutionary Optimisation Algorithm: The Two-Archive Algorithm,"Many multi-objective evolutionary algorithms (MOEAs) have been proposed in recent years. However, almost all MOEAs have been evaluated on problems with two to four objectives only. It is unclear how well these MOEAs will perform on problems with a large number of objectives. Our preliminary study (V. Khare et al., 2003) showed that performance of some MOEAs deteriorates significantly as the number of objectives increases. This paper proposes a new MOEA that performs well on problems with a large number of objectives. The new algorithm separates non-dominated solutions into two archives, and is thus called the two-archive algorithm. The two archives focused on convergence and diversity, respectively, in optimisation. Computational studies have been carried out to evaluate and compare our new algorithm against the best MOEA for problems with a large number of objectives. Our experimental results have shown that the two-archive algorithm outperforms existing MOEAs on problems with a large number of objectives",
An Empirical Comparison of Automated Generation and Classification Techniques for Object-Oriented Unit Testing,"Testing involves two major activities: generating test inputs and determining whether they reveal faults. Automated test generation techniques include random generation and symbolic execution. Automated test classification techniques include ones based on uncaught exceptions and violations of operational models inferred from manually provided tests. Previous research on unit testing for object-oriented programs developed three pairs of these techniques: model-based random testing, exception-based random testing, and exception-based symbolic testing. We develop a novel pair, model-based symbolic testing. We also empirically compare all four pairs of these generation and classification techniques. The results show that the pairs are complementary (i.e., reveal faults differently), with their respective strengths and weaknesses",
Energy and communication efficient group key management protocol for hierarchical sensor networks,"In this paper, we describe group key management protocols for hierarchical sensor networks where instead of using pre-deployed keys, each sensor node generates a partial key dynamically using a function. The function takes partial keys of its children as input. The design of the protocol is motivated by the fact that traditional cryptographic techniques are impractical in sensor networks because of high energy and computational overheads. The group key management protocol supports the establishment of two types of group keys; one for the sensor nodes within a group, and the other in a group of cluster heads. The protocol handles freshness of the group key dynamically, and eliminates the involvement of a trusted third party (TTP). We have experimentally evaluated the time and energy consumption in broadcasting partial keys and group key under two sensor routing protocols (tiny-AODV and tiny-diffusion) by varying the number of nodes and key sizes. The performance study provides the optimum number of partial keys needed for computing the group key to balance the available security and power consumption. The experimental study also concludes that the energy consumption in SPIN increases rapidly as the number of group members increases in comparison to our protocol",
A Robust IRIS Segmentation Procedure for Unconstrained Subject Presentation,"Iris as a biometric, is the most reliable with respect to performance. However, this reliability is a function of the ideality of the data, therefore a robust segmentation algorithm is required to handle non-ideal data. In this paper, a segmentation methodology is proposed that utilizes shape, intensity, and location information that is intrinsic to the pupil/iris. The virtue of this methodology lies in its capability to reliably segment non-ideal imagery that is simultaneously affected with such factors as specular reflection, blur, lighting variation, and off-angle images. We demonstrate the robustness of our segmentation methodology by evaluating ideal and non-ideal datasets, namely CASIA, Iris Challenge Evaluation (ICE) data, WVU, and WVU Off-angle. Furthermore, we compare our performance to that of Camus and Wildes, and Libor Masek's algorithms. We demonstrate an increase in segmentation performance of 7.02%, 8.16%, 20.84%, 26.61%, over the former mentioned algorithms when evaluating these datasets, respectively.",
Finite element implementation of Maxwell's equations for image reconstruction in electrical impedance tomography,"Traditionally, image reconstruction in electrical impedance tomography (EIT) has been based on Laplace's equation. However, at high frequencies the coupling between electric and magnetic fields requires solution of the full Maxwell equations. In this paper, a formulation is presented in terms of the Maxwell equations expressed in scalar and vector potentials. The approach leads to boundary conditions that naturally align with the quantities measured by EIT instrumentation. A two-dimensional implementation for image reconstruction from EIT data is realized. The effect of frequency on the field distribution is illustrated using the high-frequency model and is compared with Laplace solutions. Numerical simulations and experimental results are also presented to illustrate image reconstruction over a range of frequencies using the new implementation. The results show that scalar/vector potential reconstruction produces images which are essentially indistinguishable from a Laplace algorithm for frequencies below 1 MHz but superior at frequencies reaching 10 MHz.",
Facial Expression Classification using Gabor and Log-Gabor Filters,"Facial expression classification has achieved good results in the past using manually extracted facial points convolved with Gabor filters. In this paper, classification performance was tested on feature vectors composed of facial points convolved with Gabor and log-Gabor filters, as well as with whole image pixel representation of static facial images. Principal component analysis was performed on these feature vectors, and classification accuracies compared using linear discriminant analysis. Experiments carried out on two databases show comparable performance between Gabor and log-Gabor filters, with a classification accuracy of around 85%. This was achieved on low-resolution images, without the need to precisely locate facial points on each face image",
On the Capacity of the Cognitive Tracking Channel,"We explore the capacity of opportunistic secondary (cognitive) communication over a spectral pool of two independent channels. Due to the distributed nature of the primary user's spectral activity, the cognitive receiver does not have full knowledge of the channel used at the transmitter for secondary communication. Tracking the transmitter state at the receiver is therefore a primary issue in such channels. The problem is further complicated as the channel availability changes with time. The tracking uncertainty also makes decoding at the receiver non-trivial. Using genie based outer bounds and training based lower bounds, we estimate the capacity of the secondary link. The capacity analysis shows that the benefits of spectral pooling are lost in dynamic spectral environments",
Driving waveform for reducing temporal dark image sticking in AC plasma display panel based on perceived luminance,"Minimizing the reset discharge produced under an MgO-cathode condition and eliminating the wall charges accumulated on the address electrode prior to the reset period are the key factors involved in reducing temporal dark image sticking. Thus, based on the perceived luminance, new driving waveforms that can prohibit an MgO-cathode induced reset discharge or erase the wall charges accumulated on the address electrode prior to the reset period are examined for the complete elimination of temporal dark image sticking without deteriorating the address discharge characteristics. As a result of monitoring the difference in the infrared emission and perceived luminance between the cells with and without image sticking, the proposed driving waveform was shown to effectively remove temporal dark image sticking without deteriorating the address discharge characteristics",
Control of facial expressions of the humanoid robot head ROMAN,"For humanoid robots which are able to assist humans in their daily life, the capability for adequate interaction with human operators is a key feature. If one considers that more than 60% of human communication is conducted non-verbally (by using facial expressions and gestures), an important research topic is how interfaces for this non-verbal communication can be developed. To achieve this goal, several robotic heads have been designed. However, it remains unclear how exactly such a head should look like and what skills it should have to be able to interact properly with humans. This paper describes an approach that aims at answering some of these design choices. A behavior-based control to realize facial expressions which is a basic ability needed for interaction with humans is presented. Furthermore a poll in which the generated facial expressions should be detected is visualized. Additionally, the mechatronical design of the head and the accompanying neck joint are given",
Toward Secure Low Rate Wireless Personal Area Networks,"Low rate wireless personal area networks (LR-WPANs) offer device level wireless connectivity. They bring to light a host of new applications as well as enhance existing applications. Due to their low cost, low power consumption and self-organization features, LR-WPANs are ideal for applications such as public security, battle field monitoring, inventory tracking, as well as home and office automation. Nevertheless, one critical issue, security, needs to be solved before LR-WPANs are commonly accepted. Pursuing security in LR-WPANs is a challenging task. On one hand, wireless communications are inherently susceptible to interception and interference. On the other hand, most devices in LR-WPANs are resource-constrained and lack physical safeguards. This paper presents a systematic analysis of the threats faced by LR-WPANs with respect to the protocol stack defined by IEEE 802.15.4 and the ZigBee Alliance. Attacks are modeled and their impacts are evaluated. Some security problems within the current LR-WPAN security architecture are identified and remedies are suggested. Countermeasures of various attacks are also given","Wireless personal area networks,
Communication system security,
Power system security,
Costs,
Energy consumption,
Computerized monitoring,
Office automation,
Wireless communication,
Interference,
Protocols"
Achieving Class-Based QoS for Transactional Workloads,"Transaction processing systems lie at the core of modern e-commerce applications such as on-line retail stores, banks and airline reservation systems. The economic success of these applications depends on the ability to achieve high user satisfaction, since a single mouse-click is all that it takes a frustrated user to switch to a competitor. Given that system resources are limited and demands are varying, it is difficult to provide optimal performance to all users at all times. However, often transactions can be divided into different classes based on how important they are to the online retailer. For example, transactions initiated by a ""big spending"" client are more important than transactions from a client that only browses the site. A natural goal then is to ensure short delays for the class of important transactions, while for the less important transactions longer delays are acceptable.",
Acceleration Strategies for Gaussian Mean-Shift Image Segmentation,"Gaussian mean-shift (GMS) is a clustering algorithm that has been shown to produce good image segmentations (where each pixel is represented as a feature vector with spatial and range components). GMS operates by defining a Gaussian kernel density estimate for the data and clustering together points that converge to the same mode under a fixed-point iterative scheme. However, the algorithm is slow, since its complexity is O(kN2), where N is the number of pixels and k the average number of iterations per pixel. We study four acceleration strategies for GMS based on the spatial structure of images and on the fact that GMS is an expectation-maximisation (EM) algorithm: spatial discretisation, spatial neighbourhood, sparse EM and EM-Newton algorithm. We show that the spatial discretisation strategy can accelerate GMS by one to two orders of magnitude while achieving essentially the same segmentation; and that the other strategies attain speedups of less than an order of magnitude.",
Answering Imprecise Queries over Autonomous Web Databases,"Current approaches for answering queries with imprecise constraints require user-specific distance metrics and importance measures for attributes of interest - metrics that are hard to elicit from lay users. We present AIMQ, a domain and user independent approach for answering imprecise queries over autonomous Web databases. We developed methods for query relaxation that use approximate functional dependencies. We also present an approach to automatically estimate the similarity between values of categorical attributes. Experimental results demonstrating the robustness, efficiency and effectiveness of AIMQ are presented. Results of a preliminary user study demonstrating the high precision of the AIMQ system is also provided.",
Functional Constraints vs. Test Compression in Scan-Based Delay Testing,"We present an approach to prevent over testing in scan-based delay test. The test data is transformed with respect to functional constraints while simultaneously keeping as many positions as possible unspecified in order to facilitate test compression. The method is independent of the employed delay fault model, ATPG algorithm and test compression technique, and it is easy to integrate into an existing flow. Experimental results emphasize the severity of over testing in scan-based delay test. Influence of different functional constraints on the amount of the required test data and the compression efficiency is investigated. To the best of our knowledge, this is the first systematic study on the relationship between over testing prevention and test compression",
Analysis and modeling of individual competencies: toward better management of human resources,"A general approach is presented for the development of competence management information systems to enable competence management at all business control levels (i.e., strategic, tactical, and operational). Based on a state of the art review of competence definitions, a model named competence, resource, aspect, and individual (CRAI) for human resource competence is presented. For the sake of computerization, the CRAI model is fully defined using the set theory. It structures and formalizes the concept of competence and provides guidelines for its deployment in businesses when building a company's specific competence information system. Furthermore, the CRAI model is also analyzed on a number of queries frequently dealt with in competence management. To make them independent of any specific technology, these queries have also been formalized using the set theory. Finally, an extract of a full-scale industrial case study, on which the CRAI model has been tested, illustrates concepts discussed in the paper.",
An Epidemic Model of Mobile Phone Virus,"Considering the characteristics of mobile network, we import three important parameters: distribution density of mobile phone, coverage radius of Bluetooth signal and moving velocity of mobile phone to build an epidemic model of mobile phone virus which is different from the epidemic model of computer worm. Then analyzing different properties of this model with the change of parameters; discussing the epidemic threshold of mobile phone virus; presenting suggestions of quarantining the spreading of mobile phone virus","Mobile handsets,
Smart phones,
Computer worms,
Hardware,
Bluetooth,
Mobile computing,
Internet,
Microcomputers,
Cellular phones,
Data security"
DMA-aware memory energy management,"As increasingly larger memories are used to bridge the widening gap between processor and disk speeds, main memory energy consumption is becoming increasingly dominant. Even though much prior research has been conducted on memory energy management, no study has focused on data servers, where main memory is predominantly accessed by DMAs instead of processors. In this paper, we study DMA-aware techniques for memory energy management in data servers. We first characterize the effect of DMA accesses on memory energy and show that, due to the mismatch between memory and I/O bus band-widths, significant energy is wasted when memory is idle but still active during DMA transfers. To reduce this waste, we propose two novel performance-directed energy management techniques that maximize the utilization of memory devices by increasing the level of concurrency between multiple DMA transfers from different I/O buses to the same memory device. We evaluate our techniques using a detailed trace-driven simulator, and storage and database server traces. The results show that our techniques can effectively minimize the amount of idle energy waste during DMA transfers and, consequently, conserve up to 38.6% more memory energy than previous approaches while providing similar performance.",
Providing High Reliability in a Minimum Redundancy Archival Storage System,"Inter-file compression techniques store files as sets of references to data objects or chunks that can be shared among many files. While these techniques can achieve much better compression ratios than conventional intra-file compression methods such as Lempel-Ziv compression, they also reduce the reliability of the storage system because the loss of a few critical chunks can lead to the loss of many files. We show how to eliminate this problem by choosing for each chunk a replication level that is a function of the amount of data that would be lost if that chunk were lost. Experiments using actual archival data show that our technique can achieve significantly higher robustness than a conventional approach combining data mirroring and intra-file compression while requiring about half the storage space.",
A Comparison of Bloat Control Methods for Genetic Programming,"Genetic programming has highlighted the problem of bloat, the uncontrolled growth of the average size of an individual in the population. The most common approach to dealing with bloat in tree-based genetic programming individuals is to limit their maximal allowed depth. An alternative to depth limiting is to punish individuals in some way based on excess size, and our experiments have shown that the combination of depth limiting with such a punitive method is generally more effective than either alone. Which such combinations are most effective at reducing bloat? In this article we augment depth limiting with nine bloat control methods and compare them with one another. These methods are chosen from past literature and from techniques of our own devising. esting with four genetic programming problems, we identify where each bloat control method performs well on a per-problem basis, and under what settings various methods are effective independent of problem. We report on the results of these tests, and discover an unexpected winner in the cross-platform category.",
Joint Routing and Channel Assignment in Multi-Radio Wireless Mesh Networks,"This paper considers the problem of how to maximize throughput in multi-radio multi-channel wireless mesh networks. With mathematical model based on radio and radioto-radio link, we introduce a scheduling graph and show that the feasibility problem of time fraction vector is equal to the problem of whether the scheduling graph is [M]-colorable, where M is the number of slots in one period. We use this equivalence property to derive a sufficient condition of feasibility, and then, using this sufficient condition, we mathematically formulate the joint routing and channel assignment problem as a linear programming problem. Finally, we use vertex coloring to get a feasible schedule and lift the resulting flows. We prove that the optimality gap is above a constant factor. The numeric results demonstrate the effectiveness of our proposed algorithm.",
Monitoring Network Traffic with Radial Traffic Analyzer,"Extensive spread of malicious code on the Internet and also within intranets has risen the user's concern about what kind of data is transferred between her or his computer and other hosts on the network. Visual analysis of this kind of information is a challenging task, due to the complexity and volume of the data type considered, and requires special design of appropriate visualization techniques. In this paper, we present a scalable visualization toolkit for analyzing network activity of computer hosts on a network. The visualization combines network packet volume and type distribution information with geographic information, enabling the analyst to use geographic distortion techniques such as the HistoMap technique to become aware of the traffic components in the course of the analysis. The presented analysis tool is especially useful to compare important network load characteristics in a geographically aware display, to relate communication partners, and to identify the type of network traffic occurring. The results of the analysis are helpful in understanding typical network communication activities, and in anticipating potential performance bottlenecks or problems. It is suited for both off-line analysis of historic data, and via animation for on-line monitoring of packet-based network traffic in real time",
Identifying Document Topics Using the Wikipedia Category Network,"In the size and coverage of Wikipedia, a freely available online encyclopedia has reached the point where it can be utilized similar to an ontology or taxonomy to identify the topics discussed in a document. In this paper we show that even a simple algorithm that exploits only the titles and categories of Wikipedia articles can characterize documents by Wikipedia categories surprisingly well. We test the reliability of our method by predicting categories of Wikipedia articles themselves based on their bodies, and by performing classification and clustering on 20 newsgroups and RCV1, representing documents by their Wikipedia categories instead of their texts",
A Multipolicy Authorization Framework for Grid Security,"A grid system is a virtual organization that is composed of several autonomous domains. Authorization in such a system needs to be flexible and scalable to support multiple security policies. Basing on the Web services security specifications such as XACML, SAML, and the special security needs of the grid computing, we have constructed an authorization framework in the Globus Toolkit 4 that can support multiple policies. This paper describes the concepts of our design and introduces the structure and the components of the authorization framework. To show the flexibility and scalability of the framework, we introduce a new blacklist/whitelist-based authorization mechanism that can be seamlessly integrated into the framework",
Rain Attenuation in Millimeter Wave Ranges,"The high potential of millimeter-wave communication systems has generated the need to carry out many studies in view of rain and other climatic effects on radio propagation at these frequencies. This paper reviews rain attenuation in millimeter wave ranges. In the present study, a short-range 35 GHz radio link was used to measure rain specific attenuation with simultaneous measurement of rain rate distribution. The rainfall statistics and attenuation caused by rains are discussed, and an empirical model derived from these measurements is suggested in order to observe and investigate the attenuation caused by rains in short-range communications. A millimeter wave propagation experiment at 103 GHZ on a propagation path of 390 m is conducted. The results were compared with the rain attenuation calculations from the Marshall-Palmer, Best, Joss-Thomas-Waldvogel and Weibull distributions for raindrop size. It has been shown that the Weibull distribution has a good agreement with the experiments. Finally the analysis and discussion for measurement results respectively.",
The 'Grand Challenge' in Informatics: Engineering Software-Intensive Systems,"The science of information and information processing, informatics comprises many areas and includes principles of computing, storing, communicating, and visualizing information, and formalisms to describe information-processing procedures. The development and production of information-processing systems is based on software support systems such as software tools or product data repositories. Software and systems engineering is thus the key discipline for constructing information-processing systems. In particular, software and systems engineering addresses issues such as requirements engineering, architectural design, implementation, reliability engineering, and long-term maintenance. Developing a methodology for specifying and verifying software-intensive systems poses a grand challenge that a broad stream of research must address",
Attacks on PKM Protocols of IEEE 802.16 and Its Later Versions,"Without physical boundaries, a wireless network faces many more vulnerabilities than a wired network does. IEEE 802.16 provides a security sublayer in the MAC layer to address the privacy issues across the fixed BWA (broadband wireless access). Several articles have been published to address the flaws in IEEE 802.16 security after IEEE 802.16-2001 was released. However, even the enhanced version IEEE 802.16-2004 does not settle all the problems and additional flaws emerge. In addition, we found that PKM (privacy and key management) protocols version 2 (PKMv2), proposed by recently released IEEE 802.16e, is also vulnerable to new attacks. In this paper, we first overview the IEEE 802.16 standard, especially the security sublayer, and then investigate possible attacks on the basic PKM protocol in IEEE 802.16 as well as in its other versions from related works and the newest PKMv2. We also give possible solutions to counter those attacks and verify our analysis using formal (BAN) logic.",
"Superellipsoid-based, Real Symmetric Traceless Tensor Glyphs Motivated by Nematic Liquid Crystal Alignment Visualization","A glyph-based method for visualizing the nematic liquid crystal alignment tensor is introduced. Unlike previous approaches, the glyph is based upon physically-linked metrics, not offsets of the eigenvalues. These metrics, combined with a set of superellipsoid shapes, communicate both the strength of the crystal's uniaxial alignment and the amount of biaxiality. With small modifications, our approach can visualize any real symmetric traceless tensor",
Learning Semantic Correlations for Cross-Media Retrieval,"This paper proposes a novel cross-media retrieval approach. First, an isomorphic subspace is constructed based on canonical correlation analysis (CCA) to learn multi-modal correlations of media objects; second, polar coordinates are used to judge the general distance of media objects with different modalities in the subspace. Since the integrity of semantic correlations is not likely learned from limited training samples, users' relevance feedback is used to accurately refine cross-media similarities. We also propose methods to map new media objects into the learned subspace, and any new media object would be taken as query example. Experiment results show that our approaches are effective for cross-media retrieval, and meanwhile achieve a significant improvement over content-based image retrieval and content-based audio retrieval.",
On the Intruder Detection for Sinkhole Attack in Wireless Sensor Networks,"In a wireless sensor network, multiple nodes would send sensor readings to a base station for further processing. It is well-known that such a many-to-one communication is highly vulnerable to the sinkhole attack, where an intruder attracts surrounding nodes with unfaithful routing information, and then performs selective forwarding or alters the data passing through it. A sinkhole attack forms a serious threat to sensor networks, particularly considering that such networks are often deployed in open areas and of weak computation and battery power. In this paper, we present a novel algorithm for detecting the intruder in a sinkhole attack. The algorithm first finds a list of suspected nodes, and then effectively identifies the intruder in the list through a network flow graph. The algorithm is also robust to deal with cooperative malicious nodes that attempt to hide the real intruder. We have evaluated the performance of the proposed algorithm through both numerical analysis and simulations, which confirmed the effectiveness and accuracy of the algorithm. Our results also suggest that its communication and computation overheads are reasonably low for wireless sensor networks.",
The automatic detection of the optic disc location in retinal images using optic disc location regression,The automatic detection of the position of the optic disc is an important step in the automatic analysis of retinal images. A method to detect the approximate position of the optic disc using kNN regression is presented. The method starts by building a regression model of the optic disc position. Using a prior vessel segmentation all vessel pixels are searched for those which are inside the optic disc according to the regression model. The regression output is blurred to handle noise. The point which is closest to the middle of the optic disc is chosen. The method was tested on 1000 screening images and was able to find the correct position in 99.9% of all cases,
A Hybrid Control Architecture for Autonomous Robotic Fish,"This paper presents a hybrid control architecture for autonomous robotic fishes which are able to swim and navigate in unknown or dynamically changing environments. It has a three-layer configuration: cognitive layer, behaviour layer and swim pattern layer. The state-based planning in the cognitive layer provides a good foundation for potential adaptation through machine learning methods such as reinforcement learning (RL). The behaviour layer and the swim pattern layer are specially designed to match the needs for the real-time control of our robotic fish. To test the feasibility and performance of the proposed architecture, the experiment of ""tank border exploration"" is conducted with Q-learning",
GROUP: A Grid-Clustering Routing Protocol for Wireless Sensor Networks.,"In this paper, we propose GROUP, a grid-clustering routing protocol that provides scalable and efficient packet routing for large-scale wireless sensor networks. The sink proactively, dynamically and randomly builds a cluster grid structure in GROUP. Only small part of all sensor nodes will participate in election of cluster heads. GROUP can distribute the energy load among the sensors in the network, and provide in-network processing support to reduce the amount of information that must be transmitted to the sink. We have evaluated the performance of GROUP through simulation experiments. The results of simulations show that GROUP is an energy-efficient and scalable routing protocol for large-scale wireless sensor networks",
A comparative study of Web services-based event notification specifications,"Web services-based event notification is an emerging technology that combines the asynchronous communication feature of event notification mechanisms and the interoperability feature of Web services technologies. Web services-based event notification systems are important components for service-oriented grid computing. WS-eventing and WS-notification are two major competing specifications for these systems. This paper is a comparative study of these specifications. The focuses of this research are on identifying the similarities and differences between these two specifications and identifying their evolutionary path from previous specifications. We found that competing Web services specifications take ideas and concepts from each other during the development progress, which is good for the maturity of the Web services-based event notification technology. We also identified several major changes from previous event notification systems to Web services-based event notification systems. In the end, we presented our WS-messenger project that supports both WS-eventing and WS-notification specifications and provides mediation between them",
Multisensor Real-time Risk Assessment using Continuous-time Hidden Markov Models,"The use of tools for monitoring the security state of assets in a network is an essential part of network management. Traditional risk assessment methodologies provide a framework for manually determining the risks of assets, and intrusion detection systems can provide alerts regarding security incidents, but these approaches do not provide a real-time high level overview of the risk level of assets. In this paper we further extend a previously proposed realtime risk assessment method to facilitate more flexible modeling with support for a wide range of sensors. Specifically, the paper develops a method for handling continuous-time sensor data and for determining a weighted aggregate of multisensor input","Risk management,
Hidden Markov models,
Real time systems,
Intrusion detection,
Sensor systems,
Data security,
Computer networks,
Information security,
Quality of service,
Monitoring"
Highlight Summarization in Sports Video Based on Replay Detection,"Highlight summarization technology has been studied widely in sports video analysis. In this paper, we propose a highlight summarization system based on replays. First the replay clips in the sports video are extracted as the highlight candidates. Then the features including audio energy and motion activity are employed to rank the arousal level of the replay clips. Finally we model the highlight with the arousal rank to generate summarization. The contribution of this paper concentrates on two aspects. Firstly, Event-Replay (ER) structure is proposed and some new features are employed to represent the arousal levels of ER for general sports video. Secondly a novel highlight model is proposed considering the inter-relation of ERs. The experiments evaluate the rationality of the system",
Leveraging Channel Diversity for Key Establishment in Wireless Sensor Networks,,
Improving Evolution Strategies through Active Covariance Matrix Adaptation,"This paper proposes a novel modification to the derandomised covariance matrix adaptation algorithm used in connection with evolution strategies. In existing variants of that algorithm, only information gathered from successful offspring candidate solutions contributes to the adaptation of the covariance matrix, while old information passively decays. We propose to use information about unsuccessful offspring candidate solutions in order to actively reduce variances of the mutation distribution in unpromising directions of the search space. The resulting strategy is referred to as Active-CMA-ES. In experiments on a standard suite of test functions, Active-CMA-ES consistently outperforms other strategy variants.",
Towards the Co-Evolution of Influence Map Tree Based Strategy Game Players,"We investigate the use of genetic algorithms to play real-time computer strategy games. To overcome the knowledge acquisition bottleneck found in using traditional expert systems, scripts, or decision trees we use genetic algorithms to evolve game players. The spatial decision makers in our game players use influence maps as a basic building block from which they construct and evolve trees containing complex game playing strategies. Information from influence map trees is combined with that from an A* pathfinder, and used by another genetic algorithm to solve the allocation problems present within many game decisions. As a first step towards evolving strategic players we develop this system in the context of a tactical game. Results show the co-evolution of coordinated attacking and defending strategies superior to their hand-coded counterparts",
"The /spl Delta//sup 2/-conjecture for L(2,1)-labelings is true for direct and strong products of graphs","A variation of the channel-assignment problem is naturally modeled by L(2,1)-labelings of graphs. An L(2,1)-labeling of a graph G is an assignment of labels from {0,1,...,/spl lambda/} to the vertices of G such that vertices at distance two get different labels and adjacent vertices get labels that are at least two apart and the /spl lambda/-number /spl lambda/(G) of G is the minimum value /spl lambda/ such that G admits an L(2,1)-labeling. The /spl Delta//sup 2/-conjecture asserts that for any graph G its /spl lambda/-number is at most the square of its largest degree. In this paper it is shown that the conjecture holds for graphs that are direct or strong products of nontrivial graphs. Explicit labelings of such graphs are also constructed.",
Improved bounds for online routing and packing via a primal-dual approach,"In this work we study a wide range of online and offline routing and packing problems with various objectives. We provide a unified approach, based on a clean primal-dual method, for the design of online algorithms for these problems, as well as improved bounds on the competitive factor. In particular, our analysis uses weak duality rather than a tailor made (i.e., problem specific) potential function. We demonstrate our ideas and results in the context of routing problems. Using our primal-dual approach, we develop a new generic online routing algorithm that outperforms previous algorithms suggested earlier by Y. Azar et al. (1993, 1997). We then show the applicability of our generic algorithm to various models and provide improved algorithms for achieving coordinate-wise competitiveness, maximizing throughput, and minimizing maximum load. In particular, we improve the results obtained by A. Goel et al. (2001) by an O(log n) factor for the problem of achieving coordinate-wise competitiveness, and by an O(log log n) factor for the problem of maximizing the throughput. For some of the settings we also prove improved lower bounds. We believe our results further our understanding of the applicability of the primal-dual method to online algorithms, and we are confident that the method will prove useful to other online scenarios. Finally, we revisit the notions of coordinate-wise and prefix competitiveness in an offline setting. We design the first polynomial time algorithm that computes an almost optimal coordinate-wise routing for several routing models. We also revisit previously studied routing models by A. Kumar and J.M. Kleinberg (2000) and A. Goel and A. Meyerson (2005) and prove tight lower and upper bounds of Theta(log n) on prefix competitiveness for these models",
Wavelet analysis for EEG feature extraction in deception detection,"Deception detection has important clinical and legal implications. However, the reliability of methods for the discrimination between truthful and deceptive responses is still limited. Efforts to improve reliability have examined measures of central nervous system function such as EEG. However, EEG analyses based on either time- or frequency-domain parameters have had mixed results. Because EEG is a nonstationary signal, the use of joint time-frequency features may yield more reliable results for detecting deception. The goal of this study was to investigate the feasibility of deception detection based on EEG features extracted through wavelet transformation. EEG was recorded from 4 electrode sites (F3, F4, F7, F8) during a modified version of the Guilty Knowledge Test (GKT) in 5 subjects. Wavelet analysis revealed significant differences between deceptive and truthful responses. These differences were detected in features whose frequency range roughly corresponds to the EEG beta rhythm and within a time window which coincides with the P300 component. These preliminary results indicate that joint time-frequency EEG features extracted through wavelet analysis may provide a more reliable method for detecting deception than standard ERPs",
WSDLTest - A Tool for Testing Web Services,"A significant barrier to the use of Web services is the problem of testing them. One of the solutions to deal with the problem lies in the ability to simulate the usage of the services. Requests must be generated and responses must be validated automatically in a fast and reliable manner. To accomplish this goal, we have developed a tool called WSDLTest. WSDLTest is part of a larger complex tool set - DataTest - for generating and validating system test data. The architecture and functionality of this tool, as well as the experience gained from using it, are presented",
Computation of Adalines' sensitivity to weight perturbation,"In this paper, the sensitivity of Adalines to weight perturbation is discussed. According to the discrete feature of Adalines' input and output, the sensitivity is defined as the probability of an Adaline's erroneous outputs due to weight perturbation with respect to all possible inputs. By means of hypercube model and analytical geometry method, a heuristic algorithm is given to accurately compute the sensitivity. The accuracy of the algorithm is verified by computer simulations.",
Algebraic lattice constellations: bounds on performance,"In this work, we give a bound on performance of any full-diversity lattice constellation constructed from algebraic number fields. We show that most of the already available constructions are almost optimal in the sense that any further improvement of the minimum product distance would lead to a negligible coding gain. Furthermore, we discuss constructions, minimum product distance, and bounds for full-diversity complex rotated Z[i]/sup n/-lattices for any dimension n, which avoid the need of component interleaving.",
A Framework for Statistical Timing Analysis using Non-Linear Delay and Slew Models,"In this paper, we propose a framework for statistical static timing analysis (SSTA) considering intra-die process variations. Given a cell library, we propose an accurate method to characterize the gate and interconnect delay as well as slew as a function of underlying parameter variations. Using these accurate delay models, we propose a method to perform SSTA based on a quadratic delay and slew model. The method is based on efficient dimensionality reduction technique used for accurate computation of the max of two delay expansions. Our results indicate less than 4% error in the variance of the delay models compared to SPICE Monte Carlo and less than 1% error in the variance of the circuit delay compared to Monte Carlo simulations",
Design of High Performance MVAPICH2: MPI2 over InfiniBand,"MPICH2 provides a layered architecture for implementing MPI-2. In this paper, we provide a new design for implementing MPI-2 over InfiniBand by extending the MPICH2 ADI3 layer. Our new design aims to achieve high performance by providing a multi-communication method framework that can utilize appropriate communication channels/devices to attain optimal performance without compromising on scalability and portability. We also present the performance comparison of the new design with our previous design based on the MPICH2 RDMA channel. We show significant performance improvements in micro-benchmarks and NAS Parallel Benchmarks.",
Phenomenological Model of Diffuse Global and Regional Atrophy Using Finite-Element Methods,"The main goal of this work is the generation of ground-truth data for the validation of atrophy measurement techniques, commonly used in the study of neurodegenerative diseases such as dementia. Several techniques have been used to measure atrophy in cross-sectional and longitudinal studies, but it is extremely difficult to compare their performance since they have been applied to different patient populations. Furthermore, assessment of performance based on phantom measurements or simple scaled images overestimates these techniques' ability to capture the complexity of neurodegeneration of the human brain. We propose a method for atrophy simulation in structural magnetic resonance (MR) images based on finite-element methods. The method produces cohorts of brain images with known change that is physically and clinically plausible, providing data for objective evaluation of atrophy measurement techniques. Atrophy is simulated in different tissue compartments or in different neuroanatomical structures with a phenomenological model. This model of diffuse global and regional atrophy is based on volumetric measurements such as the brain or the hippocampus, from patients with known disease and guided by clinical knowledge of the relative pathological involvement of regions and tissues. The consequent biomechanical readjustment of structures is modelled using conventional physics-based techniques based on biomechanical tissue properties and simulating plausible tissue deformations with finite-element methods. A thermoelastic model of tissue deformation is employed, controlling the rate of progression of atrophy by means of a set of thermal coefficients, each one corresponding to a different type of tissue. Tissue characterization is performed by means of the meshing of a labelled brain atlas, creating a reference volumetric mesh that will be introduced to a finite-element solver to create the simulated deformations. Preliminary work on the simulation of acquisition artefacts is also presented. Cross-sectional and longitudinal sets of simulated data are shown and a visual classification protocol has been used by experts to rate real and simulated scans according to their degree of atrophy. Results confirm the potential of the proposed methodology",
Converting Output Scores from Outlier Detection Algorithms into Probability Estimates,"Current outlier detection schemes typically output a numeric score representing the degree to which a given observation is an outlier. We argue that converting the scores into well-calibrated probability estimates is more favorable for several reasons. First, the probability estimates allow us to select the appropriate threshold for declaring outliers using a Bayesian risk model. Second, the probability estimates obtained from individual models can be aggregated to build an ensemble outlier detection framework. In this paper, we present two methods for transforming outlier scores into probabilities. The first approach assumes that the posterior probabilities follow a logistic sigmoid function and learns the parameters of the function from the distribution of outlier scores. The second approach models the score distributions as a mixture of exponential and Gaussian probability functions and calculates the posterior probabilites via the Bayes' rule. We evaluated the efficacy of both methods in the context of threshold selection and ensemble outlier detection. We also show that the calibration accuracy improves with the aid of some labeled examples.",
The Wireless Autonomous Spanning tree Protocol for Multihop Wireless Body Area Networks,"Wireless body area networks (WBANs) have gained a lot of interest in the world of medical monitoring. Current implementations generally use a large single hop network to connect all sensors to a personal server. However recent research pointed out that multihop networks are more energy-efficient and even necessary when applied near the human body with inherent severe propagation loss. In this paper we present a slotted multihop approach to medium access control and routing in wireless body area networks, the wireless autonomous spanning tree protocol or WASP. It uses crosslayer techniques to achieve efficient distributed coordination of the separated wireless links. Traffic in the network is controlled by setting up a spanning tree and by broadcasting scheme messages over it that are used both by the parent and the children of each node in the tree. We analyze the performance of WASP and show the simulation results",
FinFET SRAM with Enhanced Read / Write Margins,"In this work, the impact of this pass-gate feedback (PGFB) technique on cell write-ability is examined, and gate workfunction (Phim) tuning for optimization of the trade-off with read margin is discussed. To further improve cell write-ability, the p-channel pull-up devices can also be operated in BG mode, with their back gates driven by a separate write word line. This pull-up write gating (PUWG) technique is effective for maintaining larger than 6 standard deviations yield down to 0.4V VDD without area penalty, making FinFET-based 6-T SRAM compelling for high-density memory applications",
Exploiting the directional features in MPEG-2 for H.264 intra transcoding,"This paper presents an MPEG-2 to H.264 intra frame video transcoder. The DCT coefficients gathered from the MPEG-2 process are used to estimate the directional features in a picture. The estimated directional features are then used to compute the intra prediction modes in the H.264 encoding stage. The INTRA 16/spl times/16 vs. INTRA4/spl times/4 MB coding mode decision is made based on the variance of the DC coefficients of the 8/spl times/8 DCT blocks in the input MPEG-2 video. The proposed approach completely avoids the MB mode and prediction mode computations in the H.264 encoding stage, there by reducing the transcoder complexity substantially. The results show that our approach results in over 30% drop in the intra frame encoding time with a negligible drop in PSNR. A comparison with a cascaded reference transcoder is presented.",
Triggered Message Sequence Charts,"This paper introduces triggered message sequence charts (TMSCs), a graphical, mathematically well-founded framework for capturing scenario-based system requirements of distributed systems. Like message sequence charts (MSCs), TMSCs are graphical depictions of scenarios, or exchanges of messages between processes in a distributed system. Unlike MSCs, however, TMSCs are equipped with a notion of trigger that permits requirements to be made conditional, a notion of partiality indicating that a scenario may be subsequently extended, and a notion of refinement for assessing whether or not a more detailed specification correctly elaborates on a less detailed one. The TMSC notation also includes a collection of composition operators allowing structure to be introduced into scenario specifications so that interactions among different scenarios may be studied. In the first part of this paper, TMSCs are introduced and their use in support of requirements modeling is illustrated via two extended examples. The second part develops the mathematical underpinnings of the language",
Service Oriented Architecture (SOA) in Industrial Systems,"Service oriented architecture (SOA) is attended as a design framework for realizing rapid and low-cost system development, easily modified system, and total system quality. SOA is mainly applied to business information systems using Web services standards and technologies, and is rapidly becoming a standard approach for enterprise IT systems. In industrial systems, SOA has been successfully applied such as SCM, order entry system, etc. However, in the low layer management and control systems, Web services cannot apply because of particular requirement. The specific functions needed to apply the concept of SOA to real-time system (response time, support of event-driven, asynchronous parallel applications, complicated human interface support, reliability, etc.). First of this talk, SOA will be briefly explained. Then, we will point out several issues arisen when the concept of SOA applied to the industrial systems. Finally, example industrial systems (business systems and command and control systems) applied SOA will be introduced.",
CDF level 2 trigger upgrade,"We describe the new CDF Level 2 Trigger, which was commissioned during Spring 2005. The upgrade was necessitated by several factors that included increased bandwidth requirements, in view of the growing instantaneous luminosity of the Tevatron, and the need for a more robust system, since the older system was reaching the limits of maintainability. The challenges in designing the new system were interfacing with many different upstream detector subsystems, processing larger volumes of data at higher speed, and minimizing the impact on running the CDF experiment during the system commissioning phase. To meet these challenges, the new system was designed around a general purpose motherboard, the PULSAR, which is instrumented with powerful FPGAs and modern SRAMs, and which uses mezzanine cards to interface with upstream detector components and an industry standard data link (S-LINK) within the system.",
Enhancing digital cephalic radiography with mixture models and local gamma correction,"We present a new algorithm, called the soft-tissue filter, that can make both soft and bone tissue clearly visible in digital cephalic radiographies under a wide range of exposures. It uses a mixture model made up of two Gaussian distributions and one inverted lognormal distribution to analyze the image histogram. The image is clustered in three parts: background, soft tissue, and bone using this model. Improvement in the visibility of both structures is achieved through a local transformation based on gamma correction, stretching, and saturation, which is applied using different parameters for bone and soft-tissue pixels. A processing time of 1 s for 5 Mpixel images allows the filter to operate in real time. Although the default value of the filter parameters is adequate for most images, real-time operation allows adjustment to recover under- and overexposed images or to obtain the best quality subjectively. The filter was extensively clinically tested: quantitative and qualitative results are reported here.",
On the Construction of a Strongly Connected Broadcast Arborescence with Bounded Transmission Delay,"Energy conservation is an important concern in wireless networks. Many algorithms for constructing a broadcast tree with minimum energy consumption and other goals have been developed. However, no previous research work considers the total energy consumption and transmission delays of the broadcast tree simultaneously. In this paper, based on an (alpha, beta)-tree, a novel concept to wireless networks, we define a new strongly connected broadcast arborescence with bounded transmission delay (SBAT) problem and design the strongly connected broadcast arborescence (SBA) algorithm with linear running time to construct a strongly connected broadcast tree with bounded total power, while satisfying the constraint that the transmission delays between the source and the other hosts are also bounded. We also propose the distributed version of the SBA algorithm. The theoretical analysis and simulation results show that the SBA algorithm gives a proper solution to the SBAT problem",
JIRiSS - an Eclipse plug-in for Source Code Exploration,"JIRiSS (information retrieval based software search for Java) is a software exploration tool that uses an indexing engine based on an information retrieval method. JIRiSS is implemented as a plug-in for Eclipse and it allows the user to search Java source code for the implementation of concepts formulated as natural language queries. The results of the query are presented as a ranked list of software methods or classes, ordered by the similarity to the user query. In addition to that, JIRiSS includes other advanced features like automatically generated software vocabulary, advanced query formulation options including spell-checking as well as fragment-based search",
A Semantic Overlay for Self- Peer-to-Peer Publish/Subscribe,"Publish/Subscribe systems provide a useful platform for delivering data (events) from publishers to subscribers in an anonymous fashion in distributed networks. In this paper, we promote a novel design principle for self-. dynamic and reliable content-based publish/subscribe systems and perform a comparative analysis of its probabilistic and deterministic implementations. More specifically, we present a generic content-based publish/subscribe system, called DPS (Dynamic Publish/Subscribe). DPS combines classical content-based filtering with self-. (self-organizing, selfconfiguring, and self-healing) subscription-driven clustering of subscribers. DPS gracefully adapts to failures and changes in the system while achieving scalable events delivery. DPS includes a variety of fault-tolerant deterministic and probabilistic content-based publication/subscription schemes. These schemes are targeted toward scalability, and aim at reducing and distributing the number of messages exchanged. Reliability and scalability of our system are shown through analytical and experimental evaluation.",
Towards Secure and Scalable Computation in Peer-to-Peer Networks,"We consider the problems of Byzantine agreement and leader election, where a constant fraction b < 1/3 of processors are controlled by a malicious adversary. The first problem requires that all uncorrupted processors come to an agreement on a bit initially held by one of the uncorrupted processors; the second requires that the uncorrupted processors choose a leader who is uncorrupted. Motivated by the need for robust and scalable computation in peer-to-peer networks, we design the first scalable protocols for these problems for a network whose degree is polylogarithmic in its size. By scalable, we mean that each uncorrupted processor sends and processes a number of bits that is only polylogarithmic in n. (We assume no limit on the number of messages sent by corrupted processors.) With high probability, our Byzantine agreement protocol results in agreement among a 1 - O(1/ln n) fraction of the uncorrupted processors. With constant probability, our leader election protocol elects an uncorrupted leader and ensures that a 1 - O(1/ln n) fraction of the uncorrupt processors know this leader. We assume a full information model. Thus, the adversary is assumed to have unlimited computational power and has access to all communications, but does not have access to processors' private random bits",
A Necessary and Sometimes Sufficient Condition for the Feasibility of Sets of Sporadic Hard-Deadline Tasks,"This paper describes a necessary condition for feasibility of scheduling a set of sporadic hard-deadline tasks on identical multiprocessor platforms, which is also a sufficient condition if there is only a single processor. The key contribution is the characterization of the maximum, over all time intervals of a given length, of the amount of computation that must be completed to meet all deadlines, and a method of computing this function efficiently to any desired degree of accuracy. Empirical data are provided to verify that the new infeasibility test can be computed efficiently and is an improvement over previously known checks for infeasibility",
Semantic-based Grid Resource Discovery and its Integration with the Grid Service Broker,This paper addresses the need of semantic component in the grid environment to discover and describe the grid resources semantically. We propose semantic grid architecture by introducing a knowledge layer at the top of Gridbus broker architecture and thereby enabling broker to discover resources semantically. The semantic component in the knowledge layer enables semantic description of grid resources with the help of ontology template. The ontology template has been created using Protege-OWL editor for different types of computing resources in the grid environment. The Globus Toolkit's MDS is used to gather grid resource information and Protege-OWL libraries are used to dynamically create knowledge base of grid resources. Algernon inference engine is used for interacting with the knowledge base to discover suitable resources.,
Smooth Collision Avoidance: Practical Issues in Dynamic Humanoid Motion,"In this paper we address smooth and collision-free whole-body motion planning for humanoid robots. A two-stage iterative planning framework is introduced where geometric motion planner and dynamic pattern generator interacts by exchanging the trajectory, to obtain 3D whole-body dynamic motions simultaneous tasks including locomotion, in complex environments. We propose a practical method for smooth motion reshaping to avoid collisions in generated dynamic motion. Based on motion editing techniques in computer graphics animation, smooth collision-avoiding motion is generated through trajectory deformation. The validity of the proposed reshaping method is verified by computer simulations and experiments using humanoid platform HRP-2",
Impact of Node Heterogeneity in ZigBee Mesh Network Routing,"Based on the IEEE 802.15.4 LR-WPAN standard, the ZigBee standard has been proposed to interconnect simple, low rate, and battery powered wireless devices. The deployment of ZigBee networks is expected to facilitate numerous applications, such as home-appliance networks, home healthcare, medical monitoring, consumer electronics, and environmental sensors. An effective routing scheme in a ZigBee network is particularly important in that it is the key to achieve resource (e.g., bandwidth and energy) efficiency in ZigBee networks. Routing in a ZigBee network is not exactly the same as in a MANET. In particular, while full function devices (FFD) can serve as network coordinators or network routers, reduced function devices (RFD) can only associate and communicate with FFDs in a ZigBee network. Therefore, different from traditional MANET routing algorithms, which only take into account node mobility to figure out a best route to a given destination, node heterogeneity plays an important role in ZigBee network routing. In this paper, we perform extensive evaluation, using NS-2 simulator, to study the impact of node heterogeneity on ZigBee mesh network routing. The results show that the ZigBee mesh routing algorithm exhibits significant performance difference when the network is highly heterogenous. We also reveal that the node type and the role of the node plays a critical role in deciding routing performances.",
Optimal Adaptation in Web Processes with Coordination Constraints,"We present methods for optimally adapting Web processes to exogenous events while preserving inter-service constraints that necessitate coordination. For example, in a supply chain process, orders placed by a manufacturer may get delayed in arriving. In response to this event, the manufacturer has the choice of either waiting out the delay or changing the supplier. Additionally, there may be compatibility constraints between the different orders, thereby introducing the problem of coordination between them if the manufacturer chooses to change the suppliers. We focus on formulating the decision making models of the managers, who must adapt to external events while satisfying the coordination constraints, using Markov decision processes. Our methods range from being centralized and globally optimal in their adaptation but not scalable, to decentralized that is suboptimal but scalable to multiple managers. We also develop a hybrid approach that improves on the performance of the decentralized approach with a minimal loss of optimality",
Automated liquid dispensing pin for DNA microarray applications,"This paper describes a new liquid dispensing/aspirating system that is capable of producing micron-size spots/droplets for molecular biology research and analysis. In particular, the application is focused on deoxyribonucleic-acid microarray fabrication with the goals of uniform spot morphology, smaller spot size, higher yield, more efficient use of biological materials, and the capacity to handle high viscosity liquids. The new system is based on active sensing and control and it is part of a fully integrated robotic microarray system for genomic and proteomic applications. The prototype system handles thick liquids such as 100% glycerol as well as aqueous solutions and generates uniform spots in a contactless manner with controllable spot size ranging from 80 microns to 200 microns. Note to Practitioners-Microarraying is a powerful tool that enables the expression profiling of a vast quantity genetic/proteomic materials in parallel. Current printing technology includes photolithography, impact pins, inkjet, etc. Although these designs have been successful in printing spot size down to 100 microns, a new approach is needed to handle the order of magnitude increase in density while reducing cost. The SmartPin described in this paper is a sensor-based motion-controlled print head created for printing the next-generation microarrays. It is capable of depositing any aqueous samples (e.g., deoxyribonucleic acid, protein, etc.) and has the flexibility and cost advantage of the printing approach and yet possesses high performance and telepresence accessibility. The system utilizes an optical-fiber-based sensor probe for both sensing and sample delivery. Sensing is then integrated with computer control so that it can generate the microarray with fully controllable spot density and size. By maintaining a uniform gap distance between the pin tip and the target slide, performance and reliability are enhanced. The current work is at the prototype development state and will need further refinement for commercialization.","DNA,
Printing,
Size control,
Liquids,
Control systems,
Proteomics,
Prototypes,
Costs,
Optical sensors,
Fabrication"
A Construction Kit for Electronic Textiles,"Construction kits have long been popular as educational artifacts, supporting and encouraging creative explorations of engineering and design; but to date, such kits have had little connection with the new and expanding field of electronic textiles (e-textiles). We believe that creating an ""e-textile construction kit"" could provide a powerful new medium to engage a diverse range of students in electrical engineering and computer science. This paper, then, describes a construction kit designed to introduce novices to electronics, computing and design via e-textiles. We describe each component of the kit, provide examples of constructions that were built with the kit, and examine the durability of these constructions. We conclude with a discussion of the results of preliminary user testing and an exploration of our plans for continued work in this area.",
An Infrastructure for Web Services Migration for Real-Time Applications,"Service-oriented architecture (SOA) is a highly promising technology for software development. It can greatly reduce the development time and cost and achieve system agility. Web service is the current practice for realizing SOA. Due to the nature of widely distributed Web service providers, the service performance could be impacted when the network traffic is congested. There are methods, such as communication cost based service selection strategies, developed to cope with the problem. In this paper, we propose a complementing technology, migratable Web services, to overcome the performance problems for real-time applications. A service can be implemented by multiple light weighted Web services and some of them can be migrated to strategic sites to minimize the communication delays so the real-time requirements can be met. We have designed an infrastructure to support Web service migration and execution. Also, appropriate decision mechanism, which decides whether a Web service should be migrated and where to send it to, is the key for major performance gains. We use genetic algorithm to efficiently make the best migration decisions. Experimental studies show that the approach can achieve significant performance improvement",
Cover Selection for Steganographic Embedding,"The primary goal of image steganography techniques has been to maximize embedding rate while minimizing the detectability of the resulting stego images against steganalysis techniques. However, one particular advantage of steganography, as opposed to other information hiding techniques, is that the embedder has the freedom to choose a cover image that result in the least detectable stego image. This resource has largely remained unexploited in the proposed embedding techniques. In this paper, we study the problem of cover selection by investigating three scenarios in which the embedder has either no knowledge, partial knowledge, or full knowledge of the steganalysis technique. For example, we illustrate through experiments how simple statistical measures could help embedder minimize detectability, at times by 65%, in the partial knowledge case.",
Systemic Management of Architectural Decisions in Enterprise Architecture Planning. Four Dimensions and Three Abstraction Levels,"This paper presents a process model for the management of architectural decisions in enterprise architecture planning. First, decisions are made at the enterprise level, with strategic business considerations on the enterprise information, systems and technology strategy and governance issues. The next step is to define the domains, to then go on with domain architecture decisions. At the systems level, the enterprise and domain architecture decisions are collected and converted into architecture descriptions accurate in precision, form and detail to be given as input to the information systems development process, following the architectural planning. The model is derived from previous work and empirical findings in three large organizations, where the enterprise architecture and enterprise systems have been developed. This case study contributes with considerations on the domains, their definition, and produces refinements to an enterprise architecture process model presented before. For the development of the model, the ""living system"" paradigm is followed.",
An Analytical Framework for Route Failure Time of Multiple Node-Disjoint Paths in Mobile Ad hoc Networks,"In a mobile ad hoc network, routes may often fail due to movements of nodes. In this paper, we analyze route failure time of multiple node-disjoint paths between a given pair of nodes. We discuss an exact expression for the expected value of route failure time when lifetimes of individual paths are independent and identically distributed random variables. In case lifetimes are not independent, we discuss an upper bound on route failure time. In order to validate the model, we simulate exponentially distributed random variables. Values of route failure time obtained from simulations are close to those obtained analytically",
TCP/IP Interaction Based on Congestion Price: Stability and Optimality,"Despite the large body of work studying congestion control and adaptive routing in isolation, much less attention has been paid to whether these two resource-allocation mechanisms work well together to optimize user performance. Most analysis of congestion control assumes static routing, and most studies of adaptive routing assume that the offered traffic is fixed. In this paper, we analyze the interaction between congestion control and adaptive routing, and study the stability and optimality of the joint system. Previous work has shown that the system can be modelled as a joint optimization problem that naturally leads to a primal-dual algorithm with shortest-path routing using congestion prices as the link weights. In practice, the algorithm is commonly unstable. We consider three alternative timescale separations and examine the stability and optimality of each system. Our analytic characterizations and simulation experiments demonstrate how the step size of the congestion-control algorithm affects the stability of the system, and how the timescale of each control loop and homogeneity of link capacities affect system stability and optimality. The stringent conditions imposed for stability suggests that congestion price would be a poor feedback mechanism in practice.",
High-speed driving method using bipolar scan waveform in AC plasma display panel,"This paper proposes a new high-speed driving method using the bipolar scan waveform with a scan width of 1 /spl mu/s in an ac-plasma display panel. The bipolar scan waveform in an address period consists of a two-step pulse with two different polarities, i.e., a forward scan pulse with a negative polarity and reverse scan pulse with a positive polarity, which can produce two address discharges, including a primary address discharge for generating wall charges and secondary address discharge for accumulating wall charges. To produce the fast address discharge stably using the bipolar scan pulse during an address period, a new reset waveform is designed based on a V/sub t/ close curve analysis, and the address discharge characteristics examined under various reset and address waveforms. As a result of adopting the proposed driving method, a high-speed address with a scan width of 1 /spl mu/s is successfully obtained when using a checkered pattern on a 4-in test panel.",
Spline-Based Robot Navigation,"This paper offers a path planning algorithm based on splines. The sought path avoids the obstacles, and is smooth and short. Smoothing is used as an integral part of the algorithm, and not only as a final improvement to a path found by other methods. In order to avoid a very difficult optimization over all the path's points, it is modeled by a sequence of splines defined by a gradually increasing number of knots",
Parallel interleaved inverters for reactive power and harmonic compensation,"This article investigates the concept of paralleling power inverters for reactive power and harmonic compensation. The investigation focuses on a topology that shares the dc-bus capacitor between two parallel interleaved inverters. The advantages of the proposed approach are: i) decreased current ripple or use of lower switching frequency due to the interleaving, ii) reduced stress in dc-link capacitor due to the shared connection, iii) efficient implementation for high power applications because of paralleling. Different comparisons between the selected topology and regular power converters are discussed. Practical tests, on a three-phase 5 kVA, 400 V prototype, are presented to validate the analysis.",
Multigrid Methods for the Stokes System,"The choice of multigrid method depends strongly on the type of discretization used and the problem formulation employed. This article gives an overview of multigrid methods for the Stokes equations, focusing on the saddle-point problem and on stable discretizations for staggered and vertex-centered grids",
Characterization of modes in coaxial vircator,"Modes' character of high-power microwave (HPM) pulses generated in coaxial vircator is discussed. The electromagnetic (EM) field gains due to beam-wave interaction for TM/sub 01/ and TE/sub 11/ modes are derived according to the E-field distribution pattern inside the anode cavity. Two-and-a-half dimensional (2.5-D) and three-dimensional (3-D) computer simulations are carried out to determine the main mode under different numerical simulation coordinate systems. Finally, the experimental verification is obtained by two different diagnostic methods. The results indicate that for most cases, the TE/sub 11/ mode dominates the output power in the coaxial vircator studied.",
Profiling Users in GUI Based Systems for Masquerade Detection,"Masquerading or impersonation attack refers to the illegitimate activity on a computer system when one user impersonates another user. Masquerade attacks are serious in nature due to the fact that they are mostly carried by insiders and thus are extremely difficult to detect. Detection of these attacks is done by monitoring significant changes in user's behavior based on his/her profile. Currently, such profiles are based mostly on the user command line data and do not represent his/her complete behavior in a graphical user interface (GUI) based system and hence are not sufficient to quickly detect such masquerade attacks. In this paper, we present a new framework for creating a unique feature set for user behavior on GUI based systems. We have collected real user behavior data from live systems and extracted parameters to construct these feature vectors. These vectors contain user information such as mouse speed, distance, angles and amount of clicks during a user session. We model our technique of user identification and masquerade detection as a binary classification problem and use support vector machine (SVM) to learn and classify these feature vectors. We show that our technique can provide detection rates of up to 96% with few false positives based on these feature vectors. We have tested our technique with various feature vector parameters and conclude that these feature vectors can provide unique and comprehensive user behavior information and are powerful enough to detect masqueraders",
XSEED: Accurate and Fast Cardinality Estimation for XPath Queries,"We propose XSEED, a synopsis of path queries for cardinality estimation that is accurate, robust, efficient, and adaptive to memory budgets. XSEED starts from a very small kernel, and then incrementally updates information of the synopsis. With such an incremental construction, a synopsis structure can be dynamically configured to accommodate different memory budgets. Cardinality estimation based on XSEED can be performed very efficiently and accurately. Extensive experiments on both synthetic and real data sets show that even with less memory, XSEED could achieve accuracy that is an order of magnitude better than that of other synopsis structures. The cardinality estimation time is under 2% of the actual querying time for a wide range of queries in all test cases.",
Discount Anonymous On Demand Routing for Mobile Ad hoc Networks,"Recent years have seen a large number of proposals for anonymity mechanisms operating on the application layer. Given that anonymity is no stronger than its weakest link, such proposals are only meaningful if one can offer anonymity guarantees on the communication layer as well. ANODR - or anonymous on demand routing - is one of the leading proposals to deal with this issue. In this paper, we propose a novel technique to address the same problem, but at a lower cost. Our proposal, which we dub discount-ANODR, is built around the same set of techniques as ANODR is. Our proposal has the benefit of achieving substantially lower computation and communication complexities at the cost of a slight reduction of privacy guarantees. In particular, discount-ANODR achieves source anonymity and routing privacy. A route is ""blindly generated"" by the intermediaries on the path between an anonymous source and an identified destination. Route requests in discount-ANODR bear strong similarities to route requests in existing source routing protocols, with the limitation that intermediaries only know the destination of the request and the identity of the previous intermediary - but not whether the latter was the originator of the request. The response to a route request protects the compiled route by means of iterated symmetric encryption, drawing on how messages are prepared before being submitted to a typical synchronous mix network (or onion router). The communication of data subsequently uses such ""route onions"" to channel the packet to the intended destination. We do not use any key exchange, nor do we utilize public key operations at any time; consequently, we do not need to rely on any PKI, CRL or related constructions",
Backtracking Algorithms and Search Heuristics to Generate Test Suites for Combinatorial Testing,"Combinatorial covering arrays have been used in several testing approaches. This paper first discusses some existing methods for finding such arrays. Then a SAT-based approach and a backtracking search algorithm are presented to solve the problem. A novel pruning strategy called SCEH is proposed to increase the efficiency of the methods. Several existing search heuristics and symmetry breaking techniques are also used in the backtracking search algorithm. Lastly, this paper introduces a tool called EXACT (exhaustive search of combinatorial test suites) which implements all the above techniques to construct the covering arrays automatically. The experimental results show that our backtracking search method outperforms other methods in many small size cases",
Functional census of mutation sequence spaces: the example of p53 cancer rescue mutants,"Many biomedical problems relate to mutant functional properties across a sequence space of interest, e.g., flu, cancer, and HIV. Detailed knowledge of mutant properties and function improves medical treatment and prevention. A functional census of p53 cancer rescue mutants would aid the search for cancer treatments from p53 mutant rescue. We devised a general methodology for conducting a functional census of a mutation sequence space by choosing informative mutants early. The methodology was tested in a double-blind predictive test on the functional rescue property of 71 novel putative p53 cancer rescue mutants iteratively predicted in sets of three (24 iterations). The first double-blind 15-point moving accuracy was 47 percent and the last was 86 percent; r = 0.01 before an epiphanic 16th iteration and r = 0.92 afterward. Useful mutants were chosen early (overall r = 0.80). Code and data are freely available (http://www.igb.uci.edu/research/research.html, corresponding authors: R.H.L. for computation and R.K.B. for biology)",
Teaching wireless communication and networking fundamentals using Wi-Fi projects,"Wireless communication and networking often proves to be a quite challenging subject to teach in a meaningful way, because many students appear to find the subject rather dry and technical, and thus quite boring. The authors have prepared some interesting projects to provide the students of wireless communication and networking with a hands-on learning experience. These projects are designed around low-cost Wi-Fi modules and PC cards that are available from local electronics shops. The projects are suitable for classroom use in introductory-level courses about wireless networking. The effectiveness of these projects has been evaluated by both students and teaching team. The feedback from students indicates that both the development and implementation of the projects were successful. This paper describes these projects, their overall effectiveness, and plans for further projects. The impact of Wi-Fi projects on student learning and comprehension is also discussed.","Wireless LAN,
Communication engineering education,
Computer science education"
Definition of the HOSVD based canonical form of polytopic dynamic models,"The main objective of the paper is to introduce how the concept of tensor HOSVD can be carried over to the TP (tensor product) dynamic models, namely, how we can define and generate the ""HOSVD like"" decomposition of linear parameter varying (LPV) dynamic models. We term this decomposition as HOSVD based canonical form of TP model or polytopic model form. The key idea and the basic concept of this decomposition was proposed with the TP model transformation based control design methodology. The novelty of this paper is to present the mathematical background of this concept. The paper shows convergency theorems how the TP model transformation is capable of reconstructing this HOSVD based canonical form numerically. The proofs of the theorems are lengthy, therefore they are omitted. The paper also presents numerical examples to show the applicability, efficiency and uniformity of the numerical reconstruction","Control design,
Tensile stress,
Informatics,
Nonlinear dynamical systems,
Vectors,
Linear matrix inequalities,
Telecommunication computing,
Automation,
Lightweight structures,
Computer aided engineering"
Evolution of Human-Competitive Agents in Modern Computer Games,"Modern computer games have become far more sophisticated than their ancestors. In this process the requirements to the intelligence of artificial gaming characters have become more and more complex. This paper describes an approach to evolve human-competitive artificial players for modern computer games. The agents are evolved from scratch and successfully learn how to survive and defend themselves in the game. Agents trained with standard evolution against a training partner and agents trained by coevolution are presented. Both types of agents were able to defeat or even to dominate the original agents supplied by the game. Furthermore, we have made a detailed analysis of the obtained results to gain more insight into the resulting agents.",
Diversity-Multiplexing Tradeoff in Cooperative Wireless Systems,"We first examine a system with a single source-destination pair and two relays, each node with a single antenna, and explore whether this virtual multi-input multi-output (MIMO) system can mimic a physical MIMO in terms of diversity-multiplexing tradeoff (DMT). We show that even under the idealistic assumption of full-duplex relays and a clustered network, the relay system can never fully mimic a real MIMO DMT, it is multiplexing gain limited. The limitation comes from the fact that source and destination are connected to relays with finite capacity links. We provide communication strategies that achieve the best DMT of this relay system. We extend our work to cover cooperative systems with multiple sources and multiple destinations and show that the same limitation is still in effect. Our results suggest that while cooperative relaying is able to provide high spatial diversity for low multiplexing gains, it can never mimic a physical MIMO for large multiplexing gains.",
Algorithms on negatively curved spaces,"We initiate the study of approximate algorithms on negatively curved spaces. These spaces have become of interest in various domains of computer science including networking and vision. The classical example of such a space is the real-hyperbolic space Hd for d ges 2, but our approach applies to a more general family of spaces characterized by Gromov's (combinatorial) hyperbolic condition. We give efficient algorithms and data structures for problems like approximate nearest-neighbor search and compact, low-stretch routing on subsets of negatively curved spaces of fixed dimension (including Hd as a special case). In a different direction, we show that there is a PTAS for the traveling salesman problem when the set of cities lie, for example, in Hd. This generalizes Arora's results for Ropf d. Most of our algorithms use the intrinsic distance geometry of the data set, and only need the existence of an embedding into some negatively curved space in order to function properly. In other words, our algorithms regard the inter-point distance function as a black box, and are independent of the representation of the input points",
A Vehicular Monitoring System with Power-Efficient Wireless Sensor Networks,"Efficient and effective vehicular traffic planning and management call for an accurate and up-to-date traffic data. In Bangkok the data collection process often involves assigning fieldworkers to monitor traffic in a specified area. The collection process is often time-consuming and inaccurate due to human error. The city has brought in several technologies, such as pneumatic tubes, inductive loops and camera videos, for automatic traffic data collection and monitoring purposes. However, these technologies involve costly installation process and maintenance. It is difficult, or in cases of inductive loops, is impossible to relocate. In this work we study an application of wireless sensor network specifically to motor vehicle monitoring. It delivers several advantages including smaller size, easier installation and maintenance, and relocatable. We propose a star-based topology with a simple polling MAC protocol. It is shown that power efficiency can be easily achieved by setting MAC parameters appropriately. The energy consumption is calculated",
Cluster analysis of dynamic cerebral contrast-enhanced perfusion MRI time-series,"We performed neural network clustering on dynamic contrast-enhanced perfusion magnetic resonance imaging time-series in patients with and without stroke. Minimal-free-energy vector quantization, self-organizing maps, and fuzzy c-means clustering enabled self-organized data-driven segmentation with respect to fine-grained differences of signal amplitude and dynamics, thus identifying asymmetries and local abnormalities of brain perfusion. We conclude that clustering is a useful extension to conventional perfusion parameter maps.",
A Distributed Algorithm for Joint Sensing and Routing in Wireless Networks with Non-Steerable Directional Antennas,"In many energy-rechargeable wireless sensor networks, sensor nodes must both sense data from the environment, and cooperatively forward sensed data to data sinks. Both data sensing and data forwarding (including data transmission and reception) consume energy at sensor nodes. We present a distributed algorithm for optimal joint allocation of energy between sensing and communication at each node to maximize overall system utility (i.e., the aggregate amount of information received at the data sinks). We consider this problem in the context of wireless sensor networks with directional, non-steerable antennas. We first formulate a joint data-sensing and data-routing optimization problem with both per-node energy-expenditure constraints, and traditional flow routing/conservation constraints. We then simplify this problem by converting it to an equivalent routing problem, and present a distributed gradient-based algorithm that iteratively adjusts the per-node amount of energy allocated between sensing and communication to reach the system-wide optimum. We prove that our algorithm converges to the maximum system utility. We quantitatively demonstrate the energy balance achieved by this algorithm in a network of small, energy-constrained X-band radars, connected via point- to-point 802.11 links with non-steerable directional antennas.",
Morphological Heart Arrhythmia Detection Using Hermitian Basis Functions and kNN Classifier,"This paper presents the results of morphological heart arrhythmia detection based on features of electrocardiography, ECG, signal. These signals are obtained from MIT/BIH arrhythmia database. The ECG beats were first modeled using Hermitian basis functions, (HBF). In this step, the width parameter, sigma, of HBF was optimized to minimize the model error. Then, the feature vector which consists of the parameters of the model is used as an input to k-nearest neighbor, kNN, classifier to examine the efficiency of the model. In our experiments, seven different types of arrhythmias have been considered. We achieved the sensitivity of 99.00% and specificity of 99.84% which are comparable to previous works. These results were obtained in less than 0.6 second which is suitable for real-time diagnosis of heart arrhythmias",
Compatible phase co-scheduling on a CMP of multi-threaded processors,"The industry is rapidly moving towards the adoption of chip multi-processors (CMPs) of simultaneous multi-threaded (SMT) cores for general purpose systems. The most prominent use of such processors, at least in the near term, is as job servers running multiple independent threads on the different contexts of the various SMT cores. In such an environment, the co-scheduling of phases from different threads plays a significant role in the overall throughput. Less throughput is achieved when phases from different threads that conflict for particular hardware resources are scheduled together, compared with the situation where compatible phases are co-scheduled on the same SMT core. Achieving the latter requires precise per-phase hardware statistics that the scheduler can use to rapidly identify possible incompatibilities among phases of different threads, thereby avoiding the potentially high performance cost of inter-thread contention. In this paper, we devise phase co-scheduling policies for a dual-core CMP of dual-threaded SMT processors. We explore a number of approaches and find that the use of ready and in-flight instruction metrics permits effective co-scheduling of compatible phases among the four contexts. This approach significantly outperforms the worst static grouping of threads, and very closely matches the best static grouping, even outperforming it by as much as 7%",
Improving Data Association in Vision-based SLAM,"This paper presents an approach to vision-based simultaneous localization and mapping (SLAM). Our approach uses the scale invariant feature transform (SIFT) as features and applies a rejection technique to concentrate on a reduced set of distinguishable, stable features. We track detected SIFT features over consecutive frames obtained by a stereo camera and select only those features that appear to be stable from different views. Whenever a feature is selected, we compute a representative feature given the previous observations. This approach is applied within a Rao-Blackwellized particle filter to make the data association easier and furthermore to reduce the number of landmarks that need to be maintained in the map. Our system has been implemented and tested on data gathered with a mobile robot in a typical office environment. Experiments presented in this paper demonstrate that our method improves the data association and in this way leads to more accurate maps",
Autonomic Provisioning of Backend Databases in Dynamic Content Web Servers,"In autonomic provisioning, a resource manager allocates resources to an application, on-demand, e.g., during load spikes. Modelling-based approaches have proved very successful for provisioning the web and application server tiers in dynamic content servers. On the other hand, accurately modelling the behavior of the back-end database server tier is a daunting task. Hence, automated provisioning of database replicas has received comparatively less attention. This paper introduces a novel pro-active scheme based on the classic K-nearest-neighbors (KNN) machine learning approach for adding database replicas to application allocations in dynamic content web server clusters. Our KNN algorithm uses lightweight monitoring of essential system and application metrics in order to decide how many databases it should allocate to a given workload. Our pro-active algorithm also incorporates awareness of system stabilization periods after adaptation in order to improve prediction accuracy and avoid system oscillations. We compare this pro-active self-configuring scheme for scaling the database tier with a reactive scheme. Our experiments using the industry-standard TPC-W e-commerce benchmark demonstrate that the pro-active scheme is effective in reducing both the frequency and peak level of SLA violations compared to the reactive scheme. Furthermore, by augmenting the pro-active approach with awareness and tracking of system stabilization periods induced by adaptation in our replicated system, we effectively avoid oscillations in resource allocation.",
A Parallel Algorithm for Enumerating All Maximal Cliques in Complex Network,"Efficient enumeration of all maximal cliques in a given graph has many applications in graph theory, data mining and bio informatics. However, the exponentially increasing computation time of this problem confines the scale of the graph. Meanwhile, recent researches show that many networks in our world are complex networks involving massive data. To solve the maximal clique problem in the real-world scenarios, this paper presents a parallel algorithm Peamc (parallel enumeration of all maximal cliques) which exploits several new and effective techniques to enumerate all maximal cliques in a complex network. Furthermore, we provide a performance study on a true-life call graph with up to 2,423,807 vertices and 5,317,183 edges. The experimental results show that Peamc can find all the maximal cliques in a complex network with high efficiency and scalability",
An Adaptive Two-Level Management for the Flash Translation Layer in Embedded Systems,"While the capacity of flash-memory storage systems keeps increasing significantly, effective and efficient management of flash-memory space has become a critical design issue! Different granularities in space management impose different management costs and mapping efficiency. In this paper, we explore an address translation mechanism that can dynamically and adaptively switch between two granularities in the mapping of logical block addresses into physical block addresses in flash memory management. The objective is to provide good performance in address mapping and space utilization and, at the same time, to have the memory space requirements, and the garbage collection overhead under proper management. The experimental results show that the proposed adaptive mechanism could provide significant performance improvement over the well-known coarse-grained management mechanism NFTL (NAND flash translation layer) over realistic workloads",
A Payment-based Incentive and Service Differentiation Mechanism for Peer-to-Peer Streaming Broadcast,"We proposes a novel payment-based incentive mechanism for peer-to-peer (P2P) live media streaming. Using this approach, peers earn points by forwarding data to others; the data streaming is divided into fixed length periods, during each of which peers compete with each other for good parents (data suppliers) for the next period in a first-price auction like procedure using their points. We design a distributed algorithm to regulate peer competitions, and consider various individual strategies for parent selection from a game theoretic perspective. We then discuss possible strategies that can be used to maximize a peer's expected media quality by planning different bids for its substreams. Finally, in order to encourage off-session users to keep staying online and continue contributing to the network, we develop an optimal data forwarding strategy that allows peers to accumulate points that can be used in future services. Simulations results show that proposed methods effectively differentiate the media qualities received by peers making different contributions (which originate from, for example, different forwarding band-widths or servicing times), and at the same time maintaining a high system-wide performance",
Topological Modelling for Human Augmented Mapping,"Service robots designed for domestic settings need to navigate in an environment that they have to share with their users. Thus, they have to be able to report their current state and whereabouts in a way that is comprehensible for the user. Pure metric maps do not usually correspond to the understanding of the environment a user would provide. Thus, the robotic map needs to be integrated with the human representation. This paper describes our framework of human augmented mapping that allows us to achieve this integration. We propose further a method to specify and represent regions that relate to a user's view on the environment. We assume an interactive setup for the specification of regions and show the applicability of our method in terms of distinctiveness for space segmentation and in terms of localisation purposes",
Ear Recognition Based on Statistical Shape Model,"Alfred system suggests that ear shape can be used as a unique and comparable feature for identity recognition. In this paper, we aim to extract ear shape features for recognition. Active shape models (ASMs) is applied to model the shape and local appearance of the ear in a statistical manner. In addition, steerable features are extracted from the ear image ahead of ASMs. Steerable features encode rich discriminant information of the local structural texture and provide accurate guidance for shape location. Eigenearshape is used for final classification. Experiments on an ear database show an encouraging performance. Some experiments on double ears combined for recognition are also carried out, and indicate that the fusion outperforms either ear alone","Ear,
Feature extraction,
Data mining,
Active shape model,
Biometrics,
Image databases,
Spatial databases,
Face recognition,
Computer science,
Image segmentation"
A Table-Driven Streaming XML Parsing Methodology for High-Performance Web Services,"This paper presents a table-driven streaming XML parsing methodology, called TDX. TDX expedites XML parsing by pre-recording the states of an XML parser in tabular form and by utilizing an efficient runtime streaming parsing engine based on a push-down automaton. The parsing tables are automatically produced from the XML schemas of a WSDL service description. Because the schema constraints are pre-encoded in a parsing table, the approach effectively implements a schema-specific XML parsing technique that combines parsing and validation into a single pass. This significantly increases the performance of XML Web services, which results in better response time and may reduce the impact of the flash-crowd effect. To implement TDX, we developed a parser construction toolkit to automatically construct parsers in C code from WSDLs and XML schemas. We applied the toolkit to an example Web services application and measured the raw performance compared to popular high-performance parsers written in C/C++, such as eXpat, gSOAP, and Xerces. The performance results show that TDX can be an order of magnitude faster","XML,
Web services,
Production,
Computer science,
Runtime,
Engines,
Automata,
Delay,
Encoding,
Filters"
Enabling Confidentiality in Content-Based Publish/Subscribe Infrastructures,"Content-based publish/subscribe (CBPS) is an interaction model where the interests of subscribers are stored in a content-based forwarding infrastructure to guide routing of notifications to interested parties. In this paper, we focus on answering the following question: can we implement content-based publish/subscribe while keeping subscriptions and notifications confidential from the forwarding brokers? Our contributions include a systematic analysis of the problem, providing a formal security model and showing that the maximum level of attainable security in this setting is restricted. We focus on enabling provable confidentiality for commonly used applications and subscription languages in CBPS and present a series of practical provably secure protocols, some of which are novel and others adapted from existing work. We have implemented these protocols in Siena, a popular CBPS system. Evaluation results show that confidential content-based publish/subscribe is practical: a single broker serving 1000 subscribers is able to route more than 100 notifications per second with our solutions",
Intrusion Response as a Resource Allocation Problem,"We study intrusion response in access control systems as a resource allocation problem, and address it within a decision and control framework. By modeling the interaction between malicious attacker(s) and the intrusion detection system (IDS) as a noncooperative non-zero sum game, we develop an algorithm for optimal allocation of the system administrator's time available for responding to attacks, which is treated as a scarce resource. This algorithm, referred to as the automatic or administrator response (AOAR) algorithm, applies neural network and LP optimization tools. Finally, we implement an IDS prototype in MATLAB based on a game theoretical framework, and demonstrate its operation under various scenarios with and without the AOAR algorithm. Our approach and the theory developed are general and can be applied to a variety of IDSs and computer networks",
Self-regulating network utilization in mobile ad hoc wireless networks,"In mobile ad hoc wireless LANs, it is very difficult to maintain a targeted network utilization due to the time-varying nature of the contention-based medium access control protocol and the lack of a central control. Furthermore, previous research has been mainly focusing on the aspect of optimizing the performance at each station. But doing so may result in a very low overall network utilization. Therefore, self-regulating network utilization is very important to provide quality-of-service (QoS) in mobile ad hoc wireless networks. Through self-disciplining its own behaviors locally, each station will optimize its protocol parameters to meet the targeted overall network utilization, which is very important for QoS provisioning to multimedia services. This paper proposes and evaluates a fully distributed scheme for each station to self-regulate its behaviors through adapting the local protocol parameters to meet the targeted overall network utilization with the changes in the network environment such as the number of stations and channel quality",
The Effectiveness of Threshold-Based Scheduling Policies in BOINC Projects,"Several scientific projects use BOINC (Berkeley Open Infrastructure for Network Computing) to perform largescale simulations using volunteers' computers (workers) across the Internet. In general, the scheduling of tasks in BOINC uses a First-Come-First-Serve policy and no attention is paid to workers' past performance, such as whether or not they have tended to perform tasks promptly and correctly. In this paper we use SimBA, a discrete-event Simulator of BOINC Applications, to study new threshold-based scheduling strategies for BOINC projects that use availability and reliability metrics to classify workers and distribute tasks according to this classification. We show that if availability and reliability thresholds are selected properly, then the workers' throughput of valid results increases significantly in BOINC projects.",
An Automatic Captioning System for Telemedicine,"In this paper, we present a first exposition of an automatic closed captioning system designed to assist hearing impaired users in telemedicine. This system automatically separates telehealth conversation speech between a health care provider and a client into two streams and provides real-time captions of health care provider's speech to client. The captioning system is based on the state-of-the-art technology of large vocabulary conversational speech recognition, encompassing speech stream separation, acoustic modeling, language modeling, real-time decoding, confidence annotation, and human-computer interface, with innovations made in several components. The system currently handles a vocabulary size over 46 K. Real-time captioning performance at the average word accuracy of 77.95% is reported",
Computationally sound compositional logic for key exchange protocols,"We develop a compositional method for proving cryptographically sound security properties of key exchange protocols, based on a symbolic logic that is interpreted over conventional runs of a protocol against a probabilistic polynomial-time attacker. Since reasoning about an unbounded number of runs of a protocol involves induction-like arguments about properties preserved by each run, we formulate a specification of secure key exchange that is closed under general composition with steps that use the key We present formal proof rules based on this game-based condition, and prove that the proof rules are sound over a computational semantics. The proof system is used to establish security of a standard protocol in the computational model",
Active Learning of Joint Attention,"Joint attention is the skill of attending to the same object another person is looking at. The acquisition of this skill is crucial in children for the development of many social and communicative abilities, and has been proposed as a critical social capability for interactive robots. Although recent attempts to model the acquisition of this skill on a robot have been moderately successful (Nagai et al., 2003; Triesch et al., 2006), they all assume that the robot remains passive during the learning process. Infants, on the other hand, have already acquired some rudimentary sensorimotor skills by the time they start to learn joint attention. We believe that these sensorimotor skills can jumpstart and considerably accelerate the learning of joint attention. In this paper we demonstrate on a humanoid robot how to use pointing and reaching to accelerate the learning of joint attention. We show that a robot can acquire this skill with a 95 % accuracy after a total of only 220 training samples compared to 85% accuracy after totals of 10,000+ samples in other approaches.",
Adaptable Replica Consistency Service for Data Grids,"In this paper, we focus our attentions on the replica consistency problems in data grids. An innovative and effective architecture called adaptable replica consistency service (ARCS) with the capability of dealing with the replica consistency is proposed. It can increase the performances and achieve load balancing for file replications in data grids. The model and experimental simulations are presented and evaluated by means of OptorSim as compared with the other two proposed coherence protocols. ARCS is superior to them as well as being suitable for the data grids environments according to our simulations results",
Latent Friend Mining from Blog Data,"The rapid growth of blog (also known as ""weblog"") data provides a rich resource for social community mining. In this paper, we put forward a novel research problem of mining the latent friends of bloggers based on the contents of their blog entries. Latent friends are defined in this paper as people who share the similar topic distribution in their blogs. These people may not actually know each other, but they have the interest and potential to find each other out. Three approaches are designed for latent friend detection. The first one, called cosine similarity-based method, determines the similarity between bloggers by calculating the cosine similarity between the contents of the blogs. The second approach, known as topic-based method, is based on the discovery of latent topics using a latent topic model and then calculating the similarity at the topic level. The third one is two-level similarity-based, which is conducted in two stages. In the first stage, an existing topic hierarchy is exploited to build a topic distribution for a blogger. Then, in the second stage, a detailed similarity comparison is conducted for bloggers that are close in interest to each other which are discovered in the first stage. Our experimental results show that both the topic-based and two-level similarity-based methods work well, and the last approach performs much better than the first two. In this paper, we give a detailed analysis of the advantages and disadvantages of different approaches.",
Reputation-Based Scheduling on Unreliable Distributed Infrastructures,"This paper presents a design and analysis of scheduling techniques to cope with the inherent unreliability and instability of worker nodes in large-scale donation-based distributed infrastructures such as P2P and Grid systems. In particular, we focus on nodes that execute tasks via donated computational resources and may behave erratically or maliciously. We present a model in which reliability is not a binary property but a statistical one based on a node’s prior performance and behavior. We use this model to construct several reputation-based scheduling algorithms that employ estimated reliability ratings of worker nodes for efficient task allocation. Through simulation of a BOINC-like distributed computing infrastructure, we demonstrate that our algorithms can significantly improve throughput, while maintaining a very high success rate of task completion.",
Combining Model-based and Genetics-based Offspring Generation for Multi-objective Optimization Using a Convergence Criterion,"In our previous work conducted by Aimin Zhou et. al., (2005), it has been shown that the performance of multi-objective evolutionary algorithms can be greatly enhanced if the regularity in the distribution of Pareto-optimal solutions is used. This paper suggests a new hybrid multi-objective evolutionary algorithm by introducing a convergence based criterion to determine when the model-based method and when the genetics-based method should be used to generate offspring in each generation. The basic idea is that the genetics-based method, i.e., crossover and mutation, should be used when the population is far away from the Pareto front and no obvious regularity in population distribution can be observed. When the population moves towards the Pareto front, the distribution of the individuals will show increasing regularity and in this case, the model-based method should be used to generate offspring. The proposed hybrid method is verified on widely used test problems and our simulation results show that the method is effective in achieving Pareto-optimal solutions compared to two state-of-the-art evolutionary multi-objective algorithms: NSGA-II and SPEA2, and our pervious method in Aimin Zhou et. al., (2005).",
The coordination of multiple autonomous systems using information theoretic political science voting models,"In this paper we introduce an approach to solving a coordination problem involving multiple heterogenous autonomous systems operating in a multi-objective mission domain. The distributed coordination mechanism presented applies an information theoretic political science voting model to the dynamic weighted aggregation method, thereby allowing the decision makers (the autonomous systems), to influence the weights assigned to each mission objective. Results from a simulated instantiation of a multiple objective mission are presented, demonstrating the use of the voting mechanism to coordinate actions undertaken by the autonomous systems. The results show a level of overall mission success comparable to that obtained in an 'ideal' centralised mechanism",
Block Mean Value Based Image Perceptual Hashing,"Image perceptual hashing has been proposed to identify or authenticate image contents in a robust way against distortions caused by compression, noise, common signal processing and geometrical modifications, while still holding a good discriminability for different ones in sense of human perception. We propose and compare four normalized block mean value based image perceptual hashing algorithms which demonstrate higher performances than other existing algorithms in robustness-anddiscriminalibility and simplicity for implementation. Overlapped blocking and rotation operations are employed to enhance the robustness to geometrical distortions. To evaluate the proposed algorithms¿ robustness and discriminability, given fixed modifications, identification ratio is used; and given fixed content classification, receiver operating curves is obtained.",
Design and Evaluation of a PLL-Based Position Controller for Sensorless Vector Control of Permanent-Magnet Synchronous Machines,"This paper presents a rotor position controller for sensorless vector control of permanent magnet synchronous machines (PMSM) based on a synchronous d-q frame phase-locked loop (PLL). Extending the capabilities of PLL's for grid-connected converters-using feedback linearization to ensure a constant dynamic response regardless of the operating region and rotor speed, the proposed controller takes advantage of the sinusoidal and balanced PMSM back-EMF voltages synchronizing the estimated and actual d-q frames. The resultant controller structure is readily linearized providing an accurate design tool based on frequency response specifications. A complete design procedure is provided, together with simulation and experimental results with a 3.5 kW PMSM drive, all of which verify the excellent results attained by the proposed PLL-based position controller.",
Toward a Seamless Communication Architecture for In-building Networks at the 60 GHz band,"This paper addresses the issues of designing an infrastructure to support seamless in-building communication at the 60 GHz band. Recently, the 60 GHz band has received much attention due to its 5 GHz of available spectrum. However, the propagation of signals in this band is strongly hindered by attenuation and line-of-sight requirements. The situation gets worse in the in-building environment where signal propagation is obstructed by physical objects such as walls, furniture etc. In this paper, we present a novel radio over fiber (RoF) architecture that is cost-effective and is able to deliver high data-rate of the order of gigabits. To ensure a seamless communication environment at the 60 GHz band, we propose the concept of extended cells (EC) in order to create sufficient overlap areas. Finally, we illustrate the effectiveness of the proposed architecture by simulating an in-building network at the 60 GHz band employing the RoF and EC concepts",
Image-Segmentation Evaluation From the Perspective of Salient Object Extraction,"Image segmentation and its performance evaluation are very difficult but important problems in computer vision. A major challenge in segmentation evaluation comes from the fundamental conflict between generality and objectivity: For general-purpose segmentation, the ground truth and segmentation accuracy may not be well defined, while embedding the evaluation in a specific application, the evaluation results may not be extended to other applications. We present in this paper a new benchmark for evaluating image segmentation. Specifically, we formulate image segmentation as identifying the single most perceptually salient structure from an image. We collect a large variety of test images that conforms to this specific formulation, construct unambiguous ground truth for each image, and define a reliable way to measure the segmentation accuracy. We then present two special strategies to further address two important issues: (a) the most salient structures in some real images may not be unique or unambiguously defined, and (b) many available image-segmentation methods are not developed to directly extract a single salient structure. Finally, we apply this benchmark to evaluate and compare the performance of several state-of-the-art image-segmentation methods, including the normalized-cut method, the level-set method, the efficient graph-based method, the mean-shift method, and the ratio-contour method.",
GENI Design Principles,"The Global Environment for Network Innovations is a major planned initiative of the US National Science Foundation to build an open, large-scale, realistic experimental facility for evaluating new network architectures. The facility's goal is to change the way we design networked and distributed systems, creating over time new paradigms that integrate rigorous theoretical understanding with compelling and thorough experimental validation. The research that GENI enables can lead to a future Internet that is more secure, available, manageable, and efficient, and better at handling mobile nodes. GENI is intended to support two general kinds of activities: running controlled experiments to evaluate design, implementation, and engineering choices; and deploying prototype systems and learning from observations of how they behave under real usage",
A Candidate Fault Model for AspectJ Pointcuts,"We present a candidate fault model for pointcuts in AspectJ programs. The fault model identifies faults that we believe are likely to occur when writing pointcuts in the AspectJ language. Categories of fault types are identified, and each individual fault type is described as categorized. We argue that a fault model that focuses on the unique constructs of the AspectJ language is needed for the systematic and effective testing of AspectJ programs. Our pointcut fault model is a first step towards such a model",
Workers' Routine Activity Recognition using Body Movements and Location Information,We propose a method for recognizing workers' routine activities by combining location information and body movements of a user. We describe methods for capturing and recognizing the nursing context with an infrared-ID location sensor system and accelerometers worn by a user. We show experimental results of recognizing typical activities in a nursing scenario in a laboratory setting.,
Palmprint Texture Analysis Using Derivative of Gaussian Filters,"This paper presents a novel approach of palmprint texture analysis based on the derivative of Gaussian filter. In this approach, the palmprint image is respectively preprocessed along horizontal and vertical direction using derivative of Gaussian (DoG) filters. And then the palmprint is encoded according to the sign of the value of each pixel of the filtered images. This code is called DoGCode of the palmprint. The size of DoGCode is 256 bytes. The similarity of two DoGCode is measured using their Hamming distance. This approach is tested on the PolyU Palmprint Database, which containing 7605 samples from 392 palms, and the EER is 0.19%, which is comparable with the existing palmprint recognition methods",
The Clear-PEM Electronics System,"The Clear-PEM detector system is a compact positron emission mammography scanner with about 12000 channels aiming at high sensitivity and good spatial resolution. Front-end, Trigger, and Data Acquisition electronics are crucial components of this system. The on-detector front-end is implemented as a data-driven synchronous system that identifies and selects the analog signals whose energy is above a predefined threshold. The off-detector trigger logic uses digitized front-end data streams to compute pulse amplitudes and timing. Based on this information it generates a coincidence trigger signal that is used to initiate the conditioning and transfer of the relevant data to the data acquisition computer. To minimize dead-time, the data acquisition electronics makes extensive use of pipeline processing structures and derandomizer memories with multievent capacity. The system operates at 100-MHz clock frequency, and is capable of sustaining a data acquisition rate of 1 million events per second with an efficiency above 95%, at a total single photon background rate of 10 MHz. The basic component of the front-end system is a low-noise amplifier-multiplexer chip presently under development. The off-detector system is designed around a dual-bus crate backplane for fast intercommunication between the system boards. The trigger and data acquisition logic is implemented in large FPGAs with 4 million gates. Monte Carlo simulation results evaluating the trigger performance, as well as results of hardware simulations are presented, showing the correctness of the design and the implementation approach",
A Systematic Approach to Generate Inputs to Test UML Design Models,"Practical model validation techniques are needed for model driven development (MDD) techniques to succeed. This paper presents an approach to generating inputs to test UML design models that are produced in the detailed design phase of an MDD project. A symbolic execution based approach is used to derive test input constraints from the paths of a variable assignment graph, which integrates information from UML class and sequence diagrams. The constraints are solved using Alloy, a configuration constraint solver, to obtain the test inputs. The results of a pilot study carried out to explore the fault detection capability of the test inputs are reported",
Recent developments in computer simulations of language competition,"A common property of many physicist-led simulations conducted during the past 50 years is that they deal with individuals (atoms, humans, and so on). This paper summarizes a trend in physics triggered by Daniel Abrams and Steven Strogatz (2003) computer simulations of language competition. The world has thousands of living languages, as well as several dying languages that have just one surviving speaker. Can we explain this language-size distribution ns with simple models? Will we all eventually speak the same language and its dialects (Sutherland, 2003; Stauffer et al., 2006)? Here, we summarize several models and present variants of our own language model in greater detail (Schulze and Stauffer, 2005; Stauffer and Schulze, 2005)",
Coordination Mechanism in Wireless Sensor and Actor Networks,"Wireless sensor and actor networks (WSANs) are composed of sensors and actors to perform distributed sensing and acting tasks. To accomplish effective sensing and acting tasks, efficient coordination mechanisms of sensor-sensor, sensor-actor and actor-actor are required. In this paper, a novel three-level coordination model for WSANs is proposed, based on a hierarchical geographical clustering paradigm in which cluster formation is formed by dividing the action area into fixed zones to form a virtual grid, so that the working area and workload can be optimally split up into different actors. In this model, sensors are stationary and location-aware, whereas actors may change their locations dynamically. In different levels, sensor-sensor, sensor-actor, actor-actor coordination mechanisms are addressed respectively. The functions of each level are loose coupling with each other so that many different routing and communication protocols can be adopted. Case study shows our model is effective",
RT(Robot Technology)-Component and its Standardization - Towards Component Based Networked Robot Systems Development,"In this paper, we propose RT-component architecture for robot system integration. We have studied modularization of RT (Robot Technology) elements and have developed RT-Middleware, which promotes application of RT in various field. RT-Middleware is a software platform for RT systems and provides RT specific functionality for the component based system development. The component which constructs RT-systems in RT-Middleware is called the RT-Component We are also standardizing the architecture of RT-component in the Object Management Group (OMG). Finally conclusion and future work will be described",
3-D reconstruction of tissue components for atherosclerotic human arteries using ex vivo high-resolution MRI,"Automatic computer-based methods are well suited for the image analysis of the different components in atherosclerotic plaques. Although several groups work on such analysis some of the methods used are oversimplified and require improvements when used within a computational framework for predicting meaningful stress and strain distributions in the heterogeneous arterial wall under various loading conditions. Based on high-resolution magnetic resonance imaging of excised atherosclerotic human arteries and a series of two-dimensional (2-D) contours we present a segmentation tool that permits a three-dimensional (3-D) reconstruction of the most important tissue components of atherosclerotic arteries. The underlying principle of the proposed approach is a model-based snake algorithm for identifying 2-D contours, which uses information about the plaque composition and geometric data of the tissue layers. Validation of the computer-generated tissue boundaries is performed with 100 MR images, which are compared with the results of a manual segmentation performed by four experts. Based on the Hausdorff distance and the average distance for computer-to-expert differences and the interexpert differences for the outer boundary of the adventitia, the adventitia-media, media-intima, intima-lumen and calcification boundaries are less than 1 pixel (0.234 mm). The percentage statistic shows similar results to the modified Williams index in terms of accuracy. Except for the identification of lipid-rich regions the proposed algorithm is automatic. The nonuniform rational B-spline-based computer-generated 3-D models of the individual tissue components provide a basis for clinical and computational analysis.",
Playing a different imitation game: Interaction with an Empathic Android Robot,"Current research has identified the need to equip robots with perceptual capabilities that not only recognise objective entities such as visual or auditory objects but that are also capable of assessing the affective evaluations of the human communication partner in order to render the communication situation more natural and social. In equivalence to Watzlawick's statement that ""one cannot not communicate"" (1968) it has been found that also in human-robot interactions one cannot be not emotional. It is therefore crucial for a robot to understand these affective signals of its communication partner and react towards them. However, up to now, online emotion recognition in realtime, interactive systems has scarcely been attempted as apparently demands concerning robustness and time constraints are very high. In this paper we present an empathic anthropomorphic robot (torso) that mirrors the emotions happiness, fear and neutral as recognised from the speech signal by facial expressions. The recognition component as well as the development of the facial expression generation are described in detail. We report on results from experiments with humans interacting with the empathic robot",
Designing Urban Pervasive Systems,"A conceptual framework describes three aspects of designing and analyzing pervasive systems in an urban environment. A systematic approach to designing the urban environment as an integrated system of architecture and pervasive technologies requires drawing on knowledge, theory, and methods from the disciplines of architecture and computer science. Key to this interdisciplinary integration is the concept of space, by which we mean not only physical location or volume but also the social protocols, conventions, and values attached to a particular space",
Evaluation of the XMesh Routing Protocol in Wireless Sensor Networks,"The evaluation of a routing protocol developed by Crossbow Technologies called XMesh is presented. The main components of the routing protocol are described and the routing algorithm explained. Experiments were conducted to determine the connectivity ranges of motes in different transmission power settings. The relationship of mote transmission power and network connectivity is presented. An energy efficiency study looked at the means of extending the lifespan of the network. Although, packet losses during the period of a node failure were significant, the routing protocol showed that it was able to adapt and reorganize to provide reliable and stable routing in a network.",
A Fuzzy Leader-Follower Approach to Formation Control of Multiple Mobile Robots,"This paper presents a fuzzy logic based system for formation control of multiple mobile robots. Two main problems of formation control are investigated - maintaining correct formation position and inter-formation collision avoidance. A leader-follower approach with minimal communication between robots is presented here. Separate fuzzy logic controllers are developed for formation position control and internal collision avoidance with a higher level fuzzy coordinator used to fuse their outputs. A major issue in formation control is the presence of uncertainties in the real world, in the form of noisy sensor data and delay in leader position transmission. Noise is added to the simulated data to prove that the system is capable of tolerating such disturbance. The system has the ability to create any desired geometrical formation shape",
Exploring Intentional Modeling and Analysis for Enterprise Architecture,"An enterprise architecture is intended to be a comprehensive blueprint describing the key components and relationships for an enterprise from strategies to business processes to information systems and technologies. Enterprise architectures have become essential for managing change in complex organizations. While ""motivation"" has been recognized since Zachman 0 as an important element of enterprise architecture, yet to date, most enterprise architecture modeling only deals with structure, function, and behaviour, neglecting the intentional dimension of motivations, rationales, and goals. The contribution at hand explores this challenge and aims to illustrate the potentials of intentional modeling in the context of enterprise architecture. After introducing two intentional modeling languages and their potential relation to an enterprise architecture construction process, we report on an explorative case study that aimed to investigate the practical implications of intentional modeling and analysis for enterprise architectures. Finally, we present key observations from interviews that were conducted with practitioners to obtain feedback regarding the material developed in the case study.",
"Workflow discovery: the problem, a case study from e-Science and a graph-based solution","Much has been written on the promise of Web service discovery and (semi-) automated composition. In this discussion, the value to practitioners of discovering and reusing existing service compositions, captured in workflows, is mostly ignored. This paper presents one solution to workflow discovery. Through a survey with 21 scientists and developers from the myGrid workflow environment, workflow discovery requirements are elicited. Through a user experiment with 13 scientists, an attempt is made to build a gold standard for workflow ranking. Through the design and implementation of a workflow discovery tool, a mechanism for ranking workflow fragments is provided based on graph sub-isomorphism matching. The tool evaluation, drawing on a corpus of 89 public workflows from bioinformatics and the results of the user experiment, finds that the average human ranking can largely be reproduced","Web services,
Bioinformatics,
Information management,
Computer science,
Humans,
Chemistry,
Gold,
Markup languages,
High level languages,
Simple object access protocol"
ID-Binary Tree Stack Anticollision Algorithm for RFID,"This paper presents an efficient anticollision algorithm for the RFID tags communication conflict. The novelty of our algorithm is that we map a set of n tags into a corresponding IDbinary tree, and see the process of collision arbitration as a process of building the ID-binary tree. In order to efficiently construct an ID-binary tree, the reader uses a stack to store the threads of the construction information, and while the tag uses a counter to keep track of the stack position where the tag is on. Theoretic results in both the number of queries sent by the reader and the tags communication complexity are derived to demonstrate the efficiency of our algorithm.",
Workload sanitation for performance evaluation,"The performance of computer systems depends, among other things, on the workload. Performance evaluations are therefore often done using logs of workloads on current productions systems, under the assumption that such real workloads are representative and reliable; likewise, workload modeling is typically based on real workloads. We show, however, that real workloads may also contain anomalies that make them non-representative and unreliable. This is a special case of multi-class workloads, where one class is the ""real"" workload which we wish to use in the evaluation, and the other class contaminates the log with ""bogus"" data. We provide several examples of this situation, including a previously unrecognized type of anomaly we call ""workload flurries"": surges of activity with a repetitive nature, caused by a single user, that dominate the workload for a relatively short period. Using a workload with such anomalies in effect emphasizes rare and unique events (e.g. occurring for a few days out of two years of logged data), and risks optimizing the design decision for the anomalous workload at the expense of the normal workload. Thus we claim that such anomalies should be removed from the workload before it is used in evaluations, and that ignoring them is actually an unjustifiable approach.",
Process Variation Aware Cache Leakage Management,"In a few technology generations, limitations of fabrication processes have made accurate design time power estimates a daunting challenge. Static leakage current which comprises a significant fraction of total power due to large on-chip caches, is exponentially dependent on widely varying physical parameters such as gate length, gate oxide thickness, and dopant ion concentration. In large structures like on-chip caches, this may mean that one portion of a cache may consume an order of magnitude larger static power than equivalently sized regions. Under this climate, egalitarian management of physical resources is clearly untenable. In this paper, we analyze the effects of within-die and die-to-die leakage variation for on-chip caches. We then propose way prioritization, a manufacturing variation aware scheme that minimizes cache leakage energy. Our results show that significant average power reductions are possible without undue hardware complexity or performance compromise",
"GRENCHMARK: A Framework for Analyzing, Testing, and Comparing Grids","Grid computing is becoming the natural way to aggregate and share large sets of heterogeneous resources. With the infrastructure becoming ready for the challenge, current grid development and acceptance hinge on proving that grids reliably support real applications, and on creating adequate benchmarks to quantify this support. However, grid applications are just beginning to emerge, and traditional benchmarks have yet to prove representative in grid environments. To address this chicken-and-egg problem, we propose a middle-way approach: create and run synthetic grid workloads comprising applications representative for today’s grids. For this purpose, we have designed and implemented GRENCHMARK, a framework for synthetic workload generation and submission. The framework greatly facilitates synthetic workload modeling, comes with over 35 synthetic and real applications, and is extensible and flexible. We show how the framework can be used for grid system analysis, functionality testing in grid environments, and for comparing different grid settings, and present the results obtained with GRENCHMARK in our multi-cluster grid, the DAS",
A Study on Automatic Recognition of Road Signs,"An automatic road sign recognition system identifies road signs from within images captured by an imaging sensor on-board of a vehicle, and assists the driver to properly operate the vehicle. Most existing systems include a detection phase and a classification phase. This paper classifies the methods applied to road sign recognition into three groups: colour-based, shape-based, and others. In this paper, the issues associated with automatic road sign recognition are addressed, the popular existing methods developed to tackle the road sign recognition problem are reviewed, and a comparison of the features of these methods is given",
Real-time lane detection in various conditions and night cases,"In this paper, we propose a real-time lane detection algorithm based on a hyperbola-pair lane boundary model and an improved RANSAC paradigm. Instead of modeling each road boundary separately, we propose a model to describe the road boundary as a pair of parallel hyperbolas on the ground plane. A fuzzy measurement is introduced into the RANSAC paradigm to improve the accuracy and robustness of fitting the points on the boundaries into the model. Our method is able to deal with existence of partial occlusion, other traffic participants and markings, etc. Experiment in many different conditions, including various conditions of illumination, weather and road, demonstrates its high performance and accuracy",
Design and Evaluation of a Wearable Optical Sensor for Monitoring Seated Spinal Posture,"This work describes the development and evaluation of a wearable plastic optical fiber (POF) sensor for monitoring seated spinal posture. A garment-integrated POF sensor was developed and tested on nine healthy subjects, and its performance compared to data taken simultaneously from a marker-based motion capture system. Sensor performance correlated strongly with motion-capture data with an average r of 0.913. Results show that the wearable sensor provides enough accuracy of measurement to reliably monitor seated spinal posture.",
A decoupled KILO-instruction processor,"Building processors with large instruction windows has been proposed as a mechanism for overcoming the memory wall, but finding a feasible and implementable design has been an elusive goal. Traditional processors are composed of structures that do not scale to large instruction windows because of timing and power constraints. However, the behavior of programs executed with large instruction windows gives rise to a natural and simple alternative to scaling. We characterize this phenomenon of execution locality and propose a microarchitecture to exploit it to achieve the benefit of a large instruction window processor with low implementation cost. Execution locality is the tendency of instructions to exhibit high or low latency based on their dependence on memory operations. In this paper we propose a decoupled microarchitecture that executes low latency instructions on a cache processor and high latency instructions on a memory processor. We demonstrate that such a design, using small structures and many in-order components, can achieve the same performance as much more aggressive proposals while minimizing design complexity.",
Consideration of stiffness and mass effects of relatively thicker electrodes with Mindlin plate theory,"Mindlin plate theory has been widely used in the high-frequency vibrations of piezoelectric crystal plates with emphasis on its applications in crystal resonator analysis and design. The plate equations were derived without considering the effect of electrodes from the beginning. But continuing efforts have been made to include the mechanical effect, or the mass loading, through the consideration of the mass ratio of the electrodes and crystal blank. Such a consideration has been effective for relatively thin electrodes before, but the ever-increasing mass ratio has been pressing further improvement to take into account relatively thicker electrodes. To extend Mindlin plate equations for these applications, we derive the plate equations systematically with the approximation of displacements in electrodes with those in the crystal blank. As a result, both mass and stiffness effects of electrodes are considered through ratios of the thickness, density, and elastic constants of the electrodes to those of the crystal blank, respectively, and the plate equations are modified accordingly. A practical design of the electrodes and crystal blank are analyzed to demonstrate the necessity of such modifications to Mindlin plate equations.",
Large System Performance of Downlink OFDMA with Limited Feedback,"We consider allocation of sub-channels to users in a downlink OFDMA system. Each user feeds back one bit per sub-channel, which indicates whether or not the gain exceeds a threshold. Users are assigned priority weights, and the thresholds are selected to maximize the weighted sum capacity. We analyze the behavior of the optimal thresholds and growth in capacity, assuming i.i.d. Rayleigh fading sub-channels, in the large system limit in which users K and sub-channels tend to infinity with fixed ratio. If all users have the same priority weight, then the optimized threshold increases as log K minus a second-order term, which is asymptotically bounded between log log K and log log log K. Furthermore, the sum capacity per sub-channel increases as log log K plus a second-order term, which decreases to a constant as log log K/ log K. We then consider two classes of users, each assigned a different weight, and show that the capacity of the low priority group tends to zero. Finally, we solve for the optimal thresholds given a fairness constraint on the ratio between the rates of different classes","System performance,
Downlink,
Feedback,
Transmitters,
H infinity control,
Feeds,
Rayleigh channels,
Interference constraints,
Frequency conversion,
Data systems"
Towards Less Supervision in Activity Recognition from Wearable Sensors,"Activity Recognition has gained a lot of interest in recent years due to its potential and usefulness for context-aware wearable computing. However, most approaches for activity recognition rely on supervised learning techniques lim iting their applicability in real-world scenarios and their scalability to large amounts of activities and training data. State-of-the-art activity recognition algorithms can roughly be divided in two groups concerning the choice of the classifier, one group using generative models and the other discriminative approaches. This paper presents a method for activity recognition which combines a generative model with a discriminative classifier in an integrated approach. The generative part of the algorithm allows to extract and learn structure in activity data without any labeling or supervision. The discriminant part then uses a small but labeled subset of the training data to train a discriminant classifier. In experiments we show that this scheme enables to attain high recognition rates even though only a subset of the training data is used for training. Also the tradeoff between labeling effort and recognition performance is analyzed and discussed.",
Generating hardware from OpenMP programs,"Various high level hardware description languages have been invented for the purpose of improving the productivity in the generation of customized hardware. Most of these languages are variants, usually parallel versions, of popular software programming languages. In this paper, we describe our effort to generate hardware from OpenMP, a software parallel programming paradigm that is widely used and tested. We are able to generate FPGA hardware from OpenMP C programs via synthesizable VHDL and Handel-C. We believe that the addition of this medium-grain parallel programming paradigm will bring additional value to the repertoire of hardware description languages",
Indoor Localization Using Camera Phones,"There has been a shift in the focus of indoor localization research from improving accuracy to minimizing infrastructure requirements. The reason is well understood: since location information only serves as a parameter to location-based services, the cost of deploying localization systems should be a minute fraction of the total cost of provisioning location-based services. We demonstrate the possibility of determining user's location indoors based on what the camera-phone ""sees""",
On opportunistic codes and broadcast codes with degraded message sets,"Diversity embedded codes are opportunistic codes which take advantage of good channel realizations while ensuring at least part of the information is received reliably for bad channels. We establish a connection between these codes and degraded message set broadcast codes. We characterize the achievable rate region for the parallel Gaussian degraded message set broadcast problem, when only the strongest user needs the private information. Using this, we partially characterize the set of achievable rate-diversity tuples for the diversity embedded problem for parallel fading channels.",
Building a Remote Supervisory Control Network System for Smart Home Applications,"Wireless sensor networks are often found in the fields of home security, industrial control and maintenance, medical assistance and traffic monitoring and the appearance of ZigBee/IEEE 802.15.4 indicates a network system which is highly reliable, cost-effective, low power consumption, programmable and fast establishing. Currently, many of the wireless sensor network systems are now using ZigBee to implement the designs. A smart sensor network is the infrastructure of home automation and supervisory control systems, as the proposed functions such as intelligence entrance guards management, home security, environmental monitor and light control can be implemented by a smart network integrating with multimedia access service, image processing, security, and sensor and control technologies. Details of the development process of a smart home network in Taiwan based on ZigBee technology with the combination of smart home appliance communication protocol, SAANet, will be given in this article.",
Mediating Group Dynamics through Tabletop Interface Design,"A Stanford University research group explores how design alternatives for tabletop interfaces can impact group dynamics to promote effective teamwork. They built and evaluated a series of novel prototypes that explore multi-user coordination policies and cooperative gesturing, encouraging equitable participation in educational tasks and supporting social skills development for special-needs populations","Protocols,
Electric breakdown,
Displays,
Voting,
Collaborative software,
Collaborative work,
Magnetohydrodynamic power generation,
Mediation,
Accidents,
Permission"
Dynamic Power Management Using Machine Learning,"Dynamic power management (DPM) work proposed to date places inactive components into low power states using a single DPM policy. In contrast, we instead dynamically select among a set of DPM policies with a machine learning algorithm. We leverage the fact that different policies outperform each other under different workloads and devices. Our algorithm adapts to changes in workloads and guarantees quick convergence to the best performing policy for each workload. We performed experiments with a policy set representing state of the art DPM policies on a hard disk drive and a WLAN card. Our results show that our algorithm adapts really well with changing device and workload characteristics and achieves an overall performance comparable to the best performing policy at any point of time",
LOD Map - A Visual Interface for Navigating Multiresolution Volume Visualization,"In multiresolution volume visualization, a visual representation of level-of-detail (LOD) quality is important for us to examine, compare, and validate different LOD selection algorithms. While traditional methods rely on ultimate images for quality measurement, we introduce the LOD map - an alternative representation of LOD quality and a visual interface for navigating multiresolution data exploration. Our measure for LOD quality is based on the formulation of entropy from information theory. The measure takes into account the distortion and contribution of multiresolution data blocks. A LOD map is generated through the mapping of key LOD ingredients to a treemap representation. The ordered treemap layout is used for relative stable update of the LOD map when the view or LOD changes. This visual interface not only indicates the quality of LODs in an intuitive way, but also provides immediate suggestions for possible LOD improvement through visually-striking features. It also allows us to compare different views and perform rendering budget control. A set of interactive techniques is proposed to make the LOD adjustment a simple and easy task. We demonstrate the effectiveness and efficiency of our approach on large scientific and medical data sets","Navigation,
Data visualization,
Distortion measurement,
Rendering (computer graphics),
Signal resolution,
Image resolution,
Chaos,
Entropy,
Information theory,
Biomedical imaging"
Guaranteeing Performance Yield in High-Level Synthesis,"Meeting timing constraint is one of the most important issues for modern design automation tools. This situation is exacerbated with the existence of process variation. Current high-level synthesis tools, performing task scheduling, resource allocation and binding, may result in unexpected performance discrepancy due to the ignorance of the impact of process variation, which requires a shift in the design paradigm, from today's deterministic design to statistical or probabilistic design. In this paper, we present a variation-aware performance yield-guaranteed high-level synthesis algorithm. The proposed approach integrates high-level synthesis and statistical static timing analysis into a simulated annealing engine to simultaneously explore solution space while meeting design objectives. Our results show that the area reduction is in the average of 14% when 95% performance yield is imposed with the same total completion time constraint",
Design and Evaluation of Navigation Techniques for Multiscale Virtual Environments,"The design of virtual environments for applications that have several levels of scale has not been deeply addressed. In particular, navigation in such environments is a significant problem. This paper describes the design and evaluation of two navigation techniques for multiscale virtual environments (MSVEs). Issues such as spatial orientation and understanding were addressed in the design process of the navigation techniques. The evaluation of the techniques was done with two experimental and two control groups. The results show that the techniques we designed were significantly better than the control conditions with respect to the time for task completion and accuracy.","Navigation,
Virtual environment,
Computer graphics,
Computer science,
Application software,
Planets,
Lungs,
Process design,
Chromium,
User interfaces"
Performance of a high sensitivity PET scanner based on LSO panel detectors,"We present the results of performance measurements for a new rotating panel detector research tomograph. The tomograph consists of five large-area lutetium oxyorthosilicate (LSO) panel detectors. A long axial field of view (53 cm), high sensitivity, and high count rate capability (due to recently developed fast electronics) are the most important features of this design. A peak noise equivalent count rate (NEC) of about 184 kcps has been measured, together with 2% absolute sensitivity for a 70 cm /sup 18/F line source and a spatial resolution less than 5 mm. Whole body patient images have been obtained, in two bed positions, 10 minutes per bed. An Ordinary Poisson Ordered Subset Expectation Maximization 3D algorithm (OP-OSEM3D) has been implemented on a dedicated computer cluster for reconstruction.",
Discovery of Collocation Episodes in Spatiotemporal Data,"Given a collection of trajectories of moving objects with different types (e.g., pumas, deers, vultures, etc.), we introduce the problem of discovering collocation episodes in them (e.g., if a puma is moving near a deer, then a vulture is also going to move close to the same deer with high probability within the next 3 minutes). Collocation episodes catch the inter-movement regularities among different types of objects. We formally define the problem of mining collocation episodes and propose two scaleable algorithms for its efficient solution. We empirically evaluate the performance of the proposed methods using synthetically generated data that emulate real-world object movements.",
A greedy strategy for tracking a locally predictable target among obstacles,"Target tracking among obstacles is an interesting class of motion planning problems that combine the usual motion constraints with robot sensors' visibility constraints. In this paper, we introduce the notion of vantage time and use it to formulate a risk function that evaluates the robot's advantage in maintaining the visibility constraint against the target. Local minimization of the risk function leads to a greedy tracking strategy. We also use simple velocity prediction on the target to further improve tracking performance. We compared our new strategy with earlier work in extensive simulation experiments and obtained much improved results",
A Power-Aware Broadcasting Algorithm,"Flooding is an expensive operation that is often required in the operation of mobile ad hoc networks (MANETs). In this paper, we present a novel algorithm to reduce the overhead imposed by flooding operations. The algorithm improves previous results by using a distributed function to elect the nodes that will provide the highest additional coverage to previous retransmissions. The algorithm does not require any signalling or impose special requirements on the participating devices",
A first draft of a Model-driven Method for Designing Graphical User Interfaces of Rich Internet Applications,"The design and development of graphical user interfaces for rich Internet applications are well known difficult tasks with tools. The designers must be aware of the computing platform, the user's characteristics (education, social background, among others) and the environment within users must interact with the application. We present a method to design this type of user interfaces that is model-based and applies an iterative series of XSLT transformations to translate the abstract modeled interface into a final user interface that is coded in a specific platform. In order to avoid the proprietary engines dependency for designing tasks. UsiXML is used to model all the levels. Several model based technologies have been proposed and in this paper we review a XML-compliant user interface description language: XAML",
Design of a WCET-Aware C Compiler,"This paper presents techniques to integrate worst-case execution time (WCET) data into a compiler. Currently, a tight integration of WCET into compilers is strongly desired, but only some ad-hoc approaches were reported currently. Previous work mainly used self-written WCET estimators with limited functionality and preciseness during compilation. A very tight integration of a high quality WCET analyzer into a compiler was not yet achieved. This work is the first to present such a tight coupling between a compiler and the WCET analyzer aiT. This is done by automatically translating the assembly-like contents of the compiler's low-level format (LLIR) to aiT's exchange format CRL2. Additionally, the results produced by aiT are automatically collected and re-imported into the compiler infrastructure. The work described in this paper is smoothly integrated into a C compiler for the Infineon TriCore processor. It opens up new possibilities for the design of WCET-aware optimizations in the future. The concepts for extending the compiler structure are kept very general so that they are not limited to WCET information. Rather, it is possible to use our concepts also for multi-objective optimization of e. g. best-case execution time (BCET) or energy dissipation",
Task Scheduling using Parallel Genetic Simulated Annealing Algorithm,"Task scheduling is a NP-hard problem and is an integral part of parallel and distributed computing. This paper combined with the advantages of genetic algorithm and simulated annealing, brings forward a parallel genetic simulated annealing algorithm and applied to solve task scheduling in grid computing. It first generates a new group of individuals through genetic operation such as reproduction, crossover, mutation, etc, and than simulated anneals independently all the generated individuals respectively. When the temperature in the process of cooling no longer falls, the result is the optimal solution on the whole. From the analysis and experiment result, it is concluded that this algorithm is superior to genetic algorithm and simulated annealing",
Adaptive Position Update in Geographic Routing,"In geographic routing, nodes need to maintain up-to-date positions of their immediate neighbours for making effective forwarding decisions. Periodic broadcasting of beacon packets that contain the geographic location coordinates of the nodes is a popular method used by most geographic routing protocols to maintain neighbour positions. We contend that periodic beaconing regardless of network mobility and traffic pattern does not make optimal ulilisation of the wireless medium and node energy. For example, if the beacon interval is too small compared to the rate at which a node changes its current position, periodic beaconing will create many redundant position updates. Similarly, when only a few nodes in a large network are involved in data forwarding, resources spent by all other nodes in maintaining their neighbour positions are greatly wasted. To address these problems, we propose the Adaptive Position Update (APU) strategy for geographic routing. Based on mobility prediction, APU enables nodes to update their position adaptively to the node mobility and traffic pattern. We embed APU into the well known Greedy Perimeter Stateless Routing Protocol (GPSR), and compare it with original GPSR in the ns-2 simulation platform. We conducted several experiments with randomly generated network topologies and mobility patterns. The results confirm that APU significantly reduces beacon overhead without having any noticeable impact on the data throughput of the network. This result is further validated through a trace driven simulation of a practical vehicular ad-hoc network topology that exhibits realistic movement patterns of public transport buses in a metropolitan city.",
Software Acoustic Modems for Short Range Mote-based Underwater Sensor Networks,"Most recent work in underwater network development has relied on using expensive commercial acoustic modems or on building custom transceivers for each application to establish acoustic communication links among the sensors. Using commercial modems or designing custom hardware may require prohibitive monetary or time investment for many applications. Our work proposes the design of software acoustic modems that can utilize built-in microphones and speakers on the relatively cheap Tmote Invent platforms. The built-in Tmote hardware and the software modem enable acoustic communication in a short range shallow water network. In this paper, we present the initial design and architecture of our acoustic communication system which targets environmental monitoring applications. Our experiments with generic acoustic hardware to profile this underwater communication channel reveal that the channel favors frequencies below 3 Khz, a result which guides the design choices for our FSK software modem. We perform experiments with our software modem/generic hardware system to explore the system's data transfer capability. The data communications experiments confirm the system's capability of transferring information in the order of tens of bits per second for a communications range of up to 10 meters.",
Evolutionary Image Synthesis Using a Model of Aesthetics,"The automatic synthesis of aesthetically pleasing images is investigated. Genetic programming with multi-objective fitness evaluation is used to evolve procedural texture formulae. With multi-objective fitness testing, candidate textures are evaluated according to multiple criteria. Each criteria designates a dimension of a multi-dimensional fitness space. The main feature test uses Ralph's model of aesthetics. This aesthetic model is based on empirical analyses of fine art, in which analyzed art work exhibits bell curve distributions of color gradients. Subjectively speaking, this bell-curve gradient measurement tends to favor images that have harmonious and balanced visual characteristics. Another feature test is color histogram scoring. This test permits some control of the color composition, by matching a candidate texture's color composition with the color histogram of a target image. This target image may be a digital image of another artwork. We found that the use of the bell curve model often resulted in images that were harmonious and easy-on-the-eyes. Without the use of the model, generated images were often too chaotic or boring. Although our approach does not guarantee aesthetically pleasing results, it does increase the likelihood that generated textures are visually interesting.",
DyXY - a proximity congestion-aware deadlock-free dynamic routing method for network on chip,"A routing algorithm, namely dynamic XY (DyXY) routing, is proposed for NoCs to provide adaptive routing and ensure deadlock-free and livelock-free routing at the same time. New router architecture is developed to support the routing algorithm. Analytical models based on queuing theory are developed for DyXY routing for a two-dimensional mesh NoC architecture, and analytical results match very well with the simulation results. It is observed that DyXY routing can achieve better performance compared with static XY routing and odd-even routing",
Optimal Superdense Coding of Entangled States,"In this paper, we present a one-shot method for preparing pure entangled states between a sender and a receiver at a minimal cost of entanglement and quantum communication. In the case of preparing unentangled states, an earlier paper showed that a 2l-qubit quantum state could be communicated to a receiver by physically transmitting only l+o(l) qubits in addition to consuming l ebits of entanglement and some shared randomness. When the states to be prepared are entangled, we find that there is a reduction in the number of qubits that need to be transmitted, interpolating between no communication at all for maximally entangled states and the earlier two-for-one result of the unentangled case, all without the use of any shared randomness. We also present two applications of our result: a direct proof of the achievability of the optimal superdense coding protocol for entangled states produced by a memoryless source, and a demonstration that the quantum identification capacity of an ebit is two qubits",
Humanoid with Interaction Ability Using Vision and Speech Information,"Intelligent robots will make a chance for us to use a computer in our daily life. We implemented a humanoid robot for the computerized university guidance at first and then some capabilities for the natural interaction are added. This paper describes the hardware and software system of this humanoid with interaction ability. HRP-2 sitting opposite to a user across a table can detect gaze direction, head pose and gestures using a stereo camera system attached to the head. In addition, our system can recognize the user's question utterance and non-stationary noise such as coughing and sneezing using microphones. Using efficiently such information, HRP-2 can answer the question by its synthesized voice with gestures and do tasks such as passing objects on the table",
Mammogram registration: a phantom-based evaluation of compressed Breast Thickness variation effects,"The temporal comparison of mammograms is complex; a wide variety of factors can cause changes in image appearance. Mammogram registration is proposed as a method to reduce the effects of these changes and potentially to emphasize genuine alterations in breast tissue. Evaluation of such registration techniques is difficult since ground truth regarding breast deformations is not available in clinical mammograms. In this paper, we propose a systematic approach to evaluate sensitivity of registration methods to various types of changes in mammograms using synthetic breast images with known deformations. As a first step, images of the same simulated breasts with various amounts of simulated physical compression have been used to evaluate a previously described nonrigid mammogram registration technique. Registration performance is measured by calculating the average displacement error over a set of evaluation points identified in mammogram pairs. Applying appropriate thickness compensation and using a preferred order of the registered images, we obtained an average displacement error of 1.6 mm for mammograms with compression differences of 1-3 cm. The proposed methodology is applicable to analysis of other sources of mammogram differences and can be extended to the registration of multimodality breast data.",
Pinhole SPECT imaging: compact projection/backprojection operator for efficient algebraic reconstruction,"We describe the efficient algebraic reconstruction (EAR) method, which applies to cone-beam tomographic reconstruction problems with a circular symmetry. Three independant steps/stages are presented, which use two symmetries and a factorization of the point spread functions (PSFs), each reducing computing times and eventually storage in memory or hard drive. In the case of pinhole single photon emission computed tomography (SPECT), we show how the EAR method can incorporate most of the physical and geometrical effects which change the PSF compared to the Dirac function assumed in analytical methods, thus showing improvements on reconstructed images. We also compare results obtained by the EAR method with a cubic grid implementation of an algebraic method and modeling of the PSF and we show that there is no significant loss of quality, despite the use of a noncubic grid for voxels in the EAR method. Data from a phantom, reconstructed with the EAR method, demonstrate 1.08-mm spatial tomographic resolution despite the use of a 1.5-mm pinhole SPECT device and several applications in rat and mouse imaging are shown. Finally, we discuss the conditions of application of the method when symmetries are broken, by considering the different parameters of the calibration and nonsymmetric physical effects such as attenuation.",
"The OptIPuter: high-performance, QoS-guaranteed network service for emerging E-science applications","Emerging large-scale scientific applications have a critical need for high bandwidth and predictable-performance network service. The OptlPuter project is pioneering a radical new type of distributed application paradigm that exploits dedicated optical circuits to tightly couple geographically dispersed resources. These private optical paths are set up on demand and combined with end resources to form a distributed virtual computer (DVC). The DVC provides high-quality dedicated network service to applications. In this article we compare the OptIPuter's approach (DVC), which exploits network resources to deliver higher-quality network services, to several alternative service models (intelligent network and asynchronous file transfer). Our simulations show that there are significant differences among the models in their utilization of resources and delivered application services. Key takeaways include that the OptlPuter approach provides applications with superior network service (as needed by emerging e-science applications and performance-critical distributed applications), at an expense in network resource consumption. The other approaches use fewer network resources, but provide lower-quality application service",
Shape Statistics Variational Approach for the Outer Contour Segmentation of Left Ventricle MR Images,"Segmentation of left ventricles is one of the important research topics in cardiac magnetic resonance (MR) imaging. The segmentation precision influences the authenticity of ventricular motion reconstruction. In left ventricle MR images, the weak and broken boundary increases the difficulty of segmenting the outer contour precisely. In this paper, we present an improved shape statistics variational approach for the outer contour segmentation of left ventricle MR images. We use the Mumford-Shah model in an object feature space and incorporate the shape statistics and an edge image to the variational framework. The introduction of shape statistics can improve the segmentation with broken boundaries. The edge image can enhance the weak boundary and thus improve the segmentation precision. The generation of the object feature image, which has homogenous ""intensities"" in the left ventricle, facilitates the application of the Mumford-Shah model. A comparison of mean absolute distance analysis between different contours generated with our algorithm and that generated by hand demonstrated that our method can achieve a higher segmentation precision and a better stability than various approaches. It is a semiautomatic way for the segmentation of the outer contour of the left ventricle in clinical applications","Shape,
Statistics,
Image segmentation,
Magnetic resonance imaging,
Computer science,
Magnetic resonance,
Space technology,
Heart,
Image reconstruction,
Algorithm design and analysis"
Executing Hardware Tasks on Dynamically Reconfigurable Devices Under Real-Time Conditions,"This paper presents a prototype system that executes a set of periodic real-time tasks utilizing dynamic hardware reconfiguration. The proposed scheduling technique, MSDL, is not only able to give an offline guarantee for the feasibility of the task set but also minimizes the number of device configurations. After describing this technique, we extend the schedulability analysis to include different runtime system overheads, including the device reconfiguration time. Then we detail a light-weight runtime system that performs the online part of the MSDL scheduling technique. The runtime system is entirely implemented in hardware. Finally, we outline the corresponding synthesis tool flow and report on the overhead posed by the runtime system",
Channel Sampling Strategies for Monitoring Wireless Networks,"Monitoring the activity on an IEEE 802.11 network is useful for many applications, such as network management, optimizing deployment, or detecting network attacks. Deploying wireless sniffers to monitor every access point in an enterprise network, however, may be expensive or impractical. Moreover, some applications may require the deployment of multiple sniffers to monitor the numerous channels in an 802.11 network. In this paper, we explore sampling strategies for monitoring multiple channels in 802.11b/g networks. We describe a simple sampling strategy, where each channel is observed for an equal, predetermined length of time, and consider applications where such a strategy might be appropriate. We then introduce a sampling strategy that weights the time spent on each channel according to the number of frames observed on that channel, and compare the two strategies under experimental conditions.",
Optimal Sensor Placement for Agent Localization,,
Satellite Image Processing Applications in MedioGRID,"This paper presents a high level architectural specification of MedioGRID, a research project aiming at implementing a real-time satellite image processing system for extracting relevant environmental and meteorological parameters on a grid system. The presentation focuses on the key architectural decisions of the GRID-aware satellite image processing system, highlighting the technologies for each of the major components. An essential part of managing a global data grid is a monitoring system that is able to monitor and track all the site facilities, networks, and tasks in progress, all in real time. Considering this issue the paper analyzes the possible grid monitoring approaches, proposes a solution and presents a set of monitoring results for the MedioGRID data management subsystem",
Separating lines of text in free-form handwritten historical documents,"We present an approach to finding (and separating) lines of text in free-form handwritten historical document images. After preprocessing, our method uses the count of foreground/background transitions in a binarized image to determine areas of the document that are likely to be text lines. Alternatively, an adaptive local connectivity map (ALCM) found in the literature can be used for this step of the process. We then use a min-cut/max-flow graph cut algorithm to split up text areas that appear to encompass more than one line of text. After removing text lines containing relatively little text information (or merging them with nearby text lines), we create output images for each line. A grayscale output image is created, as well as a special mask image containing both the foreground and information flagging ambiguous pixels. Foreground pixels that belong to other text lines are removed from the output images to provide cleaner line images useful for further processing. While some refinement is still necessary, the result of early experimentation with our method is encouraging",
On the Optimality of the Dimensionality Reduction Method,"We investigate the optimality of (1+epsi)-approximation algorithms obtained via the dimensionality reduction method. We show that: any data structure for the (1 + epsi)-approximate nearest neighbor problem in Hamming space, which uses constant number of probes to answer each query, must use nOmega(1/epsi2) space; any algorithm for the (1 + epsi)-approximate closest substring problem must run in time exponential in 1/epsi2 - gamma for any gamma > 0 (unless 3SAT can be solved in sub-exponential time). Both lower bounds are (essentially) tight","Polynomials,
Data structures,
Nearest neighbor searches,
Frequency,
Clustering algorithms,
Probes,
Pattern analysis,
Concrete,
Design methodology,
Algorithm design and analysis"
Visualizing Internet Routing Changes,"Today's Internet provides a global data delivery service to millions of end users and routing protocols play a critical role in this service. It is important to be able to identify and diagnose any problems occurring in Internet routing. However, the Internet's sheer size makes this task difficult. One cannot easily extract out the most important or relevant routing information from the large amounts of data collected from multiple routers. To tackle this problem, we have developed Link-Rank, a tool to visualize Internet routing changes at the global scale. Link-Rank weighs links in a topological graph by the number of routes carried over each link and visually captures changes in link weights in the form of a topological graph with adjustable size. Using Link-Rank, network operators can easily observe important routing changes from massive amounts of routing data, discover otherwise unnoticed routing problems, understand the impact of topological events, and infer root causes of observed routing changes",
Multi-Resolution Spin-Images,"Johnson and Hebert’s spin-images have been applied to the registration of range images and object recognition with much success because they are rotation, scale, and pose invariant. In this paper we address two issues concerning spin-images, namely: (1) comparing uncompressed spinimages across large datasets is costly, and (2) a method to select the appropriate bin size and image width for spinimages is not clearly defined. Our solution to these issues is a multi-resolution method that generates a pyramid of spin-images by successively decreasing the spin-image size by powers of two. To efficiently correlate surface points, we compare spin-images in a low-to-high resolution manner. Once multi-resolution spin-images are generated for a given object, we have found that the different resolutions can also be used to compare objects that have differing or non-uniform point densities. To select the appropriate bin sizes for comparing such objects, we use the ratio of the average edge lengths of the objects. We also show preliminary results of using the pyramid to converge on the appropriate image width by traversing the pyramid in a low-to-high resolution manner looking for the highest resolution at which the fewest number of highly correlated points are found to match a given feature point.","Image resolution,
Multiresolution analysis,
Computer science,
Image converters,
Shape measurement,
Object recognition,
Power generation,
Spline,
Image sampling,
Timing"
Multi-processor system design with ESPAM,"For modern embedded systems, the complexity of embedded applications has reached a point where the performance requirements of these applications can no longer be supported by embedded system architectures based on a single processor. Thus, the emerging embedded System-on-Chip platforms are increasingly becoming multiprocessor architectures. As a consequence, two major problems emerge, i.e., how to design and how to program such multiprocessor platforms in a systematic and automated way in order to reduce the design time and to satisfy the performance needs of applications executed on these platforms. Unfortunately, most of the current design methodologies and tools are based on Register Transfer Level (RTL) descriptions, mostly created by hand. Such methodologies are inadequate, because creating RTL descriptions of complex multiprocessor systems is error-prone and time consuming. As an efficient solution to these two problems, in this paper we propose a methodology and techniques implemented in a tool called ESPAM for automated multiprocessor system design and implementation. ESPAM moves the design specification from RTL to a higher, so called system level of abstraction. We explain how starting from system level platform, application, and mapping specifications, a multiprocessor platform is synthesized and programmed in a systematic and automated way. Furthermore, we present some results obtained by applying our methodology and ESPAM tool to automatically generate multiprocessor systems that execute a real-life application, namely a Motion-JPEG encoder.",
"V-eM: A Cluster of Virtual Machines for Robust, Detailed, and High-Performance Network Emulation","In this paper we present the design and performance evaluation of a network emulation cluster built with commodity PCs and network switches. Each emulated node runs inside its own virtual machine and is complete with a kernel and device drivers and the virtual machine monitor ensures isolation between the emulated nodes, fair access to the resources of the underlying physical node, and high performance. The load of traffic conditioning (emulating packet delays, losses and other characteristics of widearea network links) is shared between all physical nodes by conditioning only the traffic originated by the emulated nodes they host. The above organization results in an emulation testbed that is low cost and scalable while providing strict resource isolation between emulated nodes and high emulation fidelity by allowing the emulation of kernels and other system level software. In this paper we present the main considerations behind the design of our testbed and through detailed performance evaluation we demonstrate that our approach can result in a scalable emulation system with high performance.",
A Trajectory-Preserving Synchronization Method for Collaborative Visualization,"In the past decade, a lot of research work has been conducted to support collaborative visualization among remote users over the networks, allowing them to visualize and manipulate shared data for problem solving. There are many applications of collaborative visualization, such as oceanography, meteorology and medical science. To facilitate user interaction, a critical system requirement for collaborative visualization is to ensure that remote users would perceive a synchronized view of the shared data. Failing this requirement, the user's ability in performing the desirable collaborative tasks would be affected. In this paper, we propose a synchronization method to support collaborative visualization. It considers how interaction with dynamic objects is perceived by application participants under the existence of network latency, and remedies the motion trajectory of the dynamic objects. It also handles the false positive and false negative collision detection problems. The new method is particularly well designed for handling content changes due to unpredictable user interventions or object collisions. We demonstrate the effectiveness of our method through a number of experiments",
Determining RNA Secondary Structure using Set-based Particle Swarm Optimization,"Determining RNA secondary structure computationally, rather than manually, has the advantage of being cheaper and quicker. This paper introduces a new set-based particle swarm optimization algorithm to optimize the structure of an RNA molecule, using an advanced thermodynamic model. Results show that it is possible to use this SetPSO algorithm to optimise RNA secondary structure and produce candidate RNA conformations.",
Principles for the Prediction of Video Decoding Times Applied to MPEG-1/2 and MPEG-4 Part 2 Video,"In this paper, we present a method to predict per-frame decoding times of modern video decoder algorithms. By examining especially the MPEG-1, MPEG-2, and MPEG-4 pt. 2 algorithms, we developed a generic model for these decoders, which also applies to a wide range of other decoders. From this model, we derived a method to predict decoding times with an up-to-now unmatched accuracy while keeping the overhead low. We show the effectiveness of this method with an example implementation and compare the resulting predictions with the actual decoding times using video material from commercial DVDs",
Efficient localization of synchronous EEG source activities using a modified RAP-MUSIC algorithm,"Synchronization across different brain regions is suggested to be a possible mechanism for functional integration. Noninvasive analysis of the synchronization among cortical areas is possible if the electrical sources can be estimated by solving the electroencephalography inverse problem. Among various inverse algorithms, spatio-temporal dipole fitting methods such as RAP-MUSIC and R-MUSIC have demonstrated superior ability in the localization of a restricted number of independent sources, and also have the ability to reliably reproduce temporal waveforms. However, these algorithms experience difficulty in reconstructing multiple correlated sources. Accurate reconstruction of correlated brain activities is critical in synchronization analysis. In this study, we modified the well-known inverse algorithm RAP-MUSIC to a multistage process which analyzes the correlation of candidate sources and searches for independent topographies (ITs) among precorrelated groups. Comparative studies were carried out on both simulated data and clinical seizure data. The results demonstrated superior performance with the modified algorithm compared to the original RAP-MUSIC in recovering synchronous sources and localizing the epileptiform activity. The modified RAP-MUSIC algorithm, thus, has potential in neurological applications involving significant synchronous brain activities.",
Distributed Storage to Support User Interactivity in Peer-to-Peer Video Streaming,"Providing random access function in peer-to-peer on-demand video streaming is a challenging task, due to not only the asynchronous user interactivity but also the unpredictability of group dynamics. In this paper, we propose VMesh, a distributed peer-to-peer video-on-demand (VoD) streaming scheme which efficiently supports random seeking functionality. In VMesh, videos are divided into segments and stored in peers in a distributed manner. An overlay mesh is built upon peers to support jumping forward/backward, pause and restart during playback. Our scheme utilizes the large total storage capacity of peers to improve the segment supply so as to support interactive commands in a scalable manner. Through simulation, we show that our system outperforms a recent work, P2VoD. VMesh also has low segment missing rate under random member join/leave. In addition, the system achieves low joining and seeking latencies which are crucial requirements in an interactive VoD system.",
Dynamic Appearance Modeling for Human Tracking,"Dynamic appearance is one of the most important cues for tracking and identifying moving people. However, direct modeling spatio-temporal variations of such appearance is often a difficult problem due to their high dimensionality and nonlinearities. In this paper we present a human tracking system that uses a dynamic appearance and motion modeling framework based on the use of robust system dynamics identification and nonlinear dimensionality reduction techniques. The proposed system learns dynamic appearance and motion models from a small set of initial frames and does not require prior knowledge such as gender or type of activity. The advantages of the proposed tracking system are illustrated with several examples where the learned dynamics accurately predict the location and appearance of the targets in future frames, preventing tracking failures due to model drifting, target occlusion and scene clutter.",
Technologies That Make You Smile: Adding Humor to Text-Based Applications,"Natural language's creative genres are traditionally considered to be outside the scope of computational modeling. Computational linguists have paid little attention to humor in particular because it is puzzling by nature. However, given the importance of humor in our daily lives and computers in our work and entertainment, studies related to computational humor will become increasingly significant in fields such as human-computer interaction, intelligent interactive entertainment, and computer-assisted education. In this article, we explore computational approaches' applicability to the recognition and use of verbally expressed humor. Particularly, we focus on three important research questions related to this problem: Can we automatically gather large collections of humorous texts? Can we automatically recognize humor in text? And can we automatically insert humorous add-ons into existing applications?",
Watermark Synchronization: Perspectives and a New Paradigm,"Synchronization is one of the most challenging elements of a watermarking system. In this paper, we survey and classify methods employed for watermark synchronization and highlight some inherent difficulties in synchronization that arise due to the nature of multimedia signals. We then propose a framework that addresses these hurdles in practical applications. Our framework utilizes feature-based embedding in concert with error correction codes capable of handling not only substitutions caused by various perturbations but also insertion and deletion events caused by erroneous feature estimates. We present a practical scheme for speech watermarking based on the framework. Experimental results show that the proposed methodology indeed enables synchronization in scenarios where a mismatch in estimated features between embedder and receiver would otherwise cause synchronization loss. We explore connections of the framework with recent theoretical analyses of watermark synchronization.","Watermarking,
Geometry,
Degradation,
Error correction codes,
Speech,
Multimedia systems,
Robustness,
Digital communication,
Context,
Multimedia communication"
A Scalable Multi-tier Architecture for the National Taiwan University Hospital Information System based on HL7 Standard,"This article describes the successful experiences of National Taiwan University Hospital (NTUH) in moving from IBM Mainframe to connected networking computer systems. We use multi-tier architecture and HL7 standard to implement our new outpatient hospital information system (HIS). The NTUH HIS is a complex environment with several operating systems, databases, and information systems. We adopt service-oriented architecture (SOA) to reduce the complex relations between systems and solve data consistency problems among databases. We also show that the distributed architecture can provide us stable and reasonable system performances. Our main contribution is proving that the distributed environment with HL7 standard and SOA can sustain in a highly demanding environment",
Support system for guitar playing using augmented reality display,"Learning to play the guitar is difficult. We proposed a system that assists people learning to play the guitar using augmented reality. This system shows a learner how to correctly hold the strings by overlaying a virtual hand model and lines onto a real guitar. The player learning to play the guitar can easily understand the required position by overlapping their hand on a visual guide. An important issue for this system to address is the accurate registration between the visual guide and the guitar, therefore we need to track the pose and the position of the guitar. We also proposed a method to track the guitar with a visual marker and natural features of the guitar. Since we used marker information and edge information as natural features, the system could continually track the guitar. Accordingly, our system can constantly display visual guides at the required position to enable a player to learn to play the guitar in a natural manner.",
Electromagnetic Considerations in the Design of Doubly-Fed Reluctance Generators for use in Wind Turbines,This paper reviews some of the electromagnetic design issues in a doubly-fed reluctance machine. This is a novel machine that is still under early-stage development. The control has been studied previously in several papers however the electromagnet design of this machine is now being studied with the key issue being whether to use an axially-laminated rotor or a radially-laminated rotor. This is discussed in the paper,
Verifying Specifications with Proof Scores in CafeOBJ,"Verifying specifications is still one of the most important undeveloped research topics in software engineering. It is important because quite a few critical bugs are caused at the level of domains, requirements, and/or designs. It is also important for the cases where no program codes are generated and specifications are analyzed and verified only for justifying models of problems in real world. This paper gives a survey of our research activities in verifying specifications with proof scores in CafeOBJ. After explaining fundamental issues and importance of verifying specifications, an overview of CafeOBJ language, the proof score approach in CafeOBJ including its applications to several areas are given. This paper is based on our already published books or papers (Diaconescu and Futatsugi, 1998; Futatsugi et al., 2005), and refers to many of our related publications. Interested readers are invited to look into them",
Speech Emotion Recognition Based on Rough Set and SVM,"Speech emotion recognition is becoming more and more important in such computer application fields as health care, children education, etc. There are a few works have been done on speech emotion recognition using such methods as ANN, SVM, etc in the last years. Traditional feature selection method used in speech emotion recognition is computationally too expensive to determine an optimum or suboptimum feature subset. In this paper, a novel approach based on rough set theory and SVM for speech emotion recognition is proposed. The experiment results show this approach can reduce the calculation cost while keeping high recognition rate",
Trajectory Analysis for Soccer Players,"In order to make good strategies, soccer coaches analyze the archives of matches, which can be effectively considered as a set of trajectories. We can extract several useful information by analyzing the trajectories of moving objects, which consist of 22 players and a ball. Since each moving object interacts with others and produces a trajectory, its trajectory has a certain number of relationships with others, which are a basic type of information to make soccer strategies. In this paper, we propose a model to quantitatively express the performance of soccer players. This model is based on the relationships between trajectories of 22 players and a ball and allows to evaluate the performance of several players in quantitative way",
ImprovingWeb-based Image Search via Content Based Clustering,"Current image search engines on the web rely purely on the keywords around the images and the filenames, which produces a lot of garbage in the search results. Alternatively, there exist methods for content based image retrieval that require a user to submit a query image, and return images that are similar in content. We propose a novel approach named ReSPEC (Re-ranking Sets of Pictures by Exploiting Consistency), that is a hybrid of the two methods. Our algorithm first retrieves the results of a keyword query from an existing image search engine, clusters the results based on extracted image features, and returns the cluster that is inferred to be the most relevant to the search query. Furthermore, it ranks the remaining results in order of relevance.",
Multiple Mice for Computers in Education in Developing Countries,"A distinct feature observed in computer use in schools or rural kiosks in developing countries is the high student-to-computer ratio. It is not unusual to see more than five children crowding around a single display, as schools are rarely funded to afford one PC per child in a classroom. One child controls the mouse, while others are passive onlookers, without operational control of the computer. Learning benefits appear to accrue primarily to the child with the mouse, with the other children missing out. The obvious technical solution is to provide each child with a mouse and cursor on screen, thus effectively multiplying the amount of interaction per student per PC for the cost of a few extra mice. To our surprise, both the concept and the implementation appear to be unique to date, for the specific application to computers in education in resource-strapped communities, with previous work restricting studies to two mice, or for largely non-educational applications. We have developed software that allows multiple coloured cursors to co-exist on the monitor, along with two sample games with some educational content. Initial trials with both single-mouse and multiple-mice scenarios suggest that children are more engaged when in control of a mouse, and that more mice increases overall engagement. Our results suggest new areas of research in pedagogy for computers in education",
Peer-to-Peer Asynchronous Video Streaming using Skip List,"Media distribution through application-layer overlay networks has received considerable attention recently, owing to its flexibility and readily deployable nature. On-demand streaming with asynchronous requests, and in general, with VCR-like interactions, nevertheless remains a challenging task. In this paper, we introduce the skip list, a novel randomized and distributed structure that inherently accommodates dynamic and asynchronous clients. We demonstrate a practical skip list based streaming overlay with typical VCR operations. Our simulation results show that the skip list based overlay is highly scalable, with smooth playback for diverse interactivities, and low overheads",
A Comprehensive Comparison of Routing Protocols for Large-Scale Wireless MANETs,"Efficient routing protocols can provide significant benefits to mobile ad hoc networks, in terms of both performance and reliability. Many routing protocols for such networks have been proposed so far. Amongst the most popular ones are dynamic source routing (DSR), ad hoc on-demand distance vector (AODV), temporally-ordered routing algorithm (TORA) and location-aided routing (LAR). Despite the popularity of those protocols, research efforts have not focused in evaluating their performance when applied to large-scale wireless networks. Such networks are comprised of hundreds of nodes, connected via long routes. This greatly affects the network efficiency, since it necessitates frequent exchange of routing information. In this paper we present our observations regarding the behavior of the above protocols, in large-scale mobile ad hoc networks (MANETs). We consider wireless mobile terminals spread over a large geographical area, and we perform extensive simulations, using the QualNet and NS-2 simulators. The results of the simulations yield some interesting conclusions: AODV suffers in terms of packet delivery fraction (PDF) but scales very well in terms of end-to-end delay. DSR on the other hand scales well in terms of packet delivery fraction but suffers an important increase of end-to-end delay, as compared to its performance achieved in small-scale topologies. Also, the effect of maximum connections is severe on TORA, which seems unable to route large amounts of traffic. LAR, seems to scale very well, in terms of all metrics employed",
"On the girth of tanner (3, 5) quasi-cyclic LDPC codes","In this correspondence, the cycles of Tanner (3,5) quasi-cyclic (QC) low-density parity-check (LDPC) codes are analyzed and their girth values are derived. The conditions for the existence of cycles of lengths 4,6,8, and 10 in Tanner (3,5) QC LDPC codes of length 5p are expressed in terms of polynomial equations in a 15th root of unity of the prime field F/sub p/. By checking the existence of solutions for these equations over F/sub p/, the girths of Tanner (3,5) QC LDPC codes are derived.",
A Goal-driven Approach of Service Composition for Pervasive Computing,"It is an important problem for pervasive computing to coordinate and compose large numbers of devices and services to achieve users' various task goals. The pervasive computing environment is transparent for users. In order to simplify the interaction between users and the environment, this paper presents a goal-driven approach of service composition. We build a task-oriented semantic representation model of Web services and based on this model goal-driven service composition is performed dynamically to achieve user's goal. We also describe a simulated application scenario in this paper and discuss the process of task definition and service composition according to the scenario. This approach simplifies the interaction between users and pervasive computing environment and improves the flexibility of service cooperation and composition in the environment",
An Outer Bound for Multiple Access Channels with Correlated Sources,"The capacity region of the multiple access channel with correlated sources remains an open problem. Cover, El Gamal and Salehi gave an achievable region in the form of single-letter entropy and mutual information expressions, without a single-letter converse. Cover, El Gamal and Salehi also suggested a converse in terms of some n-letter mutual informations, which are incomputable. We have proposed an upper bound for the sum rate of this channel in a single-letter expression, by utilizing a new necessary condition for the Markov chain constraint on the valid channel input distributions. In this paper, we extend our results from the sum rate to the entire capacity region. We obtain an outer bound for the capacity region of the multiple access channel with correlated sources in finite-letter expressions.",
A Study of Access Control Requirements for Healthcare Systems Based on Audit Trails from Access Logs,"In healthcare, role-based access control systems are often extended with exception mechanisms to ensure access to needed information even when the needs don¿t follow the expected patterns. Exception mechanisms increase the threats to patient privacy, and therefore their use should be limited and subject to auditing. We have studied access logs from a hospital EPR system with extensive use of exception-based access control. We found that the uses of the exception mechanisms were too frequent and widespread to be considered exceptions. The huge size of the log and the use of pre-defined or uninformative reasons for access make it infeasible to audit the log for misuse. The informative reasons that were given provided starting points for requirements on how the usage needs should be accomplished without exception-based access. With more structured and fine-grained logging, analysis of access logs could be a very useful tool for learning how to reduce the need for exception-based access.",
Changing Students&#146; Perceptions: An Analysis of the Supplementary Benefits of Collaborative Software Development,"Collaborative work has been in use as an instructional tool to increase student understanding through collaborative learning and to improve student performance in computer science courses. However, little work has been done to understand how the act of collaboration, through pair programming or group work, impacts a student's knowledge of the benefits and difficulties of collaborative work experience in collaborative work is essential preparation for professional software development. A study was conducted at North Carolina State University to assess changes in advanced undergraduate students' perceptions of pair programming and collaboration. Student personality types, learning styles, and other characteristics were gathered during two semesters of an undergraduate software engineering course. The study found that, after experiencing pair programming, most students indicated a stronger preference to work with another student, believed that pairing made them more organized, and believed that pairing saved time on homework assignments. Students who disliked their collaborative experiences were predominantly reflective learners, introverts, and strong coders. Those students also cited that a non-participatory partner and difficulties scheduling meeting times outside of the classroom were primary reasons for disliking pair programming. Personality type and learning style had little effect on the changes in perceptions of collaboration",
An Attitude Compensation Technique for a MEMS Motion Sensor Based Digital Writing Instrument,"A MAG-muIMU which is based on MEMS gyroscopes, accelerometers, and magnetometers is developed for real-time estimation of human hand motions. Appropriate filtering, transformation and sensor fusion techniques are combined in the ubiquitous digital writing instrument to record handwriting on any surface. In this paper, we discuss the design of an extended Kalman filter based on MAG-muIMU (micro inertial measurement unit with magnetometers) for real-time attitude tracking. The filter utilizes the gyroscope propagation for transient updates and correction by reference field sensors, such as gravity sensors, magnetometers or star trackers. A process model is derived to separate sensor bias and to minimize wideband noise. The attitude calculation is based on quaternion which, when compared to Euler angles, has no singularity problem. Testing with synthetic data and actual sensor data proved the filter will converge and accurately track the attitude of a rigid body. Our goal is to implement this algorithm for motion recognition of a 3D ubiquitous digital pen.",
Swarm Formation Control with Potential Fields Formed by Bivariate Normal Functions,"A novel method is presented for swarm formation control with potential fields generated from bivariate normal probability density functions (pdfs) that construct the surface the swarm members move upon controlling the swarm geometry and member spacing as well as manage obstacle avoidance. Limiting functions provide tighter swarm control by modifying and adjusting a set of control variables, forcing the swarm to behave according to set constraints. Bivariate normal functions and limiting functions are combined to guarantee obstacle avoidance and control swarm member orientation and swarm movement as a whole. The presented approach, compared to others, is simple, computationally efficient, and scales well to different swarm sizes and swarm models. The method is applied to a simple vehicle model, and simulation results are presented on a homogeneous swarm of ten robot vehicles for different formations",
Epileptic Spike Detection Using a Kalman Filter Based Approach,"The electroencephalogram (EEG) consists of an underlying background process with superimposed transient nonstationarities such as epileptic spikes (ESs). The detection of ESs in the EEG is of particular importance in the diagnosis of epilepsy. In this paper a new approach for detecting ESs in EEG recordings is presented. It is based on a time-varying autoregressive model (TVAR) that makes use of the nonstationarities of the EEG signal. The autoregressive (AR) parameters are estimated via Kalman filtering (KF). In our method, the EEG signal is first preprocessed to accentuate ESs and attenuate background activity, and then passed through a thresholding function to determine ES locations. The proposed method is evaluated using simulated signals as well as real inter-ictal EEGs",
Fisher's linear spectral mixture analysis,"Linear spectral mixture analysis (LSMA) has been widely used in subpixel analysis and mixed-pixel classification. One commonly used approach is based on either the least square error (LSE) criterion such as least squares LSMA or the signal-to-noise ratio (SNR) such as orthogonal subspace projection (OSP). Unfortunately, it is known that such criteria are not necessarily optimal for pattern classification. This paper presents a new and alternative approach to LSMA, called Fisher's LSMA (FLSMA). It extends the well-known pure-pixel-based Fisher's linear discriminant analysis to LSMA. Interestingly, what can be done for the LSMA can be also developed for the FLSMA. Of particular interest are two types of constraints imposed on the LSMA, target signature-constrained LSMA and target abundance-constrained LSMA, which can be also derived in parallel for the FLSMA, to be called feature-vector-constrained FLSMA (FVC-FLSMA) and abundance-constrained FLSMA (AC-FLSMA), respectively. Since Fisher's ratio used by the FLSMA is a more appropriate classification criterion than the LSE or SNR used for the LSMA, the FVC-FLSMA improves over the classical least squares based LSMA and SNR-based OSP in mixed-pixel classification. Similarly, the AC-FLSMA also improves abundance-constrained least squares based LSMA in quantification of abundance fractions",
Exploiting Geographical and Temporal Locality to Boost Search Efficiency in Peer-to-Peer Systems,"As a hot research topic, many search algorithms have been presented and studied for unstructured peer-to-peer (P2P) systems during the past few years. Unfortunately, current approaches either cannot yield good lookup performance, or incur high search cost and system maintenance overhead. The poor search efficiency of these approaches may seriously limit the scalability of current unstructured P2P systems. In this paper, we propose to exploit two-dimensional locality to improve P2P system search efficiency. We present a locality-aware P2P system architecture called Foreseer, which explicitly exploits geographical locality and temporal locality by constructing a neighbor overlay and a friend overlay, respectively. Each peer in Foreseer maintains a small number of neighbors and friends along with their content filters used as distributed indices. By combining the advantages of distributed indices and the utilization of two-dimensional locality, our scheme significantly boosts P2P search efficiency while introducing only modest overhead. In addition, several alternative forwarding policies of Foreseer search algorithm are studied in depth on how to fully exploit the two-dimensional locality",
Capacity Limits of Cognitive Radio with Distributed and Dynamic Spectral Activity,"We investigate the capacity of opportunistic communication in the presence of dynamic and distributed spectral activity, i.e. when the time varying spectral holes sensed by the cognitive transmitter are correlated but not identical to those sensed by the cognitive receiver. Using the information theoretic framework of communication with causal and non-causal side information at the transmitter and/or the receiver, we obtain analytical capacity expressions and the corresponding numerical results. We find that cognitive radio communication is robust to dynamic spectral environments even when the communication occurs in bursts of only 3 - 5 symbols. The value of handshake overhead is investigated for both lightly loaded and heavily loaded systems. We find that the capacity benefits of overhead information flow from the transmitter to the receiver is negligible while feedback information overhead in the opposite direction significantly improves capacity.",
Mobile augmented reality interaction techniques for authoring situated media on-site,"We present a set of mobile augmented reality interaction techniques for authoring situated media: multimedia and hypermedia that are embedded within the physical environment. Our techniques are designed for use with a tracked hand-held tablet display with an attached camera, and rely on ""freezing"" the frame for later editing.","Augmented reality,
Cameras,
Virtual reality,
Mars,
Layout,
Organizing,
Books,
Scattering,
Mobile computing,
Computer science"
A Performance and Power Analysis of WK-Recursive and Mesh Networks for Network-on-Chips,"Network-on-chip (NoC) has been proposed as an attractive alternative to traditional dedicated wires to achieve high performance and modularity. Power efficiency is one of the most important concerns in NoC architecture design. The choice of network topology is important in designing a low-power and high-performance NoC. In this paper, we propose the use of the WK-recursive networks to be used as the underlying topology in NoC. We have implemented VHDL hardware model of mesh and WK-recursive topologies and measured the latency results using simulation with these implementation. We also propose a novel approach in high level power modeling based on latency for these topologies and show that the power consumption of WK-recursive topology is less than that of the equivalent mesh on a chip.",
An Optimistic Power Control MAC Protocol for Mobile Ad Hoc Networks,"Power control is a critical issue to implement Mobile Ad Hoc networks. This paper presents a novel power control protocol, namely the optimistic power MAC control (OPCM) protocol for its possible application in Mobile Ad Hoc networks. The OPCM protocol works by increasing power level in the retransmission stage to guarantee the DATA reception, rather than controlling the power in the initial transmission stage. It will be shown through comprehensive computer simulations that the proposed OPCM protocol is very energy efficient, while still being able to maintain a high throughput in a Mobile Ad Hoc network.",
Transactions Concurrency Control in Web Service Environment,"Business transactions in Web service environments run with relaxed isolation and atomicity property. In such environments, transactions can commit and roll back independently on each other. Transaction management has to reflect this issue and address the problems which result for example from concurrent access to Web service resources and data. In this paper we propose an extension to the WS-transaction protocol which ensures the consistency of the data when independent business transactions access the data concurrently under the relaxed transaction properties. Our extension is based on transaction dependency graphs maintained at the service provider side. We have implemented such a protocol on top of WS-transaction. The extension on the Web service provider side is simple to achieve as it can be an integral part of the service invocation mechanism. It has also an advantage from an engineering point of view as it does not change the way consumers or clients of Web services have to be programmed. Furthermore, it avoids direct communication between transaction coordinators which preserves security by keeping the information about business transactions restricted to the coordinators which are responsible for them",
"Stabilization of a class of uncertain ""wave"" discrete linear repetitive processes",Repetitive processes are a distinct class of two-dimensional (2D) systems (i.e. information propagation in two independent directions occurs) of both systems theoretic and applications interest. They cannot be controlled by direct extension of existing techniques from either standard (termed 1D here) or (most often) 2D systems theory. In this paper we continue the development of a systems theory for a recently proposed model of these processes necessary to represent terms which arise in some applications areas but are not included in the currently used models. The new results are on the design of control laws for stabilization in the case when there is uncertainty associated with the defining state-space model,
Practical Verification of Component Substitutability Using Subtype Relation,"The flexibility which components provide for assembling applications makes them an appealing solution to many engineering problems. Its darker side is the need to exercise much greater care when replacing and upgrading components within deployed applications, to ensure their stability. Formally strong methods for substitutability checks are therefore desirable but so far, not many are practically used. This paper presents a method of checking component substitutability based on subtyping relation. It uses a representation of the subtype evaluation on different levels of the component type structure, and makes it possible to perform the checks simply by comparing this representation. Two usage scenarios are described, as well as experiences from a prototype implementation for mainstream platforms",
A genetic algorithm for reliability-oriented task assignment with k/spl tilde/ duplications in distributed systems,"A distributed system is a collection of processor-memory pairs connected by communication links. The reliability of a distributed system can be expressed using the distributed program reliability, and distributed system reliability analysis. The computing reliability of a distributed system is an NP-hard problem. The distribution of programs & data-files can affect the system reliability. The reliability-oriented task assignment problem, which is NP-hard, is to find a task distribution such that the program reliability or system reliability is maximized. For example, efficient allocation of channels to the different cells can greatly improve the overall network throughput, in terms of the number of calls successfully supported. This paper presents a genetic algorithm-based reliability-oriented task assignment methodology (GAROTA) for computing the k/spl tilde/-DTA reliability problem. The proposed algorithm uses a genetic algorithm to select a program & file assignment set that is maximal, or nearly maximal, with respect to system reliability. Our numerical results show that the proposed algorithm may obtain the exact solution in most cases, and the computation time seems to be significantly shorter than that needed for the exhaustive method. When the proposed method fails to give an exact solution, the deviation from the exact solution is very small. The technique presented in this paper would be helpful for readers to understand the correlation between task assignment reliability, and distributed system topology.",
An improved ant-based routing protocol in Wireless Sensor Networks,"Routing in wireless sensor networks (WSNs) is very challenging due to their inherent characteristics of large scale, no global identification, dynamic topology, and very limited power, memory, and computational capacities for each sensor. Recent research on WSNs routing protocol has proved that data-centric technologies are needed for performing in-network aggregation of data to yield energy-efficient dissemination. As an effective distributed approach, Ant Colony Optimization (ACO) algorithms have been introduced to the design of data-centric routing protocol and have got many achievements, but still have some shortcomings blocking their further application in the large scale WSNs. To overcome the flaws of conventional ant-based data-centric routing algorithms, we proposed an improved protocol by adding a new type of ant, search ant, to supply prior information to the following ants. Besides, we introduced the strategy of simulating global pheromone update to accelerate the convergence of our algorithm and defined a ""retry"" rule to avoid dead-lock of the protocol. All of these modifications made the routing protocol scalable, practicable and energy-conservative. Simulation results showed the great advantages of the new protocol.",
Digital Images Phase Encryption Using Fractional Fourier Transform,"In the present paper the fractional Fourier transform was used to make phase encryption of color digital images. The image to encrypt is placed as the phase of a complex exponential, then is fractionally transformed three times and multiplied in intermediate steps by two statistically independent random phase masks thus to obtain the encrypted image, to decrypt the coding image the encryption procedure is applied in the inverse sense to the conjugated complex of the encrypted image, then is taken the negative of the phase of the resulting function from the decryption process and the original image is obtained this way that had been encrypted. The use of the fractional Fourier transform and the phase encryption of the image add much more complexity to the decryption of the image to who wants decrypt it without being authorized. In the cryptographic algorithm implemented five keys are used, made up of three fractional orders and two random phase masks, all these keys are necessary for proper decryption affording reliability to image transference via transmission networks",
Task Partitioning with Replication upon Heterogeneous Multiprocessor Systems,The heterogeneous multiprocessor task partitioning with replication problem involves determining a mapping of recurring tasks upon a set consisting of different processing units in such a way that all tasks meet their timing constraints and no two replicas of the same task are assigned to the same processing unit. The replication requirement improves the resilience of the real-time system to a finite number of processor failures. This problem is NP-hard in the strong sense. We develop a Fully Polynomial-Time Approximation Scheme (FPTAS) for this problem.,
Ontology Based Representations of Simulation Models Following the Process Interaction World View,"The discrete event simulation (DES) process interaction world view describes models that focus on simulated entities that progress through a series of temporally related activities. DES formalisms and vendor approaches for representing DES models serve as a basis for developing an open neutral representation of models that can be encoded into ontologies. This paper reviews world views, formal foundations, and ontologies as background. The process for creating ontologies for the process interaction DES domain is discussed. Next an approach to ontology based simulation model representation is presented and last conclusions and recommendations for future work are provided",
Analysis of plate piezoelectric unimorphs,"Two-dimensional (2-D) equations for coupled extensional and flexural motions of a two-layered, elastic-piezoelectric plate (unimorph) are derived systematically from the 3-D equations of linear piezoelectricity. For static problems, the equations are simplified by the introduction of a stress function. The equations are used to analyze two problems of a circular disk unimorph under a uniform mechanical load and a voltage, and a concentrated load and a voltage. Analytical solutions are obtained and examined.",
Replica Voting: a Distributed Middleware Service for Real-time Dependable Systems,"In information assurance settings, majority voting among replica processes enhances the trust-worthiness of data collected from a hostile external environment. It allows a correct data fusion and dissemination by the end-users, in the presence of content corruptions and/or timing failures that may possibly occur during data collection. Two key elements are required of the voting functionality: i) fielding asynchronously generated real-time or near-real-time data, and ii) handling large sized non-numeric data sets with problem-specific interpretations-e.g., terrain images from radar stations. Under the constraints (i) and (ii), we describe a highly asynchronous voting service to effect a timely and low-overhead delivery of data to the users. The paper analyzes the service-level properties of a voting machinery to meet the stringent needs of IA applications. The externally visible properties are prescribed in terms of `safety' and `liveness' requirements of the underlying voting protocols that reflect the application-specific data integrity and availability constraints. When voting is provided as a middleware service, an application designer may prescribe the necessary service-level parameters: the level of resiliency and the performance aspects of data delivery",
Real-time multi-agent support for decentralized management of electric power,"Establishing clean or renewable energy sources involves the problem of adequate management for the networked power sources, in particular since producers are at the same time also consumers, and vice versa. We describe the first phases of the joint R&D project DEZENT between the School of Computer Science and the College of Electrical Engineering at the University of Dortmund, devoted to decentralized and adaptive electric power management through a distributed real-time multi-agent architecture. Unpredictable consumer requests or producer problems, under distributed control or local autonomy will be the major novelty. We present a distributed real-time negotiation algorithm involving agents on different levels of negotiation, on behalf of producers and consumers of electric energy. Despite the lack of global overview we are able to prove that in our model no coalition of malicious users could take advantage of extreme situations like arising from an abundance as much as from any (artificial) shortage of electric power that are typical problems in ""free"" or deregulated markets. Our multi-agent system exhibits a very high robustness against power failures compared to centrally controlled architectures. In extensive experiments we demonstrate how, in realistic settings of the German power system structure, the novel algorithms can cope with unforeseen needs and production specifics in a very flexible and adaptive way, taking care of most of the potentially hard deadlines already on the local group level (corresponding to a small subdivision). We further demonstrate that under our decentralized approach customers pay less than under any conventional (global) management policy or structure",
Flex-SwA: Flexible Exchange of Binary Data Based on SOAP Messages with Attachments,"SOAP is the standard protocol for message exchange in Web service environments. As an XML-based protocol, SOAP is not suitable for the transmission of large amounts of binary data. This fact has been addressed by the SOAP messages with attachments specification, which regulates the transfer of a SOAP message together with an arbitrary number of binary attachments composed within a MIME multipart/related message. Although this leads to a reduction of transmission overhead, Web service communication using SOAP messages with attachments still lacks communication and processing flexibility. In this paper, we present a novel and more flexible way of handling attachments in SOAP-based Web service environments. In contrast to SOAP messages with attachments, our approach offers message forwarding without additional communication cost and demand-driven evaluation and transmission of binary data, thus providing the opportunity to save time by overlapping service execution and data transmission",
Dynamic Shape and Appearance Models,"We propose a model of the joint variation of shape and appearance of portions of an image sequence. The model is conditionally linear, and can be thought of as an extension of active appearance models to exploit the temporal correlation of adjacent image frames. Inference of the model parameters can be performed efficiently using established numerical optimization techniques borrowed from finite-element analysis and system identification techniques",
Hybrid Cluster Routing: An Efficient Routing Protocol for Mobile Ad Hoc Networks,"Routing is one of the fundamental but challenging issues in mobile ad hoc networks. During the past several years, a large number of routing protocols have been proposed, which can basically be categorized into three different groups including proactive/table-driven, reactive/on-demand, and hybrid. In this paper, we propose a novel hybrid routing protocol for large scale mobile ad hoc networks, namely HCR (Hybrid Cluster Routing). Here nodes are organized into a hierarchical structure of multi-hop clusters using a stable distributed clustering algorithm. Each cluster is composed of a clusterhead, several gateway nodes, and other ordinary nodes. The clusterhead is responsible for maintaining local membership and global topology information. In HCR, the acquisition of intra-cluster routing information operates in an on-demand fashion and the maintenance of inter-cluster routing information acts in a proactive way. Simulation results show that HCR conduces better scalability, robustness and adaptability to large scale mobile ad hoc networks compared with some well-known routing protocols, e.g. AODV, DSR, and CBRP.",
Function approximation using generalized adalines,"This paper proposes neural organization of generalized adalines (gadalines) for data driven function approximation. By generalizing the threshold function of adalines, we achieve the K-state transfer function of gadalines which responds a unitary vector of K binary values to the projection of a predictor on a receptive field. A generative component that uses the K-state activation of a gadaline to trigger K posterior independent normal variables is employed to emulate stochastic predictor-oriented target generation. The fitness of a generative component to a set of paired data mathematically translates to a mixed integer and linear programming. Since consisting of continuous and discrete variables, the mathematical framework is resolved by a hybrid of the mean field annealing and gradient descent methods. Following the leave-one-out learning strategy, the obtained learning method is extended for optimizing multiple generative components. The learning result leads to parameters of a deterministic gadaline network for function approximation. Numerical simulations further test the proposed learning method with paired data oriented from a variety of target functions. The result shows that the proposed learning method outperforms the MLP and RBF learning methods for data driven function approximation",
Integrating Speech Recognition and Machine Translation: Where do We Stand?,"This paper describes state-of-the-art interfaces between speech recognition and machine translation. We modify two different machine translation systems to effectively process dense speech recognition lattices. In addition, we describe how to fully integrate speech translation with machine translation based on weighted finite-state transducers. With a thorough set of experiments, we show that both the acoustic model scores and the source language model positively and significantly affect the translation quality. We have found consistent improvements on three different corpora compared with translations of single best recognition results",
A Grid Task Scheduling Algorithm Based on QoS Priority Grouping,"As the research of grid goes on, users demand increasingly high quality of task completion and high-quality scientific computing tasks continue to increase. This renders QoS a new problem that is to be considered in the grid scheduling algorithm. In this paper, a grid tasks scheduling strategy based on QoS priority grouping is proposed. In this algorithm, the deadline property of task, acceptation rate of tasks and makespan of systems are comprehensively considered. And QD-Sufferage, a grid task scheduling based on task priority grouping and deadline, is presented subsequently. The experiments show that the algorithm overweighs traditional algorithms a lot in makespan, throughout parameters",
Identification of the Mechanical Parameters for Servo Drive,"This paper describes a novel identification technique of the moment of inertia and the viscous friction coefficient for a servo drive system. It is developed based on the basic dynamic equation of a mechanical system, and it uses the torque reference of a speed controller and the actual rotating speed of machine for the identification. Regardless of the simplicity of the techniques it is robust to external disturbances such as measurement noise, Coulomb friction, mechanical resonance of the drive system. The identified inertia and friction can be used for auto-tuning of the gains in the speed controller and the disturbance observer. The effectiveness of the proposed method is proved by the computer simulation and experimental results",
Locally Linear Models on Face Appearance Manifolds with Application to Dual-Subspace Based Classification,"Recently, there has been a flurry of research on face recognition based on multiple images or shots from either a video sequence or an image set. This paper is also such an attempt in multiple-shot face recognition. Specifically, we propose a novel nonparametric method that first extracts discriminating local models via clustering. We apply a hierarchical distance-based clustering procedure according to some distance measure on the appearance manifold to cluster similar face images together. Based on the local models extracted, we then construct the intrapersonal and extrapersonal subspaces. Given a new test image, the angle between the projections of the image onto the two subspaces is used as a distance measure for classification. Since a test example contains multiple face images in multiple-shot face recognition, the final classification combines the classification decisions of all individual test images via a majority voting scheme. We compare our method empirically with some previous methods based on a database of video sequences of human faces, showing that out method significantly outperforms other methods.",
A Luminance- and Contrast-Invariant Edge-Similarity Measure,A novel similarity measure for edge-detection that is robust to varying luminance and contrast is presented. It incorporates a regularization term and employs directional FIR edge filters with hyperbolic tangent profiles to ensure improved noise performance and edge localization compared to classical methods,
Early Driver Fatigue Detection from Electroencephalography Signals using Artificial Neural Networks,"This paper describes a driver fatigue detection system using an artificial neural network (ANN). Using electroencephalogram (EEG) data sampled from 20 professional truck drivers and 35 non professional drivers, the time domain data are processed into alpha, beta, delta and theta bands and then presented to the neural network to detect the onset of driver fatigue. The neural network uses a training optimization technique called the magnified gradient function (MGF). This technique reduces the time required for training by modifying the standard back propagation (SBP) algorithm. The MGF is shown to classify professional driver fatigue with 81.49% accuracy (80.53% sensitivity, 82.44% specificity) and non-professional driver fatigue with 83.06% accuracy (84.04% sensitivity and 82.08% specificity)",
Design Methods for Multiple-Valued Input Address Generators,A multiple-valued input address generator produces a unique address given a multiple-valued input data vector. This paper presents methods to realize multiple-valued input address generators by multi-level networks of p-input q-output memories. It shows a method to simplify the address generators using an auxiliary memory.,
Enabling ScientificWorkflow Reuse through Structured Composition of Dataflow and Control-Flow,"Data-centric scientific workflows are often modeled as dataflow process networks. The simplicity of the dataflow framework facilitates workflow design, analysis, and optimization. However, modeling ""control-flow intensive"" tasks using dataflow constructs often leads to overly complicated workflows that are hard to comprehend, reuse, and maintain. We describe a generic framework, based on scientific workflow templates and frames, for embedding control-flow intensive subtasks within dataflow process networks. This approach can seamlessly handle complex control-flow without sacrificing the benefits of dataflow. We illustrate our approach with a real-world scientific workflow from the astrophysics domain, requiring remote execution and file transfer in a semi-reliable environment. For such workflows, we also describe a 3-layered architecture based on frames and templates where the top-layer consists of an overall dataflow process network, the second layer consists of a tranducer template for modeling the desired control-flow behavior, and the bottom layer consists of frames inside the template that are specialized by embedding the desired component implementation. Our approach can enable scientific workflows that are more robust (faulttolerance strategies can be defined by control-flow driven transducer templates) and at the same time more reusable, since the embedding of frames and templates yields more structured and modular workflow designs.","Mathematical model,
Processor scheduling,
Data structures,
Genomics,
Bioinformatics,
Computer science,
Robust control,
Data engineering,
Scientific computing,
Laboratories"
New and improved BIST diagnosis methods from combinatorial Group testing theory,"We examine the general problem of built-in-self-test (BIST) diagnosis in digital logic systems. The BIST diagnosis problem has applications that include identification of erroneous test vectors, faulty scan cells, and faulty items. We develop an abstract model of this problem and show a fundamental correspondence to the well-established subject of combinatorial group testing (CGT) (D. Du and F. K. Hwang, Combinatorial Group Testing and Its Applications, 1994). We exploit this new perspective to 1) link existing BIST diagnosis techniques to CGT techniques and provide further insights into existing diagnosis algorithms, 2) improve the performance of diagnosis algorithms, and 3) develop new techniques to address the BIST diagnosis problem. Using the ISCAS'89 benchmarks, we empirically demonstrate the effectiveness of our proposed techniques over existing BIST diagnosis techniques. The vastness of the CGT literature suggests that further improvements from existing research in CGT may be obtained.","Built-in self-test,
Circuit testing,
Circuit faults,
Fault diagnosis,
Logic testing,
System testing,
Automatic testing,
Hardware,
Computer science,
Benchmark testing"
Compression and machine learning: a new perspective on feature space vectors,"The use of compression algorithms in machine learning tasks such as clustering and classification has appeared in a variety of fields, sometimes with the promise of reducing problems of explicit feature selection. The theoretical justification for such methods has been founded on an upper bound on Kolmogorov complexity and an idealized information space. An alternate view shows compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. Thus, compression-based methods are not a ""parameter free"" magic bullet for feature selection and data representation, but are instead concrete similarity measures within defined feature spaces, and are therefore akin to explicit feature vector models used in standard machine learning algorithms. To underscore this point, we find theoretical and empirical connections between traditional machine learning vector models and compression, encouraging cross-fertilization in future work",
A Fast Arbitrary Factor Video Resizing Algorithm,We present a new algorithm for resizing video frames in the discrete cosine transform (DCT) space. We demonstrate that a frame resizing operation can be represented as multiplication by fixed matrices and propose a computation scheme which is applicable to any DCT-based compression method. The proposed approach is general enough to accommodate resizing operations with arbitrary factors conforming to the syntax of 16times16 macroblocks. The approach is shown to possess significant computational gain over the faster known state of the art algorithms while achieving similar picture quality,
A Dual-Rate Kalman Filter for Continuous Glucose Monitoring,"A dual-rate Kalman filter is developed for realtime continuous glucose monitoring. Frequent (5 minute) sampling of a noisy, continuous glucose sensor is used for estimation of glucose and its rate-of-change. Infrequent (8 hour intervals) reference glucose meter samples enable the sensor gain and its rate-of-change to be updated. The dual-rate Kalman filter formulation accounts for uncertainty in both the continuous glucose sensor and the reference glucose meter. The method is tested on simulated and experimental data, confirming its superiority to simple one-point calibration",
Using Attribute-Based Access Control to Enable Attribute-Based Messaging,"Attribute based messaging (ABM) enables message senders to dynamically create a list of recipients based on their attributes as inferred from an enterprise database. Such targeted messaging can reduce unnecessary communications and enhance privacy, but faces challenges in access control. In this paper, we explore an approach to ABM based on deriving access control information from the same attribute database exploited by the addressing scheme. We show how to address three key challenges. First, we demonstrate a manageable access control system based on attributes. Second we show how this can be used with existing messaging systems to provide a practical deployment strategy. Third, we show that such a system can be efficient enough to support ABM for mid-size enterprises. Our implementation can dispatch ABM messages approved by XACML review for an enterprise of at least 60,000 users with only seconds of latency",
Power-Aware Hierarchical Scheduling with Respect to Resource Intermittence in Wireless Grids,"Wireless grids bring more challenging issues of resource allocation and task scheduling, and mobility and power management are major additional concerns in a wireless grid. Based on a proxy-based architecture, a hierarchical scheduling model is proposed to efficiently utilize the energy of wireless nodes with respect to quality of service (QoS). The first level scheduler is responsible for mapping tasks among proxy-nodes and other fixed grid nodes; the second will conduct scheduling in each proxy-centric wireless domain. Under the guidance of four principles suitable for the intermittence, the first level performs overall scheduling based on the FIFS algorithm (first input first service). After modeling a consumed power objective function, a revised min-min heuristic algorithm is enforced to the second level in order to efficiently map tasks to wireless devices. In the latter algorithm, mobile node selection is targeted to minimize the energy consumed due to communication and computation, and its last solution is evaluated to guarantee requirement for the task deadline. This research simulation suggests that the power-aware hierarchical scheduling improves energy utilization of the overall system, and also decreases the ratio of failure of scheduled tasks in wireless grids",
The radon-split method for helical cone-beam CT and its application to nongated reconstruction,"The mathematical analysis of exact filtered back-projection algorithms is strictly related to Radon inversion. We show how filter-lines can be defined for the helical trajectory, which serve for the extraction of contributions of particular kinds of Radon-planes. Due to the Fourier-slice theorem, Radon-planes with few intersections with the helix are associated with low-frequency contributions to transversal slices. This insight leads to different applications of the new method. The application presented here enables the incorporation of an arbitrary amount of redundant data in an approximate way. This means that the back-projection is not restricted to an n-Pi interval. A detailed mathematical analysis, in which we demonstrate how the defined filter-lines work, concludes this paper",
Hierarchical Statistical Learning of Generic Parts of Object Structure,"With the growing interest in object categorization various methods have emerged that perform well in this challenging task, yet are inherently limited to only a moderate number of object classes. In pursuit of a more general categorization system this paper proposes a way to overcome the computational complexity encompassing the enormous number of different object categories by exploiting the statistical properties of the highly structured visual world. Our approach proposes a hierarchical acquisition of generic parts of object structure, varying from simple to more complex ones, which stem from the favorable statistics of natural images. The parts recovered in the individual layers of the hierarchy can be used in a top-down manner resulting in a robust statistical engine that could be efficiently used within many of the current categorization systems. The proposed approach has been applied to large image datasets yielding important statistical insights into the generic parts of object structure.",
On Demand Role Assignment for Event-Detection in Sensor Networks,"In resource-constrained wireless sensor networks (WSNs), communication protocols need to be energyefficient. In this context, information-fusion techniques have been used to reduce packet transmissions, and role assignment algorithms have been proposed to define which nodes should perform information fusion. However, most of the role assignment algorithms for WSNs are proactive in the sense that roles are assigned even when no event is being detected. In this work, we propose a reactive algorithm that assigns roles only when an event is detected, therefore saving energy during the network inactivity. In addition, to save energy during event notification, our algorithm searches for the shortest paths that maximize data aggregation.",
Pulse Contour Variability Before and After Exercise,This paper compares the radial artery pulses of 105 young graduate students. The radial artery pulses after performing progressive ergometer for five minutes are different from those at rest. All the pulses become floating and fast. The contours of pulses have three kinds of variability. The incisures of 39 subjects become especially low; sometimes the incisures are lower than the onset of pulse waveform. The tidal waves and dicrotic waves of 32 subjects become higher. The pulses of 34 subjects become smooth. Their incisures and dicrotic waves become lower. These changes can instruct the exercise and training of the young students and athletes,
"Comments on ""Modeling TCP reno performance: a simple model and its empirical Validation""","In this Comments, several errors in Padhye , 2000, are pointed out. The more serious of these errors result in an over prediction of the send rate. The expression obtained for send rate in this Comments leads to greater accuracy when compared with the measurement data than the originals end rate expression in Padhye et al.",
Layer Minimization of Escape Routing in Area Array Packaging,We devise a central triangular sequence to minimize the escape routing layers in area array packaging. We use a network flow model to analyze the bottleneck of the routable pins. The triangular patterns are generated in a reverse order from the last to the first layer. We demonstrate that the triangular pin sequence maximizes the sum of escape pins in the accumulated layers and thus minimize the number of escape routing layers. A test case is presented to illustrate the approach,
A Novel Blind Watermarking of ECG Signals on Medical Images Using EZW Algorithm,"In this paper, we present a novel blind watermarking method with secret key by embedding ECG signals in medical images. The embedding is done when the original image is compressed using the embedded zero-tree wavelet (EZW) algorithm. The extraction process is performed at the decompression time of the watermarked image. Our algorithm has been tested on several CT and MRI images and the peak signal to noise ratio (PSNR) between the original and watermarked image is greater than 35 dB for watermarking of 512 to 8192 bytes of the mark signal. The proposed method is able to utilize about 15% of the host image to embed the mark signal. This marking percentage has improved previous works while preserving the image details",
Bug Classification Using Program Slicing Metrics,"In this paper, we introduce 13 program slicing metrics for C language programs. These metrics use program slice information to measure the size, complexity, coupling, and cohesion properties of programs. Compared with traditional code metrics based on code statements or code structure, program slicing metrics involve measures for program behaviors. To evaluate the program slicing metrics, we compare them with the Understand for C++ suite of metrics, a set of widely-used traditional code metrics, in a series of bug classification experiments. We used the program slicing and the Understand for C++ metrics computed for 887 revisions of the Apache HTTP project and 76 revisions of the Latex2rtf project to classify source code files or functions as either buggy or bug-free. We then compared their classification prediction accuracy. Program slicing metrics have slightly better performance than the Understand for C++ metrics in classifying buggy/bug-free source code. Program slicing metrics have an overall 82.6% (Apache) and 92% (Latex2rtf) accuracy at the file level, better than the Understand for C++ metrics with an overall 80.4% (Apache) and 88% (Latex2rtf) accuracy. The experiments illustrate that the program slicing metrics have at least the same bug classification performance as the Understand for C++ metrics.",
"Delay-Constrained, Energy-Efficient Routing in Wireless Sensor Networks Through Topology Control","In this paper, we investigate the problem of finding energy-efficient paths for delay-constrained data in wireless sensor networks. This problem has been shown to be NP-complete and current solutions for this problem are inadequate, as they do not model the delays introduced by the channel access mechanisms. We present a heuristic solution for the aforesaid problem that employs topology control for sensor networks using 802.11 like channel access schemes. We propose a network architecture and a routing framework that enable us to model the access delays caused by the MAC layer. This in turn, allows us to obtain better estimates for the end-to-end delays along various paths. We identify a set of paths between the source and sink nodes, and index them in increasing order of their energy consumption. We then estimate the end-to-end delay along each of these ordered paths, and select the one with the lowest index that satisfies the delay constraint. Our studies show that the proposed framework achieves a good balance between latency introduced in the transfer and energy consumption, when compared with conventional solutions",
Emotion Recognition System in Images Based On Fuzzy Neural Network and HMM,"An emotion recognition system based on neuro-HMM was proposed to analyze the emotion contained in images. This system took an initial step in this direction by describing a set of proposed difficulty metrics based on cognitive principles. Both the emotion semanteme extraction and emotion model construction were considered in this system. They were respectively carried out by neural networks and HMM. According to the strong relationship between image notable lines and human dynamism sensation, the system used fuzzy neural network to establish the mapping and obtained the image emotion semanteme sequence. Then the duple hidden Markov model (HMM) was employed to simulate human emotion transition and finally confirmed different emotion models. The system also considered some outer influences to make the system rules be refined in realistic conditions. The experiment shows at least one emotion from an image can be recognized. The results illustrate the capability of the developing image recognition system",
A fast and adaptive test of static equilibrium for legged robots,"A legged robot walking on uneven terrain can avoid falling only by applying contact forces with its feet on the ground that compensate for gravity without causing slip. To plan safe motions, it is necessary to test this constraint at every posture explored at each set of foot placements. Since a huge number of postures may be explored, this test must be as fast as possible. Existing approaches either search explicitly for contact forces at each posture, or precompute the support polygon and check that the robot's center of mass lies above it. This paper presents a new algorithm that is faster than either existing approach. This algorithm is an incremental method of projection, that computes only enough of the support polygon to decide whether static equilibrium is possible at each posture. It takes advantage of information gained testing previous postures in order to test subsequent postures more quickly",
A Contribution Towards Solving the Web Workload Puzzle,"World Wide Web, the biggest distributed system ever built, experiences tremendous growth and change in Web sites, users, and technology. A realistic and accurate characterization of Web workload is the first, fundamental step in areas such as performance analysis and prediction, capacity planning, and admission control. Compared to the previous work, in this paper we present more detailed and rigorous statistical analysis of both request and session level characteristics of Web workload based on empirical data extracted from actual logs of four Web servers. Our analysis is focused on exploring phenomena such as self-similarity, long-range dependence, and heavy-tailed distributions. Identification of these phenomena in real data is a challenging task since the existing methods may perform erratically in practice and produce misleading results. We provide more accurate analysis of long-range dependence of the request and session arrival processes by removing the trend and periodicity. In addition to the session arrival process (i.e., inter-session characteristics), we study several intra-session characteristics using several different methods to test the existence of heavy-tailed behavior and cross validate the results. Finally, we point out specific problems associated with the methods used for establishing long-range dependence and heavy-tailed behavior of Web workloads. We believe that the comprehensive model presented in this paper is a step towards solving the Web workload puzzle",
Mining Relationship Between Video Concepts using Probabilistic Graphical Models,"For large scale automatic semantic video characterization, it is necessary to learn and model a large number of semantic concepts. These semantic concepts do not exist in isolation to each other and exploiting this relationship between multiple video concepts could be a useful source to improve the concept detection accuracy. In this paper, we describe various multi-concept relational learning approaches via a unified probabilistic graphical model representation and propose using numerous graphical models to mine the relationship between video concepts that have not been applied before. Their performances in video semantic concept detection are evaluated and compared on two TRECVID'05 video collections",
Algorithm-based checkpoint-free fault tolerance for parallel matrix computations on volatile resources,"As the size of today's high performance computers increases from hundreds, to thousands, and even tens of thousands of processors, node failures in these computers are becoming frequent events. Although checkpoint/rollback-recovery is the typical technique to tolerate such failures, it often introduces a considerable overhead. Algorithm-based fault tolerance is a very cost-effective method to incorporate fault tolerance into matrix computations. However, previous algorithm-based fault tolerance methods for matrix computations are often derived using algorithms that are seldomly used in the practice of today's high performance matrix computations and have mostly focused on platforms where failed processors produce incorrect calculations. To fill this gap, this paper extends the existing algorithm-based fault tolerance to the volatile computing platform where the failed processor stops working and applies it to scalable high performance matrix computations with two dimensional block cyclic data distribution. We show the practicality of this technique by applying it to the ScaLAPACK/PBLAS matrix-matrix multiplication kernel. Experimental results demonstrate that the proposed approach is able to survive process failures with a very low performance overhead",
Lowest density MDS codes over extension alphabets,"Let F be a finite field and b be a positive integer. A construction is presented of codes over the alphabet F/sup b/ with the following three properties: i) the codes are maximum-distance separable (MDS) over F/sup b/, ii) they are linear over F, and iii) they have systematic generator and parity-check matrices over F with the smallest possible number of nonzero entries. Furthermore, for the case F=GF(2), the construction is the longest possible among all codes that satisfy properties i)-iii).",
Using Sex Differences to Link Spatial Cognition and Program Comprehension,"Spatial cognition and program development have both been examined using contrasting models. We suggest that sex-based differences in one's perception of risk is the key to relating these models. Specifically, the survey map approach to navigation and the top-down development/comprehension strategy use similar and related high risk cognitive skills that males show a preference towards. Conversely, the route-based approach to navigation and the bottom-up development/comprehension strategy use similar and related low risk cognitive skills that women show a preference towards. On the assumption that programmers are consistent in their risk-taking behaviours, we believe that they, as much as possible, tend to use the same strategy when performing program development and comprehension. In an experimental setting, we compare programmer's performance on spatial cognition and program comprehension tasks. The correlations that we found suggest that programmers use equivalently risky strategies for program comprehension and spatial cognition. Thus, there is evidence that similar cognitive skills are used for spatial cognition and program comprehension/development, and that the similarities are a consequence of sex-based differences in risk-taking behaviour",
Minimum risk thresholds for data with heavy noise,"In the estimation of data with many zeros (sparse data), such as wavelet coefficients, thresholding is a common technique. This letter investigates the behavior of the minimum risk threshold for large values of the noise standard deviation. It finds that the threshold depends quadratically on the noise standard deviation. The relevance of this result is situated in the context of both Bayesian and universal thresholding.","Bayesian methods,
Computer science,
Wavelet coefficients,
Gene expression,
Image denoising,
Mathematics,
Analysis of variance,
Additives,
Random variables"
An agent-oriented approach to change propagation in software evolution,"Software maintenance and evolution are inevitable activities since almost all software that is useful and successful stimulates user-generated requests for change and improvements. One of the most critical problems in software maintenance and evolution is to maintain consistency between software artefacts by propagating changes correctly. Although many approaches have been proposed, automated change propagation is still a significant technical challenge in software engineering. In this paper we present a novel, agent-oriented approach to deal with change propagation in evolving software systems that are developed using the Prometheus methodology. A meta-model with a set of the object constraint language (OCL) rules forms the basis of the proposed framework. The underlying change propagation mechanism of our framework is based on the well-known Belief-Desire-Intention (BDI) agent architecture. Traceability information and design heuristics are also incorporated into the framework to facilitate the change propagation process.",
Speeding-up multi-robot exploration by considering semantic place information,"In this paper, we consider the problem of exploring an unknown environment with a team of mobile robots. One of the key issues in multi-robot exploration is how to assign target locations to the individual robots. To better distribute the robots over the environment and to avoid redundant work, we take into account the type of place a potential target is located in (e.g., a corridor or a room). To determine the type of a place, we apply a classifier learned with AdaBoost which additionally considers spatial dependencies between nearby locations. Our approach to incorporate the type of places in the coordination of the robots has been implemented and tested in different environments. The experiments demonstrate that our system effectively distributes the robots over the environment and allows them to accomplish their mission faster compared to approaches that ignore the semantic place labels","Robot kinematics,
Robot sensing systems,
Mobile robots,
Interference,
Collaboration,
Computer science,
Testing,
Reconnaissance,
Cleaning,
Collaborative work"
Detecting Text in Videos Using Fuzzy Clustering Ensembles,"Detection and localization of text in videos is an important task towards enabling automatic content-based retrieval of digital video databases. However, since text is often displayed against a complex background, its detection is a challenging problem. In this paper, a novel approach based on fuzzy cluster ensemble techniques to solve this problem is presented. The advantage of this approach is that the fuzzy clustering ensemble allows the incremental inclusion of temporal information regarding the appearance of static text in videos. Comparative experimental results for a test set of 10.92 minutes of video sequences have shown the very good performance of the proposed approach with an overall recall of 92.04% and a precision of 96.71%",
A Parallel Apriori Algorithm for Frequent Itemsets Mining,"Finding frequent itemsets is one of the most investigated fields of data mining. The Apriori algorithm is the most established algorithm for frequent itemsets mining (FIM). Several implementations of the Apriori algorithm have been reported and evaluated. One of the implementations optimizing the data structure with a trie by Bodon catches our attention. The results of the Bodon's implementation for finding frequent itemsets appear to be faster than the ones by Borgelt and Goethals. In this paper, we revised Bodon's implementation into a parallel one where input transactions are read by a parallel computer. The effect a parallel computer on this modified implementation is presented","Itemsets,
Data mining,
Transaction databases,
Concurrent computing,
Association rules,
Parallel processing,
Application software,
Drives,
Computer science,
Data structures"
Identifying FPGA IP-Cores Based on Lookup Table Content Analysis,"In this paper we introduce a new method to identify IP cores in an FPGA by analyzing the content of lookup tables. This technique can be used to identify registered cores for IP protection against unlicensed usage. We show methods to extract the content of the lookup tables in a design from a binary bitfile of Xilinx Virtex-II and Virtex-II Pro FPGAs. To identify a core, we compare the number of unique functions from lookup tables of the core with the lookup tables extracted from a product with an FPGA from an accused company. Also placement information can be used for increasing the reliability of the result. With these methods, no additional sources or information must be inquired from the accused company. These techniques can be used for netlist and bitfile cores, so a wide spectrum of cores can be identified.",
Location-based intelligence - modeling behavior in humans using GPS,"This paper introduces the notion of location-based intelligence by tracking the spatial properties and behavior of a single civilian participant over a two-week study period using a global positioning system (GPS) receiver, and displaying them on a geographic information system (GIS). The paper clearly shows the power of combining speed (S), distance (D), time (T) and elevation (E) data with the exact longitude and latitude position of the user. The issues drawn from the observation and the civilian's personal diary are useful in understanding the social implications of tracking and monitoring objects and subjects using GPS. The findings show that while GPS has been used in some very innovative ways, there are a plethora of ethical dilemmas associated with its use on civilians, even if they are requesting a given service and paying for its utilization. From the information recorded during the field observation, a number of inherent technical limitations in GPS were identified which add to the complexity of such related areas as law and commerce. In conclusion, while the benefits of GPS for specific applications is apparent, safeguards need to be put in place to ensure that information gathered by the GPS is not misused or abused. One can envisage that if the wrong hands obtain location information records for individual subscribers that the potential exposure and risk might be even greater than that of stolen credit cards.","Humans,
Global Positioning System,
Geographic Information Systems,
Computer science,
Monitoring,
Business,
Credit cards,
Data privacy,
Surveillance,
Tracking"
A General Model and Analysis of Physical Layer Capture in 802.11 Networks,,
Object Pose Detection in Range Scan Data,"We address the problem of detecting complex articulated objects and their pose in 3D range scan data. This task is very difficult when the orientation of the object is unknown, and occlusion and clutter are present in the scene. To address the problem, we design an efficient probabilistic framework, based on the articulated model of an object, which combines multiple information sources. Our framework enforces that the surfaces and edge discontinuities of model parts are matched well in the scene while respecting the rules of occlusion, that joint constraints and angles are maintained, and that object parts don’t intersect. Our approach starts by using low-level detectors to suggest part placement hypotheses. In a hypothesis enrichment phase, these original hypotheses are used to generate likely placement suggestions for their neighboring parts. The probabilities over the possible part placement configurations are computed using efficient OpenGL rendering. Loopy belief propagation is used to optimize the resulting Markov network to obtain the most likely object configuration, which is additionally refined using an Iterative Closest Point algorithm adapted for articulated models. Our model is tested on several datasets, where we demonstrate successful pose detection for models consisting of 15 parts or more, even when the object is seen from different viewpoints, and various occluding objects and clutter are present in the scene.","Object detection,
Layout,
Markov random fields,
Detectors,
Belief propagation,
Object recognition,
Computer science,
Iterative closest point algorithm,
Testing,
Humans"
An agenda based mobility model,"Mobility modeling is important in wireless and mobile networking research due to the fact that few real large scale networks is available for performance evaluation. While existing models only take the geographical movements into consideration, we emphasize in our model the importance of human's social roles. We use a person's social activities to drive and model a node's movements. In this paper, we propose an agenda based mobility model which combine both the social activities and the geographic movements. In the design, we use a constructive modeling approach to allow the model to well conform to the real world and to be easy to use. According to our model, each node moves based on its individual agenda, which includes all the activities (when, where and what) on a specific day. To proceed from one activity to another, a node moves from one road address to another address. In deciding each agenda item, we use NHTS survey data to obtain activity distribution, occupation distribution and dwell time distribution. We use simulation to show the network topology generated by the model and ad hoc network routing performance impacted by the model.",
Synthesis of Fault-Tolerant Schedules with Transparency/Performance Trade-offs for Distributed Embedded Systems,"In this paper we present an approach to the scheduling of fault-tolerant embedded systems for safety-critical applications. Processes and messages are statically scheduled, and we use process re-execution for recovering from multiple transient faults. If process recovery is performed such that the operation of other processes is not affected, we call it transparent recovery. Although transparent recovery has the advantages of fault containment, improved debuggability and less memory needed to store the fault-tolerant schedules, it will introduce delays that can violate the timing constraints of the application. We propose a novel algorithm for the synthesis of fault-tolerant schedules that can handle the transparency/performance trade-offs imposed by the designer, and makes use of the fault-occurrence information to reduce the overhead due to fault tolerance. We model the application as a conditional process graph, where the fault occurrence information is represented as conditional edges and the transparent recovery is captured using synchronization nodes",
PageSim: A Novel Link-Based Similarity Measure for the World Wide Web,"The requirement for measuring the similarity between Web pages arises in many applications on the Web, such as Web searching engine and Web document classification. According to the unique characteristics of the Web, which are huge, rapidly growing, high dynamic, and untrustworthy, we propose a novel link-based similarity measure called PageSim. Based on the strategy of PageRank score propagation, PageSim is efficient, scalable, stable, and ""fairly"" robust, and therefore is applicable to the Web. We present intuitions behind the PageSim model, and outline the model with mathematical definitions. We also suggest the pruning technique for efficient computation of PageSim scores, and conduct experiments to illustrate the effectiveness and specialities of PageSim",
H/sub /spl infin// control of differential linear repetitive processes,"Repetitive processes are a distinct class of two-dimensional (2-D) systems (i.e., information propagation in two independent directions) of both systems theoretic and applications interest. They cannot be controlled by direct extension of existing techniques from either standard [termed one-dimensional (1-D) here] or 2-D systems theory. Here, we give new results on the relatively open problem of the design of control laws using an Hinfin setting. These results are for the sub-class of so-called differential linear repetitive processes which arise in applications",
Scenario-driven dynamic analysis for comprehending large software systems,"Understanding large software systems is simplified when a combination of techniques for static and dynamic analysis is employed. Effective dynamic analysis requires that execution traces be generated by executing scenarios that are representative of the system's typical usage. This paper presents an approach that uses dynamic analysis to extract views of a software system at different levels, namely (1) use cases views, (2) module interaction views, and (3) class interaction views. The proposed views can be used to help maintainers locate features to be changed. The proposed approach is evaluated against a large software system, the Mozilla Web browser",
SLA-Based Coordinated Superscheduling Scheme for Computational Grids,"The service level agreement (SLA) based grid superscheduling approach promotes coordinated resource sharing. Superscheduling is facilitated between administratively and topologically distributed grid sites via grid schedulers such as resource brokers and workflow engines. In this work, we present a market-based SLA coordination mechanism, based on a well known contract net protocol. The key advantages of our approach are that it allows: (i) resource owners to have finer degree of control over the resource allocation which is something that is not possible with traditional mechanisms; and (ii) superschedulers to bid for SLA contracts in the contract net, with focus on completing a job within a user specified deadline. In this work, we use simulation to show the effectiveness of our proposed approach",
The security of the FDH variant of Chaum's undeniable signature scheme,"In this paper, a new kind of adversarial goal called forge-and-impersonate in undeniable signature schemes is introduced. Note that forgeability does not necessarily imply impersonation ability. The security of the full-domain hash (FDH) variant of Chaum's undeniable signature scheme is then classified according to three dimensions, the goal of adversaries, the attacks, and the zero-knowledge (ZK) level of confirmation and disavowal protocols. Each security is then related to some well-known computational problem. In particular, the security of the FDH variant of Chaum's scheme with noninteractive zero-knowledge (NIZK) protocol confirmation and disavowal protocols is proven to be equivalent to the computational Diffie-Hellman (CDH) problem, as opposed to the gap Diffie-Hellman (GDH) problem as claimed by Okamoto and Pointcheval.","Protocols,
Security,
Public key cryptography,
Application software,
Licenses,
Digital signatures,
Electronic voting,
Computer science,
Information science"
Modeling and execution of complex attack scenarios using interval timed colored Petri nets,"The commonly used flaw hypothesis model (FHM) for performing penetration tests provides only limited, high level guidance for the derivation of actual penetration attempts. In this paper, a mechanism for the systematic modeling, simulation, and exploitation of complex multistage and multiagent vulnerabilities in networked and distributed systems based on stochastic and interval-timed colored Petri nets is described and analyzed through case studies elucidating several properties of Petri net variants and their suitability to modeling this type of attack",
Towards Face Recognition at a Distance,"Current face recognition algorithms require the tacit cooperation of users, who must position themselves in a small area of space and face the camera. Face recognition in uncontrolled conditions, such as in security camera footage presents two extra challenges. First, it is difficult to capture good quality images of faces in this setting. Second, the pose of the face is relatively uncontrolled which causes most face recognition algorithms to fail. In this paper, we present a series of solutions to address these problems. High quality face images are captured using a foveated wide field sensor, in which a narrow-field camera is directed towards faces using information from a static wide-field camera. Feature points corresponding to the eyes/nose etc. are accurately localized and face shape is normalized. A novel algorithm is introduced to identify these (typically non-frontal) faces from a test gallery of frontal faces. Results are demonstrated to be superior to contemporary approaches",
A novel virtual FDTD-based microstrip circuit design and analysis tool [Education Column],"A novel FDTD-based virtual education/research tool is introduced, and characteristic examples are presented. The beauty of this tool comes from its visualization power, as well as easy-to-use design steps. The tool can be used effectively in microstrip circuit analysis and design problems",
SecRout: a secure routing protocol for sensor networks,"In this paper, we present a secure routing protocol for sensor networks (SecRout) to safeguard sensor networks under different types of attacks. The SecRout protocol uses the symmetric cryptography to secure messages, and uses a small cache in sensor nodes to record the partial routing path (previous and next nodes) to the destination. It guarantees that the destination will be able to identify and discard the tampered messages and ensure that the messages received are not tampered. Comparing the performance with non-secure routing protocol AODV (ad hoc on demand distance vector routing), the SecRout protocol only has a small byte overhead (less than 6%), but packet delivery ratio is almost same as AODV and packet latency is better than AODV after the route discovery.",
'Doing the right thing wrong' - Personality and tolerance to uncomfortable robot approaches,"The study presented in this paper explored the relationships between subject personality and preferences in the direction from which a robot approached the human participants (N=42) in order to deliver an object in a naturalistic `living room' setting. Personality was assessed using the Big Five Domain Scale. No consistent significant relationships were found between personality traits and preferred approach directions; however, a consistent nonsignificant trend was found in which high scores on the personality trait extraversion was associated with a higher degree of tolerance to the approach directions rated overall as most uncomfortable. The implications of the results are discussed both from a theoretical and methodological viewpoint",
Specification of Software Component Requirements Using the Trace Function Method,"This paper describes the application of the Trace Function Method to specify the requirements of a software component. We illustrate the method on a software component of a telecommunications system that was developed by Ericsson. Beginning with incomplete informal descriptions, we analysed the requirements of the system and wrote a description that contains all pertinent information in one easily used reference document. The resulting documentation is more compact and complete than traditional software documentation and provides precise information that will be useful for testing and inspection.",
Maestro-VC: a paravirtualized execution environment for secure on-demand cluster computing,"Virtualization, a technology first developed for partitioning the resources of mainframe computers, has seen a resurgence in popularity in the realm of commodity workstation computers. This paper introduces Maestro-VC, a system which explores a novel use of VMs as the building blocks of entire virtual clusters (VCs). Virtualization of entire clusters is beneficial because existing parallel code can run without modification in the virtual environment. At the same time, inserting a layer of software between a virtual cluster and native hardware allows for security enforcement and flexible resource management in a manner transparent to running parallel code. In this paper we describe the design and implementation of Maestro-VC, and give the results of some preliminary performance experiments",
Global Convergence of Decomposition Learning Methods for Support Vector Machines,"Decomposition methods are well-known techniques for solving quadratic programming (QP) problems arising in support vector machines (SVMs). In each iteration of a decomposition method, a small number of variables are selected and a QP problem with only the selected variables is solved. Since large matrix computations are not required, decomposition methods are applicable to large QP problems. In this paper, we will make a rigorous analysis of the global convergence of general decomposition methods for SVMs. We first introduce a relaxed version of the optimality condition for the QP problems and then prove that a decomposition method reaches a solution satisfying this relaxed optimality condition within a finite number of iterations under a very mild condition on how to select variables",
Asynchronous zero-copy communication for synchronous sockets in the sockets direct protocol (SDP) over InfiniBand,"Sockets direct protocol (SDP) is an industry standard pseudo sockets-like implementation to allow existing sockets applications to directly and transparently take advantage of the advanced features of current generation networks such as InfiniBand. The SDP standard supports two kinds of sockets semantics, viz., synchronous sockets (e.g., used by Linux, BSD, Windows) and asynchronous sockets (e.g., used by Windows, upcoming support in Linux). Due to the inherent benefits of asynchronous sockets, the SDP standard allows several intelligent approaches such as source-avail and sink-avail based zero-copy for these sockets. Unfortunately, most of these approaches are not beneficial for the synchronous sockets interface. Further, due to its portability, ease of use and support on a wider set of platforms, the synchronous sockets interface is the one used by most sockets applications today. Thus, a mechanism by which the approaches proposed for asynchronous sockets can be used for synchronous sockets is highly desirable. In this paper, we propose one such mechanism, termed as AZ-SDP (asynchronous zero-copy SDP), where we memory-protect application buffers and carry out communication asynchronously while maintaining the synchronous sockets semantics. We present our detailed design in this paper and evaluate the stack with an extensive set of benchmarks. The experimental results demonstrate that our approach can provide an improvement of close to 35% for medium-message unidirectional throughput and up to a factor of 2 benefit for computation-communication overlap tests and multi-connection benchmarks","Sockets,
Protocols,
Linux,
Benchmark testing,
Protection,
Computer science,
Computer industry,
Application software,
DC generators,
Throughput"
Executable acceptance tests for communicating business requirements: customer perspective,"Using an experimental method, we found that customers, partnered with an IT professional, are able to use executable acceptance test (storytest)-based specifications to communicate and validate functional business requirements. However, learnability and ease of use analysis indicates that an average customer may experience difficulties learning the technique. Several additional propositions are evaluated and usage observations made",
A 40 Gb/s 3R Burst Mode Receiver with 4 integrated MZI switches,"We demonstrate for the first time a 40 Gb/s all-optical 3R burst-mode receiver error-free operation for 9,3 dB power fluctuation between short bursty packets. It consists of a sequence of four integrated MZI switches.",
Dynamic task allocation for robots via auctions,We present an auction-based method for the allocation of tasks to a group of robots. The robots operate in a 2D environment for which they have a map. Tasks are locations in the map that have to be visited by the robots. Unexpected obstacles and other delays may prevent a robot from being able to complete its allocated tasks. Therefore tasks not yet achieved are rebid every time a robot accomplishes a task. This provides an opportunity to improve the allocation of the remaining tasks and to reduce the overall task completion time. We present experimental results that we have obtained in simulation using Player/Stage with this task allocation mechanism,
A Stable SRAM Cell Design Against Simultaneously R/W Disturbed Accesses,"A guarantee obligation of keeping the cell-margin against a simultaneously read and write (R/W) disturbed accesses in the same column is required to a 2-port SRAM. We verified that it is difficult to provide these margins without any decrease in cell-current and any increase in cell-area penalty only by using the previously proposed techniques so far. To solve this, we have developed the new cell design technology for an 8-Tr 2-port cell in a 65-nm CMOS technology and have demonstrated that the R/W margins can be improved by 45%/70%, respectively at 0.9V, and the cell-size can be reduced by 20% compared with the conventional column-based Vdd control. Another 7-Tr cell which can reduce cell-area by 31% has been also demonstrated","Random access memory,
CMOS technology,
Degradation,
MOS devices,
Computer science,
Semiconductor device noise,
Power control,
Variable structure systems"
Robust GMM Based Gender Classification using Pitch and RASTA-PLP Parameters of Speech,"A novel gender classification system has been proposed based on Gaussian mixture models, which apply the combined parameters of pitch and 10th order relative spectral perceptual linear predictive coefficients to model the characteristics of male and female speech. The performances of gender classification system have been evaluated on the conditions of clean speech, noisy speech and multi-language. The simulations show that the performance of the proposed gender classifier is excellent; it is very robust for noise and completely independent of languages; the classification accuracy is as high as above 98% for all clean speech and remains 95% for most noisy speech, even the SNR of speech is degraded to OdB",
New computational approaches to the analysis of interbeat intervals in human subjects,"Complex, self-regulating systems such as the human heart must process inputs with a broad range of characteristics to generate physiological data and time series. Many of these physiological time series seem to be highly chaotic, represent nonstationary data, and fluctuate in an irregular and complex manner. One hypothesis is that the seemingly chaotic structure of physiological time series arises from external and intrinsic perturbations that push the system away from a homeostatic set point. An alternative hypothesis is that the fluctuations are due, at least in part, to the system's underlying dynamics. In this review, we describe new computational approaches - based on new theoretical concepts - for analyzing physiological time series. We'll show that the application of these methods could potentially lead to a novel diagnostic tool for distinguishing healthy individuals from those with congestive heart failure (CHF)",
Low-Cost Hardening of Image Processing Applications Against Soft Errors,"Image processing systems are increasingly used in safety-critical applications, and their hardening against soft errors becomes an issue. The authors propose a methodology to identify soft errors as uncritical based on their impact on the system's functionality. The authors call a soft error uncritical if its impact is provably limited to image perturbations during a very short period of time (number of cycles) and the system is guaranteed to recover thereafter. Uncritical errors do not require hardening as their effects are unperceivable for the human user of the system. The authors focus on soft errors in the motion estimation subsystem of MPEG-2 and introduce different definitions of uncritical soft errors in that subsystem. A method is proposed to automatically determine uncritical errors and provide experimental results for various parameters. The concept can be adapted to further systems and enhance existing methods",
Epicardial ECG Mapping of Human Ventricular Fibrillation,"Although the mechanisms of ventricular fibrillation (VF) have been studied extensively in animal hearts, relatively few studies have investigated VF in the human heart. The aim of this study was to record epicardial activity during early human VF and analyse the electrical signals in order to characterise the complexity of activity and to estimate the number of reentrant sources driving the VF. In 10 patients undergoing routine cardiac surgery, VF was induced by burst pacing and a 20-40 s episode of fibrillatory activity was recorded from a dense array of unipolar electrodes spanning the entire epicardium. Dominant frequencies were computed using the fast Fourier transform. Activation times were computed at the minimum negative slopes of the electrograms. Phase singularities and activation wavefronts were identified and quantified. We found that early human VF was typically sustained by a small number of re-entrant sources that were associated with large and coherent wavefronts. However, at times we also observed a large number of small wavefronts. Our results suggest that multiple electrophysiological mechanisms sustain VF in the human heart.",
Explanatory and illustrative visualization of special and general relativity,"This paper describes methods for explanatory and illustrative visualizations used to communicate aspects of Einstein's theories of special and general relativity, their geometric structure, and of the related fields of cosmology and astrophysics. Our illustrations target a general audience of laypersons interested in relativity. We discuss visualization strategies, motivated by physics education and the didactics of mathematics, and describe what kind of visualization methods have proven to be useful for different types of media, such as still images in popular science magazines, film contributions to TV shows, oral presentations, or interactive museum installations. Our primary approach is to adopt an egocentric point of view: the recipients of a visualization participate in a visually enriched thought experiment that allows them to experience or explore a relativistic scenario. In addition, we often combine egocentric visualizations with more abstract illustrations based on an outside view in order to provide several presentations of the same phenomenon. Although our visualization tools often build upon existing methods and implementations, the underlying techniques have been improved by several novel technical contributions like image-based special relativistic rendering on GPUs, special relativistic 4D ray tracing for accelerating scene objects, an extension of general relativistic ray tracing to manifolds described by multiple charts, GPU-based interactive visualization of gravitational light deflection, as well as planetary terrain rendering. The usefulness and effectiveness of our visualizations are demonstrated by reporting on experiences with, and feedback from, recipients of visualizations and collaborators.",
Layered HMM for Motion Intention Recognition,"Acquiring, representing and modeling human skills is one of the key research areas in teleoperation, programming-by-demonstration and human-machine collaborative settings. One of the common approaches is to divide the task that the operator is executing into several subtask in order to provide manageable modeling. In this paper we consider the use of a layered hidden Markov model (LHMM) to model human skills. We evaluate a gestem classifier that classifies motions into basic action-primitives, or gestems. The gestem classifiers are then used in a LHMM to model a simulated teleoperated task. We investigate the online and offline classification performance with respect to noise, number of gestems, type of HMM and the available number of training sequences. We also apply the LHMM to data recorded during the execution of a trajectory-tracking task in 2D and 3D with a robotic manipulator in order to give qualitative as well as quantitative results for the proposed approach. The results indicate that the LHMM is suitable for modeling teleoperative trajectory-tracking tasks and that the difference in classification performance between one and multi-dimensional HMMs for gestem classification are small. It can also be seen that the LHMM is robust w.r.t misclassifications in the underlying gestem classifiers",
Algorithms for Determining the Demand-Based Load of a Sporadic Task System,"The load parameter of a sporadic task system is defined to be the largest possible cumulative execution requirement that can be generated by jobs of the task system over any time interval, normalized by the length of the interval. This parameter is known to play a very important role in the uniprocessor feasibility analysis of sporadic task systems. In this paper, it is shown that the load of a sporadic task system may be used as an accurate indicator of its feasibility upon preemptive multiprocessors as well. Exact algorithms, and approximate ones that can be guaranteed to be accurate to within an arbitrary additive error > 0, for computing a task system's load are presented and proven correct. The performance of these algorithms is evaluated by simulation over randomly generated task systems",
Cascaded Three-Level Inverters with Synchronized Space-Vector Modulation,"Novel method of synchronized pulse width modulation (PWM) has been disseminated for control of cascaded neutral-point-clamped inverters feeding open-end winding induction motor drive with single dc voltage source. Control algorithms provide in this case full elimination of the common-mode voltages both in each inverter and in the load. Phase voltages of the drive system are characterised by quarter-wave symmetry during the whole control range including the zone of overmodulation, and its spectra do not contain even harmonics and sub-harmonics. Simulations give the behaviour of the systems with synchronized PWM in both undermodulation and overmodulation zones. Continuous, discontinuous and ""direct-direct"" schemes of synchronized PWM, applied for control of cascaded three-level converters, have been analyzed and compared",
"A 1.2V, 2.4GHz Fully Integrated Linear CMOS Power Amplifier with Efficiency Enhancement","A 2.4GHz power amplifier is implemented with standard thin-oxide transistors in a 1.2V, 0.13 mum CMOS process. The output matching network is fully integrated on chip. The PA transmits up to 24dBm linear power with 25% drain efficiency at -1dB compression point. When driven into saturation, it transmits 27dBm peak power with 32% drain efficiency. A technique for enhancing average efficiency is proposed and demonstrated. This technique does not degrade instantaneous efficiency at peak power and maintains constant power gain with power back-off",
An application of information theory to intrusion detection,"Zero-day attacks, new (anomalous) attacks exploiting previously unknown system vulnerabilities, are a serious threat. Defending against them is no easy task, however. Having identified ""degree of system knowledge"" as one difference between legitimate and illegitimate users, theorists have drawn on information theory as a basis for intrusion detection. In particular, Kolmogorov complexity (K) has been used successfully. In this work, we consider information distance (Observed_K - Expected_K) as a method of detecting system scans. Observed_K is computed directly, Expected_K is taken from compression tests shared herein. Results are encouraging. Observed scan traffic has an information distance at least an order of magnitude greater than the threshold value we determined for normal Internet traffic. With 320 KB packet blocks, separation between distributions appears to exceed 4sigma",
Human Behavior Based Predictive Brake Assistance,"Driver assistance systems have both the potential to alert the driver to critical situations and distract or annoy the driver if the driver is already aware of the situation. As systems attempt to preemptively warn drivers more and more in advance, this problem becomes exacerbated. We present a predictive braking assistance system that identifies not only the need for braking action, but also whether or not a braking action is being planned by the driver. Our system uses a Bayesian framework to determine the criticality of the situation by assessing (1) the probability that braking should be performed given observations of the vehicle and surround and (2) the probability that the driver intends to perform a braking action. We train and evaluate our system using over 22 hours of data collected from real driving scenarios with 28 different drivers",
Antenna avoidance in layer assignment,"The sustained progress of very-large-scale-integration (VLSI) technology has dramatically increased the likelihood of the antenna problem in the manufacturing process and calls for corresponding considerations in the routing stage. In this paper, the authors propose a technique that can handle the antenna problem during the layer-assignment (LA) stage, which is an important step between global routing and detailed routing. The antenna-avoidance problem is modeled as a tree-partitioning problem with a linear-time-optimal-algorithm solution. This algorithm is customized to guide antenna avoidance in the LA stage. A linear-time optimal jumper-insertion algorithm is also derived. Experimental results on benchmark circuits show that the proposed techniques can lead to an average of 76% antenna-violation reduction and 99% via-violation reduction.",
Human Driving Behavior Recognition Based on Hidden Markov Models,"Automobiles are by now indispensable to our personal lives, but the problem of car thefts threatens the automobile security seriously. In this paper we present an intelligent vehicle security system for handling the vehicle theft problem under the framework of modeling dynamic human behaviors. We propose to recognize the drivers through their driving performances and hope this can help reduce the number of car thefts significantly. Firstly we describe our experimental system-a real time graphic driving simulator-for collecting and modeling human driving behaviors. Using the proposed machine learning method hidden Markov model (HMM), the individual driving behavior model is derived and then we demonstrate the procedure for recognizing different drivers through analyzing the corresponding models. Then we define performance measures for evaluating our resultant learning models using a hidden-Markov-model-(HMM)-based similarity measure, which helps us to derive the similarity of individual behavior and corresponding model. The experimental results of learning algorithms and evaluations are described and finally verify that the proposed method is valid and useful against the vehicle thefts problem.",
Designing open-loop plans for planar micro-manipulation,"This paper describes a test-bed for planar micro manipulation tasks and a framework for planning based on quasi-static models of mechanical systems with frictional contacts. We show how planar peg-in-the-hole assembly tasks can be designed using randomized motion planning techniques with Mason's models for quasi-static manipulation. Finally, we present simulation and experimental results in support of our methodology",
Applications of Discrete-Event Simulation to Support Manufacturing Logistics Decision-Making: A Survey,"This paper presents a literature survey on recent use of discrete-event simulation in real-world manufacturing logistics decision-making. The sample of the survey consists of 52 relevant application papers from recent Winter Simulation Conference proceedings. We investigated what decisions were supported by the applications, case company characteristics, some methodological issues, and the software tools used. We found that the majority of applications has been reported in production plant design and in the evaluation of production policies, lot sizes, WIP levels and production plans/schedules. Findings also suggest that general-purpose DES software tools are suitable in most of these cases. For different possible reasons, few applications for multi-echelon supply chain decision-making have been reported. Software requirements for supply chain simulations also seem to differ slightly from those for established application areas. The applications described were carried out in a variety of different industries, with a clear predominance in the semiconductor and automotive industries",
The TactaPack: A Wireless Sensor/Actuator Package for Physical Therapy Applications,"In this paper, we present preliminary work we have done on designing the TactaPack, a wearable sensor/actuator device that uses a Bluetooth wireless connection to return sensor data to a host, and to receive commands to initiate expressive vibrotactile stimuli. We present our work in the context of a physical therapy application designed to provide more autonomy for patients when performing rehabilitative exercises. This assistive technology has the potential to reduce injuries during therapy due to improper patient joint movement, and decrease the workload of physical therapists, thereby reducing healthcare costs. Though still in the early stages of design, we believe the TactaPack can be used to produce systems that are less cumbersome than current, wired solutions, and simplify the creation of high-level applications by offloading from the CPU to the device the process of sensing, testing against threshold values, and actuation.",
Impact of Heterogeneity on Coverage and Broadcast Reachability in Wireless Sensor Networks,"While most existing research efforts in the area of wireless sensor networks have focused on networks with identical nodes, deploying sensors with different capabilities has become a feasible choice. In this paper, we focus on sensor networks with two types of nodes that differ in their capabilities, and discuss the effects of heterogeneity of sensing and transmission ranges on the network coverage and broadcast reachability. Our work characterizes how the introduction of a few sensor nodes with better capabilities can reduce the number of total required sensors without sacrificing the coverage and the broadcast reachability. Analytical results are validated via simulations. This work can serve as a guideline for designing large-scale sensor networks cost-effectively. It can also be extended to more complicated heterogeneous wireless sensor networks with more than two types of sensors.","Broadcasting,
Wireless sensor networks,
Sensor phenomena and characterization,
Quality of service,
Computer networks,
Distributed computing,
Monitoring,
Surveillance,
Costs,
Mobile computing"
Logical topology design for dynamic traffic grooming in WDM optical networks,"Traffic grooming in optical networks refers to consolidation of subwavelength client connections onto lightpaths. Depending on whether client connections are given in advance or randomly arrive/depart, traffic grooming is classified as static and dynamic. Dynamic traffic grooming has been traditionally performed through establishing/releasing lightpaths online. In this paper, the authors propose an alternate approach to design a static logical topology a priori and then route randomly arriving client connections on it to avoid frequent lightpath setup/teardown. Two problems are considered: 1) minimize resource usage constrained by traffic blocking requirements and 2) maximize performance constrained by given resources. These are formulated as integer linear-programming (ILP) problems. The numerical results show that the resource usage dramatically decreases when the blocking requirement is relaxed, and the grooming performance slowly increases when given more resources. In addition, the number of ports at client nodes has more profound impact on traffic grooming than the number of wavelengths.",
Visualization of Fibrous and Thread-like Data,"Thread-like structures are becoming more common in modern volumetric data sets as our ability to image vascular and neural tissue at higher resolutions improves. The thread-like structures of neurons and micro-vessels pose a unique problem in visualization since they tend to be densely packed in small volumes of tissue. This makes it difficult for an observer to interpret useful patterns from the data or trace individual fibers. In this paper we describe several methods for dealing with large amounts of thread-like data, such as data sets collected using knife-edge scanning microscopy (KESM) and serial block-face scanning electron microscopy (SBF-SEM). These methods allow us to collect volumetric data from embedded samples of whole-brain tissue. The neuronal and microvascular data that we acquire consists of thin, branching structures extending over very large regions. Traditional visualization schemes are not sufficient to make sense of the large, dense, complex structures encountered. In this paper, we address three methods to allow a user to explore a fiber network effectively. We describe interactive techniques for rendering large sets of neurons using self-orienting surfaces implemented on the GPU. We also present techniques for rendering fiber networks in a way that provides useful information about flow and orientation. Third, a global illumination framework is used to create high-quality visualizations that emphasize the underlying fiber structure. Implementation details, performance, and advantages and disadvantages of each approach are discussed","Data visualization,
Scanning electron microscopy,
Computer science,
Neurons,
Lighting,
Data acquisition,
Image segmentation,
Rendering (computer graphics),
Hardware"
Collective Motion of Self-Propelled Particles: Stabilizing Symmetric Formations on Closed Curves,"We provide feedback control laws to stabilize formations of multiple, unit speed particles on smooth, convex, and closed curves with definite curvature. As in previous work we exploit an analogy with coupled phase oscillators to provide controls which isolate symmetric particle formations that are invariant to rigid translation of all the particles. In this work, we do not require all particles to be able to communicate; rather we assume that inter-particle communication is limited and can be modeled by a fixed, connected, and undirected graph. Because of their unique spectral properties, the Laplacian matrices of circulant graphs play a key role. The methodology is demonstrated using a superellipse, which is a type of curve that includes circles, ellipses, and rounded rectangles. These results can be used in applications involving multiple autonomous vehicles that travel at constant speed around fixed beacons",
Parallel Web Service Composition in MoSCoE: A Choreography-Based Approach,"We present a goal-driven approach to model a choreographer for realizing composite Web services. In this framework, the users start with an abstract, and possibly incomplete functional specification of a desired goal service. This specification is used to compose a choreographer that allows communication between the client and the set of available component services, and is functionally equivalent to the goal service. However, if such a composition cannot be realized, the proposed approach identifies the cause(s) for the failure of composition. This information can be used by the user to minimally reformulate the goal to reduce the `gap' between the desired functionality. The process can be iterated until a feasible composition is realized or the user decides to abort. The approach ensures that (i) a choreographer, if one is produced by our composition algorithm, in fact realizes the user-specified goal functionality; and (ii) the algorithm is guaranteed to find a composition that meets the user needs as captured in the goal specifications (whenever such a composition exists)",
Incremental Delta-Sigma Structures for DC Measurement: an Overview,"In this paper the theoretical operation of incremental (charge-balancing) delta-sigma (DeltaSigma) converters is reviewed, and the implementation of a 22-bit incremental A/D converter is described. Two different analyses of the first-order incremental converter are presented, and based on these results two extensions to higher-order modulators are proposed. Since line-frequency noise suppression is often important in measurement applications, modulators followed by sinck filters are also analyzed. Equations are derived for the estimation of the required number of cycles for a given resolution and architecture. Finally, the design and implementation of a third-order incremental converter with a fourth-order sine filter is briefly discussed",
Self-Adapting Payoff Matrices in Repeated Interactions,"Traditional iterated prisoner's dilemma (IPD) assumed a fixed payoff matrix for all players, which may not be realistic because not all players are the same in the real-world. This paper introduces a novel co-evolutionary framework where each strategy has its own self-adaptive payoff matrix. This framework is generic to any simultaneous two-player repeated encounter game. Here, each strategy has a set of behavioral responses based on previous moves, and an adaptable payoff matrix based on reinforcement feedback from game interactions that is specified by update rules. We study how different update rules affect the adaptation of initially random payoff matrices, and how this adaptation in turn affects the learning of strategy behaviors",
A sensory grammar for inferring behaviors in sensor networks,The ability of a sensor network to parse out observable activities into a set of distinguishable actions is a powerful feature that can potentially enable many applications of sensor networks to everyday life situations. In this paper we introduce a framework that uses a hierarchy of probabilistic context free grammars (PCFGs) to perform such parsing. The power of the framework comes from the hierarchical organization of grammars that allows the use of simple local sensor measurements for reasoning about more macroscopic behaviors. Our presentation describes how to use a set of phonemes to construct grammars and how to achieve distributed operation using a messaging model. The proposed framework is flexible. It can be mapped to a network hierarchy or can be applied sequentially and across the network to infer behaviors as they unfold in space and time. We demonstrate this functionality by inferring simple motion patterns using a sequence of simple direction vectors obtained from our camera sensor network testbed,
Dual-Homing Based Scalable Partia Multicast Protection,"In this paper, we propose a scalable multicast protection scheme based on a dual-homing architecture where each destination host is connected to two edge routers. Under such an architecture, there are two paths from the source of a multicast session to each destination host, which provides a certain level of protection for the data traffic from the source to the destination host. The protection level varies from 0 percent to 100 percent against a single link failure, depending on the number of shared links between these two paths. The major advantage of the proposed scheme lies in its scalability due to the fact that the protection is provided by constructing a dual-homing architecture at the access network while keeping the routing protocols in the core network unchanged. The selection of dual edge routers plays an important role in enhancing the protection level. Two problems arise for the proposed dual-homing partial multicast protection scheme. One is to calculate the survivability from the source to any pair of edge routers. The other is to assign a pair of edge routers for each destination host such that the total survivability is maximized for the multicast session subject to the port number constraint of each edge router. We propose an optimal algorithm to solve the first problem. We prove the decision version of the second problem is NP-complete and propose two heuristic algorithms to solve it. Simulation results show that the proposed heuristic algorithms achieve performance close to the calculated lower bound",
A Watershed Based Segmentation Method for Multispectral Chromosome Images Classification,"M-FISH (multicolor fluorescence in situ hybridization) is a recently developed cytogenetic technique for cancer diagnosis and research on genetic disorders which uses 5 fluors to label uniquely each chromosome and a fluorescent DNA stain. In this paper, an automated method for chromosome classification in M-FISH images is presented. The chromosome image is initially decomposed into a set of primitive homogeneous regions through the morphological watershed transform applied to the image intensity gradient magnitude. Each segmented area is then classified using a Bayes classifier. We have evaluated our methodology on a commercial available M-FISH database. The classifier was trained and tested on non-overlapping chromosome images and an overall accuracy of 89% is achieved. By introducing feature averaging on watershed basins, the proposed technique achieves substantially better results than previous methods at a lower computational cost",
A novel approach toward integration of rules into business processes using an agent-oriented framework,"Every action a business process performs must be explicitly anticipated, designed for, and implemented by business professionals. The business process is dependent upon business rules (BRs) to achieve its objectives, which also involve pre- and post-conditions in the business process (BP). Most of the current techniques implement these rules directly into BPs. This will result in the BP becoming even more complicated and harder to customize. Most BRs (and laws) are buried in data warehouses as raw data. Incorporating the BRs into any BP is very complex, in terms of customization, reuse, and system integration. To overcome this problem, we propose and implement an agent-oriented framework. Using this framework, we can make the process of incorporating BR into BP less complex. The agent creates a bridge between components of the business structure, the BRs, and the BP. The BP consists of activities and sub-activities. The agent is able to communicate with BP and BRs to execute a BP. The agent can also communicate with other agents and have control over their own internal states and behavior. In this paper, we propose and implement a simple case study to execute a BP dependent upon BRs. The case study shows how the agent will synchronize, integrate, and deploy the BP",
Time-significant Wavelet Coherence for the Evaluation of Schizophrenic Brain Activity using a Graph theory approach,"Among the various frameworks in which electroencephalographic (EEG) signal synchronization has been traditionally formulated, the most widely studied and used is the coherence that is entirely based on frequency analysis. However, at present time it is possible to capture information about the temporal profile of coherence, which is particularly helpful in studying non-stationary time-varying brain dynamics, like the wavelet coherence (WC). In this paper we propose a new approach of studying brain synchronization dynamics by extending the use of WC to include certain statistically significant (in terms of signal coherence) time segments, to study and characterize any disturbances present in the functional connectivity network of schizophrenia patients. Graph theoretical measures and visualization provide the tools to study the ""disconnection syndrome"" as proposed for schizophrenia. Specifically, we analyzed multichannel EEG data from twenty stabilized patients with schizophrenia and controls in an experiment of working memory (WM) using the gamma band (i.e., the EEG frequency of ca. 40 Hz), which is activated during the connecting activity (i.e., the ""binding"" of the neurons). The results are in accordance with the disturbance of connections between the neurons giving additional information related to the localization of most prominent disconnection",
Visualizing Graphs - A Generalized View,"The visualization of graphs has proven to be very useful for exploring structures in different application domains. However, in certain fields of computer science, graph visualization is understood and focused quite differently. While ""graph drawing"" focuses on optimized layouts for node-link-representations of networks, ""information visualization"" prefers to work on hierarchies focusing on very large structures, different views and interactivity. This paper gives a systematic view of the problem of graph visualization by combining both approaches. We introduce a general view of different visualization methods as well as describe occurring problems and discuss basic constraints. These are used to propose a visualization framework for graphs, whose development motivated this paper",
Hybrid MPI-OpenMP Programming for Parallel OSEM PET Reconstruction,"To improve the parallel efficiency (PE) of the ordered-subsets expectation-maximization (OSEM) algorithm for three-dimensional (3-D) positron emission tomography (PET) image reconstruction, we focused on reducing the computational imbalance among parallel processes and interprocess data exchange time which were the dominant limiting factors of PE when a large number of networked compute nodes were used. As clusters with multiple processors on each compute node have become increasingly common, we have aimed to take advantage of the load-balancing mechanism and the inherently lower latency of shared memory threads across processors within a single node. We, therefore, implemented the OSEM algorithm with a hybrid message passing interface (MPI) and OpenMP approach on the basis of a standard MPI implementation. The contributing components to the total reconstruction time for the hybrid technique were quantified and compared to that using only MPI. The hybrid MPI-OpenMP technique achieved a consistent PE improvement of approximately 7% to 17% compared to the pure MPI approach on the same number of compute nodes. As clusters of larger shared-memory multiprocessor (SMP) machines continue to become more cost effective, we expect this hybrid MPI-OpenMP approach to be increasingly valuable",
Keyphrase Extraction Using Semantic Networks Structure Analysis,"Keyphrases play a key role in text indexing, summarization and categorization. However, most of the existing keyphrase extraction approaches require human-labeled training sets. In this paper, we propose an automatic keyphrase extraction algorithm, which can be used in both supervised and unsupervised tasks. This algorithm treats each document as a semantic network. Structural dynamics of the network are used to extract keyphrases (key nodes) unsupervised. Experiments demonstrate the proposed algorithm averagely improves 50% in effectiveness and 30% in efficiency in unsupervised tasks and performs comparatively with supervised extractors. Moreover, by applying this algorithm to supervised tasks, we develop a classifier with an overall accuracy up to 80%.",
An Automatic 3D Ear Recognition System,"Previous works have shown that the ear is a good candidate for a human biometric. However, in prior work, the pre-processing of ear images has been a manual process. Also, prior algorithms were sensitive to noise in the data, especially that caused by hair and earrings. We present a novel solution to the automated cropping of the ear and implement it in an end-to-end solution for biometric recognition. We demonstrate our automatic recognition process with the largest study to date in ear biometrics, 415 subjects, achieving a rank one recognition rate of 97.6%. This work represents a breakthrough in ear biometrics and paves the way for commercial quality, fully automatic systems.",
"High-performance, Dependable Multiprocessor","With the ever-increasing demand for higher bandwidth and processing capacity of today's space exploration, space science, and defense missions, the ability to efficiently apply commercial-off-the-shelf (COTS) processors for on-board computing is now a critical need. In response to this need, NASA's new millennium program office has commissioned the development of Dependable Multiprocessor (DM) technology for use in payload and robotic missions. The Dependable Multiprocessor technology is a COTS-based, power-efficient, high-performance, highly dependable, fault-tolerant cluster computer. To date, Honeywell has successfully demonstrated a TRL4 prototype of the Dependable Multiprocessor (Ramos et al., 2005), and is now working on the development of a TRL5 prototype. For the present effort Honeywell has teamed up with the University of Florida via its high-performance computing and simulation (HCS) research laboratory, and together the team has demonstrated major elements of the Dependable Multiprocessor TRL5 system. This paper provides a detailed description of the basic Dependable Multiprocessor technology, and the TRL5 technology prototype currently under development",
Opportunistic Networks: Challenges in Specializing the P2P Paradigm,"We introduce the notion of opportunistic networks or oppnets, some of which can be considered a subclass of the peer-to-peer (P2P) networks. Initially, a relatively small seed oppnet is deployed, which grows into a bigger expanded oppnet. Oppnet growth starts with detecting diverse systems existing in its relative vicinity. Systems with best evaluations are invited by an oppnet to become its helpers. The oppnet leverages vast collective capabilities and resources of its helpers, employing them to execute diverse tasks in support of its goals. Though oppnet characteristics make them a natural fit for emergency response applications, we expect that they will prove beneficial in many other application areas. We discuss challenges in the development and use of the oppnet technology. Oppnets that use P2P interactions can be viewed as a specialization of the more general paradigm of P2P networks. To the best of our knowledge, we were the first to define and are now the first to investigate oppnets",
"Wireless Channels that Exhibit ""Worse than Rayleigh"" Fading: Analytical and Measurement Results","We describe several disparate practical settings in which cases of severe, or ""worse than Rayleigh, ""fading have been encountered. To explain this behavior, we present two physical models. The first model employs a statistically non-stationary random process that switches between two distributions, akin to the multi-state models proposed for land mobile satellite channels. The other model we propose is a multiplicative model of two small scale fading processes. We also describe the analytical pdf for such a multiplicative model using two Weibull random variables as the underlying small scale fading distributions. Measured data is used to corroborate these models, and our computer simulations confirm the ability to replicate the statistics of these severe fading processes. These proposed models can be used by researchers investigating communication system designs in severe fading environments",
Approximation Algorithms for Non-Uniform Buy-at-Bulk Network Design,"We consider approximation algorithms for non-uniform buy-at-bulk network design problems. The first non-trivial approximation algorithm for this problem is due to Charikar and Karagiozova (STOC 05); for an instance on h pairs their algorithm has an approximation guarantee of exp(O(radic(log h log log h)))for the uniform-demand case, and log D middot exp(O(radic(log h log log h))) for the general demand case, where D is the total demand. We improve upon this result, by presenting the first poly-logarithmic approximation for this problem. The ratio we obtain is O(log3 h middot min{log D, gamma(h2)}) where his the number of pairs and gamma(n) is the worst case distortion in embedding the metric induced by a n vertex graph into a distribution over its spanning trees. Using the best known upper bound on gamma(n) we obtain an O(min{log3 h middot log D, log5 h log log h}) ratio approximation. We also give poly-logarithmic approximations for some variants of the single-source problem that we need for the multicommodity problem",
Data Security in MANETs using Multipath Routing and Directional Transmission,"A cross-layer approach is investigated to improve data security in Mobile Ad Hoc Networks (MANETs). The use of directional antennas and intelligent multipath routing is proposed to enhance end-to-end data confidentiality and data availability with respect to outsider attacks. The goal is to impede rogue attempts to gain unauthorized access to classified information or disrupt the information flow. The interplay between the physical, link, and network layers is considered. A novel simulator is developed to accurately quantify the data confidentiality benefits of these approaches. This study leverages the existence of multiple paths between end-nodes to statistically improve data confidentiality and data availability in hostile MANET environments, where both insider and outsider adversaries may be present. Simulation results show that the proposed mechanisms can greatly improve data confidentiality as compared to existing schemes. These mechanisms can also improve data availability.","Data security,
Routing,
Availability,
Mobile ad hoc networks,
Authentication,
Protection,
Computer science,
Directional antennas,
Impedance,
Military communication"
A Fault-Tolerant Routing Algorithm for Mobile Ad Hoc Networks Using a Stochastic Learning-Based Weak Estimation Procedure,"Designing routing schemes that would successfully operate in the presence of adversarial environments in mobile ad hoc networks (MANETs) is a challenging issue. In this paper we discuss fault-tolerant routing schemes where there are malfunctioning nodes in the network. Most existing MANET protocols were postulated considering scenarios where all the mobile nodes in the ad hoc network function properly, and in an idealistic manner. However, adversarial environments are common in MANET environments, and there are misbehaving nodes that degrade the performance of these routing protocols. The need for fault tolerant routing protocols was identified to address routing in adversarial environments in the presence of faulty nodes by exploring network redundancies in networks. In this paper, we present a new fault-tolerant routing scheme using a stochastic learning-based weak estimation procedure. The superiority of our algorithm, as compared to the existing algorithms, was experimentally established",
Schemes for eliminating transient-width clock overhead from SET-tolerant memory-based systems,"In the presence of radiation, particle strikes can cause temporary signal errors in ICs. Particle strikes that directly affect memory are known as single event upsets (SEUs), while strikes that affect combinational logic and spread to memory are called single event transients (SETs). In this paper, we propose two novel approaches to hardening integrated circuits against SEUs and SETs. The proposed approaches are fully-differential dual-interlocked storage cell (DICE) and triple path DICE (TPDICE). The fully-differential DICE and TPDICE approaches are compared against two existing approaches, which are triple modular redundancy (TMR) and basic SET-tolerant DICE. All approaches except for the basic SET-tolerant DICE scheme share a common theme, which is the ability to bypass SEUs and SETs. This is critical for performance, as it allows the system to proceed with subsequent operations while a cell is recovering from the effects of a particle strike. SET pulse widths can be substantial (up to 2 ns), and so high-performance systems cannot afford to pause operations while these pulses are present. The minimum clock periods obtained for the basic SET-tolerant approach were 515 ps with no SET, and 1310 ps with a 500 ps SET (in 0.18 /spl mu/m CMOS). In contrast, the clock periods for the bypass-capable approaches with no SET/500 ps SET were 628/749 ps for TMR, 348/480 ps for fully-differential DICE, and 434/552 ps for TPDICE. Among the approaches that bypass transient pulses, TPDICE is the most balanced. TMR suffers from overhead due to its need for external voting circuitry. In addition to this, fully-differential DICE cannot be used with combinational logic, while TPDICE can.",
Combinatorial Search Methods for Multi-SNP Disease Association,"Recent improvements in the accessibility of high-throughput genotyping have brought a deal of attention to genome-wide association studies for common complex diseases. Although, such diseases can be caused by multi-loci interactions, locus-by-locus studies are prevailing. Recently, two-loci analysis has been shown promising (Marchini et al, 2005), and multi-loci analysis is expected to find even deeper disease-associated interactions. Unfortunately, an exhaustive search among all possible corresponding multi-markers can be unfeasible even for small number of SNPs let alone the complete genome. In this paper we first propose to extract informative (indexing) SNPs that can be used for reconstructing of all SNPs almost without loss (He and Zelikovsky, 2006). In the reduced set of SNPs, we then propose to apply a novel combinatorial method for finding disease-associated multi-SNP combinations (MSCs). Our experimental study shows that the proposed methods are able to find MSCs whose disease association is statistically significant even after multiple testing adjustment. For (Daly et al, 2001) data we found a few unphased MSCs associated with Crohn's disease with multiple testing adjusted p-value below 0.05 while no single SNP or pair of SNPs show any significant association. For (Ueda et al, 2003) data we found a few new unphased and phased MSCs associated with autoimmune disorder",
ALBA: An Adaptive Load - Balanced Algorithm for Geographic Forwarding in Wireless Sensor Networks,"In this paper we propose and analyze ALBA, an original packet forwarding protocol for ad hoc and sensor networks. ALBA follows an integrated approach that combines geographic routing and medium access control (MAC), exploiting the knowledge of node positions in order to achieve energy-efficient data forwarding. The scenario we consider is very critical for medium-high traffic, as contentions for channel access and the resulting collisions lead to performance degradation. To counter this effect, we leverage on network density, favoring the choice relay candidates that are not in overload. With our protocol, nodes strive to channelize traffic toward uncongested network regions, rather than just maximizing the advancement towards the final destination. We carry out extensive simulations that compare ALBA to GeRaF and MACRO, two recently proposed cross-layer approaches with similar goals. The results show that our design achieves very good delivery and latency performance, and can greatly limit energy consumption",
Preparing Computer Science Students for Global Software Development,"This paper describes an innovative study undertaken in an undergraduate capstone software engineering course to give students exposure to the realities of global software development. In this study, students from Pace University in New York worked as part of an extended team with students from the Institute of Technology of Cambodia, developing software for Cambodian customers. This paper explains the goals of the study, describes the teams and their projects, and emphasizes the required logistics for a study of this nature. The findings are discussed, and some lessons for computer science education and global software development are provided. The paper finishes with a brief account of our continuing work in the area",
Stochastic Differential Equation Approach to Model BitTorrent-like P2P Systems,"In this paper, we propose to model the dynamics of BitTorrent (BT) P2P file sharing systems using the stochastic differential equation method. Unlike previous approach, our method can capture more realistic network environment and peers behavior. Closed-form solutions of various performance measures such as the average number of downloaders, seeders, the system throughput and file downloading time are derived. We also validate our mathematical results via simulation and show that not only our mathematical model can closely track the dynamics of BT-like systems, but the model has a much higher accuracy than previous proposed methods. Also, many important properties can be derived from the close-form solution such as performance scalability, sensitivity of the measurements to various system parameters. We believe the proposed method can provide better understanding in the design and analysis of BT-like P2P systems.","Stochastic systems,
Differential equations,
Peer to peer computing,
Mathematical model,
Protocols,
Application software,
Computer science,
Scalability,
Performance analysis,
Transient analysis"
A Hybrid HMM-SVM Method for Online Handwriting Symbol Recognition,"This paper presents a combined approach for online handwriting symbols recognition. The basic idea of this approach is to employ a set of left-right HMMs as a feature extractor to produce HMM features, and combine them with global features into a new feature vector as input, and then use SVM as a classifier to finally identify unknown symbols. The new feature vector consists of the global features and several pairs of maximum probabilities with their associated different model labels. A recogniser based on this method inherits the practical and dynamical modeling abilities from HMM, and robust discriminating ability from SVM for classification tasks. This technique also reduces the dimensions of feature vectors significantly and solves the speed and size problem when using only SVM. The experimental results show that this combined hybrid approach outperforms several classifiers reported in recent researches, and could achieve recognition rates of 97.48%, 91.99% and 91.74% for digits and upper/lower case characters respectively on the UNIPEN database benchmarks","Hidden Markov models,
Handwriting recognition,
Support vector machines,
Support vector machine classification,
Feature extraction,
Pattern recognition,
Vocabulary,
Character recognition,
Computer science,
Informatics"
On the Long-term Evolution of the Two-Tier Gnutella Overlay,"Peer-to-Peer (P2P) file sharing applications have witnessed a dramatic increase in popularity during the past few years. To accommodate the rapid growth in user population, developers introduced new features in their client software, in particular a two-tier overlay topology. The effect of the two- tier overlay topology in a widely-deployed P2P system primarily depends on the availability and coherency of its implementations among participating clients throughout the system. This paper sheds some light on the long-term evolution of such a two-tier overlay topology in the Gnutella network during a 15-month period over which the system quadrupled in size, exceeding three million concurrent peers. Our results show two interesting phenomena including: (i) During this period, the two- tier overlay has repeatedly begun to lose its balance. However, proper modifications in major client software coupled with the rapid upgrade rate of users, has enabled the developers to maintain the overlay's desired properties, (ii) Despite its random connectivity, the Gnutella overlay exhibits a strong bias towards intra-continent connectivity, especially in continents with smaller user populations, that has not changed as the system scaled.","Peer to peer computing,
Network topology,
Software maintenance,
Open source software,
Application software,
Continents,
Information science,
Scalability,
Internet,
Telecommunication traffic"
Computing for LQCD: apeNEXT,apeNEXT is the latest in the APE collaboration's series of parallel computers for computationally intensive calculations such as quantum chromo dynamics on the lattice. The authors describe the computer architectural choices that have been shaped by almost two decades of collaboration activity.,"Quantum computing,
Concurrent computing,
Collaboration,
Kernel,
Lattices,
Biology computing,
Computer aided manufacturing,
Physics computing,
Costs,
Collaborative software"
Enhancing the robustness of a speciation-based PSO,"Speciation encourages an evolutionary algorithm to locate multiple solutions in multimodal environments. Speciation algorithms often require a user to specify a parameter to define the species radius, which can be a major drawback since this knowledge may not be available a priori. This paper proposes a technique using a time-based convergence measure to overcome this problem. The proposed method is used to enhance the performance of a speciation-based PSO (SPSO) and has been shown to be robust over a wide range of values for this user-specified parameter.",
Field weakening control of interior permanent magnet machine using improved current interpolation technique,An improved torque control under field weakening for interior permanent magnet machines (IPMM) is presented. The approach is founded on the recently proposed method to adjust d- and q-axis reference rotor current along constant torque curves using two-dimensional lookup tables. A new way of generating the reference current lookup tables based on machine data is presented. It allows to reduce complexity of the overall torque controller while maintaining all advantageous properties such as full torque linearity and highly dynamic behaviour. Normally invalid combinations of torque and flux linkage in the tables are replaced by operating points that are interpolated on the boundary of the operating area. This makes pre-limiting of the torque reference unnecessary. The scheme has been tested successfully on a 6 kW IPMM used in an ISG application.,"Torque,
Couplings,
Interpolation,
Stators,
Limiting,
Torque control,
Rotors"
Performance Evaluation of IEEE 802.15.4 Ad Hoc Wireless Sensor Networks: Simulation Approach,"This paper presents a preliminary performance investigation of the recently released IEEE 802.15.4 standard focusing on multiple sources and multi-hop peer-to-peer wireless sensor networks. This standard was developed to work in all-wireless environment supporting either peer-to-peer or star network topology. Since the release of IEEE 802.15.4, several efforts were made to study its performance where simple star network topology was the main focus. This literature attempts to extend existing efforts but focuses on evaluating the performance of peer-to-peer networks on a small scale basis using ns2 simulator. We analyze the performance based on commonly known metrics such as throughput, packet delivery ratio, and average delay. In addition, we propose ad hoc wireless sensor networks (AD-WSNs) paradigm as part of the extension to the IEEE 802.15.4 standard. From the experiments conducted we identified 'hidden node' problem in high-rate traffic as a soruce for performance degradation and proposed slot-based channel access as the potential solution.",
Robust Ground Plane Detection with Normalized Homography in Monocular Sequences from a Robot Platform,"We present a homography-based approach to detect the ground plane from monocular sequences captured by a robot platform. By assuming that the camera is fixed on the robot platform and can at most rotate horizontally, we derive the constraints that the homograph of the ground plane must satisfy and then use these constraints to design algorithms for detecting the ground plane. Due to the reduced degree of freedom, the resultant algorithm is not only more efficient and robust, but also able to avoid false detection due to virtual planes. We present experiments with real data from a robot platform to validate the proposed approaches.",
Optimum Association of Mobile Wireless Devices with a WLAN-3G Access Network,"In this paper, we consider the problem of association of wireless stations (STAs) with an access network served by a wireless local area network (WLAN) and a 3G cellular network. There is a set of WLAN Access Points (APs) and a set of 3G Base Stations (BSs) and a number of STAs each of which needs to be associated with one of the APs or one of the BSs. We concentrate on downlink bulk elastic transfers. Each association provides each ST with a certain transfer rate. We evaluate an association on the basis of the sum log utility of the transfer rates and seek the utility maximizing association. We also obtain the optimal time scheduling of service from a 3G BS to the associated STAs. We propose a fast iterative heuristic algorithm to compute an association. Numerical results show that our algorithm converges in a few steps yielding an association that is within 1% (in objective value) of the optimal (obtained through exhaustive search); in most cases the algorithm yields an optimal solution.","Wireless LAN,
Land mobile radio cellular systems,
Iterative algorithms,
Downlink,
Signal to noise ratio,
Base stations,
Femtocell networks,
Multiaccess communication,
Throughput,
Sufficient conditions"
Measuring the Performance and Reliability of Production Computational Grids,"In this work we report on data gathered via a deployment of a monitoring and benchmarking infrastructure on two production grid platforms, TeraGrid and Geon. Our result show that these production grids are rather unavailable, with success rates for benchmark and application runs between 55% and 80%. We also found that performance fluctuation was in the 50% range, expectedly mostly due to batch schedulers. We also investigate whether the execution time of a typical grid application can be predicated based on previous runs of simple benchmarks. Perhaps surprisingly, we find that application execution time can be predicted with a relative error as low as 9%",
On efficient clustering of wireless sensor networks,"Wireless sensor networks are poised to increase the efficiency of many applications, such as target detection and disaster management. Typically sensors collect data about their surrounding and forward it to a command center, either directly or through a base-station (gateway). Due to inhospitable conditions, sensors may not always be uniformly deployed in the area of interest. Some sensors can be unreachable to the gateway due to their distance or the existence of obstacles in their communication path. Additionally, in many applications a large set of sensors is usually deployed and network scalability becomes a major concern. This paper investigates two different methodologies for clustering sensor networks with the goal of increasing the sensor coverage and availability allowing for dependable and efficient operation for large network setups. The first approach deploys multiple gateways and partitions sensors among these gateways. In the second approach some sensors are designated as agents for a single gateway in order to reach out-of range nodes. The performance of both approaches is compared in a simulated environment",
MASC: A Speech Corpus in Mandarin for Emotion Analysis and Affective Speaker Recognition,"In this paper, a large emotional speech database MASC (Mandarin affective speech corpus) is introduced. The database contains recordings of 68 native speakers (23 female and 45 male) and five kinds of emotional states: neutral, anger, elation, panic and sadness. Each speaker pronounces 5 phrases, 10 sentences for three times for each emotional states and 2 paragraphs only for neutral. These materials covers all the phonemes in Chinese. This corpus is constructed for prosodic and linguistic investigation of emotion expression in Mandarin. It can also be used for recognition of affectively stressed speakers. Furthermore, prosodic feature analysis and speaker recognition baseline experiment are performed on this database",
Capacity Evaluation of Various Multiuser MIMO Schemes in Downlink Cellular Environments,"Presented in this paper is a study of the capacity evaluation of various multiuser MIMO schemes in cellular environments. The throughputs per user of the generalized zero-forcing with rank adaptation and vector perturbation schemes are compared with the capacity bound of the Gaussian MIMO broadcast channel, obtained by dirty paper coding under proportional fairness scheduling. The average cell throughputs of these schemes are also compared. From these comparisons, this study provides vital information for applying multiuser MIMO schemes in multicell environments",
An Application of Capacitive Electrode for Detecting Electrocardiogram of Neonates and Infants,"A new system has been developed for obtaining electrographic potential through thin underwear inserted between the measuring electrodes and the skin of a neonate or an infant when lying supine. The system is based on capacitive coupling involving the electrode, the underwear, and the skin. Validation of the system revealed the following: (1) the signal detected using the system displayed a periodic waveform synchronized with the simultaneously recorded ECG, even when thin underwear was inserted between the electrode and the skin, (2) the gain of the system when the cloth was inserted decreased as the frequency increased. The present system appears promising for application to bedding as a non-invasive and awareness-free method for ECG monitoring of neonates or infants. However, there is still room for improvement in terms of its practical use, because the high-frequency component of the signal was depressed in comparison with the reference ECG",
Increasing pose estimation performance using multi-cue integration,"We have developed a system which integrates the information output from several pose estimation algorithms and from several views of the scene. It is tested in a real setup with a robotic manipulator. It is shown that integrating pose estimates from several algorithms increases the overall performance of the pose estimation accuracy as well as the robustness as compared to using only a single algorithm. It is shown that increased robustness can be achieved by using pose estimation algorithms based on complementary features, so called algorithmic multi-cue integration (AMC). Furthermore it is also shown that increased accuracy can be achieved by integrating pose estimation results from different views of the scene, so-called temporal multi-cue integration (TMC). Temporal multi-cue integration is the most interesting aspect of this paper",
Modularity Analysis of Logical Design Models,"Traditional design representations are inadequate for generalized reasoning about modularity in design and its technical and economic implications. We have developed an architectural modeling and analysis approach, and automated tool support, for improved reasoning in these terms. However, the complexity of constraint satisfaction limited the size of models that we could analyze. The contribution of this paper is a more scalable approach. We exploit the dominance relations in our models to guide a divide-and-conquer algorithm, which we have implemented it in our Simon tool. We evaluate its performance in case studies. The approach reduced the time needed to analyze small but representative models from hours to seconds. This work appears to make our modeling and analysis approach practical for research on the evolvability and economic properties of software design architectures",
UWB Channel Measurements and Results for Office and Industrial Environments,"This paper presents the results of ultra wide band (UWB) channel measurements carried out at the campus of Delft University of Technology. The measurements were conducted in an indoor office and industrial environment using a time domain setup which allows measurements from 3.1 to 10.6 GHz. Results on large scale path-loss exponent, shadowing, small scale fading and rms delay spread (RDS) for indoor office and industrial area propagation are presented",
METERG: Measurement-Based End-to-End Performance Estimation Technique in QoS-Capable Multiprocessors,"Multiprocessor systems present serious challenges in the design of real-time systems due to the wider variation of execution time of an instruction sequence compared to a uniprocessor system. Even if non-determinism is tightly controlled by adding conventional QoS support, it is generally difficult to find the minimal hardware resource request settings (e.g., memory bandwidth) for a given user-level performance goal (e.g., transactions per second). In this paper, we introduce the METERG (Measurement-Time Enforcement and Run-Time Guarantee) QoS system that provides an easy method of obtaining a tight estimate of the lower bound on end-to-end performance for a given configuration of resource reservations. Every QoS-capable block in the METERG system supports two operation modes for each task requiring QoS: enforcement mode for estimating the lower bound on a task’s execution time and deployment mode for maximizing its performance. We evaluate the effectiveness of our approach with an execution-driven multiprocessor simulator implementing the METERG QoS memory subsystem. We show that the performance lower bound is easy to obtain by simply running an application in enforcement mode, and that this estimated lower bound is tight.","Real time systems,
Multiprocessing systems,
Hardware,
Bandwidth,
Costs,
Handheld computers,
Resource management,
Control systems,
Time measurement,
Computer science"
Iterative Local-Global Energy Minimization for Automatic Extraction of Objects of Interest,"We propose a novel global-local variational energy to automatically extract objects of interest from images. Previous formulations only incorporate local region potentials, which are sensitive to incorrectly classified pixels during iteration. We introduce a global likelihood potential to achieve better estimation of the foreground and background models and, thus, better extraction results. Extensive experiments demonstrate its efficacy",
Padded Frames: A Novel Algorithm for Stable Scheduling in Load-Balanced Switches,"The load-balanced Birkhoff-von Neumann switching architecture consists of two stages: a load balancer and a deterministic input-queued crossbar switch. The advantages of this architecture are its simplicity and scalability, while its main drawback is the possible out-of-sequence reception of packets belonging to the same flow. Several solutions have been proposed to overcome this problem; among the most promising are the uniform frame spreading (UFS) and the full ordered frames first (FOFF) algorithms. In this paper, we present a new algorithm called padded frames (PF), which eliminates the packet reordering problem, achieves 100% throughput, and improves the delay performance of previously known algorithms.",
Implementing CS1 with embedded instructional research design in laboratories,"Closed laboratories are becoming an increasingly popular approach to teaching introductory computer science courses. Unlike open laboratories that tend to be an informal environment provided for students to practice their skills with attendance optional, closed laboratories are structured meeting times that support the lecture component of the course, and attendance is required. This paper reports on an integrated approach to designing, implementing, and assessing laboratories with an embedded instructional research design. The activities reported here are parts of a departmentwide effort not only to improve student learning in computer science and computer engineering (CE) but also to improve the agility of the Computer Science and Engineering Department in adapting the curriculum to changing technologies, incorporate research, and validate the instructional strategies used. This paper presents the design and implementation of the laboratories and the results and analysis of student performance. Also described in this paper is cooperative learning in the laboratories and its impact on student learning.",
Tracking using dynamic programming for appearance-based sign language recognition,We present a novel tracking algorithm that uses dynamic programming to determine the path of target objects and that is able to track an arbitrary number of different objects. The traceback method used to track the targets avoids taking possibly wrong local decisions and thus reconstructs the best tracking paths using the whole observation sequence. The tracking method can be compared to the nonlinear time alignment in automatic speech recognition (ASR) and it can analogously be integrated into a hidden Markov model based recognition process. We show how the method can be applied to the tracking of hands and the face for automatic sign language recognition,
A Practice Oriented Approach to Intelligent Computing Assisted Distance Education for Engineering,"Some advancements in information technology as application support for Internet browsers and modeling of interrelated objects are well appropriate to establish distance learning in advanced computer environment. One of the best developing areas in application of information technology is engineering. A great promise is integration of virtual environments in engineering and Internet based distance education. Methodology reported in (I. J. Rudas et al., 2000) and (L. Horvath et al., 2002) represents one of the possible directions of development towards this integration. Considering some basic concepts and methods from the proposals of the authors of these papers, this paper discusses problem and methodology, and details some practical issues for the implementation of the proposed method in the higher education practice. In this paper, cooperating managers and managing techniques are introduced for virtual classrooms. Next, selection or definition of a course and constraints in classroom models are discussed. Following this, structure and construction of classroom model are explained. Finally, integration of modeled classroom with engineering system is detailed as an implementation issue",
Evolving Neural Network Classifiers and Feature Subset Using Artificial Fish Swarm,"As a novel simulated evolutionary computation technique, artificial fish swarm algorithm (AFSA) shows many promising characters. This paper presents the use of AFSA as a new tool which sets up a neural network (NN), adjusts its parameters, and performs feature reduction, all simultaneously. In the optimization process, all features and hidden units are encoded into a real-valued artificial fish (AF), and give out the method of designing fitness function. The experimental results on several public domain data sets from UCI show that our algorithm can obtain an optimal NN with fewer input features and hidden units, and perform almost as good as even better than an original complex NN with entire input features. And also indicate that optimizing a network classifier for a specific task has the potential to produce a simple classifier with low classification error and good generalization ability",
Keyword Generation for Search Engine Advertising,"Keyword generation for search engine advertising is an important problem for sponsored search or paid-placement advertising. A recent strategy in this area is bidding on nonobvious yet relevant words, which are economically more viable. Targeting many such nonobvious words lowers the advertising cost, while delivering the same click volume as expensive words. Generating the right nonobvious yet relevant keywords is a challenging task. The challenge lies in not only finding relevant words, but also in finding many such words. In this paper, we present TermsNet, a novel approach to this problem. This approach leverages search engines to determine relevance between terms and captures their semantic relationships as a directed graph. By observing the neighbors of a term in such a graph, we generate the common as well as the nonobvious keywords related to a term",
Effects of Phase Noise at Both Transmitter and Receiver on the Performance of OFDM Systems,"OFDM system suffers from significant performance degradation due to the presence of phase noise, which causes the common phase error (CPE) and intercarrier interference (ICI) among subcarriers. Various phase noise analysis and mitigation schemes were presented for OFDM in the literature, which only concerned with phase noise generated at the receiver, assuming the transmitter noise is negligible. However, this is inappropriate for the uplink application, where transmitter employs relatively low cost local oscillators. In this paper, we aim to analyze an OFDM system with phase noise at both transmitter and receiver in a WSSUS multipath fading channel. Results show that such system is equivalent to an OFDM system with phase noise at receiver only, hence simplifies the system model and provides an easy way to examine phase noise effect at both transmitter and receiver.",
Supervised Learning of Topological Maps using Semantic Information Extracted from Range Data,"This paper presents an approach to create topological maps from geometric maps obtained with a mobile robot in an indoor-environment using range data. Our approach utilizes AdaBoost, a supervised learning algorithm, to classify each point of the geometric map into semantic classes. We then apply a segmentation procedure based on probabilistic relaxation labeling on the resulting classifications to eliminate errors. The topological graph is then extracted from the individual different regions and their connections. In this way, we obtain a topological map in the form of a graph, in which each node indicates a region in the environment with its corresponding semantic class (e.g., corridor, or room) and the edges indicate the connections between them. Experimental results obtained with data from different real-world environments demonstrate the effectiveness of our approach",
Primitive Quantum BCH Codes over Finite Fields,"An attractive feature of BCH codes is that one can infer valuable information from their design parameters (length, size of the finite field, and designed distance), such as bounds on the minimum distance and dimension of the code. In this paper, it is shown that one can also deduce from the design parameters whether or not a primitive, narrow-sense BCH contains its Euclidean or Hermitian dual code. This information is invaluable in the construction of quantum BCH codes. A new proof is provided for the dimension of BCH codes with small designed distance, and simple bounds on the minimum distance of such codes and their duals are derived as a consequence. These results allow us to derive the parameters of two families of primitive quantum BCH codes as a function of their design parameters",
On the Impact of Combinatorial Structure on Congestion Games,"We study the impact of combinatorial structure in congestion games on the complexity of computing pure Nash equilibria and the convergence time of best response sequences. In particular, we investigate which properties of the strategy spaces of individual players ensure a polynomial convergence time. We show, if the strategy space of each player consists of the bases of a matroid over the set of resources, then the lengths of all best response sequences are polynomially bounded in the number of players and resources. We can also prove that this result is tight, that is, the matroid property is a necessary and sufficient condition on the players' strategy spaces for guaranteeing polynomial time convergence to a Nash equilibrium. In addition, we present an approach that enables us to devise hardness proofs for various kinds of combinatorial games, including first results about the hardness of market sharing games and congestion games for overlay network design. Our approach also yields a short proof for the PLS-completeness of network congestion games. In particular, we can show that network congestion games are PLS-complete for directed and undirected networks even in case of linear latency functions",
MONETA: an embedded monitoring system for ubiquitous network environments,"Accurate and efficient monitoring of dynamically changing environments is one of the most important requirements for ubiquitous network environments. To exploit these ubiquitous environments, we designed and implemented a monitoring system called MONETA that can obtain sensor data transmitted from wireless sensors to hub nodes in embedded equipment. MONETA adopts Web technology for the implementation of a simple but efficient user interface that allows an operator to visualize any of the processes, elements, or related information in a convenient graphic form. We developed MONETA using a kernel wrapper mechanism. This wrapper is a software module that enables us to monitor the state information of several embedded devices without any modification to the kernel. This wrapping method has allowed us to increase the portability of the monitoring system to other platforms and reduce the time required to write monitoring software modules.",
"Studying the Characteristics of a ""Good"" GUI Test Suite","The widespread deployment of graphical-user interfaces (GUIs) has increased the overall complexity of testing. A GUI test designer needs to perform the daunting task of adequately testing the GUI, which typically has very large input interaction spaces, while considering tradeoffs between GUI test suite characteristics such as the number of test cases (each modeled as a sequence of events), their lengths, and the event composition of each test case. There are no published empirical studies on GUI testing that a GUI test designer may reference to make decisions about these characteristics. Consequently, in practice, very few GUI testers know how to design their test suites. This paper takes the first step towards assisting in GUI test design by presenting an empirical study that evaluates the effect of these characteristics on testing cost and fault detection effectiveness. The results show that two factors significantly effect the fault-detection effectiveness of a test suite: (1) the diversity of states in which an event executes and (2) the event coverage of the suite. Test designers need to improve the diversity of states in which each event executes by developing a large number of short test cases to detect the majority of ""shallow"" faults, which are artifacts of modern GUI design. Additional resources should be used to develop a small number of long test cases to detect a small number of ""deep"" faults",
Analysis of Bottleneck Delay and Throughput in Wireless Mesh Networks,"Wireless mesh networking has emerged as a promising technology in providing economical and scalable broadband Internet accesses. A wireless mesh network consists of mesh routers and mesh clients, connected in an ad hoc manner via wireless links. A subset of the mesh routers, referred to as gateway nodes, are capable of external Internet connections. Since other mesh routers and clients have to access the Internet through the gateway nodes, these nodes can easily become bottlenecks in the network. In this paper, we present a novel queuing model based analysis of the delay and throughput of the gateway nodes. Our simulation results suggest that the analytical results are quite accurate, which provides an effective guideline for gateway-related design and optimization in wireless mesh networks",
Multi-Channel Interference Measurements for Wireless Sensor Networks,"This paper presents measurements of radio interference using ""ambient munode"" sensor nodes. By varying distances and frequencies we get a measure of the interference caused by transmissions on adjacent bands. Our observations show that adjacent spectrum interference influences the data delivery, considerably. Channels should be separated in the spatial or in the frequency domains if interference is to be avoided. In addition, the distance to simultaneous transmitters and the number of simultaneous transmissions are highly correlated with channel spacing. Therefore, channel spacing can be adjusted according to spatial distances so that multiple concurrent transmissions can be performed without interference. We also give proposals for further investigation on the usage of this correlation that are relevant to the design of future multi-channel protocols",
Filmtrust: movie recommendations from semantic web-based social networks,,
What is the Dimension of Your Binary Data?,"Many 0/1 datasets have a very large number of variables; however, they are sparse and the dependency structure of the variables is simpler than the number of variables would suggest. Defining the effective dimensionality of such a dataset is a nontrivial problem. We consider the problem of defining a robust measure of dimension for 0/1 datasets, and show that the basic idea of fractal dimension can be adapted for binary data. However, as such the fractal dimension is difficult to interpret. Hence we introduce the concept of normalized fractal dimension. For a dataset D, its normalized fractal dimension counts the number of independent columns needed to achieve the unnormalized fractal dimension of D. The normalized fractal dimension measures the degree of dependency structure of the data. We study the properties of the normalized fractal dimension and discuss its computation. We give empirical results on the normalized fractal dimension, comparing it against PCA.",
The Performance of Elliptic Curve Based Group Diffie-Hellman Protocols for Secure Group Communication over Ad Hoc Networks,"The security of the two party Diffie-Hellman key exchange protocol is currently based on the discrete logarithm problem (DLP). However, it can also be built upon the elliptic curve discrete logarithm problem (ECDLP). Most proposed secure group communication schemes employ the DLP-based Diffie-Hellman protocol. This paper proposes the ECDLP-based Diffie-Hellman protocols for secure group communication and evaluates their performance on wireless ad hoc networks. The proposed schemes are compared at the same security level with DLP-based group protocols under different channel conditions. Our experiments and analysis show that the Tree-based Group Elliptic Curve Diffie-Hellman (TGECDH) protocol is the best in overall performance for secure group communication among the four schemes discussed in the paper. Low communication overhead, relatively low computation load and short packets are the main reasons for the good performance of the TGECDH protocol.",
Application-Transparent Checkpoint/Restart for MPI Programs over InfiniBand,"Ultra-scale computer clusters with high speed interconnects, such as InfiniBand, are being widely deployed for their excellent performance and cost effectiveness. However, the failure rate on these clusters also increases along with their augmented number of components. Thus, it becomes critical for such systems to be equipped with fault tolerance support. In this paper, we present our design and implementation of checkpoint/restart framework for MPI programs running over InfiniBand clusters. Our design enables low-overhead, application-transparent checkpointing. It uses coordinated protocol to save the current state of the whole MPI job to reliable storage, which allows users to perform rollback recovery if the system runs into faulty states later. Our solution has been incorporated into MVAPICH2, an open-source high performance MPI-2 implementation over InfiniBand. Performance evaluation of this implementation has been carried out using NAS benchmarks, HPL benchmark, and a real-world application called GROMACS. Experimental results indicate that in our design, the overhead to take checkpoints is low, and the performance impact for checkpointing applications periodically is insignificant. For example, time for checkpointing GROMACS is less than 0.3% of the execution time, and its performance only decreases by 4% with checkpoints taken every minute. To the best of our knowledge, this work is the first report of checkpoint/restart support for MPI over InfiniBand clusters in the literature",
Investigation on relationship between information flow rate and mental workload of accident diagnosis tasks in NPPs,"The objective of this study is to investigate experimentally the relationship between an operator's mental workload and the information flow rate of accident diagnosis tasks and further to propose the information flow rate as an analytic method for measuring the mental workload. There are two types of mental workload in the advanced main control room of nuclear power plants: the information processing workload, which is the processing that the human operator must actually perform in order to complete the diagnosis task, and emotional stress workload experienced by the operator. In this study, the focus is on the former. Three kinds of methods are compared to measure the operator's workload: information flow rate, subjective methods, and physiological measures. Information flows for eight accident diagnosis tasks are modeled qualitatively using a stage model and are quantified using Conant's model. The information flow rate is obtained by imposing time limit restrictions for the tasks. National Aeronautics and Space Administration-Task Load Index (NASA-TLX) and Modified Cooper-Harper (MCH) scale are selected as subjective methods. For the physiological measurements, an eye tracking system analyzes eye movements related to the operator's blinking and fixation on regions of interests. Through the experiments, the relationship between the information flow rate of accident diagnosis tasks and the selected measures is investigated. Results indicate that the information flow rate of diagnosis tasks is in high agreement with both subjective rating scores and eye movement parameters related to blinking and fixation on the regions of interest. It appears, then, that information flow rate can be an alternative as an analytic approach for measuring mental workload. By using data on the information flow rate, we can predict the mental workload required for a task without performing experiments in advance.",
Adaptation of Haptic Interfaces for a LabVIEW-based System Dynamics Course,"This paper describes the development of haptic paddle laboratory kits and associated National Instruments LabVIEW virtual instrumentation to support the adaptation of laboratory experiments for a required undergraduate system dynamics course at Rice University. The laboratory experiments use simple haptic interfaces, devices that allow the students to interact via the sense of touch with virtual environments. A clear benefit of this laboratory series is that students study the haptic paddle as a real electromechanical system in addition to using the haptic paddle as a tool to interact with virtual mechanical systems. The haptic paddle hardware has been modified to improve robustness, and the LabVIEW graphical programming language is used for data acquisition and control throughout the laboratory series.",
Analysis of Abnormality in Endoscopic images using Combined HSI Color Space and Watershed Segmentation,"In this paper, a method for detecting possible presence of abnormality in the endoscopic images is presented. The pre-processed endoscopic color images are segmented in the HSI color space. The pixels in the input color image corresponding to the segmented image are extracted for further processing. This image is smoothened using average filter and converted into grayscale image. Its inverse transform is obtained for further processing and extended minima is imposed on the processed image using morphological reconstruction. Then the morphological watershed segmentation is carried out on this image and the number of regions is counted and is compared with the threshold value. If the number of regions is more than the threshold value, then the output image is an indicative of possible presence of abnormality in the image",
Natural Motion Generation for Humanoid Robots,"This paper presents a method of generating natural-looking motion primitives for humanoid robots. An optimization-based approach is used to generate these primitives, but the objective function is tailored to each one and complexity is reduced by identifying relevant degrees of freedom. Several examples are shown in simulation: for an arm movement to reach an object, it is better to minimize the acceleration of key parts of the robot over its entire trajectory; for a single step on flat ground, it is better to minimize the torque and instantaneous angular momentum at every posture. The primitives are precomputed off-line, but might be used by on-line planner either to provide a fixed set of maneuvers or to bias a probabilistic, sample-based search for motions",
Multiplexed CMOS Sensor Arrays for Die Stress Mapping,"A multiplexed array of 512 current mirror (CM) type CMOS piezoresistive FET stress sensor cells has been fabricated on an MOSIS tiny chip. Driven by an on-chip counter, the sequentially scanned arrays produce the highest resolution mapping of in-plane normal stress and shear stress in an integrated circuit die reported to date. The stress sensor array is calibrated using a chip-on-beam calibration technique, and the measured stress distributions agree well with finite element simulation results. This close correspondence gives the first indirect proof that the MOS shear stress sensors are truly responding to shear stresses",
Allocation Cost Minimization for Periodic Hard Real-Time Tasks in Energy-Constrained DVS Systems,"Energy-efficiency and power-awareness for electronic systems have been important design issues in hardware and software implementations. We consider the scheduling of periodic hard real-time tasks along with the allocation of processors under a given energy constraint. Each processor type could be associated with its allocation cost. The objective of this work is to minimize the entire allocation cost of processors so that the timing and energy constraints are both satisfied. We develop approximation algorithms for processor types with continuous processor speeds or discrete processor speeds. The capability of the proposed algorithms was evaluated by a series of experiments, and it was shown that the proposed algorithms always derived solutions with system costs close to those of optimal solutions in the experiments",
Design and Performance Studies of an Adaptive Scheme for Serving Dynamic Web Content in a Mobile Computing Environment,"Currently, people gain easy access to an increasingly diverse range of mobile devices such as personal digital assistants (PDAs), smart phones, and handheld computers. As dynamic content has become dominant on the fast-growing World Wide Web (C. Yuan et al., 2003), it is necessary to provide effective ways for the users to access such prevalent Web content in a mobile computing environment. During a course of browsing dynamic content on mobile devices, the requested content is first dynamically generated by remote Web server, then transmitted over a wireless network, and, finally, adapted for display' on small screens. This leads to considerable latency and processing load on mobile devices. By integrating a novel Web content adaptation algorithm and an enhanced caching strategy, we propose an adaptive scheme called MobiDNA for serving dynamic content in a mobile computing environment. To validate the feasibility and effectiveness of the proposed MobiDNA system, we construct an experimental testbed to investigate its performance. Experimental results demonstrate that this scheme can effectively improve mobile dynamic content browsing, by improving Web content readability on small displays, decreasing mobile browsing latency, and reducing wireless bandwidth consumption",
Guessing Facets: Polytope Structure and Improved LP Decoder,"A new approach for decoding binary linear codes by solving a linear program (LP) over a relaxed codeword polytope was recently proposed by Feldman et al. In this paper we investigate the structure of the polytope used in the LP relaxation decoding. We begin by showing that for expander codes, every fractional pseudocodeword always has at least a constant fraction of non-integral bits. We then prove that for expander codes, the active set of any fractional pseudocodeword is smaller by a constant fraction than the active set of any codeword. We exploit this fact to devise a decoding algorithm that provably outperforms the LP decoder for finite blocklengths. It proceeds by guessing facets of the polytope, and resolving the linear program on these facets. While the LP decoder succeeds only if the ML codeword has the highest likelihood over all pseudocodewords, we prove that for expander codes the proposed algorithm succeeds even with a constant number of pseudocodewords of higher likelihood. Moreover, the complexity of the proposed algorithm is only a constant factor larger than that of the LP decoder",
Palmprint Classification using Dual-Tree Complex Wavelets,"A new palmprint classification method is proposed in this paper by using the dual-tree complex wavelet transform. The dual-tree complex wavelet transform has such important properties as the approximate shift-invariance and high directional selectivity. These properties are very important in invariant palmprint classification. Support vector machines are used as a classifier and the Gaussian radial basis function kernel is selected in the experiments. Experimental results show that the dual-tree complex wavelet features outperform the scalar wavelet features, and three previously developed methods. We conclude that the dual-tree complex wavelet features should be used for invariant palmprint classification instead of the scalar wavelet features.",
Energy efficient watermarking on mobile devices using proxy-based partitioning,"Digital watermarking embeds an imperceptible signature or watermark in a digital file containing audio, image, text, or video data. The watermark can be used to authenticate the data file and for tamper detection. It is particularly valuable in the use and exchange of digital media, such as audio and video, on emerging handheld devices. However, watermarking is computationally expensive and adds to the drain of the available energy in handheld devices. In this paper, we first analyze the energy profile of various watermarking algorithms. We also study the impact of security and image quality on energy consumption. Second, we present an approach in which we partition the watermarking embedding and extraction algorithms and migrate some tasks to a proxy server. This leads to a lower energy consumption on the handheld without compromising the security of the watermarking process. Experimental results show that executing the watermarking tasks that are partitioned between the proxy and the handheld devices, reduces the total energy consumed by 80%, and improves performance by two orders of magnitude compared to running the application on only the handheld device",
A Stochastic Approach to Measuring the Robustness of Resource Allocations in Distributed Systems,"Often, parallel and distributed computing systems must operate in an environment replete with uncertainty. Determining a resource allocation that accounts for this uncertainty in a way that can provide a probabilistic guarantee that a given level of quality of service (QoS) is achieved is an important research problem. This paper defines a stochastic methodology for quantifiably determining a resource allocation's ability to satisfy QoS constraints in the midst of uncertainty in system parameters. Uncertainty in system parameters and its impact on system performance are modeled stochastically. This stochastic model is then used to derive a quantitative expression for the robustness of a resource allocation. The paper investigates the utility of the proposed stochastic robustness metric by applying the metric to resource allocations in a simulated distributed system. The simulation results are then compared with deterministically defined metrics from the literature",
VLE-WFBus: A Scientific Workflow Bus for Multi e-Science Domains,"In e-Science, a Grid environment enables data and computing intensive tasks and provides a new supporting infrastructure for scientific experiments. Scientific workflow management systems (SWMS) hide the integration details among Grid resources and allow scientists to prototype an experimental computing system at a high level of abstraction. However, the development of an effective SWMS requires profound knowledge on both application domains and the network programming, and is often time consuming and domain specific. Integrating mature implementations of domain specific SWMS improves reusability of workflow resources and promotes a generic framework for different e-Science domains. In this paper, we discuss different options to derive a generic workflow management system from domain specific implementations, and propose a workflow bus based solution, called VLE-WFBus. Legacy SWMSs are wrapped as federated components and are loosely coupled as one workflow system via a runtime infrastructure. An agent based prototype is presented; the integration among different workflow management systems has been demonstrated.",
Improved Nearly-MDS Expander Codes,"A construction of expander codes is presented with the following three properties: i) the codes lie close to the Singleton bound, ii) they can be encoded in time complexity that is linear in their code length, and iii) they have a linear-time bounded-distance decoder. By using a version of the decoder that corrects also erasures, the codes can replace maximum-distance separable (MDS) outer codes in concatenated constructions, thus resulting in linear-time encodable and decodable codes that approach the Zyablov bound or the capacity of memoryless channels. The presented construction improves on an earlier result by Guruswami and Indyk in that any rate and relative minimum distance that lies below the Singleton bound is attainable for a significantly smaller alphabet size","Graph theory,
Iterative decoding,
Error correction codes,
Concatenated codes,
Channel capacity,
Memoryless systems,
Error correction,
Information theory,
Computer science,
Materials science and technology"
Local Linear Regression (LLR) for Pose Invariant Face Recognition,"The variation of facial appearance due to the viewpoint (/pose) degrades face recognition systems considerably, which is well known as one of the bottlenecks in face recognition. One of the possible solutions is generating virtual frontal view from any given non-frontal view to obtain a virtual gallery/probe face. By formulating this kind of solutions as a prediction problem, this paper proposes a simple but efficient novel local linear regression (LLR) method, which can generate the virtual frontal view from a given non-frontal face image. The proposed LLR inspires from the observation that the corresponding local facial regions of the frontal and non-frontal view pair satisfy linear assumption much better than the whole face region. This can be explained easily by the fact that a 3D face shape is composed of many local planar surfaces, which satisfy naturally linear model under imaging projection. In LLR, we simply partition the whole non-frontal face image into multiple local patches and apply linear regression to each patch for the prediction of its virtual frontal patch. Comparing with other methods, the experimental results on CMU PIE database show distinct advantage of the proposed method",
Image Matching by Normalized Cross-Correlation,"Correlation is widely used as an effective similarity measure in matching tasks. However, traditional correlation based matching methods are limited to the short baseline case. In this paper we propose a new correlation based method for matching two images with large camera motion. Our method is based on the rotation and scale invariant normalized cross-correlation. Both the size and the orientation of the correlation windows are determined according to the characteristic scale and the dominant direction of the interest points. Experimental results on real images demonstrate that the new method is effective for matching image pairs with significant rotation and scale changes as well as other common imaging conditions",
Teaching DSP software development: from design to fixed-point implementations,"In this paper, a digital signal processing (DSP) software development process is described. It starts from the conceptual algorithm design and computer simulation using MATLAB, Simulink, or floating-point C programs. The finite-word-length analysis using MATLAB fixed-point functions or Simulink follows with fixed-point blockset. After verification of the algorithm, a fixed-point C program is developed for a specific fixed-point DSP processor. Software efficiency can be further improved by using mixed C-and-assembly programs, intrinsic functions, and optimized assembly routines in DSP libraries. This integrated software-development process enables students and engineers to understand and appreciate the important differences between floating-point simulations and fixed-point implementation considerations and applications.",
Sparse Forward-Backward Using Minimum Divergence Beams for Fast Training Of Conditional Random Fields,"Hidden Markov models and linear-chain conditional random fields (CRFs) are applicable to many tasks in spoken language processing. In large state spaces, however, training can be expensive, because it often requires many iterations of forward-backward. Beam search is a standard heuristic for controlling complexity during Viterbi decoding, but during forward-backward, standard beam heuristics can be dangerous, as they can make training unstable. We introduce sparse forward-backward, a variational perspective on beam methods that uses an approximating mixture of Kronecker delta functions. This motivates a novel minimum-divergence beam criterion based on minimizing KL divergence between the respective marginal distributions. Our beam selection approach is not only more efficient for Viterbi decoding, but also more stable within sparse forward-backward training. For a standard text-to-speech problem, we reduce CRF training time fourfold - from over a day to six hours - with no loss in accuracy",
Forgetting Test Cases,"Adaptive random testing (ART) methods are software testing methods which are based on random testing, but which use additional mechanisms to ensure more even and widespread distributions of test cases over an input domain. Restricted random testing (RRT) is a version of ART which uses exclusion regions and restriction of test case generation to outside these regions. RRT has been found to perform very well, but incurs some additional computational cost in its restriction of the input domain. This paper presents a method of reducing overheads called forgetting, where the number of test cases used in the restriction algorithm can be limited, and thus the computational overheads reduced. The motivation for forgetting comes from its importance as a human strategy for learning. Several implementations are presented and examined using simulations. The results are very encouraging",
A Brain Computer Interface Based on Neural Network with Efficient Pre-Processing,"Brain computer interface (BCI) is one of hopeful interface technologies between human and machine. However, brain waves are very weak and there exist many kinds of noises. Therefore, what kinds of features are useful, how to extract the useful features, how to suppress noises, and so on are very important. On the other hand, neural networks are very useful technology for pattern classification. Especially, multilayer neural networks trained through the error back-propagation algorithm have been widely used in a wide variety of field. In this paper, the neural network is applied to the BCI. Amplitude of the FFT of the brain waves are used for the input data. Several kinds of techniques are introduced in this paper. Segmentation along the time axis for fast response, nonlinear normalization for emphasizing important information with small magnitude, averaging samples of the brain waves for suppressing noise effects and reduction in the number of the samples for achieving a small size network, and so on are newly introduced. Simulation was carried out by using the brain waves, which are available from the Web site of Colorado State University. The number of mental tasks is five. Ten data sets for each mental task are prepared. Among them, 9 data sets are used for training, and the rest one data set is used for testing. Selection of the one data set for testing is changed and accuracy of the correct classifications are averaged over the possible selections. Approximately, 80 % of correct classification of the brain waves is obtained, which is higher than the conventional",
Adaptive Resource Allocation for Delay Differentiated Traffic in Multiuser OFDM Systems,"Most existing work on adaptive allocation of subcarriers and power in multiuser OFDM systems has focused on homogeneous traffic consisting of delay-constrained data (guaranteed service) or delay-tolerant data (best-effort service) only. In this work, we investigate the resource allocation problem in a heterogeneous multiuser OFDM system with both delay-constrained (DC) and no-delay-constrained (NDC) traffic. The objective is to maximize the sum-rate of all the users with NDC traffic while maintaining guaranteed rates for the DC traffic under a total transmission power constraint. Finding the optimal allocation of subcarriers and power is formulated as a convex programming problem. An iterative algorithm is proposed to compute the optimal solutions numerically. A low-complexity suboptimal allocation algorithm is also presented. Simulation experiments are conducted to evaluate the performance of the proposed algorithms in terms of service outage probability and achievable transmission rate pairs for DC and NDC traffic.",
The Research of Applying Wireless Sensor Networks to Intelligent Transportation System(ITS) Based On IEEE 802.15.4,"The intelligent transportation system (ITS) is envisioned by linking existing and emerging technologies of computers, wireless radio communications systems and sophisticated sensors to be used in vehicles and roads. IEEE 802.15.4 is a new standard designed for low rate wireless personal area networks (LR_WPANs) with a focus on enabling wireless sensor networks for vehicular, residential, commercial and industrial applications. This standard is characterized for its simplicity, low data rate, low power consumption and low cost in networks. The release of IEEE 802.15.4 is anticipated to make a significant contribution to the application of wireless sensor networks (WSNs) to ITS. In this article we first give a brief description of IEEE 802.15.4 standard and the characteristic of 802.15.4 for WSNs, then present a few application scenarios in vehicles and roads to show the potential applicability to which the new standard can be used to enable WSNs in ITS",
Transient Currents Generated by Heavy Ions With Hundreds of MeV,"Single Event Transient (SET) current is one of the most interesting issues for understanding and predicting Single Event Effects (SEEs) in real space environments. Since space radiation consists of high energy heavy ions, estimation of the energy dependence of SET currents is required. In order to estimate this effect, we have developed the Transient Ion Beam Induced Current (TIBIC) system combined with collimated high energy ion beams accelerated by AVF Cyclotron. We have demonstrated the function of TIBIC developed here. Using the newly developed TIBIC system and the Technology Computer Aided Design (TCAD) simulator, the influence of ion energy on SET currents is estimated",
Metrics for Mass-Count Disparity,"Mass-count disparity is the technical underpinning of the ""mice and elephants"" phenomenon - that most samples are small, but a few are huge - which may be the most important attribute of heavy-tailed distributions. We propose to visualize this phenomenon by plotting the conventional distribution and the mass distribution together in the same plot. This then leads to a natural quantification of the effect based on the distance between the two distributions. Such a quantification addresses this important phenomenon directly, taking the full distribution into account, rather than focusing on the mathematical properties of the tail of the distribution. In particular, it shows that the Pareto distribution with tail index 1 \le a \le 2 actually has a relatively low mass-count disparity; the effects often observed are the result of combining some other distribution with a Pareto tail.",
Towards a General Model for Selection in Virtual Environments,"Selection is one of the fundamental building blocks of all interactive virtual environment systems. Selection is the ability of the user to specify which object, or sub-part of an object in the environment, is the target for subsequent actions. Examples include selecting 3D buttons thus invoking an action or selecting a target upon which an action will occur. Selection is also an implicit or explicit part of manipulation techniques. In a virtual environment selection can be performed in many different ways. In this paper we develop a generalized model of how interaction is and could be performed in virtual environments using 3D gestures. The purpose of this model is to highlight some potential areas for development and evaluation of novel selection techniques. The model is based on an analysis of the complexity of selection. We develop a model for selection that is based on time-varying scalar fields (TVSFs) that encompasses a very broad range of existing techniques. This model will be abstract, in that a direct implementation will be prohibitively complex, but we show how some standard implementation strategies are good approximations to the formal model.",
"Magnetism, Structure, and Cation Distribution in MnFe_2
O_4
Films Processed by Conventional and Alternating Target Laser Ablation Deposition","A series of manganese ferrite thin film samples were prepared by alternating target laser ablation deposition and conventional pulsed laser deposition techniques on (111) MgO substrates. By extended X-ray absorption fine structure (EXAFS) analysis, we have discovered that the cation distribution was sensitive to the processing oxygen pressure and the preparation technique. Correspondingly, the saturation magnetization and uniaxial anisotropy fields change. Saturation magnetization was found to decrease while the percentage of Mn ions on the octahedral site increased, as a function of oxygen processing pressure. The highest magnetization (~4.5 kG) and anisotropy field (~0.5 kOe) corresponded to the sample grown at the lowest oxygen pressure",
A Tool for Prioritizing DAGMan Jobs and Its Evaluation,"It is often difficult to perform efficiently a collection of jobs with complex job dependencies due to temporal unpredictability of the grid. One way to mitigate the unpredictability is to schedule job execution in a manner that constantly maximizes the number of jobs that can be sent to workers. A recently developed scheduling theory provides a basis to meet that optimization goal. Intuitively, when the number of such jobs is always large, high parallelism can be maintained, even if the number of workers changes over time in an unpredictable manner. In this paper we present the design, implementation, and evaluation of a practical scheduling tool inspired by the theory. Given a DAGMan input file with interdependent jobs, the tool prioritizes the jobs. The resulting schedule significantly outperforms currently used schedules under a wide range of system parameters, as shown by simulation studies. For example, a scientific data analysis application, AIRSN, was executed at least 13% faster with 95% confidence. An implementation of the tool was integrated with the Condor high-throughput computing system",
Trust-based Resource Allocation in Web Services,"With the number of e-Business applications dramatically increasing, service level agreement (SLA) plays an important part in Web services. A SLA is a combination of several quality of services (QoS), such as security, performance, and availability, agreed between a customer and a service provider. Most existing research addresses only one QoS metric, and in the case of the response time, the average time to process and complete a job is typically used. In this paper, we study trustworthiness, percentile response time and availability. We consider all these qualities for a trust-based resource allocation problem which typically arises in Web services applications. We formulate the trust-based resource allocation problem as an optimization problem under SLA constraints, and we solve it using an efficient numerical procedure",
A Policy-Definition Language and Prototype Implementation Library for Policy-based Autonomic Systems,"This paper presents work towards generic policy toolkit support for autonomic computing systems in which the policies themselves can be adapted dynamically and automatically. The work is motivated by three needs: the need for longer-term policy-based adaptation where the policy itself is dynamically adapted to continually maintain or improve its effectiveness despite changing environmental conditions; the need to enable non autonomics-expert practitioners to embed self-managing behaviours with low cost and risk; and the need for adaptive policy mechanisms that are easy to deploy into legacy code. A policy definition language is presented; designed to permit powerful expression of self-managing behaviours. The language is very flexible through the use of simple yet expressive syntax and semantics and facilitates a very diverse policy behaviour space through both hierarchical and recursive uses of language elements. A prototype library implementation of the policy support mechanisms is described. The library reads and writes policies in well-formed XML script. The implementation extends the state of the art in policy-based autonomics through innovations which include support for multiple policy versions of a given policy type, multiple configuration templates and meta-policies to dynamically select between policy instances and templates. Most significantly, the scheme supports hot-swapping between policy instances. To illustrate the feasibility and generalised applicability of these tools, two dissimilar example deployment scenarios are examined. The first is taken from an exploratory implementation of self-managing parallel processing and is used to demonstrate the simple and efficient use of the tools.",
"Computing with Trust: Definition, Properties, and Algorithms","Trust is an important facet of relationships in social networks. Whether it is for use in security, determining data access, or recommender systems, the definition of trust guides the development of algorithms to make computations over the trust relationships. In this paper, we present a definition of trust based on sociological foundations. We then describe several properties of trust, including transitivity and composability, that follow from the definition and research in the social sciences, to help guide the methods used for computing with trust, and we discuss the range of values used for expressing trust. We then present a review of algorithms that compute trust in social networks",
Activity Topology Estimation for Large Networks of Cameras,"Estimating the paths that moving objects can take through the fields of view of possibly non-overlapping cameras, also known as their activity topology, is an important step in the effective interpretation of surveillance video. Existing approaches to this problem involve tracking moving objects within cameras, and then attempting to link tracks across views. In contrast we propose an approach which begins by assuming all camera views are potentially linked, and successively eliminates camera topologies that are contradicted by observed motion. Over time, the true patterns of motion emerge as those which are not contradicted by the evidence. These patterns may then be used to initialise a finer level search using other approaches if required. This method thus represents an efficient and effective way to learn activity topology for a large network of cameras, particularly with a limited amount of data.",
Opal: SimpleWeb Services Wrappers for Scientific Applications,"The grid-based infrastructure enables large-scale scientific applications to be run on distributed resources and coupled in innovative ways. However, in practice, grid resources are not very easy to use for the end-users who have to learn how to generate security credentials, stage inputs and outputs, access grid-based schedulers, and install complex client software. There is an imminent need to provide transparent access to these resources so that the end-users are shielded from the complicated details, and free to concentrate on their domain science. Scientific applications wrapped as Web services alleviate some of these problems by hiding the complexities of the back-end security and computational infrastructure, only exposing a simple SOAP API that can be accessed programmatically by application-specific user interfaces. However, writing the application services that access grid resources can be quite complicated, especially if it has to be replicated for every application. In this paper, we present Opal which is a toolkit for wrapping scientific applications as Web services in a matter of hours, providing features such as scheduling, standards-based grid security and data management in an easy-to-use and configurable manner",
Predicting Machine Availabilities in Desktop Pools,"This paper describes a study of predicting machine availabilities and user presence in a pool of desktop computers. The study is based on historical traces collected from 32 machines, and shows that robust prediction accuracy can be achieved even in this highly volatile environment. The employed methods include a multitude of classification methods known from data mining, such as Bayesian methods and support vector machines. Further contribution is a time series framework used in the study which automates correlations search and attribute selection, and allows for easy reconfiguration and efficient prediction. The results illustrate the utility of prediction techniques in highly dynamic computing environments. Potential applications for proactive management of desktop pools are discussed",
An Efficient Reference-Based Approach to Outlier Detection in Large Datasets,"A bottleneck to detecting distance and density based outliers is that a nearest-neighbor search is required for each of the data points, resulting in a quadratic number of pairwise distance evaluations. In this paper, we propose a new method that uses the relative degree of density with respect to a fixed set of reference points to approximate the degree of density defined in terms of nearest neighbors of a data point. The running time of our algorithm based on this approximation is 0(Rn log n) where n is the size of dataset and R is the number of reference points. Candidate outliers are ranked based on the outlier score assigned to each data point. Theoretical analysis and empirical studies show that our method is effective, efficient, and highly scalable to very large datasets.",
"An Approach for Developing Adaptive, Mobile Applications with Separation of Concerns","Modern mobile computing paradigms have set new challenges for the development of distributed mobile applications and services. Because of the variability which characterizes the context of such environments, it is important that mobile applications are developed so that they can dynamically adapt their extra-functional behavior, in order to optimize the experience perceived by their users. This paper proposes an approach for developing adaptive, mobile applications. It is argued that this approach eases the development effort by clearly separating the work required for the development of the application logic from that required for enabling its adaptive behavior. It is argued that in addition to mitigating the development complexity, this approach also enables a new generation of distributed applications. The novelty in the latter is that the applications can dynamically and collaboratively adapt in an ad-hoc manner to improve the quality of the services offered to mobile users",
QoS-aware Composite Service Selection in Grids,"Grid computing powered by Web service technology is creating abundant service-oriented applications which are promoting the sharing and collaboration of global resources. Complex tasks can be fulfilled quickly and efficiently by combining (physically distributed) individual services into a competent composite service. Since a lot of available individual services have the same functional properties while different non-functional properties (e.g. quality of service), decisions have to be made to select superior individual services from candidates for service composition. In this paper, the authors present a quality of service (QoS) model of service composition, as well as a composite service selection approach which can optimally choose the competent services, in the sense of satisfying users QoS requirement, for the demanded composite services with specified composite structures",
Cryptography from Anonymity,"There is a vast body of work on implementing anonymous communication. In this paper, we study the possibility of using anonymous communication as a building block, and show that one can leverage on anonymity in a variety of cryptographic contexts. Our results go in two directions. middot Feasibility. We show that anonymous communication over insecure channels can be used to implement unconditionally secure point-to-point channels, broadcast, and general multi-party protocols that remain unconditionally secure as long as less than half of the players are maliciously corrupted. middot Efficiency. We show that anonymous channels can yield substantial efficiency improvements for several natural secure computation tasks. In particular, we present the first solution to the problem of private information retrieval (PIR) which can handle multiple users while being close to optimal with respect to both communication and computation",
Experiences with Open Source Software Engineering Tools,"For software engineering (SE) and computer science (CS) programs to deliver on their promises, they must go beyond teaching students about principles, processes, models, and strategies and offer them realistic, practical experience as well. Although industry has been pressing to increase the emphasis on practical aspects, many CS programs continue to give students relatively simple problems focused on selected computing and software concepts and theories. Open source software offers CS and SE educators an opportunity to give their students practical, hands-on software engineering experience",
Spatial Information Based Image Segmentation Using a Modified Particle Swarm Optimization Algorithm,"This article proposes a particle swarm based segmentation algorithm for automatically grouping the pixels of an image into different homogeneous regions. In contrast to most of the existing evolutionary image segmentation techniques, we have incorporated spatial information into the membership function for clustering. The spatial function is the summation of the membership function in the neighborhood of each pixel under consideration. The two very important advantages of the new method are: 1) it does not require a priori knowledge of the number of partitions in the image and 2) it yields regions, more homogeneous than the existing methods even in presence of noise","Image segmentation,
Particle swarm optimization,
Clustering algorithms,
Pixel,
Computer science,
Electronic mail,
Automatic programming,
Genetic programming,
Genetic algorithms,
Evolutionary computation"
Quality Assessment of Mutation Operators Dedicated for C# Programs,"The mutation technique inserts faults in a program under test in order to assess or generate test cases, or evaluate the reliability of the program. Faults introduced into the source code are defined using mutation operators. They should be related to different, also object-oriented features of a program. The most research on OO mutations was devoted to Java programs. This paper describes analytical and empirical study performed to evaluate the quality of advanced mutation operators for C# programs. Experimental results demonstrate effectiveness of different mutation operators. Unit tests suites and functional tests were used in experiments. A detailed analysis was conducted on mutation operators dealing with delegates and exception handling",
Vortex Visualization for Practical Engineering Applications,"In order to understand complex vortical flows in large data sets, we must be able to detect and visualize vortices in an automated fashion. In this paper, we present a feature-based vortex detection and visualization technique that is appropriate for large computational fluid dynamics data sets computed on unstructured meshes. In particular, we focus on the application of this technique to visualization of the flow over a serrated wing and the flow field around a spinning missile with dithering canards. We have developed a core line extraction technique based on the observation that vortex cores coincide with local extrema in certain scalar fields. We also have developed a novel technique to handle complex vortex topology that is based on k-means clustering. These techniques facilitate visualization of vortices in simulation data that may not be optimally resolved or sampled. Results are included that highlight the strengths and weaknesses of our approach. We conclude by describing how our approach can be improved to enhance robustness and expand its range of applicability",
Secure Dissemination of XML Content Using Structure-based Routing,"The paper proposes an approach to content dissemination that exploits the structural properties of XML document object model in order to provide efficient dissemination by at the same time assuring content integrity and confidentiality. Our approach is based on the notion of encrypted post-order numbers that support the integrity and confidentiality requirements of XML content as well as facilitate efficient identification, extraction and distribution of selected content portions. By using such notion, we develop a structure-based routing scheme that prevents information leaks in XML-data dissemination and assures that content is delivered to users according to the access control policies, that is, policies specifying which users can receive which portions of the contents. Our proposed dissemination approach further enhances such structure-based, policy-based routing by combining it with multicast in order to provide high efficiency in terms of bandwidth usage and speed of data delivery, thereby enhancing scalability",
The GRAPE project,"The goal of the Gravity Pipe (GRAPE) project is to accelerate astrophysical N-body simulations. Because almost all computing time is spent evaluating the gravitational force between particles, we can greatly accelerate many N-body simulations by developing a specialized hardware system for the force calculation.",
The Role of Ontologies in Context-Aware Recommender Systems,"This position paper describes the role ontologies can play in Mobile Context-Aware recommender systems. In a Semantic Web vision of recommender systems, the adoption of ontologies for modeling the domain, the context and the adaptation process can contribute to tailor the right information/service to users and thus facilitate the user-system interaction and the system communication with other agents.",
A Link Prediction Approach to Anomalous Email Detection,"In many security informatics applications, it is important to monitor traffic over various communication channels and efficiently identify those communications that are unusual for further investigation. This paper studies such anomaly detection problems using a graph-theoretic link prediction approach. Data from the publicly-available Enron email corpus were used to validate the proposed approach.","Monitoring,
Electronic mail,
Data security,
Informatics,
Communication channels,
Predictive models,
Educational institutions,
Cybernetics,
Communication system security,
Computer security"
Adaptive reproducing kernel particle method for extraction of the cortical surface,"We propose a novel adaptive approach based on the Reproducing Kernel Particle Method (RKPM) to extract the cortical surfaces of the brain from three-dimensional (3-D) magnetic resonance images (MRIs). To formulate the discrete equations of the deformable model, a flexible particle shape function is employed in the Galerkin approximation of the weak form of the equilibrium equations. The proposed support generation method ensures that support of all particles cover the entire computational domains. The deformable model is adaptively adjusted by dilating the shape function and by inserting or merging particles in the high curvature regions or regions stopped by the target boundary. The shape function of the particle with a dilation parameter is adaptively constructed in response to particle insertion or merging. The proposed method offers flexibility in representing highly convolved structures and in refining the deformable models. Self-intersection of the surface, during evolution, is prevented by tracing backward along gradient descent direction from the crest interface of the distance field, which is computed by fast marching. These operations involve a significant computational cost. The initial model for the deformable surface is simple and requires no prior knowledge of the segmented structure. No specific template is required, e.g., an average cortical surface obtained from many subjects. The extracted cortical surface efficiently localizes the depths of the cerebral sulci, unlike some other active surface approaches that penalize regions of high curvature. Comparisons with manually segmented landmark data are provided to demonstrate the high accuracy of the proposed method. We also compare the proposed method to the finite element method, and to a commonly used cortical surface extraction approach, the CRUISE method. We also show that the independence of the shape functions of the RKPM from the underlying mesh enhances the convergence speed of the deformable model","Kernel,
Deformable models,
Shape,
Equations,
Merging,
Data mining,
Magnetic resonance,
Magnetic resonance imaging,
Computer interfaces,
Computational efficiency"
Sentiment Classification for Movie Reviews in Chinese by Improved Semantic Oriented Approach,"Sentiment classification aims at mining reviews of customers for a certain product by automatic classifying the reviews into positive or negative opinions. With the fast developing of World Wide Web applications, sentiment classification would have huge opportunity to help people automatic analysis of customers’ opinions from the web information. Automatic opinion mining will benefit to both consumers and sellers. Up to now, it is still a complicated task with great challenge. There are mainly two types of approaches for sentiment classification, machine learning methods and semantic orientation methods. Though some pioneer researches explored the approaches for English movie review classification, few jobs have been done on sentiment classification for Chinese reviews. The improved semantic approach for sentiment classification on movie reviews written in Chinese was proposed in this paper. Data experiment shows the capability of this approach.",
General Architecture for Hardware Implementation of Genetic Algorithm,"In this paper, the authors propose a technique to flexibly implement genetic algorithms (GAs) for various problems on FPGAs. For the purpose, the authors propose a common architecture for GA. The proposed architecture allows designers to easily implement a GA as a hardware circuit consisting of parallel pipelines which execute GA operations. The proposed architecture is scalable to increase the number of parallel pipelines. The architecture is applicable to various problems and allows designers to estimate the size of resulting circuits. The authors give a model for predicting the size of resulting circuits from given parameters. Based on the proposed method, the authors have implemented a tool to facilitate GA circuit design and development. Through experiments using knapsack problem and traveling salesman problem (TSP), the authors show that the FPGA circuits synthesized based on the proposed method run much faster and consume much lower power than software implementation on a PC and the model can predict the size of the resulting circuit accurately enough",
Windowed Erasure Codes,"The design of erasure correcting codes and their decoding algorithms is now at the point where capacity achieving codes are available with decoding algorithms that have complexity that is linear in the number of information symbols. One aspect of these codes is that the overhead (number of coded symbols beyond the number of information symbols required to achieve decoding completion with high probability) is linear in k. This work considers a new class of random codes which have the following advantages: (i) the overhead is constant (in the range of 5 to 10) (ii) the probability of completing decoding for such an overhead is essentially one (iii) the codes are effective for a number of information symbols as low as a few tens. The price for these properties is that the decoding complexity is greater, on the order of k 3/2. However, for the lower values of k where these codes are of particular interest, this increase in complexity might be outweighed by other significant advantages. The parity check matrices of these codes are chosen at random as windowed matrices i.e. for each column an initial starting position of a window of length w is chosen and the succeeding w positions are chosen at random by zero or one. It can be shown that it is necessary that w = O(k1/2) for the probabilistic matrix rank properties to behave as a non-windowed random matrix. The sufficiency of the condition has so far been established by extensive simulation, although other arguments strongly support this conclusion",
An Efficient Skin Illumination Compensation Model for Efficient Face Detection,"Face detection (human) plays an important role in applications such as human computer interface, face recognition video surveillance and face image database management. In the human face detection applications, face(s) most frequently form an inconsequential part of the images. Consequently, preliminary segmentation of images into regions that contain ""non-face"" objects and regions that may contain ""face"" candidates can greatly accelerate the process of human face detection. Most existing face detection approaches have assumptions, which make them applicable only under some specific conditions. Existing techniques for face detection in colour images are plagued by poor performance in the presence of scale variation, variation in illumination, variation in skin colours, complex backgrounds etc. In this research work we have made a humble attempt to propose an algorithm for face detection in colour images in the presence of varying lighting conditions, for varied skin colours as well as with complex backgrounds. Based on a novel tangible skin component extraction modus operandi and detection of the valid face candidates, our method detects skin regions over the entire image and engenders face candidates based on the signatures of the detected skin patches. The algorithm constructs the boundary for each face candidate. Experimental results demonstrate successful face detection over a wide range of facial variations in colour, position, scale, varying lighting conditions, orientation, 3D pose, and expression in images from the database which is enriched with several photo collections (both indoors and outdoors with all the above mentioned extreme cases taken into consideration). In view of the capability to handle high degree of variability, that our approach can handle we articulate that our approach outperforms all the published popular methods",
FleaNet: A Virtual Market Place on Vehicular Networks,"Over recent years, mobile Internet devices such as laptops, PDAs, smart phones etc, have become extremely popular and widespread. Once on board of a vehicle, these devices can automatically connect to the vehicle processor and thus greatly amplify the communications and processing capabilities available to the owner in a ""pedestrian mode"". We envision that this ""amplification"" opportunity will be one of the drivers of car to car and car to curb communications. In fact, the car communications system will not be used exclusively for mobile Internet access, but also as a distributed platform for the ""opportunistic"" cooperation among people with shared interests/goals. Exchanging safety messages among vehicles is a compelling example. Stretching opportunistic cooperation well beyond safety messages, we discuss in this paper the concept of virtual ""flea market"" over VANET called FleaNet In FleaNet, customers, either mobile (i.e., vehicles) or stationary (i.e., pedestrians, roadside shop owner), express their demands/offers, e.g., want to buy or sell an item, via radio queries. These queries are opportunistically disseminated exploiting in part the mobility of other customers in order to find the customer/vendor with matching needs/resources. In the paper we identify the key performance metrics, namely query resolution latency, scalability, and mobility. Based on the metrics, using models and simulation, we show that FleaNet can efficiently support a market place over vehicular networks",
SIM: Scalable Island Multicast for Peer-to-Peer Media Streaming,"Despite the fact that global multicast is still not possible in today's Internet, many local networks are already multicast-capable (the so-called multicast ""islands""). However, most application-layer multicast (ALM) protocols for streaming has not taken advantage of the underlying IP multicast capability. As IP multicast is more efficient, it would be beneficial if ALM can take advantage of such capability in building overlay trees. In this paper, we propose a fully distributed protocol called Scalable Island Multicast (SIM), which effectively integrates IP multicast and ALM. Hosts in SIM first form an overlay tree using a scalable protocol. They then detect IP multicast islands and employ IP multicast whenever possible. Through simulations on Internet-like topologies, we show that SIM achieves much lower end-to-end delay and link stress as compared with traditional ALM protocols","Streaming media,
Peer to peer computing,
Multicast protocols,
Delay,
IP networks,
Access protocols,
Internet,
Topology,
Stress,
Unicast"
A hierarchical secure routing protocol against black hole attacks in sensor networks,"A black hole attack is a severe attack that can be easily employed against routing in sensor networks. In a black hole attack, a malicious node spuriously announces a short route to the sink node (the destination) to attract additional traffic to the malicious node and then drops them. In this paper, we propose a hierarchical secure routing protocol for detecting and defending against black hole attacks. The proposed protocol uses only symmetric key cryptography to discover a safe route against black hole attacks. The comparison of the proposed protocol with two other existing approaches proves that the proposed scheme is faster in detecting black hole attacks with much lower message overhead",
ArchTrace: Policy-Based Support for Managing Evolving Architecture-to-Implementation Traceability Links,"Traditional techniques of traceability detection and management are not equipped to handle evolution. This is a problem for the field of software architecture, where it is critical to keep synchronized an evolving conceptual architecture with its realization in an evolving code base. ArchTrace is a new tool that addresses this problem through a policy-based infrastructure for automatically updating traceability links every time an architecture or its code base evolves. ArchTrace is pluggable, allowing developers to choose a set of traceability management policies that best match their situational needs and working styles. We discuss ArchTrace, its conceptual basis, its implementation, and our evaluation of its strengths and weaknesses in a retrospective analysis of data collected from a 20 month period of development of Odyssey, a large-scale software development environment. Results are promising: with respect to the ideal set of traceability links, the policies applied resulted in 95% precision at 89% recall",
Modeling Multiple Time Units Delayed Gene Regulatory Network Using Dynamic Bayesian Network,"Most of the current applications which use dynamic Bayesian network to model gene regulatory network assume that the time delay between regulators and their targets is one time unit in a time series gene expression dataset. In fact, multiple time units delay is indicated to exist in a gene regulation process. In this paper, we propose using higher-order Markov dynamic Bayesian network (DBN) to model multiple time units delayed gene regulatory network. A two steps heuristic learning framework is designed to learn higher-order Markov DBN from time series gene expression data. We apply the learning framework to a yeast cell cycle gene expression dataset. The predicted gene regulatory network is strongly supported by biological evidence and consistent with the yeast cell cycle phase information",
State Estimation in Discrete Event Systems Modeled by Labeled Petri Nets,"In this paper, we address the problem of state estimation in discrete event systems (DES) modeled by labeled Petri nets that may have nondeterministic transitions (i.e., transitions that share the same label) or unobservable transitions (i.e., transitions that are associated with the null label). More specifically, given knowledge of the initial Petri net state (or set of states), we show that the number of consistent markings in a Petri net with nondeterministic transitions is at most polynomial in the length of the observation sequence (i.e., in the number of labels observed) even though the set of possible firing sequences can be exponential in the length of the observation sequence. The result applies to general Petri nets without any specific assumption on the structure of the Petri net or the nature of the labeling function. By restricting attention to bounded Petri nets with acyclic unobservable subnets, this polynomial dependency of the number of consistent markings on the length of the observation sequence also applies to Petri nets with unobservable transitions",
Teaching Evolution of Open-Source Projects in Software Engineering Courses,"In the traditional software engineering courses, the students develop small programs from scratch. This does not correspond to industry practice where programmers spend most of their time evolving medium to large systems. In order to narrow this gap, we developed a course where students practice software evolution through the implementation of change requests on medium-sized open-source software systems. The results of the course show that this type of software engineering course gives students a more realistic experience than traditional software engineering courses. In the survey at the end of the course, the students expressed a higher level of satisfaction with both rating the course and assessing how much they learned. Additionally, the resources required by such a course are not excessive",
Effective Density Queries on ContinuouslyMoving Objects,"This paper assumes a setting where a population of objects move continuously in the Euclidean plane. The position of each object, modeled as a linear function from time to points, is assumed known. In this setting, the paper studies the querying for dense regions. In particular, the paper defines a particular type of density query with desirable properties and then proceeds to propose an algorithm for the efficient computation of density queries. While the algorithm may exploit any existing index for the current and near-future positions of moving objects, the Bx-tree is used. The paper reports on an extensive empirical study, which elicits the performance properties of the algorithm.","Computer science,
Consumer electronics,
Mobile communication,
Communications technology,
Paper technology,
Proposals,
Nearest neighbor searches,
Data engineering"
Multivariate Microaggregation Based Genetic Algorithms,"Microaggregation is a clustering problem with cardinality constraints that originated in the area of statistical disclosure control for micro data. This article presents a method for multivariate microaggregation based on genetic algorithms (GA). The adaptations required to characterize the multivariate microaggregation problem are explained and justified. Extensive experimentation has been carried out with the aim of finding the best values for the most relevant parameters of the modified GA: the population size and the crossover and mutation rates. The experimental results demonstrate that our method finds the optimal solution to the problem in almost all experiments when working with small data sets. Thus, for small data sets the proposed method performs better than known polynomial heuristics and can be combined with these for larger data sets. Moreover, a sensitivity analysis of parameter values is reported which shows the influence of the parameters and their best values","Genetic algorithms,
Genetic mutations,
Polynomials,
Sensitivity analysis,
Data security,
Intelligent systems,
Control systems,
Privacy,
Evolutionary computation,
Clustering algorithms"
Scalable shape sculpting via hole motion: motion planning in lattice-constrained modular robots,"We describe a novel shape formation algorithm for ensembles of 2-dimensional lattice-arrayed modular robots, based on the manipulation of regularly shaped voids within the lattice (""holes""). The algorithm is massively parallel and fully distributed. Constructing a goal shape requires time proportional only to the complexity of the desired target geometry. Construction of the shape by the modules requires no global communication nor broadcast floods after distribution of the target shape. Results in simulation show 97.3% shape compliance in ensembles of approximately 60,000 modules, and we believe that the algorithm will generalize to 3D and scale to handle millions of modules",
Visualization Tools for Vorticity Transport Analysis in Incompressible Flow,Vortices are undesirable in many applications while indispensable in others. It is therefore of common interest to understand their mechanisms of creation. This paper aims at analyzing the transport of vorticity inside incompressible flow. The analysis is based on the vorticity equation and is performed along pathlines which are typically started in upstream direction from vortex regions. Different methods for the quantitative and explorative analysis of vorticity transport are presented and applied to CFD simulations of water turbines. Simulation quality is accounted for by including the errors of meshing and convergence into analysis and visualization. The obtained results are discussed and interpretations with respect to engineering questions are given,
L0-Norm-Based Sparse Representation Through Alternate Projections,"We present a simple and robust method for finding sparse representations in overcomplete transforms, based on minimization of the L0-norm. Our method is better than current solutions based on minimization of the L1-norm in terms of energy compaction. These results strongly question the equivalence of minimizing both norms in real conditions. We also show application to in-painting (interpolation of lost pixels).",
LTL with the Freeze Quantifier and Register Automata,"Temporal logics, first-order logics, and automata over data words have recently attracted considerable attention. A data word is a word over a finite alphabet, together with a datum (an element of an infinite domain) at each position. Examples include timed words and XML documents. To refer to the data, temporal logics are extended with the freeze quantifier, first-order logics with predicates over the data domain, and automata with registers or pebbles. We investigate relative expressiveness and complexity of standard decision problems for LTL with the freeze quantifier (LTLdarr), 2-variable first-order logic (FO2) over data words, and register automata. The only predicate available on data is equality. Previously undiscovered connections among those formalisms, and to counter automata with incrementing errors, enable us to answer several questions left open in recent literature. We show that the future-time fragment of LTLdarr which corresponds to FO2 over finite data words can be extended considerably while preserving decidability, but at the expense of non-primitive recursive complexity, and that most of further extensions are undecidable. We also prove that surprisingly, over infinite data words, LTLdarr without the 'until' operator, as well as nonemptiness of one-way universal register automata, are undecidable even when there is only 1 register",
A Simulation Study on Optically Decoding Reflecting Windows for PMT Quadrant Sharing Scintillation Detector Block,"A large number of decodable crystals per photomultiplier tube (PMT) can be achieved by using the PMT-quadrant-sharing (PQS) technique with proper optically reflecting windows to channel light distribution in scintillation detector block. However, to develop brand new optically decoding reflecting windows for a detector block with different crystal material, PMT size or decoding resolution, is still very time-consuming and also requires much experience. This study is to develop a computer software tool that can simulate an expected two-dimensional (2-D) crystal decoding map before implementing a real detector block with a new set of decoding reflectors. After comparing the experimental decoding data to the simulated results with the same reflector set on a block, data are feed to adjust the software parameters. More accurate decoding reflectors will then be created by the adjusted parameters. Our result shows the decoding simulation of a 10times10 BGO block can be finished within a few minutes, which is much faster than using the Monte Carlo simulation with ""DETECT"". This software has been evaluated by five different PQS blocks. Our preliminary study shows this simulation tool is very promising which can significantly reduce a new product developing time; only about two-three development cycles are needed to get to the final optimized decoding reflectors","Decoding,
Scintillation counters,
Computational modeling,
Crystals,
Photomultipliers,
Solid scintillation detectors,
Crystalline materials,
Optical materials,
Software tools,
Computer simulation"
Impact of preprogramming course curriculum on learning in the first programming course,"This paper reports the results of research that measured the value of integrating problem solving, algorithm development, pseudocode, and diagramming techniques into introductory computer science courses. The hypothesis was that the introduction of these topics prior to the introduction of a programming language would reduce the learning curve requirements and increase the success rate for beginning programmers. Posttest programming scores for like tests exhibited some difference within the sample, with the treatment group performing better than the control group. Members of the treatment group reported a better experience in the introductory course and saw more applicability for that course than members of the control group. In addition, treatment group members utilized pseudocode more consistently and applied the idea of drawing pictures to aid in writing code during their posttest experience. The combined results indicated that learners, in general, found that the introductory course contained useful content and that they could successfully apply processes learned in that course as part of a programming strategy.",Computer science education
Generating Domain-Specific Visual Language Editors from High-level Tool Specifications,"Domain-specific visual language editors are useful in many areas of software engineering but developing such editors is challenging and time-consuming. We describe an approach to generating a wide range of these graphical editors for use as plug-ins to the Eclipse environment. Tool specifications from an existing meta-tool, Pounamu, are interpreted to produce dynamic, multi-view, multiuser Eclipse graphical editors. We describe the architecture and implementation of our approach, examples of its use realizing domain-specific modelling tools, and strengths and limitations of the approach","Web services,
Software engineering,
Domain specific languages,
Unified modeling language,
Navigation,
Computer aided software engineering,
Computer science,
Usability,
Computer architecture,
Software architecture"
Distance-increasing mappings from binary vectors to permutations that increase hamming distances by at least two,"In this correspondence, for any k ges 2, we first propose two constructions of (n,k) distance-increasing mappings (DIMs) from the set of binary vectors of length n to the set of permutations of the same length that strictly increase the Hamming distance by at least k except when it is obviously not possible. Next, we prove that for any k ges 2, there is a smallest positive integer nk such that an (n,k) DIM can be constructed for any n ges nk. An explicit upper bound on nk is also given",
OLSR-FastSync: Fast Post-Handoff Route Discovery in Wireless Mesh Networks,"Wireless mesh networks (WMNs) are a promising networking concept to extend the range of wireless gateways by leveraging fixed relays as packet forwarders for nodes further away. WMNs use ad-hoc routing protocols such as AODV or OLSR to route packets between wireless gateways, fixed relays, and mobile nodes (MNs). This paper focuses on handoff-related delays specific to WMNs: delays due to re-discovery and setup of multihop routes after handoffs. Since in case of OLSR routing information is only sent periodically to reduce routing overhead, route discovery can take up to a few seconds, which is far too long for delay-sensitive applications like Voice over IP. Therefore, we propose the FastSync extension for OLSR, which enables MNs to obtain all topology and gateway information immediately after handoff from a fixed relay of the handoff-target WMN. As a result, the long waiting phase for the first regular routing update after handoff is avoided. We implemented our concept and performed extensive simulations. The results show that OLSR-FastSync reduces handoff-related route setup delays significantly.",
Cooperative Multiple Access for Wireless Networks: Protocols Design and Stability Analysis,"A new cooperative multiple-access protocol is proposed in which a relay utilizes the empty time slots available in a TDMA frame. In particular, the relay helps the users in the network forwarding their unsuccessfully transmitted packets during the empty time slots. This will better utilize the channel resources that are otherwise wasted, and will introduce on-demand spatial diversity into the network. The proposed cooperation protocol is also different from those proposed in the literature as it does not result in any bandwidth loss. Two different protocols are proposed to implement this new multiple-access scheme, and their stability regions are characterized. The stability regions of the proposed protocols are shown to contain that of TDMA without relaying, hence, relay deployment in wireless networks can increase the network throughput capacity. Moreover, the analysis and numerical results reveal that the proposed cooperative multiple access protocols can simultaneously achieve higher stable throughput and less energy expenditure compared to TDMA without relaying.","Wireless networks,
Wireless application protocol,
Access protocols,
Stability analysis,
Time division multiple access,
Throughput,
Frame relay,
Bandwidth,
Stability criteria,
Energy efficiency"
Integrated Buffer and Route Management in a DTN with Message Ferry,"Unlike normal wireless ad hoc networks, end-to-end connection may not exist in DTNs. Thus, the message ferrying (MF) scheme has been proposed as a strategy for providing connectivity in disruption tolerant network (DTN)s, where a set of nodes called ferries are responsible for carrying messages for all nodes in the networks. In such store-and-forward networks, buffers at ferry and regular nodes become critical resources and need to be allocated fairly among different users. In this paper, we propose a max-min fairness model for a DTN with a message ferry. Based on this model, we propose a buffer allocation scheme that can achieve fairness among different sessions. We also design an integrated buffer and routing management scheme called buffer efficient routing scheme (BERS). Via simulations, we demonstrate that our fair buffer allocation scheme assigns buffers fairly to different sessions. Our simulation studies also show that BERS can achieve higher session throughput and lower packet delivery latency than the only-store-and-forward routing scheme that is typically used in a message ferry system",
Modeling and Analysis of Frame Aggregation in Unsaturated WLANs with Finite Buffer Stations,"Frame aggregation is one of the several enhancements proposed by IEEE 802.11 Task Group n to improve channel utilization. In frame aggregation, more than one data frame is encapsulated to form an aggregate, and once an aggregate is formed, a station contends to access the medium to transmit the entire aggregate. We refer to the number of data frames encapsulated within an aggregated frame as aggregate size. We claim that a static assignment of aggregate size leads to the following performance trade-off: a small value might be insufficient to mitigate the transmission overheads, thereby nullifying the whole purpose of frame aggregation; whereas, a large value might affect the quality of service experienced by higher layers due to the extra wait time to build an aggregate. In this paper, we characterize this trade-off by studying the impact of aggregate size on metrics like frame latency and channel utilization. To estimate these metrics, we model the transmission queue of an 802.11n station as a bulk service queuing system. We study the impact of aggregate size over a wide range of operating conditions covering several traffic arrival rates from higher layers, service distribution and data frame sizes. Apart from validating the existence of above-mentioned performance trade-off, our results indicate that the choice of aggregate size not only depends on the traffic arrival rate, but also (more interestingly) on data frame sizes. This calls for a dynamic assignment of aggregate size.","Aggregates,
Physical layer,
Media Access Protocol,
Quality of service,
Traffic control,
Computer science,
Delay,
Encapsulation,
Analytical models,
Computer networks"
"Integrating Data, Models, and Reasoning in Critical Care","Modern intensive care units (ICUs) employ an impressive array of technologically sophisticated instrumentation to provide detailed measurements of the pathophysiological state of each patient. Providing life support in the ICU is becoming an increasingly complex task, however, because of the growing volume of relevant data from clinical observations, bedside monitors, mechanical ventilators, and a wide variety of laboratory tests and imaging studies. The enormous amount of ICU data and its poor organization makes its integration and interpretation time-consuming and inefficient. There is a critical need to integrate the disparate clinical information into a single, rational framework and to provide the clinician with hypothesis-driven displays that succinctly summarize a patient's trajectory over time. In this paper, we present our recent efforts towards the development of such an advanced patient monitoring system that aims to improve the efficiency, accuracy, and timeliness of clinical decision making in intensive care",
A Secure fragile Watermarking Algorithm for medical Image Authentication in the DCT Domain,The ease and extensive use of digital imaging systems raise serious questions about integrity and authenticity of digital images particularly in medical applications. A medical image requires great integrity of content since any change in the image might affect the diagnosis even if it is of small amount. Fragile watermark could be the best technique to provide efficient image authentication tool for medical images. In this paper we present different methods developed in image watermarking used in order to verify integrity and authenticity of images. We present after that our proposed method and validated the embedding and extracting procedures,"Watermarking,
Biomedical imaging,
Authentication,
Discrete cosine transforms,
Digital images,
Medical diagnostic imaging,
Medical services,
Biomedical equipment,
Computer networks,
Image storage"
Exploring Perturbation Based Testing for Web Services,"Web service is a modern technology commonly used to integrate software projects among different platforms, operating systems or even programming languages. This distributed and heterogeneous nature complicates the testing activity which is, in general, expensive and effort demanding. Adequate and cost effective testing methods are needed for Web services. An extended approach based on XML messages perturbation has been introduced to test pairs of Web services. Perturbation operators produce modified XML messages, which are used as test cases. This paper explores the use of such promising approach by introducing new perturbation operators for SOAP messages and describing a supporting tool, named SMAT-WS. An experimental study was accomplished with this tool. The obtained results allow an evaluation of the perturbation operators regarding cost and efficacy",
Adaptive Channel Selection Through Collaborative Sensing,Proper channel selection is essential to exploit the benefits of multi-channel systems by distributing conflicting transmissions across non-interfering channels. Critical to channel selection is the channel quality metric. We propose a busy time ratio (BTR) metric that captures channel contention and user traffic load under a variety of network dynamics. We also propose a distributed collaborative sensing scheme to reduce sensing overhead and energy consumptions. The proposed algorithms can be implemented using conventional 802.11 hardware with single radio interface. The proposed metric can be integrated with routing and channel selection. Experimental results show that the proposed scheme significantly outperforms the existing channel selection methods.,
Gabor Filter Analysis for Texture Segmentation,"Gabor features are a common choice for texture analysis. The particular set of Gabor filters used for extracting the features is usually designed for optimal signal representation. We propose here an alternative criterion for designing the filter set. We consider a set of filters and its response to pairs of harmonic signals. Two signals are considered separable if the corresponding two sets of vector responses are disjoint in at least one of the components. We look for the set of Gabor filters that maximizes the fraction of separable harmonic signal pairs. The resulting filters are significantly different from the traditional ones. We test these maximal harmonic discrimination (MHD) filters using two texture discrimination methods, and describe their advantages over traditional filters.",
Variables as Resource in Hoare Logics,"Hoare logic is bedevilled by complex but coarse side conditions on the use of variables. We define a logic, free of side conditions, which permits more precise statements of a program's use of variables. We show that it admits translations of proofs in Hoare logic, thereby showing that nothing is lost, and also that it admits proofs of some programs outside the scope of Hoare logic. We include a treatment of reference parameters and global variables in procedure call (though not of parameter aliasing). Our work draws on ideas from separation logic: program variables are treated as resource rather than as logical variables in disguise. For clarity we exclude a treatment of the heap",
Iris Recognition Algorithm Optimized for Hardware Implementation,Iris recognition is accepted as one of the most efficient biometric method. Implementing this method to the practical system requires the special image preprocessing where the iris feature extraction plays a crucial role. Recognition is preceeded by iris localization which consists in finding the iris boundaries as well as eyelids. In this paper the short introduction into iris localization and wavelet-based iris identification method optimized for embedded systems is described. Proposed solution was applied to high resolution images taken under near infrared light,"Iris recognition,
Hardware,
Histograms,
Image segmentation,
Optical reflection,
Feature extraction,
Eyelids,
Embedded system,
Image resolution,
Infrared imaging"
Fault tolerant system design method based on self-checking circuits,This paper describes a highly reliable digital circuit design method based on totally self checking blocks implemented in FPGAs. The bases of the self checking blocks are parity predictors. The parity predictor design method based on multiple parity groups is proposed. Proper parity groups are chosen in order to obtain minimal area overhead and to decrease the number of undetectable faults,"Fault tolerant systems,
Design methodology,
Field programmable gate arrays,
Circuit faults,
Single event transient,
Minimization,
Input variables,
Computer science,
Reliability engineering,
Design engineering"
Wavelet-based Real Time Detection of Network Traffic Anomalies,"Real time network monitoring for intrusions is offered by various host and network based intrusion detection systems. These systems largely use signature or pattern matching techniques at the core and thus are ineffective in detecting unknown anomalous activities. In this paper, we apply signal processing techniques in intrusion detection systems, and develop and implement a framework, called Waveman, for real time wavelet-based analysis of network traffic anomalies. Then, we use two metrics, namely percentage deviation and entropy, to evaluate the performance of various wavelet functions on detecting different types of anomalies like denial of service (DoS) attacks and portscans. Our evaluation results show that Coiflet and Paul wavelets perform better than other wavelets in detecting most anomalies considered in this work","Telecommunication traffic,
Real time systems,
Intrusion detection,
Monitoring,
Pattern matching,
Signal processing,
Wavelet analysis,
Signal analysis,
Entropy,
Computer crime"
C4W: An Energy Efficient Public Key Cryptosystem for Large-Scale Wireless Sensor Networks,"With hardware support and software optimization, public key cryptography (PKC) has been announced feasible on micro sensors recently. A number of experiments proved that the elliptic curve cryptography (ECC) is more suitable for resource constraint motes compared with RSA, But even ECC based protocols still cost too much energy. In this paper, we propose C4W, an identity-based public key infrastructure specially designed for wireless sensor networks (WSNs), in which all nodes can generate other's ECC public keys directly from their identities. Without certificates, no energy will be consumed for certificates communication and verification, which makes C4W especially energy efficient. C4W uses a protocol without certificates to realize mutual authentication and key agreement. Compared with a simplified SSL (SSSL) protocol using an abbreviated certificate, C4W consumes lower than 35% energy, and the communication consumption of C4W is only 28.5% of that consumed by SSSL. Furthermore, the energy analysis of C4W illuminates that the expensive public key computational cost is almost neglectable compared with the heavy communication consumption in a large-scale WSNs, which gives the asymmetric key management in WSNs a bright future",
Factor Graphs for Region-based Whole-scene Classification,"Semantic scene classification is still a challenging problem in computer vision. In contrast to the common approach of using low-level features computed from the scene, our approach uses explicit semantic object detectors and scene configuration models. To overcome faulty semantic detectors, it is critical to develop a region-based, generative model of outdoor scenes based on characteristic objects in the scene and spatial relationships between them. Since a fully connected scene configuration model is intractable, we chose to model pairwise relationships between regions and estimate scene probabilities using loopy belief propagation on a factor graph. We demonstrate the promise of this approach on a set of over 2000 outdoor photographs, comparing it with existing discriminative approaches and those using low-level features.","Layout,
Detectors,
Computer science,
Computer vision,
Pattern recognition,
Conferences,
Terminology,
Upper bound,
Maximum likelihood detection,
Object detection"
Simulation and Experimental Study of the Impedance Detection Anti-Islanding Method in the Single-Inverter Case,"The impedance detection method of islanding prevention is often touted as one that possesses no nondetection zone in the single-inverter case. However, simulation results and experiments indicate that this is not necessarily true; the effectiveness of impedance detection is highly dependent on the specific implementation of the method. This paper discusses a simulation and experimental study to examine the origins of this nondetection zone, and a simple modification that appears to significantly reduce the size of the nondetection zone","Impedance,
Inverters,
Voltage,
Mathematical model,
Power system modeling,
Power harmonic filters,
RLC circuits,
Band pass filters,
Computational modeling,
Computer simulation"
Designing Stable Inverters and State Observers for Switched Linear Systems with Unknown Inputs,"We present a method for estimating the inputs and states in discrete-time switched linear systems with unknown inputs. We first investigate the problem of system invertibility, which reconstructs the unknown inputs based on knowledge of the output of the system, the switching sequence, and the initial system state. We then relax the assumption on the knowledge of the initial system state, and construct observers that asymptotically estimate the state. Our design, which considers a general class of switched linear observers that switch modes based on the known (but arbitrary) switching sequence, shows that system invertibility is necessary in order to construct state observers. Furthermore, some portion of the observer gain must be used to recover the unknown inputs, and the remaining freedom must be used to ensure stability. The state of the observer is then used to asymptotically estimate the unknown inputs (i.e., it forms the dynamic portion of a stable inverter for the given switched system)",
Some Optimization Trade-offs in Wireless Network Coding,"In this paper, we consider different optimization trade-offs in wireless networks with saturated or possibly emptying packet queues and specify the resulting cross-layer interactions in medium access control (MAC) and network layers. We separately consider scheduled and random access for MAC layer, whereas network layer operations are modeled by network coding or plain routing. Our objective is to analyze the trade-offs among performance objectives of maximizing aggregate or minimum throughput over different source-destination pairs and minimizing (transmission and coding) energy costs using a tandem wireless network model with the assumptions of omnidirectional transmissions (and node costs), interference effects and single transceiver per node. We do not limit the throughput measures to a common minimum value for all source-destination pairs (such as the Max-flow Min-cut value) but specify the entire achievable throughput region that provides the optimization constraints for the general case of multiple sources. We also extend the network optimization problem to the case of possibly emptying queues by specifying the stability region as the constraint set. We consider different multicast communication problems (such as broadcasting and unicasting) and discuss the trade-offs with anycast communication (with arbitrary throughput rates).","Wireless networks,
Throughput,
Costs,
Constraint optimization,
Media Access Protocol,
Network coding,
Routing,
Performance analysis,
Aggregates,
Interference"
Integrating Influence Mechanisms into Impact Analysis for Increased Precision,"Software change impact analysis is the process of determining the potential effects, or impacts, of a change to a program. Strategies for impact analysis vary in their approach toward the opposing goals of high precision and low analysis time. Fine-grained techniques, such as slicing, can be used to gain very precise knowledge of a change's impact, but may be prohibitively expensive. Coarse-grained techniques such as method-level impact analyses sacrifice precision for faster analysis. In this paper, we present static and dynamic method-level impact analysis algorithms that utilize value propagation information from the source code to increase precision and keep analysis times low. We experimentally compare the results of our analyses with common static and dynamic impact analysis techniques. Our results show that the precision of the common method-level analyses can be improved with very little added overhead","Algorithm design and analysis,
Information analysis,
Software maintenance,
Heuristic algorithms,
Computer science,
Costs,
Testing,
Software systems,
Application software"
PUVAME - New French Approach for Vulnerable Road Users Safety,"In France, about 33% of roads victims are VRU. In its 3rd framework, the French PREDIT includes VRU Safety. The PUVAME project was created to generate solutions to avoid collisions between VRU and bus in urban traffic. An important part of these collisions take place at intersection or bus stop. In this paper, we detail the hardware and software architecture designed and developed in the project. This solution is based on offboard cameras observing particular places (intersection and bus stop in our case) to detect and track VRU present in the environment. The position of the bus is also computed and a risk of collision between each VRU and the bus is determined. In case of high risk of collision, the bus driver is warned. The HMI to warn the bus driver is also described. Finally, some experimental results are presented","Road safety,
Road accidents,
Cameras,
Hardware,
Software architecture,
Software design,
Vehicle driving,
Vehicle detection,
Computer science,
Robots"
WSN18-5: Asymptotic Distribution of The Number of Isolated Nodes in Wireless Ad Hoc Networks with Unreliable Nodes and Links,"In randomly-deployed wireless ad hoc networks with reliable nodes and links, vanishment of isolated nodes asymptotically implies connectivity of networks. However, in a realistic system, nodes may become inactive, and links may become down. The inactive nodes and down links cannot take part in routing/relaying and thus may affect the connectivity. In this paper, we study the connectivity of a wireless ad hoc network that is composed of unreliable nodes and links by investigating the distribution of the number of isolated nodes in the network. We assume that the wireless ad hoc network consists of n nodes which are distributed independently and uniformly in a unit-area disk or square. Nodes are active independently with probability 0<p 1les1, and links are up independently with probability 0<p 2les1. A node is said to be isolated if it doesn't have an up link to an active node. We show that if all nodes have a maximum transmission radius r n=radiclnn+xi/pip 1 p 2 n for some constant xi, then the total number of isolated nodes is asymptotically Poisson with mean e -xi and the total number of isolated active nodes is also asymptotically Poisson with mean p 1 e -xi. In addition, the work can be extended for secure wireless networks which adopt m-composite key predistribution schemes in which a node is said to be isolated if it doesn't have a secure link. Let p denote the probability of the event that two neighbor nodes have a secure link. We show that if all nodes have a maximum transmission radius r n=radiclnn+xi/pipn for some constant xi, then the total number of isolated nodes is asymptotically Poisson with mean e -xi.",
Efficient Mutant Generation for Mutation Testing of Pointcuts in Aspect-Oriented Programs,"Fault-based testing is an approach where the designed test data is used to demonstrate the absence of a set of prespecified faults, typically being frequently occurring faults. Mutation testing is a fault-based testing technique used to inject faults into an existing program, i.e., a variation of the original program and see if the test suite is sensitive enough to detect common faults. Aspect-oriented programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. AspectJ, a language designed to support AOP uses abstractions like pointcuts, advice, and aspects to achieve AOP's primary functionality. Developers tend to write pointcut expressions with incorrect strength, thereby selecting additional events than intended to or leaving out necessary events. This incorrect strength causes aspects, the set of crosscutting concerns, to fail. Hence there is a need to test the pointcuts for their strength. Mutation testing of pointcuts includes two steps: creating effective mutants (variations) of a pointcut expression and testing these mutants using the designed test data. The number of mutants for a pointcut expression is usually large due to the usage of wildcards. It is tedious to manually identify effective mutants that are of appropriate strength and resemble closely the original pointcut expression. Our framework automatically generates mutants for a pointcut expression and identifies mutants that resemble closely the original expression. Then the developers could use the test data for the woven classes against these mutants to perform mutation testing.",
Novel Nickel-Alloy Silicides for Source/Drain Contact Resistance Reduction in N-Channel Multiple-Gate Transistors with Sub-35nm Gate Length,"In this work, we examined the Schottky-barrier height modulation of NiSi by the incorporation of aluminum (Al), titanium (Ti), erbium (Er), and ytterbium (Yb) in NiSi to form different NiSi-alloys. Among the NiSi-alloy candidates investigated, it was found that the NiAl-alloy silicide provides the most effective Schottky-barrier height lowering (~250 meV) on n-Si(001) substrates. Integration of NiAl-alloy silicides as the source and drain (S/D) silicide material for multiple-gate transistors (MuGFETs) was explored, and shown to deliver a drive current IDsat enhancement of 34% compared to MuGFETs employing NiSi S/D. We further showed that the novel NiAl-alloy silicidation process is compatible with lattice-mismatched silicon-carbon (SiC) S/D stressors. NiAl-alloy silicide is therefore a promising S/D silicide material for reducing the high parasitic series resistance in narrow fin MuGFETs for enhanced device performance",
End-to-End BER Performance of Cooperative MIMO Transmission with Antenna Selection in Rayleigh Fading,"Combining a dual-hop relaying with multi-input multi-output (MIMO) transmission is a natural way to overcome the channel impairments. The antenna selection is a simple but efficient MIMO method since it enjoys the full diversity with moderate channel state information (CSI) from the receiver and no complicated spatial coding. For amplify-and-forward (AF) MIMO relaying with best antenna selection, we derive the probability density function (PDF) of the received signal-to-noise ratio (SNR) and provide a BER equation when using M-ary PSK in Rayleigh fading channels.",
Securing interaction between threads and the scheduler,"The problem of information flow in multithreaded programs remains an important open challenge. Existing approaches to specifying and enforcing information flow security often suffer from over-restrictiveness, relying on non-standard semantics, lack of compositionality, inability to handle dynamic threads, scheduler dependence, and efficiency overhead for code that results from security-enforcing transformations. This paper suggests a remedy for some of these shortcomings by developing a novel treatment of the interaction between threads and the scheduler. As a result, we present a permissive noninterference-like security specification and a compositional security type system that provably enforces this specification. The type system guarantees security for a wide class of schedulers and provides a flexible and efficiency-friendly treatment of dynamic threads",
The Effect of Node Density and Propagation Model on Throughput Scaling of Wireless Networks,"This paper derives a lower bound of the form ngamma-1 to the per-node throughput achievable by a wireless network when n source-destination pairs are randomly distributed throughout a disk of radius ngamma, 0 < gamma < 1/2 and propagation is modeled by an attenuation of the form 1/(1 + d)alpha, alpha > 2","Throughput,
Wireless networks,
Attenuation,
Directional antennas,
Network topology,
Physical layer,
Interference,
H infinity control,
Helium"
Concurrent Multipath Transfer using Transport Layer Multihoming: Performance Under Network Failures,"Recent research on concurrent multipath transfer using SCTP multihoming (CMT) proposed various retransmission policies to minimize the negative impacts of receiver buffer (rbuf) blocking that occur during congestion. Here we investigate CMT's throughput degradation caused by rbuf blocking during complete and/or short-term network failures. To improve CMT's performance during failure, we introduce a new state for each destination called the ""potentially failed"" (PF) state and propose a retransmission policy that takes the PF state into account. Using simulation, we evaluate our solution (called CMT-PF), and demonstrate its improved throughput over CMT in failure-prone networks such as FCS battlefield networks","Throughput,
Degradation,
Fault tolerance,
Data communication,
Probes,
Delay,
Protocols,
Computational Intelligence Society,
Computer science,
Educational institutions"
Location-Based Flooding Techniques for Vehicular Emergency Messaging,"This paper analyzes the scalability of message flooding protocols in networks with various node densities, which can be expected in vehicular scenarios. Vehicle safety applications require reliable delivery of warning messages to nearby and approaching vehicles. Due to potentially large distances and shadowing, the delivery protocol must forward messages over multiple hops, thereby increasing network congestion and packet collisions. In addition to application-layer backoff delay and duplicate message suppression mechanisms, location-based backoff techniques have been proposed for vehicular networks. We propose a new hybrid method of location-based and counter-based method, and study several variants through simulations. Our preliminary results in the various density scenarios indicate that the proposed hybrid methods outperform conventional backoff delay techniques and adaptively operate in extremely congested network condition",
Combining Prosodic Lexical and Cepstral Systems for Deceptive Speech Detection,"We report on machine learning experiments to distinguish deceptive from nondeceptive speech in the Columbia-SRI-Colorado (CSC) corpus. Specifically, we propose a system combination approach using different models and features for deception detection. Scores from an SVM system based on prosodic/lexical features are combined with scores from a Gaussian mixture model system based on acoustic features, resulting in improved accuracy over the individual systems. Finally, we compare results from the prosodic-only SVM system using features derived either from recognized words or from human transcriptions",
Speech Enhancement Based on Hilbert-Huang Transform Theory,"Speech enhancement is effective in solving the problem of noisy speech. Hilbert-Huang transform (HHT) is efficient for describing the local features of dynamic signals and is a new and powerful theory for the time-frequency analysis. According to the theory of HHT, this text introduced a new method of speech enhancement to improve the speech quantity and the signal noise ratio (SNR) of processed data. By the method of empirical mode composition (EMD), the speech signal is decomposed into several IMFs. Then remove the background noise from each IMF according to its own characters and rebuild the signal. While the SNR of the speech is low, the experiment results show that this algorithm is valid on tested noise conditions for most of speech signals and is capable to improve the SNR of the speech. Comparing with some other methods for speech enhancement such as methods based on spectrum subtraction as well as the wavelet transform, we can find that the HHT-based method is better to a certain extent",
Model Composability,Composition of models is considered essential in developing heterogeneous complex systems and in particular simulation models capable of expressing a system's structure and behavior. This paper describes model composability concepts and approaches in terms of modeling formalisms. These composability approaches along with some of the key capabilities and challenges they pose are presented in the context of semiconductor supply chain manufacturing systems,
A Multiobjective Resources Scheduling Approach Based on Genetic Algorithms in Grid Environment,"Resources scheduling plays an important role in grid. This paper converts resources scheduling problem in grid into a multiobjective optimization problem, and presents a resources scheduling approach based on multiobjective genetic algorithms. This approach deals with dependent relationships of jobs, and regards multi-dimensional QoS metrics, completion time and execution cost of jobs, as multiobjective. Based on Pareto sorting and niched sharing method, our approach determines optimal solutions. Experimental results show that our approach gets less completion time of jobs and total execution cost of jobs than min-min algorithm and max-min algorithm",
A unified framework for nearby and distant landmarks in bearing-only SLAM,"Bearing-only SLAM describes the process of simultaneously localizing a mobile robot while building a map of the unknown surroundings, using bearing measurements to landmarks as the only available exteroceptive sensor information. Commonly, the position of map features is estimated along with the robot pose. However, consistent initialization of these positions is a difficult problem in bearing-only SLAM, in particular for distant landmarks. In previous approaches, measurements to remote landmarks often had to be discarded, thus losing valuable orientation information. In this paper, we present for the first time a unifying framework allowing for non-delayed initialization of both nearby and distant features. This is made possible by a four-element landmark parametrization, combined with a constraint-based inferred measurement","Simultaneous localization and mapping,
Robot kinematics,
Robot sensing systems,
Position measurement,
Orbital robotics,
Cameras,
Delay,
Computer science,
Mobile robots,
Sensor phenomena and characterization"
Crowd Analysis at Mass Transit Sites,"We propose a novel method for detecting and estimating the count of people in groups, dense or otherwise, as well as tracking them. Using prior knowledge obtained from the scene and accurate camera calibration, the system learns the parameters required for estimation. This information can then be used to estimate the count of people in the scene, in realtime. There are no constraints on camera placement. Groups are tracked in the same manner as individuals, using Kalman filtering techniques. Results are provided for groups of various sizes moving in an unconstrained fashion in crowded scenes",
Facilitating Cooperation in Global Software Design via Micro-Estimation,"Rescheduling design tasks is essential to reduce the impact of communication delay in global cooperative software design. However, it is difficult due to the undetermined duration, delay and task sequence. Micro-estimation on demand refines the estimation of effort and duration for the tasks in the next short period. When using multiple component status transition graph (MCSTG) together with micro-estimation, we may easily obtain the possibility of a task and the available time for this task to be finished. With the extended MCSTG, the designers may reschedule the tasks according to the critical and importance so as to facilitate the cooperation",
Multi-Differential Slightly Frequency-Shifted Reference Ultra-wideband (UWB) Radio,"The frequency-shifted reference (FSR) ultra-wideband (UWB) communication scheme has been recently proposed by the authors. The key idea of the FSR-UWB system is to employ a reference signal that is slightly shifted in frequency from the data-bearing signal, and it has been shown that this results in a very simple receiver architecture. In particular, such a scheme obviates the need for the delay element that greatly complicates implementation of the receiver in standard transmitted reference UWB (TR-UWB) systems. In this paper, we propose a multi-differential FSR-UWB system, where multiple data carriers employ a single reference carrier. This modification essential increases the number of (differential) degrees of freedom available for signaling in the system. However, unlike most communication systems that provide such a dimensionality increase, the large ratio between the UWB system bandwidth and the carrier separation allows the multi-differential FSR-UWB to achieve this significant increase in signal space dimensionality over the standard FSR-UWB system with only a negligible increase in bandwidth. After a general performance characterization, applications to parallel data signaling, multidimensional signaling, and narrowband interference cancellation are considered that demonstrate the utility of the proposed scheme.",
Detecting Duplications in Sequence Diagrams Based on Suffix Trees,"With the popularity of UML and MDA, models are replacing source code as core artifacts of software development and maintenance. But duplications in models reduce models' maintainability and reusability. To address the problem, we should detect duplications first. As an initial step to address the problem, we propose an approach to detect duplications in sequence diagrams. With special preprocessing, we convert 2-dimensional sequence diagrams into a 1-dimensional array. Then we construct a suffix tree of the array. We revise the traditional construction algorithm of suffix trees by proposing a special algorithm to detect common prefixes of suffixes. The algorithm ensures that every duplication detected with the suffix tree can be extracted into a separate reusable sequence diagram. With the suffix tree, duplications are found as refactoring candidates. With tool support, the proposed approach has been applied to real industrial projects, and the evaluation results suggest that the approach is effective.","Unified modeling language,
Software maintenance,
Programming,
Maintenance engineering,
Computer science,
Computer industry,
Computer architecture,
Context modeling,
Collaboration,
Asia"
Implementation of Emotional Controller for Interior Permanent Magnet Synchronous Motor Drive,"This paper presents for the first time the real-time implementation of an emotional controller for interior permanent magnet synchronous motor (IPMSM) drives. The proposed novel controller is called brain emotional learning based intelligent controller (BELBIC). The utilization of BELBIC is based on emotion processing mechanism in brain, and is essentially an action, which is based on sensory inputs and emotional cues. The emotional learning occurs mainly in the amygdala. The amygdala is mathematically modeled, and the BELBIC controller is introduced. This type of controller is insensitive to noise and variance of the parameters. The controller is successfully implemented in real-time using a digital signal processor board ds1102 for a laboratory 1-hp IPMSM. The results show superior control characteristics especially very fast response, simple implementation and robustness with respect to disturbances and manufacturing imperfections. The proposed method enables the designer to shape the response in accordance with the multiple objectives of choices","Permanent magnet motors,
AC motors,
Machine vector control,
Rotors,
Electric variables control,
Control theory,
Magnetic materials,
DC motors,
Synchronous motors,
Velocity control"
The HOY Tester-Can IC Testing Go Wireless?,"Test cost is becoming a more and more significant portion of the cost structure in advanced semiconductor products. To address this issue, we propose HOY - a novel wireless test system with enhanced embedded test features. We present the concept, architecture, and test flow for future semiconductor products tested by HOY. Necessary technologies for the success of HOY also are presented, though most of which require further investigation. A preliminary demonstration system has been constructed, and experiments are being conducted",
I/O Scheduling Service for Multi-Application Clusters,"Distributed applications, especially the ones being I/O intensive, often access the storage subsystem in a nonsequential way (stride requests). Since such behaviours lower the overall system performance, many applications use parallel I/O libraries such as ROMIO to gather and reorder requests. In the meantime, as cluster usage grows, several applications are often executed concurrently, competing for access to storage subsystems and, thus, potentially canceling optimisations brought by parallel I/O libraries. The aIOLi project aims at optimising the I/O accesses within the cluster and providing a simple POSIX API. This article presents an extension of aIOLi to address the issue of disjoint accesses generated by different concurrent applications in a cluster. In such a context, good trade-off has to be assessed between performance, fairness and response time. To achieve this, an I/O scheduling algorithm together with a ""requests aggregator"" that considers both application access patterns and global system load, have been designed and merged into aIOLi This improvement led to the implementation of a new generic framework pluggable into any I/O file system layer. A test composed of two concurrent IOR benchmarks has shown improvements on read accesses by a factor ranging from 3.5 to 35 with POSIX calls and from 3.3 to 5 with ROMIO, both reference benchmarks have been executed on a traditional NFS server without any additional optimisations",
Fast Volume Preservation for a Mass-Spring System,"This article presents a new method to model fast volume preservation of a mass-spring system to achieve a realistic and efficient deformable object animation, without using internal volumetric meshing. With this method the simulated behavior is comparable to a finite-element-method-based model at a fraction of the computational cost","Muscles,
Springs,
Biological system modeling,
Humans,
Material properties,
Deformable models,
Animation,
Joints,
Graphical models,
Finite element methods"
The Generalized Shape Distributions for Shape Matching and Analysis,"This paper presents a novel 3D shape descriptor ""the generalized shape distributions"" for effective shape matching and analysis, by taking advantage of both local and global shape signatures. We start this process by generating spin images on meshes. These local shape descriptors are then quantized via k-means clustering. The key contribution of this paper is to represent a global 3D shape as the spatial configuration of a set of specific local shapes. We achieve this goal by computing the distributions of the Euclidean distance of pairs of local shape clusters. Because of the spatial, sparse distribution of local shapes defined over a 3D model, an indexing data structure is adopted to reduce the space complexity of the proposed shape descriptor. The technical merits of our new approach are at least two-fold: (1) it is robust to non-trivial shape occlusions and deformations, since there are statistically a large number of chances that some local shape signatures and their spatial layouts are unchanged and users can easily identify those unchanged parts; (2) it is more discriminative than a simple collection of local shape signatures, since the spatial layouts of a global shape are explicitly computed. Our preliminary experiments have shown the effectiveness of this new approach for shape comparison and analysis",
The Contribution of Cepstral and Stylistic Features to SRI's 2005 NIST Speaker Recognition Evaluation System,"Recent work in speaker recognition has demonstrated the advantage of modeling stylistic features in addition to traditional cepstral features, but to date there has been little study of the relative contributions of these different feature types to a state-of-the-art system. In this paper we provide such an analysis, based on SRI's submission to the NIST 2005 speaker recognition evaluation. The system consists of 7 subsystems (3 cepstral 4 stylistic). By running independent N-way subsystem combinations for increasing values of N, we fines that (1) a monotonic pattern in the choice of the best N systems allows for the inference of subsystem importance; (2) the ordering of subsystems alternates between cepstral and stylistic; (3) syllable-based prosodic features are the strongest stylistic features, and (4) overall subsystem ordering depends crucially on the amount of training data (1 versus 8 conversation sides). Improvements over the baseline cepstral system, when all systems are combined, range from 47% to 67%, with larger improvements for the 8-side condition. These results provide direct evidence of the complementary contributions of cepstral and stylistic features to speaker discrimination",
A Maximum Likelihood Doppler Frequency Estimator for OFDM Systems,"This paper derives a maximum likelihood Doppler frequency estimator for orthogonal frequency division multiplexing (OFDM) systems in time-varying multipath channels. The proposed scheme is a frequency-domain approach that utilizes pilot subcarriers, which are commonly implemented in most practical systems. Time-varying fading causes intercarrier interference (ICI) in OFDM systems. Thus, in the proposed estimator, the effect of ICI is taken into consideration with a proper model for accurate results. The estimator can be implemented using a finite impulse response (FIR) filter bank whose coefficients can be pre-calculated and stored in order to lower the computational complexity. We evaluate various methods to improve the estimation accuracy and analyze their complexity-performance tradeoffs. We also derive the Cramér-Rao bound and provide simulation results to quantify the performance of the proposed algorithm.","Frequency estimation,
Maximum likelihood estimation,
OFDM,
Time varying systems,
Multipath channels,
Fading,
Interference,
Finite impulse response filter,
Filter bank,
Computational complexity"
Introducing Risk Management into the Grid,"Service Level Agreements (SLAs) are explicit statements about all expectations and obligations in the business partnership between customers and providers. They have been introduced in Grid computing to overcome the best effort approach, making the Grid more interesting for commercial applications. However, decisions on negotiation and system management still rely on static approaches, not reflecting the risk linked with decisions. The EC-funded project ""AssessGrid"" aims at introducing risk assessment and management as a novel decision paradigm into Grid computing. This paper gives a general motivation for risk management and presents the envisaged architecture of a ""risk-aware"" Grid middleware and Grid fabric, highlighting its functionality by means of three showcase scenarios.",
Toward UML Profiles for Web Services and their Extra-Functional Properties,"Web service technologies offer a successful way for interoperability among applications. Now it is important to face how to model systems based on service functionality and also how to add extra-functional properties to them. This is the reason why we propose first of all a versatile and simple UML profile based on the service component architecture specification for modeling services and, secondly, a new UML profile is proposed in order to model and reuse extra-functional properties in the named models. Besides, the property profile provides enough information to enable property code and description generation at a later stage",
When Do We Eat? An Evaluation of Food Items Input into an Electronic Food Monitoring Application,"We present a formative study that examines what, when, and how participants in a chronic kidney disease (stage 5) population input food items into an electronic intake monitoring application. Participants scanned food item barcodes or voice recorded food items they consumed during a three week period. The results indicated that a learning curve was associated with barcode scanning; participants with low literacy skills had difficulty describing food items in voice recordings; and participants input food items depending on when they had dialysis treatment. Participants thought this electronic self monitoring application would be helpful for chronically ill populations in their first year of treatment",
"See, walk, and kick: Humanoid robots start to play soccer","Robotic soccer superseded chess as a challenge problem and benchmark for artificial intelligence research and poses many challenges for robotics. The international RoboCup championships grew to the most important robotic competition worldwide. This paper describes the mechanical and electrical design of the robots that we constructed for RoboCup 2006, which took place in Bremen, Germany. The paper also covers the software used for perception, behavior control, communication, and simulation. Our robots performed well. The KidSize robots won the Penalty Kick competition and came in second the overall Best Humanoid ranking, next only to the titleholder, Team Osaka.","Humanoid robots,
Robot kinematics,
Robot sensing systems,
Intelligent robots,
Machine intelligence,
Artificial intelligence,
Humans,
Mobile robots,
Legged locomotion,
Computer science"
RAIN: A Reliable Wireless Network Architecture,"Despite years of research and development, pioneering deployments of multihop wireless networks have not proven successful. The performance of routing and transport is often unstable due to contention-induced packet losses, especially when the network is large and the offered load is high. In this paper we propose RAIN, a reliable wireless network architecture for large-scale multihop wireless networks. A RAIN network enforces contention control by limiting the queue length at intermediate wireless routers to the minimum. To keep the queue short a RAIN network enforces congestion control through in-network implicit back-pressure. RAIN congestion control is built on wireless datalink layer mechanisms, e.g., mandatory per-frame acknowledgement and inter-frame backoff in popular CSMA/CA wireless transceivers, therefore very efficient and effective compared with those defined at the network or transport layer for the wired Internet. As a result of the built-in contention and congestion control, RAIN presents the end hosts a highly reliable network service model, even more reliable than that of the wired Internet. The end hosts only need to deal with packet losses due to router or routing failures. Therefore, the transport protocol can be significantly simplified. This is in stark contrast to the existing approach of adding more and more complexity to adapt TCP for multihop wireless networks. We propose the details of RAIN datalink layer protocol, and a simple transport protocol at the end hosts. Performance evaluation through intensive simulations shows that RAIN improves the throughput by up to 92% and fairness by up to 48%, with packet losses due to contention and congestion significantly reduced.",
How Bayesians Debug,"Manual debugging is expensive. And the high cost has motivated extensive research on automated fault localization in both software engineering and data mining communities. Fault localization aims at automatically locating likely fault locations, and hence assists manual debugging. A number of fault localization algorithms have been developed in recent years, which prove effective when multiple failing and passing cases are available. However, we notice what is more commonly encountered in practice is the two-sample debugging problem, where only one failing and one passing cases are available. This problem has been either overlooked or insufficiently tackled in previous studies. In this paper, we develop a new fault localization algorithm, named BayesDebug, which simulates some manual debugging principles through a Bayesian approach. Different from existing approaches that base fault analysis on multiple passing and failing cases, BayesDebug only requires one passing and one failing cases. We reason about why BayesDebug fits the two- sample debugging problem and why other approaches do not. Finally, an experiment with a real-world program grep-2.2 is conducted, which exemplifies the effectiveness of BayesDebug.",
Virtually Pipelined Network Memory,"We introduce virtually-pipelined memory, an architectural technique that efficiently supports high-bandwidth, uniform latency memory accesses, and high-confidence throughput even under adversarial conditions. We apply this technique to the network processing domain where memory hierarchy design is an increasingly challenging problem as network bandwidth increases. Virtual pipelining provides a simple to analyze programing model of a deep pipeline (deterministic latencies) with a completely different physical implementation (a memory system with banks and probabilistic mapping). This allows designers to effectively decouple the analysis of their algorithms and data structures from the analysis of the memory buses and banks. Unlike specialized hardware customized for a specific data-plane algorithm, our system makes no assumption about the memory access patterns. In the domain of network processors this will be of growing importance as the size of the routing tables, the complexity of the packet classification rules, and the amount of packet buffering required, all continue to grow at a staggering rate. We present a mathematical argument for our system's ability to provably provides bandwidth with high confidence and demonstrate its functionality and area overhead through a synthesizable design. We further show that, even though our scheme is general purpose to support new applications such as packet reassembly, it outperforms the state of the art in specialized packet buffering architectures",
Out-of-Core Remeshing of Large Polygonal Meshes,"We propose an out-of-core method for creating semi-regular surface representations from large input surface meshes. Our approach is based on a streaming implementation of the MAPS remesher of Lee et al. Our remeshing procedure consists of two stages. First, a simplification process is used to obtain the base domain. During simplification, we maintain the mapping information between the input and the simplified meshes. The second stage of remeshing uses the mapping information to produce samples of the output semi-regular mesh. The out-of-core operation of our method is enabled by the synchronous streaming of a simplified mesh and the mapping information stored at the original vertices. The synchronicity of two streaming buffers is maintained using a specially designed write strategy for each buffer. Experimental results demonstrate the remeshing performance of the proposed method, as well as other applications that use the created mapping between the simplified and the original surface representations",
A Cluster Ensemble Framework for Large Data sets,"Combining multiple clustering solutions is important for obtaining a robust clustering solution, merging distributed clustering solutions, and scaling to large data sets. The combination of multiple clustering solutions within a scalable and robust framework for large data sets is discussed. A scalable framework requires both cluster ensemble creation and merging to be efficient in terms of time and memory complexity. We also introduce the concept of filtering malformed clusters from the ensemble. They result from unfortunate initialization or unbalanced data distribution or noise. Experimental results on real data sets show that this approach will scale and provide cluster partitions which are functionally better or equivalent when compared to clustering all the data at once and clustering solutions contained in the ensemble. We have also compared our algorithm with other ensemble merging and scalable algorithms to point out its strengths and limitations.",
Job Failure Analysis and Its Implications in a Large-Scale Production Grid,"In this paper we present an initial analysis of job failures in a large-scale data-intensive Grid. Based on three representative periods in production, we characterize the interarrival times and life spans of failed jobs. Different failure types are distinguished and the analysis is carried out further at the Virtual Organization (VO) level. The spatial behavior, namely where job failures occur in the Grid, is also examined. Cross-correlation structures, including how arrivals correlate with life spans of job failures, are analyzed and illustrated. We further investigate statistical models to fit the failure data and propose several failureaware scheduling strategies at the Grid level. Our results show that the overall failure rates in the Grid are quite significant, ranging from 25% to 33% of all submitted jobs. However, only 5% to 8% of the jobs failed after running on a certain Computing Element (CE). The rest of failed jobs are aborted or cancelled without running. A majority of failed jobs come from several large production VOs and a large amount of these failures are centered around several main CEs. The interarrival time processes of failed jobs are shown to be bursty, and the life spans exhibit strong autocorrelations. Based on the failure patterns we argue that it is important for the Grid resource brokers to track historical failure and take it into account in decision making. Some proactive measures and accountability issues are also discussed.",
Connected D-Hop Dominating Sets in Mobile Ad Hoc Networks,"A mobile ad hoc network may be logically represented as a set of clusters, and the clusterheads form what is known as a dominating set in graph theory. The clusterheads can be used to perform a variety of functions for nodes in their cluster such as channel access, routing, data collection, broadcasting, etc. There have been several heuristics proposed for forming 1-hop as well as d-hop clusters. In d-hop clustering, the value of d, a constant representing the number of wireless hops in ad hoc networks, is a parameter of the heuristic and controls the size of the clusters. The problem of finding a minimum d-hop connected dominating set was shown to be NP-complete in [9], where the authors raised the question of whether the problem is still NP-complete for the restricted model of unit disk graphs. In this paper, we provide an affirmative answer to this open question. In fact, we show that the minimum d-hop connected dominating set problem is NP-complete for planar unit disk graphs with maximum degree 4. We also review some known 1-hop heuristics that we generalize to the d-hop case and compare their performance. The experimental results show that the algorithm proposed in [9] is the most efficient one.",
Statistical modeling and EM clustering of white matter fiber tracts,"A statistical model of the fiber bundles is calculated as the average and standard deviation of a parametric representation of the fiber tracts, using the coefficients of the 3D quintic B-spline representation of the tracts. An atlas of the fiber tracts is constructed by averaging the bundle models over a population with the fiber tracts mapped onto the atlas coordinate. Using the model representation and with the atlas as the prior map, expectation-maximization (EM) is performed to cluster the fiber tracts in a mixture model framework. As an application, the method is applied to cluster the corpus callosum fiber tracts into its subdivisions and to calculate quantitative parameters for each region",
Robotic Wheelchair Moving with the Caregiver,"This paper presents a robotic wheelchair that moves with a caregiver. Moving with the caregiver does not only mean following the caregiver. When the caregiver moves forward to push a button to call the elevator or to open the door, the wheelchair waits until the elevator comes or the door is opened. Such actions can be done without any caregiver's intentional commands by setting `non-reactive area' around the wheelchair. We conducted some actual experiments which showed the usefulness of the wheelchair",
A Novel and Practical Control Scheme for Inter-Clock At-Speed Testing,"The quality of at-speed testing is being severely challenged by the problem that an inter-clock logic block existing between two synchronous clocks is not efficiently tested or totally ignored due to complex test control. This paper addresses the problem with a novel inter-clock at-speed test control scheme, featuring a compact and robust on-chip inter-clock enable generator design. The new scheme can generate inter-clock at-speed test clocks from PLLs, and is feasible for both ATE-based scan testing and logic BIST. Successful applications to industrial circuits have proven its effectiveness in improving the quality of at-speed testing",
"Fast Algorithms for Logconcave Functions: Sampling, Rounding, Integration and Optimization","We prove that the hit-and-run random walk is rapidly mixing for an arbitrary logconcave distribution starting from any point in the support. This extends the work of Lovasz and Vempala (2004), where this was shown for an important special case, and settles the main conjecture formulated there. From this result, we derive asymptotically faster algorithms in the general oracle model for sampling, rounding, integration and maximization of logconcave functions, improving or generalizing the main results of Lovasz and Vempala (2003), Applegate and Kannan (1990) and Kalai and Vempala respectively. The algorithms for integration and optimization both use sampling and are surprisingly similar",
Formation Control of Multiple Behavior-based robots,"Reactive control has the character of flexibility and timeliness. Aimed at the formation control of multiple robots, this paper presents a behavior-based method on it. In the process of moving to the goal, the robots take the leader-referenced technique. And the dynamic-dead-zone method is used to maintain the formation predetermined. The combination of potential-based avoidance and following-wall behavior ensure the robots are able to avoid the obstacles smoothly. To get rid of the following-wall behavior when it is not needed, a step of judging its necessity is added. Finally the experiment on the AmigoBot robots showed that the robots can return the formation after passing the obstacles","Robot sensing systems,
Communication system control,
Educational institutions,
Computer science,
Robot control,
Control systems,
Computer architecture,
Maintenance engineering,
Feedback,
Kinematics"
"Discrete Barium Strontium Titanate (BST) Thin-Film Interdigital Varactors on Alumina: Design, Fabrication, Characterization, and Applications","Discrete barium strontium titanate (BST) thin-film capacitors in industry standard 0603 footprint are introduced and characterized. BST capacitors have a voltage-dependent permittivity, enabling BST thin-film capacitors to be used as tuning elements in frequency agile devices. The capacitance changed by 1.5:1 at 35 V (116 kV/cm) bias. The temperature dependence of the capacitance was measured to be less than plusmn 20 % from -100 degC to +100 degC. A 2nd-order tunable combline bandpass filter on FR4 substrate has been implemented using the discrete BST varactors. The filter showed a center frequency tuning of 22% from 2.14 GHz to 2.61 GHz upon application of 130 V (433 kV/cm) bias. The zero-bias insertion loss was 4.9 dB which decreased to 2.9 dB at the high bias state. The return loss was better than 11 dB over the tuning range. Nonlinear characterization of the filter using two-tone test and a digitally-modulated CDMA 2000 signal showed an IP3 of +32 dBm and an ACPR of better than -50 dBc up to 26 dBm of input power, respectively","Barium,
Strontium,
Titanium compounds,
Binary search trees,
Transistors,
Varactors,
Fabrication,
Capacitors,
Tuning,
Frequency"
Energy-efficient forwarding schemes for wireless sensor networks,"Energy-efficient forwarding becomes important if resources and battery lifetime are limited such as in wireless sensor networks. Although widely used, simple hop-based forwarding along a path from one node towards a sink can be very inefficient in terms of delivery rate as well as energy efficiency, especially in lossy environments. We will show that just minimizing the expected number of transmissions within the network is not always the most efficient forwarding strategy. Using a realistic link loss model, we derive two new forwarding schemes named single-link and multi-link energy-efficient forwarding that trade off delivery rate and energy costs best by maximizing energy efficiency. Multi-link forwarding further benefits from addressing multiple receivers during packet forwarding, instead of a single one. By mathematical analyses, extensive simulations, and experimental evaluations we contrast the performance of our approaches against a comprehensive framework of other forwarding strategies",
Incorporating Scenarios And Heuristics To Improve Flexibility In Real-Time Embedded Systems,"Flexibility, the ability to adapt to change, is important for real-time systems. As in any type of system, changes arise from maintenance, enhancements and upgrades. These changes are only feasible if timing requirements imposed by the real-time nature of the system can still be met. A flexible design will allow tasks to be added without impinging on other tasks, causing them to miss deadlines. The design space for these systems consists of many configurations describing how tasks and messages are allocated to hardware and scheduled on a hardware platform. Heuristic search is a well recognised strategy for solving allocation and scheduling problems but most research is limited to finding any valid solution for a current set of requirements. The technique proposed here incorporates scenario based analysis into heuristic search strategies where the ability of a solution to meet a scenario is included as another heuristic for the changeability of a system. This allows future requirements to be taken into account when choosing a solution so that future changes can be accommodated with minimal alterations to the existing system.","Real time systems,
Embedded system,
Costs,
Hardware,
Aerospace electronics,
Contracts,
Computer science,
Timing,
Job shop scheduling,
Testing"
A Non-Parametric Bayesian Approach to Spike Sorting,"In this work we present and apply infinite Gaussian mixture modeling, a non-parametric Bayesian method, to the problem of spike sorting. As this approach is Bayesian, it allows us to integrate prior knowledge about the problem in a principled way. Because it is non-parametric we are able to avoid model selection, a difficult problem that most current spike sorting methods do not address. We compare this approach to using penalized log likelihood to select the best from multiple finite mixture models trained by expectation maximization. We show favorable offline sorting results on real data and discuss ways to extend our model to online applications",
Performance and energy benefits of instruction set extensions in an FPGA soft core,"Performance of applications can be boosted by executing application-specific instruction set extensions (ISEs) on a specialized hardware coupled with a processor core. Many commercially available customizable processors have communication overheads in their interface with the specialized hardware. However, existing ISE generation approaches have not considered customizable processors that have communication overheads at their interface. Furthermore, they have not characterized the energy benefits of such ISEs. We present a soft-processor customization framework that takes an input 'C' application and realizes a customized processor capturing the microarchitectural details of its interface with the specialized unit. We are able to accurately measure the speedup, energy, power and code size benefits of our ISE approach on a real system implementation by applying the design flow to a popular Xilinx Microblaze soft-processor core synthesized for four real-life applications. We show that only one large ISE per application is sufficient to get an average 1.41/spl times/ speedup over pure software execution in spite of incurring communication overheads in the ISE implementation. We also observe a simultaneous savings in energy (up to 40%) and power (up to 12% peak power reduction) with this increased performance.","Field programmable gate arrays,
Application software,
Hardware,
Energy consumption,
Computer aided instruction,
Embedded computing,
Microarchitecture,
Energy measurement,
Power measurement,
Computer science"
Dynamic Data Structure Analysis for Java Programs,"Analysis of dynamic data structure usage is useful for both program understanding and for improving the accuracy of other program analyses. Static analysis techniques, however, suffer from reduced accuracy in complex situations, and do not necessarily give a clear picture of runtime heap activity. We have designed and implemented a dynamic heap analysis system that allows one to examine and analyze how Java programs build and modify data structures. Using a complete execution trace from a profiled run of the program, we build an internal representation that mirrors the evolving runtime data structures. The resulting series of representations can then be analyzed and visualized, and we show how to use our approach to help understand how programs use data structures, the precise effect of garbage collection, and to establish limits on static data structure analysis. A deep understanding of dynamic data structures is particularly important for modern, object-oriented languages that make extensive use of heap-based data structures",
DDDAS Approaches to Wildland Fire Modeling and Contaminant Tracking,"We report on two ongoing efforts to build dynamic data driven application systems (DDDAS) for (1) short-range forecasting of weather and wildfire behavior from real time weather data, images, and sensor streams, and (2) contaminant identification and tracking in water bodies. Both systems change their forecasts as new data is received. We use one long term running simulation that self corrects using out of order, imperfect sensor data. The DDDAS versions replace codes that were previously run using data only in initial conditions. DDDAS entails the ability to dynamically incorporate additional data into an executing application, and in reverse, the ability of an application to dynamically steer the measurement process",
Evolving Efficient Recursive Sorting Algorithms,"Object Oriented Genetic Programming (OOGP) is applied to the task of evolving general recursive sorting algorithms. We studied the effects of language primitives and fitness functions on the success of the evolutionary process. For language primitives, these were the methods of a simple list processing package. Five different fitness functions based on sequence disorder were evaluated. The time complexity of the successfully evolved algorithms was measured experimentally in terms of the number of method invocations made, and for the best evolved individuals this was best approximated as O(n times log(n)). This is the first time that sorting algorithms of this complexity have been evolved.",
Controlled and Conditioned Invariants for Linear Impulsive Systems,Linear impulsive systems are a class of hybrid systems in which the state propagates according to linear continuous-time dynamics except for a countable set of times at which the state can change instantaneously. Our aim is to extend the geometric control theory for linear time invariant systems to this system class. In this paper we define controlled invariant and conditioned invariant subspaces for linear impulsive systems with time-driven impulses and establish compensator synthesis results and accompanying connections with closed-loop invariant subspaces. We apply these results to the solution of an output regulation problem with disturbance decoupling for linear impulsive systems,"Control systems,
Control theory,
Artificial intelligence,
State-space methods,
Control system synthesis,
Bismuth,
State feedback,
USA Councils,
Time invariant systems,
Nonlinear dynamical systems"
Impact orientation invariant robot design: an approach to projectile deployed robotic platforms,"Within the fields of law enforcement and urban search and rescue, there is always a need to obtain information from areas that may be hard to reach or unsafe to enter. One method of obtaining this reconnaissance information is to deploy a robot as a projectile. This may be accomplished with mechanical aids or simply by throwing the robot manually. This rapid deployment method has the ability to attain locations inaccessible to other technologies. The miniature nature of the presented design has the ability to operate discretely and avoid detection, making it desirable for law enforcement. In the case of urban search and rescue, the diminutive form minimizes the impact on potentially unsound structures. During the course of deployment, unexpected impacts and drops are inevitable, generating a need for an impact invariant design. Design decisions to create this system are presented, and experimental validation of design aspects is discussed",
Improving the fairness of FAST TCP to new flows,"It has been observed that FAST TCP, and the related protocol TCP Vegas, suffer unfairness when many flows arrive at a single bottleneck link, without intervening departures. We show that the effect is even more marked if a new flow arrives when existing flows share bandwidth fairly, and propose a simple method to ameliorate this effect.","Propagation delay,
Delay estimation,
Bandwidth,
Delay effects,
Computer science,
Protocols,
Throughput,
Time measurement,
Australia Council,
Electronic mail"
Multi-Source Grid Scheduling for Divisible Loads,"The applicability of min cost flow and multi-commodity flow mathematical programming problems to steady state, multi-source divisible load scheduling is examined. Applying the linear model concept of superposition to such steady state multi-source load distribution is suggested for linear and more general topologies. Finally, the use of heuristic optimization for a transient multi-source load distribution problem is discussed.",
Fast Layer 3 Handoffs in AODV-based IEEE 802.11 Wireless Mesh Networks,"Wireless mesh networks (WMNs) are a promising networking concept to extend the range of wireless gateways by utilizing fixed relays as packet forwarders for nodes further away. WMNs use commodity IEEE 802.11 hardware running in ad-hoc mode, and routing protocols such as ad-hoc on-demand distance vector routing (AODV) to route packets between wireless gateways, fixed relays, and mobile nodes (MNs). This paper focuses on handoff-related delays specific to WMNs: delays due to rediscovery of multihop routes after a handoff. We present the pre- handoff route discovery (PRD) concept for AODV, which avoids these delays be enabling MNs to set up routes in the handoff- target network prior to handoff. Furthermore, we illustrate how PRD can be integrated with the fast handovers for mobile IPv6 (FMIPv6) approach to achieve low-latency layer 3 (L3) handoffs in AODV-based IEEE 802.11 wireless mesh networks. Our simulation results show that PRD reduces the total L3 handoff delay from multiple hundred milliseconds to less than 20 ms.",
Path Diminution is Unavoidable in Node-Disjoint Multipath Routing with Single Route Discovery,"In an ad hoc network, identification of all node-disjoint paths between a given pair of nodes is a challenging task. The phenomena that a protocol is not able to identify all node-disjoint paths that exist between a given pair of nodes is called path diminution. In this paper, we discuss that path diminution is unavoidable when a protocol discovers multiple node-disjoint paths in a single route discovery. We discuss schemes to mitigate path diminution. However, no such scheme is guaranteed to discover all node-disjoint paths that exist between a given pair of nodes. We have proved that one cannot devise an efficient algorithm that is guaranteed to compute all node-disjoint paths between a given pair of nodes in a single route discovery",
Layout Analysis of Urdu Document Images,"Layout analysis is a key component of an OCR system. In this paper, we present a layout analysis system for extracting text-lines in reading order from Urdu document images. For this purpose, we evaluate an existing system for Roman script text on Urdu documents and describe its methods and the main changes necessary to adapt it to Urdu script. The main changes are: 1) the text-line model for Roman script is modified to adapt to Urdu text, 2) reading order of an Urdu document is defined. The method is applied to a collection of scanned Urdu documents from various books, magazines, and newspapers. The results show high text-line detection accuracy on scanned images of Urdu prose and poetry books and magazines. The algorithm also works reasonably well on newspaper images. We also identify directions for future work which may further improve the accuracy of the system.",
Flow-Level Stability of Channel-Aware Scheduling Algorithms,"Channel-aware scheduling strategies provide an effective mechanism for improving the throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mainly been examined at the packet level for a static user population, often assuming infinite backlogs. Recently, some studies have also explored the flow-level performance in a scenario with user dynamics governed by the arrival and completion of random service demands over time. Although in certain cases the performance may be evaluated by means of a Processor-Sharing model, in general the flow-level behavior has remained largely intractable, even basic stability properties. In the present paper we derive simple necessary stability conditions, and show that these are also sufficient for a wide class of utility-based scheduling policies. This contrasts with the fact that the latter class of strategies generally fail to provide maximum-throughput guarantees at the packet level.","Stability,
Scheduling algorithm,
Throughput,
Processor scheduling,
Multiaccess communication,
Mathematics,
Computer science,
Fluctuations,
Telecommunication traffic,
Traffic control"
Packet Forwarding Using Pipelined Multibit Tries,"We propose a heuristic for the construction of variablestride multibit tries. These multibit tries are suitable for packet forwarding using a pipelined architecture. The variable-stride tries constructed by our heuristic require upto 1/32 of the per-stage memory required by optimal pipelined fixed-stride tries. We also develop a tree packing heuristic, which dramatically reduces the per-stage memory required by fixed- and variable-stride multibit tries constructed for pipelined architectures. On publicly available router databases, our tree packing heuristic reduces the maximum per-stage memory required by optimal pipelined fixed-stride tries",
Heart Rate Variability in Pediatric Obstructive Sleep Apnea,"Obstructive sleep apnea syndrome (OSAS) is observed in approximately 2% of children. Heart rate variability (HRV) is a potentially simple, non-invasive diagnostic screening tool for OSAS. In this study, we investigated the diagnostic potential of HRV using power spectral analysis, numerical titration, sample entropy, and detrended fluctuation analysis. Effects of sleep stages (REM and NREM sleep) are evaluated. The results show that the heart rate chaos intensity, as measured by the noise limit in numerical titration, is significantly higher during REM sleep than NREM sleep in all patient groups. By using the receiver-operating characteristic analysis, the detection of OSAS yielded a specificity of 72.2% and sensitivity of 81.3% using the numerical-titration technique. The findings suggest that sleep state and disordered breathing are important determinants of cardiac autonomic control. Nonlinear techniques such as numerical titration, when used in conjunction with spectral analysis of HRV could be an effective screening tool for pediatric OSAS",
Creation and Application of a Simulated Database of Dynamic [^18F]MPPF PET Acquisitions Incorporating Inter-Individual Anatomical and Biological Variability,"During the process of validation of a new tracer, estimation of performance and validation of processing algorithms have to be investigated with data sets representative of the ground truth. Because this ground truth is hardly accessible in positron emission tomography (PET), validations of processing algorithms often rely on the use of simulated data sets. Considering that Monte Carlo simulators are very time consuming and are not very easy to use, the building of publicly available databases of simulated PET volumes are becoming highly desirable. We present here the methodology employed for the creation of a database of simulated dynamic [18F]MPPF-PET data, including inter-individual anatomical and biological variability which meets the criteria of a gold standard database as defined by Lehmann: reliance, equivalence, independence, relevance, significance. The assessment of the realism of the built database against actual MPPF PET data is also presented here. Whereas the database was specifically created for the investigations of quantification of activity and binding of ligand-receptor with the [18F]MPPF PET tracer, it may serve the community with countless purposes. The full strength of this database, does not only stem from the knowledge of important information such as the true activity map and underlying anatomical data, but also from the possibility to fully control the biological difference between sets of simulated PET data. Indeed, time activity curves included in the simulated data sets are controlled by a multicompartmental model of ligand-receptor exchanges. This latter feature is of a great interest in the context of the improvement of the detectability of biological variation in PET","Biological system modeling,
Positron emission tomography,
Image databases,
Monte Carlo methods,
Computational modeling,
Biological information theory,
Transaction databases,
Gold,
Biological control systems,
Spatial databases"
Detection of Syn Flooding Attacks using Linear Prediction Analysis,"This paper presents a simple but fast and effective method to detect TCP SYN flooding attacks. Linear prediction analysis is proposed as a new paradigm for DoS attack detection. The proposed SYN flooding detection mechanism makes use of the exponential backoff property of TCP used during timeouts. By modeling the difference of SYN and SYN+ACK packets, we are successfully able to detect an attack within short delays. We use this method at leaf routers and firewalls to detect the attack without the need of maintaining any state",
Self-Learning Collision Avoidance for Wireless Networks,,"Collision avoidance,
Wireless networks,
Interference,
Computer science,
Availability,
Hardware,
Analytical models,
Throughput,
Wireless LAN,
Transceivers"
Stochastic Analysis and File Availability Enhancement for BT-like File Sharing Systems,"In this paper, we present the mathematical analysis of two important performance measures for a BitTorrent (BT) like P2P file sharing system, namely, average file downloading time and file availability. For the file downloading time, we develop a model using the ""stochastic differential equation"" approach, which can capture the system more accurately than some previous approach and can capture various network settings and peers behavior. We study the steady-state behavior and obtain the closed-form solutions for performance measures which allow us to carry sensitivity analysis on various performance measures for various system parameters. We then extend this model to consider multiclass peers wherein some peers are behind firewalls which may impede the uploading service. We also present the mathematical model to study the file availability of a BT-like system. The model helps us gain the understanding of why the ""rarest-first"" chunk selection policy is used in today's BT protocol. We propose a novel chunk selection algorithm to enhance the overall system file availability. Extensive simulations are carried to validate our analysis",
NetLens: Iterative Exploration of Content-Actor Network Data,"Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases","Data visualization,
Software libraries,
Data models,
Displays,
User interfaces,
Law,
Legal factors,
Graphical user interfaces,
Visual analytics,
Laboratories"
Torque Ripples Minimization in PMSM using Variable Step-Size Normalized Iterative Learning Control,"Periodic torque ripples exist due to non-perfect sinusoidal flux distribution, cogging torque and current measurement errors in permanent magnet synchronous motor (PMSM). These ripples are reflected as periodic oscillations in the motor speed and deteriorate the performance of application of PMSM as a high-precision tracking applications. In this paper, we propose a variable step-size normalized iterative learning control (VSS-NILC) scheme to reduce periodic torque ripples. VSS-NILC is combined to existing PI current controller and generates compensated reference current iteratively from cycle to cycle so as to minimize the mean square torque error. VSS-NILC scheme alters the step-size of the update equation to reduce the conflict between speed of convergence and minimum mean square error (MSE). Consequently VSS-NILC scheme has faster convergence rate and lower mean square torque error. Simulation results show significant improvements in the steady-state torque response and the effectiveness in minimizing torque ripples","Torque control,
Magnetic variables control,
Permanent magnet motors,
Convergence,
Forging,
Current measurement,
Synchronous motors,
Error correction,
Equations,
Mean square error methods"
WSN10-3: Maximizing Network Lifetime under QoS Constraints in Wireless Sensor Networks,"In this paper, we study a randomized scheduling algorithm, and analyze the problem of maximizing network lifetime under quality of service constraints such as bounded values of detection delay, detection probability, and network coverage intensity in wireless sensor networks. We show that the optimal solutions exist and provide the conditions of the existence of the optimal solutions.","Wireless sensor networks,
Peer to peer computing,
Quality of service,
Scheduling algorithm,
Computer science,
Event detection,
Intrusion detection,
Algorithm design and analysis,
Delay,
Electronic mail"
Consistency of Pseudolikelihood Estimation of Fully Visible Boltzmann Machines,"A Boltzmann machine is a classic model of neural computation, and a number of methods have been proposed for its estimation. Most methods are plagued by either very slow convergence or asymptotic bias in the resulting estimates. Here we consider estimation in the basic case of fully visible Boltzmann machines. We show that the old principle of pseudolikelihood estimation provides an estimator that is computationally very simple yet statistically consistent.",
Pupil and Iris Localization for Iris Recognition in Mobile Phones,"Until now, iris recognition has been used in many fields. Recently, there have been attempts to adopt iris recognition technology for the security of mobile phones. For example, in case of bank transaction service by using a mobile phone, using a mobile phone can use high level of security based on iris recognition. In this paper, we propose a new pupil & iris segmentation method apt for the mobile environment. We find the pupil & iris at the same time, using both information of the pupil and iris. And we also use characteristic of the eye image. Experimental result shows that our algorithm has good performance in various images, which include motion or optical blurring, ghost, specular reflection and etc. from various environments for iris recognition system",
,,
Semantic Analysis of Web Pages Using Web Patterns,This paper introduces a novel method for semantic analysis of Web pages. Analysis is performed with regard to unwritten and empirically proven agreement between users and Web designers using Web patterns. This method is based on extraction of patterns which are characteristics for concrete domain. Patterns provide formalization of the agreement and allow assignment of semantics to parts of Web pages. Experimental results verify the effectives of the proposed method,"Pattern analysis,
Web pages,
Computer science,
Software design,
Performance analysis,
Concrete,
Information analysis,
Data mining,
Tree data structures,
Content management"
Sequential Monte Carlo Method for Fixed Symbol Timing Estimation and Data Detection,"Accurate estimation of symbol sampling instants is critical for reliable detection of transmitted symbols in communication systems. In this paper, we deal with the problem of joint estimation of symbol timing and transmitted symbols using sequential Monte Carlo (SMC) methods. Previous research work has developed SMC methods that achieve excellent performance in estimating channel reference parameters that include the symbol timing. However, these SMC methods are developed based on the assumption that the symbol-timing is time-varying, as a result, their application may be limited to systems whose symbol timing can be modeled as such. In this paper, we extend the work to problems where the symbol-timing is fixed. The posterior probability density of the fixed symbol timing is approximated by a beta distribution function whose parameters are sequentially updated. With such an approximation, an efficient SMC algorithm is developed that estimate the transmitted symbols and the symbol timing jointly.",
Metrics for analyzing the evolution of C-space models,"There are many sampling-based motion planning methods that model the connectivity of a robot's configuration space (C-space) with a graph whose nodes are valid configurations and whose edges represent valid transitions between nodes. One of the biggest challenges faced by users of these methods is selecting the right planner for their problem. While researchers have tried to compare different planners, most accepted metrics for comparing planners are based on efficiency, e.g., number of collision detection calls or samples needed to solve a particular set of queries, and there is still a lack of useful and efficient quantitative metrics that can be used to measure the suitability of a planner for solving a problem. That is, although there is great interest in determining which planners should be used in which situations, there are still many questions we cannot answer about the relative performance of different planning methods. In this paper we make some progress towards this goal. We propose a metric that can be applied to each new sample considered by a sampling-based planner to characterize how that sample improves, or not, the planner's current C-space model. This characterization requires only local information and can be computed quite efficiently, so that it can be applied to every sample. We show how this characterization can be used to analyze and compare how different planning strategies explore the configuration space. In particular, we show that it can be used to identify three phases that planners go through when building C-space models: quick learning (rapidly building a coarse model), model enhancement (refining the model), and learning decay (oversampling - most samples do not provide additional information). Hence, our work can also provide the basis for determining when a particular planning strategy has 'converged' on the best C-space model that it is capable of building",
The evolving tree-analysis and applications,"In this paper, we enhance and analyze the Evolving Tree (ETree) data analysis algorithm. The suggested improvements aim to make the system perform better while still maintaining the simple nature of the basic algorithm. We also examine the system's behavior with many different kinds of tests, measurements and visualizations. We compare the ETree's performance against classical data analysis methods and very similar modern systems. We find that the ETree is a suitable method for unsupervised analysis of huge data sets",
Teaching Practical Software Engineering and Global Software Engineering: Case Study and Recommendations,"In this paper we present the innovative methods and experiences from several years of teaching practical software (SW) engineering at the Computer Science Departments of San Francisco State University (SFSU), USA in conjunction with the University of Applied Sciences, Fulda University, Germany. The key objectives and desired outcomes of our course were to train future SW developers, technical leads and managers in practical SW engineering practices, including global SW engineering, where team members work in different locations. Our key approach was to combine and synchronize class teaching about SW engineering methods and processes with actual SW development work in a setting designed to simulate a small SW company. Students were divided in ""local"" groups of 4-6 members, each forming a small SW ""company"" in charge of developing a complete working WWW application as a final class project. Several groups of students at SFSU were ""virtually"" paired with groups of students at Fulda University, whom they never met face to face, to form ""global"" groups, thus simulating global SW engineering in a realistic setting. Students developed their final projects incorporating five well-defined milestones typical for SW development lifecycle. All student groups (including global ones) produced impressive final project applications and gave very positive feedback on this class","Software engineering,
Education,
Teamwork,
Computer science,
Globalization,
Disaster management,
Collaborative software,
Management training,
World Wide Web,
Application software"
The Visual Exploration ofWeb Search Results Using HotMap,"While the information retrieval techniques used by Web search engines have improved substantially over the years, the search results have continued to be represented in a simple list-based format. Although this list-based representation makes it easy to evaluate a single document, it does not support the users in the broader tasks of manipulating the search results, comparing documents, or finding a set of relevant documents. HotMap provides a compact visual representation of Web search results at two levels of detail, and supports the interactive exploration of Web search results. User studies have shown that HotMap can result in fewer low-relevance documents being considered, and generates a higher level of confidence, ease of use, and satisfaction than a Google-like interface",
Generative function block design and composition,,
Modeling the Impact of Process Variation on Critical Charge Distribution,"In this paper, we investigate the impact of process variation on soft error vulnerability with Monte Carlo analysis. Our simulation results show that Qcritical variation (3sigma/mean) of four types of storage circuits caused by process variation can be as large as 13.6%. We also propose an empirical model to estimate the Qcritical variation caused by gate length and threshold voltage variations. Simulation results show that this simple model is very accurate. Based on this model, the dependence of Qcritical variation on gate length variation, threshold voltage variation, and correlation between gate lengths is studied, using 70 nm SRAM as benchmark circuit.","Threshold voltage,
Monte Carlo methods,
Circuit simulation,
Neutrons,
Timing,
Error analysis,
Computer science,
Computer errors,
Computational modeling,
Random access memory"
The MORABIT Approach to Runtime Component Testing,"Runtime testing is important for improving the quality of software systems. This fact holds true especially for systems which cannot be completely assembled at development time, such as mobile or ad-hoc systems. The concepts of built-in-test (BIT) can be used to cope with runtime testing, but to our knowledge there does not exist an implemented infrastructure for BIT. The MORABIT project realizes such an infrastructure and extends the BIT concepts to allow for a smooth integration of the testing process and the original business functionality execution. In this paper the requirements on the infrastructure and our solution are presented",
Ubiquitous Robot: Recent Progress and Development,"This paper provides an overview on the recent progress and developments in the development of the ubiquitous robot. First the concepts of the ubiquitous robot, which incorporates the software robot (Sobot), embedded robot (Embot) and mobile robot (Mobot), are presented. The Ubiquitous robot, Ubibot, comprising of a core intelligence of a Sobot Rity, perceptive abilities through a variety of Embots, and a personal robot type Mobot called Mybot is presented. Rity, an artificial creature testing robot genome is described in detail including the technique for creating the chromosome which dictates its personality and traits. The technique for evolving a new personality, thus creating a Genetic Robot, is also briefed upon. The nearly limitless possibilities evident in this new paradigm in robotics can be obviously seen.","Intelligent robots,
Mobile robots,
Service robots,
Robot sensing systems,
Competitive intelligence,
Orbital robotics,
Embedded software,
Context awareness,
Artificial intelligence,
Genomics"
Adaptive Approach to Information Dissemination in Self-Organizing Grids,"The size, complexity, heterogeneity, and dynamism of large-scale computational grids make autonomic grid services and solutions necessary. In particular, grid schedulers must map applications onto resources whose state (1) influences the effectiveness of scheduling choices, and (2) changes frequently and considerably. A grid resource state information dissemination service must negotiate the inherent tradeoff between covering a large portion of the grid (so that all schedulers can make informed decisions with the largest number of options), and limiting the protocol's overhead (i.e. the number of packets sent). This paper argues that probabilistic forwarding protocols must adapt to state changes, because static assignments of forwarding probabilities lead to excessive overhead or lower-than-possible query satisfaction rates in some scenarios. We introduce an approach that compares a node's local utilization and query generation rates to corresponding rates in the node's vicinity, and in the grid as a whole. These comparisons, in turn, produce a score that is used to adjust forwarding probabilities. We show that even this simple initial adaptive approach can work better than protocols with static forwarding probability assignments",
Consistency Checking of UML Dynamic Models Based on Petri Net Techniques,"To aid the development of high quality software applications, we present an approach for consistency checking of UML dynamic models based on Petri net techniques. ECPN, an extended colored Petri net, is used to formally describe state transitions of individual objects and interactions among objects, and is therefore capable of verifying the consistency of the models based on Petri net theory. In this work, we consider UML sequence diagrams and statecharts. The approach begins with a flattening strategy for UML dynamic models and then discusses translation of statecharts with composite states to an ECPN notation. The coverability graph is used to drive the consistency checking process. The paper discusses all phases of the approach and illustrates the concept by an example",
Patching Processor Design Errors,"Microprocessors can have design errors that escape the test and validation process. The cost to rectify these errors after shipping the processors can be very expensive as it may require replacing the processors and stalling the shipment. In this paper, we discuss architecture support to allow patching the design errors in the processors that have already been shipped out. A contribution of this paper is our analysis showing that a majority of errors can be detected by monitoring a subset of signals in the processors. We propose to incorporate a programmable error detector in the processor that monitors these signals to detect and initiate recovery using one of the mechanisms that we discuss. The proposed hardware units can be programmed using patches consisting of the errata signatures which the manufacturer develops and distributes when errors are discovered in the post-design phase.",
Packet-Level Integration of Fluid TCP Models in Real-Time Network Simulation,"We present a hybrid network traffic model that combines time-stepping fluid model with discrete-event packet-oriented simulation. We propose an integration scheme allowing packet flows to interact with fluid flows within each network queue. Different from previous schemes that require physical division of the virtual network between the fluid model and packet-oriented simulation, our hybrid model allows full integration of the two paradigms making it possible to dynamically change the composition of traffic flows to allow the simulation to keep up with real time. Experiments show that our model provides a good prediction of the network behavior. More important, as we increase the proportion of packet flows, the simulation is capable of capturing more detail of the network traffic behavior at the expense of more computing time. Hence the tradeoff","Telecommunication traffic,
Traffic control,
Computational modeling,
Mathematical model,
Fluid flow,
Discrete event simulation,
Differential equations,
Computer networks,
Computer simulation,
Large-scale systems"
"Action, State and Effect Metrics for Robot Imitation","This paper addresses the problem of body mapping in robotic imitation where the demonstrator and imitator may not share the same embodiment (degrees of freedom (DOFs), body morphology, constraints, affordances and so on). Body mappings are formalized using a unified (linear) approach via correspondence matrices, which allow one to capture partial, mirror symmetric, one-to-one, one-to-many, many-to-one and many-to-many associations between various DOFs across dissimilar embodiments. We show how metrics for matching state and action aspects of behaviour can be mathematically determined by such correspondence mappings, which may serve to guide a robotic imitator. The approach is illustrated in a number of examples, using agents described by simple kinematic models and different types of correspondence mappings. Also, focusing on aspects of displacement and orientation of manipulated objects, a selection of metrics are presented, towards a characterization of the space of effect metrics",
Feature Selection Using Rough Set in Intrusion Detection,"Most of existing intrusion detection systems use all data features to detect an intrusion. Very little works address the importance of having a small feature subset in designing an efficient intrusion detection system. Some features are redundant and some contribute little to the intrusion detection process. The purpose of this study is to investigate the effectiveness of rough set theory in identifying important features in building an intrusion detection system. Rough set was also used to classify the data. Here, we used KDD Cup 99 data. Empirical results indicate that rough set is comparable to other feature selection techniques deployed by few other researchers",
On The Issue of Learning Weights from Observations for Fuzzy Signatures,"We investigate the issue of obtaining weights, which are associated with aggregation in fuzzy signatures, from real world data. Our approach will provide a way to extract the relevance of lower levels to the higher levels of the hierarchical fuzzy signature structure. We also handle the non-differentiability of max-min aggregation functions for gradient based learning. A mathematically proved method, which is found in the literature to approximate the derivatives of max-min functions, has been used.","Fuzzy sets,
Australia,
Data mining,
Automation,
Computer science,
Informatics,
Environmental economics,
Information technology,
Humans,
Learning systems"
Partial Discharge Characteristics of Nanocomposite Enameled Wire for Inverter-Fed Motor,"Recently, partial discharge (PD)-resistant enameled wires mixed with inorganic particles of nanometer order, referred to as the nanocomposite enameled wires, have been developed and verified to have longer life compared with conventional enameled wires. We investigated the fundamental characteristics on PD inception, propagation and breakdown (BD) of nanocomposite enameled wire under surge voltage conditions. Electrical and optical PD measurements were carried out for twisted pair samples with nanocomposite and conventional enameled wires. Experimental results revealed that there was no difference in PD inception voltage (PDIV) between both wires. The relationship between applied voltage and time to BD (V-t characteristics) exhibited the longer life of nanocomposite enameled wires, e.g. 2-20 times of conventional enameled wires, which was highly influenced by the applied voltage level and the repetition rate.","Partial discharges,
Wire,
Micromotors,
Surges,
Voltage,
Coatings,
Dielectrics and electrical insulation,
Pulse width modulation inverters,
Pulse generation,
Testing"
Combining Appearance-based and Model-based Methods for Real-Time Object Recognition and 6D Localization,"A general solution for image-based object recognition and localization is still a goal far away. Therefore, the only way to tackle the problem is to apply the suitable approach for each specific problem. The most common techniques can be classified into global appearance-based, model-based, or histogram-based approaches, and approaches based on local features. In this paper, we concentrate on recognition and full 6D localization of solid colored objects of any geometry for real-time application on a humanoid robot system. State-of-the-art model-based methods can only deal with object geometries which can be broken down into 3D lines and planes, and thus can be efficiently projected into the image plane, which is not the case for most objects in a realistic scenario. In contrast, appearance-based methods have the power to be applicable for any object geometry, but are rarely combined with full 6D localization of objects, which is required for any realistic application in the context of grasping with a humanoid robot. We present a system which combines the benefits of global appearance-based and model-based approaches, resulting in a system which can acquire object representations automatically given its 3D model, and can recognize and localize solid-colored objects in 6D in an arbitrary scene in real-time","Object recognition,
Machine vision,
Humanoid robots,
Computational geometry,
Layout,
Real time systems,
Robot kinematics,
Solid modeling,
Robot vision systems,
Power system modeling"
The Effect of Channel State Information on Optimum Energy Allocation and Energy Efficiency of Cooperative Wireless Transmission Systems,"This paper considers the problem of how to efficiently allocate transmission energy in a wireless communication system with two delay-constrained cooperating sources and one destination. The sources in the system relay each other's transmissions via an orthogonal amplify-and-forward protocol. The channels are assumed to be flat fading and the sources are each required to satisfy an outage probability constraint. The analysis focuses on optimum energy allocation and energy efficiency for two distinctly different scenarios: (i) the sources have access to partial channel state information (the instantaneous channel amplitudes) and (ii) the sources have access to only the channel statistics. Numerical examples are presented for independent Rayleigh fading channels demonstrating that partial channel state information significantly improves the energy efficiency of cooperative transmission. Our results also show that, while cooperative transmission tends to have better energy efficiency than direct (noncooperative) transmission, opportunistic direct transmission with partial channel state information is often more energy efficient than cooperative transmission without knowledge of the channel state.","Channel state information,
Energy efficiency,
Fading,
Wireless communication,
Delay systems,
Relays,
Access protocols,
Probability,
Information analysis,
Statistical analysis"
Stochastic games with branching-time winning objectives,"We consider stochastic turn-based games where the winning objectives are given by formulae of the branching-time logic PCTL. These games are generally not determined and winning strategies may require memory and or randomization. Our main results concern history-dependent strategies. In particular, we show that the problem whether there exists a history-dependent winning strategy in 1frac12-player games is highly undecidable, even for objectives formulated in the Lscr(F=5/8 ,F=1,F>0,G=1) fragment of PCTL. On the other hand, we show that the problem becomes decidable (and in fact EXPTIME-complete) for the Lscr(F=1,F>0,G=1) fragment of PCTL, where winning strategies require only finite memory. This result is tight in the sense that winning strategies for Lscr(F=1,F>0,G=1,G>0 ) objectives may already require infinite memory","Stochastic processes,
Logic,
Probability distribution,
Computer science,
Informatics,
History,
Mathematics"
Active fingerprinting of 802.11 devices by timing analysis,,"Fingerprint recognition,
Timing,
Hardware,
Wireless networks,
Communication system security,
Support vector machines,
Support vector machine classification,
Performance evaluation,
Radio transmitters,
Computer science"
Detecting MAC Layer Back-off Timer Violations in Mobile Ad Hoc Networks,"In IEEE 802.11 based ad hoc networks, by simply manipulating the back-off timers and/or wait times prior to transmission, malicious nodes can cause a drastically reduced allocation of bandwidth to well-behaved nodes. This can result in causing bandwidth starvation and hence, a denial of service to legitimate nodes. We propose a combination of deterministic and statistical methods that facilitate detection of such misbehavior. With our approach, each of the nodes is made aware of the pseudo-random sequences that dictate the back-off times of all its one-hop neighbors. A blatant violation of the timer is thus, immediately detected. In certain cases, a node may be unable to monitor the activities of its neighbor and therefore deterministically ascertain if the neighbor is misbehaving. To cope with such cases, we propose a statistical inference method, wherein based on an auto-regressive moving average (ARMA) of observations of the system state, a node is able to estimate if its neighbor is indulging in misbehavior. Simulation results show that with our methods, it is possible to detect a malicious node with a probability close to one. Furthermore, the probability of false alarms is lower than 1%.",
A Video Game-Based Mobile Robot Simulation Environment,"Simulation is becoming an increasingly important aspect of mobile robots. As we are better able to simulate the real world, we can usefully perform more research in simulated environments. The key aspects of a good simulator, an accurate physics simulation and realistic graphical rendering system, are also central to modern computer games. In this paper, we describe a robot simulation environment built from technologies typically used in computer video games. The simulator is capable of simulating multiple robots, with realistic physics and rendering. It can also support human-controlled avatars using a traditional first-person interface. This allows us to perform robot-human interaction and collaboration studies in the simulated environment. The distributed nature of the simulation allows us to perform large-scale experiments, with users participating from geographically remote locations","Games,
Mobile robots,
Computational modeling,
Computer simulation,
Computer graphics,
Rendering (computer graphics),
Physics computing,
Avatars,
Collaboration,
Application software"
Measuring the Effectiveness of Honeypot Counter-Counterdeception,"Honeypots are computer systems that try to fool cyberattackers into thinking they are ordinary computer systems, when in fact they are designed solely to collect data about attack methods and thereby enable better defense against attackers. Honeypots are more effective the more ordinary they appear, but so far designers have just used intuition in designing them. So it is valuable to develop metrics for measuring the effectiveness of honeypot deception. We report on several software tools we have developed for assessing the effectiveness of honeypots, particularly a metric-calculating tool that summarizes a file system by a vector of 72 numbers. Comparison of vectors between fake and real systems can guide design of the fake. We show that this metric tool, applied to a detailed fake file system we constructed, confirms that it is convincing in most ways.",
Study of Different Types of Attacks on Multicast in Mobile Ad Hoc Networks,"Security is an essential requirement in mobile ad hoc networks (MANETs). Compared to wired networks, MANETs are more vulnerable to security attacks due to the lack of a trusted centralized authority, easy eavesdropping, dynamic network topology, and limited resources. The security issue of MANETs in group communications is even more challenging because of the involvement of multiple senders and multiple receivers. In this paper, we present a simulationbased study of the impacts of different types of attacks on mesh-based multicast in MANETs. We consider the most common types of attacks, namely rushing attack, blackhole attack, neighbor attack and jellyfish attack. Specifically we study how the processing delay of legitimate nodes, the number of attackers and their positions affect the performance metrics of a multicast session such as packet delivery ratio, throughput, end-to-end delay, and delay jitter. To the best of our knowledge, this is the first paper that studies the vulnerability and the performance of multicast in MANETs under various security threats.","Intelligent networks,
Mobile ad hoc networks,
Multicast protocols,
Mobile communication,
Delay,
Vehicle dynamics,
Network topology,
Unicast,
Bandwidth,
Computer science"
Using reinforcement learning to improve exploration trajectories for error minimization,"The mapping and localization problems have received considerable attention in robotics recently. The exploration problem that drives mapping has started to generate similar attention, as the ease of construction and quality of map is strongly dependent on the strategy used to acquire sensor data for the map. Most exploration strategies concentrate on selecting the next best measurement to take, trading off information gathering for regular relocalization. What has not been studied so far is the effect the robot controller has on the map quality while executing exploration plans. Certain kinds of robot motion (e.g, sharp turns) are hard to estimate correctly, and increase the likelihood of errors in the mapping process. We show how reinforcement learning can be used to generate good motion control while executing a simple information gathering exploration strategy. We show that the learned policy reduces the overall map uncertainty by reducing the amount of uncertainty generated by robot motion",
APAMAT: A Prescription Algebra for Medication Authoring Tool,"We describe here the prescription algebra and its implementation for medication authoring tools. The tools are parts of medication use process, which consists of prescription entry systems, medication authoring tool, medication scheduling specification, medication scheduler, and programmable pill dispenser. A medication authoring tool aids the pharmacists to collect and integrate prescriptions, to verify drug-drug interactions amongst prescriptions one took for the prescribed duration, and to generate the scheduling specifications for pill dispensers. We design a prescription algebra for medication authoring tool to correctly complete its work. We have implemented the platform-independent medication authoring tool using JAVA. The screen snapshot are shown in the paper.","Algebra,
Drugs,
DSL,
Cybernetics,
Java,
Hospitals,
Physics computing,
Costs,
Libraries,
Information systems"
A QoS model and testing mechanism for quality-driven Web services selection,"With the proliferation of Web services, quality of service (QoS) becomes a key factor to differentiate the Web services and their providers. In selecting a Web service for use, it is important to consider nonfunctional properties of the Web service so as to satisfy the constraints or requirements of users. In this paper, we present a Web services QoS model which takes non-functional properties into account in various aspects. In this model, Web services qualities are classified into three categories according to the point of view - system-level view, service-level view, and business-level view. We also provide a quality testing mechanism that tests the manageability qualities of our QoS Model","Testing,
Web services,
Quality of service,
Quality management,
Publishing,
Conferences,
Collaborative software,
Computer science,
Service oriented architecture,
Q factor"
Exhaustive optimization phase order space exploration,"The phase-ordering problem is a long standing issue for compiler writers. Most optimizing compilers typically have numerous different code-improving phases, many of which can be applied in any order. These phases interact by enabling or disabling opportunities for other optimization phases to be applied. As a result, varying the order of applying optimization phases to a program can produce different code, with potentially significant performance variation amongst them. Complicating this problem further is the fact that there is no universal optimization phase order that will produce the best code, since the best phase order depends on the function being compiled, the compiler, and the target architecture characteristics. Moreover, finding the optimal optimization sequence for even a single function is hard as the space of attempted optimization phase sequences is huge and the interactions between different optimizations are poorly understood. Most previous studies performed to search for the most effective optimization phase sequence assume the optimization phase order search space to be extremely large, and hence consider exhaustive exploration of this space infeasible. In this paper we show that even though the attempted search space is extremely large, with careful and aggressive pruning it is possible to limit the actual search space with no loss of information so that it can be completely evaluated in a matter of minutes or a few hours for most functions. We were able to exhaustively enumerate all the possible function instances that can be produced by different phase orderings performed by our compiler for more than 98% of the functions in our benchmark suite. In this paper we describe the algorithm we used to make exhaustive search of the optimization phase order space possible. We then analyze this space to automatically calculate relationships between different phases. Finally, we show that the results of this analysis can be used to reduce the compilation time for a conventional batch compiler.","Space exploration,
Optimizing compilers,
Computer science,
Program processors,
Lifting equipment,
Performance analysis,
Energy consumption"
PathExpander: Architectural Support for Increasing the Path Coverage of Dynamic Bug Detection,"Dynamic software bug detection tools are commonly used because they leverage run-time information. However, they suffer from a fundamental limitation, the path coverage problem: they detect bugs only in taken paths but not in non-taken paths. In other words, they require bugs to be exposed in the monitored execution. This paper makes one of the first attempts to address this fundamental problem with a simple hardware extension. First, we propose PathExpander, a novel design that dynamically increases the code path coverage of dynamic bug detection tools with no programmer involvement. As a program executes, PathExpander selectively executes non-taken paths in a sandbox without side effects. This enables dynamic bug detection tools to find bugs that are present in these non-taken paths and would otherwise not be detected. Second, we propose a simple hardware extension to control the huge overhead in its pure software implementation to a moderate level. To further minimize overhead, PathExpander provides an optimization option to execute non-taken paths on idle cores in chip multi-processor architectures that support speculative execution. To evaluate PathExpander, we use three dynamic bug detection methods: dynamic software-only checker (CCured), dynamic hardware-assisted checker (iWatcher) and assertions; and conduct side-by-side comparison with PathExpander's counterpart software implementation. Our experiments with seven buggy programs using general inputs that do not expose the tested bugs show that PathExpander is able to help these tools detect 21 (out of 38) tested bugs that are otherwise missed. This is because PathExpander increases the code coverage of each test case from 40% to 65% on average, based on the branch coverage metric. When applications are tested with multiple inputs, the cumulative coverage also significantly improves by 19%. We also show that PathExpander introduces modest false positives (4 on average) and overhead (less than 9.9%). The 3-4 orders of magnitude lower overhead compared with pure-software implementation further justifies the hardware design in PathExpander",
Quantitative Evaluation of Near Regular Texture Synthesis Algorithms,"Near regular textures are pervasive in man-made and natural world. Their global regularity and local randomness pose new difficulties to the state of the art texture analysis and synthesis algorithms. We carry out a systematic comparison study on the performance of four texture synthesis algorithms on near-regular textures. Our results confirm that faithful near-regular texture synthesis remains a challenging problem for the state of the art general purpose texture synthesis algorithms. In addition, we provide comparison of human perception with computer evaluations on the quality of the texture synthesis results.",
Detecting Behavioral Microsleeps from EEG Power Spectra,"EEG spectral power has been shown to correlate with level of arousal and alertness in humans. In this paper, we assess its usefulness in the detection of behavioral microsleeps (BMs). Eight non-sleep-deprived normal subjects performed two 1-hour sessions of a continuous tracking task while EEG and facial video were recorded. BMs were identified independent of tracking performance by a human rater by viewing the video recordings. Spectral power, normalized spectral power, and power ratios in the standard EEG bands were calculated using the Burg method on 16 bipolar derivations to form an EEG feature matrix. PCA was used to reduce the dimensionality of the feature matrix and linear discriminant analysis used to form a classifier for each subject. The 8 classifiers were combined using stacked generalization to create an overall detection model and N-fold cross-validation used to determine its performance (Phi=0.30plusmn0.05, meanplusmnSE). While modest, the detection of BMs at such a high temporal resolution (1 s) has not been achieved previously other than by our group",
Beam Hardening Correction for Middle-Energy Industrial Computerized Tomography,"In this paper, a new beam hardening correction (BHC) method for middle-energy industrial computerized tomography (CT) is presented. Our method is derived from linearization and is straightforward without iteration involved. The linearization is commonly used as a preprocessing method in BHC. Conventionally, only one-material objects can be conveniently corrected by linearization. In industrial CT, two-material objects, especially cylinders with high-Z material outside and low-Z material inside are frequently encountered. Our approach focuses on this kind of objects. The new method works well as long as the two-material object meets the conditions that the thickness of the outer material (usually wall) is thick enough and the second-order item of the Taylor expansion of the linearization is relatively small. We pointed out and proved that there is an approximately constant scaling factor difference between our linearization step and an ideal correction based on prior knowledge of objects. The scaling factor magnifies the attenuation coefficient of the inner material after reconstruction. Therefore, a weighting function is introduced into our algorithm as a restoration. To sum up, there are three steps in our method: 1) correct raw projections by the mapping function of the outer material; 2) reconstruct the cross-section image from the modified projections; 3) scale the image by a weighting function. With this method, the beam hardening artifacts are greatly reduced and the overall attenuation coefficients are accurately obtained. We also presented a compensation step to remove the countercupping artifacts in case that the conditions are not fully met. Our method is well verified in both numerical simulations and practical experiments on a 450-KeV CT system","Computer industry,
Computed tomography,
Image reconstruction,
Attenuation,
Filtration,
Physics,
X-ray imaging,
Taylor series,
Image restoration,
Raw materials"
The CICQ Switch with Virtual Crosspoint Queues for Large RTT,"The memory size required for a combined input and crosspoint queued (CICQ) switch with an existing credit-based flow control is proportional to the flow control latency between the line cards and switch fabric. Recently, scalable packet switches were implemented with distributed units introducing a large RTT latency between the line cards and switch fabric, making the CP buffer requirement impractical. In this paper, a CICQ switch with virtual crosspoint queues (VCQs) is investigated. The VCQs unit resides inside the switch fabric and is dynamically shared among virtual output queues (VOQ) from the same source port and is operated at the line rate, making the implementation practical. The CICQ switch with VCQ can achieve high throughput for unbalanced traffic while reducing over 87% of the total buffer size requirement in the switch fabric, thus making the scalable implementation of a distributed CICQ switch practical.",
Radiative Heat Transfer Simulation Using Programmable Graphics Hardware,"To analyze physical behaviors of a thermal environment, we have to simulate several heat transfer phenomena such as heat conduction, convection, and radiation. Among those phenomena, radiative heat transfer simulation is much time-consuming. In this paper, therefore, one of acceleration techniques developed in the graphics community that exploits a graphics processing unit (GPU) is applied to the basic radiative heat transfer simulation. Implementation of the simulation on GPU makes GPU's computing power available for the most time-consuming part of the simulation, calculation of form factors between surfaces. This paper improves the computational accuracy of the radiative heat transfer simulation running on GPU, and then examines its performance, in terms of the trade-off between the execution time and computational accuracy. The experimental results clearly show that GPU co-processing can significantly accelerate the form factor calculation. Therefore, the GPU implementation is a promising approach to acceleration of the radiative transfer simulation, especially in the case where the form factor matrix becomes too large to be stored in the main memory","Heat transfer,
Graphics,
Hardware,
Computational modeling,
Acceleration,
Cogeneration,
Lighting,
Analytical models,
Heat engines,
Computer simulation"
Benefit-based Data Caching in Ad Hoc Networks,"Data caching can significantly improve the efficiency of information access in a wireless ad hoc network by reducing the access latency and bandwidth usage. However, designing efficient distributed caching algorithms is non-trivial when network nodes have limited memory. In this article, we consider the cache placement problem of minimizing total data access cost in ad hoc networks with multiple data items and nodes with limited memory capacity. The above optimization problem is known to be NP-hard. Defining benefit as the reduction in total access cost, we present a polynomial-time centralized approximation algorithm that provably delivers a solution whose benefit is at least one-fourth (one-half for uniform-size data items) of the optimal benefit. The approximation algorithm is amenable to localized distributed implementation, which is shown via simulations to perform close to the approximation algorithm. Our distributed algorithm naturally extends to networks with mobile nodes. We simulate our distributed algorithm using a network simulator (ns2), and demonstrate that it significantly outperforms another existing caching technique (by Yin and Cao [30]) in all important performance metrics. The performance differential is particularly large in more challenging scenarios, such as higher access frequency and smaller memory.","Ad hoc networks,
Approximation algorithms,
Distributed algorithms,
Mobile ad hoc networks,
Delay,
Bandwidth,
Algorithm design and analysis,
Cost function,
Polynomials,
Measurement"
Specifying and proving properties of timed I/O automata in the TIOA toolkit,"Timed I/O Automata (TIOA) is a mathematical framework for modeling and verification of distributed systems that involve discrete and continuous dynamics. TIOA can be used for example, to model a real-time software component controlling a physical process. The TIOA model is sufficiently general to subsume other models in use for timed systems. The TIOA toolkit, currently under development, is aimed at supporting system development based on TIOA specifications. The TIOA toolkit is an extension of the IOA toolkit, which provides a specification simulator, a code generator, and both model checking and theorem proving support for analyzing specifications. This paper focuses on modeling of timed systems with TIOA and the TAME-based theorem proving support provided in the toolkit, for proving system properties, including timing properties. Several examples are provided by way of illustration","Automata,
Laboratories,
Mathematical model,
Mechanical factors,
Distributed computing,
Computer science,
Artificial intelligence,
Automatic control,
Process control,
Analytical models"
Exploiting statistical information for implementation of instruction scratchpad memory in embedded system,"A method to both reduce energy and improve performance in a processor-based embedded system is described in this paper. Comprising of a scratchpad memory instead of an instruction cache, the target system dynamically (at runtime) copies into the scratchpad code segments that are determined to be beneficial (in terms of energy efficiency and/or speed) to execute from the scratchpad. We develop a heuristic algorithm to select such code segments based on a metric, called concomitance. Concomitance is derived from the temporal relationships of instructions. A hardware controller is designed and implemented for managing the scratchpad memory. Strategically placed custom instructions in the program inform the hardware controller when to copy instructions from the main memory to the scratchpad. A novel heuristic algorithm is implemented for determining locations within the program where to insert these custom instructions. For a set of realistic benchmarks, experimental results indicate the method uses 41.9% lower energy (on average) and improves performance by 40.0% (on average) when compared to a traditional cache system which is identical in size","Embedded system,
Scanning probe microscopy,
Australia,
Cache memory,
Heuristic algorithms,
Hardware,
Memory management,
Application specific integrated circuits,
Computer science,
Space heating"
On Multicast in Quantum Networks,"We consider quantum multicast networks in which quantum states generated by multiple sources have to be simultaneously delivered to multiple receivers. We demonstrate that in addition to the apparent similarity to the multicommodity flow problems, quantum networks, to a certain extent, behave as classical communication networks. In particular, we show that lossless compression of special multicast quantum states is possible and significantly reduces the edge capacity requirements of the multicast.",
Low Power Wearable Audio Player Using Human Body Communications,"This paper presents a prototype wearable audio player system to playback the digital audio signal transmitted through the wearer's body without any wire. To significantly reduce the power consumption of the digital audio transmission, the system exploits a novel wideband signaling human body communication scheme. The transceiver chip designed, fabricated, and assembled on the prototype system shows 5-mW power consumption at 2-Mb/s data rate, better than the Bluetooth transceiver. Real-time audio playing through the human body is successfully demonstrated.",
A Sniffer Based Approach to WS Protocols Conformance Checking,"To reduce interoperability problems arising from ambiguous or incomplete Web services protocol specifications, we have recently introduced a formal framework, which allows modelling and automatic verification of such protocols. However, interoperability problems can still occur due to incorrect implementations. In this paper, we introduce a sniffer based approach to check the conformance of a protocol's implementation to its specification; messages of the actual implementations are captured, processed and checked against the specification's formal model. We also briefly illustrate the application of our framework using a version of the WS-AtomicTransaction protocol","Protocols,
Testing,
Australia,
Web services,
Automata,
Formal specifications,
Computer science,
Asia,
Communication standards,
Programming profession"
Overview on Development of Remote Teaching Laboratories: from LabVIEW to Web Services,"For last years, a large number of universities have been focusing their attention on the opportunity of making students capable to observe experimental activities, from their home and at any time, through Internet. The realization of remote laboratories for engineering disciplines, in particular for a measurement course, presents further problems, since it is required that students interact with remote circuit and handle measurement instruments. The crucial task is, hence, to implement Web/Internet-enabled applications that can be fully controlled and monitored from remote locations. There is a wide variety of solutions enabling the remote control of measurement instrumentation; each one is characterized by its peculiarities, advantages, and drawbacks. In the paper an overview on the most adopted technologies is performed. Particular attention is paid on Web services technology that proves to be a general solution, suitable for each type of application","Education,
Web services,
Internet,
Particle measurements,
Instruments,
Educational institutions,
Remote laboratories,
Circuits,
Remote monitoring,
Paper technology"
Advanced Incremental Conductance MPPT Algorithm with a Variable Step Size,"This paper proposes an advanced incremental conductance MPPT algorithm with a variable step size for a solar array regulator (SAR) system. This approach adjusts automatically a step size to the solar array operating point, thus improving the MPPT (maximum peak power tracking) speed and accuracy compared with the conventional method with a fixed step size. For the analysis of stability of the proposed algorithm near the maximum power point, the small signal modeling is carried out. Also, it is verified by experiment using a 180 W parallel connected prototype hardware","Control systems,
Regulators,
Prototypes,
Hardware,
Low earth orbit satellites,
Computer science,
Signal analysis,
Algorithm design and analysis,
Stability analysis,
Voltage"
Distributed Intrusion Detection Framework based on Autonomous and Mobile Agents,"Implementation of intrusion detection systems with agent technology is one of the new paradigms for intrusion detection for computer systems. In this paper, we propose a distributed intrusion detection framework based on autonomous and mobile agents. In this framework, the mobile agent platform ""aglets"" is utilized. The system has five types of agents: administrator agents, analyzer agents, connection agents, crisis agents, and update behavior agents. These agents interact with each other to perform the detection task. We also discuss the implementation issues about our system","Intrusion detection,
Mobile agents,
Expert systems,
Postal services,
Distributed computing,
Computer security,
Data security,
Statistical analysis,
Neural networks,
Neurons"
An Effective Anytime Anywhere Parallel Approach for Centrality Measurements in Social Network Analysis,"With the broad application of electronic communication monitoring tools and data-sharing techniques, the size of networks to be studied by social network analysis (SNA) has grown rapidly. However, current SNA techniques are not particularly scalable. For example, even centrality, which is one of the most frequently used SNA parameters, cannot be measured by most current SNA software when the network is large. This paper presents the design of an effective and scalable anytime anywhere parallel methodology for SNA with large-scale networks emphasizing centrality measurement algorithms. The efficiency and effectiveness of the methodology is validated by experiments of centrality analysis for large networks.","Social network services,
Computer networks,
Current measurement,
Power measurement,
Collaboration,
Application software,
Large-scale systems,
Cybernetics,
Monitoring,
Public healthcare"
New Face Recognition Method Based on DWT/DCT Combined Feature Selection,A new face recognition method is proposed based on DWT/DCT combined feature selection. The combination of the DWT and the DCT is used to extract features of face images. This reduces the dimension of the original face image and preserves the property of data distribution in the feature subspace. Then the support vector machine (SVM) based on the structural risk minimization principle is used to classify these feature vectors. The new strategy is tested on ORL face databases and the results demonstrate that it performs very well compared with other approaches,
Analysis and Comparison of Basic Schemes of Synchronized PWM for Dual Inverter-Fed Drives,"Two basic schemes of synchronized pulsewidth modulation (PWM) have been disseminated for control of dual inverter-fed open-end winding induction motor drives with full elimination of the common-mode voltages. The algorithms of synchronized PWM provide symmetry of the phase voltage of the system during the whole control range including the zone of overmodulation. Spectra of the motor phase voltage of dual inverter-fed drives do not contain even harmonics and sub-harmonics (combined harmonics), which is especially important for the systems with increased power rating. Analysis of processes in drive systems with two schemes of synchronized PWM, based on the modelling and simulations, has been done","Pulse width modulation inverters,
Voltage control,
Pulse width modulation,
Motor drives,
Pulse inverters,
Induction motor drives,
Control systems,
Power system harmonics,
Topology,
Space vector pulse width modulation"
A Framework for Compositional and Hierarchical Real-Time Scheduling,"Hierarchical scheduling frameworks have lately received a lot of attention for component-based design of complex real-time systems. The specification of the resource reservation policy play a dominant role in such frameworks. In this context, the notion of real-time virtual resources results a very flexible representation of resource reservation schemes. We intend to combine the advantages offered by virtual resource scheduling with very general event models specified using real-time calculus. Our proposed framework permits resource partitioning to be extended to multiple levels and handles a wider range of scheduling algorithms and task models. In addition, it allows the handling of data dependencies between tasks from different task groups in the hierarchy",
Skin Colour-Based Face Detection in Colour Images,"We propose in this work a method for detecting faces in colour images with complex backgrounds. The approach starts with the transformation of the image pixels from the RGB colour space to the chrominance space (YCbCr). Secondly, a Gaussian model is fitted on the transformed image in order to calculate the likelihood of skin for each pixel and to create a likelihood image. Thirdly, by thresholding the likelihood image, skin pixels are segmented to form a binary skin map, which contains the candidate face regions. Finally, a verification process is carried out to determine whether these candidate face regions are real faces or not.","Skin,
Face detection,
Pixel,
Image segmentation,
Face recognition,
Biometrics,
Chromium,
Videoconference,
Computer science,
Information security"
Limits to List Decoding Reed&#8211;Solomon Codes,"In this paper, we prove the following two results that expose some combinatorial limitations to list decoding Reed-Solomon codes. 1) Given n distinct elements alpha1,...,alphan from a field F, and n subsets S1,...,Sn of F, each of size at most l, the list decoding algorithm of Guruswami and Sudan can in polynomial time output all polynomials p of degree at most k that satisfy p(alphai)isinSi for every i, as long as l<lceiln/krceil. We show that the performance of this algorithm is the best possible in a strong sense; specifically, when l=lceiln/krceil, the list of output polynomials can be superpolynomially large in n. 2) For Reed-Solomon codes of block length n and dimension k+1 where k=ndelta for small enough delta, we exhibit an explicit received word with a superpolynomial number of Reed-Solomon codewords that agree with it on (2-epsi)k locations, for any desired epsi>0 (agreement of k is trivial to achieve). Such a bound was known earlier only for a nonexplicit center. Finding explicit bad list decoding configurations is of significant interest-for example, the best known rate versus distance tradeoff, due to Xing, is based on a bad list decoding configuration for algebraic-geometric codes, which is unfortunately not explicitly known",
Generation of Conformance Test Suites for Compositions of Web Services Using Model Checking,"Testing compositions of Web services is complex, due to their distributed nature and asynchronous behaviour. However, research in this field is scarce. We propose a new testing method for compositions of Web services. A formal verification tool (the SPIN model checker) is used to automatically generate test suites for compositions specified in an industry standard language: BPEL. Adequacy criteria is employed to define a systematic procedure to select the test cases. Preliminary results have been obtained using a transition coverage criterion","Web services,
Formal verification,
Software testing,
Unified modeling language,
Computer science,
Automatic testing,
System testing,
Investments,
Computer industry,
Application software"
Run-time detection of covert channels,"The authors are interested in the characterization of policies which are enforced by execution monitoring mechanisms with an extra structure that is an extension of Schneider's enforcement mechanism. This paper is a starting point for continuing in this area. We use an emulator as the extra structure, which emulates the behavior of a system by running a subsequence from an interleaved state sequence of processes, in order to detect several covert channels at run time. We then define a security automaton for this extended mechanism and show a class of properties which is enforced by the security automaton. Further, our mechanism can enforce information flow policies, which are specified by system developers, under an information flow property to be defined for the aim of this study. We show that the information flow property include O'Halloran's noninference. In the last of this paper, we give a simple example for the policy and an outline of our mechanism.","Runtime,
Information security,
Permission,
Automata,
Mechanical factors,
Computer science,
Information science,
Computerized monitoring,
Timing,
Access control"
Decentralized robustness,"Robustness links confidentiality and integrity properties of a computing system and has been identified as a useful property for characterizing and enforcing security. Previous characterizations of robustness have been with respect to a single idealized attacker; this paper shows how to define robustness for systems with mutual distrust. Further, we demonstrate that the decentralized label model (DLM) can be extended to support fine-grained reasoning about robustness in such systems. The DLM is a natural choice for capturing robustness requirements because decentralized labels are explicitly expressed in terms of principals that can be used to characterize the power of attackers across both the confidentiality and integrity axes. New rules are proposed for statically checking robustness and qualified robustness using an extended DLM; the resulting type system is shown to soundly enforce robustness. Finally, sound approximations are developed for checking programs with bounded but unknown label parameters, which is useful for security-typed languages. In sum, the paper shows how to use robustness to gain assurance about secure information flow and information release in systems with complex security requirements","Robustness,
Information security,
Computer security,
Power system modeling,
Robust control,
Computer science,
Programming profession,
Data security,
Power system security,
Conferences"
Visual Servoing with Moments of SIFT Features,"Robotic manipulation of daily-life objects is an essential requirement in service robotic applications. In that context image based visual servoing is a means to position the end-effector in order to manipulate objects of unknown pose. This contribution proposes a 6 DOF visual servoing scheme that relies on the pixel coordinates, scale and orientation of SIFT features. The control is based on geometric moments computed over an alterable set of redundant SIFT feature correspondences between the current and the reference view. The method is generic as it does not depend on a geometric object model but automatically extracts SIFT features from images of the object. The foundation of visual servoing on generic SIFT features renders the method robust with respect to loss of redundant features caused by occlusion or changes in view point. The moment based representation establishes an approximate one-to-one relationship between visual features and degrees of motion. This property is exploited in the design of a decoupled controller that demonstrates superior performance in terms of convergence and robustness compared with an inverse image Jacobian controller. Several experiments with a robotic arm equipped with a monocular eye-in-hand camera demonstrate that the approach is efficient and reliable.","Visual servoing,
Robot kinematics,
Automatic control,
Solid modeling,
Feature extraction,
Rendering (computer graphics),
Robustness,
Convergence,
Robust control,
Jacobian matrices"
Making lockless synchronization fast: performance implications of memory reclamation,"Achieving high performance for concurrent applications on modern multiprocessors remains challenging. Many programmers avoid locking to improve performance, while others replace locks with non-blocking synchronization to protect against deadlock, priority inversion, and convoying. In both cases, dynamic data structures that avoid locking, require a memory reclamation scheme that reclaims nodes once they are no longer in use. The performance of existing memory reclamation schemes has not been thoroughly evaluated. We conduct the first fair and comprehensive comparison of three recent schemes -quiescent-state-based reclamation, epoch-based reclamation, and hazard-pointer-based reclamation - using a flexible microbenchmark. Our results show that there is no globally optimal scheme. When evaluating lockless synchronization, programmers and algorithm designers should thus carefully consider the data structure, the workload, and the execution environment, each of which can dramatically affect memory reclamation performance","Data structures,
Programming profession,
Yarn,
Linux,
System recovery,
Computer science,
Application software,
Protection,
Algorithm design and analysis,
Content addressable storage"
Optimal register sharing for high-level synthesis of SSA form programs,"Register sharing for high-level synthesis of programs represented in static single assignment (SSA) form is proven to have a polynomial-time solution. Register sharing is modeled as a graph-coloring problem. Although graph coloring is NP-Complete in the general case, an interference graph constructed for a program in SSA form probably belongs to the class of chordal graphs that have an optimal O(|V|+|E|) time algorithm. Chordal graph coloring reduces the number of registers allocated to the program by as much as 86% and 64.93% on average compared to linear scan register allocation.","High level synthesis,
Registers,
Resource management,
Processor scheduling,
Interference,
Flow graphs,
Polynomials,
Silicon,
Optimal control,
Computer science"
Strategies for Language Model Web-Data Collection,"This paper presents an analysis of the use of textual information collected from the Internet via a search engine for the purpose of building domain specific language models. A framework to analyse the effect of search query formulation on the resulting Web-data language model performance in an evaluation is developed. The framework gives rise to improved methods of selecting n-gram search engine queries, which return documents that make better domain specific language models",
Expander Graph based Key Distribution Mechanisms in Wireless Sensor Networks,"Secure communications between large number of sensor nodes that are randomly scattered over a hostile territory, necessitate efficient key distribution schemes. However, due to limited resources at sensor nodes such schemes cannot be based on post deployment computations. Instead, pairwise (symmetric) keys are required to be pre-distributed by assigning a list of keys, (a.k.a. key-chain), to each sensor node. If a pair of nodes does not have a common key after deployment then they must find a key-path with secured links. The objective is to minimize the keychain size while (i) maximizing pairwise key sharing probability and resilience, and (ii) minimizing average key-path length. This paper presents a deterministic key distribution scheme based on Expander Graphs. It shows how to map the parameters (e.g., degree, expansion, and diameter) of a Ramanujan Expander Graph to the desired properties of a key distribution scheme for a physical network topology.","Graph theory,
Wireless sensor networks,
Resilience,
Computer science,
Raman scattering,
Intrusion detection,
Laboratories,
Network topology"
Using Quick-Start to Improve TCP Performance with Vertical Hand-offs,"Vertical hand-offs between different wireless access technologies have become more relevant after the introduction of multi-access mobile terminals with wireless LAN (WLAN) and wireless WAN (WWAN) technologies. While the IP mobility mechanisms are rather well known, the performance of TCP still has problems when moving between WLAN and WWAN accesses. First, with a high-latency WWAN link technology such as GPRS it takes several seconds before the TCP congestion window has reached the path capacity. Second, when the notification of the first packet loss arrives at the TCP sender, several packets have already been lost due to the slow-start overshoot and the TCP sender needs to retransmit a large number of the packets from the last transmission window. Third, after a vertical hand-off the path characteristics might have changed dramatically in which case the TCP congestion control state is not valid any more. In this paper we investigate Quick-Start, a mechanism for avoiding the initial slow-start delay, in the context of wireless multi-access terminals. We also propose an enhancement to Quick-Start to alleviate the effects of slow-start overshoot and apply Quick-Start after a vertical hand-off to quickly learn the available capacity on the new end-to-end path. An explicit cross-layer hand-off notification is employed to trigger Quick-Start when the hand-off completes. We conduct simulations with different hand-off models, and our simulations yield promising results with Quick-Start",
Attentional Landmark Selection for Visual SLAM,"In this paper, we introduce a new method to automatically detect useful landmarks for visual SLAM. A biologically motivated attention system detects regions of interest which ""pop-out"" automatically due to strong contrasts and the uniqueness of features. This property makes the regions easily redetectable and thus they are useful candidates for visual landmarks. Matching based on scene prediction and feature similarity allows not only short-term tracking of the regions, but also redetection in loop closing situations. The paper demonstrates how regions are determined and how they are matched reliably. Various experimental results on real-world data show that the landmarks are useful with respect to be tracked in consecutive frames and to enable closing loops",
Using the Clinical Document Architecture as Open Data Exchange Format for Interfacing EMRs with Clinical Decision Support Systems,"Clinical decision support systems (CDSS) can significantly increase the quality of care while decreasing cost and effort. They are difficult to develop and most existing systems are proprietary, tightly integrated with specific electronic medical record (EMR) systems, and expensive to own. EGADSS is an open-source CDSS that has been developed as a standalone, standards-based, re-usable component to make decision-support available for any EMR. In order to realize this vision, the EGADSS team has had to develop an open interface for medical data exchange, which maximizes interoperability, simplicity and standard-conformance. This paper reports on a solution to this challenge based on the HL7 clinical document architecture (CDA) and the electronic medical summary standard. CDA-based medical summaries are used to encapsulate virtual medical records about patients that serve the DSS component as temporary databases. We show how these temporary databases can be queried from within Arden syntax-based guidelines in a standard query language. Moreover, we show how the CDA can be used to communicate CDSS alerts and recommendations back to the EMR. We report on our evaluation with a prototype implementation and compare it with alternative approaches","Decision support systems,
Guidelines,
Open source software,
Computer science,
Costs,
Database languages,
Back,
Prototypes,
Software maintenance,
Investments"
Efficient Sharing of Sensor Networks,"In this paper we tackle the problem of allowing applications to request different data at different rates from different sensors of the same sensor network while still being able to run the sensor network in an efficient manner. Our approach is to merge an arbitrary number of user queries into a network query. By doing this, traffic is minimised and the sensors have better energy consumption behavior than if all user queries would have been directly sent to the network. In the paper we describe the algorithms for the transformation of queries and the resulting data streams. We also provide an extensive performance evaluation of the algorithms using sets of over hundred overlapping user queries executing on the same sensor network",
On Filtering of DDoS Attacks Based on Source Address Prefixes,"Distributed denial of service (DDoS) attacks are a grave threat to Internet services and even to the network itself. Widely distributed ""zombie"" computers subverted by malicious hackers are used to orchestrate massive attacks. Any defense against such flooding attacks must solve the hard problem of distinguishing the packets that are part of the attack from legitimate traffic, so that the attack can be filtered out without much collateral damage. We explore one technique that can be used as part of DDoS defenses: using ACL rules that distinguish the attack packets from the legitimate traffic based on source addresses in packets. One advantage of this technique is that the ACL rules can be deployed in routers deep inside the network where the attack isn't large enough to cause loss of legitimate traffic due to congestion. The most important disadvantage is that the ACL rules can also cause collateral damage by discarding some legitimate traffic. We use simulations to study this damage how it is influenced by various factors. Our technique is much better than uninformed dropping due to congestion, but it produces larger collateral damage than more processing-intensive approaches. For example it can reduce the attack size by a factor of 3 while also dropping between 2% and 10% of the legitimate traffic. We recommend the use of source address prefix based filtering in combination with other techniques, for example as a coarse pre-filter that ensures that devices performing the processing-intensive filtering are not overwhelmed","Filtering,
Computer crime"
Ensuring Area k-Coverage in Wireless Sensor Networks with Realistic Physical Layers,"Wireless sensor networks are composed of hundreds of small and low power devices deployed over a field to monitor. Energy consumption is balanced by taking advantage of the redundancy induced by the random deployment of nodes. Some nodes are active while others are in sleep mode. Area coverage protocols aim at turning off redundant sensor nodes while preserving satisfactory monitoring by the set of active nodes. The problem addressed here consists in building k distinct subsets of active nodes (layers), in a fully decentralized manner, so that each layer covers the area. In our protocol, each node selects a waiting timeout, listening to messages from neighbors. Activity messages include the layer at which a node has decided to be active. Depending on the physical layer used for sensing modeling, any node can evaluate if the provided coverage is sufficient for each layer. If so, node can sleep, otherwise it selects a layer to be active. Here, we describe a localized area coverage protocol able to maintain an area k-covered under realistic physical layer assumptions for both sensing and communicating modules.",
Study on the Rotor Levitation of one High Speed Switched Reluctance Motor,"One two-phase switched reluctance motor with a magnetic levitated rotor is presented for high speed applications. Firstly the lamination configuration and the rotor pole shape are optimized through electromagnetic field calculations. Then the production of radial forces is investigated and the characteristics are calculated. Based on the analysis, the motor structure for the levitation of the laminated rotor is proposed. And the radial forces with similar amplitude can be generated at any rotor angular positions, and are decoupled inherently in this structure","Rotors,
Reluctance motors,
Torque,
Permanent magnet motors,
Magnetic levitation,
Strontium,
Stators,
Shape,
Force control,
Induction motors"
Traffic Identification and Overlay Measurement of Skype,"Nowadays many P2P applications over the Internet utilize sophisticated protocols and employ various techniques to avoid detection and recognition with standard measurement tools. The communication analysis of P2P network is significant for recognizing P2P traffic and managing P2P applications effectively. In this paper, Skype is studied as a case to identify P2P traffic and measure an overlay network of P2P system. Because Skype protocol is proprietary and its control information is encrypted, little is understood about Skype network. We analyzed large amounts of data of Skype traffic and extracted many application level signatures. Based on such signatures, we can identify Skype traffic, then an effective tool named Super Nodes Collecting and Probing Platform (SNCPP) was designed to measure the overlay of Skype network. The SNCPP probes online super nodes in real-time, and it displays the distribution in a three-dimensional world map. The results in this paper should be useful for future architects of P2P overlay networks as well as for network security management","Communication system traffic control,
Protocols,
Telecommunication traffic,
Internet,
Measurement standards,
Communication system control,
Cryptography,
Data mining,
Probes,
Three dimensional displays"
A Function-Parallel Architecture for High-Speed Firewalls,"Firewalls enforce a security policy by inspecting and filtering traffic arriving or departing from a secure network. This is typically done by comparing an arriving packet to a set of rules and performing the matching rule action, which is accept or deny. Unfortunately packet inspections can impose significant delays on traffic due to the complexity and size of policies. Therefore, improving firewall performance is important given the next generation of high-speed networks. This paper introduces a new firewall architecture that can perform packet inspections under increasing traffic loads, higher traffic speeds, and strict QoS requirements. The architecture consists of multiple firewalls configured in parallel that collectively enforce a security policy. Each firewall implements part of the policy and arriving packets are processed by all the firewalls simultaneously. Since multiple firewalls are used to process every packet, the proposed function-parallel system has significantly lower delays (e.g. 74% lower for a four firewall system) and a higher throughput than other data-parallel (load-balancing) firewalls. These findings will be demonstrated empirically. Furthermore unlike data-parallel systems, the function-parallel design allows the stateful inspection of packets, which is critical to prevent certain types of network attacks.","Telecommunication traffic,
Inspection,
Information security,
Filtering,
Delay,
Data security,
Throughput,
Switches,
Computer architecture,
Computer science"
Proportional Fairness for Overlapping Cells in Wireless Networks,"Coordination of cellular base stations (BS) with overlapping coverage enables joint optimization of radio resource allocation in a multiple cell environment. This paper extends existing Proportional Fairness model for wired networks and single wireless cell, to the context of multiple (probably heterogeneous) wireless cells with overlapping coverage. The proposed fair allocation achieves both global Pareto optimality and inter- cell fairness (load balance). However, the ideal allocation is not practical as it requires a mobile station (MS) be simultaneously associated with multiple BSs. Instead, we use a simple GLS (Greedy Logarithmic Sum) scheme, which associates each new arrival MS with only one BS, to approximate the optimal allocation. Simulation result shows that GLS performs close to optimal scheme in a wide range of network settings.",
Self-Organization in Community Mesh Networks The Berlin RoofNet,"A community network must be usable for inexperienced end users; thus self-organization is essential. On the one hand, we propose an approach for self-organization in ad-hoc wireless multi-hop mesh networks, where the client is fully freed from such mundane tasks as IP configuration, etc. On the other hand, the community mesh network itself is fully self-organized thus no operator or provider is required. We present the architecture of the Berlin RoofNet (BRN) and a distributed realization of services like DHCP, ARP and Internet gateway discovery and selection. In addition, results of a detailed simulation and experimental evaluation comparing our distributed hash table based approach to traditional methods are presented. We show that our approach is more reliable, efficient and responsive","Mesh networks,
Computer architecture,
Spread spectrum communication,
IP networks,
Cities and towns,
Protocols,
Web and internet services,
Computer network reliability,
Telecommunication network reliability,
Wireless mesh networks"
Shape Topics: A Compact Representation and New Algorithms for 3D Partial Shape Retrieval,"This paper develops an efficient new method for 3D partial shape retrieval. First, a Monte Carlo sampling strategy is employed to extract local shape signatures from each 3D model. After vector quantization, these features are represented by using a bag-of-words model. The main contributions of this paper are threefold as follows: 1) a partial shape dissimilarity measure is proposed to rank shapes according to their distances to the input query, without using any timeconsuming alignment procedure; 2) by applying the probabilistic text analysis technique, a highly compact representation ""Shape Topics"" and accompanying algorithms are developed for efficient 3D partial shape retrieval, the mapping from ""Shape Topics"" to ""object categories"" is established using multi-class SVMs; and 3) a method for evaluating the performance of partial shape retrieval is proposed and tested. To our best knowledge, very few existing methods are able to perform well online partial shape retrieval for large 3D shape repositories. Our experimental results are expected to validate the efficacy and effectiveness of our novel approach.","Shape measurement,
Text analysis,
Computer vision,
Vector quantization,
Biological system modeling,
Computer aided manufacturing,
Laboratories,
Computer science,
Monte Carlo methods,
Testing"
"Efficient, Energy Conserving Transaction Processing in Wireless Data Broadcast","Broadcasting in wireless mobile computing environments is an effective technique to disseminate information to a massive number of clients equipped with powerful, battery operated devices. To conserve the usage of energy, which is a scarce resource, the information to be broadcast must be organized so that the client can selectively tune in at the desired portion of the broadcast. In this paper, the efficient, energy conserving transaction processing in mobile broadcast environments is examined with widely accepted approaches to indexed data organizations suited for a single item retrieval. The basic idea is to share the index information on multiple data items based on the predeclaration technique. The analytical and simulation studies have been performed to evaluate the effectiveness of our methodology, showing that predeclaration-based transaction processing with selective tuning ability can provide a significant performance improvement of battery life, while retaining a low access time. Tolerance to access failures during transaction processing is also described","Broadcasting,
Batteries,
Mobile computing,
Information retrieval,
Personal digital assistants,
Indexing,
Bandwidth,
Energy efficiency,
Performance analysis,
Computational modeling"
Recognition of Multi-Object Events Using Attribute Grammars,"We present a method for representing and recognizing visual events using attribute grammars. In contrast to conventional grammars, attribute grammars are capable of describing features that are not easily represented by finite symbols. Our approach handles multiple concurrent events involving multiple entities by associating unique object identification labels with multiple event threads. Probabilistic parsing and probabilistic conditions on the attributes are used to achieve a robust recognition system. We demonstrate the effectiveness of our method for the task of recognizing vehicle casing in parking lots and events occurring in an airport tarmac.","Pattern recognition,
Stochastic processes,
Yarn,
Robustness,
Vehicles,
Airports,
Error correction,
Production,
Computer science,
Automation"
On the Optimal SINR in Random Access Networks with Spatial Reuse,"In this paper we consider a simple model for a homogeneous random access system with spatial reuse. Our aim is to obtain insight into how physical layer parameters should be determined in order to maximize the total throughput-distance per unit area. In an infinitely dense network, we find that the optimal SINR threshold is asymptotically zero. Furthermore, we find that the optimal distance between transmitter and receiver is asymptotically non-zero. For finitely dense networks, we find that the optimal SINR threshold for packet detection is considerably smaller than that used in many current systems, and that the optimal distance between transmitters and intended receivers is larger than would be expected from prevailing conventional wisdom. These result suggest that the throughput performance of current systems may be improved by operating at lower values of SINR.",
Performance modeling of communication and computation in hybrid MPI and OpenMP applications,"Performance evaluation and modeling is a crucial process to enable the optimization of parallel programs. Programs written using two programming models, such as MPI and OpenMP, require an analysis to determine both performance efficiency and the most suitable numbers of processes and threads for their execution on a given platform. To study both of these problems, we propose the construction of a model that is based upon a small number of parameters, but is able to capture the complexity of the runtime system. We must incorporate measurements of overheads introduced by each of the programming models, and thus need to model both the network and computational aspects of the system. We have combined two different techniques: static analysis, driven by the OpenUH compiler, to retrieve application signatures and a parallelization overhead measurement benchmark, realized by Sphinx and Perfsuite, to collect system profiles. Finally, we propose a performance evaluation measurement to identify communication and computation efficiency. In this paper we describe our underlying framework, the performance model, and show how our tool can be applied to a sample code",
Empirical Studies on Multi-label Classification,"In classic pattern recognition problems, classes are mutually exclusive by definition. However, in many applications, it is quite natural that some instances belong to multiple classes at the same time. In other words, these applications are multi-labeled, classes are overlapped by definition and each instance may be associated to multiple classes. In this paper, we present a comparative study on various multi-label approaches using both gene and scene data sets. We expect our research efforts provide useful insights on the relationships among various classifiers as well as various evaluation measures and shed lights on future research. Although there is no clear winner across various performance measures, SVM binary and multi-label ADTree perform better than the others on most counts. We then propose a meta-learning approach by combining SVM binary and ADTree. Our experiments demonstrate that the combined method can take the advantages of the single approaches",
Monitoring the Earth System Grid with MDS4,"In production Grids for scientific applications, service and resource failures must be detected and addressed quickly. In this paper, we describe the monitoring infrastructure used by the Earth System Grid (ESG) project, a scientific collaboration that supports global climate research. ESG uses the Globus Toolkit Monitoring and Discovery System (MDS4) to monitor its resources. We describe how the MDS4 Index Service collects information about ESG resources and how the MDS4 Trigger Service checks specified failure conditions and notifies system administrators when failures occur. We present monitoring statistics for May 2006 and describe our experiences using MDS4 to monitor ESG resources over the last two years.","Earth,
Condition monitoring,
Portals,
Computer science,
Laboratories,
International collaboration,
Grid computing,
Computerized monitoring,
Geoscience,
Mathematics"
NIS07-5: Security Vulnerabilities in Channel Assignment of Multi-Radio Multi-Channel Wireless Mesh Networks,"In order to fully exploit the aggregate bandwidth available in the radio spectrum, future wireless mesh networks (WMN) are expected to take advantage of multiple orthogonal channels, with nodes having the ability to communicate with multiple neighbors simultaneously using multiple radios (NICs) over orthogonal channels. Dynamic channel assignment is critical for ensuring effective utilization of the non-overlapping channels. Several algorithms have been proposed in recent years, which aim at achieving this. However, all these schemes inherently assume that the mesh nodes are well-behaved without any malicious intentions. In this paper, we expose the vulnerabilities in channel assignment algorithms and unveil three new security attacks: Network Endo-Parasite Attack (NEPA), Channel Ecto-parasite Attack (CEPA) and low-cost ripple effect attack (LORA). These attacks can be launched with relative ease by a malicious node and can cause significant degradation in the network performance. We also evaluate the effectiveness of these attacks through simulation based experiments and briefly discuss possible solutions to counter these new threats.",
Cross-layer analysis for detecting wireless misbehavior,,
System Integration of the LabPET Small Animal PET Scanner,"To address modern molecular imaging requirements, a digital positron emission tomography scanner for small animals has been developed at Universite de Sherbrooke. Based on individual readout of avalanche photodiodes (APD) coupled to a LYSO/LGSO phoswich array, the scanner supports up to 3072 channels in a 16.2 cm diameter, 7.5 cm axial field of view with an isotropic 1.2 mm FWHM intrinsic spatial resolution at the center of the FOV. Custom data acquisition boards sample APD signals at 45 MHz and compute in real time crystal identification, energy and timing information of detected events at rates of up to 1250 raw counts per second per mm2 (10k cps/channel). Real time digital signal analysis also filters out events outside the photopeak with crystal granularity to eliminate Compton events and electronic noise. Retained events are then merged into a single stream through a real-time sorting tree, at which end the prompt and delayed coincidences are extracted. A single Firewire link handles both control and data transfers with a computer. The LabPETtrade features four data recording modes, giving the user the choice to retain data for research or to minimize file size for high coincidence count rate and imaging purposes. The electronic system also supports time synchronized data insertion for flags such as vital signs used in gated image reconstruction. Aside from data acquisition, hardware can generate live energy and discrimination histograms suitable for fast, automatic channel calibration.","Animals,
Positron emission tomography,
Data acquisition,
Firewire,
Molecular imaging,
Avalanche photodiodes,
Spatial resolution,
Signal processing,
Timing,
Event detection"
Data Mining Approaches to Criminal Career Analysis,"Narrative reports and criminal records are stored digitally across individual police departments, enabling the collection of this data to compile a nation-wide database of criminals and the crimes they committed. The compilation of this data through the last years presents new possibilities of analyzing criminal activity through time. Augmenting the traditional, more socially oriented, approach of behavioral study of these criminals and traditional statistics, data mining methods like clustering and prediction enable police forces to get a clearer picture of criminal careers. This allows officers to recognize crucial spots in changing criminal behaviour and deploy resources to prevent these careers from unfolding. Four important factors play a role in the analysis of criminal careers: crime nature, frequency, duration and severity. We describe a tool that extracts these from the database and creates digital profiles for all offenders. It compares all individuals on these profiles by a new distance measure and clusters them accordingly. This method yields a visual clustering of these criminal careers and enables the identification of classes of criminals. The proposed method allows for several user-defined parameters.",
PBIRCH: A Scalable Parallel Clustering algorithm for Incremental Data,We present a parallel version of BIRCH with the objective of enhancing the scalability without compromising on the quality of clustering. The incoming data is distributed in a cyclic manner (or block cyclic manner if the data is bursty) to balance the load among processors. The algorithm is implemented on a message passing share-nothing model. Experiments show that for very large data sets the algorithm scales nearly linearly with the increasing number of processors. Experiments also show that clusters obtained by PBIRCH are comparable to those obtained using BIRCH,"Clustering algorithms,
Message passing,
Computer science,
Scalability,
Delay,
Broadcasting,
Algorithm design and analysis,
Partitioning algorithms,
Memory management,
Time factors"
Guided Formation Control for Wheeled Mobile Robots,"This paper considers the topic of formation control for wheeled mobile robots (WMRs). Specifically, the work deals with unicycle-type WMRs (i.e., underactuated vehicles that experience a lateral zero-speed constraint, such that their linear velocity at all times is aligned with the longitudinal axis of symmetry). Within a leader-follower framework, a so-called guided formation control scheme is developed by means of a modular design procedure which is inspired by concepts from integrator backstepping and cascade theory. Control, guidance, and synchronization laws ensure that each individual formation member is able to converge to and maintain its assigned formation position such that the overall formation is able to assemble and maintain itself while traversing a regularly parameterized path that is chosen by a formation control designer. The proposed version of the guided approach is completely decentralized in the sense that no variables need to be communicated between the formation members (hence, the formation suffers from graceful degradation). A key quality of the suggested scheme is helmsman-like transient motion behavior, which is illustrated through a computer simulation involving three unicycle-type WMRs",
Modified Belief Propagation Decoding Algorithm for Low-Density Parity Check Code Based on Oscillation,"A low-density parity check (LDPC) code with the belief propagation (BP) or the log-likelihood ratio belief propagation (LLR-BP) can achieve good bit error rate (BER) performance approaching the Shannon limit. When a parity check matrix of the LDPC code has the cycle, the BP and LLR-BP decoding algorithms achieve approximate maximum a posterior probability (MAP) decoding. Although the decoding algorithms are approximate MAP decoding, LDPC codes can achieve very good BER. For the short and middle length LDPC codes, BER and block error rate (BLER) performances are affected by cycle largely. In each iteration, the magnitudes of a posterior LLRs of some bits oscillate owing to cycles. The oscillation is the dominant error factor in the high Eb/N o region for short and middle length LDPC codes. In this paper, we extend the definition of oscillation to extrinsic LLR (ex-LLR) derived in the bit node process and propose the modified LLR-BP and the modified UMP-BP decoding algorithms. To reduce effects of oscillating ex-LLRs on decoding, for oscillating ex-LLRs, we add the previous ex-LLR to the current ex-LLR. From the computer simulation, we show that for short and middle length LDPC codes, with a simple modification, our proposed decoding algorithms can improve the conventional LLR-BP and UMP-BP decoding algorithms. In particular, we show that the modified UMP-BP decoding algorithm with low complexity can achieve better BER and BLER than the conventional LLR-BP decoding algorithm",
Robust Accounting in Decentralized P2P Storage Systems,"A peer-to-peer (P2P) storage system allows a network of peer computers to increase the availability of their data by replicating it on other peers in the network. In such networks, a central challenge is preventing ""freeloaders"", or nodes that use disproportionately more storage on other peers than they contribute to the network. While several existing systems claim to solve this problem, we show that all known approaches are vulnerable to various attacks by either a single greedy peer or a small group of peers. To address this problem, we describe a robust distributed system to account for the storage activities of each peer. We analyze the security of this system, prove that it is secure under a much stronger attack model than previous work, and evaluate the efficiency of a prototype implementation.",
Efficient SMP-aware MPI-level broadcast over InfiniBand's hardware multicast,"Most of the high-end computing clusters found today feature multi-way SMP nodes interconnected by an ultra-low latency and high bandwidth network. InfiniBand is emerging as a high-speed network for such systems. InfiniBand provides a scalable and efficient hardware multicast primitive to efficiently implement many MPI collective operations. However, employing hardware multicast as the communication method may not perform well in all cases. This is true especially when more than one process is running per node. In this context, shared memory channel becomes the desired communication medium within the node as it delivers latencies which are of an order of magnitude lower than the inter-node message latencies. Thus, to deliver optimal collective performance, coupling hardware multicast with shared memory channel becomes necessary. In this paper we propose mechanisms to address this issue. On a 16-node 2-way SMP cluster, the Leader-based scheme proposed in this paper improves the performance of the MPI_Bcast operation by a factor of as much as 2.3 and 1.8 when compared to the point-to-point and original solution employing only hardware multicast. We have also evaluated our designs on NUMA based system and obtained a performance improvement of 1.7 using our designs on 2-node 4-way system. We also propose a dynamic attach policy as an enhancement to this scheme to mitigate the impact of process skew on the performance of the collective operation",
Aligning ASL for Statistical Translation Using a Discriminative Word Model,"We describe a method to align ASL video subtitles with a closed-caption transcript. Our alignments are partial, based on spotting words within the video sequence, which consists of joined (rather than isolated) signs with unknown word boundaries. We start with windows known to contain an example of a word, but not limited to it. We estimate the start and end of the word in these examples using a voting method. This provides a small number of training examples (typically three per word). Since there is no shared structure, we use a discriminative rather than a generative word model. While our word spotters are not perfect, they are sufficient to establish an alignment. We demonstrate that quite small numbers of good word spotters results in an alignment good enough to produce simple English-ASL translations, both by phrase matching and using word substitution.",
The EURON Roboethics Roadmap,"This paper deals with the results of the Euron Roboethics Atelier 2006 (Genoa, Italy, Feb.-March. 2006), comprising in the Roboethics Roadmap; and it offers a short overview of the ethical problems involved in the development of the next generation of the humanoid robots",
Energy-Efficient Cooperative Communication in Clustered Wireless Sensor Networks,"We study a clustered wireless sensor network where sensors within each cluster forward the message to another cluster via cooperative communication techniques. Only those sensors that correctly decode the packet from the source can participate in the subsequent cooperative communication. Hence, the number of cooperating sensors is a random variable depending on both channel and noise realizations. We formulate a multi-variable optimization problem to minimize the overall energy consumption. With numerical methods, we investigate how the energy efficiency is affected by the transmit power allocation, the total number of sensors in a cluster, the end-to-end packet error rate requirement, and the relative magnitudes of intra-cluster and inter-cluster distances.",
Performance and Availability Analysis of an E-Commerce Site,"To encourage customers to prefer e-commerce services over their on-site counterparts, it is necessary that e-commerce services be offered with superior performance and availability. A systematic methodology to enable a quantitative analysis of the performance and availability attributes of an e-commerce site can assist in identifying bottlenecks and suggesting strategies for improvement. The development of such a methodology is the subject of this paper. The analysis methodology derives expressions for performance and availability metrics of a site which are important from a customer's perspective, namely, the session length and session availability. The methodology is hierarchical, in the first step expressions are derived for each user group based on the navigational pattern of the group. The navigational pattern of a user group is represented by a customer behavior model graph (CBMG), which is mapped to a discrete time Markov chain (DTMC) for analysis. In the second step, expressions for the session length and availability of the site are derived using the distribution of the customer groups and the expressions for each customer group. We illustrate the potential of the methodology with a case study","Availability,
Performance analysis,
Navigation,
Pattern analysis,
Web and internet services,
Electronic commerce,
Business,
Companies,
Computer science,
Costs"
On the MAC protocols for Radio over Fiber indoor networks,"Radio over fiber (RoF) techniques have been long recognized as the promising solution for indoor networking at millimeter-wave bands. We discuss the possible deployment scenarios of a RoF network for indoor environment. In this paper, we try to find the answer to the question as to which medium access control (MAC) protocol is suitable for an RoF network. To substantiate our claims, performance analysis of two popular MAC protocols employing RoF EEE 802.11 representing the distributed control protocol family and ETSI HiperLAN/2 representing the centralized family - is presented. We show that RoF techniques can be applied to both protocols. We also show that the centralized control protocol is an appropriate candidate for RoF networks. This study is expected to contribute to the speedy deployment of RoF for indoor network deployment.",
Wrekavoc: a tool for emulating heterogeneity,"Computer science and especially heterogeneous distributed computing is an experimental science. Simulation, emulation, or in-situ implementation are complementary methodologies to conduct experiments in this context. In this paper, we address the problem of defining and controlling the heterogeneity of a platform. We evaluate the proposed solution, called Wrekavoc, with micro-benchmark and by implementing algorithms of the literature.",
SCCS: A Scalable Clustered Camera System for Multiple Object Tracking Communicating Via Message Passing Interface,"We introduce the scalable clustered camera system, a peer-to-peer multi-camera system for multi-object tracking, where different CPUs are used to process inputs from distinct cameras. Instead of transferring control of tracking jobs from one camera to another, each camera in our system performs its own tracking and keeps its own tracks for each target object, thus providing fault tolerance. A fast and robust tracking method is proposed to perform tracking on each camera view, while maintaining consistent labeling. In addition, we introduce a new communication protocol, where the decisions about when and with whom to communicate are made such that frequency and size of transmitted messages are minimized. This protocol incorporates variable synchronization capabilities, so as to allow flexibility with accuracy tradeoffs. We discuss our implementation, consisting of a parallel computing cluster, with communication between the cameras performed by MPI. We present experimental results which demonstrate the success of the proposed peer-to-peer multi-camera tracking system, with accuracy of 95% for a high frequency of synchronization, as well as a worst-case of 15 frames of latency in recovering correct labels at low synchronization frequencies",
An Integer Wavelet Based Multiple Logo-watermarking Scheme,"An integer wavelet based multiple logo-watermarking scheme for copyright protection of digital image is presented. A visual meaningful binary logo is used as watermark. The process of watermark embedding is carried out by transforming the host image in the integer wavelet domain. To construct a blind watermarking scheme, wavelet coefficients of HH and LL bands are modified depending on the watermark bits. To add the security, permutation is used to preprocess the watermark. From the experimental results it can be observed that proposed method is robust to a wide variety of attacks. Comparison with the existing methods shows the superiority of the proposed method",
An Integrated Approach for Processor Allocation and Scheduling of Mixed-Parallel Applications,"Computationally complex applications can often be viewed as a collection of coarse-grained data-parallel tasks with precedence constraints. Researchers have shown that combining task and data parallelism (mixed parallelism) can be an effective approach for executing these applications, as compared to pure task or data parallelism. In this paper, we present an approach to determine the appropriate mix of task and data parallelism, i.e., the set of tasks that should be run concurrently and the number of processors to be allocated to each task. An iterative algorithm is proposed that couples processor allocation and scheduling of mixed-parallel applications on compute clusters so as to minimize the parallel completion time (makespan). Our algorithm iteratively reduces the makespan by increasing the degree of data parallelism of tasks on the critical path that have good scalability and a low degree of potential task parallelism. The approach employs a look-ahead technique to escape local minima and uses priority based backfill scheduling to efficiently schedule the parallel tasks onto processors. Evaluation using benchmark task graphs derived from real applications as well as synthetic graphs shows that our algorithm consistently performs better than CPR and CPA, two previously proposed scheduling schemes, as well as pure task and data parallelism","Processor scheduling,
Parallel processing,
Application software,
Scheduling algorithm,
Iterative algorithms,
Computer applications,
Computer science,
Biomedical engineering,
Data engineering,
Biomedical informatics"
Cooperative checkpointing theory,"Cooperative checkpointing uses global knowledge of the state and health of the machine to improve performance and reliability by dynamically deciding when to skip checkpoint requests made by applications. Using results from cooperative checkpointing theory, this paper proves that periodic checkpointing is not expected to be competitive with the offline optimal. By leveraging probabilistic information about the future, cooperative checkpointing gives flexible algorithms that are optimally competitive. The results prove that simulating periodic checkpointing; by performing only every dth checkpoint, is not competitive with the offline optimal in the worst case; a simple modification gives a provably competitive algorithm. Calculations using failure traces from a prototype of IBM's Blue Gene/L show an application using cooperative checkpointing may make progress 4 times faster than one using periodic checkpointing, under realistic conditions. We contribute an approach to providing large-scale system reliability through cooperative checkpointing and techniques for analyzing the approach",
Time-frequency evaluation of segmentation methods for neonatal EEG signals,"In order to analyse non-stationary signals, like neonatal EEG, it is sometimes easier to segment signals into pseudo-stationary segments. An evaluation was performed on three previously proposed EEG segmentation methods in order to determine which method is most suited to neonatal EEG analysis. The three methods evaluated are spectral error measurement (SEM), generalised likelihood ratio (GLR) and non-linear energy operator (NLEO). A windowed version of NLEO was also tested in an attempt to minimise the effect of any temporary transients on the segmentation algorithm. The results from the segmentation algorithm were compared with the time-frequency distribution of the original signal to determine the appropriateness of the segments. It was found that GLR was the most appropriate segmentation method, and that the windowed version of the NLEO method performed better than the non-windowed version, both of which are less computationally expensive than the other methods",
Pilot Trial Results from a Virtual Reality System Designed to Enhance Recovery of Skilled Arm and Hand Movements after Stroke,"Rehabilitation programs designed to develop skill in upper extremity (UE) function after stroke require learner-centered opportunities for active problem solving. Virtual realty (VR) provides a unique environment where the presentation of stimuli can be systematically controlled to enable an optimal level of challenge by progressing task difficulty as performance improves. We describe four VR tasks that were developed and tested to improve skilled arm and hand movements in individuals with hemiparesis. Two participants post-stroke with different levels of motor severity attended 12 training sessions lasting 1 to 2 hours each over a 3-week period. Behavioral measures and questionnaires were administered pre-, mid-, and post-training. The less impaired participant averaged more time on task, practiced a greater number of blocks per session, and progressed at a faster rate over sessions than the more impaired participant. Differences in functional outcomes for these two cases can be explained in part by which tasks were practiced, the level of task difficulty applied during practice, and the amount of repetition included in practice","Virtual reality,
Medical treatment,
Extremities,
Problem-solving,
Control systems,
Testing,
Optimal control,
Helium,
Neuroplasticity,
Computer science"
Nonlinear state-dependent Riccati equation control of a quadrotor UAV,"Small quadrotor UAVs represent a very interesting class of small flying robots because of their ability to fly in- and outdoor. Therefore, these vehicles have an enormous potential for near-area surveillance and exploration. However, especially indoor flight is a difficult task for vehicle control which has to stabilize the desired velocity vector and the required attitude of the quadrotor. This paper mainly describes the development of such a nonlinear vehicle control system based on state-dependent Riccati equations (SDRE). The controller is integrated in an overall mission system concept for UAVs.",
Workload-Aware Dual-Speed Dynamic Voltage Scaling,"Dynamic voltage scaling (DVS) is a frequently used technique in mobile and embedded systems, aimed at reducing the energy consumption of mobile processors. In systems with a discrete number of frequency levels, existing dual-speed DVS approaches compute an optimal theoretical CPU speed and approximate it by choosing the two neighboring discrete speed levels. By comparing experimentally the energy savings attained with different frequency combinations on a mobile platform, this work shows that choosing the two neighboring frequency levels does not necessarily yield the highest energy savings. As a result of the above observation, this work introduces an online approach to dual-speed DVS that a) formulates a model for speed selection based on the workload characteristics of the current task set, b) computes a frequency pair that yields the best possible energy savings for a given taskset and workload","Dynamic voltage scaling,
Frequency,
Voltage control,
Mobile computing,
Energy consumption,
Runtime,
Embedded computing,
Computer science,
Power engineering and energy,
Embedded system"
Electric Load Forecasting Using Support Vector Machines Optimized by Genetic Algorithm,Electric load forecasting has become an important research area for secure operation and management of the modern power systems. In this paper we have proposed a seven support vector machines model for daily peak load demand long range forecasting. One support vector machine for each day of the week is trained on the past data and then used for the forecasting. In tuning process of support vector machines there are few parameters to optimize. We have used genetic algorithm for optimization of these parameters. The proposed model is evaluated on the electric load data used in EUNITE load competition in 2001 arranged by East-Slovakia Power Distribution Company. A better result is found as compare to best result found in the competition.,"Load forecasting,
Support vector machines,
Genetic algorithms,
Support vector machine classification,
Power system management,
Artificial neural networks,
Autoregressive processes,
Power system reliability,
Artificial intelligence,
Energy management"
Collective optimization over average quantities,"This paper addresses the design of algorithms for the collective optimization of a cost function defined over average quantities in the presence of limited communication. We argue that several meaningful collective optimization problems can be formulated in this way. As an application of the proposed approach, we propose a novel algorithm that achieves synchronization or balancing in phase models of coupled oscillators under mild connectedness assumptions on the (possibly time-varying and unidirectional) communication graphs",
Learning Object Shape: From Drawings to Images,"We consider the important challenge of recognizing a variety of deformable object classes in images. Of fundamental importance and particular difficulty in this setting is the problem of ""outlining"" an object, rather than simply deciding on its presence or absence. A major obstacle in learning a model that will allow us to address this task is the need for hand-segmented training images. In this paper we present a novel landmark-based, piecewise-linear model of the shape of an object class. We then formulate a learning approach that allows us to learn this model with minimal user supervision. We circumvent the need for hand-segmentation by transferring the shape ""essence"" of an object from drawings to complex images. We show that our method is able to automatically and effectively learn and localize a variety of object classes.",
A Virtual Rhomb Grid-Based Movement-Assisted Sensor Deployment Algorithm in Wireless Sensor Networks,"Sensor deployment, to a large extent, affects the performance and effectiveness of wireless sensor networks (WSN). For the conservation of energy, selecting only a subset of nodes is active and others are sleep. The problem is represented by the connected dominating set (CDS) of the graph, and the minimum connected dominating set (MCDS) is NP-hard for arbitrary graphs. The deployment is either deterministic or self-organizing. By analyzing of the relationship between both of the deployment, a virtual rhomb grid-based movement-assisted sensor deployment algorithm (VRGMSD) is proposed, which integrates both deterministic and self-organizing deployment in a unified framework. The VRGMSD algorithm (1) can form a MCDS; (2) ensures that there is no ""holes"" in the sensor field; (3) can select different k (the degree of coverage or connectivity) according to the demand on different applications. This flexibility allows the network to self-configure for a wide range of applications","Wireless sensor networks,
Computer networks,
Sleep,
Application software,
Monitoring,
Force sensors,
Educational institutions,
Grid computing,
Power engineering and energy,
Algorithm design and analysis"
Interactive Web Information Retrieval Using WordBars,"It is common for Web searchers to have difficulties crafting queries to fulfil their information needs. Even when they provide a good query, users often find it challenging to evaluate the results of their Web searches. Sources of these problems include the lack of support for query refinement, and the static nature of the list-based representations of Web search results. To address these issues, we have developed WordBars, an interactive tool for Web information retrieval. WordBars visually represents the frequencies of the terms found in the first 100 document surrogates returned from the initial query. This system allows the users to interactively re-sort the search results based on the frequencies of the selected terms within the document surrogates, as well as to add and remove terms from the query, generating a new set of search results. Examples illustrate how WordBars can provide valuable support for query refinement and search results exploration, both when specific and vague initial queries are provided",
dRamDisk: efficient RAM sharing on a commodity cluster,"Recent work on distributed RAM sharing has largely focused on leveraging low-latency networking technologies to optimize remote memory access. In contrast, we revisit the idea of RAM sharing on a commodity cluster with an emphasis on the prevalent gigabit Ethernet technology. The main point of the paper is to present a practical solution-a distributed RAM disk (dRamDisk) with an adaptive read-ahead scheme-which demonstrates that spare RAM capacity can greatly benefit I/O-constrained applications. Specifically, our experiments show that sequential read/write operations can be sped up approximately 3.5 times relative to a commodity hard drive and that, for more random access patterns, such as the ones experienced on a server, the speedup can be much higher. Our experiments demonstrate that this speedup is approximately 90% of what is practically achievable for the tested system","Read-write memory,
Random access memory,
Digital forensics,
Performance gain,
Computer science,
Drives,
Ethernet networks,
System testing,
Central Processing Unit,
Computer aided instruction"
Application of Componential IRT Model for Diagnostic Test in a Standard-Conformant eLearning System,"Diagnostic test plays an important role in personalized elearning by providing information about cognitive levels of students' learning states. While many diagnosis algorithms have been proposed, most of them lack a solid theory base. On the other hand, item response theory (IRT) is a widely-accepted test theory and has been shown very effective in estimating a learner's latent ability. However, it did not tell much about conceptual cognitive states. This paper proposes an extension of IRT model for diagnostic test by extending it into a componential IRT model. This diagnostic test feature has been added into a standard-conformant elearning system, called IDEAL, where a model-based diagnosis and remediation architecture is implemented. Preliminary results show that the proposed approach is effective","System testing,
Electronic learning,
Psychometric testing,
Solids,
Application software,
Computer science,
State estimation,
Ontologies,
Materials testing,
Conducting materials"
Evolutionary Testing Using an Extended Chaining Approach,"Fitness functions derived from certain types of white-box test goals can be inadequate for evolutionary software test data generation (Evolutionary Testing), due to a lack of search guidance to the required test data. Often this is because the fitness function does not take into account data dependencies within the program under test, and the fact that certain program statements may need to have been executed prior to the target structure in order for it to be feasible. This paper proposes a solution to this problem by hybridizing Evolutionary Testing with an extended Chaining Approach. The Chaining Approach is a method which identifies statements on which the target structure is data dependent, and incrementally develops chains of dependencies in an event sequence. By incorporating this facility into Evolutionary Testing, and by performing a test data search for each generated event sequence, the search can be directed into potentially promising, unexplored areas of the test object's input domain. Results presented in the paper show that test data can be found for a number of test goals with this hybrid approach that could not be found by using the original Evolutionary Testing approach alone. One such test goal is drawn from code found in the publicly available libpng library.",
Security Requirements Engineering for Software Systems: Case Studies in Support of Software Engineering Education,"Software engineering curricula too often neglect the development of security requirements for software systems. As a consequence, programmers often produce buggy code with weak security measures. This report focuses on three case studies in which graduate students applied a novel security requirements engineering methodology to real-world software development projects. The experiences showed promise for curriculum integration in educating students about the importance of security requirements in software engineering, as well as how to develop such requirements",
A Design Tool to Reason about Ambient Assisted Living Systems,"This paper proposes a design tool to investigate the properties and emergent behaviours of a special class of ambient assisted living systems, namely mutual assistance communities where the dwellers contribute to each other's well being. Purpose of our system is to understand how mutual assistance communities work, what consequences a design decision could ultimately bring about, and how to construct care communities providing timely and cost-effective service for elderly and disabled people. We prove that mutual assistance between dwellers can provide care in time, and decrease the requirement for professional medical service. The simulation results show that with the existing rules most of the requirements for help can be solved or promptly initiated inside the community before their members resort to external professionals",
Harnessing Disruptive Innovation in Formal Verification,"Technological innovations are sweeping through the field of formal verification. These changes are disruptive to tools based on interactive theorem proving, which needs new ways to integrate the capabilities of novel technologies. I describe two approaches. One is development and use of SMT solvers: these use techniques from theorem proving but apply them in ways that enable model checking, while also supporting highly automated theorem proving. The other is a proposal for an evidential tool bus: a loosely coupled architecture that allows many different verification components to collaborate to solve problems beyond the capability of any single component","Technological innovation,
Formal verification,
Concrete,
Data structures,
Refining,
Computer science,
Laboratories,
Surface-mount technology,
Computer architecture,
Collaborative tools"
"""GeoAnalytics"" - Exploring spatio-temporal and multivariate data","The voluminous nature of social scientific, spatial-temporal statistical databases calls for high interactive performance and creative integrated information and geo-visualization tools. A solution to this challenge can be found in the emerging visual analytics (VA), a science of analytical reasoning facilitated by interactive visual interfaces and innovative visualization and is now actively pursued by research groups worldwide. In this paper, we present a tool called ""GeoAnalytics"", based on the principles behind VA. Our objective is to define new suitable approaches and tools for exploring time variant and multivariate attributes simultaneous including a spatial dimension. We introduce parallel coordinates integrated with time series and trend graph that serves as the visual control panel for the application. Multivariate attribute dynamic queries can express simultaneously queries involving time varying spatial data. VA encourages the need to build a bridge between the advantages of both human perception and computer science technologies. The sense of immediacy and speed-of-thought interaction is achieved in our dynamically linked components and maximum allocation of screen area for visual displays that helps users stay focused on their work and shortens their time to enlightenment",
Off-lineWriter Identification Using Gaussian Mixture Models,"Writer identification is the task of determining the author of a sample handwriting from a set of writers. In this paper, we propose Gaussian mixture models (GMMs) to address the task of off-line, text independent writer identification of text lines. The resulting system is compared to a system that uses a hidden Markov model (HMM) based approach. While the GMM based system is conceptually much simpler and faster to train than the HMM based system, it achieves a significantly higher writer identification rate of 98.46% on a data set of 4,103 text lines coming from 100 writers",
Feasibility of multi-protocol attacks,"Formal modeling and verification of security protocols typically assumes that a protocol is executed in isolation, without other protocols sharing the network. We investigate the existence of multi-protocol attacks on protocols described in literature. Given two or more protocols, that share key structures and are executed in the same environment, are new attacks possible? Out of 30 protocols from literature, we find that 23 are vulnerable to multi-protocol attacks. We identify two likely attack patterns and sketch a tagging scheme to prevent multi-protocol attacks.",
Extended Protection against Stack Smashing Attacks without Performance Loss,In this paper we present an efficient countermeasure against stack smashing attacks. Our countermeasure does not rely on secret values (such as canaries) and protects against attacks that are not addressed by state-of-the-art countermeasures. Our technique splits the standard stack into multiple stacks. The allocation of data types to one of the stacks is based on the chances that a specific data element is either a target of attacks and/or an attack vector. We have implemented our solution in a C-compiler for Linux. The evaluation shows that the overhead of using our countermeasure is negligible,"Protection,
Performance loss,
Buffer overflow,
Computer science,
Linux,
Data security,
NIST,
Databases,
Libraries,
Operating systems"
Topographic Product Models Applied to Natural Scene Statistics,"We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to “natural” data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous work—in particular, gaussian scale mixture models and variants of independent components analysis.",
Managing Large-Scale Scientific Workflows in Distributed Environments: Experiences and Challenges,"In this paper we discuss several challenges associated scientific workflow design and management in distributed, heterogeneous environments. Based on our prior work with a number of scientific applications, we describe the workflow lifecycle and examine our experiences and the challenges ahead as they pertain to the user experience, planning the workflow execution and managing the execution itself.","Environmental management,
Large-scale systems,
Catalogs,
Gas insulated transmission lines,
Process planning,
Collaborative work,
Vehicles,
Libraries,
Computer networks,
Grid computing"
Multigrid Methods on Adaptively Refined Grids,"Using multigrid solvers in the adaptive finite element method yields a powerful tool for solving large-scale partial differential equations that exhibit localized features such as singularities or shocks. In addition to describing the basic method and related theory, this article numerically demonstrates the method's performance and utility on 2D and 3D problems","Multigrid methods,
Mesh generation,
Partial differential equations,
Finite element methods,
Piecewise linear approximation,
Space technology,
Testing,
Space heating,
Boundary conditions,
Grid computing"
Situations in Conceptual Modeling of Context,"In previous work, we have defined conceptual foundations that can be beneficially used in context modeling. These conceptual foundations include the separation of entity and context, and the characterization of context as either Intrinsic or Relational. This paper aims at extending this approach by introducing the ontological concept of Situation as means of composing the elements of our ontology (entities, intrinsic and relational contexts) to model particular states of affairs of interest. Our concepts have been inspired by and aligned with conceptual theories from the fields of philosophy and cognitive sciences.","Context modeling,
Ontologies,
Mobile handsets,
Context-aware services,
Telematics,
Information technology,
Computer science,
Laboratories,
Influenza,
Technological innovation"
Effects of JPEG2000 data compression on an automated system for detecting clustered microcalcifications in digital mammograms,"The functionalities of the JPEG2000 standard have led to its incorporation into digital imaging and communications in medicine (DICOM), which makes this compression method available for medical systems. In this study, we evaluated the compression of mammographic images with JPEG2000 (16:1, 20:1, 40:1, 60.4:1, 80:1, and 106:1) for applications with a computer-aided detection (CAD) system for clusters of microcalcifications. Jackknife free-response receiver operating characteristic (JAFROC) analysis indicated that differences in the detection of clusters of microcalcifications were not statistically significant for uncompressed versus 16:1 (T=-0.7780;p=0.4370),20:1(T=1.0361;p=0.3007), and 40:1 (T=1.6966;p=0.0904); and statistically significant for uncompressed versus 60.4:1 (T=5.8883;p<0.008), 80:1 (T=7.8414;p<0.008), and 106:1 (T=17.5034;p=<0.008). Although there is a small difference in peak signal-to-noise ratio (PSNR) between compression ratios, the true-positive (TP) and false-positive (FP) rates, and the free-response receiver operating characteristic (FROC), figure of merit values considerably decreased from a 60:1 compression ratio. The performance of the CAD system is significantly reduced when using images compressed at ratios greater than 40:1 with JPEG2000 compared to uncompressed images. Mammographic images compressed up to 20:1 provide a percentage of correct detections by our CAD system similar to uncompressed images, regardless of the characteristics of the cluster. Further investigation is required to determine how JPEG2000 affects the detectability of clusters of microcalcifications as a function of their characteristics",
Characterization of Mouse Brain and Its Development using Diffusion Tensor Imaging and Computational Techniques,"Diffusion tensor magnetic resonance imaging (DTI) was used to study mouse brain development from early embryonic stage to adult. DTI provides necessary resolution and superb white matter and gray matter contrast in embryonic and neonatal brains for characterization of morphological changes during mouse brain development. A database and a digital atlas of developing mouse brains based on our DTI results are being constructed. To characterize the spatial and temporal patterns of mouse brain development, we applied landmark based computational techniques to analyze the database",
Services and Policies for Care At Home,"It is argued that various factors including the increasingly ageing population will require more care services to be delivered to users in their own homes. Desirable characteristics of such services are outlined. The Open Services Gateway initiative has been adopted as a widely accepted framework that is particularly suitable for developing home care services. Service discovery in this context is enhanced through ontologies that achieve greater flexibility and precision in service description. A service ontology stack allows common concepts to be extended for new services. The architecture of a policy system for home care is explained. This is used for flexible creation and control of new services. The core policy language and its extension for home care are introduced, and illustrated through typical examples. Future extensions of the approach are discussed.","Ontologies,
Aging,
Computer networks,
Distributed computing,
Home computing,
Mathematics,
Computer network management,
Service oriented architecture,
Context-aware services,
Government"
On the Importance of Asymmetries in Grasp Quality Metrics for Tendon Driven Hands,"Grasp quality measures are important for understanding how to plan for and maintain appropriate and secure grasps for pick and place operations and tool use. Most grasp quality measures assume certain symmetries about the mechanism or the task. For example, contact points may be considered to be independent and identical, or an ellipsoidal measure such as the force manipulability ellipsoid may be used. However, many tasks have strong asymmetries, where wrenches in certain directions dominate. Tendon driven hand designs may also have strong asymmetries, leading to differing abilities to apply contact forces in different directions. This paper begins to explore empirically the validity of some of the symmetry assumptions employed by common grasp quality metrics. We examine the human hand and the shadow robot hand, and find that force abilities vary with finger choice and with location of the contact on the finger for both hands. However, while the human hand shows dramatic changes for different poses due to its asymmetric design, the shadow hand, with a symmetric design shows much smaller changes and resembles the assumption of identical and independent contact points reasonably well. Thus, we suggest that the underlying design of the hand is a very important factor to consider for grasp quality metrics and for grasp planning and control. The specific grasp quality metric we study in this paper also brings together a variety of previous research. We outline a linear programming approach for computing a grasp quality metric that includes tendon force constraints and contact constraints and can handle any task described as a polytope in wrench space","Tendons,
Force measurement,
Humans,
Torque,
Extraterrestrial measurements,
Intelligent robots,
USA Councils,
Ellipsoids,
Fingers,
Linear programming"
A Novel Algorithm for Scrambling Digital Image Based on Cat Chaotic Mapping,"This paper proposes a new algorithm for scrambling digital image based on the cat chaotic mapping. We disorder the pixel coordinates of the digital image through using a cat chaotic mapping, and then perform exclusive OR operation between certain pixel value of the digital image and a chaotic value. The experiment results have shown that the new diffusion technique proposed in this paper is very effective to uniform the statistical characteristics of the encrypted graph, and the efficiency of this method is very high.","Digital images,
Chaos,
Cryptography,
Pixel,
Signal processing algorithms,
Computer science,
Image restoration,
Discrete transforms,
Resists,
Signal processing"
A Dangerous Location Aware System for Assisting Kids Safety Care,"This system focuses on a location-aware computing application and environment which provide preemptive information on dangerous locations in the kid's surroundings; i.e., the system alerts a kid on the move of the possible dangers whenever he/she is near accident prone areas. Using satellite-based GPS position sensing, this system can gradually learn and remember about the different locations that a kid routinely visits and/or passes by in his daily life, based on space-oriented contexts. The full set of system functionalities, including the map display, requires a graphical user interface. This location based services (LBSs) can compute the serious situations and alert kids to avoid dangerous situations. Our research can simulate the virtual environment as hyperspaces for kids safety care",
Distributed coloring in O/spl tilde/(/spl radic/(log n)) bit rounds,"We consider the well-known vertex coloring problem: given a graph G, find a coloring of the vertices so that no two neighbors in G have the same color. It is trivial to see that every graph of maximum degree Delta can be colored with Delta + 1 colors, and distributed algorithms that find a (Delta + 1)-coloring in a logarithmic number of communication rounds, with high probability, are known since more than a decade. This is in general the best possible if only a constant number of bits can be sent along every edge in each round. In fact, we show that for the n-node cycle the bit complexity of the coloring problem is Omega(log n). More precisely, if only one bit can be sent along each edge in a round, then every distributed coloring algorithm (i.e., algorithms in which every node has the same initial state and initially only knows its own edges) needs at least Omega(log n) rounds, with high probability, to color the cycle, for any finite number of colors. But what if the edges have orientations, i.e., the end-points of an edge agree on its orientation (while bits may still flow in both directions)? Does this allow one to provide faster coloring algorithms? Interestingly, for the cycle in which all edges have the same orientation, we show that a simple randomized algorithm can achieve a 3-coloring with only O(radic(log n)) rounds of bit transmissions, with high probability (w.h.p.). This result is tight because we also show that the bit complexity of coloring an oriented cycle is Omega(radic(log n)), with high probability, no matter how many colors are allowed. The 3-coloring algorithm can be easily extended to provide a (Delta + 1)-coloring for all graphs of maximum degree Delta in O(radic(log n)) rounds of bit transmissions, w.h.p., if Delta is a constant, the edges are oriented, and the graph does not contain an oriented cycle of length less than radic(log n). Using more complex algorithms, we show how to obtain an O(Delta)-coloring for arbitrary oriented graphs of maximum degree Delta using essentially O(log Deltaradic(log n)) rounds of bit transmissions, w.h.p., provided that the graph does not contain an oriented cycle of length less than radic(log n)","Computer science,
Distributed computing,
Distributed algorithms,
Wireless networks,
Contracts,
Large-scale systems,
Information systems,
Processor scheduling,
Resource management,
Routing"
Improving Recognition of Novel Input with Similarity,"Many sources of information relevant to computer vision and machine learning tasks are often underused. One example is the similarity between the elements from a novel source, such as a speaker, writer, or printed font. By comparing instances emitted by a source, we help ensure that similar instances are given the same label. Previous approaches have clustered instances prior to recognition. We propose a probabilistic framework that unifies similarity with prior identity and contextual information. By fusing information sources in a single model, we eliminate unrecoverable errors that result from processing the information in separate stages and improve overall accuracy. The framework also naturally integrates dissimilarity information, which has previously been ignored. We demonstrate with an application in printed character recognition from images of signs in natural scenes.","Character recognition,
Computer vision,
Information resources,
Machine learning,
Application software,
Text analysis,
Optical character recognition software,
Labeling,
Computer science,
Layout"
Effects of Power Lines on Performance of Home Control System,"Home control system (HCS) helps to monitor and control the home appliances as well as security aspects of the digital home that is expected to be the standard for the future home. Chiefly, HCS is an integration of home appliance control systems (HACS) and home security systems (HSS). HACS enables the home-owner to control appliances such as stove, refrigerator, air-conditioner, and the like, remotely, while the HSS helps to monitor the status of various networked security devices in the home and control certain aspects of the devices. Monitoring and control may be done by a personal digital device such as a laptop, PDA, telephone, or even a cell phone. One of the technologies widely used by HCS to connect the home controller with the appliances, equipments, and devices, is the X10 protocol that uses power lines for data transmission. In this paper we analyze the performance of power lines for HCS and suggest recommendations that will help increase the performance of HCS.","Control systems,
Home appliances,
Remote monitoring,
Power system security,
Refrigeration,
Portable computers,
Telephony,
Cellular phones,
Protocols,
Data communication"
Slack reclamation for real-time task scheduling over dynamic voltage scaling multiprocessors,"In the past decades, a number of research results have been reported for energy-efficient task scheduling over uniprocessor and multiprocessor environments. While researchers have started the exploring of slack reclaiming for tasks during run time, little work has been done for multiprocessor cases. This paper proposes a set of multiprocessor energy-efficient task scheduling algorithms with different task remapping and slack reclaiming schemes, where tasks have the same arrival time and share a common deadline. Tasks are reassigned to processors dynamically, and the slack time is reclaimed to slow down the execution speeds of the remaining tasks for energy efficiency. Extensive simulations were performed to provide insights. The energy consumption could be reduced up to 29% in the experiments, compared to the previous work","Dynamic scheduling,
Dynamic voltage scaling,
Processor scheduling,
Energy efficiency,
Energy consumption,
Scheduling algorithm,
Computer science,
Power engineering and energy,
Circuit simulation,
Very large scale integration"
Incorruptible system self-cleansing for intrusion tolerance,"Despite the increased focus on security, critical information systems remain vulnerable to cyber attacks. The problem stems in large part from the constant innovation and evolution of attack techniques. The trend leads importance to the concept of intrusion tolerance a critical system must fend off or at least limit the damage caused by unknown and/or undetected attacks. In prior work, we developed a self-cleansing intrusion tolerance (SCIT) architecture that achieves the above goal by constantly cleansing the servers and rotating the rule of individual servers. In this paper, we show that, with simple hardware enhancements strategically placed in a SCIT system, incorruptible intrusion containment can be realized. We then present an incorruptible SCIT design for use by one of the most critical infrastructures of the Internet, the domain name services. It is our belief that incorruptible intrusion containment as presented here constitutes a new, effective layer of system defense for critical information system","Computer science,
Information systems,
Intrusion detection,
Face detection,
Web server,
Hardware,
Switches"
WLC29-1: Addressing the Link Adaptation Problem for VoWLAN using Codec Adaptation,"In this paper a scheme is proposed to deal with congestion problems that can arise due to Link Adaptation (LA) in Wireless Local Area Networks (WLANs) in the presence of VoIP traffic. The proposed scheme operates by first determining if LA has resulted in the system becoming congested. If not, no further action is taken. However, if, LA has resulted in an overloaded system, the voice codec of the handset which has undergone LA is adapted so as to restore the system to its earlier state, thereby alleviating the congestion. This is achieved by specifically adapting the codec so as to maintain approximately the same level of medium usage as was used prior to the LA. The proposed scheme was evaluated on an experimental test-bed and results show that the codec adaptation was very effective at overcoming the problem of LA.",
"Detecting cheaters for multiplayer games: theory, design and implementation[1]",,"Game theory,
Computer hacking,
Graphics,
Prototypes,
Internet,
Computer science,
Computer industry,
Toy industry,
Investments,
Rendering (computer graphics)"
From sequential programs to concurrent threads,"Chip multiprocessors are of increasing importance due to difficulties in achieving higher clock frequencies in uniprocessors, but their success depends on finding useful work for the processor cores. This paper addresses this challenge by presenting a simple compiler approach that extracts non-speculative thread-level parallelism from sequential codes. We present initial results from this technique targeting a validated dual-core processor model, achieving speedups ranging from 9-48% with an average of 25% for important benchmark loops over their single-threaded versions. We also identify important next steps found during our pursuit of higher degrees of automatic threading",
Implementation of Listing's Law for a Tendon Driven Robot Eye,"This paper presents a model for a tendon driven robot eye designed to emulate the actual saccadic and smooth pursuit movements performed by human eyes. Physiological saccadic motions obey the so called Listing's law which constrains the admissible eye's angular velocities. The paper discusses conditions making possible to implement the Listing's law on a purely mechanical basis, i.e. without active control",
Sensor Aggregation and Integration in Healthcare Location Based Services,"Complex and dynamic working environments such as health care facilities consist of staff, patients and equipment constantly moving in response to changing medical requirements. Knowing the current location of people and equipment is essential for the smooth running of a facility, yet creating a global view through tracking is a challenging task. It is clear that many common hospital situations can be improved with real-time access to the various actors' location information. One of the main problems with implementing such services is that current location based applications tend to be proprietary and the data generated closed. The realisation of ubiquitous location based services demands the exploration of hybrid models and methods that can utilise existing and subsequent infrastructures in novel and complimentary ways. We describe a number of hospital scenarios that use location-based services and make available all the location data gathered. We propose that by aggregating location data by a range of acquisition methods it is possible to improve the performance of location applications and readily adapt to the introduction of new location detection technologies","Medical services,
Hospitals,
Sensor systems,
Delay,
Space technology,
Computer science,
Biomedical informatics,
User centered design,
Aerodynamics,
Medical treatment"
Exploiting Multi-Channel Clustering for Power Efficiency in Sensor Networks,"Sensor networks typically comprise of a number of inexpensive small devices with processing, communication and sensing abilities that collaborate to perform a common task. Sensor devices use batteries as their sole power supply. The operational lifetime of a sensor network, therefore, depends entirely on the better utilization of the devices. Typically a sensor network is divided into clusters to optimize power utilization by performing division of labor and data aggregation within a cluster. This paper introduces a novel approach to naturally distributed clustering of sensor nodes in a sensor net using multi channel data planes. Our technique incorporates a virtual sense mechanism that reduces energy spent in sampling and transmission. It also decreases network traffic, thereby decreasing contention, potential collisions and retransmissions. This approach inherently implements a sleep-awake mechanism based on virtual sensing that contributes towards increasing the network lifetime by efficient utilization. The proposed technique can be used to track spreading phenomenon like forest fires and water flows. A spreading phenomenon can be represented by a field whose value changes dynamically with time over area. We focus on following the movement of such a dynamically changing field rather than obtaining the value of the field at different locations at disjoint random times",
Defending Against TCP SYN Flooding Attacks Under Different Types of IP Spoofing,"TCP-based flooding attacks are a common form of Distributed Denial-of-Service (DDoS) attacks which abuse network resources and can bring about serious threats to the Internet. Incorporating IP spoofing makes it even more di..cult to defend against such attacks. Among di..erent IP spoofing techniques, which include random spoofing, subnet spoofing and fixed spoofing, subnet spoofing is the most di..cult type to fight against. In this paper, we propose a simple and e..cient method to detect and defend against TCP SYN flooding attacks under di..erent IP spoofing types, including subnet spoofing. The method makes use of a storage-e..cient data structure and a change-point detection method to distinguish complete three-way TCP handshakes from incomplete ones. Simulation experiments consistently show that our method is both efficient and e..ective in defending against TCP-based flooding attacks under di..erent IP spoofing types.",
Assessing Vulnerabilities in Apache and IIS HTTP Servers,"We examine the feasibility of quantitatively characterizing the vulnerabilities in the two major HTTP servers. In particular, we investigate the applicability of quantitative empirical models to the vulnerabilities discovery process for these servers. Such models can allow us to predict the number of vulnerabilities that may potentially be present in a server but may not yet have been found. The data on vulnerabilities found in the two servers is mined and analyzed. We explore the applicability of a time-based and an effort-based vulnerability discovery model. The effort-based model requires data of the current market-share of a server. Both models have been successfully used for vulnerabilities in the major operating systems. Our results show that both vulnerabilities discovery models fit the data for the HTTP servers well. We also examine a separate classification schemes for server vulnerabilities that based on the source of error, and then explore the applicability of the quantitative methods to individual classes",
Chinese Spoken Document Summarization Using Probabilistic Latent Topical Information,"The purpose of extractive summarization is to automatically select a number of indicative sentences, passages, or paragraphs from the original document according to a target summarization ratio and then sequence them to form a concise summary. In the paper, we proposed the use of probabilistic latent topical information for extractive summarization of spoken documents. Various kinds of modeling structures and learning approaches were extensively investigated. In addition, the summarization capabilities were verified by comparison with the conventional vector space model and latent semantic indexing model, as well as the HMM model. The experiments were performed on the Chinese broadcast news collected in Taiwan. Noticeable performance gains were obtained","Hidden Markov models,
Speech,
Indexing,
Data mining,
Computer science,
Performance gain,
Wireless communication,
Man machine systems,
Digital multimedia broadcasting,
Multimedia communication"
Detection and Localization in Sensor Networks Using Distributed FDR,"Wireless sensor networks (SNET) have gained substantial interest for detection and localization of objects, however network energy constraints make it difficult to implement optimal solutions in distributed settings. For object localization with known power, maximum likelihood estimation is the optimal solution, however in many applications it involves performing optimization over a non-convex function, which can be hard to solve even in centralized schemes. Furthermore, the computational complexity of finding this solution is prohibitively large to be implemented in distributed SNETs under energy constraints. In this work we consider the problem of distributed detection and localization of an object that emits a signal with unknown power. Considering the energy constraints of SNETs, we propose a novel technique that makes use of the false discovery rate (FDR) procedure and a belief propagation (BP) like algorithm for detection and localization problems. Inclusion of FDR to the detection process limits the communications necessary to detect the presence of the object as well as the energy consumption to locate it, prolonging the network lifetime. The simulation studies show that this approach is very well suited for detection and localization problems where the signal power of the object decays rapidly with distance.","Object detection,
Wireless sensor networks,
Sensor fusion,
Computer networks,
Distributed computing,
Power engineering and energy,
Maximum likelihood estimation,
Computational complexity,
Belief propagation,
Power engineering computing"
SAD-Based Stereo Matching Circuit for FPGAs,This paper presents a novel FPGA-based stereo matching system. The proposed circuit operates on 512times512 stereo images with a maximum disparity of 255. It achieves a 286 MHz running frequency and a frame rate of 25.6 f/s.,
A New Internet Architecture for Robot Remote Control,"A new server-decentralized Internet architecture based on Jabber for robot remote control is proposed in order to have high availability, performance, scalability, fault tolerance and security. Four components of the architecture: operators, robots, transfer servers and data-keeper are defined and functioned. The robot-controlling data/robot state data are packed with XML stanzas and delivered to the addressed robot/operator through XML streams. In order to test its availability, the architecture is implemented and instanced as a remote control simulation system of Puma560 robot. The system is experimented and a network test is carried out to evaluate the structure; the results show that the architecture is suitable for many kinds of robot remote control scenarios despite the tough network environment","Internet,
Robot control,
Availability,
XML,
System testing,
Scalability,
Fault tolerance,
Data security,
Web server,
Control system synthesis"
Random Projections of Signal Manifolds,"Random projections have recently found a surprising niche in signal processing. The key revelation is that the relevant structure in a signal can be preserved when that signal is projected onto a small number of random basis functions. Recent work has exploited this fact under the rubric of compressed sensing (CS): signals that are sparse in some basis can be recovered from small numbers of random linear projections. In many cases, however, we may have a more specific low-dimensional model for signals in which the signal class forms a nonlinear manifold in RN. This paper provides preliminary theoretical and experimental evidence that manifold-based signal structure can be preserved using small numbers of random projections. The key theoretical motivation comes from Whitney's embedding theorem, which states that a K-dimensional manifold can be embedded in Ropf2K+1. We examine the potential applications of this fact. In particular, we consider the task of recovering a manifold-modeled signal from a small number of random projections. Thanks to our more specific model, we can recover certain signals using far fewer measurements than would be required using sparsity-driven CS techniques",
Interpretative Dynamics in Human Robot Interaction,"Current technologies often tend to emphasise utilitarian versions of work, entertainment, and consumer activity by embodying a representation of the privileged activities they support and the values predefined by the designer. Social robots offer an extraordinary opportunity to design technologies with open-ended possibilities for interaction and engagement with humans. In the paper, we present the results of a case study conducted in a nursing home with elderly people interacting with the seal robot Paro. The results of the study show that the robot actively supports our natural disposition to attribute intentional states to inanimate or artificial objects. In addition interesting interpretative dynamics developing in human robot interaction emerged in the study: mental compromised subjects alternate their assessment of the robot from an inanimate object to an agent, depending on the severity of their disease. However, the observation shows that also subjects talking about the robot as an inanimate object, continue to be emotionally and intellectually involved in the experience. In this respect, agentivity does not seem to be a key factor in assuring a pleasurable and intriguing interaction experience","Human robot interaction,
Cognitive robotics,
Service robots,
Orbital robotics,
Medical services,
Senior citizens,
Dementia,
Seals,
Diseases,
Computer interfaces"
Text Map Explorer: a Tool to Create and Explore Document Maps,"This paper presents a tool, called text map explorer, which can be used to create and explore document maps (visual representations of document collections). This tool is capable of grouping (and separating) documents by their contents, revealing to the user relationships amongst them. This paper also presents a novel multi-dimensional projection technique for text that reduces the quadratic time complexity of our previous approach to O(N3/2), keeping the same quality of maps. The technique creates a surface that reveals intrinsic patterns and supports various kinds of exploration of a text collection",
"Second-Generation, Tri-Modality Pre-Clinical Imaging System","A second-generation, tri-modality instrument, comprising PET, SPECT, and CT subsystems, for pre-clinical imaging is described. Significant improvements have been made in each of the modalities as well as the integrated computer system. The SPECT system uses up to four CZT detectors with multiple pinhole apertures for each head, thereby increasing count sensitivity. Geometric corrections for pinhole size and alignment combine with spiral acquisition to give high-resolution, whole body images. Energy resolution of less than 5% at 140 keV gives full separation of gamma-ray peaks for dual-isotope imaging and scatter rejection. Dual-isotope images (I-123 and Tc-99m) from the new CZT SPECT system have been acquired in a mouse tumor model. The CT imager is positioned on the same plane as the SPECT system, providing precise co-registration. Unique shutters protect the SPECT and PET sensors from overexposure during CT data collection. The custom-designed X-ray tube has been configured to allow variable magnification and focal spot, thereby giving resolution down to less than 15 microns. CT acquisition can be completed in less than one minute, and the display software has been enhanced to display clinically-relevant Hounsfield units. Improved production methods yield superior light output for the BGO crystals used in the PET ring, resulting in better signal-to-noise ratios and consequently lower detection limits. Additionally, new software tools allow a significantly improved work flow and analysis capability.","Computed tomography,
Positron emission tomography,
Displays,
Instruments,
Detectors,
Apertures,
Head,
Spirals,
Energy resolution,
High-resolution imaging"
Image Mapping and Visual Attention on a Sensory Ego-Sphere,"The sensory ego-sphere (SES) is a short-term memory for a robot in the form of an egocentric, tessellated, spherical, sensory-motor map of the robot's locale. This paper reports on: (1) the mapping to the SES of images with much higher resolution than the SES itself, and (2) the processing of visual attention on the SES. Described is a procedure to store spatially-overlapping imagery in the SES database and to generate and update a spherical composite of its visual contents. The composite image serves both as a visual map of the locale and as a representation of the local contents of the underlying full-resolution imagery. Visual attention enables fast alignment of overlapping images without warping or position optimization, since an attentional point (AP) on the composite typically corresponds to one on each of the collocated regions in the images. Such alignment speeds analysis of the multiple images of the area. Compositing and attention were performed two ways and compared: (1) APs were computed directly on the composite and not on the full resolution images until the time of retrieval. (2) The attentional operator was applied to all incoming imagery. Then the locations and values of the APs on the composite were computed as a function of those points within the corresponding regions of the collocated images. It was found that although the second method was slower, it produced consistent and, thereby, more useful APs","Robot sensing systems,
Cognitive robotics,
Intelligent robots,
Visual databases,
Image resolution,
Image databases,
Cognition,
Orbital robotics,
Image storage,
Spatial resolution"
SSR: Segment-by-Segment Routing in Large-Scale Mobile Ad Hoc Networks,"Location-based routing in mobile ad hoc networks (MANETs) does not need to use pre-computed routes for forwarding packets thus scales very well. However, location-based routing suffers from two major problems: hole-induced local optimum and mobility-induced location errors. To solve these problems, in this paper, we propose a segment-by-segment routing (SSR), which is a combination of location-based routing and topology-based routing. It maintains a k-hop vicinity routing table for each cluster head (CH), and uses location-based routing between neighboring k-hop vicinities while applies topology-based routing in the k-hop vicinity. The k-hop vicinity routing table provides useful reachability information used by an avoidance-based strategy to deal with holes, and helps to achieve the degree of tolerance of location inaccuracy with k-hop long radio transmissions. Comparative analysis shows that the proposed protocol outperforms the well-known GPSR routing protocol in terms of reliability, tolerance of location inaccuracy, and communication complexity, with a little larger cost in control messages",
Vertical Parallax from Moving Shadows,"This paper presents a method for capturing and computing 3D parallax. 3D parallax, as used here, refers to vertical offset from the ground plane, height. The method is based on analyzing shadows of vertical poles (e.g., a tall building’s contour) that sweep the object. Unlike existing beam-scanning approaches, such as shadow or structured light, that recover the distance of a point from the camera, our approach measures the height from the ground plane directly. Previous methods compute the distance from the camera using triangulation between rays outgoing from the light-source and the camera. Such a triangulation is difficult when the objects are far from the camera, and requires accurate knowledge of the light source position. In contrast, our approach intersects two (unknown) planes generated separately by two casting objects. This omits the need to precompute the location of the light source. Furthermore, it allows a moving light source to be used. The proposed setup is particularly useful when the camera cannot directly face the scene or when the object is far away from the camera. A good example is an urban scene captured by a single webcam.",
Applying Data Mining to Pseudo-Relevance Feedback for High Performance Text Retrieval,"In this paper, we investigate the use of data mining, in particular the text classification and co-training techniques, to identify more relevant passages based on a small set of labeled passages obtained from the blind feedback of a retrieval system. The data mining results are used to expand query terms and to re-estimate some of the parameters used in a probabilistic weighting function. We evaluate the data mining based feedback method on the TREC HARD data set. The results show that data mining can be successfully applied to improve the text retrieval performance. We report our experimental findings in detail.","Data mining,
Feedback,
Information retrieval,
Text categorization,
Training data,
Information technology,
Testing,
Computer science,
Labeling,
Supervised learning"
Standardized Evaluation of Haptic Rendering Systems,"The development and evaluation of haptic rendering algorithms presents two unique challenges. Firstly, the haptic information channel is fundamentally bidirectional, so the output of a haptic environment is fundamentally dependent on user input, which is difficult to reliably reproduce. Additionally, it is difficult to compare haptic results to real-world, “gold standard” results, since such a comparison requires applying identical inputs to real and virtual objects and measuring the resulting forces, which requires hardware that is not widely available. We have addressed these challenges by building and releasing several sets of position and force information, collected by physically scanning a set of real-world objects, along with virtual models of those objects. We demonstrate novel applications of this data set for the development, debugging, optimization, evaluation, and comparison of haptic rendering algorithms.",
Swara Indentification for South Indian Classical Music,"This paper deals with the identification of the swaras in a given Carnatic song, which is an essential component, needed for raga identification of South Indian Classical - Carnatic music. Raga is a representation of the possible swaras that can occur in a given Carnatic song. This paper first deals with the segmentation of Carnatic vocal music and then attempts to find the pitch associated with each segment. We propose a two level segmentation algorithm to yield the probable swara segments. For each segment, pitch information is extracted to find the frequency of that particular segment. The fundamental frequency associated with the music signal conveys the overall pitch of the singer, which in turn corresponds to the middle octave swara 'Sa'. The frequencies associated with the segments are identified and the exact tagging of swara is performed to find the 7-swara combinations in the given music signal.",
A web-based educational environment for teaching the computer cache memory,"In this paper, a Web-based educational setting for teaching the computer cache memory is presented, aiming at supporting and enhancing the learning process and promoting the active and constructive involvement of students. The educational setting includes Web text-based educational material, a Web-based cache memory simulation program, and educational activities that enable the students to participate actively in the learning process and to collaborate in groups. The design of the educational setting was based on the principles derived from the conceptual change approach regarding the students' theory framework, the students learning difficulties/misconceptions, the text comprehension theory, and contemporary teaching approaches. The results obtained from the application/evaluation of a set of educational activities have been encouraging, indicating that the simulation program and the context of the activities can effectively support and enhance the learning process.","Cache memories,
Computer science education,
Internet"
On minimal representations of Petri net languages,"Given a measure of size of a labeled Petri net, we consider the existence of a procedure that takes as input a description of an arbitrary, labeled Petri net, and returns a description of a (possibly different) labeled Petri net with the smallest size that generates the same language as the input. We refer to such procedures as minimization procedures. In this note, we investigate the existence of minimization procedures for a variety of measures. We show that these procedures cannot exist for Petri net languages for a large class of measures. However, for families of Petri net languages where controllability (cf. ), and consequently language-containment, is decidable , there can be minimization procedures for a restricted class of measures. After showing that minimization procedures for a family of measures are intractable for languages generated by bounded Petri nets, it is argued that a similar conclusion has to be reached for any family of Petri net languages that includes the family of regular languages for which there are minimization procedures.","Rivers,
Discrete event systems,
Supervisory control,
Automata,
Control system synthesis,
Control systems,
Software packages,
Computer science,
Asynchronous transfer mode,
Process control"
PRIVATE-IYE: A Framework for Privacy Preserving Data Integration,"Data integration has been a long standing challenge to the database and data mining communities. This need has become critical in numerous contexts, including building e-commerce market places, sharing data from scientific research, and improving homeland security. However, these important activities are hampered by legitimate and widespread concerns of data privacy. It is necessary to develop solutions that enable integration of data, especially in the domains of national priorities, while effective privacy control of the data. In this paper, we present an architecture and key research issues for building such a privacy preserving data integration system called PRIVATE-IYE.",
A General Iterative Regularization Framework For Image Denoising,Many existing techniques for image denoising can be expressed in terms of minimizing a particular cost function. We address the problem of denoising images in a novel way by iteratively refining the cost function. This allows us some control over the tradeoff between the bias and variance of the image estimate. The result is an improvement in the mean-squared error as well as the visual quality of the estimate. We consider four different methods of updating the cost function and compare and contrast them. The framework presented here is extendable to a very large class of image denoising and reconstruction methods. The framework is also easily extendable to deblurring and inversion as we briefly demonstrate. The effectiveness of the proposed methods is illustrated on a variety of examples.,"Image denoising,
Cost function,
Noise reduction,
Tin,
Filters,
Frequency estimation,
Iterative algorithms,
Reconstruction algorithms,
Radiometry,
Pixel"
An Adaptive Algorithm Selection Framework for Reduction Parallelization,"Irregular and dynamic memory reference patterns can cause performance variations for low level algorithms in general and for parallel algorithms in particular. In this paper, we present an adaptive algorithm selection framework which can collect and interpret the characteristics of a particular instance of parallel reduction algorithms and select the best performing one from an existing library. The framework consists of the following components: 1) an offline systematic process for characterizing the input sensitivity of parallel reduction algorithms and a method for building corresponding predictive performance models, 2) an online input characterization and algorithm selection module, and 3) a small library of parallel reduction algorithms, which represent the algorithmic choices made available at runtime. We also present one possible integration of this framework in a restructuring compiler. We validate our design experimentally and show that our framework 1) selects the most appropriate algorithms in 85 percent of the cases studied, 2) overall, delivers 98 percent of the optimal performance, 3) adaptively selects the best algorithms for dynamic phases of a running program (resulting in performance improvements otherwise not possible), and 4) adapts to the underlying machine architectures (evaluated on IBM Regatta and HP V-class systems)","Adaptive algorithm,
Optimizing compilers,
Libraries,
Parallel algorithms,
Predictive models,
Runtime,
Algorithm design and analysis,
Programming profession,
Program processors,
Sorting"
Robust 3D Head Tracking Using Camera Pose Estimation,"In this paper, we present a robust method to recover 3D position and orientation (pose) of a moving head using a single stationary camera. Head pose is recovered via a camera pose estimation formulation. 3D feature points (artificial or natural occurring) are acquired from the head prior to tracking and used as a model. Pose is estimated by solving a robust version of ""Perspective n Point"" problem. The proposed algorithm can handle self occlusions, outliers and recover from tracking failures. Results were validated by simultaneous tracking using our system and an accurate magnetic field 3D measuring device. Our contribution is a system that is not restricted to track only human heads, and is accurate enough to be used as a measuring device. To demonstrate the applicability of our method, three types of heads (human, barn owl, chameleon) were tracked in a series of biological experiments","Robustness,
Cameras,
Magnetic heads,
Humans,
Magnetic field measurement,
Solid modeling,
Geometry,
Computer science,
Application software,
Video compression"
Model- and Architecture-Driven Development in the Context of Cross-Enterprise Business Process Engineering,"Modelling and enacting cross-enterprise business processes (CBPs) is a key ability for successfully setting up and managing virtual organizations, e.g. supply chains. In this paper we present and compare approaches for modelling CBPs based on the service-oriented architecture paradigm. By embedding our overall approach into a model- and architecture-driven development perspective, we show how service-oriented systems realising CBPs can be derived from business-level modelling",
"Data-driven traffic engineering: techniques, experiences and challenges","This paper presents a global view of measurement-driven traffic engineering, explores the interplay between traffic matrix estimation and routing optimization and demonstrates how demand uncertainties can be accounted for in the optimization step to guarantee a robust and reliable result. Based on a unique data set of complete measured traffic matrices, we quantify the demand uncertainties in an operational IP network and demonstrate how a number of robust optimization schemes allow to find fixed MPLS configurations that are close to the performance limits given by time-varying routing under full demand knowledge. We present a novel scheme for computing a sparse MPLS mesh to complement a baseline routing, and explore how the performance depends on the size of the partial mesh. Corresponding methods for robust OSPF optimization are discussed and a number of challenges are detailed.",
"Modeling Civil Violence: An Evolutionary Multi-Agent, Game Theoretic Approach","This paper focuses on the design and development of a spatial evolutionary multi-agent social network (EMAS) to investigate the underlying emergent macroscopic behavioral dynamics of civil violence, as a result of the microscopic local movement and game-theoretic interactions between multiple goal-oriented agents. Agents are modeled from multi-disciplinary perspectives and their behavioral strategies are evolved over time via collective co-evolution and independent learning. Experimental results reveal the onset of fascinating global emergent phenomenon as well as interesting patterns of group movement and behavioral development. Analysis of the results provides new insights into the intricate behavioral dynamics that arises in civil upheavals. Collectively, EMAS serves as a vehicle to facilitate the behavioral development of autonomous agents as well as a platform to verify the effectiveness of various violence management policies which is paramount to the mitigation of casualties.",
Relo: Helping Users Manage Context during Interactive Exploratory Visualization of Large Codebases,"As software systems grow in size and use more third-party libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this paper, we present a tool, Relo, which supports developers' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages the context in visualization, thereby helping build the developer's mental representation of the code. Developers can group viewed artifacts or use the viewed items to ask Relo for further exploration suggestions, with Relo providing features to limit the growth of the diagram. To ensure developers don't get overwhelmed, Relo has been built with a user-centered approach, and preliminary evaluations with developers exploring new code have shown them to find the tool intuitive and helpful",
Protocol assessment issues in low duty cycle sensor networks: the switching energy,"Energy assessment of MAC protocols for wireless sensor networks is generally based on the times of transmit, receive and sleep modes. The switching energy between two consecutive states is generally considered negligible with respect to them. Although such an assumption is valid for traditional wireless ad hoc networks, is this assumption valid also for low duty cycle wireless sensor networks? The primary objective of this work is to shed some light on relationships between node switching energy and node duty cycle over the total energy consumption. In order to achieve the target, initially, we revisit the energy spent in each state and transitions of three widespread hardware platforms for wireless sensor networks by direct measurements on the EYES node. Successively, we apply the values obtained to the SMAC protocol by using the OmNet++ simulator. The main reason for using SMAC is that it is the protocol normally used as a benchmark against other architectures proposed","Intelligent networks,
Wireless sensor networks,
Media Access Protocol,
Energy consumption,
Hardware,
Access protocols,
Wireless application protocol,
Energy efficiency,
Computer science,
Energy measurement"
Secure and Private Collaborative Linear Programming,"The growth of the Internet has created tremendous opportunities for online collaborations. These often involve collaborative optimizations where the two parties are, for example, jointly minimizing costs without violating their own particular constraints (e.g., one party may have too much inventory, another too little inventory but too much production capacity, etc). Many of these optimizations can be formulated as linear programming problems, or, rather, as collaborative linear programming, in which two parties need to jointly optimize based on their own private inputs. It is often important to have online collaboration techniques and protocols that carry this out without either party revealing to the other anything about their own private inputs to the optimization (other than, unavoidably, what can be deduced from the collaboratively computed optimal solution). For example, two organizations who jointly invest in a project may want to minimize some linear objective function while satisfying both organizations' private and confidential constraints. Constraints are usually private when they reveal too much about the organizations' financial health, its future business strategy, etc. Linear programming problems have been widely studied in the literature. However, the existing solutions (e.g., the simplex method) do not extend to the above-mentioned framework in which the linear constraints are shared by the two parties, who do not want to disclose their own to the other party. In this paper, we give an efficient protocol for solving linear programming problems in the honest-but-curious model, such that neither party reveals anything about their private input to the other party (other than what can be inferred from the result). The amount of communication and computation done by our protocol is proportional to the time complexity of the simplex method, a widely used linear programming algorithm. We also provide a practical solution that prevents certain malicious behavior of the participants. The use of the known general circuit-simulation solutions to secure function evaluation is unacceptable for the simplex method, as it implies an exponential size circuit","Linear programming,
Online Communities/Technical Collaboration,
Protocols,
Cost accounting,
Production,
Circuits,
Computer science,
Internet,
Constraint optimization,
Cost function"
"Adequacy, Accuracy, Scalability, and Uncertainty of Architecture-based Software Reliability: Lessons Learned from Large Empirical Case Studies","Our earlier research work on applying architecture-based software reliability models on a large scale case study allowed us to test how and when they work, to understand their limitations, and to outline the issues that need future research. In this paper we first present an additional case study which confirms our earlier findings. Then, we present uncertainty analysis of architecture-based software reliability for both case studies. The results show that Monte Carlo method scales better than the method of moments. The sensitivity analysis based on Monte Carlo method shows that (1) small number of parameters contribute to the most of the variation in system reliability and (2) given an operational profile, components' reliabilities have more significant impact on system reliability than transition probabilities. Finally, we summarize the lessons learned from conducting large scale empirical case studies for the purpose of architecture-based reliability assessment and uncertainty analysis",
A design method of dual-frequency wilkinson power divider,"This paper treats a novel Wilkinson power divider operating at two arbitrary different frequencies. The proposed divider consists of two-section transmission lines and a series RLC circuit connected between two output ports. The circuit parameters for a dual-frequency operation are derived by the even/odd mode analysis. Equal power split, complete matching, and good isolation between two output ports are demonstrated. Finally, verification of this design concept is shown by using an electromagnetic simulator.",
Routing and Wavelength Assignment for Prioritized Demands Under a Scheduled Traffic Model,"In the scheduled traffic model, the design problem is to allocate resources to a set of demands whose setup and teardown times are known in advance. A number of integer linear program (ILP) solutions for this problem have been presented in the literature. In this paper we present a new ILP formulation for routing and wavelength allocation, under the scheduled traffic model that minimizes the congestion of the network. We propose two levels of service, where idle backup resources can be used to carry low priority traffic, under fault-free conditions. When a fault occurs, and resources for a backup path need to be reclaimed, any low priority traffic on the affected channels is dropped. The results demonstrate that this can lead to significant improvements over single service level models. We are able to generate optimal solutions for moderate sized networks, within a reasonable amount of time. Finally, we present a simple and fast heuristic that can quickly generate good solutions for much larger networks.",
An algebraic perspective to single-transponder underwater navigation,"This paper studies the position estimation of an underwater vehicle using a single acoustic transponder. The chosen estimation approach is based on nonlinear differential algebraic methods which allow to express very simply conditions for observability. These are then used in combination with an integrator-based time-derivative estimation technique to design an algebraic estimator, which, contrary to asymptotic observers, does not require sometimes tedious convergence verification. Simple simulation results are presented to illustrate the approach.",
Trajectory Similarity Search in Spatial Networks,"In several applications, data objects are assumed to move on predefined spatial networks such as road segments, railways, and invisible air routes. Moving objects may exhibit similarity with respect to their traversed paths, and therefore two objects can be correlated based on their path similarity. In this paper, we study similarity search for moving object trajectories for spatial networks. The problem poses some important challenges, since it is quite different from the case where objects are allowed to move without motion restrictions. Experimental results performed on real-life spatial networks show that trajectory similarity can be supported in an effective and efficient manner by using metric-based access methods",
Dirac Mixture Density Approximation Based on Minimization of the Weighted Cramer-von Mises Distance,"This paper proposes a systematic procedure for approximating arbitrary probability density functions by means of Dirac mixtures. For that purpose, a distance measure is required, which is in general not well defined for Dirac mixture densities. Hence, a distance measure comparing the corresponding cumulative distribution functions is employed. Here, we focus on the weighted Cramer-von Mises distance, a weighted integral quadratic distance measure, which is simple and intuitive. Since a closed-form solution of the given optimization problem is not possible in general, an efficient solution procedure based on a homotopy continuation approach is proposed. Compared to a standard particle approximation, the proposed procedure ensures an optimal approximation with respect to a given distance measure. Although useful in their own respect, the results also provide the basis for a recursive nonlinear filtering mechanism as an alternative to the popular particle filters","Particle filters,
Density measurement,
Measurement standards,
Particle measurements,
Distribution functions,
Intelligent systems,
Probability density function,
Filtering,
Testing,
Statistical analysis"
,,
A Method of User-oriented Reliability Assessment for Open Source Software and Its Applications,"Software development environment has been changing into new development paradigms such as concurrent distributed development environment and the so-called open source project by using network computing technologies. In case of considering the effect of the debugging process on an entire system in the development of a method of reliability assessment for open source project, it is necessary to grasp the deeply-intertwined factors, such as programming path, size of each component, skill of fault reporter, and so on. In order to consider the effect of each software component on the reliability of an entire system under such new distributed development paradigm, we propose a new approach to software reliability assessment by creating a fusion of neural network and software reliability growth model. In this paper, we show application examples of software reliability assessment based on neural network and software reliability growth model for open source software. Also, we analyze actual software fault count data to show numerical examples of software reliability assessment for the open source software. Then, we consider the efficiency and effectiveness of the software reliability assessment method for the actual open source software.",
Using GVF Snake to Segment Liver from CT Images,"Liver segmentation on computed tomography (CT) images is a challenging task because the images are often corrupted by noise and sampling artifacts. Thus we choose GVF snake to perform the task. Unfortunately, GVF snake use Gaussian function to generate the edge map. We find that this often cause new problems such as blur the liver boundary. To avoid this, a Canny edge detector is a good choice. Another problem during the segmentation is that GVF snake cannot works well with bad initialization, especially when encounter deep concavities. Fortunately we find that if the initial contour can cross the ""bottleneck"" of the deep concave, it can easily reach the boundary of liver. Thus an algorithm was developed to generate the initial contour automatically. We introduce a new ""maximum force angle map"" to evaluate the direction variability of the GVF forces. This map can mark up the ""bottleneck "" and give a trace to run through it. There may be other trace we do not need in the map. With the help of transcendental knowledge about the liver, such as the position, the shape and the Hounsfield unit range of the liver, the correct trace can be found. The contour of this trace is suitable for using as initial contour for GVF snake. By this means we finally segment the liver slice by slice correctly.","Image segmentation,
Liver,
Computed tomography,
Image edge detection,
Shape,
Active contours,
Computer science,
Biomedical imaging,
Image sampling,
Detectors"
Effectively utilizing global cluster memory for large data-intensive parallel programs,"Large scientific parallel applications demand large amounts of memory space. Current parallel computing platforms schedule jobs without fully knowing their memory requirements. This leads to uneven memory allocation in which some nodes are overloaded. This, in turn, leads to disk paging, which is extremely expensive in the context of scientific parallel computing. To solve this problem, we propose a new peer-to-peer solution called parallel network RAM. This approach avoids the use of disk, better utilizes available RAM resources, and will allow larger problems to be solved while reducing the computational, communication, and synchronization overhead typically involved in parallel applications. We proposed several different parallel network RAM designs and evaluated the performance of each under different conditions. We discovered that different designs are appropriate in different situations.","Read-write memory,
Random access memory,
Peer to peer computing,
Parallel processing,
Processor scheduling,
Context,
Concurrent computing,
Computational modeling,
Network servers,
Scientific computing"
A Multi-Stakeholder Multi-Criteria Assessment Framework of Mobile Payments: An Illustration with the Swiss Public Transportation Industry,"As wireless technologies are becoming more mature for payment systems, the public transportation companies are facing great challenges in designing their next fare collection system. They have to decide whether they want to keep their legacy systems or innovate with the implementation of new mobile technologies for their upcoming payment instrument. Therefore, they need to consider various technical and business aspects during the design process. Moreover, in such an industry, a technological consensus between the stakeholders involved is desirable to eventually reach a global market. In this paper, we propose a multi-actor multi-criteria framework to facilitate the assessment of mobile payments for the Swiss public transportation industry. To compute our proposed model, we recently conducted a first set of structured interviews with Swiss key experts to collect data.","Process design,
Switches,
Decision support systems,
Companies,
Instruments,
Globalization,
Mobile computing,
Computer industry,
Rail transportation,
Telephone sets"
Thermal-aware high-level synthesis based on network flow method,"Lowering down the chip temperature is becoming one of the important design considerations, since temperature adversely and seriously affects many of design qualities, such as reliability, performance and leakage power of chip, and also increases the packaging cost. In this work, we address a new problem of thermal-aware module binding in high-level synthesis, in which the objective is to minimize the peak temperature of the chip. The two key contributions are (1) to solve the binding problem with the primary objective of minimizing the 'peak' switched capacitance of modules and the secondary objective of minimizing the 'total' switched capacitance of modules and (2) to control the switched capacitances with respect to the floorplan of modules in a way to minimize the 'peak' heat diffusion between modules. For (1), our proposed thermal-aware binding algorithm, called TA-b, formulates the thermal-aware binding problem into a problem of repeated utilization of network flow method, and solve it effectively. For (2), TA-b is extended, called TA-bf, to take into account a floorplan information, if exists, of modules to be practically effective. From experiments using a set of benchmarks, it is shown that TA-bf is able to use 10.1degC and 11.8degC lower peak temperature on the average, compared to that of the conventional low-power and thermal- aware methods, which target to minimizing total switched capacitance only ([18]) and to minimizing peak switched capacitance only ([16]), respectively.","High level synthesis,
Capacitance,
Energy consumption,
Equations,
Temperature measurement,
Iterative algorithms,
Computer science,
Computer network reliability,
Packaging,
Costs"
An Approach to Test Data Generation for Killing Multiple Mutants,"Software testing is an important technique for assurance of software quality. Mutation testing has been identified as a powerful fault-based technique for unit testing, and there has been some research on automatic generation of test data for mutation testing. However, existing approaches to this kind of test data generation usually generate test data according to one mutant at one time. Thus, more test data that are needed for achieving a given mutation score. In this paper, we propose a new approach to generating one test data according to multiple mutants that are mutated at the same location at one time. Thus, our approach can generate smaller test suite that can achieve the same mutation testing score. To evaluate our approach, we implemented a prototype tool based on our approach and carried out some preliminary experiments. The experimental results show that our approach is more cost-effective",
Probabilistic Message Passing in Peer Data Management Systems,"Until recently, most data integration techniques involved central components, e.g., global schemas, to enable transparent access to heterogeneous databases. Today, however, with the democratization of tools facilitating knowledge elicitation in machine-processable formats, one cannot rely on global, centralized schemas anymore as knowledge creation and consumption are getting more and more dynamic and decentralized. Peer Data Management Systems (PDMS) provide an answer to this problem by eliminating the central semantic component and considering instead compositions of local, pair-wise mappings to propagate queries from one database to the others. PDMS approaches proposed so far make the implicit assumption that all mappings used in this way are correct. This obviously cannot be taken as granted in typical PDMS settings where mappings can be created (semi) automatically by independent parties. In this work, we propose a totally decentralized, efficient message passing scheme to automatically detect erroneous mappings in PDMS. Our scheme is based on a probabilistic model where we take advantage of transitive closures of mapping operations to confront local belief on the correctness of a mapping against evidences gathered around the network. We show that our scheme can be efficiently embedded in any PDMS and provide a preliminary evaluation of our techniques on sets of both automatically-generated and real-world schemas.",
"A Comment on ""HEED: A Hybrid, Energy-Efficient, Distributed Clustering Approach for Ad Hoc Sensor Networks'","We provide a better sufficient condition for the connectivity of cluster heads asymptotically almost surely (a.a.s.) and a tighter bound on the number of cluster heads in HEED (O. Younis and S. Fahmy, 2004)","Energy efficiency,
Sufficient conditions,
Distributed algorithms,
Intelligent networks,
Spread spectrum communication,
Mobile computing,
Computer science"
Thickness dependent tortuosity estimation for retinal blood vessels,"This paper describes a framework for the automated estimation of vessel tortuosity in retinal images. We introduce a new tortuosity metric that takes into account vessel thickness, yielding estimates plausibly closer to intuition and medical judgement than those from previous metrics. We also propose an algorithm identifying automatically a vasculature segment connecting two points specified manually. Starting from a binary image of the vasculature, the algorithm computes a skeletal (medial axis) representation on which all terminal and branching points are located. This is then converted to a graph representation including connectivity as well as thickness information for all vessels. Target segments for tortuosity estimation are identified automatically from end points selected manually using a shortest-path algorithm. Results are presented and compared with those provided by clinical classification on 50 vessels from DRIVE images. An overall agreement with clinical judgement of 92.4% is achieved, superior to that of comparison measures",
Lp metrics on the Heisenberg group and the Goemans-Linial conjecture,"We prove that the function d : Ropf3 times Ropf 3 rarr [0,infin] given by d((x,y,z),(t,u,v)) = ([((t-x) 2+(u-y)2)2 + (v-z+2xu-2yt)2] frac12 + (t-x)2 + (u-y)2)frac12 is a metric on Ropf3 such that (Ropf3,radicd) is isometric to a subset of Hilbert space, yet (Ropf3, d) does not admit a bi-Lipschitz embedding into L1. This yields a new simple counter example to the Goemans-Linial conjecture on the integrality gap of the semidefinite relaxation of the sparsest cut problem. The metric above is doubling, and hence has a padded stochastic decomposition at every scale. We also study the Lp version of this problem, and obtain a counter example to a natural generalization of a classical theorem of Bretagnolle et al. (1996) (of which the Goemans-Linial conjecture is a particular case). Our methods involve Fourier analytic techniques, and a breakthrough of Cheeger and Kleiner (2006), together with classical results of Pansu (1989) on the differentiability of Lipschitz functions on the Heisenberg group","Extraterrestrial measurements,
Hilbert space,
Counting circuits,
Approximation algorithms,
Polynomials,
Stochastic processes,
NP-hard problem,
Computer science,
Complexity theory,
Geometry"
Efficient Data-Movement for Lightweight I/O,"Efficient data movement is an important part of any high-performance I/O system, but it is especially critical for the current and next-generation of massively parallel processing (MPP) systems. In this paper, we discuss how the scale, architecture, and organization of current and proposed MPP systems impact the design of the data-movement scheme for the I/O system. We also describe and analyze the approach used by the lightweight file systems (LWFS) project, and we compare that approach to more conventional data-movement protocols used by small and mid-range clusters. Our results indicate that the data-movement strategy used by LWFS clearly outperforms conventional data-movement protocols, particularly as data sizes increase",
Predicting Academic Performance,"The importance placed on matriculation increases at each level of academia, with the greatest significance placed at the tertiary level. The task of standardizing entry-level requirements at the tertiary level has lead to the implementation of assessments such as SAT GMAT and GRE. Prior research conducted at the University of Technology, Jamaica (UTECH) indicated that the task of finding effective predictors of academic performance remains incomplete. This study examines the relationship between students' overall academic performance (GPA) and matriculation requirements performance in first year courses in the Bachelor of Science and Information Technology (BSCIT) program at UTECH. The study evaluates undergraduate students that completed the BSCIT program in 2005. The files for all BSCIT undergraduate students of 2005 were surveyed and specific data was collected. The findings pointed out that performance in first year gateway courses had some level of significance in predicting performance. The findings from this study will be instrumental in restructuring the admissions policy for the program","Testing,
Information technology,
Instruments,
Educational programs,
Educational institutions,
Councils,
Mathematics,
Natural languages,
Crops,
Qualifications"
Impact of Mobility on the Performance of Relaying in Ad Hoc Networks,,"Relays,
Ad hoc networks,
Throughput,
Mobile ad hoc networks,
Performance analysis,
Computer science,
System performance,
Steady-state,
Diversity methods,
Delay"
Dynamics Based Robust Motion Segmentation,"In this paper we consider the problem of segmenting multiple rigid motions using multi-frame point correspondence data. The main idea of the method is to group points according to the complexity of the model required to explain their relative motion. Intuitively, this formalizes the idea that points on the same rigid share more modes of motion (for instance a common translation or rotation) than points on different objects, leading to less complex models. By exploiting results from systems theory, the problem of estimating the complexity of the underlying model is reduced to simply computing the rank of a matrix constructed from the correspondence data. This leads to a simple segmentation algorithm, computationally no more expensive than a sequence of SVDs. Since the proposed method exploits both spatial and temporal constraints, is less sensitive to the effect of noise or outliers than approaches that rely solely on factorizations of the measurements matrix. In addition, the method can also naturally handle ""degenerate cases"", e.g. cases where the objects partially share motion modes. These results are illustrated using several examples involving both degenerate and non-degenerate cases.","Robustness,
Motion segmentation,
Computer vision,
Propellers,
Airplanes,
Computer science,
Data engineering,
Noise measurement,
Geometrical optics,
Optical sensors"
Computer simulation of interaction of arc-gas flow in SF/sub 6/ puffer circuit breaker considering effects of ablated nozzle vapor,"Computer simulation is performed in a whole breaking period except the region of current zero of gas circuit breaker. The Euler equation is solved using the finite volume fluid in cell method. In order to consider the effects of arc, electric field intensity and radiation transport are calculated using the finite element method and the method of partial characteristics, respectively. Also, the quantity and the electrical conductivity of ablated nozzle vapor are calculated, and its effects are investigated through computer simulation. Improved simulation results are obtained and compared with the experimental one",
On the lower bound of the linear complexity over F/sub p/ of Sidelnikov sequences,"For a Sidelnikov sequence of period p/sup m/-1, tight lower bounds are obtained on its linear complexity L over F/sub p/. In particular, these bounds imply that, uniformly over all p and m, L is close to its largest possible value p/sup m/-1.","Cryptography,
Polynomials,
Codes,
Galois fields,
Sufficient conditions,
Iterative methods,
Boolean functions,
Computer science,
Signal design,
Wireless communication"
A simple test to assess the static and dynamic accuracy of an inertial sensors system for human movement analysis,"In the present study we introduced a simple test to assess the orientation error of an inertial sensors system for human movement analysis, both in static and dynamic conditions. In particular, the test was intended to quantify the sensitivity of the orientation error to direction and velocity of rotation. The test procedure was performed on a 5 MT9B sensors Xsens acquisition system, and revealed that the system orientation error, expressed by Euler angles decomposition, was sensitive both to direction and to velocity, being higher for fast movements: for mean rotation velocities of 180deg/s and 360deg/s, the worst case orientation error was 5.4deg and 11.6deg, respectively. The test can be suggested therefore as a useful tool to verify the user specific system accuracy without requiring any special equipment. In addition, the test provides further error information concerning direction and velocity of the movement which are not supplied by the producer, since they depend on the specific field of application",
Monitoring the Security Health of Software Systems,"Detecting security bugs during the development cycle of a software is extremely difficult as effective testing approaches for such bugs do not exist. Applications are often deployed without being tested for security vulnerabilities even though the application domain demands highly secure software. Hence there is a need to develop systems which can monitor such applications for security violations and take immediate actions if any violation occurs. In this paper we describe an approach for monitoring the security health of a software system. Our methodology involves an agent based approach which communicates with the health monitoring system running as an independent process. We make this agent a part of the application (binary) and modify the binary at appropriate locations to transfer the control to the agent attached. The agent sends critical information regarding the execution to the monitoring system. The monitoring system analyzes the data and takes suitable actions. Currently our system monitors the following security bugs uffer overflow, race conditions (time of check to time to use vulnerability), random number vulnerability and can be extended for other vulnerabilities also","Software systems,
Information security,
Computerized monitoring,
Computer bugs,
Computer security,
Application software,
Data security,
Computer science,
Software testing,
Data analysis"
Lifetime Analysis in Heterogeneous Sensor Networks,"Wireless sensor networks (WSN) are composed of battery-driven communication entities performing multiple, usually different tasks. In order to complete a given task, all sensor nodes, which are deployed in an ad-hoc fashion have to collaborate by exchanging and forwarding measurement data. We define the behavior of the overall sensor network based on the parameters lifetime and functional density. The functional density describes the distribution of all necessary tasks in a given geographical area. The lifetime is primarily given by the time each task is successfully performed by at least one node, i.e. the functional density of all necessary tasks. Nodes can become unavailable due to insufficient remaining energy. We assume that sensor nodes can be reconfigured or reprogrammed by a mobile robot system. There are various reasons for considering robots for this reconfiguration, e.g. reliability, security, and deployment issues. In this paper, we evaluate the advantages of exploiting reconfiguration and reprogramming schemes WSN using mobile robots. The primary objective is to increase the lifetime of the overall network. This goal is achieved by optimizing the functional density of heterogeneous tasks. Based on a developed simulation model, we discuss the advantages and performance characteristics",
Exploiting Diversity Gain in MIMO Equipped Ad hoc Networks,"MIMO links coupled with space-time codes can combat fading and hence can significantly increase the capacity of ad hoc networks. This ability of providing "" diversity gain"" can increase the capacity of ad hoc networks. Currently, most of the studies on MIMO links systems are focused on the physical layer without taking into consideration the intricacies of a network-wide deployment. In this work we study the benefits of a network-wide deployment of MIMO links in mobile ad hoc networks. In particular, we examine the trade-offs between using the possible diversity gain for an increase in range or an increase in rate. We make minor modifications to traditionally used MAC and routing schemes popularly considered for ad hoc networks and perform extensive simulations to understand the above trade-offs. We quantify the performance trade-offs in terms of the achieved throughput and end-to-end latency. Our studies can serve as a precursor to the design of adaptive schemes that can exploit the achievable diversity to increase range/rate depending on the scenario at hand.","Diversity methods,
MIMO,
Ad hoc networks,
Space time codes,
Fading,
Physical layer,
Mobile ad hoc networks,
Routing,
Throughput,
Delay"
Opinion Searching in Multi-Product Reviews,"It is becoming common that people browse Web for product reviews before purchasing. However, to retrieve opinions relevant to customer desire still remains challenging. In this paper, we studied the problem of opinion searching, whose aim is to search the opinions about specific feature of specific product and locate them in multi-product reviews. Our solution includes two steps: opinion indexing and opinion retrieving. Opinion indexing is to identify opinion fragments and generate opinion tuples (product,feature and sentiment). Opinion retrieving is to look up the opinion tuples matching users' retrieving interests, and help users to locate the corresponding opinion fragments in documents. Fundamentally, opinion indexing should be able to identify the feature-product dependencies (i.e., a feature mentioned in somewhere of reviewing text is semantically associated with which product). We explore to resolve the problem with machine-learning techniques.",
Distributed EM Algorithms for Acoustic Source Localization in Sensor Networks,"Acoustic source localization is one of the interesting applications of sensor networks. Localization algorithms that use acoustic signal energy measurements at individual sensor nodes are proposed. The maximum likelihood (ML) algorithm is known as an algorithm for source localization. The ML algorithm has high accuracy to estimate source location, however the calculation amount of the ML algorithm is large. The expectation- maximization (EM) algorithm is proposed to realize the ML algorithm with less amount of calculation. An EM algorithm processed at a center, that is, the centralized EM algorithm is proposed in which each sensor node sends observed information to the center. The total cost of the centralized data processing is expensive, because the communication cost is large when the transmission distance from each sensor node to the center is long. On the other hand, the distributed data processing can reduce the transmission cost and thus the total cost, because data is processed in each sensor node and thus the amount of communication can be reduced. In this paper, we propose a distributed EM algorithm for acoustic source localization that is applicable to a source with unknown signal energy. We show that the proposed algorithm has high accuracy and reduces the communication cost.",
Registration of expressions data using a 3D morphable model,"The registration of 3D scans of faces is a key step for many applications, in particular for building 3D morphable models. Although a number of algorithms are already available for registering data with neutral expression, the registration of scans with arbitrary expressions is typically performed under the assumption of a known, fixed identity. We present a novel algorithm which breaks this restriction, allowing to register 3D scans of faces with arbitrary identity and expression. Furthermore, our algorithm can process incomplete data, yielding results which are both continuous and with low reconstruction error. Even in the case of complete, expression-less data, our method can yield better results than previous algorithms, due to an adaptive smoothing, which regularizes the results surface only where the estimated correspondence is unreliable","Humans,
Shape,
Face recognition,
Robustness,
Computer science,
Application software,
Smoothing methods,
Surface reconstruction,
Yield estimation,
Minimization methods"
Comparison of PM motor structures and sensorless control techniques for zero-speed rotor position detection,"The rotor position of a synchronous Permanent Magnet (PM) motor can be detected by means of the superimposition of a high-frequency component to the steady-state stator voltage. Thanks to the anisotropy of the rotor, the corresponding high-frequency current is modulated and used to determine the rotor position. Two techniques can be considered: the first one adopts a pulsating voltage vector in the estimated synchronous reference frame, while the second one adopts a rotating voltage vector. These techniques are effective at zero and at low motor speed. The accuracy of the rotor position detection depends strictly on the rotor saliency, that is, on the geometry of the PM rotor. In fact both saturation and cross-coupling have a heavy influence on the correct rotor position detection. The aim of this paper is to compare the two sensorless control techniques, in conjunction with some rotor geometries. In order to highlight the effectiveness of the sensorless technique, the tests are carried out at various operating conditions. It is found that the effectiveness of the sensorless rotor position detection strongly depends on the choice of the sensorless control technique together with a proper PM rotor geometry.",
Impact of Overlay Routing on End-to-End Delay,"The prosperity of various real-time applications triggers significant challenges for the Internet to meet the critical end-to-end delay requirements. This paper investigates the feasibility and practical issues of using overlay routing to improve end-to-end delay performance, with the analysis of round trip time data collected over three months by the all pairs pings project between each pair of hundreds of nodes on the Planet- Lab. The results show: 1) overlay routing has the potential to reduce the round trip time by 40 milliseconds and increase network connectivity by 7% on average; 2) even simple static overlay paths can reduce end-to-end delay, while the dynamic algorithm does better; 3) Over 80% of the shortest overlay paths have no more than 4 hops, and a simple algorithm leveraging only one relay node can efficiently take the advantages of overlay routing.","Routing,
Internet,
Bandwidth,
Delay effects,
Quality of service,
Protocols,
Algorithm design and analysis,
Automation,
Information technology,
Information science"
"A New Paradigm for Low-power, Variation-Tolerant Circuit Synthesis Using Critical Path Isolation","Design considerations for robustness with respect to variations and low power operations typically impose contradictory design requirements. Low power design techniques such as voltage scaling, dual-Vth etc. can have a large negative impact on parametric yield. In this paper, we propose a novel paradigm for low-power variation-tolerant circuit design, which allows aggressive voltage scaling. The principal idea is to (a) isolate and predict the set of possible paths that may become critical under process variations, (b) ensure that they are activated rarely, and (c) avoid possible delay failures in the critical paths by dynamically switching to two-cycle operation (assuming all standard operations are single cycle), when they are activated. This allows us to operate the circuit at reduced supply voltage while achieving the required yield. Simulation results on a set of benchmark circuits at 70nm process technology show average power reduction of 60% with less than 10% performance overhead and 18% overhead in die-area compared to conventional synthesis. Application of the proposed methodology to pipelined design is also investigated","Circuit synthesis,
Dynamic voltage scaling,
Delay,
Timing,
Low voltage,
Robustness,
Logic design,
Power dissipation,
Design methodology,
Logic gates"
Secure Multiparty Quantum Computation with (Only) a Strict Honest Majority,"Secret sharing and multiparty computation (also called ""secure function evaluation"") are fundamental primitives in modern cryptography, allowing a group of mutually distrustful players to perform correct, distributed computations under the sole assumption that some number of them will follow the protocol honestly. This paper investigates how much trust is necessary $that is, how many players must remain honest - in order for distributed quantum computations to be possible. We present a verifiable quantum secret sharing (VQSS) protocol, and a general secure multiparty quantum computation (MPQC) protocol, which can tolerate any cheaters among n players. Previous protocols for these tasks tolerated lfloor (n - 1)/4 rfloor and lfloor (n - 1)/6 rfloor cheaters, respectively. The threshold we achieve is tight - even in the classical case, ""fair"" multiparty computation is not possible if any set of n/2 players can cheat. Our protocols rely on approximate quantum error-correcting codes, which can tolerate a larger fraction of errors than traditional, exact codes. We introduce new families of authentication schemes and approximate codes tailored to the needs of our protocols, as well as new state purification techniques along the lines of those used in fault-tolerant quantum circuits","Quantum computing,
Cryptography,
Distributed computing,
Performance evaluation,
Cryptographic protocols,
Error correction codes,
Authentication,
Purification,
Fault tolerance,
Circuits"
A Distributed Channel Access Protocol for Ad Hoc Networks with Feedback Power Control,"Distributed power control schemes are extensively employed in the cellular networks and are capable of improving the capacity of the network. However, the power control schemes from the cellular networks suffer from performance degradation due to self and direct-interference and hidden-terminal problems when directly employed in ad hoc networks. Most of the existing channel reservation-based power control protocols for ad hoc networks employ incremental power allocation rather than global allocation of the power to the incoming links; thus, they may not effectively utilize the spatial frequency reuse in the network. This paper presents a distributed channel access protocol that couples the channel reservation and the iterative/global transmission power control schemes in ad hoc networks. The designed protocol considers the convergence problem of the global power control in ad hoc networks. The designed access criteria employ the local admission control based on the sufficient criteria for admissibility and global power control for balancing the SIR (signal to interference ratio) of the links. In the performance evaluation study of the designed protocol, an almost two-fold increase in the throughput and capacity is observed compared to the existing power-controlled protocol for ad hoc networks","Access protocols,
Ad hoc networks,
Feedback,
Power control,
Land mobile radio cellular systems,
Degradation,
Radio spectrum management,
Couplings,
Convergence,
Signal design"
Low-overhead testing of delay faults in high-speed asynchronous pipelines,"We propose a low-overhead method for delay fault testing in high-speed asynchronous pipelines. The key features of our work are: (i) testing strategies can be administered using low-speed testing equipment; (ii) testing is minimally-intrusive, i.e. very little testing hardware needs to be added; (iii) testing methods are extended to pipelines with forks and joins, which is an important first step to testing pipelines with arbitrary topologies; (iv) test pattern generation takes into account the likely event that one delay fault causes several bits of data to become corrupted; and (v) test generation can leverage existing stuck-at ATPG tools. In describing our testing strategy, we use examples of faults from three very different high-speed pipeline styles: MOUSETRAP, GasP, and high-capacity (HC) pipelines. In addition, we give an in-depth example - including test pattern generation - for both linear and non-linear MOUSETRAP pipelines",
Placement and distributed deployment of sensor teams for triangulation based localization,"We address the problem of placing a sensor network so as to minimize the uncertainty in estimating the position of targets. The novelty of our formulation is in the sensing model: we focus on stereo sensors where the measurements from two sensors must be combined for an estimation. We study two versions of this problem. In the first version, which we call the placement problem, we are given a workspace and an error threshold. The objective is to place a minimum number of cameras so that no matter where the target is located in the workspace, the uncertainty in localizing it is less than the threshold. For this problem, we present an approximation algorithm and prove that the deviation of its performance from the optimal value is bounded by a constant. In the second version, called the deployment problem, we study the problem of relocating a mobile sensor team to minimize the uncertainty in localizing possibly moving targets. We present a distributed, discrete-time algorithm which explicit addresses communication and motion constraints and show how to compute the optimal move within the time-step for a given target/sensor-pair assignment. The utility of the algorithm is demonstrated with simulations","Cameras,
Robot sensing systems,
Robot vision systems,
Robotics and automation,
Approximation algorithms,
Uncertainty,
Mobile communication,
Distributed computing,
Mobile robots,
Computer science"
Debunking the nerd stereotype with pair programming,"Our studies show that using pair programming as a structure for incorporating collaboration in the classroom helps increase and broaden participation in computing fields and helps debunk the myth that programmers work alone all the time. It's also a way for students to get a better view of and feel more confident in their preparation for working in the real world. The face of the IT workforce is changing. As the millennial generation makes its way into the working world, the archetype of the nerd as the introverted, obsessed computer programmer must share equal space with the chatty, social software engineer.","Programming profession,
Calculus,
Computer science,
Mathematical programming,
Collaborative work,
Fellows,
Educational institutions,
Mathematics,
Engineering profession,
Degradation"
Semi-Supervised Classification Using Linear Neighborhood Propagation,"In this paper, we address the general problem of learning from both labeled and unlabeled data. Based on the reasonable assumption that the label of each data can be linearly reconstructed from its neighbors’ labels, we develop a novel approach, called Linear Neighborhood Propagation (LNP), to learn the linear construction weights and predict the labels of the unlabeled data from the labeled data. The main procedure is composed of two steps: (1) it computes the local neighborhood geometry according to the image information by solving a constrained least squares problem, (2) the obtained geometry will be used to predict the labels of unlabeled data by solving a linearly constrained quadratic optimization problem. LNP has at least three characteristics. Firstly, it is much practical in that only one free parameter, the size of neighborhood, requires user-tuning. Secondly it can be easily extended to out-of-sample examples. Finally the labels achieved from LNP can be sufficiently smooth with respect to the intrinsic structure collectively revealed by the labeled and unlabeled points. Many promising experimental results, such as object recognition, data ranking and interactive image segmentation, demonstrate the effectiveness and efficiency of the proposed approach.","Image reconstruction,
Computational geometry,
Information geometry,
Constraint optimization,
Image segmentation,
Semisupervised learning,
Laboratories,
Intelligent systems,
Automation,
Computer science"
Multistage successive refinement for Wyner-Ziv source coding with degraded side informations,"We provide a complete characterization of rate region for the multistage successive refinement of Wyner-Ziv source coding problem with degraded side information at the decoder. This problem was left open in a recent work by Steinberg and Merhav (T-IT, 2004), where it was solved for the special case of two stages. Furthermore, we introduce the notion of generalized successive refinability with multiple side informations. This captures whether progressive encoding to satisfy the distortion constraints for different side information as good as encoding without progressive requirement. For degraded side-information, we give necessary and sufficient conditions for generalized successive refinability. Using this, we show that for Gaussian source, the failure of being successively refinable with multiple side informations is only due to the inherent uncertainty on which side information will occur at the decoder, but not the progressive encoding requirement",
Using Site-Level Modeling to Evaluate the Performance of Parallel System Schedulers,"The conventional performance evaluation methodology for parallel system schedulers uses an open model to generate the workloads used in simulations. In many cases recorded workload traces are simply played back, assuming that they are reliable representatives of real workloads, and leading to the expectation that the simulation results actually predict the scheduler’s true performance. We show that the lack of feedback in these workloads results in performance prediction errors, which may reach hundreds of percents. We also show that load scaling, as currently performed, further ruins the representativeness of the workload, by generating conditions which cannot exist in a real environment. As an alternative, we suggest a novel sitelevel modeling evaluation methodology, in which we model not only the actions of the scheduler but also the activity of users who generate the workload dynamically. This advances the simulation in a manner that reliably mimics feedback effects found in real sites. In particular, saturation is avoided because the generation of additional work is throttled when the system is overloaded. While our experiments were conducted in the context of parallel scheduling, the idea of site-level simulation is applicable to many other types of systems.",
Security Vulnerabilities: From Analysis to Detection and Masking Techniques,"This paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. A finite-state machine (FSM) approach is employed to decompose programs into multiple elementary activities, making it possible to extract simple predicates to be ensured for security. The FSM analysis pinpoints common characteristics among a broad range of security vulnerabilities: predictable memory layout, unprotected control data, and pointer taintedness. We propose memory layout randomization and control data randomization to mask the vulnerabilities at runtime. We also propose a static analysis approach to detect potential security vulnerabilities using the notion of pointer taintedness.","Data security,
Runtime,
Data analysis,
Databases,
Buffer overflow,
Data mining,
Computer security,
Protection,
Computer science,
Gain measurement"
On-demand channel switching for multi-channel wireless MAC protocols,"We propose and analyze the on-demand switching (ODC), a broadcast-based medium access control (MAC) protocol for ad-hoc wireless networks with multiple channels and a single half-duplex transceiver at each host. The ODC performs an on-demand, dynamic, channel selection based on the traffic conditions of the channels and communication patterns of the participating hosts. A host stays on a channel as long as its traffic share on that channel is above a certain threshold, below which it switches to another channel. It broadcasts its departure and arrival before and after each channel switch, respectively. Hosts keep track of these broadcasts and use them to discover their receivers. The channel selection is thus based on reducing the unnecessary receives, while still being able to send and receive legitimate traffic. It results in increased bandwidth utilization, reduced packet delays, and increased energy savings. We evaluated the performance of the ODC with extensive simulations and compared it with the IEEE 802.11 and another related MAC protocol (MMAC). We found that the flow distribution and traffic among hosts have a great impact on its performance. For example, in one-to-one schemes with heavy load traffic, there is an up to 180% increase in the aggregate throughput compared to the IEEE 802.11, and a 15-35% increase compared to the MMAC. On the other hand, star or clique flow distributions tend to favor the IEEE 802.11 over the ODC and MMAC.","Transceivers,
Receivers,
Switches"
Accuracy and Dynamics of Hash-Based Load Balancing Algorithms for Multipath Internet Routing,"This paper studies load balancing for multipath Internet routing. We focus on hash-based load balancing algorithms that work on the flow level to avoid packet reordering which is detrimental for the throughput of transport layer protocols like TCP. We propose a classification of hash-based load balancing algorithms, review existing ones and suggest new ones. Dynamic algorithms can actively react to load imbalances which causes route changes for some flows and thereby again packet reordering. Therefore, we investigate the load balancing accuracy and flow reassignment rate of load balancing algorithms. Our exhaustive simulation experiments show that these performance measures depend significantly on the traffic properties and on the algorithms themselves. As a consequence, our results should be taken into account for the application of load balancing in practice.","Load management,
Internet,
Routing,
Heuristic algorithms,
Throughput,
Transport protocols,
Telecommunication traffic,
Traffic control,
IP networks,
TCPIP"
Probabilistic location recognition using reduced feature set,"The localization capability is central to basic navigation tasks and motivates development of various visual navigation systems. In this paper we describe a two stage approach for localization in indoor environments. In the first stage, the environment is partitioned into several locations, each characterized by a set of scale-invariant keypoints and their associated descriptors. In the second stage the keypoints of the query view are integrated probabilistically yielding an estimate of most likely location. The novelty of our approach is in the selection of discriminative features, best suited for characterizing individual locations. We demonstrate that high location recognition rate is maintained with only 10% of the originally detected features, yielding a substantial speedup in recognition and capability of handling larger environments. The ambiguities due to the self-similarity and dynamic changes in the environment are resolved by exploiting spatial relationships between locations captured by hidden Markov model",
A User-Centered Approach to Active Replica Management in Mobile Environments,"A replication scheme determines the number and location of replicas in a distributed system. Traditional static replication schemes do not perform well in mobile environments due to the assumptions of fixed hosts, relatively static access patterns, and the lack of considerations on users' behalf. For effective data management in mobile environments, we define the notion of activity-data dependency for inferring the information requirement of mobile users based on their scheduled activities. We then propose a dynamic replication scheme which employs user profiles for recording users' mobility schedules, accesses behaviors and read/write patterns, and actively reconfigures the replicas to adapt to changes in user locations, data requests, and system status. Simulation results demonstrate that the scheme can accurately predict the data requirement to facilitate effective replication, reduce response time, and increase data availability","Environmental management,
Statistics,
Dynamic scheduling,
Costs,
Processor scheduling,
Mobile computing,
Predictive models,
Delay,
Prediction algorithms,
Heuristic algorithms"
A Lightweight Intrusion Detection Model Based on Feature Selection and Maximum Entropy Model,"Intrusion detection is a critical component of secure information systems. Current intrusion detection systems (IDS) especially NIDS (network intrusion detection system) examine all data features to detect intrusions. However, some of the features may be redundant or contribute little to the detection process and therefore they have great impact on the system performance. This paper proposes a lightweight intrusion detection model that is computationally efficient and effective based on feature selection and maximum entropy (ME) model. Firstly, the issue of identifying important input features is addressed. Since elimination of the insignificant and/or useless inputs leads to a simplification of the problem, therefore results to faster and more accurate detection. Secondly, classic ME model is used to learn and detect intrusions using the selected important features. Experimental results on the well-known KDD 1999 dataset show the proposed model is effective and can be applied to real-time intrusion detection environments.",
Distributed Control of Robot Functions using RT Middleware,"In the ubiquitous robot system, various embedded elements (ex. sensors, actuators, and computers) are distributed and connected each other over network. Ubiquitous Functions Activation Module (UFAM) has been developed in our research group as one of key device for ubiquitous robots. UFAM can add wireless communication function into embedded elements. UFAM has a relatively low performance micro processor, therefore light weight middleware is needed for improving reusability. RT component lite(RTC-Lite) has been developed as light weighted RT component. In this paper, the application of RTC-Lite to UFAM is discussed. Some modifications of RTC-Lite are required for handling UFAM protocol by RTC-Lite. We discuss about this modification, especially","Distributed control,
Middleware,
Robot sensing systems,
Sensor systems,
Actuators,
Computer networks,
Distributed computing,
Embedded computing,
Pervasive computing,
Wireless communication"
Mean-Variance-Skewness-Kurtosis-based Portfolio Optimization,"In the mean-variance-skewness-kurtosis framework, this study solve multiple conflicting and competing portfolio objectives such as maximizing expected return and skewness and minimizing risk and kurtosis simultaneously, by construction of a polynomial goal programming (PGP) model into which investor preferences over higher return moments are incorporated. To examine its practicality, the approach is tested on four major stock indices. Empirical results indicate that, for all examined investor preferences and stock indices, the PGP approach is significantly efficient way to solve multiple conflicting portfolio objectives in the mean-variance-skewness-kurtosis framework. In the meantime, we find that the different investors' preferences not only affect asset allocations of portfolio, but also affect the four moment statistics of return",
Blind Video Watermarking for H.264,"Blind watermarking method is proposed to evaluate data embedding techniques for H.264 video signals with robustness against various signal processing functions. H.264 is a new advanced standard. The applications of video on Internet or wireless networks become very popular nowadays. However, these digital contents can be easily modified and copied by end users. Hence copyright protection, copy protection and integrity verification have become important issues in recent years. In the proposed system, block polarity and index modulation are used to achieve blind watermark detection. The block polarity is determined based on the nonzero quantized DC coefficient in each 4times4 integer DCT block. The block index is the pseudo-quantized block activity that is represented by the sum of magnitude of quantized AC coefficients. The block index modulation is performed based on the watermark and the blocks polarity. The watermark embedding is performed by the index modulation, which modifies the quantized AC coefficient values by a small amount to force the activity to be quantized into a specific region. Simulation results show that the proposed method performs well, and extracts embedding watermark without the original video signal. Additionally, the algorithm is not very complex and appropriate for real-time applications",
Stabilizing reconfiguration in wireless sensor networks,"A commonly desired feature of large-scale, multi-hop, wireless sensor networks (WSNs) is the ability to reconfigure them after deployment. This reconfiguration could be as simple as changing a single parameter or as complex as replacing the entire application. Several protocols have been proposed to enable reconfiguration in WSNs, most of which use version numbers to distinguish new configurations from old ones. Due to constraints on memory and message sizes, version numbers are bounded and use wraparound arithmetic to handle rollover. While this simple scheme works well in the common case, we identify in this paper, a serious version management problem in existing protocols due to which a reconfiguration operation may never stabilize. We analyze potential causes of this problem and its effects on the quality and lifetime of the network. Through extensive simulations and experiments, we demonstrate the significant likelihood of this problem occurring in real deployments. Finally, we provide a solution to this problem using a novel approach which we call human-in-the-loop stabilization. Our stabilizing reconfiguration protocol uses local detectors and correctors that can detect version inconsistencies and prevent their propagation in a timely and efficient manner, while ultimately allowing the human operator to restore the network to the correct configuration. Our simulations and experiments also demonstrate the performance benefits of our solution over previous, non-stabilizing protocols","Intelligent networks,
Wireless sensor networks,
Protocols,
Arithmetic,
Hardware,
Computer science,
Large-scale systems,
Spread spectrum communication,
Memory management,
Cause effect analysis"
Performance of the OFDM-based Transform Domain Communication System in Cognitive Radio contexts,"In this paper, performance in term of BER of the OFDM-based TDCS is investigated in realistic CR contexts. According to the analyses and simulation results, it is observed that the OFDM-based TDCS operates better in random spectrum availability contexts than in continuous spectrum availability ones. Geometrical explanations are provided to give some insights to this phenomenon. Based on the explications, a new interleaved OFDM-based TDCS is proposed. Computer simulation results verify that the new proposal outperforms the OFDM-based TDCS without an interleaver, especially in the scenario where the percentage of available spectrum is rather low",
A model transformation approach for design pattern evolutions,"The evolution of a design pattern typically involves the addition or removal of a group of modeling elements, such as classes, attributes, operations, and relationships. However, the possible evolutions of each design pattern are often not explicitly documented. Missing part of the evolution process may result in inconsistent evolution. In this paper, we define the evolution processes of design patterns in terms of two-level transformations, thus making the possible evolutions of each design pattern explicit. In addition, we automate the evolution processes as XSLT transformations that can transform the UML model of a design pattern application into the evolved UML model of the pattern. Both the original and evolved UML models are represented in the XML metadata interchange (XMI) format to facilitate the transformations. Furthermore, we check the consistency of the evolution results using the Java Theorem Prover",
A Review on Vision-Based Pedestrian Detection for Intelligent Vehicles,"Vision-based pedestrian detection techniques for smart vehicles have emerged as a hot research topic in the field of vehicular electronics and driving safety. A vision-based system can recognize pedestrians in front of the moving vehicle, then warns the driver of the dangerous situation loudly or slows the vehicle down automatically to protect both drivers and pedestrians. In general, the vision-based pedestrian detection process can be divided into three consecutive steps: pedestrian detection, pedestrian recognition, and pedestrian tracking. In this paper, a great variety of methods associated with these three steps is introduced and compared in detail. In addition, the implementation of vision-based pedestrian detection on vehicles is also presented. In the end, we analyze the difficulties and the research trend in the future.","Vehicle detection,
Intelligent vehicles,
Vehicle driving,
Vehicle safety,
Driver circuits,
Protection,
Road accidents,
Radar detection,
Vehicular and wireless technologies,
Microwave sensors"
A Simulation Analysis of Multicasting in Delay Tolerant Networks,"Delay tolerant networks (DTNs) are a class of systems that experience frequent and long-duration partitions. As in all distributed systems, DTN multicasting is a desirable feature for applications where some form of group communication is needed. The topological impairments experienced within a DTN pose unique challenges for designing effective DTN multicasting protocols. In this paper, we examine multicasting in DTNs. Unlike earlier work we assume no knowledge of node connectivity or mobility patterns. We propose the use of both single-copy and multi-copy routing DTN routing algorithms. We also explore the use of gossiping and core nodes in DTNs to decrease the number of redundant messages while maintaining high message delivery ratios. We have performed extensive evaluations of our proposed methods. Our results show that with careful protocol parameter selection it is possible to achieve high delivery rates for various system scenarios","Analytical models,
Disruption tolerant networking,
Routing,
Mobile ad hoc networks,
Delay,
Spraying,
Computational modeling,
Mobile communication,
History,
Computer simulation"
"See Me, Teach Me: Facial Expression and Gesture Recognition for Intelligent Tutoring Systems","Many software systems would significantly improve performance if they could adapt to the emotional state of the user, for example if intelligent tutoring systems, ATM's and ticketing machines could recognise when users were confused, frustrated or angry they could provide remedial help so improving the service. This paper presents research leading to the development of Easy with Eve, an affective tutoring systems (ATS) for mathematics. The system detects student emotion, adapts to students and displays emotion via a lifelike agent called Eve. Eve's is guided by a case-based system which uses data that was generated by an observational study. This paper presents the observational study, the case-based method, and the ATS",
Buffer sizing theory for bursty TCP flows,"In a router serving many TCP flows, queues will build up from time to time. The manner in which queues build up depends on the buffer space available and on the burstiness of the TCP traffic. Conversely, the traffic generated by a TCP flow depends on the congestion it sees at queues along its route. In order to decide how big buffers should be, we need to understand the interaction between these effects. This paper reviews the buffer-sizing theory in G. Raina and D. Wischik (2005) and extends it to cope with bursty TCP traffic. This enables us to explain an observation about TCP pacing made in A. Aggarwal et al. (2000)","Traffic control,
Fluctuations,
Telecommunication traffic,
Throughput,
Mathematical model,
Computer science,
Queueing analysis,
Internet,
Stress,
Bandwidth"
Automated Global Structure Extraction for Effective Local Building Block Processing in XCS,"Learning Classifier Systems (LCSs), such as the accuracy-based XCS, evolve distributed problem solutions represented by a population of rules. During evolution, features are specialized, propagated, and recombined to provide increasingly accurate subsolutions. Recently, it was shown that, as in conventional genetic algorithms (GAs), some problems require efficient processing of subsets of features to find problem solutions efficiently. In such problems, standard variation operators of genetic and evolutionary algorithms used in LCSs suffer from potential disruption of groups of interacting features, resulting in poor performance. This paper introduces efficient crossover operators to XCS by incorporating techniques derived from competent GAs: the extended compact GA (ECGA) and the Bayesian optimization algorithm (BOA). Instead of simple crossover operators such as uniform crossover or one-point crossover, ECGA or BOA-derived mechanisms are used to build a probabilistic model of the global population and to generate offspring classifiers locally using the model. Several offspring generation variations are introduced and evaluated. The results show that it is possible to achieve performance similar to runs with an informed crossover operator that is specifically designed to yield ideal problem-dependent exploration, exploiting provided problem structure information. Thus, we create the first competent LCSs, XCS/ECGA and XCS/BOA, that detect dependency structures online and propagate corresponding lower-level dependency structures effectively without any information about these structures given in advance.",
Multiprocessor Synthesis for Periodic Hard Real-Time Tasks under a Given Energy Constraint,"The energy-aware design for electronic systems has been an important issue in hardware and/or software implementations, especially for embedded systems. This paper targets a synthesis problem for heterogeneous multiprocessor systems to schedule a set of periodic real-time tasks under a given energy consumption constraint. Each task is required to execute on a processor without migration, where tasks might have different execution times on different processor types. Our objective is to minimize the processor cost of the entire system under the given timing and energy consumption constraints. The problem is first shown being NP-hard and having no polynomial-time algorithm with a constant approximation ratio unless NP = P. We propose polynomial-time approximation algorithms with (m + 2)-approximation ratios for this challenging problem, where m is the number of the available processor types. Experimental results show that the proposed algorithms could always derive solutions with system costs close to those of optimal solutions",
Rapid Multi-modality preRegistration based on SIFT descriptor,"This paper describes the scale invariant feature transform (SIFT) method for rapid preregistration of medical image. This technique originates from Lowe's method wherein preregistration is achieved by matching the corresponding keypoints between two images. The computational complexity has been reduced when we applied SIFT preregistration method before refined registration due to its O(n) exponential calculations. The features of SIFT are highly distinctive and invariant to image scaling and rotation, and partially invariant to change in illumination and contrast, it is robust and repeatable for cursorily matching two images. We also altered the descriptor so our method can deal with multimodality preregistration",
Charge Sharing Study in the Case of Neutron Induced SEU on 130 nm Bulk SRAM Modeled by 3-D Device Simulation,The charge sharing quantification in the case of neutron induced SEUs in a 130 nm bulk SRAM is presented. Conclusions on its contribution to the soft errors sensitivity evaluation using Monte-Carlo codes are underlined,"Computer aided software engineering,
Neutrons,
Random access memory,
Research and development,
Atmospheric modeling,
Alpha particles,
Error analysis,
Semiconductor device reliability,
Consumer electronics,
Electrons"
Face recognition with image sets using hierarchically extracted exemplars from appearance manifolds,"An unsupervised nonparametric approach is proposed to automatically extract representative face samples (exemplars) from a video sequence or an image set for multiple-shot face recognition. Motivated by a nonlinear dimensionality reduction algorithm called Isomap, we use local neighborhood information to approximate the geodesic distances between face images. A hierarchical agglomerative clustering (HAC) algorithm is then applied to group similar faces together based on the estimated geodesic distances which approximate their locations on the appearance manifold. We define the exemplars as cluster centers for template matching at the subsequent testing stage. The final recognition is the outcome of a majority voting scheme which combines the decisions from all the individual frames in the test set. Experimental results on a 40-subject video database demonstrate the effectiveness and flexibility of our proposed method",
A Novel Algorithm for Decapsulation and Decoding of DVB-H Link Layer Forward Error Correction,"DVB-H, which is an amendment of DVB-T, offers reliable high data rate reception for mobile handheld and battery-powered devices. A link layer with error correction is defined to work on top of the DVB-T physical layer. The DVB-H standard suggests to use Reed-Solomon coding combined with cyclic redundancy check error detection as the link layer forward error correction. Drawbacks of the decoding solution have been recognized in previous research. This paper presents a novel algorithm for decapsulation and decoding at DVB-H link layer. The algorithm is based on information provided in the transport stream packet headers and requires no changes to the DVB-H standard.","Decoding,
Digital video broadcasting,
Forward error correction,
Cyclic redundancy check,
Reed-Solomon codes,
Telecommunication standards,
OFDM,
Physical layer,
Error correction,
Receivers"
Learning-Based SMT Processor Resource Distribution via Hill-Climbing,"The key to high performance in simultaneous multithreaded (SMT) processors lies in optimizing the distribution of shared resources to active threads. Existing resource distribution techniques optimize performance only indirectly. They infer potential performance bottlenecks by observing indicators, like instruction occupancy or cache miss counts, and take actions to try to alleviate them. While the corrective actions are designed to improve performance, their actual performance impact is not known since end performance is never monitored. Consequently, potential performance gains are lost whenever the corrective actions do not effectively address the actual bottlenecks occurring in the pipeline. We propose a different approach to SMT resource distribution that optimizes end performance directly. Our approach observes the impact that resource distribution decisions have on performance at runtime, and feeds this information back to the resource distribution mechanisms to improve future decisions. By evaluating many different resource distributions, our approach tries to learn the best distribution over time. Because we perform learning on-line, learning time is crucial. We develop a hill-climbing algorithm that efficiently learns the best distribution of resources by following the performance gradient within the resource distribution space. This paper conducts an in-depth investigation of learning-based SMT resource distribution. First, we compare existing resource distribution techniques to an ideal learning-based technique that performs learning off-line. This limit study shows learning-based techniques can provide up to 19.2% gain over ICOUNT, 18.0% gain over FLUSH, and 7.6% gain over DCRA across 21 multithreaded workloads. Then, we present an on-line learning algorithm based on hill-climbing. Our evaluation shows hill-climbing provides a 12.4% gain over ICOUNT, 11.3% gain over FLUSH, and 2.4% gain over DCRA across a larger set of 42 multiprogrammed workloads","Surface-mount technology,
Resource management,
Yarn,
Performance gain,
Multithreading,
Hardware,
Computer science,
Monitoring,
Pipelines,
Runtime"
Internet Cache Pollution Attacks and Countermeasures,"Proxy caching servers are widely deployed in today's Internet. While cooperation among proxy caches can significantly improve a network's resilience to denial-of-service (DoS) attacks, lack of cooperation can transform such servers into viable DoS targets. In this paper, we investigate a class of pollution attacks that aim to degrade a proxy's caching capabilities, either by ruining the cache file locality, or by inducing false file locality. Using simulations, we propose and evaluate the effects of pollution attacks both in Web and peer-to- peer (p2p) scenarios, and reveal dramatic variability in resilience to pollution among several cache replacement policies. We develop efficient methods to detect both false-locality and locality-disruption attacks, as well as a combination of the two. To achieve high scalability for a large number of clients/requests without sacrificing the detection accuracy, we leverage streaming computation techniques, Le., bloom filters. Evaluation results from large-scale simulations show that these mechanisms are effective and efficient in detecting and mitigating such attacks. Furthermore, a squid-based implementation demonstrates that our protection mechanism forces the attacker to launch extremely large distributed attacks in order to succeed.","Internet,
Pollution,
Network servers,
Web server,
Resilience,
Computer crime,
Degradation,
Scalability,
Filters,
Large-scale systems"
Assessing the accuracy of non-rigid registration with and without ground truth,"We compare two methods for assessing the performance of groupwise non-rigid registration algorithms. The first approach, which has been described previously, utilizes a measure of overlap between ground-truth anatomical labels. The second, which is new, exploits the fact that, given a set of nonrigidly registered images, a generative statistical model of appearance can be constructed. We observe that the quality of this model depends on the quality of the registration, and define measures of model specificity and generalisation - based on comparing synthetic images sampled from the model, with those in the original image set - that can be used to assess model/registration quality. We show that both approaches detect the loss of registration accuracy as the alignment of a set of correctly registered MR images of the brain is progressively perturbed. We compare the sensitivities of the two approaches and show that, as well as requiring no ground truth, specificity provides the most sensitive measure of misregistration","Biomedical imaging,
Biomedical measurements,
Fuzzy sets,
Volume measurement,
Biomedical engineering,
Biomedical computing,
Computer science,
Educational institutions,
Image generation,
Image analysis"
Improving BGP Convergence Delay for Large-Scale Failures,"Border gateway protocol (BGP) is the standard routing protocol used in the Internet for routing packets between the autonomous systems (ASes). It is known that BGP can take hundreds of seconds to converge after isolated failures. We have also observed that the convergence delay can be even greater for large-scale failures. In this study, we first investigate some of the factors affecting the convergence delay and their relative impacts. We observe that the minimum route advertisement interval (MRAI) and the processing overhead at the routers during the re-convergence have a significant effect on the BGP recovery time. We propose a couple of new schemes to reduce processing overload at BGP routers during large failures, which in turn leads to decreased convergence delays. We show that these schemes combined with the tuning of the MRAI value decrease the BGP convergence delay significantly, and can thus limit the impact of large scale failures in the Internet","Convergence,
Large-scale systems,
Routing protocols,
Computer science,
Delay estimation,
Failure analysis,
Fault tolerance,
Web and internet services,
Computer hacking,
Terrorism"
WLC10-4: Performance Measures of Dynamic Spectrum Access Networks,"In this paper we give insight into the performance of Dynamic Spectrum Access Networks (DSAN), analyzing Quality of Detection of Primary User (PU) and DSAN blocking probability, when the channel is under the effect of log-normal shadowing. Specifically we propose a distributed power conserving PU detection architecture and investigate the impact of PU detection accuracy on DSAN performance. We measure DSAN blocking probability as a function of the number of PU channels and their utilization. Finally we propose two efficient DSAN channel access schemes called Least-used and Least-used with Channel Hopping which aim at minimizing packet dropping due to the arrival of the PU.",
Bringing Relational Data into the SemanticWeb using SPARQL and Relational.OWL,"Despite all the efforts to build up a Semantic Web, where each machine can understand and interpret the data it processes, information is usually still stored in ordinary relational databases. Semantic Web applications needing access to such semantically unexploited data, have to create their own manual relational database to Semantic Web mappings. In this paper we analyze, whether the combination of Relational.OWL as a Semantic Web representation of relational databases and a semantic query language like SPARQL could be an alternative. The benefits of such an approach are clear, since it enables Semantic Web applications to access and query data actually stored in relational databases using their own built-in functionality.",
Algorithmic approaches for genome rearrangement: a review,"Genome rearrangement is a new and important research area that studies the gene orders and the evolution of gene families. With the development of fast sequencing techniques, large-scale DNA molecules are investigated with respect to the relative order of genes in them. Contrary to the traditional alignment approach, genome rearrangements are based on comparison of gene orders. Recently, it became a topic capturing wide attention. In this paper, we cover many kinds of rearrangement events such as reversal, transposition, translocation, fussion, fission, and so on. Different types of distances between genomes or chromosomes are discussed. A variety of mathematic models are included",
Energy-Aware Duplication Strategies for Scheduling Precedence-Constrained Parallel Tasks on Clusters,"Optimizing energy consumption has become a major concern in designing economical clusters. Scheduling precedence-constrained parallel tasks on clusters is challenging because of high communication overhead. Although duplication-based strategies are applied to minimize communication overhead, most of them merely consider schedule lengths, completely ignoring energy consumption of clusters. In this regard, we propose two energy-aware duplication scheduling algorithms, called EADUS and TEBUS, to schedule precedence-constrained parallel tasks. Unlike existing duplication-based scheduling algorithms that replicate all possible predecessors of each task, the proposed algorithms judiciously replicate predecessors only if the duplication can help in conserving energy. Our energy-aware scheduling strategies are conducive to balancing the scheduling length and energy consumption of precedence-constrained parallel tasks. Extensive experimental results based on real-world applications demonstrate the effectiveness and practicality of the proposed scheduling strategies","Scheduling algorithm,
Energy consumption,
Processor scheduling,
Clustering algorithms,
Energy conservation,
Design optimization,
Power generation economics,
Switches,
Power system interconnection,
Power system modeling"
Scalable Design and Implementations for MPI Parallel Overlapping I/O,"We investigate the message passing interface input/output (MPI I/O) implementation issues for two overlapping access patterns: the overlaps among processes within a single I/O operation and the overlaps across a sequence of I/O operations. The former case considers whether I/O atomicity can be obtained in the overlapping regions. The latter focuses on the file consistency problem on parallel machines with client-side file caching enabled. Traditional solutions for both overlapping I/O problems use whole file or byte-range file locking to ensure exclusive access to the overlapping regions and bypass the file system cache. Unfortunately, not only can file locking serialize I/O, but it can also increase the aggregate communication overhead between clients and I/O servers. For atomicity, we first differentiate MPI's requirements from the portable operating system interface (POSIX) standard and propose two scalable approaches, graph coloring and process-rank ordering, which can resolve access conflicts and maintain I/O parallelism. For solving the file consistency problem across multiple I/O operations, we propose a method called persistent file domains, which tackles cache coherency with additional information and coordination to guarantee safe cache access without using file locks","Message passing,
Parallel machines,
File systems,
Aggregates,
File servers,
Operating systems,
Parallel processing,
Programming,
Hardware,
Bandwidth"
An alternate line erasure and readout (ALER) method for implementing slot-scan imaging technique with a flat-panel detector-initial experiences,"This paper describes and demonstrates an electronic collimation method, referred to as the alternate line erasure and readout (ALER) technique, for implementing slot-scan digital radiography technique with an amorphous silicon (a-Si) thin-film transistor (TFT) array based flat-panel detector. An amorphus selenium (a-Se) flat-panel detector was modified to implement the ALER technique for slot-scan imaging. A stepping-motor driven fore-collimator was mounted in front of an X-ray tube to generate a scanning X-ray fan beam. The scanning speed and magnification were adjusted to synchronize the fan beam motion with the image line readout rate. The image lines on the leading and trailing edges of the fan beam were tracked and alternately reset and read out, respectively. The former operation resulted in the erasure of the scatter signals accumulated in the leading edge image line prior to the arrival of the fan beam. The latter operation resulted in the acquisition of fan beam exposure data integrated in the trailing edge image line right after the fan beam passed. To demonstrate the scatter rejection capability of this technique, an anthropomorphic chest phantom was placed in PA position and scanned at a speed of 576 lines (8.0 cm)/s at 117 kVp and 32 mA. A tungsten bar is placed at the entrance side of the chest phantom to measure the scatter-to-primary ratio (SPR), scatter reduction factor (SRF), and contrast-to-noise ratio degradation factor (CNRDF) in the slot-scan images to evaluate the effectiveness of scatter rejection and the resultant improvement of image quality. SPR and CNRDF in the open-field images were also measured and used as the reference for comparison. A scatter reduction by 86.4 to 95.4% across lower lung and heart regions has been observed with slot-scan imaging. The CNRs have been found to be improved by a factor of 2 in the mediastinum areas over the open-field image as well.",
A Condition-based Intra Prediction Algorithm for H.264/AVC,"This paper proposes a condition-based algorithm for H.264/AVC 4times4 intra prediction. Exploiting high correlation existed in neighboring intra prediction modes, we propose the three conditions to skip the less possible candidates in doing intra4times4 block mode decision. When compared to the 9 prediction modes in the full search algorithm, the proposed algorithm can complete a 4times4 intra prediction using 4.4 prediction modes operation in average. The simulation result shows that the proposed algorithm can reduce computational complexity up to 44% at the cost of less than 0.1 dB PSNR loss in average",
Expediting GA-Based Evolution Using Group Testing Techniques for Reconfigurable Hardware,"Autonomous repair and refurbishment of reprogrammable logic devices using genetic algorithms can improve the fault tolerance of remote mission-critical systems. The goal of increasing availability by minimizing the repair time is addressed in this paper using a CGT-pruned genetic algorithm. The proposed method utilizes resource performance information obtained using combinatorial group testing (CGT) techniques to evolve refurbished configurations in fewer generations than conventional genetic algorithms. A 3-bit times 2-bit multiplier circuit was evolved using both conventional and CGT-pruned genetic algorithms. Results show that the new approach yields completely refurbished configurations 37.6% faster than conventional genetic algorithms. In addition it is demonstrated that for the same circuit, refurbishment of partially-functional configurations is a more tractable problem than designing the configurations when using genetic algorithms as results show the former to take 80% fewer generations","Hardware,
Circuit faults,
Genetic algorithms,
Field programmable gate arrays,
Logic testing,
Fault tolerant systems,
Medical tests,
Mission critical systems,
Availability,
Circuit testing"
Efficient instruction schedulers for SMT processors,"We propose dynamic scheduler designs to improve the scheduler scalability and reduce its complexity in the SMT processors. Our first design is an adaptation of the recently proposed instruction packing to SMT. Instruction packing opportunistically packs two instructions (possibly from different threads), each with at most one non-ready source operand at the time of dispatch, into the same issue queue entry. Our second design, termed 2OP/spl I.bar/BLOCK, takes these ideas one step further and completely avoids the dispatching of the instructions with two non-ready source operands. This technique has several advantages. First, it reduces the scheduling complexity (and the associated delays) as the logic needed to support the instructions with 2 non-ready source operands is eliminated. More surprisingly, 2OP/spl I.bar/BLOCK simultaneously improves the performance as the same issue queue entry may be reallocated multiple times to the instructions with at most one non-ready source (which usually spend fewer cycles in the queue) as opposed to hogging the entry with an instruction which enters the queue with two non-ready sources. For the schedulers with the capacity to hold 64 instructions, the 2OP/spl I.bar/BLOCK design outperforms the traditional queue by 11%, on the average, and at the same time results in a 10% reduction in the overall scheduling delay.",
An Architecture of Process-centered Context-aware Software Development Environment,"Software development is considered to be a kind of collaborative activity now. In this paper, we first analyze the software development activity using activity theory, and highlight its collaborative features. We then propose an architecture of process-centered context-aware software development environment, CASDE, which fully considers the key elements of PCSDEs, especially the context element. The supportive and integrated nature of the environment is emphasized in CASDE. As illustrated in activity theory, the architecture can support the three levels of collaboration, i.e., co-ordinated, co-operative, and co-constructive level. In particular, the co-operative level is supported sufficiently by the introduction of context model and context process mechanism. Based on the architecture, the software development activity can be more collaborative and quality of software system can be improved",
Sieve: A Tool for Automatically Detecting Variations Across Program Versions,"Software systems often undergo many revisions during their lifetime as new features are added, bugs repaired, abstractions simplified and refactored, and performance improved. When a revision, even a minor one, does occur, the changes it induces must be tested to ensure that invariants assumed in the original version are not violated unintentionally. In order to avoid testing components that are unchanged across revisions, impact analysis is often used to identify code blocks or functions that are affected by a change. In this paper, we present a novel solution to this general problem that uses dynamic programming on instrumented traces of different program binaries to identify longest common subsequences in strings generated by these traces. Our formulation allows us to perform impact analysis and also to detect the smallest set of locations within the functions where the effect of the changes actually manifests itself. Sieve is a tool that incorporates these ideas. Sieve is unobtrusive, requiring no programmer or compiler intervention to guide its behavior. Our experiments on multiple versions of op ensource C programs shows that Sieve is an effective and scalable tool to identify impact sets and can locate regions in the affected functions where the changes manifest. These results lead us to conclude that Sieve can play a beneficial role in program testing and software maintenance",
Fast Distance Preserving Level Set Evolution for Medical Image Segmentation,"Accurate and fast image segmentation algorithms are of paramount importance for a wide range of medical imaging applications. Level set algorithms based on narrow band implementation have been among the most widely used segmentation algorithms. However, the accuracy of standard level set algorithms is compromised by the fact that their evolution schemes deteriorate the signed distance level set functions required for accurate computation of normals and curvatures. The most common remedy is to use an ad-hoc reinitialization step to rebuild the signed distance function frequently. Meanwhile, complex upwind finite difference schemes are required for stable evolution. They together make the overall computation expensive. In this paper, we propose a novel fast narrow band distance preserving level set evolution algorithm that eliminates the need for both reinitialization and complex upwind finite difference schemes. This is achieved by incorporating into a variational level set formulation with a signed distance preserving term that regularizes the evolution. As a result, stable, accurate, fast evolution could be obtained using a simple finite difference scheme within a very narrow band, defined as the union of all 3times3 pixel blocks around the zero crossing pixels. Also, our method allows the use of larger time step to speed up the convergence while ensuring accurate result, as well as the use of more general and computational efficient initial level set functions rather than the signed distance functions required by standard level set methods. The proposed algorithm has been applied on both synthetic and real images of different modalities with promising results",
Fast 3D Iterative Reconstruction of PET Images Using PC Graphics Hardware,"Using iterative reconstruction algorithms in 3D positron emission tomography (PET) studies produce images with superior quality, however the run time is too long for these algorithms to be used routinely, especially for dynamic studies. Recently several new hardware architectures are available to speedup 3D iterative reconstructions, including the graphics processing unit (GPU), which is very attractive due to its fast performance improvement and low cost. In this study, we investigate the possibility of implementing fast iterative 3D PET reconstruction using commercial graphics hardware. A 3D ordered subset expectation maximization (OSEM) algorithm was implemented and a GPU used to calculate the geometric forward and back projections using a line-integral model. Results show that the GPU can be used for fast 3D iterative PET image reconstructions. Future work would include exploring the possibility of implementing more accurate geometric models on the GPU.",
Resource control for the EDCA mechanism in multi-rate IEEE 802.11e networks,"We investigate the problem of efficient resource control for elastic traffic in IEEE 802.11e's enhanced distributed channel access (EDCA) mechanism. Our approach considers an economic modelling framework based on congestion pricing that captures how various factors, such as the probability of attempting to transmit a frame, the use of the basic CSMA/CA or the RTS/CTS procedure, and the physical layer transmission rate, contribute to congestion. We discuss the application of the framework for achieving class-based throughput differentiation, for performing explicit congestion notification (ECN) marking based on the level of congestion in the wireless channel, and for modelling the performance of TCP congestion control over EDCA. In these application scenarios we discuss how to estimate the optimum minimum contention window and the congestion prices based on the 802.11e parameters and actual measurements, in order to achieve efficient channel utilization",
Self-Optimization as a Framework for Advanced Control Systems,"Optimization is a usual step of control design. To do so, clear design goals have to be defined and sufficient system information must be given as prerequisites. However, data are often inaccurate or missing and design goals may change during operation. That is why a concept of self-optimizing systems is proposed, which is able to optimize the system even during operation. The proposed concept should be understood as a framework to incorporate various control and optimization methods. A key element of the proposal is the operator controller module, which consists of a cognitive part for planning tasks with lower real-time requirements and a reflective part for the execution level. A particular focus is given to how to handle multi-objective optimization with on-line adaptation of the objectives depending on internal and external design goals. Examples how to employ the concept in practice are given","Control systems,
Adaptive control,
Design optimization,
Control design,
Optimization methods,
Proposals,
Control theory,
Programmable control,
Optimal control,
Computer science"
Extracting Users' Interests from Web Log Data,Analyzing users' Web log data and extracting their interests of Web-watching behaviors are important and challenging research topics of Web usage mining. Users visit their favorite sites and sometimes search new sites by performing keyword search on search engines. Users' Web-watching behaviors can be regarded as a graph since visited Web sites and entered search keywords are connected with each other in a time sequence. We call this graph a site-keyword graph. This paper describes a method for clarifying users' interests based on an analysis of the site-keyword graph. The method is for extracting subgraphs representing users' main interests from a site-keyword graph which is generated from augmented Web audience measurement data (Web log data). Experimental results show that our new method succeeds in finding subgraphs which contain most of the sites that users are interested in,"Data mining,
Uniform resource locators,
Search engines,
Microcomputers,
Computer science,
Keyword search,
Electric shock,
Time measurement,
Data analysis,
Pattern analysis"
Detection and Removal of Long Scratch Lines in Aged Films,"Historical films usually have defects. We study the type of defects, and propose a series of solutions to detect defects before they are repaired by our inpainting algorithms. This paper focuses on a difficult issue to locate long vertical line defects in aged films. A progressive detection algorithm is proposed. We are able to detect more than 86% (recall rate) of effective line defects. These line defects are then removed step by step. The experiments use real historical video collected from national museum and public channel, instead of using computer generated noise. The results are visually pleasant based on our subjective evaluation by volunteers",
Tensor Discriminant Analysis for View-based Object Recognition,"In this paper, we use a general Mth order tensor discriminant analysis approach (Tao et al. 2005) for view based object recognition. This method is an extension of the 2D image coding technique (Shashua and Levin, 2001) to general Mth order tensors for discriminant analysis, and has good convergence property. We demonstrate the performance advantages of this approach over existing techniques using experiments on the COIL-100 and the ETH-80 datasets. Specifically, our experimental results on ETH-80 show the particular strength of this tensor discriminant analysis method when only a small number of training samples with big intra-class variation are available",
Energy-Efficient Sleep-Mode Operations for Broadband Wireless Access Systems,"Broadband wireless access systems usually provide flexible sleep-mode operations for mobile stations to conserve energy during idle or active modes. For example, the IEEE 802.16e offers several sleep-mode classes that can be applied to mobile subscriber stations (MSSs) to reduce their power consumptions for different applications. Unfortunately, previous studies did not fully utilize sleep-mode features, and the power consumption of an MSS with multiple network connections is not optimized. In this work, two scheduling algorithms for sleep-mode operations based on the IEEE 802.16e are proposed. These schemes guarantee the quality of services (QoSs) of the network connections for an MSS and also minimize the power consumption of the MSS. Simulation results demonstrate that the proposed schemes outperform the traditional approaches.","Energy efficiency,
Energy consumption,
Switches,
Wireless LAN,
Packet switching,
Computer science,
Mobile computing,
Scheduling algorithm,
Quality of service,
Analytical models"
Evaluation of Android Using Unconscious Recognition,"In human-robot interaction, both appearance and motion are essential aspects of the robot. We study human-robot interaction using an android that has human-like appearance. Mori (1970) hypothesized the 'uncanny valley' which describes a relationship between appearance of a robot and the feeling it produces in humans. To design robots that interact successfully with humans, we must know the structure of the uncanny valley. Humans show unconscious behaviors when interacting with another human. We expect the same behaviors when interacting with a very human-like robot. Then we can change appearance and motion to study how the unconscious behavior of the human changes. In this way, we explore the uncanny valley. The unconscious behavior we consider in this study is gaze behavior. It has been found (McCarthy et al., 2001 and 2003) that eye movements are used to send social signals during conversation. In particular, when thinking about the answer to a question, humans tend to look away from the questioner. We hypothesize that if the human-likeness of the questioner changes, this gaze behavior also changes. We compare three different questioners: human, android and robot with a 'mechanical' appearance. We found that the subject changes gaze to the left of the face a longer time in case of a human or android questioner. The subject changes gaze to look down from the face in the case of a mechanical robot questioner. There is a significant difference between these two behaviors. We conclude that the android questioner is unconsciously treated in the same way as the human questioner. The mechanical robot is treated differently from the human questioner. These results will become clues to the uncanny valley and contribute to the progress of human-robot communication",
Personalized Hierarchical Clustering,"A hierarchical structure can provide efficient access to information contained in a collection of documents. However, such a structure is not always available, e.g. for a set of documents a user has collected over time in a single folder or the results of a Web search. We therefore investigate in this paper how we can obtain a hierarchical structure automatically, taking into account some background knowledge about the way a specific user would structure the collection. More specifically, we adapt a hierarchical agglomerative clustering algorithm to take into account user specific constraints on the clustering process. Such an algorithm could be applied, e.g., for user specific clustering of Web search results, where the user's constraints on the clustering process are given by a hierarchical folder or bookmark structure. Besides the discussion of the algorithm itself, we motivate application scenarios and present an evaluation of the proposed algorithm on benchmark data","Clustering algorithms,
Web search,
Computer science,
Cultural differences,
Search engines,
Information retrieval,
Clustering methods,
Web pages,
Libraries,
Catalogs"
ROSETTA: Robust And Secure Mobile Target Tracking In A Wireless Ad Hoc Environment,"In this paper, we study the problem of accurate tracking of a mobile target by a central authority, using distance estimates obtained by a group of untrusted anchors within the communication range of the target. We show how to perform accurate localization of the target in the presence of some compromised and colluding malicious anchors that lie about the position of the target. We also show how to identify most of these malicious anchors. In the case where measurements are error-free, we derive an upper bound (B) on the number of malicious anchors that may be involved in localizing the target while still not being able to undermine its accurate localization. We propose a scheme to correctly localize the target when the number of malicious anchors within its range is no more than B. It also identifies all the malicious anchors. In the presence of positive measurement errors, we propose a scheme based on convex optimization that can localize the target despite the presence of an arbitrary number of malicious anchors in its range. When the number of malicious anchors are no more than B, our scheme localizes the target with an error less than 1m and is also able to identify more than 80% of the malicious anchors. Both our schemes are simple and easy to implement.",
Energy-Aware QoS Control for Wireless Sensor Network,"While a lot of research has been done on some important aspects of WSN such as architecture and protocol design, supporting quality of service in WSN is still a largely unexplored research field. An application-specific QoS has been defined as the sensor network resolution. The mathematical Gur game algorithm is used to achieve optimal active sensor number. In this paper, we design a novel energy-aware algorithm based on the previous work to solve the QoS problems. The periodical sleeping mechanism is introduced into the algorithm to save energy. When the optimal number sensors are achieved in initialization stage, not all of the remaining sleep state nodes need to wake up every second. Simulation results show our method can save more energy than the previous",
Gang scheduling and adaptive resource allocation to mitigate advance reservation impact,"Simultaneous parallel computational grid jobs require reservation by the local job schedulers to ensure allocation of matching time slots at the different sites involved. However, reservations create road blocks in the local schedule, leading to only a small percentage of reservations being tolerable. A large number of reservations typically has adverse effects on local response times and machine utilization. We have extended our SCOJO scheduler to enable advance reservations. SCOJO can perform space sharing or gang scheduling and can run as either adaptive or traditional non-adaptive variant. We show that gang scheduling is more flexible than space sharing in regards to tolerating reservations. We also show that, for space sharing and a low multiprogramming level, the adaptive variants can tolerate reservations better than the non-adaptive variants.",
Instantaneous Radar Polarimetry with Multiple Dually-polarized Antennas,"Fully polarimetric radar systems are capable of simultaneously transmitting and receiving in two orthogonal polarizations. Instantaneous radar polarimetry exploits both polarization modes of a dually-polarized radar transmitter and receiver on a pulse by pulse basis, and can improve the radar detection performance and suppress range sidelobes . In this paper, we extend the use of instantaneous radar polarimetry for radar systems with multiple dually-polarized transmit and receive antennas. Alamouti signal processing is used to coordinate transmission of Golay pairs of phase codes waveforms across polarizations and multiple antennas. The integration of multi- antenna signal processing with instantaneous radar polarimetry can further improve the detection performance, at a computational cost comparable to single channel matched filtering.",
Particle Swarm Optimization for Constrained Portfolio Selection Problems,"Constructing a portfolio of investments is one of the most significant financial decisions facing individuals and institutions, modern portfolio theory is based on a rational investor choosing the proportions of assets in a portfolio so as to minimize risk and maximize the expected return. In this paper, the constrained portfolio selection problem is studied and a heuristic algorithm based on the particle swarm optimization (PSO) is applied to solve this problem. At first, considering of some complex realistic constrains a new portfolio selection model is formulated. In addition, PSO algorithm is given to solve this new model because traditional optimization algorithms fail to work efficiently. Finally, a numerical example of a portfolio selection problem is given to illustrate our proposed effective means",
The SIMPLE Presence and Event Architecture,"Current presence systems offer only basic services that are mostly useful for dial-up environments, while event services have not seen widespread deployment. We describe the SIP-based event architecture that is being developed within the SIMPLE working group that supports richer functionality, multiple event sources and better privacy control","Protocols,
Computer architecture,
Privacy,
Humans,
Computer science,
Communication system control,
Internet,
Telephony,
Postal services,
Control systems"
Privacy Preserving C4.5 Algorithm Over Horizontally Partitioned Data,"Privacy preserving decision tree classification algorithm is to solve such a distributed computation problem that the participant parties jointly build a decision tree over the data set distributed among them, and they do not want their private sensitive data to be revealed to others during the tree-building process. The existing privacy preserving decision tree classification algorithms over the data set horizontally partitioned and distributed among different parties only can cope with the data with discrete attribute values. This paper propose a solution to privacy preserving C4.5 algorithm based on secure multi-party computation techniques, which can securely build a decision tree over the horizontally partitioned data with both discrete and continuous attribute values. Moreover, we propose a secure two-party bubble sort algorithm to solve the privacy preserving sort problem in our solution",
Optimization of Back-Propagation Network Using Simulated Annealing Approach,"The back-propagation network (BPN) is a popular data mining technique. Nevertheless, different problems may require different network architectures and parameters. Therefore, rule of thumb or ""try and error"" methods are usually used to determine them. However, these methods may lead worse network architectures and parameters. A dataset may contain many features; however, not all features are beneficial for classification in BPN. Therefore, a simulated annealing (SA) approach is proposed to select the beneficial subset of features and to obtain the better network architectures and parameters which result in a better classification. In order to verify the developed approach, three dataset, namely PIMA, IONOS, and CANCER from UCI (University of California, Irvine) machine learning database, are employed for evaluation, and the 10-fold cross-validation is applied to calculate the classification result. Compared with the MONNA (multiple ordinate neural network architecture) structure developed by Leazoray and Cardot, the classification accurate rates of the developed approach are superior to those of the MONNA. When the feature selection is taken into consideration, the classification accurate rates of three dataset are increased. Therefore, the developed approach can be utilized to find out the network architecture and parameters of BPN, and discover the useful attributes effectively.","Simulated annealing,
Information management,
Computational efficiency,
Cancer,
Support vector machines,
Support vector machine classification,
Classification tree analysis,
Circuits,
Computer architecture,
Cybernetics"
Inter-Vehicle Data Dissemination in Sparse Equipped Traffic,"We address the following question: How can data be disseminated in an inter-vehicle communications (IVC) network when the traffic of IVC-equipped vehicles is very sparse (e.g., one IVC-equipped vehicle every few minutes), using minimal extra-vehicular infrastructure? We present a data-dissemination scheme that uses wireless dead drops (dead letter boxes) as intermediaries between vehicles. These dead drops are simple transceivers with storage and are not connected to other dead drops, the Internet, or other networks. An important question is the placement of such dead drops. We formulate the corresponding optimization problem and prove that it is NP-hard. We present an optimal greedy approximation algorithm for solving the optimization problem. Our algorithm is based on mapping the dead-drop placement problem to the problem of computing a minimum-weight spanning sub-hypergraph","Telecommunication traffic,
Transceivers,
Routing,
IP networks,
Road vehicles,
Internet,
Protocols,
Investments,
Intelligent networks,
Computer science"
Detecting Coarticulation in Sign Language using Conditional Random Fields,"Coarticulation is one of the important factors that makes automatic sign language recognition a hard problem. Unlike in speech recognition, coarticulation effects in sign languages are over longer durations and simultaneously impact different aspects of the sign such as the hand shape, position, and movement. Due to this effect, the appearance of a sign, especially at the beginning and at the end, can be significantly different under different sentence contexts, which makes the recognition of signs in sentences hard. We advocate a two-step approach, where in the first step one segments the individual signs in a sentence and in the next step one recognizes the signs. In this work, we show how the first step, i.e. sign segmentation, can be performed effectively by using the conditional random fields (CRF) to directly detect the coarticulation points. The CRF approach does not make conditional independence assumptions about the observations and can be trained with fewer samples than hidden Markov models (HMMs). We validate our approach by demonstrating performance with American sign language (ASL) sentence level data and show that the CRF approach is 85% accurate in segmenting signs compared to 60% for the HMM approach at 0.1 false alarm rate",
Evaluating the Effectiveness of a Goal-Oriented Requirements Engineering Method,"As an attempt to answer the need for methods and tools in requirements engineering (RE) which are domain specific and can address the main RE objectives (REOs), and the growing interest in the goal oriented requirements engineering (GORE) approach that overcomes the inadequacy of the traditional systems analysis approaches, we systematically evaluate the KAOS method, and the Objectiver tool, using the major REOs widely accepted as being important attributes of requirements specifications. In addition, we examine whether KAOS and Objectiver meet their own selfdefined objectives. We use two target problems as a basis for the evaluation. The result of the target problems is raw data consisting of error reports and observations that support the evaluator's judgment. The evaluation itself is qualitative, not a statistical experimental evaluation. Its result will help to answer the research questions: (i) How well do KAOS and Objectiver meet the criteria established in the discipline of RE; and (ii) How well do KAOS and Objectiver achieve their own self-defined objectives.","Educational institutions,
Computer science,
Software tools,
Software systems,
Object oriented modeling,
Computer industry"
"High Quality, Efficient Hierarchical Document Clustering Using Closed Interesting Itemsets","High dimensionality remains a significant challenge for document clustering. Recent approaches used frequent itemsets and closed frequent itemsets to reduce dimensionality, and to improve the efficiency of hierarchical document clustering. In this paper, we introduce the notion of ""closed interesting"" itemsets (i.e. closed itemsets with high interestingness). We provide heuristics such as ""super item"" to efficiently mine these itemsets and show that they provide significant dimensionality reduction over closed frequent itemsets. Using ""closed interesting"" itemsets, we propose a new, sub-linearly scalable, hierarchical document clustering method that outperforms state of the art agglomerative, partitioning and frequent-itemset based methods both in terms of clustering quality and runtime performance, without requiring dataset specific parameter tuning. We evaluate twenty interestingness measures and show that when used to generate ""closed interesting"" itemsets, and to select parent nodes, mutual information, added value, Yule's Q and Chi- Square offer best clustering performance.",
MoSCoE: A Framework for Modeling Web Service Composition and Execution,"Development of sound approaches and software tools for specification, assembly, and deployment of composite Web services from independently developed components promises to enhance collaborative software design and reuse. In this context, the proposed research introduces a new incremental approach to service composition, MoSCoE (Modeling Web Service Composition and Execution), based on the three steps of abstraction, composition and refinement. Abstraction refers to the high-level description of the service desired (goal) by the user, which drives the identification of an appropriate composition strategy. In the event that such a composition is not realizable, MoSCoE guides the user through successive refinements of the specification towards a realizable goal service that meets the user requirements.",
Randomizing Functions: Simulation of a Discrete Probability Distribution Using a Source of Unknown Distribution,"In this paper, we characterize functions that simulate independent unbiased coin flips from independent coin flips of unknown bias. We call such functions randomizing. Our characterization of randomizing functions enables us to identify the functions that generate the largest average number of fair coin flips from a fixed number of biased coin flips. We show that these optimal functions are efficiently computable. Then we generalize the characterization, and we present a method to simulate an arbitrary rational probability distribution optimally (in terms of the average number of output digits) and efficiently (in terms of computational complexity) from outputs of many-faced dice of unknown distribution. We also study randomizing functions on exhaustive prefix-free sets",
Load balancing techniques for distributed stream processing applications in overlay environments,"Service overlays that support distributed stream processing applications are increasingly being deployed in wide-area environments. The inherent heterogeneous, dynamic and large-scale nature of these systems makes it difficult to meet the quality of service (QoS) requirements of the distributed stream processing applications. In this paper we address the load balancing problem for distributed stream processing applications and present a decentralized and adaptive algorithm that allows the composition of distributed stream processing applications on the fly across a large-scale system, while satisfying their QoS demands. The algorithm fairly distributes the load on the resources and adapts dynamically to changes in the resource utilization or the QoS requirements of the applications. Our experimental results demonstrate the scalability, efficiency and performance of our approach","Load management,
Streaming media,
Peer to peer computing,
Large-scale systems,
Quality of service,
Application software,
Resource management,
Bandwidth,
Scalability,
Computer science"
Allowing Overlapping Boundaries in Source Code using a Search Based Approach to Concept Binding,"One approach to supporting program comprehension involves binding concepts to source code. Previously proposed approaches to concept binding have enforced non-overlapping boundaries. However, real-world programs may contain overlapping concepts. This paper presents techniques to allow boundary overlap in the binding of concepts to source code. In order to allow boundaries to overlap, the concept binding problem is reformulated as a search problem. It is shown that the search space of overlapping concept bindings is exponentially large, indicating the suitability of sampling-based search algorithms. Hill climbing and genetic algorithms are introduced for sampling the space. The paper reports on experiments that apply these algorithms to 21 COBOL II programs taken from the commercial financial services sector. The results show that the genetic algorithm produces significantly better solutions than both the hill climber and random search","Genetic algorithms,
Search problems,
Sampling methods,
Software maintenance,
Educational institutions,
Computer science,
Gold,
Computational efficiency,
Software algorithms"
Adaptive Electrocardiogram Feature Extraction on Distributed Embedded Systems,"Tiny embedded systems have not been an ideal outfit for high performance computing due to their constrained resources-limitations in processing power, battery life, communication bandwidth, and memory constrain the applicability of existing complex medical analysis algorithms such as the electrocardiogram (ECG) analysis. Among various limitations, battery lifetime has been a major key technological constraint. In this paper, we address the issue of partitioning such a complex algorithm while the energy consumption due to wireless transmission is minimized. ECG analysis algorithms normally consist of preprocessing, pattern recognition, and classification. Considering the orientation of the ECG leads, we devise a technique to perform preprocessing and pattern recognition locally in small embedded systems attached to the leads. The features detected in the pattern recognition phase are considered for the classification. Ideally, if the features detected for each heartbeat reside in a single processing node, the transmission will be unnecessary. Otherwise, to perform classification, the features must be gathered on a local node and, thus, the communication is inevitable. We perform such a feature grouping by modeling the problem as a hypergraph and applying partitioning schemes which yield a significant power saving in wireless communications. Furthermore, we utilize dynamic reconfiguration by software module migration. This technique, with respect to partitioning, enhances the overall power saving in such systems. Moreover, it adaptively alters the system configuration in various environments and on different patients. We evaluate the effectiveness of our proposed techniques on MIT/BIH benchmarks and, on average, achieve 70 percent energy saving",
Automatic generation of Correct Web Services Choreographies and Orchestrations with Model Checking Techniques,"In previous work we have presented the generation of WS-CDL and WS-BPEL documents. In this paper we show the unification of both generations. The aim is to generate correct WS-BPEL skeleton documents from WS-CDL documents by using the Timed Automata as an intermediary model in order to check the correctness of the generated Web Services with Model Checking Techniques. The model checker used is UPPAAL, a well known tool in theoretical and industrial cases that performs the verification and validation of Timed Automata. Note that our interest is focused on Web services where the time constraints play a critical role.",
Clinical data based optimal STI strategies for HIV: a reinforcement learning approach,"This paper addresses the problem of computing optimal structured treatment interruption strategies for HIV infected patients. We show that reinforcement learning may be useful to extract such strategies directly from clinical data, without the need of an accurate mathematical model of HIV infection dynamics. To support our claims, we report simulation results obtained by running a recently proposed batch-mode reinforcement learning algorithm, known as fitted Q iteration, on numerically generated data",
Video Surveillance of Medication Intake,"In the context of the growing proportion of seniors in the western world population and the efforts provided in home care services, we have developed a computer vision system for monitoring medication intake. The system detects automatically medication intake using a single low-cost webcam. Person detection and tracking over the video sequence is done using color-based techniques while the recognition of the medication intake activity is performed using our main contribution, a three-level scenario model. Experimental results in controlled conditions are shown and we discuss improvements to our system",
Lower bounds on the minimal delay of complex orthogonal designs with maximal rates,"The maximal rates and the minimal delays are basic problems of space-time block codes from complex orthogonal designs. Liang systematically solved the problem on the maximal rates of complex orthogonal designs, and posed an open problem on the minimal delays. Recently, the authors gave the negative answer for the open problem. In this letter, we give lower bounds on the minimal delays.","Delay,
Transmitting antennas,
Block codes,
Wireless communication,
Receiving antennas,
Computer science,
Information science,
Embryo,
Algorithm design and analysis,
Maximum likelihood decoding"
Mitigating denial of service threats in GSM networks,"Mobile networks not only provide great benefits to their users but they also introduce inherent security issues. With respect to security, the emerging risks of denial of service (DOS) attacks will evolve into a critical danger as the availability of mobile networks becomes more and more important for the modern information society. This paper outlines a critical flaw in GSM networks which opens the avenue for distributed denial of service attacks. We propose a way to mitigate the attacks by adding minimal authentication to the GSM channel assignment protocol.",
Optimal replica placement strategy for hierarchical data grid systems,"Grid computing is an important mechanism for utilizing distributed computing resources. These resources are distributed in different geographical locations, but are organized to provide an integrated service. In order to speed up data access efficiency data grid systems replicate essential data in multiple locations, so that a user can access the data from a site in his vicinity. This paper studies replica placement in data grid systems, taking into account several important issues described below. First, the replicas should be placed in proper server locations so that the workload on each server is balanced. Second, we choose the optimal number of replicas to balance the data access efficiency, and the expensive maintenance costs for multiple copies of data. Clearly, optimizing access cost of data requests and reducing the cost of replication are two conflicting goals. Finding a good balance between them is a challenging task. We propose efficient algorithms for selecting optimal locations for placing the replicas so that the workload among these replicas is balanced. Also when given the data usage from each user site and the maximum workload allowed for each replica server, our algorithm efficiently determines the minimum number of replicas required, as well as their locations.","Grid computing,
Distributed computing,
Cost function,
Databases,
Clustering algorithms,
Intserv networks,
Computer science,
Information science,
Resource management,
Bioinformatics"
Single-chip optical beam forming network in LPCVD waveguide technology based on optical ring resonators,"Optical ring resonators (ORRs) can be used as continuously tunable delay elements in a beam forming network for a phased-array antenna system. The bandwidth of such delay elements can be enhanced by cascading multiple ORRs. When delays and splitting/combining circuitry are integrated in one optical circuit, an optical beam forming network (OBFN) is obtained. In this paper, the principles of optical beam forming using ORRs are explained and demonstrated, by presenting measurements on a 1times4 OBFN chip, realized in LPCVD waveguide technology. To our knowledge, this is the first single-chip demonstration of continuous optical beam forming",
Wiki Pedagogy -- A Tale of Two Wikis,"This paper reports on a research project investigating the deployment of two different wikis in two Masters of IT subjects. As well, two different learning task designs were used (weekly whole class extension question tasks versus semester long group projects) providing further basis for comparison and contrast within the project. Data collection mechanisms included a 22 item student survey, interviews with academics and observations on student contributions to the wikis. Key results regarding students' patterns of behaviour, the impact of learning task design upon contributions, the effect of training, and the quality of the wiki applications themselves are discussed. Based on these results, a set of 12 principles for implementing wiki based learning tasks are recommended","Electronic learning,
Collaborative tools,
Computer science education,
Online Communities/Technical Collaboration,
Information technology,
Programming profession,
Performance analysis,
Libraries,
Object oriented modeling,
Technological innovation"
Adaptive Neural Network Control for Piezoelectric Hysteresis Compensation in A Positioning System,"Piezoelectric (PZT) actuators are popularly applied as actuators in micro-positioning systems due to its characteristic of infinitely small displacement resolution. Piezoelectricity material has nonlinear hysteresis phenomena. Owing to its nonlinear hysteresis effect, the tracking control accuracy of the precision positioning system is difficultly achieved. Hence, it is desirable to take hysteresis effect into consideration for improving the control performance. In this paper, a trajectory tracking controllers composed of adaptive neural network and PI controller is presented for a nano-positioner driven by a piezoactuator to realize the control performance that the displacement of a nano-positioner driven by a piezo-actuator can follow accurately the reference input signal and the transient response of the system can be improved. In this paper, some computer simulations are made to show that the system output can track the reference input exactly and the system transient response can be improved",
Implementation of Multiple-Valued CAM Functions by LUT Cascades,"In this paper, we introduce three types of multiplevalued content-addressable memories (CAMs): ordinary CAMs (CAMs), distance d CAMs, and *CAMs. Ordinary CAMs require an exact match, while *CAMs allow wildcard matches. In a distance d CAM, a match occurs even if at most d digits differ. Then, we define multiple-valued CAM functions represented by these CAMs. Next, we show an approach to realize each CAM function by an LUT cascade, which is a series connection of RAMs. Experimental results for both two-valued and multi-valued cases are shown.",
Scalable Packet Classification for Enabling Internet Differentiated Services,"Nowadays, IP networks are rapidly evolving toward a QoS-enabled infrastructure. The need for packet classification is increasing in accordance with emerging differentiated services. While the new differentiated services could significantly increase the number of rules, it has been demonstrated that performing packet classification on a potentially large number of rules is difficult and has poor worst-case performance. In this work, we present an enhanced tuple pruning search algorithm called ""tuple pruning plus"" (TPP) for packet classification, which outperforms the existing schemes on the scalability. Our main idea is to simplify the lookup procedure and to avoid unnecessary tuple probing by maintaining the least-cost property of rule through precomputation and the proposed information marker. With extra rules added for information marker, only one tuple access is required in each packet classification. In our experiments, 70 MB DRAM is used to achieve 50 million packets per second (MPPS) for a 1 M-rule set, showing a performance improvement by a factor of 50. We also present a heuristic to further reduce the required storage to about 20 MB. These results demonstrate the effectiveness of the TPP scheme to achieve high speed packet classification","Web and internet services,
IP networks,
Computer science,
Biomedical engineering,
Traffic control,
Scalability,
Random access memory,
Multidimensional systems,
Internet telephony,
Videoconference"
Syndrome-Based Robust Video Transmission Over Networks with Bursty Losses,This paper addresses the problem of low-latency robust video delivery over packet networks characterized by a bursty loss process. We propose a joint source-channel coding based video codec which uses distributed source coding (DSC) principles. This codec can efficiently tune to both the source content as well as to the network loss characteristics while respecting stringent latency constraints. Simulation results show that the proposed system is both objectively (in PSNR) and subjectively (visual quality) superior to predictive video coding systems where the encoded bitstream is protected with forward error correction (FEC) codes.,"Robustness,
Propagation losses,
Forward error correction,
Video codecs,
Source coding,
Delay,
Predictive models,
PSNR,
Video coding,
Protection"
Automated CAD/CAM-based nanolithography using a custom atomic force microscope,"We report the development of a novel nanolithographic system that combines the design capabilities of computer-aided design/computer-aided manufacturing (CAD/CAM) software with the nanolithographic abilities of the atomic force microscope (AFM). The AFM is a powerful tool for research at the nanoscale and can be used to perform a variety of serial nanolithographic techniques. A custom-built three-axis AFM system, designed to execute nanolithography, has been constructed and interfaced with a CAD/CAM design environment. This technique utilizes the CAD/CAM software to create, in a virtual design environment, the desired nanoscale patterns. Then, using a G-code interpreter and software algorithms to control the three-dimensional motion of the system, the design is replicated automatically by using conventional nanolithographic procedures. In this report, AFM-based anodization lithography on a silicon wafer and subsequent AFM imaging is used to confirm the successful automatic replication of the desired nanoscale patterns. Note to Practitioners-The impetus for this research was based on the desire to create a custom nanolithographic platform that could be changed and manipulated as per the users specifications and operated easily by anyone. Commercial atomic force microscopes (AFMs) that are used for nanolithography and other studies have their own specific software that enables little or no change to the workings of the system, rendering prototyping of new research techniques to be difficult. These closed AFM instruments can be difficult to operate and are user unfriendly. This report delineates our construction of an AFM system and how we have incorporated very familiar software environments and common computer-aided-design programming language to conduct nanolithography. A brief verification of how our system performs is included; we have deposited and imaged a series of surface features in a direct write fashion on a silicon surface using a common lithographic technique that exists in the research environment.","Atomic force microscopy,
Nanolithography,
Design automation,
Computer aided manufacturing,
CADCAM,
Software algorithms,
Automatic control,
Motion control,
Control systems"
Real-Time Estimation of User-Level QoS of Audio-Video Transmission over IP Networks,"This paper proposes a real-time estimation method of perceptual QoS (i.e., user-level QoS) of audio-video transmission over IP networks. The estimated quality reflects cross-modal influences of audio and video in the framework of ITU-T Recommendation J. 148. As objective measures of audio and video for the use of the perceptual QoS estimation, we select application-level QoS parameters for media synchronization quality. The perceptual quality is expressed in terms of the user-level QoS parameter of the interval scale, which is referred to as the psychological scale. As the multimedia quality integration function, we propose QoS parameter mapping between the application-level and the user-level. We made subjective quality assessment of simulated audio-video transmission over an IP network by the rating-scale method. We then calculate the psychological scale, utilizing the law of categorical judgment. Furthermore, we obtain the QoS mapping relations by the principal component analysis and multiple regression analysis. The regression formula thus derived can estimate the psychological scale with high accuracy unless the video frame rate is low. Finally we present an extension of the proposed method to continuous time estimation of user-level QoS.","IP networks,
Quality of service,
Quality assessment,
Psychology,
Delay estimation,
Data engineering,
Indium tin oxide,
Computer science,
Broadcast technology,
Broadcasting"
Fault-tolerant Routing Approach for Reconfigurable Networks-on-Chip,"We introduce fault-tolerant on-chip routing philosophy for two-dimensional meshes. It is an extension to the concept of packet connected circuit, PCC. In order to increase reliability we have designed an automatic rerouting property to a single switch node and added return channel to the communication route. An autonomic routing switch node is modeled asynchronously and implemented using Haste language. The logical functionality of routing is illustrated as a single study case in 7*8 mesh. The routing success is further analyzed in congesting and faulty environment",
Efficient Protocols Achieving the Commitment Capacity of Noisy Correlations,"Bit commitment is an important tool for constructing zero-knowledge proofs and multi-party computation. Unconditionally secure bit commitment can be based, in particular, on noisy channel or correlation where noise considered a valuable resource. Recently, Winter, Nascimento and Imai introduced the concept of commitment capacity, the maximal ratio between the length of a string which the sender commits to and the number of times the noisy channel/correlation is used. They also proved that for any discrete memoryless channel there exists a secure protocol achieving its commitment capacity however, no particular construction was given. Solving their open question, we provide an efficient protocol for achieving the commitment capacity of discrete memoryless systems (noisy channels and correlations)","Protocols,
Acoustic noise,
Information security,
Electronic mail,
Memoryless systems,
Industrial electronics,
Communication industry,
Computer industry,
Electronics industry,
Mathematics"
LUGrid: Update-tolerant Grid-based Indexing for Moving Objects,"Indexing moving objects is a fundamental issue in spatiotemporal databases. In this paper, we propose an adaptive Lazy-Update Grid-based index (LUGrid, for short) that minimizes the cost of object updates. LUGrid is designed with two important features, namely, lazy insertion and lazy deletion. Lazy insertion reduces the update I/Os by adding an additional memory-resident layer over the disk index. Lazy deletion reduces update cost by avoiding deleting single obsolete entry immediately. Instead, the obsolete entries are removed later by specially designed mechanisms. LUGrid adapts to object distributions through cell splitting and merging. Theoretical analysis and experimental results indicate that LUGrid outperforms former work by up to eight times when processing intensive updates, while yielding similar search performance.","Indexing,
Costs,
Computer science,
Data engineering,
Spatiotemporal phenomena,
Spatial databases,
Merging,
Performance analysis,
Monitoring,
Intrusion detection"
A New Identification Method for Hammerstein Model Based on PSO,"This paper presents a new approach of structure identification and parameter estimation for Hammerstein Model by using particle swarm optimization (PSO). The average square error criterion (ASE) has been proposed to decrease computation and obtain the true optimal structure effectively. Meanwhile, the modified identification algorithm is always converging by backward algorithm, and thus obtaining a high precision for the parameter estimation. Simulation results indicate that the ASE is an efficient order selection criterion, but Akaike's information criterion (AIC) and minimum description length (MDL) are not good for order selection and parameter estimation in Hammerstein model","Nonlinear systems,
Particle swarm optimization,
Parameter estimation,
Genetic algorithms,
Computational modeling,
Control systems,
Evolutionary computation,
Mechatronics,
Automation,
Information science"
BCBS: An Efficient Load Balancing Strategy for Cooperative Overlay Live-Streaming,"In this paper, we present Bandwidth Class Based Streaming (BCBS), an application layer multicast for multimedia services. BCBS focuses on multi source live streaming, and following a locality model based on round trip times, it creates network efficient streaming meshes. The load balancing selects multiple nodes as streaming sources and is organised subscription based instead of request based. We describe a simulation study of the load balancing and tree construction procedures. The results show that BCBS creates network efficient overlays with respect to stretch and link stress.",
High speed robust current sense amplifier for nanoscale memories: a winner take all approach,"The design of fast, low power and robust sense amplifier circuits is a challenge for nanoscale SRAMs due to the increasing bit line capacitance and process variations. Current sensing in SRAMs is promising to achieve high-speed operation in low-voltage application. In this paper, we propose a process variation tolerant, high performance and scalable current sense amplifier that uses a winner take all (WTA) approach for nanoscale SRAMs. Simulation of worst-case threshold voltage mismatch on our WTA sense amplifier shows that it could tolerate up to 10% variation in the threshold voltage, which is expected within die in a 70nm process. Detailed analysis of variation in the effective channel length (L/sub eff/) and supply voltage variation are also presented. A comparison of the sensing delay and energy consumption in 70nm technology shows that our WTA sense amplifier provides around 70-80% improvement in the sensing speed and consumes 28-70% less energy than the traditional voltage mode and current sense amplifiers.",
An Approach for Specifying the ADC and AGC Requirements for UWB Digital Receivers,"Digital pulsed ultra wideband (UWB) receivers offer numerous advantages compared to architectures based on analogue correlation, but present unique implementation challenges. The specification of the Analogue-to-Digital Converter (ADC) resolution constitutes one of the most critical activities in the design of digital UWB receivers. Moreover, multi-bit ADCs must be associated with an Automatic Gain Control block which avoids the sub-optimal regimes of the ADC dominated by quantisation or saturation noise. This paper shows how to determine the ADC resolution requirements and the optimal Automatic Gain Control (AGC) settings for the general class of digital receivers. We conclude that 3 bits provide sufficient resolution on condition that the AGC scales the input signal at a reference root mean squared (RMS) level of half the ADC saturation voltage.","ultra wideband communication,
analogue-digital conversion,
automatic gain control,
radio receivers"
Representations of Elementary Functions Using Binary Moment Diagrams,"This paper considers representations for elementary functions such as polynomial, trigonometric, logarithmic, square root, and reciprocal functions. These real valued functions are converted into integer functions by using fixed-point representation, and they are represented by using binary moment diagrams (BMDs). Elementary functions are represented compactly by applying the arithmetic transform to the functions. For polynomial functions, upper bounds on the numbers of nodes in BMDs and multiterminal binary decision diagrams (MTBDDs) are derived. These results show that for polynomial functions, BMDs require fewer nodes than MTBDDs. Experimental result for 16-bit precision sin(x) function shows that the BMD requires only 20% of the nodes for the MTBDD.",
An Adaptive Method for Subband Decomposition ICA,"Subband decomposition ICA (SDICA), an extension of ICA, assumes that each source is represented as the sum of some independent subcomponents and dependent subcomponents, which have different frequency bands. In this article, we first investigate the feasibility of separating the SDICA mixture in an adaptive manner. Second, we develop an adaptive method for SDICA, namely band-selective ICA (BS-ICA), which finds the mixing matrix and the estimate of the source independent subcomponents. This method is based on the minimization of the mutual information between outputs. Some practical issues are discussed. For better applicability, a scheme to avoid the high-dimensional score function difference is given. Third, we investigate one form of the overcomplete ICA problems with sources having specific frequency characteristics, which BS-ICA can also be used to solve. Experimental results illustrate the success of the proposed method for solving both SDICA and the over-complete ICA problems.",
Vertical Handoff between WWAN and WLAN,"This paper reports on the design and implementation of an IP-centric vertical handoff decision algorithm and execution scheme between a wireless wide area network (WWAN) and a wireless local area network (WLAN) as part of the ""IP-centric Field Workforce Automation"". The project aims to ensure free movement of field workers between networks while always connected in an IP mode, and to allow a user to use one device across many types of networks. We use a fuzzy logic inference system to process a multi-criteria vertical handoff decision metrics, and present a mobility management scheme for IP-based networks using the transport layer protocol SCTP and the application layer protocol SIP. The multi-homing capability and DAR extension of SCTP are applied to provide vertical handoff between heterogeneous networks. SIP is used to provide location management when a fixed node initiates an association with a mobile node.",
Integrating Databases into the Semantic Web through an Ontology-Based Framework,"To realize the Semantic Web, it will be necessary to make existing database content available for emerging Semantic Web applications, such as web agents and services, which use ontologies to formally define the semantics of their data. Our research in the design and implementation of an ontology-based system, OntoGrate, addresses the critical and challenging problem of supporting human experts in multiple domains to interactively integrate information that is heterogenous in both structure and semantics. Databases, knowledge bases, the World Wide Web, and the emerging Semantic Web are some of the resources for which scalable integration remains a challenge. To integrate databases into the Semantic Web, we use Semantic Web ontologies to incorporate database schemas. An expressive first order ontology language, Web-PDDL, is used to define the structure, semantics, and mappings of data resources. A powerful inference engine, OntoEngine, can be used for query answering and data translation. In this paper, besides introducing new ideas in the OntoGrate system, we will elaborate on two case studies for which our system works well.",
Panoramic View-Based Navigation in Outdoor Environments Based on Support Vector Learning,"This paper describes a panoramic view-based navigation in outdoor environments. We have been developing a two-phase navigation method. In the training phase, the robot acquires image sequences along the desired route and automatically learns the route visually. In the subsequent autonomous navigation phase, the robot moves by localizing itself by comparing input images with the learned route representation. To be robust to changes of weather and seasons, an object-based comparison is adopted. Our previous method applied a support vector machine (SVM) algorithm to object recognition and localization and exhibited a satisfactory performance but was sometimes sensitive to the variation of the robot's heading. This paper thus extends the method to use panoramic images. By searching the image for the region which matches the model image the most, a new method can considerably improve the localization performance and provide the robot with globally correct directions to move",
Modelling Voice Communication in Disaster Area Scenarios,"This paper deals with voice communication models for disaster area scenarios. The goal is to design models that can be used to generate realistic push to talk traffic for single talk groups. The modelling is based on an analysis of empirical measurements during a catastrophe maneuver. The analysis shows that the time series comprise heavy load periods and significant correlations. Based on these characteristics, different Markov and semi-Markov models are considered. Synthetic traffic streams for the different models are generated and evaluated by visual and statistical analysis. Finally, a case study outlines the impact of the different traffic models in network performance simulation",
802.11a transmitter: a case study in microarchitectural exploration,"Hand-held devices have rigid constraints regarding power dissipation and energy consumption. Whether a new functionality can be supported often depends upon its power requirements. Concerns about the area (or cost) are generally addressed after a design can meet the performance and power requirements. Different micro-architectures have very different area, timing and power characteristics, and these need RTL-level models to be evaluated. In this paper we discuss the microarchitectural exploration of an 802.11a transmitter via synthesizable and highly-parameterized descriptions written in Bluespec SystemVerilog (BSV). We also briefly discuss why such architectural exploration would be practically infeasible without appropriate linguistic facilities. No knowledge of 802.11a or BSV is needed to read this paper","Transmitters,
Computer aided software engineering,
Microarchitecture,
Hardware,
Energy consumption,
Protocols,
OFDM,
Clocks,
Combinational circuits,
Computer science"
Impact of Power and Rate Selection on the Throughput of Ad Hoc Networks,"With the advance of wireless technology, wireless devices are capable of adjusting transmit power and physical (PHY) layer data-rate. In this paper, we investigate the problem of how to adjust the power level and the PHY rate in order to maximize the network throughput in wireless ad hoc networks. Solving this problem can help compute the network capacity of ad hoc networks, which has drawn a lot of attention recently. In our study, we find that there exist intertwined relationships among maximum network throughput, power, and PHY rate. These intertwined relationships make computing the maximum network throughput a difficult problem. To get around the coupled relationships among power, PHY rate and network throughput, we take a simulation-based optimization approach, i.e., use a recursive randomized algorithm to find the solution. We study the convergence and complexity of our algorithm. Our results show that our algorithm always converges and the computation complexity is polynomial. The simulations also show that our algorithm can iteratively improve the network throughput, given an initial feasible power and rate setting.","Throughput,
Ad hoc networks,
Physical layer,
Iterative algorithms,
Wireless networks,
Mobile ad hoc networks,
Computer networks,
Computational modeling,
Power control,
Communication system traffic control"
A Meta-Model for Enterprise Applications,"In the last years, as object-oriented software systems became more and more complex, the need of performing automatically reverse engineering upon these systems has increased significantly. This applies also to enterprise applications, a novel category of software systems. As it is well known, one step toward a research infrastructure accelerating the progress of reverse engineering is the creation of an intermediate representation of software systems. This paper shows why existing intermediate representations of object-oriented software are not suitable for performing reverse engineering upon enterprise applications and proposes an intermediate representation (a model) for enterprise applications which facilitates the process of reverse engineering upon this type of applications. Based on an experimental study conducted on three enterprise applications, we prove the reliability of the introduced approach, discuss its benefits and touch the issues that need to be addressed in the future",
Filecules in High-Energy Physics: Characteristics and Impact on Resource Management,"Grid computing has reached the stage where deployments are mature and many collaborations run in production mode. Mature grid deployments offer the opportunity for revisiting and perhaps updating traditional beliefs related to workload models, which in turn leads to the re-evaluation of traditional resource management techniques. This paper analyzes usage patterns in a typical grid community, a large-scale data-intensive scientific collaboration in high-energy physics. We focus mainly on data usage, since data is the major resource for this class of applications. Our observations led us to propose a new abstraction for resource management in scientific data analysis applications: we define a filecule as a group of files that is always used together. We show that filecules exist and present their characteristics. The existence of filecules suggests a new granularity for data management, which, if incorporated in design, can significantly outperform the traditional solutions for data caching, replication and placement based on single-file granularity. We reason about the impact of filecules on resource management and show compelling evidence for using this abstraction when designing data management services",
Activity Recognition from Silhouettes using Linear Systems and Model (In)validation Techniques,"In this work we propose a model (invalidation approach to gait recognition, using a system that tries to discriminate specific activities of people. The recognition process departs from an abstraction obtained from video image sequences for different activities performed by different people, by first using a suitable representation for each frame and for each frame sequence. For each frame two commonly used models for describing silhouettes are employed: Fourier descriptors and vectors of widths. Then each sequence is modeled as a linear time invariant (LTI) system that captures the dynamics of the evolution of the frame description vectors in time. Finally a standard classification tool, SVM, is used to recognize activities using similarity measures obtained through model (in)validation. The main contribution of this work is the provision of an activity recognition model and the performance evaluation of this model using two different feature spaces",
Estimation of Anthropomeasures from a Single Calibrated Camera,"We are interested in the recovery of anthropometric dimensions of the human body from calibrated monocular sequences, and their use in multi-target tracking across multiple cameras and identification of individual people. In this paper, we focus on two specific anthropomeasures that are relatively easy to estimate from low-resolution images: stature and shoulder breadth. Precise average estimates are obtained for each anthropomeasure by combining measurements from multiple frames in the sequence. Our contribution is two-fold: (i) a novel technique for automatic and passive estimation of shoulder breadth, that is based on modelling the shoulders as an ellipse, and (U) a novel method for increasing the accuracy of the mean estimates of both anthropomeasures. The latter is based on the observation that major sources of error in the measurements are landmark localization the 2D image and 3D modelling error, and that both of these are correlated with gait phase and body orientation with respect to the camera. Consequently, estimation error can be significantly reduced via appropriate selection or control of these two variables","Cameras,
Anthropometry,
Humans,
Estimation error,
Computer science,
Phase measurement,
Biological system modeling,
Length measurement,
Hip,
Video sequences"
Automated Tracking of Multiple C. Elegans,This paper presents a method for model based automated tracking of multiple worm-like creatures. These methods are essential for accurate quantitative analysis into the genetic basis of behavior that involve more than one organism. An accurate worm model is designed using the geometry of planar curves and nonlinear estimation of the model's parameters are performed using a central difference Kalman filter (CDKF). The filter can naturally be expanded to estimate the locations of multiple worms and determine when they are occluding each other. The predicted location of the models at each iteration allows for an efficient method to determine the regions that are undergoing occlusions. Experiments on actual C. Elegans mating sequence data demonstrate the quality of the proposed method,
Phased Array Antenna Steering Using a Ring Resonator-Based Optical Beam Forming Network,"A novel beam steering mechanism for a phased array antenna receiver system is introduced. The core of the system is a ring resonator-based integrated optical beam forming network chip. Its principles are explained and demonstrated by presenting some measurement results. The system architecture around the chip is based on a combination of frequency down conversion, filter-based optical single sideband modulation and balanced coherent detection. It is proven that such an architecture has significant advantages with respect to a straightforward architecture using double sideband modulation and direct detection, namely relaxed bandwidth requirements on the optical modulators and detectors, reduced complexity and optical losses of the beam forming chip, and enhanced dynamic range","Phased arrays,
Optical arrays,
Antenna arrays,
Optical resonators,
Optical beams,
Optical modulation,
Optical receivers,
Optical filters,
Optical ring resonators,
Beam steering"
Wheedham: An Automatically Designed Block Cipher by means of Genetic Programming,"In this work, we present a general scheme for the design of block ciphers by means of genetic programming. In this vein, we try to evolve highly nonlinear and efficient functions to be used for the key expansion and the F-function of a Feistel network. Following this scheme, we propose a new block cipher design called Wheedham, that operates on 512 bit blocks and keys of 256 bits, of which we offer its C code (directly translated from the GP Trees) and some preliminary security results.",
Analysis and Visualization of Co-authorship Networks for Understanding Academic Collaboration and Knowledge Domain of Individual Researchers,"This paper proposed a new approach for collecting, analyzing and visualizing co-authoring data of individuals. This approach can be used for understanding the academic collaboration and knowledge domain of individual researchers in a past period through repetitive co-published works. Particularly we extracted the co-authoring data from the DBLP which is one of the largest on-line computer science bibliographic databases available on the Internet. To help users to understand the academic collaboration and knowledge domain of individuals, we developed an InterRing visualizer which shows not only the weight of co-authorship of an individual with other researchers in particular academic year, but also the knowledge domain of the individual that was covered by his/her publications published in a past period","Collaboration,
Information analysis,
Data visualization,
Collaborative work,
Data analysis,
Information technology,
Australia,
Data mining,
Computer science,
Visual databases"
Relevance Feedback Technique for Content-Based Image Retrieval using Neural Network Learning,"Relevance feedback (RF) is an interactive process in content-based image retrieval (CBIR), which refines the retrievals to a particular query by using user's feedback on previously retrieved results. In this paper, by changing the process of relevance feedback into a learning problem of neural network, a relevance feedback technique for content-based images retrieval by neural network learning (NELIR) is introduced, which can improve user interaction with image retrieval systems by fully exploiting similarity information. NELIR can describe the distribution of positive feedback sample images in feature space with a set of neighboring clusters produced through constructing neural network, for accurately reflecting their semantic relevance. In particular, constructing neural network is dynamic. The neural network depends on which images are retrieved in response to the query. On the other hand, NELIR is independent of the specific feature extraction and similarity measure. Thus, it may be embedded in many current CBIR systems to improve the performance of image retrieval. The performance of a prototype system using NELIR is evaluated on a database of 2,000 images. Experimental results demonstrate improved performance compared with a traditional CBIR system without NELIR algorithm using the same image similarity measure",
Belief Update in Bayesian Networks Using Uncertain Evidence,"This paper reports our investigation on the problem of belief update in Bayesian networks (BN) using uncertain evidence. We focus on two types of uncertain evidences, virtual evidence (represented as likelihood ratios) and soft evidence (represented as probability distributions). We review three existing belief update methods with uncertain evidences: virtual evidence method, Jeffrey's rule, and IPFP (iterative proportional fitting procedure), and analyze the relations between these methods. This in-depth understanding leads us to propose two algorithms for belief update with multiple soft evidences. Both of these algorithms can be seen as integrating the techniques of virtual evidence method, IPFP and traditional BN evidential inference, and they have clear computational and practical advantages over the methods proposed by others in the past",
An Integrated Scheme for Fully-Directional Neighbor Discovery and Topology Management in Mobile Ad hoc Networks,"With directional antennas, it is extremely important that a node maintains information with regards to the positions of its neighbors. This would allow the node to ""track"" the neighbors as they move; otherwise, a node will have to resort to either omnidirectional or circular directional transmissions (or receptions) fairly often. This can be overhead intense and can reduce spatial reuse. Maintaining directional information with regards to a large number of neighbors can itself be expensive; therefore it is important to limit a node's degree. We propose a topology control scheme (Di-ATC) that works with fully directional communications and offers a low degree bound while preserving network connectivity. The key idea is to execute Di-ATC on the discovered neighbors to select and maintain connectivity with only a subset of these neighbors. The members of this subset are those with high angular separations. We perform extensive simulations and demonstrate that our scheme effectively limits node degree while at the same time, preserves network connectivity and achieves low path stretch",
Frequency Reassignment for Coherent Modulation Filtering,"Modulation filtering is a technique for filtering slowly-varying envelopes of frequency subbands of a signal, without affecting the signal's phase and fine-structure. Coherent modulation filtering is a promising subtype of such techniques where subband envelopes are determined through demodulation of the subband signal with a coherently detected subband carrier. In this paper we propose a coherent modulation filtering technique that detects the carriers using the frequency reassignment (FR) operator from time-frequency reassignment. We show how this technique avoids the use of finite differences in the computation of instantaneous frequency (IF), and that it estimates IF more accurately than a past technique as a result. We confirm that the FR-enhanced technique retains the desirable modulation filtering properties (superposition and the preservation of zero-crossings) and show that it performs better on the same single-channel music source separation task than the past technique",
Visualization of Support Vector Machines with Unsupervised Learning,"The visualization of support vector machines in realistic settings is a difficult problem due to the high dimensionality of the typical datasets involved. However, such visualizations usually aid the understanding of the model and the underlying processes, especially in the biosciences. Here we propose a novel visualization technique of support vector machines based on unsupervised learning, specifically self-organizing maps. Conceptually, self-organizing maps can be thought of as neural networks that investigate a high-dimensional data space for clusters of data points and then project the clusters onto a two-dimensional map preserving the topologies of the original clusters as much as possible. This allows for the visualization of high-dimensional datasets together with their support vector models. With this technique we investigate a number of support vector machine visualization scenarios based on real world biomedical datasets",
Fast and Effective Spam Sender Detection with Granular SVM on Highly Imbalanced Mail Server Behavior Data,"Unsolicited commercial or bulk emails or emails containing virus currently pose a great threat to the utility of email communications. A recent solution for filtering is reputation systems that can assign a value of trust to each IP address sending email messages. By analyzing the query patterns of each participating node, reputation systems can calculate a reputation score for each queried IP address and serve as a platform for global collaborative spam filtering for all participating nodes. In this research, we explore a behavioral classification approach based on spectral sender characteristics retrieved from such global messaging patterns. Due to the large amount of bad senders, this classification task has to cope with highly imbalanced data. In order to solve this challenging problem, a novel granular support vector machine - boundary alignment algorithm (GSVM-BA) is designed. GSVM-BA looks for the optima] decision boundary by repetitively removing positive support vectors from the training dataset and rebuilding another SVM. Compared to the original SVM algorithm with cost-sensitive learning, GSVM-BA demonstrates superior performance on spam IP detection, in terms of both effectiveness and efficiency",
Using Visual Momentum to Explain Disorientation in the Eclipse IDE,"We report on a field study about how software developers experience disorientation when using the Eclipse Java integrated development environment. We analyzed the data using the theory of visual momentum, identifying three factors that may lead to disorientation: the absence of connecting navigation context during program exploration, thrashing between displays to view necessary pieces of code, and the pursuit of sometimes unrelated subtasks","Java,
Computer displays,
Programming,
Open source software,
User interfaces,
Computer science,
Data analysis,
Joining processes,
Navigation,
Pattern analysis"
Efficient Aggregation of Ranked Inputs,"A top-k query combines different rankings of the same set of objects and returns the k objects with the highest combined score according to an aggregate function. We bring to light some key observations, which impose two phases that any top-k algorithm, based on sorted accesses, should go through. Based on them, we propose a new algorithm, which is designed to minimize the number of object accesses, the computational cost, and the memory requirements of top-k search. Adaptations of our algorithm for search variants (exact scores, on-line and incremental search, top-k joins, other aggregate functions, etc.) are also provided. Extensive experiments with synthetic and real data show that, compared to previous techniques, our method accesses fewer objects, while being orders of magnitude faster.",
WSN19-4: Efficient Construction of Weakly-Connected Dominating Set for Clustering Wireless Ad Hoc Networks,"In most of the proposed clustering algorithms for wireless ad hoc networks, the cluster-heads form a dominating set in the network topology. A variant of dominating set which is more suitable for cluster formation is the weakly-connected dominating set (WCDS). We propose an area based distributed algorithm for WCDS formation with time and message complexity O(n). In this Area algorithm, we partition the wireless nodes into different areas, use some deterministic criteria to select the nodes for the WCDS in each area and adjust the area borders by adding additional nodes to the final WCDS. The effectiveness of our algorithm is confirmed through analysis and comprehensive simulation study.",
Control of Artificial Pneumatic Muscle for Robot Application,"Pneumatic muscle has many advantages such as elasticity, high power and structural similarity to a living thing's muscle. There has been many researches to control robot actuated by pneumatic muscles, but conventional theories are hard to apply on real robot plants because of their assumptions and disregards of pneumatic muscle's physical aspects like size of pneumatic muscle and its controller. Here, the new method for saving space which is occupied by many controllers to operate robot actuated by pneumatic muscles is proposed. Actually there is easy way to control pneumatic muscle using the commercial proportional pressure regulator, but its size is not suitable to be embedded on stand alone robot. So, new method using the pressure switches of compact size and encoders is suggested. This new method is tested on a robot link with ball joints, actuated by four pneumatic muscles",
Fast Quality Driven Selection of Composite Web Services,"The composite Web service selection is the process of building an executable composite Web service. Generally, the selection process involves decision making in terms of non-functional properties of Web services such as QoS requirements. Since the number of Web services is rapidly increasing and the QoS of the Web services environment changes dynamically, the fast selection is important. This paper presents a fast method for quality driven composite Web services selection based on a workflow partition strategy. The proposed method partitions an abstract workflow into two sub-workflows to decrease the number of candidate services that should be considered. A mixed integer linear programming is utilized for solving the service selection problem. Experimental results show that the partition strategy performed faster than the optimal strategy. Also, the qualities of the selected composite Web services are not significantly different from the optimal one",
Accidental Algorthims,"We provide evidence for the proposition that the computational complexity of individual problems, and of whole complexity classes, hinge on the existence of certain solvable polynomial systems that are unlikely to be encountered other than by systematic explorations for them. We consider a minimalist version of Cook's 3CNF problem, namely that of monotone planar 3 CNF formulae where each variable occurs twice. We show that counting the number of solutions of these modulo 2 is oplusP-complete (hence NP-hard) but counting them modulo 7 is polynomial time computable (sic). We also show a similar dichotomy for a vertex cover problem. To derive completeness results we use a new holographic technique for proving completeness results in oplusP for problems that are in P. For example, we can show in this way that oplus2CNF, the parity problem for 2CNF, is oplusP-complete. To derive efficient algorithms we use computer algebra systems to find appropriate holographic gates. In order to explore the limits of holographic techniques we define the notion of an elementary matchgrid algorithm to capture a natural but restricted use of them. We show that for the NP-complete general 3 CNF problem no such elementary matchgrid algorithm can exist. We observe, however, that it remains open for many natural #P-complete problems whether such elementary matchgrid algorithms exist, and for the general CNF problem whether non-elementary matchgrid algorithms exist","Polynomials,
Holography,
Encoding,
Computational complexity,
Fasteners,
Algebra,
Mathematics,
Computer science,
Bipartite graph,
Equations"
Interface-Based Rate Analysis of Embedded Systems,"Interface-based design is now considered to be one of the keys to tackling the increasing complexity of modern embedded systems. The central idea is that different components comprising such systems can be developed independently and a system designer can connect them together only if their interfaces match, without knowing the details of their internals. We use the concept of rate interfaces for compositional (correct-by-construction) design of embedded systems whose components communicate through data streams. Using the associated rate interface algebra, two components can be connected together if the output rate of one component is ""compatible"" with the input rate of the other component. We formalize this notion of compatibility and show that such an algebra is non-trivial because it has to accurately model the burstiness in the arrival rates of such data streams and the variability in their processing requirements. We discuss how rate interfaces simplify compositional design and at the same time help in functional and performance verification which would be difficult to address otherwise. Finally, we illustrate these advantages through a realistic case study involving a component-based design of a multiprocessor architecture running a picture-in-picture application","Embedded system,
Algebra,
Computer science,
Computer interfaces,
Computer networks,
Embedded computing,
Design engineering,
Laboratories,
Proposals,
System analysis and design"
Using a Queue Genetic Algorithm to Evolve Xpilot Control Strategies on a Distributed System,"In this paper, we describe a distributed learning system used to evolve a control program for an agent operating in the network game Xpilot. This system, which we refer to as a queue genetic algorithm, is a steady state genetic algorithm that uses stochastic selection and first-in-first-out replacement. We employ it to distribute fitness evaluations over a local network of dissimilar computers. The system made full use of our available computers while evolving successful controller solutions that were comparable to those evolved using a regular generational genetic algorithm.",
Performance Enhancement of a Haptic Arm Exoskeleton,"A high-quality haptic interface is typically characterized by low apparent inertia and damping, high structural stiffness, minimal backlash, and absence of mechanical singularities in the workspace. In addition to these specifications, exoskeleton haptic interface design involves consideration of space and weight limitations, workspace requirements, and the kinematic constraints placed on the device by the human arm. In this paper, the authors present the redesign of an existing five degree-of-freedom haptic arm exoskeleton. The redesign efforts focus primarily on ensuring smooth operation of the exoskeleton’s moving parts to minimize backlash, reducing cost and build time by simplifying the design, and increasing the torque output while continuing to use electric actuators for ease of control. The accompanying computer control system was developed in parallel with the mechanical redesign effort. The newly redesigned exoskeleton presented is capable of providing kinesthetic feedback to the joints of the lower arm and wrist of the operator, and will be used in future work for robot-assisted rehabilitation and training.","Haptic interfaces,
Exoskeletons,
Damping,
Kinematics,
Humans,
Costs,
Actuators,
Torque control,
Concurrent computing,
Control systems"
Have Green - A Visual Analytics Framework for Large Semantic Graphs,"A semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology. In intelligence analysis, investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends, patterns, and insights. However, as new information continues to arrive from a multitude of sources, the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses. We introduce a powerful visual analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs. The paper describes the overall framework design, presents major development accomplishments to date, and discusses future directions of a new visual analytics system known as Have Green","Visual analytics,
Visualization,
Information analysis,
Ontologies,
Computer displays,
Pattern analysis,
Fuses,
Performance analysis,
Laboratories,
Software systems"
Cooperative load balancing for a network of heterogeneous computers,"In this paper, we present a game theoretic approach to solve the static load balancing problem in a distributed system which consists of heterogeneous computers connected by a single channel communication network. We use a cooperative game to model the load balancing problem. Our solution is based on the Nash bargaining solution (NBS) which provides a Pareto optimal solution for the distributed system and is also a fair solution. An algorithm for computing the NBS is derived for the proposed cooperative load balancing game. Our scheme is compared with that of other existing schemes under simulations with various system loads and configurations. We show that the solution of our scheme is near optimal and is superior to the other schemes in terms of fairness.",
Ad-hoc Storage Overlay System (ASOS): A Delay-Tolerant Approach in MANETs,"Mobile ad-hoc networks (MANETs) are most useful in unprepared emergencies where critical applications must be launched quickly. However, they often operate in an adverse environment where end-to-end connectivity is highly susceptible to disruption. Adjusting the motion of existing nodes or deploying additional nodes can improve the connectivity under some circumstances, but for scenarios where connectivity cannot be immediately improved, disruption must be coped with properly. In this paper we propose the ad-hoc storage overlay system (ASOS). ASOS is a self-organized overlay of storage-abundant nodes to jointly provide distributed and reliable storage to data flows under disruption. ASOS is a delay-tolerant networking (DTN) approach that significantly improves the applicability of MANETs in practice",
A Novel Solution to the Reader Collision Problem in RFID System,"Radio frequency identification (RFID) is one of the hottest research topics in recent communication area. In the RFID system, the reader collision problem (RCP) is considered to be the bottleneck of the system throughput and reading efficiency. In this paper, a novel RCP solution-central cooperator (CC)-RFID is proposed to reduce the reader collisions by sharing the tag information among adjacent readers in RFID network. Both analytical and simulation results show that the proposed schemes improve the reading speed and efficiency dramatically compared to current popular RCP solutions","Radiofrequency identification,
Relays,
Feedback,
IEEE members,
Computer science,
Frequency,
Throughput,
Analytical models,
Interference,
Radio spectrum management"
Store-and-Forward Performance in a DTN,"Delay and disruption tolerant networks have been proposed to address data communication challenges in network scenarios where an instantaneous end-to-end path between a source and destination may not exist, and the links between nodes may be opportunistic, predictably connectable, or periodically-(dis)connected. In this paper, we describe the store-and-forward and custody transfer concepts that are used in DTNs. Then, we present simulation results that illustrate the usefulness of the custody transfer feature, and a message ferry in improving the end-to-end message delivery ratio in a multihop scenario where link availability can be as low as 20%. In particular, our results indicate that one can achieve a delivery ratio as high as 90-99% with appropriate buffer allocations. We also provide some preliminary insights on the design factors that influence the end to end delivery ratio, e.g., the link availability patterns and buffer allocation strategies","Disruption tolerant networking,
Protocols,
Electronic mail,
Intelligent networks,
Computer science,
Data engineering,
Drives,
Data communication,
Robustness,
Scalability"
Early Register Deallocation Mechanisms Using Checkpointed Register Files,"Modern superscalar microprocessors need sizable register files to support a large number of in-flight instructions for exploiting instruction level parallelism (ILP). An alternative to building large register files is to use a smaller number of registers, but manage them more effectively. More efficient management of registers can also result in higher performance if the reduction of the register file size is not the goal. Traditional register file management mechanisms deallocate a physical register only when the next instruction writing to the same destination architectural register commits. In this paper, we propose several techniques for deallocating physical registers much earlier. Our designs rely on the use of a checkpointed register file (CRF), where a local shadow copy of each bitcell is used to temporarily save the values of the early deallocated registers should they be needed to recover from branch mispredictions or to reconstruct the precise state after exceptions or interrupts. The proposed techniques try to release registers as soon as possible and are more aggressive than the previously proposed schemes for early deallocation of registers",
A Framework for Regional Association Rule Mining in Spatial Datasets,"The immense explosion of geographically referenced data calls for efficient discovery of spatial knowledge. One of the special challenges for spatial data mining is that information is usually not uniformly distributed in spatial datasets. Consequently, the discovery of regional knowledge is of fundamental importance for spatial data mining. This paper centers on discovering regional association rules in spatial datasets. In particular, we introduce a novel framework to mine regional association rules relying on a given class structure. A reward-based regional discovery methodology is introduced, and a divisive, grid-based supervised clustering algorithm is presented that identifies interesting subregions in spatial datasets. Then, an integrated approach is discussed to systematically mine regional rules. The proposed framework is evaluated in a real-world case study that identifies spatial risk patterns from arsenic in the Texas water supply.","Association rules,
Data mining,
Clustering algorithms,
Computer science,
Explosions,
Phase measurement,
Itemsets,
Data engineering,
Knowledge engineering,
Statistical distributions"
A new ant colony optimization for the knapsack problem,"The knapsack problem is one of the classical NP-hard problems in operations research. It has been thoroughly studied in the last few decades and several exact algorithms for its solution can be found in the literature. In this paper, we propose a new ant colony optimization (ACO) algorithm for solving the knapsack problem. Comparing with the basic ACO, this improved algorithm combines inner mutation and outer mutation that make it more effective and efficient in solving the knapsack problem. Numerical example is presented to illustrate the model",
A Constructive Algorithm for Reversible Logic Synthesis,"This paper presents a constructive synthesis algorithm for any n-qubit reversible function. Given any n-qubit reversible function, there are N distinct input patterns different from their corresponding outputs, where N les 2n, and the other (2n - N) input patterns will be the same as their outputs. We show that this circuit can be synthesized by at most 2nldrN '(n - 1)'-CNOT gates and 4n2 ldr N NOT gates. The time complexity of our algorithm has asymptotic upper bound O(n ldr 4n). The space complexity of our synthesis algorithm is also O(n ldr 2n). The computational complexity of our synthesis algorithm is exponentially lower than the complexity of breadth-first search based synthesis algorithm.","Logic circuits,
Circuit synthesis,
Logic gates,
Upper bound,
Quantum computing,
Computational complexity,
Energy dissipation,
Power dissipation,
Libraries,
Logic functions"
People Identification with Limited Labels in Privacy-Protected Video,"People identification is an essential task for video content analysis in a surveillance system. To construct a good classifier requires a large amount of training data, which may not be obtained in some scenario. In this paper, we propose an approach to augment insufficient training data by labeling identical video images that have removed people's identities by masking faces. We show user study results that human subjects can perform reasonably well in labeling pairwise constraints from face obscured images. We also present a new discriminative learning algorithm WPKLR to handle uncertainties in pairwise constraints. The effectiveness of the proposed approach is demonstrated using video captured in a nursing home environment. The experiments show that the WPKLR approach can obtain a high accuracy of people identification using limited labeled data and noisy pairwise constraints, and meanwhile minimize the risk of exposing people's identities",
AGA: Adaptive GTS Allocation with Low Latency and Fairness Considerations for IEEE 802.15.4,"IEEE 802.15.4 is a new standard uniquely designed for low-rate wireless personal area networks. It targets ultra-low complexity, cost, and power for low-rate wireless connectivity among inexpensive, portable, and moving devices. IEEE 802.15.4 provides a Guaranteed Time Slots (GTS) mechanism to allocate a specific duration within a superframe for time-critical transmissions. In this paper, we propose an adaptive GTS allocation (AGA) scheme for IEEE 802.15.4 with the considerations of low-latency and fairness. The scheme is designed based on the existing IEEE 802.15.4 medium access control protocol without any modification. A simulation model validated by the developed mathematical analysis is presented to investigate the performance of our AGA scheme. The capability of the proposed AGA scheme is evaluated by a series of experiments. It is shown that the proposed scheme significantly outperforms the existing IEEE 802.15.4 implementations.",
A Hierarchical Anonymous Routing Scheme for Mobile Ad-Hoc Networks,"Privacy and anonymity are critical security issues to many large-scale MANET applications such as military communication networks. These applications are more likely deploying the networks heterogeneously and hierarchically due to administrative needs or routing efficiency. When the size of the network scales up, the routing overhead incurred by existing flat anonymous routing protocols increases fast as the required number of public key operations increases. This results in deteriorated routing and data communication performance. In this paper, we introduce a novel hierarchical anonymous on-demand routing protocol tackling this limitation. In addition to guaranteeing routing and data delivering security, the scheme provides two levels of anonymity: intra-group and inter-group. By exploiting the hierarchical network structure, it effectively controls computational overhead while preserving anonymity, hence accommodates to larger-scale MANETs",
Requirements to UPnP for Robot Middleware,"The UPnP (universal plug and play) defines an architecture for pervasive peer-to-peer network connectivity of intelligent appliances. It shares the service oriented architecture with emerging Web service technology, and has many advantages for future robot middleware such as automatic discovery of services and accommodation of dynamic distributed computing environment. However, the UPnP needs some additional features for being used as a robot middleware. This paper discusses them, and presents some requirements when developing a UPnP SDK for robot middleware as well. This paper also presents an experimental result of applying the UPnP to robot components",
Development of Intelligent Visual Inspection System (IVIS) for Bottling Machine,"This paper presents a research on developing an intelligent visual inspection system (IVIS) for bottling machine, focusing on the development of image processing framework for defect detection. The objective of the research is to contribute a method on modeling, integrating and enhancing IVIS for the process of quality control in industrial area. IVIS application for quality control was studied using plastic bottles on a production line simulation. An experiment had done by using developed software and special equipments such as conveyor belt, lighting source, and a Web camera (Webcam) to capture the image. The experiment result shows that the system is accurate enough to detect moving object on the speed at 106 rpm with the accuracy of the image acquisition is 94.264%.",
Hybrid Fault Detection Technique: A Case Study on Virtex-II Pro's PowerPC 405,"Hardening processor-based systems against transient faults requires new techniques able to combine high fault detection capabilities with the usual design requirements, e.g., reduced design-time, low area overhead, reduced (or null) accessibility to processor internal hardware. This paper proposes the adoption of a hybrid approach, which combines ideas from previous techniques based on software transformations with the introduction of an Infrastructure IP with reduced memory and performance overheads, to harden system based on the PowerPC 405 core available in Virtex-II Pro FPGAs. The proposed approach targets faults affecting the memory elements storing both the code and the data, independently of their location (inside or outside the processor). Extensive experimental results including comparisons with previous approaches are reported, which allow practically evaluating the characteristics of the method in terms of fault detection capabilities and area, memory and performance overheads","Fault detection,
Hardware,
Field programmable gate arrays,
Costs,
Application software,
Software performance,
Registers,
Redundancy,
Computer aided instruction,
Software systems"
A PCA Based Visual DCT Feature Extraction Method for Lip-Reading,"This paper proposes a PCA based method to reduce the dimensionality of DCT coefficients for visual only lip-reading systems. A three-stage pixel based visual front end is adopted. First, DCT or block-based DCT features are extracted. Second, Principal Component Analysis is applied for dimension reduction. Finally, all the feature vectors are normalized into a uniform scale. This work investigates this three-stage method, comparing with PCA and two DCT based approaches whose features are selected manually. In the latter manner, PCA coefficients are selected according to energy while the reduction of DCT coefficients leans to the left components in the left-top corner. Experiments prove that the dimension reduction task based on PCA does improve the recognition accuracy when the final dimension is below a certain value. They also show that DCT and block-based DCT work similarly for lip reading task, outperforming PCA slightly.","Principal component analysis,
Discrete cosine transforms,
Feature extraction,
Discrete wavelet transforms,
Linear discriminant analysis,
Pixel,
Data mining,
Frequency,
Speech recognition,
Computer science"
"The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction","The realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop CVE. Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an “emotibox” that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.",
Humanoid Mobile Manipulation Using Controller Refinement,"An important class of mobile manipulation problems are ""move-to-grasp"" problems where a mobile robot must navigate to and pick up an object. One of the distinguishing features of this class of tasks is its coarse-to-fine structure. Near the beginning of the task, the robot can only sense the target object coarsely or indirectly and make gross motion toward the object. However, after the robot has located and approached the object, the robot must finely control its grasping contacts using precise visual and haptic feedback. This paper proposes that move-to-grasp problems are naturally solved by a sequence of controllers that iteratively refines what ultimately becomes the final solution. This paper introduces the notion of a refining sequence of controllers and defines it in terms of controller goal regions and domains of attraction. Refining sequences are shown to be more robust than other types of controller sequences. In addition, a procedure for converting a refining sequence into an equivalent ""parallelized"" controller is proposed. Executing this parallelized controller confers all the advantages of iteratively executing the controllers sequentially. The approach is demonstrated in a move-to-grasp task where Robonaut, the NASA/JSC dexterous humanoid, is mounted on a mobile base and navigates to and picks up a geological sample box",
Improving Feature Subset Selection Using a Genetic Algorithm for Microarray Gene Expression Data,"Microarray data usually contains a huge number of genes (features) and a comparatively small number of samples, which make accurate classification or prediction of diseases challenging. Feature selection techniques can help us identify important and irrelevant (unimportant) features by applying certain selection criteria. However, different feature selection algorithms based on various theoretical arguments often produce different results when applied to the same data set. This makes selecting an optimal or near optimal feature subset for a data set difficult. In this paper, we propose using a genetic algorithm to improve feature subset selection by combining valuable outcomes from multiple feature selection methods. The goal of our genetic algorithm is to achieve a balance between the classification accuracy and the size of the feature subsets selected. The advantages of this approach include the ability to accommodate different feature selection criteria and find small subsets of features that perform well for a particular inductive learning algorithm of interest to build the classifier. The experimental results demonstrate that our approach can find subsets of features with higher classification accuracy and/or smaller size compared with each individual feature selection algorithm.",
Decision Tree Based FPGA-Architecture for Texture Sea State Classification,"The target detection process in sea clutter background involves the use of different types of CFAR (constant false alarm rate) algorithms. These algorithms and their parameters should be configured to obtain the maximum detection probability and minimum false alarm probability at the current sea state (Beaufort scale). This paper present an FPGA-architecture for automatic classification based on texture recognition of sea states. The sea state texture classification allows select the appropriate CFAR algorithm and its parameters for the target detection process. The paper is centered in the hardware implementation for sea state texture classification, based on decision tree. The rules for decision tree are obtained from the analysis of the grey levels co-occurrence matrix features applied in an image of the sea state obtained in a radar scan. Results with simulated and real data are presented and discussed","Decision trees,
Classification tree analysis,
Clutter,
Radar imaging,
Object detection,
Radar detection,
Radar remote sensing,
Image analysis,
Image texture analysis,
Computer science"
Parallel Hybrid Genetic Algorithms on Consumer-Level Graphics Hardware,"In this paper, we report a parallel hybrid genetic algorithm (HGA) on consumer-level graphics cards. HGA extends the classical genetic algorithm by incorporating the Cauchy mutation operator from evolutionary programming. In our parallel HGA, all steps except the random number generation procedure are performed in graphics processing unit (GPU) and thus our parallel HGA can be executed effectively and efficiently. We propose the pseudo-deterministic selection method which is comparable to the traditional global selection approach with significant execution time performance advantages. We perform experiments to compare our parallel HGA with our previous parallel FEP (fast evolutionary programming) and demonstrate that the former is much more effective and efficient than the latter. The parallel and sequential implementations of HGA are compared in a number of experiments, it is observed that the former outperforms the latter significantly. The effectiveness and efficiency of the pseudo-deterministic selection method is also studied.","Genetic algorithms,
Hardware,
Genetic programming,
Genetic mutations,
Random number generation,
Computer graphics,
Central Processing Unit,
Parallel programming,
Rendering (computer graphics),
Microcomputers"
Frame Alignment Stability Issues in Natural Field Orientation,"Natural field orientation (NFO) is a technique for generating a rotating reference frame position for an induction machine which is aligned with the stator flux. The term ""natural"" is applied because there is an implicit tendency for the rotating frame to realign with the correct stator flux frame position if there is a perturbation away from this position. However, under regeneration conditions this realignment property may not occur, and the frame position, if perturbed, will move the control reference frame away from stator flux alignment. This paper examines this frame alignment stability problem, and proposes a solution that retains the essential simplicity of the NFO concept","Stators,
Voltage,
Induction machines,
Angular velocity,
Position measurement,
Shafts,
Frequency,
Error correction,
Stability analysis,
Computer science"
Adding Self-Healing Capabilities into Legacy Object Oriented Application,Adding self healing functionalities into legacy applications without user involvement is immensely useful for users and programmers of such systems. This paper presents a technique of injecting user code with self-healing primitives by statically analyzing the legacy object oriented code and instrumenting it to become a self-manageable and self-healing component. Our experiments show that it is worthwhile to instrument legacy code to provide such autonomic behavior,
SCTP based Handover Mechanism for VoIP over IEEE 802.11b Wireless LAN with Heterogeneous Transmission Rates,In this paper a transport layer handover mechanism for Voice over Internet Protocol (VoIP) in 802.11b using the Stream Control Transmission Protocol (SCTP) is proposed. The multi-homing feature of SCTP is used to allow connections to several 802.11b Access Points (AP's). Probing packets that model VoIP encoded data at various transmission rates are used to obtain quality metrics from each of the available networks. Handover decisions are made based on the Mean Opinion Score (MOS) calculated from the ITU-T E-Model for voice quality assessment using the obtained measurements. The handover mechanism is shown to operate in 802.11b networks with heterogeneous transmission rates using multiple VoIP codecs. The results show a high correlation between the MOS predicted by the proposed mechanism and the MOS experienced by a VoIP call present in the network. Results verify the accurate operation of the scheme using multiple VoIP codecs at various transmission rates. A simulation showing an SCTP endpoint handover between heterogeneous transmission rate AP's is presented.,
Microneedle Array for Measuring Wound Generated Electric Fields,"A microneedle array has been fabricated and applied to the measurement of transdermal skin potentials in human subjects. Potential changes were recorded in the vicinity of superficial wounds, confirming the generation of a lateral electric field in human skin. The measured electric field decays with distance from the wound edge, and is directed towards the wound. The measurement of endogenous fields in skin is a prelude to the study of the therapeutic efficacy of applied electric fields to chronic non-healing wounds",
The SystemJ approach to system-level design,"In this paper, we propose a new system-level design language, called SystemJ. It extends Java with synchronous reactive features present in Esterel and asynchronous constructs suitable for modelling globally asynchronous locally synchronous systems. The strength of SystemJ comes from its ability to offer the data processing and encapsulation elegance of Java, Esterel-like reactivity and synchrony, and the asynchronous de-coupling of CSP all within the Java framework. Using standard Java environments, for specification and modelling, or specialised reactive embedded processors, for high performance implementation, the SystemJ design flow is extremely versatile. With the increasing attention that Java gets in embedded systems, SystemJ comes to address data and control, software and hardware, modelling and implementation in a unified manner","System-level design,
Java,
Embedded system,
Embedded computing,
Automatic control,
Design automation,
Circuits,
Computational modeling,
Formal verification,
Computer science"
Critical personality traits in successful pair programming,"Pair programming (PP) is a common practice in Extreme programming, in which two programmers work together using a single computer. The close interaction required by PP makes it difficult to apply. The hypothesis is that certain personality traits are crucial for the success of PP, and PP partners should be chosen based on these personality traits. In this research, we first survey the programmers in industry to identify the perceived important personality traits for PP, and then conduct experiments to determine the significance of these personality traits in successful PP",
Evaluation of short-term traffic forecasting algorithms in wireless networks,"Our goal is to characterize the traffic load in an IEEE 802.11 infrastructure. This can be beneficial in many domains, including coverage planning, resource reservation, and network monitoring for anomaly detection, and producing more accurate simulation models. We conducted an extensive measurement study of wireless users on a major university campus using the IEEE 802.11 wireless infrastructure. This paper proposes and evaluates several traffic forecasting algorithms based on various traffic models that employ the periodicity, recent traffic history, and flow-related information. Finally, it discusses the impact of time-scale and history on the prediction accuracy","Telecommunication traffic,
Intelligent networks,
Wireless networks,
Traffic control,
Predictive models,
History,
Demand forecasting,
Load forecasting,
Computer science,
Accuracy"
Application of DSTATCOM for Mitigation of Voltage Sag for Motor Loads in Isolated Distribution Systems,"This paper deals with one of the potential applications of distribution static compensator (DSTATCOM) to industrial systems for mitigation of voltage dip problem. The dip in voltage is generally encountered during the starting of an induction motor. Isolated distribution systems are comparatively not as stiff as grid systems; so large starting currents and objectionable voltage drop during starting of an induction motor could be critical for the entire system. DSTATCOM is one effective solution for isolated power systems facing such power quality problems. The model of DSTATCOM connected in shunt configuration to such an isolated system (3phase, 42.5 kVA alternator) feeding dynamic motor loads is developed using Simulink and PSB of MATLAB software. Simulated results demonstrate that DSTATCOM can be a considered as a viable solution for solving such voltage dip problems","Voltage fluctuations,
Induction motors,
Power system modeling,
Power system simulation,
Power system dynamics,
STATCOM,
Power quality,
Mathematical model,
Alternators,
MATLAB"
Coded Cooperation in OFDMA Systems,"A novel time-frequency cooperative communication strategy is proposed for coded OFDMA system over quasi-static frequency-selective Rayleigh fading. To take advantage of the frequency dispersion, each user transmits his partner's and his own data on different subcarriers spreading over the entire available bandwidth. The allocation of subcarriers to each user and his partners can be either pre-specified or dynamically determined. Compared to other cooperative strategies using the time division orthogonal channel allocation, this cooperative strategy benefits from the freedom of subcarrier allocation and the ""created"" block fading channel during the cooperation process. The performance characteristic is developed in terms of pairwise error probability (PEP) and frame error probability to illustrate the improved diversity gain for various cooperation scenarios. Computer simulation results are provided to verify our theoretical analysis.",
Nexus: a novel weighted-graph-based prefetching algorithm for metadata servers in petabyte-scale storage systems,"An efficient, accurate and distributed metadata-oriented prefetching scheme is critical to the overall performance in large distributed storage systems. In this paper, we present a novel weighted-graph-based prefetching technique, built on successor relationship, to gain performance benefit from prefetching specifically for clustered metadata servers, an arrangement envisioned necessary for petabyte-scale distributed storage systems. Extensive trace-driven simulations show that by adopting our new prefetching algorithm, the hit rate for metadata access on the client site can be increased by up to 13%, while the average response time of metadata operations can be reduced by up to 67%, compared with LRU and an existing state of the art prefetching algorithm.","Prefetching,
File servers,
Computer architecture,
Scalability,
Clustering algorithms,
Throughput,
Large-scale systems,
File systems,
Computer science,
Performance gain"
Coexistence Mechanism Using Dynamic Fragmentation for Interference Mitigation between Wi-Fi Bluetooth,"In this paper, we present a non-collaborative coexistence mechanism using dynamic fragmentation that functions very well at the Wi-Fi in the presence of Bluetooth interference. The proposed scheme tries to optimize MAC layer packet length such that the Wi-Fi device has better chance to avoid the interference caused by Bluetooth devices. The mechanism dynamically adjusts the fragmentation level based on the current packet error rate (PER). We developed an analytical model that provides the MAC the necessary information (in terms of PER) to decide the right time for further packet fragmentation. The developed model is also employed to measure the throughput of the Wi-Fi device. Simulations are also performed to validate the developed model. We show that our coexistence mechanism could significantly improve the performance of Wi-Fi in both throughput and transmission delay, though there is only slight performance improvement at the Bluetooth side","Interference,
Bluetooth,
Frequency,
Collaborative work,
Throughput,
Wireless LAN,
Time division multiple access,
Error analysis,
Analytical models,
Delay"
Structure from Motion with Known Camera Positions,"The wide availability of GPS sensors is changing the landscape in the applications of structure from motion techniques for localization. In this paper, we study the problem of estimating camera orientations from multiple views, given the positions of the viewpoints in a world coordinate system and a set of point correspondences across the views. Given three or more views, the above problem has a finite number of solutions for three or more point correspondences. Given six or more views, the problem has a finite number of solutions for just two or more points. In the three-view case, we show the necessary and sufficient conditions for the three essential matrices to be consistent with a set of known baselines. We also introduce a method to recover the absolute orientations of three views in world coordinates from their essential matrices. To refine these estimates we perform a least-squares minimization on the group cross product SO(3) × SO(3) × SO(3). We report experiments on synthetic data and on data from the ICCV2005 Computer Vision Contest.",
Attribute Grammar-Based Event Recognition and Anomaly Detection,We propose to use attribute grammars for recognizing normal events and detecting abnormal events in a video. Attribute grammars can describe constraints on features (attributes) in addition to the syntactic structure of the input. Events are recognized using an extension of the Earley parser that handles attributes and concurrent event threads. Abnormal events are detected when the input does not follow syntax of the grammar or the attributes do not satisfy the constraints in the attribute grammar to some degree. We demonstrate the effectiveness of our method for the task of recognizing normal events and detecting anomalies in a parking lot.,
Bitmap indexes for large scientific data sets: a case study,"The data used by today's scientific applications are often very high in dimensionality and staggering in size. These characteristics necessitate the use of a good multidimensional indexing strategy to provide efficient access to the data. Researchers have previously proposed the use of bitmap indexes for high-dimension scientific data as a way of overcoming the drawbacks of traditional multidimensional indexes such as R-trees and KD-trees, which are bulky and whose performance does not scale well as the number of dimensions increases. However, the techniques proposed in previous work on bitmap indexes are not sufficient to address all problems that arise in practice. In experiments with real datasets, we experienced problems with index size and query performance. To overcome these shortcomings, we propose the use of adaptive, multilevel, multi-resolution bitmap indexes, and evaluate their performance in two scientific domains. Our preliminary experiments with a parallel query processor and index creator also show that it is very easy to parallelize a bitmap index",
Nonlinear Multidimensional Bayesian Estimation with Fourier Densities,"Efficiently implementing nonlinear Bayesian estimators is still an unsolved problem, especially for the multidimensional case. A trade-off between estimation quality and demand on computational resources has to be found. Using multidimensional Fourier series as representation for probability density functions, so called Fourier densities, is proposed. To ensure non-negativity, the approximation is performed indirectly via Psi-densities, of which the absolute square represent the Fourier density. It is shown that Psi-densities can be determined using the efficient fast Fourier transform algorithm and their coefficients have an ordering with respect to the Hellinger metric. Furthermore, the multidimensional Bayesian estimator based on Fourier densities is derived in closed form. That allows an efficient realization of the Bayesian estimator where the demands on computational resources are adjustable",
Identification of Attitude Flight Dynamics for An Unconventional UAV,"The unconventional air frames used by many small fixed-wing unmanned aerial vehicles (UAVs) pose difficulties in determining their dynamic models accurately. This is further complicated by the low Reynolds numbers flight regimes encountered by these aircraft. This paper studies the attitude flight dynamics identification of an unconventional model-scale UAV which has only two independently actuated elevon surfaces (no rudder or elevators). The study presents the first steps towards the development of autopilots for this kind of UAV. Utilizing the real-time flight data collected from a human-controlled test flight, the standard identification approaches were applied to obtain a MIMO linear model with given configuration deduced from a theoretical study. The simulation-based validation of a simplified model was undertaken which shows acceptable modelling accuracy. Further flight tests controlled by an autopilot tuned according to the model are currently being undertaken","Unmanned aerial vehicles,
Mathematical model,
Vehicle dynamics,
Aerospace control,
Military aircraft,
Aircraft navigation,
Elevators,
Intelligent robots,
System identification,
Testing"
Improving Sequential Single-Item Auctions,"We study how to improve sequential single-item auctions that assign targets to robots for exploration tasks such as environmental clean-up, space-exploration, and search and rescue missions. We exploit the insight that the resulting travel distances are small if the bidding and winner-determination rules are designed to result in hillclimbing, namely to assign an additional target to a robot in each round of the sequential single-item auction so that the team cost increases the least. We study the impact of increasing the lookahead of hillclimbing and using roll-outs to improve the evaluation of partial target assignments. We describe the bidding and winner-determination rules of the resulting sequential single-item auctions and evaluate them experimentally, with surprising results: larger lookaheads do not improve sequential single-item auctions reliably while only a small number of roll-outs in early rounds already improve them substantially","Robot kinematics,
Costs,
Orbital robotics,
Intelligent robots,
Computer science,
Centralized control,
Concurrent computing,
Aerospace industry,
Computer industry,
Systems engineering and theory"
Dejong Function Optimization by Means of a Parallel Approach to Fuzzified Genetic Algorithm,"Genetic Algorithms are very powerful search methods that are used in different optimization problems. Parallel versions of genetic algorithms are easily implemented and usually increase algorithm performance [4]. Fuzzy control as another optimization solution along with genetic algorithms can significantly increase algorithm performance. Two variations for genetic algorithm and fuzzy system composition exist. In the first approach Genetic algorithms are used to optimize and model the structure of fuzzy systems through knowledge base or membership function design while the second approach exploits fuzzy to dynamically supervise genetic algorithm performance by speedily reaching an optimal solution. In this paper we propose a new method for fuzzy parallel genetic algorithms, in which a parallel client-server single population fuzzy genetic algorithm is configured to optimize the performance of the first three Dejong functions in order to reach a global solution in the least possible iterations. Simulations show much improvement in genetic algorithm performance evaluation.","Genetic algorithms,
Fuzzy systems,
Genetic mutations,
Search methods,
Optimization methods,
Computer science,
Power engineering computing,
Genetic engineering,
Power engineering and energy,
Fuzzy control"
"Notice of Violation of IEEE Publication Principles
Practical approaches for analysis, visualization and destabilizing terrorist networks","Notice of Violation of IEEE Publication Principles

""Practical Approaches for Analysis, Visualization and Destabilizing Terrorist Networks""
by Nasrullah Memon and Henrik Legind Larsen
in Proceedings of the First International Conference on Availability, Reliability and Security (ARES), April 2006

After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.

This paper has copied portions of text from the sources cited below. The lead author, Nasrullah Memon, was found to be solely responsible for the violation. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.

""Untangling Criminal Networks: A Case Study""
by Jennifer Xu, Hsinchun Chen
Proceedings of the First NSF/NIJ Symposium Intelligence and Security Informatics, ISI, June 2003

""The Exploratory Construction of Database Views""
by M.N. Smith, P.J.H. King
Research Report BBKCS, School of Computer Science and Information Systems, Birbeck College, University of London, 2002


Traditionally most of the literature in social network analysis (SNA) has focused on networks of individuals. Although SNA is not conventionally considered as a data mining technique, it is especially suitable for mining a large volume of association data to discover hidden structural patterns in terrorist networks. After September 11 attacks, SNA has increasingly been used to study terrorist networks. As these covert networks share some features with conventional networks, they are harder to identify because they mask their transactions. The most complicating factor is that terrorist networks are often embedded in a much larger population (i.e., adversaries have links with both covert and innocent individuals). Hence, it is desirable to have tools to correctly classify individuals in covert networks so that the resources for isolating them will be used more efficiently. This paper uses centrality measures from complex networks to discuss how to destabilize adversary networks. We propose newly introduced algorithms for constructing hierarchy of the covert networks, so that investigators can view the structure of the ad hoc networks/atypical organizations, in order to destabilize the adversaries. The algorithms are also demonstrated by using publicly available dataset. Moreover we also demonstrate techniques for filtering graphs (networks)/detecting particular cells in adversary networks using a fictitious dataset.",
Yield-Driven Redundant Via Insertion Based on Probabilistic Via-Connection Analysis,"In this paper, based on probabilistic via-connection analysis of single vias and redundant vias, it is well known that on-track redundant via insertion is more important and critical than off-track redundant via insertion for yield optimization. Furthermore, a two-phase insertion approach for yield optimization is proposed to insert on-track redundant vias by finding a maximum matching result in a bipartite graph and insert off-track redundant vias by using a maximum constrained edge-pair matching result in a constrained edge-pair matching with via-sharing constraints. According to the Poisson yield model for redundant via insertion, the experimental results show that our proposed two-phase insertion approach can increase 0.3%~7.4% wirelength to improve 4.3%~44.8% chip yield for the tested benchmarks.",
Groupwise point pattern registration using a novel CDF-based Jensen-Shannon Divergence,"In this paper, we propose a novel and robust algorithm for the groupwise non-rigid registration of multiple unlabeled point-sets with no bias toward any of the given pointsets. To quantify the divergence between multiple probability distributions each estimated from the given point sets, we develop a novel measure based on their cumulative distribution functions that we dub the CDF-JS divergence. The measure parallels the well known Jensen-Shannon divergence (defined for probability density functions) but is more regular than the JS divergence since its definition is based on CDFs as opposed to density functions. As a consequence, CDF-JS is more immune to noise and statistically more robust than the JS. We derive the analytic gradient of the CDF-JS divergence with respect to the non-rigid registration parameters for use in the numerical optimization of the groupwise registration leading a computationally efficient and accurate algorithm. The CDF-JS is symmetric and has no bias toward any of the given point-sets, since there is NO fixed reference data set. Instead, the groupwise registration takes place between the input data sets and an evolving target dubbed the pooled model. This target evolves to a fully registered pooled data set when the CDF-JS defined over this pooled data is minimized. Our algorithm is especially useful for creating atlases of various shapes (represented as point distribution models) as well as for simultaneously registering 3D range data sets without establishing any correspondence. We present experimental results on non-rigid registration of 2D/3D real point set data.","Pattern matching,
Robustness,
Shape,
Computer vision,
Iterative closest point algorithm,
Information science,
Probability distribution,
Distribution functions,
Density measurement,
Probability density function"
"An Improved Formal Framework of Actions, Individual Intention and Group Intention for Multi-agent Systems","In order to improve existing work on investigating actions and intentions in cooperation logics, a new model is given to extend concurrent game structures. The assumption that different intelligent agents can not execute the same actions which obviously does not fit human commonsense understanding of the world has been done away with. Five functions that involved actions are given at length so that we can investigate the relations among agents, actions and states in depth within the context of social law. Two kinds of intentions (individual intention and group intention) have been investigated in the extended cooperation logic.",
Engineering emergence,"We explore various definitions and characteristics of emergence, how we might recognise and measure emergence, and how we might engineer emergent systems. We discuss the TUNA (Theory Underpinning Nanotech Assemblers) project, which is investigating emergent engineering in the context of molecular nanotechnology, and use the TUNA case study to explore an architecture suitable for emergent complex systems","Systems engineering and theory,
Nanotechnology,
Character recognition,
Centralized control,
Biology computing,
Solid state circuits,
Physics,
Heat engines,
Computer science,
Nanobioscience"
How to fool an unbounded adversary with a short key,"The symmetric encryption problem which manifests itself when two parties must securely transmit a message m with a short shared secret key is considered in conjunction with a computationally unbounded adversary. As the adversary is unbounded, any encryption scheme must leak information about m; in particular, the mutual information between m and its ciphertext cannot be zero. Despite this, a family of encryption schemes is presented that guarantee that for any message space in {0,1}n with minimum entropy n-lscr and for any Boolean function h:{0,1}n rarr {0,1}, no adversary can predict h(m) from the ciphertext of m with more than 1/nomega(1) advantage; this is achieved with keys of length lscr+omega(logn). In general, keys of length lscr+s yield a bound of 2-Theta(s) on the advantage. These encryption schemes rely on no unproven assumptions and can be implemented efficiently. Applications of this to cryptosystems based on complexity-theoretic assumptions are discussed and, in addition, a simplified proof of a fundamental ""elision lemma"" of Goldwasser and Micali is provided",
On agile performance requirements specification and testing,"Underspecified performance requirements can cause performance issues in a software system. However, a complete, upfront analysis of a software system is difficult, and usually not desirable. We propose an evolutionary model for performance requirements specifications and corresponding validation testing. The principles of the model can be integrated into agile development methods. Using this approach, the performance requirements and test cases can be specified incrementally, without big upfront analysis. We also provide a post hoc examination of a development effort at IBM that had a high focus on performance requirements. The examination indicates that our evolutionary model can be used to specify performance requirements such that the level of detail is commensurate with the nature of the project. Additionally, the IBM experience indicates that test driven development-type validation testing corresponding to the model can be used to determine if performance objectives have been met",
Fast Handover over Micro-MPLS-Based Wireless Networks,"Recently, there has been increasing interest in applying MPLS to IP-based wireless access networks [1]. During a handover on Mobile MPLS-based networks, a registration update message is sent from the Foreign Agent (FA) to the Home Agent (HA). This update message results in a handover delay because of the distance the registration update message must travel from the FA to the HA. To decrease frequent registration handover delays, a micro-mobility approach over MPLS-based networks (also known as Hierarchical Mobile MPLS (H-MPLS)) [2] method was proposed to overcome the limitations of the Mobile MPLS protocol. Studies have shown that with H-MPLS, the Mobile Node (MN) experiences considerably lower handover delays when compared with the delay incurred with conventional mobile MPLS [2], [3]. In this paper we propose to extend the use of H-MPLS by introducing Intermediate Label Information Base (I-LIB) on a Label Switching Router (LSR), which uses the MPLS forwarding and localized signaling. Our simulations demonstrate that with an I-LIB, the end-to-end delay is reduced as a result of a decrease in the handover latency during the establishment of a new Label Switched Path (LSP). With increasing usage of wireless devices supporting delay sensitive traffic (such as voice and video over IP), it is essential to provide lower end-to-end by minimizing handover delays since large delays cause Quality of Service (QoS) degradations.",
Simultaneous and interleaved polling: an upstream protocol for WDM-PON,"We present a simultaneous and interleaved polling algorithm for upstream traffic scheduling for a WDM-PON broadband access network. We analyze the guaranteed and maximum bandwidth, and average packet delay performance for the WDM-PON users.","Optical network units,
Passive optical networks,
Bandwidth,
Wavelength division multiplexing,
EPON,
Optical fiber networks,
Performance analysis,
Channel allocation,
Access protocols,
Computer science"
Electronic Word of Mouth: A Genre Analysis of Product Reviews on Consumer Opinion Web Sites,"Consumer opinion Web sites enable consumers to post reviews of products and services or view the experiences of other consumers. This form of writing can be considered a truly digital genre, as consumers were not able to share their opinions with other consumers in a structured, written format before the advent of the Internet. To identify rules and conventions established by the genre community, a sample of 358 product reviews was examined using a methodology that combines elements of case study research, corpus linguistics, and textual analysis. More precisely, the analysis focused on structure, content, audience appeals, sentence style, and word choice. The results of this analysis have implications for improving the design of consumer opinion Web sites with a view to making them more useful sources of consumer knowledge.",
A Coordinated Detection and Response Scheme for Distributed Denial-of-Service Attacks,"Distributed denial-of-service (DDoS) attacks present serious threats to servers in the Internet. They can exhaust critical resources at a target host with the help of a large number of compromised Internet hosts and hence deny services to legitimate clients. This paper studies some existing schemes for the detection and defense against TCP-based DDoS attacks. We propose a distributed scheme that can mitigate the damage caused by DDoS through a coordinated detection and response framework. This proposed scheme composes of a number of heterogeneous defense systems which cooperate with each other in protecting Internet servers. We have set up a network testbed for carrying out extensive experiments using real server machines, routers and software attack tools. Experimental results show that, compared to existing schemes, our proposed scheme can greatly improve the throughput of legitimate traffic and reduce the attack traffic during DDoS attacks. To investigate the scale-up behavior of our scheme, we have also developed a software simulator for larger-scale experiments. Simulation results show that our scheme performs consistently well even in networks with more than 3000 nodes and under high traffic load.",
Robust Demand-Driven Video Multicast over Ad hoc Wireless Networks,"In this paper, we address the problem of video multicasting in ad hoc wireless networks. The salient characteristics of video traffic make conventional multicasting protocols perform quite poorly, hence warranting application-centric approaches in order to increase robustness to packet losses and lower the overhead. By exploiting the path-diversity and the error resilience properties of multiple description coding (MDC), we propose a robust demand-driven video multicast routing (RDVMR) protocol. Our protocol uses a novel path based Steiner tree heuristic to reduce the number of forwarders in each tree, and constructs multiple trees in parallel with reduced number of common nodes among them. Moreover, unlike other on-demand multicast protocols, RDVMR specifically attempts to reduce the periodic (non on-demand) control traffic. We extensively evaluate RDVMR in the NS2 simulation framework and show that it outperforms existing single-tree and two-tree multicasting protocols.",
A Cross-layer Decentralized BitTorrent for Mobile Ad hoc Networks,"In recent years, a number of P2P systems, for instance, Gnutella, KaZaA, Napster, and BitTorrent, have been proposed for the wired Internet. However, these protocols are not immediately applicable to the mobile ad hoc networks (MANETs) owing to the extreme conditions MANETs operate under. Of the above protocols, although BitTorrent has several features which make it an ideal candidate for adapting to MANETs, the current specification of BitTorrent has several drawbacks which make a straightforward implementation of BitTorrent for MANETs an undesirable solution. In this paper, we investigate a straightforward implementation of BitTorrent in MANETs, termed BTI, and compare its performance with a cross-layer adaptation of BitTorrent for MANETs, termed BTM. We resolve the issues of centralized control and single point of failure in BTI by proposing mechanisms to decentralize the BitTorrent model for MANETs and provide resource/data redundancy to improve the protocol performance. In addition, the cross-layer model of BTM is more suited for use in a MANET. Our performance comparison studies show that BTM is able to outperform BTI in terms of goodput, and the number of pieces delivered, in the context of amortizing the client download expenses over more connections (that is, BTM has a higher average peer degree)",
A Cognitive Control Architecture for an Artificial Creature using Episodic Memory,"This paper describes a new cognitive control architecture incorporating episodic memory for the artificial creature Rity, the software robot component of the Ubibot, the ubiquitous robot system. The episodic memory is defined as a scalable structure that stores episodic perceptual snapshots as Rity's experience grows. The system also utilizes a temporally variant spatial map to store spatial information and a higher level procedural memory using finite state machines. The system is designed to enable Rity to be cognitive in its approach to task selection through the dual process of experiential and spatial learning. This is brought about through a multi-agent strategy based on six principal modules: perception module, internal state module, behavior selection module, interactive learning module, memory module, and motor module, to control its behavior considering its internal state. Experiments on completion of task and maintenance of ideal internal state are described. The results show that the artificial creature possesses the ability to improve its performance as its experience grows","Intelligent robots,
Cognitive robotics,
Orbital robotics,
Mobile robots,
Artificial intelligence,
History,
Control systems,
Computer architecture,
Electronic mail,
Memory architecture"
QRP01-6: Resource Optimization Subject to a Percentile Response Time SLA for Enterprise Computing,"We consider a set of computer resources used by a service provider to host enterprise applications subject to service level agreements. We present an approach for resource optimization in such an environment that minimizes the total cost of computer resources used by a service provider for an enterprise application while satisfying the QoS metric that the response time for executing service requests is statistically bounded. That is, gamma% of the time the response time is less than a pre-defined value. This QoS metric is more realistic than the mean response time typically used in the literature. Numerical results show the applicability of the approach and validate its accuracy.","Delay,
Application software,
Cost function,
Network servers,
Time factors,
Resource management,
Grid computing,
Probability distribution,
Computer science,
Computer network management"
Approximately List-Decoding Direct Product Codes and Uniform Hardness Amplification,"We consider the problem of approximately locally list-decoding direct product codes. For a parameter k, the k-wise direct product encoding of an N-bit message msg is an Nk-length string over the alphabet {0, l}k indexed by k-tuples (i1, .. ., ik) isin {1,..., N}k so that the symbol at position (i1, .. ., ik) of the codeword is msg(i 1)...msg(ik). Such codes arise naturally in the context of hardness amplification of Boolean functions via the direct product lemma (and the closely related Yao 's XOR Lemma), where typically k Lt N (e.g., k = poly log N). We describe an efficient randomized algorithm for approximate local list-decoding of direct product codes. Given access to a word which agrees with the k-wise direct product encoding of some message msg in at least an epsiv fraction of positions, our algorithm outputs a list of poly(l/epsiv) Boolean circuits computing N-bit strings (viewed as truth tables of log N-variable Boolean functions) such that at least one of them agrees with msg in at least 1 - delta fraction of positions, for delta = O(k-0.1), provided that epsiv = Omega(poly(l/k); the running time of the algorithm is polynomial in log N and 1/epsiv. When epsiv > epsivkalpha for a certain constant alpha > 0, we get a randomized approximate list-decoding algorithm that runs in time quasi-polynomial in 1/epsiv (i.e., (1/epsiv)poly log 1epsiv/)By concatenating the k-wise direct product codes with Hadamard codes, we obtain locally list-decodable codes over the binary alphabet, which can be efficiently approximately list-decoded from fewer than frac12 - epsiv fraction of corruptions as long as epsiv = Omega(poly(l/k)). As an immediate application, we get uniform hardness amplification for PNP par, the class of languages reducible to NP through one round of parallel oracle queries: If there is a language in PNP par that cannot be decided by any BPP algorithm on more that 1 $1/nOmega(1) fraction of inputs, then there is another language in PNP par that cannot be decided by any BPP algorithm on more than frac12 + 1/nomega(1) fraction of inputs","Product codes,
Error correction codes,
Decoding,
Circuits,
Encoding,
Boolean functions,
Error correction,
Code standards,
Polynomials,
Computational complexity"
Predictability of earliest deadline zero laxity algorithm for multiprocessor real-time systems,"Validation methods for hard real-time jobs are usually performed based on the maximum execution time. The actual execution time of jobs are assumed to be known only when the jobs arrive or not known until they finish. A predictable algorithm must guarantee that it can generate a schedule for any set of jobs such that the finish time for the actual execution time is no later than the finish time for the maximum execution time. It is known that any job-level fixed priority algorithm (such as earliest deadline first) is predictable. However, job-level dynamic priority algorithms (such as least laxity first) may or may not. In this paper, we investigate the predictability of a job-level dynamic priority algorithm EDZL (earliest deadline zero laxity). We show that EDZL is predictable on the domain of integers regardless of the knowledge of the actual execution times. Based on this result, furthermore, we also show that EDZL can successfully schedule any periodic task set if the total utilization is not greater than (m + 1)/2, where m is the number of processors",
ATCA: its performance and application for real time systems,"The Advanced Telecom Computing Architecture (ATCA), describes a high bandwidth, high connectivity, chassis based architecture designed principally to appeal to the telecommunications industry. The object of the exercise was to closely connect compute engines within the chassis to multiple user services brought in at the front panel. This maps closely to the needs of real time systems and the main points of the architecture are reviewed and discussed in that light. The performance of an ATCA backplane has been tested and measured using a Backplane Tester developed within a 10 Gb/s Ethernet switch project that was an early adopter of the ATCA standard. Some results from these tests are presented.",
Analysis of Physiological Responses to a Social Situation in an Immersive Virtual Environment,"An experiment was conducted in a Cave-like environment to explore the relationship between physiological responses and breaks in presence and utterances by virtual characters towards the participants. Twenty people explored a virtual environment (VE) that depicted a virtual bar scenario. The experiment was divided into a training and an experimental phase. During the experimental phase breaks in presence (BIPs) in the form of whiteouts of the VE scenario were induced for 2 s at four equally spaced times during the approximately 5 min in the bar scenario. Additionally, five virtual characters addressed remarks to the subjects. Physiological measures including electrocardiagram (ECG) and galvanic skin response (GSR) were recorded throughout the whole experiment. The heart rate, the heart rate variability, and the event-related heart rate changes were calculated from the acquired ECG data. The frequency response of the GSR signal was calculated with a wavelet analysis. The study shows that the heart rate and heart rate variability parameters vary significantly between the training and experimental phase. GSR parameters and event-related heart rate changes show the occurrence of breaks in presence. Event-related heart rate changes also signified the virtual character utterances. There were also differences in response between participants who report more or less socially anxious.",
Tversky's Parameterized Similarity Ratio Model: A Basis for Semantic Relatedness,"Numerous measures have been proposed to determine the semantic relatedness between words. Earlier approaches rely on the location of the words within the structure of a terminological ontology and are categorized into distance-based or information content models. More recently research has considered the overlapping of attributes or relationships, i.e., feature matching, between words or lexical concepts. This paper focuses on Tversky's prominent parameterized ratio model and its fundamental role in semantic relatedness measures. Existing semantic relatedness measures from the distance-based and information content categories are unified through this model. Through appropriate feature selection of properties of word objects in terminological ontologies, the model may also be used as the basis for proposing numerous other approaches to creating a variety of semantic relatedness measures",
Perspectives on Service-Oriented Computing and Service-Oriented System Engineering,"The service-oriented computing (SOC) paradigm leads the innovation of today's computing system and software development, and it can transform the human society into a new republic of all-to-all connected world. There are a number of interesting and innovative research challenges involved in achieving this vision of service-oriented system engineering (SOSE). This paper highlights some of these significant research opportunities and also the need for enhanced education in this emerging area","Systems engineering and theory,
Service oriented architecture,
Reliability engineering,
Application software,
Conferences,
Analytical models,
Collaborative software,
Collaborative work,
International collaboration,
Manufacturing"
Bandwidth Expansion Shannon Mapping for Analog Error-Control Coding,"Based on an initial idea of Shannon, we investigate an approach, named Shannon mapping, for transmission of analog messages, that is fundamentally different from current digital communication methods. We investigate 1:2 bandwidth expansion Shannon mapping based on two specific mapping curves. We demonstrate that bandwidth expansion Shannon mapping using a mapping curve does not introduce any delay and its encoding and decoding processes exhibit very low complexity. More importantly, our simulations show that the distortion performance of 1:2 bandwidth expansion Shannon mapping is significantly better than the rate-distortion bound without bandwidth expansion at moderate to high SNRs, and about 3-5 dB inferior to the rate-distortion bound with the same bandwidth expansion.","Bandwidth,
Signal mapping,
Rate-distortion,
Frequency modulation,
Digital communication,
Pulse modulation,
Delay,
Decoding,
Transmitters,
Computer errors"
An Overlay Subscription Network for Live Internet TV Broadcast,"We propose a framework, called overlay subscription network (OSN), for live Internet TV broadcast, where a subscriber can choose to watch at any time. This framework allows the source server to incrementally build a topology graph that contains the network connections not only from the server to each subscriber, but also among the subscribers themselves. With such a topology graph in place, we consider efficient overlay multicast for scalable OSN services. We first show that idling nodes, which do not receive video data for their own playback, can actually be used for data forwarding to significantly reduce the cost of overlay multicast. In light of this observation, we then propose a novel overlay multicast technique that distinguishes itself from existing schemes with these three aspects. First, the proposed technique is centered on the topology graph and can take advantage of the actual network connections among the subscribing nodes. Second, the new scheme is able to find and incorporate appropriate idling nodes in multicast to reduce network traffic. Third, with our approach, a node can be used in multiple multicast trees for data forwarding to improve the overall system performance. We evaluate the performance of the proposed technique through simulation. Our extensive studies show that the proposed framework has the potential to enable the Internet, a vehicle up to date mainly for transferring text and image data, for large-scale and cost-effective TV broadcast",
Numerical Simulation of Hall Thruster Plasma Plumes in Space,"Hall thrusters are an important form of electric propulsion that are being developed and implemented to replace chemical systems for many on-orbit propulsion tasks on communications and exploration spacecraft. A concern in the use of these devices is the possible damage their plasma plumes may cause to the host spacecraft. Computer models of Hall thruster plumes play an important role in integration of these devices onto spacecraft as the space environment is not easily reproduced in ground testing facilities. In this paper, an advanced hybrid particle-fluid model is applied to model the plumes of the Hall thrusters used on the Russian Express and European SMART-1 spacecraft. The results from the model are compared directly with measurements of ion current density and ion energy distributions taken in space by these spacecraft","Numerical simulation,
Plasma simulation,
Space vehicles,
Plasma chemistry,
Plasma devices,
Propulsion,
Chemicals,
Testing,
Density measurement,
Current measurement"
Dynamic Fingerspelling Recognition using Geometric and Motion Features,This paper presents the Australian sign language (Auslan) fingerspelling recognizer (APR): a system capable of recognizing signs consisting of Auslan manual alphabet letters from video sequences. The APR system uses a combination of geometric features and motion features based on optical flow which are extracted from video sequences. The sequence of features are then classified using hidden Markov models (HMMs). Tests using a vocabulary of twenty signed words showed the system could achieve 97% accuracy at the letter level and 88% at the word level by using a finite state grammar network and embedded training.,"Feature extraction,
Handicapped aids,
Hidden Markov models,
Testing,
Skin,
Australia,
Shape,
Video sequences,
Geometrical optics,
Image motion analysis"
Firewall Policy Reconstruction by Active Probing: An Attacker's View,Having a firewall policy that is correct and complete is crucial to the safety of the computer network. An adversary will benefit a lot from knowing the policy or its semantics. In this paper we show how an attacker can reconstruct a firewall's policy by probing the firewall by sending tailored packets into a network and forming an idea of what the policy looks like. We present two approaches of compiling this information into a policy that can be arbitrary close to the original one used in the deployed firewall. The first approach is based on region growing from single firewall response to sample packets. The other approach uses split-and-merge in order to divide the space of the firewall's rules and analyzes each independently. Both techniques merge the results obtained into a more compact version of the policies reconstructed.,
Cryptanalysis and Improvement of User Authentication Scheme using Smart Cards for Multi-Server Environments,"For providing the login service in multi-server environments, Fan, Xu, and Li presented a remote user authentication scheme using smart cards. In this paper, we demonstrate that Fan-Xu-Li's scheme is vulnerable to the parallel session attack. That is, when a legal user logs in a server, an adversary without knowing any secret information can easily impersonate the user to log in other authorized servers. It means that a serious security flaw exists in Fan-Xu-Li's scheme. In addition to being practical, it is desirable to avoid relying on timestamps for security in their scheme. We therefore propose an improved scheme to overcome above disadvantages. As a unilateral authentication mechanism, our improved scheme is more suitable for real-life cryptographic applications than Fan-Xu-Li's scheme",
Efficiency of Multi-Valued Encoding in SAT-based ATPG,"Automatic Test Pattern Generation (ATPG) is one of the core algorithms in testing of digital circuits and systems. Due to recent advances in algorithms to solve Boolean Satisfiability (SAT), there is a renewed interest in SAT-based ATPG. While the early approaches only used two-valued logic, modern tools have to use multiple values to model unknown values and tri-state elements for buses. In this paper we present a detailed study on how to chose the multi-valued encoding for SAT-based ATPG. The techniques have been implemented and evaluated on large industrial benchmarks.",
Dimensions Characterizing Programming Feature Usage by Information Workers,"Information workers such as administrative staff, consultants, and their managers constitute one of the largest groups of end users, yet little research about their usage of programming features is available to guide development of end user programming tools. In this paper, we describe our survey of over 800 information workers and our analysis of their feature usage in applications such as spreadsheets, browsers, and databases. Our factor analysis reveals three clusters of features - macro features, linked structure features, and imperative features - such that information workers with an inclination to use a feature in each cluster also were inclined to use other features in that cluster, even though each cluster spans several tools. We discuss the implications for research aimed at providing end user programming tools for information workers","Programming profession,
Spatial databases,
Information analysis,
Web pages,
Computer science,
Research and development management,
Data analysis,
Radio access networks,
Home appliances,
Design automation"
Skin Detection using a Markov Random Field and a New Color Space,"In this paper, human skin detection is performed using a new color space coordinate and a Markov random field based approach. The proposed color space uses a variant of the principal component analysis technique to reduce the number of color components. The MRF model takes into account the spatial relations within the image that are included in the labeling process through statistical dependence among neighboring pixels. Since only two classes are considered the Ising model is used to perform the skin/non-skin classification process.","Skin,
Markov random fields,
Eigenvalues and eigenfunctions,
Image color analysis,
Principal component analysis,
Clustering algorithms,
Image segmentation,
Covariance matrix,
Pixel,
Face detection"
Message Dropping Attacks in Overlay Networks: Attack Detection and Attacker Identification,"Overlay multicast networks are used by service providers to distribute contents such as Web pages, streaming multimedia data, or security updates to a large number of users. However, such networks are extremely vulnerable to message dropping attacks by malicious or selfish nodes that intentionally drop packets they are required to forward. It is difficult to detect such attacks both efficiently and effectively, not mentioning to further identify the attackers, especially when members in the overlay switch between online/offline statuses frequently. We propose a random-sampling-based scheme to detect such attacks, and a path-resolving-based scheme to identify the attack nodes. Our schemes work for dynamic overlay networks and do not assume the global knowledge of the overlay hierarchy. Analysis and simulation results show that our schemes are bandwidth-efficient and they both have high detection/identification rates but low false positive rates",
Current and New Developments in Spam Filtering,"This paper provides an overview of current and potential future spam filtering techniques. We examine the problems spam introduces, what spam is and how we can measure it. The paper primarily focuses on automated, non-interactive filters, with a broad review ranging from commercial implementations to ideas confined to current research papers. Both machine learning and non-machine learning based filters are reviewed as potential solutions and a taxonomy of known approaches presented. While a range of different techniques have and continue to be evaluated in academic research, heuristic and Bayesian filtering - along with its variants - provide the greatest potential for future spam prevention","Unsolicited electronic mail,
Filters,
Filtering,
Protocols,
Humans,
Machine learning,
Legislation,
Computer science,
Software engineering,
Taxonomy"
A Dynamic Multi-Objective Evolutionary Algorithm Based on an Orthogonal Design,"There are rather few articles in the literature so far that deal with dynamic multi-objective optimization problems. This article introduces a dynamic orthogonal multi-objective evolutionary algorithm called ""DOMOEA"", that generalizes an earlier paper of ours (on an orthogonal multi-objective evolutionary algorithm (OMOEA-II) (Zeng et al., 2005)) to dynamic environments. DOMOEA solves a particular class of dynamic multi-objective optimization problems, namely those that have continuous decision variables. This new algorithm uses the evolutionary results, before any environmental change, as the initial population after the environmental change. It applies an ""orthogonal design method"" to enhance the fitness of the population during the static stages between two successive changes of environment. We obtained satisfactory results when testing this algorithm against the benchmark problems proposed in the literature. Our new algorithm is based on an ordinary evolutionary algorithm that does not have the capacity to detect environmental changes. Hence it has a comparatively simple structure, making comparisons with other dynamic multi-objective evolutionary algorithms relatively easy.","Evolutionary computation,
Algorithm design and analysis,
Design optimization,
Space technology,
Design methodology,
Heuristic algorithms,
Change detection algorithms,
Constraint optimization,
Computer science,
Convergence"
"""Services Science"" and Services Layer Added Strategic Technology Roadmapping","Next generation MOT moves its focus to the higher value added services innovations by integrating more sophisticated service functions to the conventional products and systems. This paper provides a scheme for the newly emerging ""service science"" expected to support services innovations and derives practical methodology to integrate new services to the strategic technology roadmap and roadmappinng by introducing a new independent layer of services between the market and products layers. Here, the concept of ""service"" is widely defined as ""a supporting activity to help an individual or organization to achieve its objective"". Accordingly, it includes physical supporting functions, psychological supporting functions, intellectual supporting functions, spiritual supporting functions, as well as technological supporting functions provided through products. Valuating products and systems with their added services improves customer satisfaction and the total customer value should be considered as the summation of the products/system value, added services value, and individually user added value. As for a new methodology for managing the service integrated technology/products innovation, this paper proposes a service-integrated technology roadmap/roadmapping, which involves a new concept of functions, ""requiring functions"" and ""supplying functions"" to fill the gaps between the market and service layers, as well as the service and products layers",
Better Burst Detection,"A burst is a large number of events occurring within a certain time window. Many data stream applications require the detection of bursts across a variety of window sizes. For example, stock traders may be interested in bursts having to do with institutional purchases or sales that are spread out over minutes or hours. In this paper, we present a new algorithmic framework for elastic burst detection [1]: a family of data structures that generalizes the Shifted Binary Tree, and a heuristic search algorithm to find an efficient structure given the input. We study how different inputs affect the desired structures and the probability to trigger a detailed search. Experiments on both synthetic and real world data show a factor of up to 35 times improvement compared with the Shifted Binary Tree over a wide variety of inputs, depending on the inputs.",
Robust and Efficient Data Gathering for Wireless Sensor Networks,"This paper describes a new strategy for data gathering in wireless sensor networks that takes into account the need for both energy saving, typical of such networks, and for a reasonable tradeoff between robustness and efficiency. The proposed algorithm implements an efficient strategy for retransmission of lost packets by discovering alternative routes and making clever use of multiple paths when necessary; in order to do that we build upon the general framework presented in recent works, that provided a formulation of duplicate and order insensitive aggregation functions, and by taking advantage of some intrinsic characteristics of the wireless sensor networks, we exploit implicit acknowledgment of reception and smart caching of the data. Assuming that, unlike in an ideal scenario, data originates from only a subset of all sensors, our approach provides a better usage of the resources and a minimization of the traffic in the network, and, as a consequence, of the overall consumed energy.",
Cooperative Geographic Routing in Wireless Sensor Networks,"Node cooperation is one unique feature distinguishing wireless sensor networks from conventional wireless networks. In this paper, we explore cooperative geographic routing in coalition-aided wireless sensor networks. We first propose a new communication paradigm, i.e., cooperative data transport, where sensor nodes within a coalition cooperatively transmit data via a three-phase procedure. We then treat the routing path selection as a multistage decision problem, where at each stage, the coalition head would choose the next hop destination to minimize the corresponding energy consumption. Next, we investigate the impact of location errors and synchronization errors on the routing performance degradation. Furthermore, we generalize our study to let the nodes with good channel conditions participate in cooperative forwarding, and keep others silent to reduce energy consumption further. Our results demonstrate that the coalition-aided sensor network architecture is most amenable to cooperative sensor networks, simply because it improves energy efficiency and reduces control overhead significantly","Wireless sensor networks,
Routing protocols,
Broadcasting,
Energy consumption,
Degradation,
Performance gain,
Transmitters,
Intelligent networks,
Computer science,
Energy efficiency"
A Novel Approach for Evaluating Performance of Discontinuous Pulse Width Modulation Schemes for Three Phase Voltage Source Inverter,A simple approach is presented for obtaining the switching loss factor (SLF) and quality factor (QF) used as merit factors for evaluating performances of various discontinuous pulse width modulating (DPWM) signals derived from a generalized discontinuous pulse width modulation scheme. This approach precludes the need for rigorous calculation of inverter losses as employed in most traditional methods. Results are given to validate performance criteria and to show areas of application,
Zero-Error Source&#8211;Channel Coding With Side Information,"This correspondence presents a novel application of the theta function defined by Lovasz. The problem of coding for transmission of a source through a channel without error when the receiver has side information about the source is analyzed. Using properties of the Lovasz theta function, it is shown that separate source and channel coding is asymptotically suboptimal in general. By contrast, in the case of vanishingly small probability of error, separate source and channel coding is known to be asymptotically optimal. For the zero-error case, it is further shown that the joint coding gain can in fact be unbounded. Since separate coding simplifies code design and use, conditions on sources and channels for the optimality of separate coding are also derived","Memoryless systems,
Channel coding,
Communication systems,
Materials science and technology,
Information analysis,
Laboratories,
Information theory,
Codes,
Computer errors,
Feedback"
MasePlanner: A Card-Based Distributed Planning Tool for Agile Teams,"Supporting agile teams from afar as they plan projects is a challenge. In this paper, we present a planning tool, MasePlanner, which supports agile teams as they collaborate in a distributed environment during their planning meetings. Combined with an audio link, our tool supports natural interaction similar to those found in collocated agile planning environments. MasePlanner supports planning in both vertical and horizontal tabletop workspaces by allowing participants to use interaction techniques such as moving and piling of story cards. By implementing a strong index-card metaphor, the tool aims to be more closely aligned with natural environments",
Towards Whole Arm Manipulation by Contact State Transition,"This paper discusses the whole arm manipulation allowing the contact state transition. For manipulation of an object under fully constrained, the contact state transition becomes necessary. In order to realize the object manipulation, we first derive the feasible direction of the object manipulation by analyzing the active/passive closure properties for every combination of contact states. Second, we derive the set of joint torque to move the object in the feasible direction. These analyses also provide the joint torque to realize the manipulation at the planned contact states. Effectiveness of the proposed method is confirmed by some simulation results",
Abstract Interpretation of PIC Programs through Logic Programming,"A logic based general approach to abstract interpretation of low-level machine programs is reported. It is based on modelling the behavior of the machine as a logic program. General purpose program analysis and transformation of logic programs, such as partial evaluation and convex hull analysis, are applied to the logic based model of the machine. A small PIC microcontroller is used as a case study. An emulator for this microcontroller is written in Prolog, and standard programming transformations and analysis techniques are used to specialise this emulator with respect to a given PIC program. The specialised emulator can now be further analysed to gain insight into the given program for the PIC microcontroller. The method describes a general framework for applying abstractions, illustrated here by linear constraints and convex hull analysis, to logic programs. Using these techniques on the specialised PIC emulator, it is possible to obtain constraints on and linear relations between data registers, enabling detection of for instance overflows, branch conditions and so on.","Logic programming,
Performance analysis,
Microcontrollers,
Programming profession,
Computer science,
High level languages,
Performance evaluation,
Performance gain,
Software libraries,
Memory management"
Combining Generative and Discriminative Methods for Pixel Classification with Multi-Conditional Learning,"It is possible to broadly characterize two approaches to probabilistic modeling in terms of generative and discriminative methods. Provided with sufficient training data the discriminative approach is expected to yield superior accuracy as compared to the analogous generative model since no modeling power is expended on the marginal distribution of the features. Conversely, if the model is accurate the generative approach can perform better with less data. In general it is less vulnerable to overfitting and allows one to more easily specify meaningful priors on the model parameters. We investigate multi-conditional learning - a method combining the merits of both approaches. Through specifying a joint distribution over classes and features we derive a family of models with analogous parameters. Parameter estimates are found by optimizing an objective function consisting of a weighted combination of conditional log-likelihoods. Systematic experiments in the context of foreground/background pixel classification with the Microsoft-Berkeley segmentation database using mixtures of factor analyzers illustrate tradeoffs between classifier complexity, the amount of training data and generalization accuracy. We show experimentally that this approach can lead to models with better generalization performance than purely generative or discriminative approaches",
SDP gaps and UGC-hardness for MAXCUTGAIN,"Given a graph with maximum cut of (fractional) size c, the Goemans-Williamson semidefinite programming (SDP) algorithm by M. Goemans and D. Williamson (1995) is guaranteed to find a cut of size .878 middot c. However this guarantee becomes trivial when c is near frac12, since a random cut has expected size frac12. Recently, M. Charikar and K. Worth (2004) (analyzing an algorithm of U. Feige and G. Langberg (2001)) showed that given a graph with maximum cut frac12 + epsiv, one can find a cut of size frac12 + Omega(epsiv/ log(1/epsiv)). The main contribution of our paper is twofold: 1. We give a natural frac12 + epsiv vs. frac12 + O(epsiv/ log(1/epsiv)) SDP gap for MAXCUT in Gaussian space. This shows that the SDP-rounding algorithm of Charikar-Worth is essentially best possible. Further, the ""s-linear rounding functions"" used in the works of M. Charikar and K. Worth (2004) and U. Freige and M. Langberg (2001) arise as optimizers in our analysis, somewhat confirming a suggestion of U. Freige and M. Langberg (2001). 2. We show how this SDP gap can be translated into a long code test with the same parameters. This implies that beating the Charikar-Worth guarantee with any efficient algorithm is NP-hard, assuming the unique games conjecture (UGC) by S. Khot (2002). We view this result as essentially settling the approximability of MAXCUT, assuming UGC. Building on (1) we show how ""randomness reduction"" on related SDP gaps for the QUADRATICPROGRAMMING programming problem lets us make the Omega(log(1/epsiv)) gap as large as Omega(log n) for n-vertex graphs. In addition to optimally answering an open question of N. Alen et al. (2006), this technique may prove useful for other SDP gap problems. Finally, illustrating the generality of our technique in (2), we also show how to translate Reeds's SDP gap by J. Reeds (1993) for the Grothendieck Inequality into a UGC-hardness result for computing the par middot parinfin rarr 1 norm of a matrix",
Multi-Project System-on-Chip (MP-SoC): A Novel Test Vehicle for SoC Silicon Prototyping,"In this paper, we propose a novel SoC design methodology referred to as multi-project system-on-a-chip (MP-SoC), which can integrate multiple heterogeneous SoC design projects into a single chip such that the total silicon prototyping cost for these projects can be greatly reduced due to the sharing of a common SoC platform. The design flows for the system architecture, individual IP blocks, as well as the logic and physical implementations of MP-SoC are explored. The isolation mechanism to prevent interference among the IPs and the arbitration mechanism to grant the bus usage for master IPs are also presented. A test chip named MP-SoC-l that includes 8 SoC projects from 4 universities was selected as a demonstration example for verifying the MP-SoC design concept. This chip is designed and implemented in TSMC 0.13 mum CMOS generic logic process technology, and the total silicon area for MP-SoC-l test chip is 4950 mum x 4938 mum. Experimental results of MP-SoC-l test chip show that all projects are successfully implemented in the common platform and 82.91% silicon area is saved with this MP- SoC methodology as compared with the case where multiple SoC projects are fabricated individually.",
Strategic Network Formation through Peering and Service Agreements,"We introduce a game theoretic model of network formation in an effort to understand the complex system of business relationships between various Internet entities (e.g., autonomous systems, enterprise networks, residential customers). This system is at the heart of Internet connectivity. In our model we are given a network topology of nodes and links where the nodes (modeling the various Internet entities) act as the players of the game, and links represent potential contracts. Nodes wish to satisfy their demands, which earn potential revenues, but nodes may have to pay (or be paid by) their neighbors for links incident to them. By incorporating some of the qualities of Internet business relationships, we hope that our model has predictive value. Specifically, we assume that contracts are either customer-provider or peering contracts. As often occurs in practice, we also include a mechanism that penalizes nodes if they drop traffic emanating from one of their customers. For a natural objective function, we prove that the price of stability is at most 2. With respect to social welfare, however, the prices of anarchy and stability can both be unbounded, leading us to consider how much we must perturb the system to obtain good stable solutions. We thus focus on the quality of Nash equilibria achievable through centralized incentives; solutions created by an ""altruistic entity"" (e.g., the government) able to increase individual payouts for successfully routing a particular demand. We show that if every payout is increased by a factor of 2, then there is a Nash equilibrium as good as the original centrally defined social optimum. We also show how to find equilibria efficiently in multicast trees. Finally, we give a characterization of Nash equilibria as flows of utility with certain constraints, which helps to visualize the structure of stable solutions and provides us with useful proof techniques",
Comparison of Robust Cooperation Strategies for P2P Content Distribution Networks with Multiple Source Download,"The performance of peer-to-peer (P2P) content distribution networks depends highly on the coordination of the peers. Sophisticated cooperation strategies, such as the multiple source download, are the foundation for efficient file exchange. The detailed performance of the strategies are determined by the peer characteristics and the peer behaviour, such as the number of parallel upload connections, the selfishness, or the altruistic re-distribution of data. The purpose of this work is to evaluate and investigate different cooperation strategies for multiple source download and select the best one for a scenario for even leeching peers, i.e. peers which depart as soon as they have finished their download. The question arises whether the cooperation strategy can smoothen the overall performance degradation caused by a selfish peer behaviour. As performance indicator the evolution of the numbers of copies of a chunk and the experienced download times of files is applied. The considered scenarios comprise best-case (altruistic peers) and worst-case scenarios (selfish peers). We further propose a new cooperation strategy to improve the file transfer even when mainly selfish peers are present, the CygPriM (cyclic priority masking) strategy. The strategy allows an efficient P2P based content distribution using ordered chunk delivery with only local information available at a peer",
Synthesis of Distributed Controllers by Means of a Monolithic Approach,"This contribution presents an approach for synthesis of distributed logic controllers based on modular plant models and forbidden states. Modular plant models are defined that keep the state distributed. Forbidden states are formulated by means of predicates. A backward search algorithm is applied to determine a monolithic controller for the specification. Afterwards, the controller is split into a network of communicating local controllers each imposing control on a part of the plant. It is proven that the network of local controllers realizes the same behaviour at the plant as the monolithic controller. An example illustrates the methodology.",
Interactive and Extensible Framework for Execution and Monitoring of Wireless Sensor Networks,"As sensor networks become more prevalent in complex and sophisticated application domains, there is need for a simple execution and monitoring environment for repeatable experimentation and for easy transfer to in-situ real world environments. We have developed an environment for the execution and monitoring of sensor network services. It supports the requirements for the verification and testing of sensor network services, whether simulated, emulated, or real. The running of simulated, emulated, and real sensor networks allows different levels of abstraction and virtualization necessary for the verification and performance evaluation of sensor network applications and protocols. ISEE is an interactive sensor network execution environment that allows for control and access to simulated, emulated, and real sensor networks. The control and access environment provides remote execution, logging, interaction, and analysis facilities independent of the implementation of the sensor network. This framework allows for extensibility, scenario creation, and experiment repeatability. This provides visibility and repeatability to sensor network experimentation that may not be otherwise available. The combination of our framework with our distributed service framework allows for reactive and language independent user and developer interaction to a sensor networks",
The Simulation Project Life-Cycle: Models and Realities,"Various issues regarding simulation life-cycle models are discussed in the panel session. Simulation life-cycles models have received little attention, and interest generation in this topic is sought and it is hoped that new ideas for development, teaching, and use of these models are stimulated",
New Limits on Fault-Tolerant Quantum Computation,"We show that quantum circuits cannot be made fault-tolerant against a depolarizing noise level of thetas = (6 - 2radic2)/7 ap 45%, thereby improving on a previous bound of 50% (due to Razborov, 2004). More precisely, the circuit model for which we prove this bound contains perfect gates from the Clifford group (CNOT, Hadamard, S, X, Y, Z) and arbitrary additional one-qubit gates that are subject to depolarizing noise thetas. We prove that this set of gates cannot be universal for arbitrary (even classical) computation, from which the upper bound on the noise threshold for fault-tolerant quantum computation follows","Quantum computing,
Fault tolerance,
Computational modeling,
Noise level,
Circuit faults,
Upper bound,
Error correction codes,
Circuit noise,
Physics computing,
Computer errors"
Design and Analysis of a Self-Tuning Proportional and Integral Controller for Active Queue Management Routers to Support TCP Flows,,
Efficient detection and exploitation of infeasible paths for software timing analysis,"Accurate estimation of the worst-case execution time (WCET) of a program is important for real-time embedded software. Static WCET estimation involves program path analysis and architectural modeling. Path analysis is complex due to the inherent difficulty in detecting and exploiting infeasible paths in a program's control flow graph. In this paper, we propose an efficient method to exploit infeasible path information for WCET estimation without resorting to exhaustive path enumeration. We demonstrate the efficiency of our approach for some real-life control-intensive applications",
Particle Swarm Optimization for Fuzzy c-Means Clustering,"A new fuzzy c-means clustering algorithm based on particle swarm optimization (PSOFCM) is presented after analyzing the advantages and disadvantages of the classical fuzzy c-means clustering algorithm. It avoids the local optima, and also is robust to initialization. The fluctuation however has appeared in the new algorithm, so the improved PSOFCM has been proposed finally which has better convergence to lower quantization errors. We compared the performance of PSOFCM, improved PSOFCM and FCM with IRIS testing data. The experiments show that the performance of improved PSOFCM is far better than FCM and this is a viable and effective clustering algorithm",
Boosting Genetic Algorithms with Self-Adaptive Selection,"In this paper we evaluate a new approach to selection in genetic algorithms (GAs). The basis of our approach is that the selection pressure is not a superimposed parameter defined by the user or some Boltzmann mechanism. Rather, it is an aggregated parameter that is determined collectively by the individuals in the population. We implement this idea in two different ways and experimentally evaluate the resulting genetic algorithms on a range of fitness landscapes. We observe that this new style of selection can lead to 30-40% performance increase in terms of speed.",
On The Reliability of Wireless Sensor Networks,"In wireless sensor networks (WSN), reliable monitoring of a phenomenon (or event detection) depends on the collective data provided by the target cluster of sensors and not on any individual node. In this paper we define a WSN reliability measure that considers the aggregate flow of sensor data into a sink node (gateway or cluster head). Given an estimation of the data generation rate and the failure probability of each sensor, we formulate the reliability measure and show that computing this measure for an arbitrary WSN is WSN. We then consider some special cases where we can either compute or approximate (bound) the reliability using an efficient algorithm. Finally, we present some numerical results that demonstrate some of the applications of our algorithms. Reliability evaluation tools are important in the context of design and analysis of sensitive information gathering sensor networks.","Wireless sensor networks,
Sensor phenomena and characterization,
Clustering algorithms,
Temperature sensors,
Remote monitoring,
Relays,
Computer network reliability,
Computer networks,
Event detection,
Fluid flow measurement"
Multi-Scale Integrated Information and Telecommunications System (MIITS): First Results from a Large-Scale End-to-End Network Simulator,"Performing realistic simulations of Internet packet traffic on a national or global level is a daunting task from both the modeling and the computational perspective. We present the MIITS (multi-scale integrated information and telecommunications system) tool that implements a novel approach to network simulation and report first scaling results from a realistic Internet scenario. MIITS' end-to-end approach to network simulation relies on modules for (i) accurate network topology and capacity representation, (ii) realistic communication session generation based on the activities of an agent population that is statistically equivalent to the population in a large metropolitan area, (iii) the actual scalable packet-level network simulation that is based on distributed event-driven technology, and (iv) analysis of large amounts of simulation output data. We present a sample simulation of a Los Angeles network as an intermediate step toward the vision of national-level simulation","Large scale integration,
Computational modeling,
Analytical models,
Discrete event simulation,
Internet,
Telecommunication traffic,
Traffic control,
Telecommunication computing,
IP networks,
Network topology"
New Channel Estimation Exploiting Reliable Decision-Feedback Symbols for OFDM Systems,"The conventional pilot-aided orthogonal frequency division multiplexing (OFDM) systems use the fixed number of pilots to estimate the channel. In this paper, we propose a new channel estimation scheme that exploits reliable decision-feeback symbols in the OFDM systems. The reliable symbols having low symbol error rate can be used to refine the channel estimate again. The resultant algorithm gives a more accurate channel estimate and lower bit error rate (BER) than the conventional method. The analysis of Cramér Rao lower bound (CRLB) and simulation results verify our claims that the enhanced channel estimation and lower BER can be achieved.","Channel estimation,
OFDM,
Maximum likelihood estimation,
Bit error rate,
Error analysis,
Analytical models,
Digital video broadcasting,
Wireless LAN,
Estimation error,
Bandwidth"
MODEL ADAPTATION FOR DIALOG ACT TAGGING,"In this paper, we analyze the effect of model adaptation for dialog act tagging. The goal of adaptation is to improve the performance of the tagger using out-of-domain data or models. Dialog act tagging aims to provide a basis for further discourse analysis and understanding in conversational speech. In this study we used the ICSI meeting corpus with high-level meeting recognition dialog act (MRDA) tags, that is, question, statement, backchannel, disruptions, and floor grabbers/holders. We performed controlled adaptation experiments using the Switchboard (SWBD) corpus with SWBD-DAMSL tags as the out-of-domain corpus. Our results indicate that we can achieve significantly better dialog act tagging by automatically selecting a subset of the Switchboard corpus and combining the confidences obtained by both in-domain and out-of-domain models via logistic regression, especially when the in-domain data is limited.",
Necessary and Sufficient Condition for a Class of Planar Dynamical Systems Related to CNNs to be Completely Stable,"We study global dynamical behavior of cellular neural networks (CNNs) consisting of two cells. Since the output characteristic of each cell is expressed by a piecewise-linear function, a CNN with two cells is considered as a planar piecewise-linear dynamical system. We present the necessary and sufficient condition for such a CNN to be completely stable under the assumptions that: 1) self-coupling coefficients take the same value greater than one and 2) biases are set to zero. The condition is explicitly expressed in terms of coupling coefficients between cells","Sufficient conditions,
Cellular neural networks,
Neural networks,
Stability analysis,
Piecewise linear techniques,
Circuit stability,
Differential equations,
Computer science,
Image processing,
Recurrent neural networks"
Modeling Intrusion Detection System by Discovering Association Rule in Rough Set Theory Framework,"In Intrusion Detection Systems, many intelligent information processing methods, data miming technology and so on have been applied to generating attack signatures automatically, updating signatures easily and improving detection accuracy with ultra data set. This paper presents an improved association rule discovering system under rough set theory framework of modeling IDSs. The system makes association rule applicable in classifying fields. The system exploits data reductions, rule selection, feature selection to improve detection accuracy and reduce false alarm and unreal alarm. Empirical results illustrate that the intrusion detection model can detect intrusion accurately.",
An Fpga Based Coprocessor for Cancer Classification Using Nearest Neighbour Classifier,"This paper discusses the suitability of reconfigurable computing to speedup classification problems using the nearest neighbour (1NN) classifier. INN classifier is widely used in the literature especially in real-time applications such as face recognition, on-line hand-written character recognition and medical applications where the performance enhancement in terms of speed is desirable. To evaluate the effectiveness of our implementation on field programmable gate arrays (FPGAs), experiments were carried out on two medical data sets. Results have shown that the classification accuracy is exactly same for both FPGAs and microprocessor (muP) based solutions with FPGA has superior speed performances",
Stability of Internet-Based Control Systems with Uncertainties and Multiple Time-Varying Delays,"In this paper, based on remote control and local control strategy, a class of hybrid multi-rate control models with uncertainties and multiple time-varying delays is formulated and their robust stability properties are investigated. By Lyapunov-Krasovskii functions and apply it to a descriptor model transformation, some new criteria of robust stability for such Internet-based control systems are established. Numerical example and simulation are given to illustrate the effectiveness of the theoretical results",
Routing in Multi-Commodity Sensor Networks Based on Partial Differential Equations,"In this work, a method for routing in a wireless sensor network that carries the traffic of multiple commodities is introduced. A multi-commodity sensor network is an extension of a single commodity sensor network. In a single commodity sensor network, all of the packets generated by the sensors are of the same type, and it is desired that they be sent to one or more sinks in an anycast way. In a multi-commodity sensor network, the packets are categorized into several commodities, and the packets belonging to each commodity must be sent to one of the sinks of that commodity. The single commodity problem is studied in previous work, where we introduced a mathematical framework based on partial differential equations analogous to Maxwell's equations in electrostatics. In this work we extend the single commodity methodology to multiple commodities. This extension is made by appropriate changes in the cost function of the optimization problem that gives routes. We show that under certain conditions on the cost function, the multi-commodity problem can be decomposed into multiple single commodity problems, and the solution of the multi-commodity problem can be uniquely expressed in terms of the solutions of the single-commodity problems.","Routing,
Partial differential equations,
Wireless sensor networks,
Telecommunication traffic,
Electrostatics,
Cost function,
Temperature sensors,
Sensor phenomena and characterization,
Maxwell equations,
Frequency"
Image Alignment Using Learning Prior Appearance Model,"A new approach to align an image of a textured object with a given prototype is proposed. Visual appearance of the images, after equalizing their signals, is modeled with a Markov-Gibbs random field with pairwise interaction. Similarity to the prototype is measured by a Gibbs energy of signal cooccurrences in a characteristic subset of pixel pairs derived automatically from the prototype. An object is aligned by an affine transformation maximizing the similarity by using an automatic initialization followed by gradient search. Experiments confirm that our approach aligns complex objects better than popular conventional algorithms.",
Loop Pipelining for High-Throughput Stream Computation Using Self-Timed Rings,We present a technique for increasing the throughput of stream processing architectures by removing the bottlenecks caused by loop structures. We implement loops as self-timed pipelined rings that can operate on multiple data sets concurrently. Our contribution includes a transformation algorithm which takes as input a high-level program and gives as output the structure of an optimized pipeline ring. Our technique handles nested loops and is further enhanced by loop unrolling. Simulations run on benchmark examples show a 1.3 to 4.9times speedup without unrolling and a 2.6 to 9.7times speedup with twofold loop unrolling,
Defense against Intrusion in a Live Streaming Multicast System,"Application-level multicast systems are vulnerable to attacks that impede nodes from receiving desired data. Live streaming protocols are especially susceptible to packet loss induced by malicious behavior. We describe SecureStream, an application-level live streaming system built using a pull-based architecture that results in improved tolerance of malicious behavior. SecureStream is implemented as a layer running over Fireflies, an intrusion-tolerant membership protocol. Our paper describes the SecureStream system and offers simulation and experimental results confirming its resilience to attack",
Pit Pattern Classification of Zoom-Endoscopic Colon Images using Histogram Techniques,"Histogram-based techniques for an automated classification of magnifying endoscope images with respect to pit patterns of colon lesions are discussed and compared. Currently, the results only allow a support of human observation especially due to the large number of false negatives of neoplastic lesions","Pattern classification,
Colon,
Histograms,
Lesions,
Cancer,
Endoscopes,
Biomedical imaging,
Colonoscopy,
Colonic polyps,
Spraying"
Learning Top-Down Grouping of Compositional Hierarchies for Recognition,"The complexity of real world image categorization and scene analysis requires compositional strategies for object representation. This contribution establishes a compositional hierarchy by first performing a perceptual bottom-up grouping of edge pixels to generate salient contour curves. A subsequent recursive top-down grouping yields a hierarchy of compositions. All entities in the compositional hierarchy are incorporated in a Bayesian network that couples them together by means of a shape model. The probabilistic model underlying top-down grouping as well as the shape model is learned automatically from a set of training images for the given categories. As a consequence, compositionality simplifies the learning of complex category models by building them from simple, frequently used compositions. The architecture is evaluated on the highly challenging Caltech 101 database1 which exhibits large intra-category variations. The proposed compositional approach shows competitive retrieval rates in the range of 53 .0 ± 0 .49%.",
AgentJ: Enabling Java NS-2 Simulations for Large Scale Distributed Multimedia Applications,"We present a framework, called AgentJ that extends the NS-2 platform to support the simulation and performance analysis of Java network applications. We particularly focus on the simulation of P2P networks and have already integrated the P2PS middleware for the simulation of super-peer networks for discovering participants in large scale Internet applications, which has a high significance for distributed multimedia applications to enable overlays for the discovery and searching across large numbers of distributed multimedia resources. AgentJ builds upon the numerous years of networking research with NS-2 and leverages the Protolib toolkit from NRL to facilitate the passing of real data between NS-2 nodes, thereby creating a platform for the simulation of content-based middleware and applications. AgentJ currently supports UDP unicast and multicast and maintains compatible programming interfaces with the standard Java network package as well as the NS-2 scripting interfaces employed for staging simulations","Java,
Large-scale systems,
Application software,
Protocols,
Peer to peer computing,
Middleware,
Analytical models,
Computational modeling,
Computer simulation,
Computer science"
Underwater Image Segmentation with Maximum Entropy based on Particle Swarm Optimization (PSO),"The contrast of the underwater images is often extraordinarily low due to the ray, assimilating of water, illuminating condition and so on. It is not good for the pretreatment like edge detection and image segmentation. The theory of entropy has been widely used in the pre-process of under water images. However the time-consuming computation is often an obstacle in real time application systems. In this paper, the image thresholding approach with the index of entropy maximization of the grayscale histogram based on a new optimization algorithm, namely, the particle swarm optimization (PSO) algorithm is proposed to deal with underwater image. The experiments of segmenting the underwater images are illustrated to show that the proposed method can get ideal segmentation result with less computation cost",
Emergency Care Management with Location-Aware Services,"As indoor localization technologies become more affordable and sophisticated. An emerging market has yet to be realized commercially in the indoor environment. We think that the first adopter will arise from the healthcare domain where the need is the most critical. Thus, we aim to implement a set of location-aware services to assist in the management of the emergency department. We apply NTU Taroko, an active RFID module for to real time location tracking on patients, hospital assets, and medical staffs. In addition, we integrate context-aware system proactively to infer event notifications for reminding physicians and nurses. With the proposed system, we can shorten the process of emergency visit effectively and improve the quality of emergency care",
Pain Recognition Using Artificial Neural Network,"Facial expressions are a key index of emotion. To make use of the information afforded by facial expression for emotion science and clinical practice, reliable, valid, and efficient methods of measurement are critical. Enabling computer systems to recognize facial expressions and infer emotions from them is a challenging research topic. In this paper, we present an efficient video analysis technique for recognition of a specific expression, pain, from human faces. We employ an automatic face detector and facial feature tracker for face detection and feature extraction respectively. The face detector uses skin color modeling approach. For pain recognition, location and shape features of the detected faces are computed. These features are then used as inputs to the artificial neural network which uses standard error backpropagation algorithm for classification of painful and painless faces",
Modular Mobile Docking Station Design,"Large scale robotic teams are capable of working independently or cooperatively to carry out a variety of missions. However, for large teams of robots to function for extended periods of time, the individual members of a team must be able to generate or find energy to re-supply themselves. One approach to providing power for a robotic team is to couple larger systems with significant energy reserves so that the smaller systems can be recharged directly from the larger. This paper presents an implementation of such an approach. Here, a modular docking station is given locomotion through the cooperation of two larger robots. The docking station is capable of transporting, deploying, retrieving, and recharging many smaller robots. The kinematic model which will govern the cooperation of the maneuvering robots and will be used to develop control is presented and discussed. The design of the individual bays of the docking station and how they facilitate the deployment, recovery, and recharge of the smaller robots is also presented. The development of this system makes possible a number of applications, including autonomous long-term environmental monitoring and reconnaissance in various locations",
Simultaneous Perturbation Particle Swarm Optimization,"In this paper, we propose two hybrid algorithms of the particle swarm optimization and the simultaneous perturbation optimization method. The proposed algorithms can utilize local information of an objective function and global shape of the function at the same time. The first information is given by the simultaneous perturbation. The second one is from the particle swarm optimization. The proposed scheme has good properties of global search and efficient local search. However, the algorithms themselves are very simple and easy to implement. Moreover, this method only requires values of the function similar to the original particle swarm optimization and the simultaneous perturbation method. Three examples including an application for a neural network are shown.",
3D Scene Reconstruction from Reflection Images in a Spherical Mirror,"This paper proposes a method for reconstructing a 3D scene structure by using the images reflected in a spherical mirror. In our method, the mirror is moved freely within the field of view of a camera in order to observe a surrounding scene virtually from multiple viewpoints. The observation scheme, therefore, allows us to obtain the wide-angle multi-viewpoint images of a wide area. In addition, the following characteristics of this observation enable multi-view stereo with simple calibration of the geometric configuration between the mirror and the camera; (1) the distance and direction from the camera to the mirror can be estimated directly from the position and size of the mirror in the captured image and (2) the directions of detected points from each position of the moving mirror can be also estimated based on reflection on a spherical surface. Some experimental results show the effectiveness of our 3D reconstruction method",
Dominant Feature Extraction in Block-DCT Domain,"Automatically retrieving images through their low-level visual features has become one of the challenging areas of research recently. Among those distinguishing features, the texture features are one of the main themes in content-based image retrieval (CBIR). In this paper, we propose a novel technique to extract dominant features of images in block-DCT domain. The image is first converted to YUV color space and divided into four subblocks. The Y-component in each subblock is then transformed into DCT coefficients, some regions of which characterize different directional texture feature of that subblock. The directional textures in all subblocks are concatenated together as a single feature vector and used for indexing and retrieval of images. The experimental results show that using proper size of block-DCT to emphasize the regional properties of an image while maintaining its global view performs well in CBIR.",
Precise Asymptotic Analysis of the Tunstall Code,"We study the Tunstall code using the machinery from the analysis of algorithms literature. In particular, we propose an algebraic characterization of the Tunstall code which, together with tools like the Mellin transform and the Tauberian theorems, leads to new results on the variance and a central limit theorem for dictionary phrase lengths. This analysis also provides a new argument for obtaining asymptotic results about the mean dictionary phrase length and average redundancy rates","Dictionaries,
Algorithm design and analysis,
Mathematics,
Computational geometry,
Computer science,
Machinery,
Binary codes,
Visualization,
Entropy,
Minimax techniques"
Display Composed of Water Drops for Filling Space with Materialized Virtual Three-dimensional Objects,"Recently, the field of media art has become popular. However, most existing media art works are not suitable to exhibit in a public space. To develop technologies that enable media art works to be exhibited in a public space, we pay attention to the ""spatial coexistence between the real and the virtual in public spaces""; the audience in a public space can feel as if the virtual space and real space exist together completely. In this paper, we propose a new concept, named the ""controllable particle display"", which is to display threedimensional objects by filling space with small particles. On the basis of this concept we developed a prototype system using water drops as particles. In this system, a set of water drops, falling from a tank, are designed to form a plane surface. Patterns of images are projected upward on the falling water drops by a projector under the water drops. Three-dimensional objects can be observed by projecting a set of tomographic images in accordance with the position of the water drops. We also demonstrated the effectiveness of our concept and system.",
Optimal intratask dynamic voltage-scaling technique and its practical extensions,"This paper presents a set of comprehensive techniques for the intratask voltage-scheduling problem to reduce energy consumption in hard real-time tasks of embedded systems. Based on the execution profile of the task, a voltage-scheduling technique that optimally determines the operating voltages to individual basic blocks in the task is proposed. The obtained voltage schedule guarantees minimum average energy consumption. The proposed technique is then extended to solve practical issues regarding transition overheads, which are totally or partially ignored in the existing approaches. Finally, a technique involving a novel extension of our optimal scheduler is proposed to solve the scheduling problem in a discretely variable voltage environment. We also present a novel voltage set-up technique to determine each voltage level for customizable systems-on-chips (SoCs) with discretely variable voltages. In summary, it is confirmed from experiments that the proposed optimal scheduling technique reduces energy consumption by 20.2% over that of one of the state-of-the-art schedulers (Shin and Kim, 2001) and, further, the extended technique in a discrete-voltage environment reduces energy consumption by 45.3% on average.",
Support Vector Machine Model in Electricity Load Forecasting,"With the development of electronic industry, accurate load forecasting of the future electricity demand plays an important role in regional or national power system strategy management. Electricity load forecasting is difficult due to the nonlinearity of its influencing factors. Support vector machine (SVM) have been successfully applied to solve nonlinear regression and time series problems. However, the application to load forecasting is rare. In this study, a model of support vector machine is proposed to forecast electricity load. The model overcomes the disadvantages of general artificial neural network (ANN), such as it is not easy to converge, liable to trap in partial minimum and unable to optimize globally, and the generalization of the model is not good, etc. The SVM model ensured the forecasting is optimized globally. Subsequently, examples of electricity load data from Hebei province of China are used to illustrate the performance of the proposed model. The empirical results reveal that the proposed model outperforms the general artificial neural network model, and the forecasting accuracy improved effectively. Therefore, the model provides a promising arithmetic to forecasting electricity load in power industry",
Is IEEE 802.11 ready for VoIP?,"In this paper, we empirically explore voice communication over IEEE 802.11 networks (VoWiFi). The objective is to understand the limitations of the current WiFi network for VoWiFi deployment. Our experiment finds two major problems of VoWiFi: unstable and excessively long handoffs and unpredictable occurrence of bursts. We also discuss several other minor factors that could hinder VoWiFi deployment, such as network capacity, fairness, and interference susceptibility. Finally, we describe the scenarios where VoWiFi could be used. We conclude that VoWiFi is feasible if used moderately, with low mobility and good signal strength","Delay,
Jitter,
Throughput,
Speech,
Wireless networks,
Computer science,
Mathematics,
Statistics,
Collaboration,
Interference"
ASEHA: A Framework for Modelling and Verification ofWeb Services Protocols,"Agents require standard and reliable protocols to interact with different service providers in order to provide high quality service to customers over the Web. Many useful protocols are coming into the market, but are often ambiguously specified by protocol designers and not fully verified. These can lead to interoperability problems among implementations of the same protocol and high software maintenance costs. In this paper, we propose a hierarchical automata-based framework to model the necessary features of protocols to verify their correctness. Our experience shows that the graphical models help uncover subtle scenarios and reduce, if not eliminate, ambiguities. We illustrate our formalism with a version of WS-atomic transaction protocol","Protocols,
Australia,
Reliability engineering,
Software maintenance,
Costs,
Graphical models,
Automata,
Computer science,
Asia,
Web services"
Handwritten Signature Verification Using Image Invariants and Dynamic Features,"In this paper, a development of automatic signature classification system is proposed. We have presented offline and online signature verification system, based on the signature invariants and its dynamic features. The proposed system segments each signature based on its perceptually important points and then, for each segment, computes a number of features that are scale, rotation and displacement invariant. The normalized moments and the normalized Fourier descriptors are used for this invariancy, while the speed of pen is used as a dynamic feature of the signature. In both cases the data acquisition, pre-processing, feature extraction and comparison steps are analyzed and discussed. Both static and dynamic features were used as an input to a neural network. The neural network used for classification is a multi-layer perceptron (MLP) with one input layer, one hidden layer and one output layer. The performance of the proposed system is presented through simulation examples",
Gauss-Vanicek spectral analysis of the Sepkoski compendium: no new life cycles,"New periods can emerge from data as a byproduct of incorrect processing or even the method applied. A good way to avoid this error is to use Gauss-Vanicek spectral analysis. It easily detects periods in raw and gapped records, and in physical sciences, it can detect eigenfrequencies and relative dynamics accurately and simultaneously.","Gaussian processes,
Spectral analysis,
Sampling methods,
Data processing,
Polynomials,
Numerical analysis,
Time series analysis,
Geology,
Testing,
Laboratories"
A Comparative Study of Four Ontology Visualization Techniques in Protege: Experiment Setup and Preliminary Results,"The continuing need for more effective information retrieval has lead to the creation of the notions of the semantic Web and personalized information management, areas of study that very often employ ontologies to represent the semantic context of a domain. Consequently, the need for effective ontology visualization for design, management and browsing has arisen. There are several ontology visualizations available through the existing ontology management tools, but not as many evaluations to determine their advantages and disadvantages and their suitability for various ontologies and user groups. This work presents the preliminary results of an evaluation of four visualization methods in Protege","Ontologies,
Informatics,
Information retrieval,
Computer science,
Semantic Web,
Information management,
Data visualization,
Memory,
Application software,
Concrete"
Coherence of Radial Implicative Fuzzy Systems,"In the paper we address the problem of coherence of a special class of fuzzy systems -the class of so called radial implicative fuzzy systems. A fuzzy system Is coherent if for an arbitrary input there is guaranteed that a non-empty appropriate output exists, which corresponds to the statement that there are no contradictory rules in the system's rule base. For conjunctive fuzzy systems coherence is generally always satisfied. However, for implicative fuzzy systems this is not automatically the case. In the paper we specify sufficient conditions for coherence of radial implicative fuzzy systems. Radial fuzzy systems are systems exhibiting the radial property which simplifies their computational scheme and enables efficiently answer questions on their important properties.","Fuzzy systems,
Fuzzy sets,
Coherence,
Sufficient conditions,
Shape,
Computational modeling,
Inspection,
Computer science,
Chromium,
Frequency selective surfaces"
Output Analysis for Simulations,We discuss methods for statistically analyzing the output from stochastic discrete-event or Monte Carlo simulations. Terminating and steady-state simulations are considered,
Cryptanalysis of a suite of deniable authentication protocols,"A deniable authentication protocol allows a sender to transfer an authenticated message to a receiver in such a way that the receiver cannot prove to a third party about the source of the message. In 2001, Deng et al. proposed two deniable authentication protocols under the communication model defined by Aumann and Rabin in 1998. In this paper, we show that these two protocols have a common issue, which incurs vulnerability to the person-in-the-middle (PIM) attack. We also propose a modification for solving the problem",
Electrooculogram based system for computer control using a multiple feature classification model,"This paper discusses the creation of a system for computer-aided communication through automated analysis and processing of electrooculogram signals. In situations of disease or trauma, there may be an inability to communicate with others through standard means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, the Telepathix system was designed to accept eye movement commands denoted by looking to the left, looking to the right, and looking straight ahead to navigate a virtual keyboard. Using a ternary virtual keyboard layout and a multiple feature classification model, a typing speed of 6 letters per minute was achieved",
An academic perspective on globalization in the software industry,"In this paper, the author has presented the challenges in global software development and then identified the gaps in the academic curriculum that should be addressed in order to train software engineers to be cognizant of current issues in executing large, global software projects. We have also outlined a practically oriented academic course that will stress on the skills needed to actually execute global projects. Over multiple offerings of such a course we hope to abstract out the actual issues faced and present suitable solutions as learnings",
FPGA-based fault simulator,"Fault simulation allows evaluation of reliability properties of developed designs. The complexity of the designs is growing, which makes software-based simulation methods unusable. Hardware-based fault simulation can bring desired speedup. Partial dynamic reconfiguration is a way of fault injection. Reconfiguration time is often considered as a main weakness of this technique. This paper describes an FPGA-based fault simulator, where reconfiguration is performed by an embedded processor core, which eliminates this drawback. Error-detection-code based CED circuits are used in experiments; the results of the experiments are reported",
Adapting TCP for Vertical Handoffs in Wireless Networks,With the growing use of multiradio mobile terminals a vertical handoff between different wireless access technologies is becoming increasingly common. The vertical hand-off may result in a significant change in the access link characteristics that can affect the performance of TCP dramatically as its behaviour depends on the end-to-end path properties that also change consequently. We propose a number of simple enhancements to the TCP sender algorithm which make use of explicit information about the change in the link characteristics due to handoff. We study the effectiveness of the enhancements in a simulated WLAN-GPRS environment with different handoff scenarios. The enhancements are shown to improve TCP performance significantly,"Wireless networks,
Wireless LAN,
Bandwidth,
Delay,
Transport protocols,
Propagation losses,
Computer science,
Mobile computing,
IP networks,
Wide area networks"
Optimal local map size for EKF-based SLAM,"In this paper we show how to optimize the computational cost and maximize consistency in EKF-based SLAM for large environments. We combine local mapping with map joining in a way that the total cost of computing the final map is minimized compared to full global EKF-SLAM. This solution is not now only shown to be (1) computationally optimal, but in addition, it is empirically shown that (2) it also produces the most consistent environment map. For a given environment size and sensor range, we can determine the optimal size of the local maps required to minimize the total computational cost and maximize map consistency. The motivation of this work is described in a map building experiment in our lab, and the statistical significance of the proposed method is validated using Monte Carlo simulations",
Visualization and Analysis in Automated Trace Retrieval,"This paper describes a new visualization technique for helping analysts to understand the potential impact of changing requirements, and for providing valuable early feedback on the quality of a software design. Based on the candidate links that are automatically generated by a trace retrieval tool, VisMatrix creates a graphical representation of the requirements trace matrix showing not only where candidate links exist, but also the strength of those links. New metrics derived from the trace visualization, and its underlying trace matrix, are introduced. These include 'trace clustering' for evaluating modularity, and 'trace scope' for analyzing fan-in and fan-out behavior of traces, where fan-in represents the number of requirements influencing a design artifact, and fan-out represents the degree of influence a single requirement has on the set of design artifacts. Examples are drawn from the automatically generated trace matrices of four different data sets to illustrate the visualization techniques and analyze the proposed metrics.",
Network Protocol System Fingerprinting - A Formal Approach,,"Protocols,
Fingerprint recognition,
Operating systems,
Web server,
Information security,
Intrusion detection,
Testing,
Automata,
Internet,
Computer science"
Fine Grained Resource Reservation in Open Grid Economies,"The CORA (Coallocative, Oversubscribing Resource Allocation) architecture is a market based resource reservation system that utilises a trustworthy Vickrey auction to make combinatorial allocations of resources. This paper provides an overview of several significant components of the CORA architecture. Firstly, CORA utilises a novel combination of techniques to improve utilisation, including oversubscription, coallocation, just-in-time reallocation and a flexible contract structure. Secondly, this paper utilises a new auction architecture that does not require the auctioneers to be trusted. The advantage is that any entity (untrusted or otherwise) can conduct a privacy preserving Vickrey auction, removing the need for a trusted and privileged auction service within the system. CORA demonstrates how a practical, efficient and trustworthy auction scheme can be implemented in a Grid Economy.",
Regaining lost knowledge through dynamic analysis and aspect orientation $an industrial experience report,"This paper describes our experiences of applying dynamic analysis solutions on an industrial legacy application written in C, with the help of aspect orientation (AO). We use a number of dynamic analysis techniques that can help in alleviating the problem of (1) establishing the quality of the available regression test and (2) regaining lost knowledge of the application. We also show why our aspect language for C, aspicere, is well-suited for using dynamic analysis in legacy environments. Finally, we present the case study itself the results we have obtained and the validation thereof by the original developers and current maintainers of the application. We also mention some typical pitfalls that we encountered while dealing with legacy applications in a reengineering context",
A Case Study of Load Sharing Based on Popularity in Distributed VoD Systems,"In our research, we consider a distributed video-on-demand (VoD) system in which only the most popular videos are replicated in all the servers, whereas the rest of them are distributed through the system following some allocation scheme. In this paper, we present an algorithm to efficiently share the load in such a system and an analytical model that captures the performance of this algorithm, which we validate through simulations. One novelty in our work is that our analytical model lets us relate popularity and partial replication of some of the videos and to predict the user waiting time. We exploit such relationships to assist the system designer to select the size of the servers and network, the optimal number of servers to maintain short waiting time and to predict when the network encounters bottleneck","Videos,
Switches,
Network servers,
Analytical models,
Bandwidth,
Asynchronous transfer mode,
Costs,
Streaming media,
Computer science education,
Computer architecture"
Transferring bioelasticity knowledge through haptic interaction,"This study establishes a practical environment for transferring knowledge on bioelasticity between expert and trainee medical practitioners. Through haptic interaction with a deformable virtual anatomical model, experts set the model's elasticity conditions by simulating a surgical procedure. Trainees experience the elasticity by attempting the same surgical manipulation",
A Multipurpose Wavelet-Based Image Watermarking,"To address copyright protection and content authentication, researchers have proposed various digital watermarking methods. In this paper, an improved, multipurpose method is proposed where the authentication watermark and the robust watermark are embedded in the wavelet transform domain. Through a series of experiment, supportive evidence is provided to demonstrate the proposed method being effective in image authentication and pre-empting image processing attacks","Watermarking,
Authentication,
Discrete wavelet transforms,
Wavelet transforms,
Computer science,
Copyright protection,
Robustness,
Frequency,
Wavelet domain,
Linear approximation"
Integrated class DE synchronized DC-DC converter for on-chip power supplies,"A 12.5-mW dc-dc resonant converter with a 500 MHz class DE power inverter and class E synchronous rectifier was designed and simulated on a 1.2 μm CMOS process chip. A 20 nH spiral inductor was used in the resonant circuit. The class DE inverter requires lower inductance than the PWM buck converter. Soft switching operation enables high conversion efficiency at high operating frequencies. The class DE inverter is a good candidate for on-chip switching converters because of very high efficiency, smaller passive elements, and low peak switch voltage.","Switches,
Capacitance,
Inverters,
Inductors,
Rectifiers,
DC-DC power converters,
Capacitors"
Matrix Factorizations for Parallel Integer Transformation,"Integer mapping is critical for lossless source coding and has been used for multicomponent image compression in the new international image compression standard JPEG 2000. In this paper, starting from block factorizations for any nonsingular transform matrix, we introduce two types of parallel elementary reversible matrix (PERM) factorizations which are helpful for the parallelization of perfectly reversible integer transforms. With improved degree of parallelism and parallel performance, the cost of multiplications and additions can be, respectively, reduced to O(logN) and O(log2N) for an N by N transform matrix. These make PERM factorizations an effective means of developing parallel integer transforms for large matrices. We also present a scheme to block the matrix and allocate the load of processors for efficient transformation","Image coding,
Transform coding,
Wavelet transforms,
Information science,
Computer science,
Dynamic range,
Source coding,
Parallel processing,
Costs,
Parallel algorithms"
{\rm E} - A Generic Event Model for Event-Centric Multimedia Data Management in eChronicle Applications,"eChronicle applications are inherently event-centric, enabling users to find and explore important events in an application domain and providing unified access to any media that document them. Today’s multimedia data management components such as multimedia databases, however, are largely media-centric, considering events - if at all - just as one of many pieces of media metadata. Obfuscating event exploration and event-driven access to media, they are only of limited use for the implementation of eChronicle applications. Using a concrete eChronicle application in the defense domain, this paper motivates the need for event-centric multimedia data management components. As a foundation, the paper proposes the multimedia event model and discusses essential design considerations for the development of that model. {\rm E}’s genericity and profound adaptability to varying application needs make the model a suitable foundation for reusable multimedia data management components that are useful not only for eChronicle applications, but for any multimedia application where event-driven access to content is of interest.",
Low Complexity High Quality Fractional Motion Estimation Algorithm and Architecture Design for H.264/AVC,"In this paper, we propose a low complexity high quality fractional motion estimation design for H.264/AVC. A mode reduction algorithm of sub-macroblock partitions reduces about 30% of the hardware cost for FME block matching. The algorithm provides the continuous search points in a modified search area to boost hardware utilization and own high feasibility for the VLSI array processing. Simulation results show that the proposed FME has 0.01196dB worse than and 0.0115dB better than JM9.3 at CIF and D1 formats, respectively. Moreover, an associated FME architecture with a configurable flexibility is also proposed in the paper. It adopts flexible mode selection between several sets of macroblock partitions for providing trade-off in computation complexity and video quality. According to the TSMC 0.13mum CMOS technology, the proposed design costs 112.7K gates with the maximum working frequency of 158 MHz. This design can realize the real-time H.264/AVC encoding on a D1 video and HD720 video at operation frequency of 40 MHz and 108 MHz, respectively",
A Host Port Interface Board to Enhance the TMS320C6713 DSK,"The fairly recent introduction of the TMS320C6713 DSP starter kit (DSK) from Texas Instruments (TI) brought a much more capable, stable, and robust DSP development environment to both university and industry engineers. However, while this new DSK had many improvements over the TMS320C6711 DSK it replaced, it did not include any way to transfer data to and from the host computer except through the debugger interface. Unfortunately, this interface is extremely limited in bandwidth and requires that the TI software tools be available. This means that the existing suite of winDSKG demonstration tools cannot be run on the 6713 DSK, denying educators a valuable teaching and classroom demonstration resource. Also, there is no way to interface an application on the host PC directly to the DSK, limiting the ability of students to create stand-alone, interactive projects using the DSK. To solve these problems, the authors have created an interface to the TMS320C6713 DSK that uses the host port interface (HPI) to provide both a means for a PC host application to boot software onto the DSK, and to permit the transfer of data between the DSK and the host PC application. A software package makes it possible for students to create stand-alone Windows applications that communicate directly with the DSK. In addition to parallel port communication, the interface provides USB, RS-232, and digital input/output ports as user selectable resources. This paper discusses the specific capabilities of the hardware and software interface, summarizes the software applications and library calls available, and relates a few of our teaching experiences using this new capability. The authors freely distribute the software components of the interface for educational use",
Visual Mapping of Text Collections through a Fast High Precision Projection Technique,"This paper introduces least square projection (LSP), a fast technique for projection of multi-dimensional data onto lower dimensions developed and tested successfully in the context of creation of text maps based on their content. Current solutions are either based on computationally expensive dimension reduction with no proper guarantee of the outcome or on faster techniques that need some sort of post-processing for recovering information lost during the process. LSP is based on least square approximation, a technique originally employed for surface modeling and reconstruction. Least square approximations are capable of computing the coordinates of a set of projected points based on a reduced number of control points with defined geometry. We extend the concept for general data sets. In order to perform the projection, a small number of distance calculations is necessary and no repositioning of the final points is required to obtain a satisfactory precision of the final solution. Textual information is a typically difficult data type to handle, due to its intrinsic dimensionality. We employ document corpora as a benchmark to demonstrate the capabilities of the LSP to group and separate documents by their content with high precision",
Design-Intent Coverage&#8212;A New Paradigm for Formal Property Verification,"It is essential to formally ascertain whether the register-transfer level (RTL) validation effort effectively guarantees the correctness with respect to the design's architectural intent. The design's architectural intent can be expressed in formal properties. However, due to the capacity limitations of formal verification, these architectural properties cannot be directly verified on the RTL. As a result, a set of lower level RTL properties are developed and verified against the RTL modules. In a top-down design approach, the architect would ideally like to formally guarantee the coverage of the architectural intent at the time of creating the specifications for the component RTL modules (that is, before they are passed to the designers for implementation). In this paper, the authors present: 1) a method for checking whether the RTL properties are covering the architectural properties, that is, whether verifying the RTL properties guarantees the correctness of the design's architectural intent; 2) a method to identify which architectural properties are still uncovered, that is, not guaranteed by the RTL properties; and 3) a methodology for representing the gap between the specifications in a legible form",
A real time low complexity codec for use in low Earth orbit small satellite missions,"A single-board computer system created specifically to meet the demands of a new generation of small satellite missions has been designed, built and tested at Surrey Satellite Technology Limited (SSTL). The satellite onboard computer is an MPC8260 based system that was originally developed for use onboard the first Algerian satellite (Alsat-1). For the secure transaction of data between the central processing unit (CPU) of the onboard computer and its local random access memory (RAM), the program memory has been designed with triple modular redundancy (TMR), which is a hardware implementation that includes replicated memory circuits and voting logic to detect and correct a faulty value. TMR error correction technique allows single correction of one error bit per stored word. For computers on board a satellite, there is however a definite risk of two error bits occurring within one byte of stored data. In this paper, a real time low complexity codec for use in low earth orbit small satellite missions is presented and implemented in field programmable gate array (FPGA) technology. The proposed device is transparent to the routine transfer of data between the CPU and its local RAM. A simple method of decoding is adopted that allows considerable simplification of the codec with a low complexity.","Low earth orbit satellites,
Codecs,
Central Processing Unit,
Random access memory,
Read-write memory,
Error correction,
Computer errors,
Field programmable gate arrays,
System testing,
Redundancy"
A Novel Distributed and Practical Incentive Mechanism for Peer to Peer Live Video Streaming,"The successful deployment of peer-to-peer (P2P) live video streaming systems has practically demonstrated that it can scale to reliably support a large population of peers. However, peers, representing rational end users, tend to be non-cooperative when it comes to the duty rather than the self-interests, running counter to the fundamental design philosophy of P2P concept. The objective of this paper is to investigate the problem of encouraging users to balance what they take from the system with what they contribute. We first make a statistical analysis to the service logs of a practical P2P live video streaming system and reveal intrinsic characteristic of users' online duration. Second, we thus propose a novel incentive mechanism based on the composite contributions which consist of two objective metrics, i.e. the on-line duration and effective upstream traffic. This mechanism offers service differentiation to users with different contributions and has some desirable properties: (1) distributed nature upon gossip-based overlay structure and (2) practical oriented evaluation criteria. The experiment results over PlanetLab further verify its effectiveness",
Memoryful Branching-Time Logic,"Traditional branching-time logics such as CTL* are memoryless: once a path in the computation tree is quantified at a given node, the computation that led to that node is forgotten. Recent work in planning suggests that CTL* cannot easily express temporal goals that refer to whole computations. Such goals require memoryful quantification of paths. With such a memoryful quantification, Epsi holds at a node s of a computation tree if there is a path pi starting at the root of the tree and going through s such that pi satisfies the linear-time formula psi. We define the memoryful branching-time logic mCTL* and study its expressive power and algorithmic properties. We show that mCTL* is as expressive, but exponentially more succinct, than CTL*, and that the ability of mCTL* to refer to the present is essential for this equivalence. From the algorithmic point of view, while the satisfiability problem for mCTL* is 2EXPTIME-complete - not harder than that of CTL*, its model-checking problem is EXPSPACE-complete - exponentially harder than that of CTL*. The upper bounds are obtained by extending the automata-theoretic approach to handle memoryful quantification, and are much more efficient than these obtained by translating mCTL* to branching logics with past. The EXPSPACE lower bound for the model-checking problem applies already to formulas of restricted form (in particular, to AGEpsi, which is useful for specifying possibility properties), and implies that reasoning about a memoryful branching-time logic is harder than reasoning about the linear-time logic of its path formulas",
Texture Display Mouse KAT: Vibrotactile Pattern and Roughness Display,"This paper presents a novel haptic mouse KAT (KAIST artificial touch) that can be used as a human-computer interface and offers the capability of displaying properties such as patterns, gratings and roughness. A small planar-distributed pin array is developed. The array can represent micro-scale shapes with various surfaces, such as gratings, grooves, patterns, shapes of icons, and Braille, and provides the user with cutaneous stimuli. Since the tactile display unit is small enough to be embedded into a computer mouse, we developed a new texture display mouse. The performance of its texture display capability was verified. In addition, two psychophysical experiments have been conducted in order to ascertain the influence of vibrotactile stimuli. The first experiment showed that vibrational stimuli could be effective in the perception of patterns while from the second experiment, the functional relation between perceived roughness and components of vibrotactile stimuli was obtained. The experimental results have been applied to the development of a test-bed program",
Training Bao Game-Playing Agents using Coevolutionary Particle Swarm Optimization,"Bao, an African board game of the Mancala family, is a complex two-player game with a very large search space and complex rule set. The success of game tree approaches to create game-playing agents rests heavily on the usually handcrafted, static evaluation function. One of the first steps towards using a game tree is to design an appropriate, efficient evaluation function. This paper investigates the effectiveness of a revolutionary particle swarm optimization (PSO) approach to evolve the evaluation function for the game of Bao. This approach uses a PSO algorithm to evolve a neural network as evaluation function, using an unsupervised, competitive learning approach. The revolutionary approach to evolving game-playing agents assumes no prior knowledge of game strategies. The only domain specific information used by the model are the rules of the game, and the outcomes of games played. The performance of the evolved game-playing agents is compared to a game tree-based agent using a handcrafted evaluation function, as well as a player that makes random moves. Results show that the coevolutionary PSO approach succeeded in learning playing strategies for Bao",
Clustering-Based Source Selection for Efficient Image Retrieval in Peer-to-Peer Networks,"In peer-to-peer (P2P) networks, computers with equal rights form a logical (overlay) network in order to provide a common service that lies beyond the capacity of every single participant. Efficient similarity search is generally recognized as a frontier in research about P2P systems. One way to address it is using data source selection based approaches where peers summarize the data they contribute to the network, generating typically one summary per peer. When processing queries, these summaries are used to choose the peers (data sources) that are most likely to contribute to the query result. Only those data sources are contacted. There are two main contributions of this paper. We extend earlier work, adding a data source selection method for high-dimensional vector data, comparing different peer ranking schemes. More importantly, we present a method that uses progressive stepwise data exchange between peers to better each peer's summary and therefore improve the system's performance",
Anomaly Detection Based Intrusion Detection,"This paper is devoted to the problem of neural networks as means of intrusion detection. We show that properly trained neural networks are capable of fast recognition and classification of different attacks. The advantage of the taken approach allows us to demonstrate the superiority of the neural networks over the systems that were created by the winner of the KDD Cups competition and later researchers due to their capability to recognize an attack, to differentiate one attack from another, i.e. classify attacks, and, the most important, to detect new attacks that were not included into the training set. The results obtained through simulations indicate that it is possible to recognize attacks that the intrusion detection system never faced before on an acceptably high level",
Ant-based Reputation Evidence Distribution in P2P Networks,"How to manage trust is regarded as a main problem of security issues in P2P overlay networks. Reputation-based trust management is regarded as a promising way to solve the problem. However, in P2P systems, obtaining reputation evidence for building trust relationships is not an easy job. We tackle this problem by employing a scheme for the distribution of reputation evidence, which is based on the swarm intelligence paradigm. Our simulations show that it performs very well in P2P environment. We hope that this method will help move the reputation based trust management closer to fulfilling its promise in P2P networks","Peer to peer computing,
Distributed computing,
Computer network management,
Computational modeling,
Particle swarm optimization,
Cryptography,
Computer science,
Computer networks,
High performance computing,
Electronic mail"
Packet Dispersion in IEEE 802.11 Wireless Networks,"Packet dispersion techniques have been commonly used to estimate bandwidth in wired networks. However, current packet dispersion techniques were developed for wired network environments and can provide inaccurate results in wireless networks due to wireless capacity variability over short time scales. This paper develops an analytical model to investigate packet dispersion behavior in wireless networks. The packet dispersion model is validated using both an extended ns-2 simulator that includes 802.11 MAC layer rate adaptation and wireless 802.11b testbed measurements. Utilizing the model, this study shows that packet dispersion measures effective capacity and achievable throughput of wireless networks instead of the maximum capacity as in wired networks. Additionally, mean and variance of packet dispersion in IEEE 802.11 wireless networks is analyzed while considering the impact of channel conditions such as packet size, link rate, bit error rate and RTS/CTS",
An Evolutionary Artificial Potential Field Algorithm for Dynamic Path Planning of Mobile Robot,"The artificial potential field (APF) method is widely used for autonomous mobile robot path-planning due to its simplicity and mathematical elegance. However, most researches are focused on solving the path-planning problem in a stationary environment, where both targets and obstacles are stationary. This paper proposes a new APF method for path-planning of mobile robots in a dynamic environment where the target and obstacles are moving. First, the new force function and the relative threat coefficient function are defined. Then, a new APF path-planning algorithm based on the relative threat coefficient is presented. Finally, computer simulation and experiment are used to demonstrate the effectiveness of the dynamic path-planning scheme",
High Capacity Lossless Secure Image Steganography using Wavelets,"The modern steganography presents a challenging task of embedding data that should be imperceptible to the human visual system (HVS) and also escape the detection of powerful machine vision of computers. In this paper we present a high capacity, lossless, secure wavelet steganographic algorithm in which payload bitstream is encrypted and embedded into the wavelet coefficients of the cover image to derive a stego-image. The payload is embedded in the approximation band of the wavelet domain that increases its robustness. It is observed through simulations that mean square error (MSE), mean absolute error (MAE), bit error rate (BER) and histogram analysis that the pay load is retrieved without any errors and its performance is better than the earlier insignificant coefficient replacement (ICR) technique.",
Neuroelectrical source imaging of mu rhythm control for BCI applications,"In the last decade, the possibility to noninvasively estimate cortical activity has been highlighted by the application of the techniques known as high resolution EEG. These techniques include a subject's multi-compartment head model (scalp, skull, dura mater, cortex) constructed from individual magnetic resonance images, multi-dipole source model, and regularized linear inverse source estimates of cortical current density. The aim of this paper is to demonstrate that the use of cortical activity estimated from noninvasive EEG recordings of motor imagery is useful in the context of a brain computer interface as compared with others scalp spatial filters usually used on-line",
Document Clustering with Semantic Analysis,"Document clustering generates clusters from the whole document collection automatically and is used in many fields, including data mining and information retrieval. In the traditional vector space model, the unique words occurring in the document set are used as the features. But because of the synonym problem and the polysemous problem, such a bag of original words cannot represent the content of a document precisely. In this paper, we investigate using the sense disambiguation method to identify the sense of words to construct the feature vector for document representation. Our experimental results demonstrate that in most conditions, using sense can improve the performance of our document clustering system. But the comprehensive statistical analysis performed indicates that the differences between using original single words and using senses of words are not statistically significant. In this paper, we also provide an evaluation of several basic clustering algorithms for algorithm selection.",
An Image Encryption Scheme Based on Chaotic Systems,"An image encryption scheme based on chaotic systems is proposed in this paper. By combining the spatial-domain encryption of digital images and the traditional stream ciphers technology, the security of the encryption scheme is enhanced effectively. Instead of encrypting an image in a chaotic signal directly, the proposed scheme uses two chaotic systems based on the thought of higher secrecy of multi-system. One of the chaotic systems is used to generate a chaotic sequence. Then this chaotic sequence is transformed into a binary stream by a threshold function. The other chaotic system is used to construct a permutation matrix. Firstly, the pixel values of a plain image are modified randomly using the binary stream as a key stream. Secondly, the modified image is encrypted again by permutation matrix. The security of encryption scheme is analyzed in detail. And the simulation results show that the proposed scheme has a better efficiency than others' and a high security performance","Cryptography,
Chaos,
Streaming media,
Digital images,
Pixel,
Information security,
Internet,
Computer science,
Electronic mail,
Data security"
Capturing and Reusing Functional and Non-functional Requirements Knowledge: A Goal-Object Pattern Approach,"Pattern-based approach has proven to be an effective means for capturing and reusing past experience and best practices to facilitate communication and shorten the time required for software modeling. However, current knowledge capture is limited to mostly functional knowledge. Knowledge reuse is also often manual and limited to low-scale small model fragments. This paper presents a goal-object pattern framework that captures both functional and non-functional requirements knowledge that is modeled using UML and a goal-oriented method. Patterns in this framework are a collection of model refinement methods, each defining a single model element generation step in the target model. The patterns are model-driven in that they are defined using a UML metamodel and then used as such. The framework also supports incremental knowledge capture in that large-grain patterns can be composed from fine-grain ones, and also application-independent patterns can be specialized into application-specific ones. The framework is illustrated using a simplified on-line bookstore application to demonstrate how large-scale reuse may be possible","Object oriented modeling,
Unified modeling language,
Best practices,
Large-scale systems,
Computer science,
Packaging,
Collaboration,
Pattern analysis,
Application software,
Communication system security"
Semi-Automatic Lymph Node Segmentation in LN-MRI,"Accurate staging of nodal cancer still relies on surgical exploration because many primary malignancies spread via lymphatic dissemination. The purpose of this study was to utilize nanoparticle-enhanced lymphotropic magnetic resonance imaging (LN-MRI) to explore semi-automated noninvasive nodal cancer staging. We present a joint image segmentation and registration approach, which makes use of the problem specific information to increase the robustness of the algorithm to noise and weak contrast often observed in medical imaging applications. The effectiveness of the approach is demonstrated with a given lymph node segmentation problem in post-contrast pelvic MRI sequences.",
How robust is the SVM wound segmentation?,"This paper investigates the robustness of automatic wound segmentation. The work builds upon an automatic segmentation procedure by the support vector machine (SVM)-classifier presented in [M. Kolesnik et al. (2004), (2005)]. Here we extend the procedure by incorporating textural features and the deformable snake adjustment to refine SVM-generated wound boundary. The robustness of SVM-based segmentation is tested against different feature spaces using a long sample of training images featuring a broad variety of wounds' appearance. Recommendations drawn from these experiments provide a useful guideline for the development of a software support system for the visual monitoring of chronic wounds in wound care units","Robustness,
Support vector machines,
Wounds,
Image segmentation,
Image edge detection,
Support vector machine classification,
Image sampling,
Histograms,
Color,
Skin"
A Novel Spatial Multiplexing Architecture with Finite Rate Feedback,"The V-BLAST (vertical Bell Labs layered space-time) architecture with equal rate and power per antenna and a fixed order of detection is known to suffer from poor performance; in a multi-input multi-output (MIMO) Rayleigh fading channel with Mt transmit antennas and Mr receive antennas (MrgesMt), the diversity order is only Mr-Mt+1. There are two remedies available, namely, (a) channel ordered detection at the receiver and (b) allocation of rates and powers at the transmitter. However, given independent coding/decoding at each layer, the diversity gain of V-BLAST even by applying these remedies is fundamentally upper bounded by Mr. In this paper, we show that the story would be dramatically different if the receiver is allowed to feed a few bits of channel state information (CSI) back to the transmitter. We develop an approach of jointly designed ordered detection at the receiver and rate/power allocation at the transmitter. Herein the receiver feeds back a finite number of bits (=log2(Mt!)) to the transmitter regarding the detection order. In particular, we propose what we call the Greedy ordering rate tailored V-BLAST (GRT-VB) scheme which, with independent coding/decoding on each layer, is shown to have the diversity-multiplexing (D-M) gain tradeoff which is close to the optimal one (e.g. it has the maximal diversity gain of MrMt). The simulation result validates the superior performance of the proposed scheme.",
BP neural network optimized with PSO algorithm and its application in forecasting,"An approach that neural network optimized with PSO algorithm is proposed in the paper. Unlike conventional training method with gradient descent method only, this paper introduces a hybrid training algorithm by combining the PSO and BP algorithm. The PSO is used to optimize the initial parameters of the BP neural network, including the weights and biases. It can effectively better the cases that network is easily trapped to a local optimum and has a slow velocity of convergence. The experiment results show the method in the paper above conventional one has greater improvement in both accuracy and velocity of convergence for BP neural network.",
Numerical Computations in US Undergraduate Physics Courses,A recent study conducted via email and Web surveys gathered responses from physics faculty members across the US about their use of numerical computations in the classroom. Responses showed a strong commitment to computational activities from some physics faculty and a frustration over the lack of such activities from others,"Physics computing,
Computer aided instruction,
Winches,
Cities and towns,
Calculus,
Educational institutions,
Electronic mail,
Databases,
Portals"
Post-Routing Redundant Via Insertion and Line End Extension with Via Density Consideration,"Redundant via insertion and line end extension employed in the post-routing stage are two well known and highly recommended techniques to reduce yield loss due to via failure. However, if the amount of inserted redundant vias is not well controlled, it could violate via density rules and adversely worsen the yield and reliability of the design. In this paper, we first study the problem of redundant via insertion, and present two methods to accelerate a state-of-the-art approach (which is based on a maximum independent set (MIS) formulation) to solve it. We then consider the problem of simultaneous redundant via insertion and line end extension. We formulate the problem as a maximum weighted independent set (MWIS) problem and modify the accelerated MIS-based approach to solve it. Lastly, we investigate the problem of simultaneous redundant via insertion and line end extension subject to the maximum via density rule, and present a two-stage approach for it. In the first stage, we ignore the maximum via density rule, and enhance the MWIS-based approach to find the set of regions which violate the maximum via density rule after performing simultaneous redundant via insertion and line end extension. In the second stage, excess redundant vias are removed from those violating regions such that after the removal, the maximum via density rule is met while the total amount of redundant vias removed is minimized. This density-aware redundant via removal problem is formulated as a set of zero-one integer linear programming (0-1 ILP) problems each of which can be solved independently without sacrificing the optimality. The superiorities of our approaches are all demonstrated through promising experimental results","Routing,
Timing,
Integrated circuit layout,
Computer science,
Acceleration,
Design for manufacture,
Permission,
Chaos,
Manufacturing processes,
Contact resistance"
A New k-means Based Clustering Algorithm in Aspect Mining,"Clustering is a division of data into groups of similar objects. Aspect mining is a process that tries to identify cross-cutting concerns in existing software systems. The goal is to refactor the existing systems to use aspect oriented programming, in order to make them easier to maintain and to evolve. This paper aims at presenting a new k-means based clustering algorithm used in aspect mining. Clustering is used in order to identify crosscutting concerns. We propose some quality measures in order to evaluate the results both from the clustering point of view and the aspect mining point of view, and we also report two case studies",
Towards Autonomic Distribution of Existing Object Oriented Programs,"By harnessing computational power of distributed heterogeneous resources, it is possible to build a large scale integrated system so that a centralized program is partitioned and distributed to those resources in a way that results in both efficient execution of the program and maximized resource utilization. However, building such a system is a staggering challenge because of the associated complexities and required user intervention. This paper proposes an autonomic distributed architecture that statically analyzes the existing Java application, partitions it to self-managed components that handles the complexities related to distribution and coordination without user involvement. An efficient static analysis mechanism is implemented that identifies run time program instances and their dependencies in terms of a graph. It is observed that such a view of the program is essential towards self optimization and self management","Resource management,
Distributed computing,
Programming profession,
Application software,
Large scale integration,
Computer science,
Electronic mail,
Buildings,
Java,
Large-scale systems"
The Study of Parallel K-Means Algorithm,"Clustering analysis plays an important role in scientific research and commercial application. K-means algorithm is a widely used partition method in clustering. As the dataset's scale increases rapidly, it is difficult to use k-means to deal with massive amount of data. A parallel strategy is incorporated into clustering method and a parallel k-means algorithm is proposed. For enhancing the efficiency of parallel k-means, dynamic load balance is introduced. Data parallel strategy and master/slave model are adopted. The experiments demonstrate that the parallel K-means has higher efficiency and universal use",
Multi-Agent Coordination and Cooperation through Classical Planning,"Multi-agent planning is a fundamental problem in multi-agent systems that has acquired a variety of meanings in the relative literature. In this paper we focus on a setting where multiple agents with complementary capabilities cooperate in order to generate non-conflicting plans that achieve their respective goals. We study two situations. In the first, the agents are able to achieve their subgoals by themselves, but they need to find a coordinated course of action that avoids harmful interactions. In the second situation, some agents may ask the assistance of others in order to achieve their goals. We formalize the two problems and present algorithms for their solution. These algorithms are based on an underlying classical planner which is used by the agents to generate their individual plans, but also to find plans that are consistent with those of the other agents. The procedures generate optimal plans under the plan length criterion. The central role that has been given to the classical planning algorithm, can be seen as an attempt to establish a stronger link between classical and multi-agent planning.","Strips,
Computer science,
Upper bound,
Space technology,
Technology planning,
Encoding,
Mathematics,
Multiagent systems,
Intelligent agent,
Constraint theory"
A data access method considering power consumption in ad hoc networks,"In ad hoc networks, it is effective that each mobile host creates replicas of data items held by other mobile hosts for improving data accessibility. When a mobile host requests a data item (replica) held by another mobile host, it usually uses the shortest path for the data transmission. In this case, mobile hosts at the center of the network relay many data items and consume much power. In this paper, we propose a data access method to balance the power consumption among mobile hosts. This method selects a path between the request issuing host and the requested data holder considering the path length and the remaining amount of batteries of mobile hosts along the path. We present simulation results to evaluate the performance of our proposed method.","Energy consumption,
Intelligent networks,
Ad hoc networks,
Mobile computing,
Batteries,
Relays,
Computer networks,
Data communication,
Portable computers,
Information science"
A collaborative Web browsing system for multiple mobile users,"In mobile computing environments, handheld devices with low functionality restrict the services provided for mobile users. We propose a new concept of collaborative browsing, where mobile users collaboratively browse Web pages designed for desktop PC. In collaborative browsing, a Web page is divided into multiple components, and each is distributed to a different device. In mobile computing environments, the number of handheld devices, their capabilities, and other conditions can vary widely amongst mobile users who want to browse content. Therefore, we developed a page partitioning method for collaborative browsing, which divides a Web page into multiple components. Moreover, we designed and implemented a collaborative Web browsing system in which users can search and browse their target information by discussing and watching partial pages displayed on multiple devices",
Evaluation of Driver Stress Using Biomarker in Motor-vehicle Driving Simulator,"Employing the analysis of a biomarker, an oculomotor angle and a subjective evaluation, we have examined the acute, psychological effect human stress of driving using a motor-vehicle driving simulator. Salivary amylase is used as a biomarker, as it is considered to be one of the indicators of sympathetic nervous activity. 20 healthy female subjects in their early twenties were enrolled in this study. The time-course change of their salivary amylase activity (sAMY) is analyzed before and during the driving. At the same time, using a questionnaire, subjective evaluations are conducted with each subject. As for comparison, the effect of operating a car navigation device, which is not directly associated with driving, is also evaluated. Our results indicate that the psychological effect of driving-induced stress, a condition that can not be easily detected or recognized by a subjective evaluation, is quickly quantified using a biomarker in saliva. Moreover, the results suggest that operation of a non-driving-related device may also reduce the capacity to concentrate on driving. These data imply that evaluation of driver stress using a biomarker can be very useful for improvement of safety during driving",
Channel Modeling for V2V Communications,"In this paper we describe results of a channel measurement campaign for modeling the V2V channel. After review of applications, potential frequency bands, and related work, we describe the measurements and results for delay spreads and multipath component fading amplitudes and correlations, made in multiple V2V environments. We also note how the V2V channel can differ appreciably from other common terrestrial (e.g., cellular) channels. We describe considerations used in developing the statistical channel models for these environments, and provide some example measurement and modeling results that should be useful for system designers in future V2V applications","Frequency,
Road transportation,
Vehicles,
Antenna measurements,
Fading,
Telecommunication traffic,
Scattering,
Traffic control,
Broadband antennas,
Parameter estimation"
A 40-Gb/s Decision Circuit in 90-nm CMOS,"A low-power 40-Gb/s decision circuit for fiber-optic and mm-wave analog-to-digital converter applications was implemented in two 90-nm processes from two different foundries. The circuit uses a MOS-CML master-slave latch topology with only two vertically stacked transistors. It combines low and high-VT MOSFETs to allow for operation from a 1.2-V supply, without compromising speed. Full-rate retiming with jitter reduction and 7 ps rise/fall times is demonstrated at 37 Gb/s and 40 Gb/s from 1.2 V and 1.5 V, respectively. The entire decision circuit dissipates 130 mW from 1.2 V, with a record low power consumption of 10.8 mW per latch","Circuits,
Latches,
MOSFETs,
Current density,
Switches,
Flip-flops,
Intrusion detection,
Clocks,
Topology,
Transformers"
The Application of a Convolution Neural Network on Face and License Plate Detection,"In this paper, two detectors, one for face and the other for license plates, are proposed, both based on a modified convolutional neural network (CNN) verifier. In our proposed verifier, a single feature map and a fully connected MLP were trained by examples to classify the possible candidates. Pyramid-based localization techniques were applied to fuse the candidates and to identify the regions of faces or license plates. In addition, geometrical rules filtered out false alarms in license plate detection. Some experimental results are given to show the effectiveness of the approach",
An Integrated Cattle Health Monitoring System,"Clinical techniques for monitoring live stock health are insufficient, as they provide only sporadic information and require too much resource investment in terms of time and veterinary expertise. A sophisticated system capable of continuously assessing the health of individual animals, aggregating these data, and reporting the results to owners and regional authorities could provide tremendous benefit to the livestock industry. Such a system would not only improve individual animal health, but it would help to identify and pre vent widespread disease, whether it originated from natural causes or from biological attacks. This paper presents results from a prototype telemonitoring system that utilizes wearable technology to provide continuous animal health data. The infrastructure, hardware, software, and representative physiological measurements are presented","Cows,
Animals,
Biomedical monitoring,
Investments,
Agriculture,
Vents,
Diseases,
Software prototyping,
Prototypes,
Hardware"
Globally Optimal Grouping for Symmetric Boundaries,"Many natural and man-made structures have a boundary that shows certain level of bilateral symmetry, a property that has been used to solve many computer-vision tasks. In this paper, we present a new grouping method for detecting closed boundaries with symmetry. We first construct a new type of grouping token in the form of a symmetric trapezoid, with which we can flexibly incorporate various boundary and region information into a unified grouping cost function. Particularly, this grouping cost function integrates Gestalt laws of proximity, closure, and continuity, besides the desirable boundary symmetry. We then develop a graph algorithm to find the boundary that minimizes this grouping cost function in a globally optimal fashion. Finally, we test this method by some experiments on a set of natural and medical images.",
Evaluation of UDDI as a provider of resource discovery services for OGSA-based grids,"Grid computing involves networks of heterogeneous resources working in collaboration to solve problems that cannot be addressed by the resources of any one organization. A pervasive problem for grid users is how best to discover the resources they need given dynamic grid environments. UDDI, the universal description, discovery and integration framework, is an OASIS standard for publishing and querying discovery information for Web services, which to date, has received surprisingly little analysis as a discovery mechanism for Web service-based grids, e.g. those based on the open grid services architecture (OGSA). This work identifies issues that must be addressed in order to make UDDI meet the requirements of OGSA discovery. We examine the performance implications of these issues using a freely available implementation of UDDI version 2. Based on our experimental results, we conclude that UDDI can be used for OGSA discovery, but the cost may be prohibitive for large grids.","Grid computing,
Web services,
Service oriented architecture,
Publishing,
Costs,
Open source software,
Computer science,
Collaborative work,
Standards publication,
Information analysis"
Interactive Visualization of Intercluster Galaxy Structures in the Horologium-Reticulum Supercluster,"We present GyVe, an interactive visualization tool for understanding structure in sparse three-dimensional (3D) point data. The scientific goal driving the tool's development is to determine the presence of filaments and voids as defined by inferred 3D galaxy positions within the horologium-reticulum supercluster (HRS). GyVe provides visualization techniques tailored to examine structures defined by the intercluster galaxies. Specific techniques include: interactive user control to move between a global overview and local viewpoints, labelled axes and curved drop lines to indicate positions in the astronomical RA-DEC-cz coordinate system, torsional rocking and stereo to enhance 3D perception, and geometrically distinct glyphs to show potential correlation between intercluster galaxies and known clusters. We discuss the rationale for each design decision and review the success of the techniques in accomplishing the scientific goals. In practice, GyVe has been useful for gaining intuition about structures that were difficult to perceive with 2D projection techniques alone. For example, during their initial session with GyVe, our collaborators quickly confirmed scientific conclusions regarding the large-scale structure of the HRS previously obtained over months of study with 2D projections and statistical techniques. Further use of GyVe revealed the spherical shape of voids and showed that a presumed filament was actually two disconnected structures","Data visualization,
Astronomy,
Collaboration,
Shape,
Collaborative tools,
Computer science,
Control systems,
Large-scale systems,
Spectroscopy,
Space technology"
Multicast of Real-Time Multi-View Video,"As a recently emerging service, multi-view video provides a new viewing experience with high degree of freedom. However, due to the huge data amounts transferred, multi-view video's delivery remains a daunting challenge. In this paper, we propose a multi-view video-streaming system based on IP multicast. It can support a large number of users while still keeping a high degree of interactivity and low bandwidth consumption. Based on a careful user study, we have developed two schemes: one is for automatic delivery and the other for on-demand delivery. In automatic delivery, a server periodically multicasts special effect snapshots at a certain time interval. In on-demand delivery, the server delivers the snapshots based on distribution of users' requests. We conducted extensive experiments and user-experience studies to evaluate the proposed system's performance, and found that the system could provide satisfying multi-view video service for users on a large scale","Video compression,
Network servers,
Switches,
Video on demand,
Cameras,
Bandwidth,
Unicast,
Real time systems,
Personal communication networks,
Streaming media"
"OpenModelica - A free open-source environment for system modeling, simulation, and teaching","Modelica is a modern, strongly typed, declarative, and object-oriented language for modeling and simulation of complex systems. This paper gives a quick overview of some aspects of the OpenModelica environment - an open-source environment for modeling, simulation, and development of Modelica applications. An introduction of the objectives of the environment is given, an overview of the architecture is outlined and a number of examples are illustrated.",
RDF/RDFS-based Relational Database Integration,"We study the problem of answering queries through a RDF/RDFS ontology, given a set of view-based mappings between one or more relational schemas and this target ontology. Particularly, we consider a set of RDFS semantic constraints such as rdfs:subClassof, rdfs:subPropertyof, rdfs:domain, and rdfs:range, which are present in RDF model but neither XML nor relational models. We formally define the query semantics in such an integration scenario, and design a novel query rewriting algorithm to implement the semantics. On our approach, we highlight the important role played by RDF Blank Node in representing incomplete semantics of relational data. A set of semantic tools supporting relational data integration by RDF are also introduced. The approach have been used to integrate 70 relational databases at China Academy of Traditional Chinese Medicine.","Resource description framework,
Relational databases,
Ontologies,
XML,
Semantic Web,
Educational institutions,
Computer science,
Algorithm design and analysis,
Natural languages,
Data engineering"
Bringing Together Human and Robotic Environment Representations - A Pilot Study,"Human interaction with a service robot requires a shared representation of the environment for spoken dialogue and task specification where names used for particular locations are depending on personal preferences. A question is how such human oriented models can be tied to the geometric robotic models needed for precise localisation and navigation. We assume that this integration can be based on the information potential users give to a service robot about its working environment. We further believe that this information is best given in an interactive setting (a ""guided tour"") in this particular environment. This paper presents a pilot study that investigates how humans present a familiar environment to a mobile robot. The study is set up within our concept of human augmented mapping, for which we assume an initial ""guided tour"" scenario to teach a robot its environment. Results from this pilot study are used to validate a proposed generic environment model for a service robot","Service robots,
Mobile robots,
Robot sensing systems,
Navigation,
Educational robots,
Intelligent robots,
Human robot interaction,
Solid modeling,
Simultaneous localization and mapping,
Computer science"
A Mixed Process Neural Network and its Application to Churn Prediction in Mobile Communications,"Churn prediction is an increasingly pressing issue in today's ever-competitive commercial environments, especially in mobile communication arena. In this paper, a mixed process neural network (MPNN) based on Fourier orthogonal base function has been proposed to support churn management, which can deal with both static value and time-varied continuous value simultaneously. To further improve its performance, an optimized network, c-MPNN, has been presented, which adopts Fourier expansion based preprocessing and hidden layer combination techniques to optimize MPNN's structure. Most important of all, our method has been used in real applications in China Mobile. Experiments based on the real datasets also show that our proposed churn prediction method has good maneuverability and performance","Neural networks,
Mobile communication,
Predictive models,
Communication industry,
Data mining,
Neurons,
Application software,
Computer science,
Laboratories,
Pressing"
The new DAQ system for WASA at COSY,"Wide angle shower apparatus (WASA), a detector for charged and neutral particles is currently operated at the CELSIUS storage ring by TSL, Uppsala University. It shall be transferred to the Cooler Synchrotron (COSY) at Forschungszentrum Julich (FZJ) after the anticipated shutdown of CELSIUS in summer 2005. For operation in Julich a new DAQ system is being developed, which conforms to the third generation of the DAQ systems at COSY. This comprises new readout electronics based on an optimized parallel bus with LVDS technology and FPGA-controlled event and buffer management, a synchronization system and a high speed optical link to the readout computer. Architecture of the new DAQ system and the technical issues of subsystems are presented.","Data acquisition,
Detectors,
Storage rings,
Synchrotrons,
Adders,
Readout electronics,
Optical buffering,
Technology management,
Optical fiber communication,
Concurrent computing"
Optimal Super-peer Selection for Large-scale P2P System,"The peer-to-peer (P2P) systems have grown significantly over the last few years due to their potential for sharing various resources. Unstructured hybrid P2P system can improve the performance of the entire network and system using SP (Super-peer), which has the responsibility for query processing instead of OPs (Ordinary-Peer). In these systems, selecting the best SP to join is an important problem, but it is difficult to choose the optimal SP by the various reasons such as heterogeneous capacity, content similarity and dynamic capacity change. In this paper, we present the SP selection¿s problem and SP selection strategy based on dynamic capacity of the SP and content similarity. Also we measure the SP¿s score with weight to the factors distance cost, processing power and content similarity. Through the simulation, we show query processing performance is improved when OPs use our strategy to choose the best SP","Large-scale systems,
Peer to peer computing,
Query processing,
Power measurement,
Costs,
Computer science,
Floods,
Network topology,
Computational modeling,
Information technology"
Internode Distance and Optimal Routing in a Class of Alternating Group Networks,"Alternating group graphs AGn, studied by Jwo and others, constitute a class of Cayley graphs that possess certain desirable properties compared with other regular networks considered by researchers in parallel and distributed computing. A different form, AN n, of such graphs, proposed by Youhou and dubbed alternating group networks, has been shown to possess advantages over AGn. For example, ANn has a node degree that is smaller by a factor of about 2 while maintaining a diameter comparable to that of AGn, is maximally fault-tolerant, and shares some of the positive structural attributes of the well-known star graph. In this paper, we characterize the distance between any two nodes in ANn and present an optimal (shortest-path) routing algorithm for this class of networks","fault tolerant computing,
graph theory,
multiprocessor interconnection networks,
network routing"
Spatial Reasoning for Real-time Robotic Manipulation,"Presented in this paper is an approach to real-time spatial reasoning for manipulative robotic tasks. When a service robot is requested to manipulate an object, it should determine the directions along which it can access and remove the object. The potential accessible directions for the object are retrieved from the object database. Then, spatial reasoning with the surrounding environment and the gripper geometry is invoked to verify the directions. The verification process mainly utilizes the visibility test of the commodity graphics hardware. Then, the directions along which both of the object and gripper are translated without colliding with the surrounding obstacles are computed using Minkowski sum and cube map of the graphics hardware. The access and removal directions are passed to the potential field path planning algorithm to determine the robot arm's full path for accessing, removing and delivering the object. The experimental results show the feasibility of using graphics hardware for manipulative robotic tasks and further its performance gain in real-time manipulation","Graphics,
Hardware,
Grippers,
Service robots,
Information retrieval,
Spatial databases,
Computational geometry,
Testing,
Path planning,
Performance gain"
An extended verifiable secret redistribution protocol for archival systems,"Existing protocols for archival systems make use of verifiability of shares in conjunction with a proactive secret sharing scheme to achieve high availability and long term confidentiality, besides data integrity. In this paper, we extend an existing protocol (Wong et al. [2002]) to take care of more realistic situations. For example, it is assumed in the protocol of Wong et al. that the recipients of the secret shares are all trustworthy; we relax this by requiring that only a majority is trustworthy.","Cryptography,
Availability,
Secure storage,
Access protocols,
Computer science,
Automation,
File servers,
Delay,
Variable structure systems,
Layout"
Quantitative Evaluation of Three Calibration Methods for 3-D Freehand Ultrasound,"In this paper, three different calibration methods for three-dimensional (3-D) freehand ultrasound (US) are evaluated. Calibration is the process of estimating the rigid transformation from US image coordinates to the coordinate system of the tracking sensor mounted onto the probe. Calibration accuracy has an important impact on quantitative studies. Geometrical precision can also be crucial in many interventions and surgery. The proposed evaluation framework relies on a single point phantom and a 3-D US phantom which mimics the US characteristics of human liver. Four quality measures are used: 3-D point localization criterion, distance and volume measurements, and shape based criterion. Results show that during the acquisition procedure, volumetric measurements and shapes of the reconstructed object depend on probe motion used, particularly fan motions for which errors are larger. It is also shown that accurate calibration is essential to obtain reliable quantitative information","Calibration,
Ultrasonic imaging,
Shape measurement,
Volume measurement,
Probes,
Imaging phantoms,
Image sensors,
Sensor phenomena and characterization,
Sensor systems,
Surgery"
A Robust Algorithm for Classification Using Decision Trees,"Decision trees algorithms have been suggested in the past for classification of numeric as well as categorical attributes. SLIQ algorithm was proposed (Mehta et al., 1996) as an improvement over ID3 and C4.5 algorithms (Quinlan, 1993). Elegant Decision Tree Algorithm was proposed (Chandra et al. 2002) to improve the performance of SLIQ. In this paper a novel approach has been presented for the choice of split value of attributes. The issue of reducing the number of split points has been addressed. It has been shown on various datasets taken from UCI machine learning data repository that this approach gives better classification accuracy as compared to C4.5, SLIQ and Elegant Decision Tree Algorithm (EDTA) and at the same time the number of split points to be evaluated is much less compared to that of SLIQ and EDTA",
Towards a Quality Model for Effective Data Selection in Collaboratories,"Data-driven scientific applications utilize workflow frameworks to execute complex dataflows, resulting in derived data products of unknown quality. We discuss our on-going research on a quality model that provides users with an integrated estimate of the data quality that is tuned to their application needs and is available as a numerical quality score that enables uniform comparison of datasets, providing a way for the community to trust derived data.",
Fuzzy clustering with novel separable criterion,"Fuzzy clustering has been used widely in pattern recognition, image processing, and data analysis. An improved fuzzy clustering algorithm was developed based on the conventional fuzzy c-means (FCM) to obtain better quality clustering results. The update equations for the membership and the cluster center are derived from the alternating optimization algorithm. Two fuzzy scattering matrices in the objective function assure the compactness between data points and cluster centers, and also strengthen the separation between cluster centers in terms of a novel separable criterion. The clustering algorithm properties are shown to be an improvement over the FCM method's properties. Numerical simulations show that the clustering algorithm gives more accurate clustering results than the FCM method.",
Application of Artificial Neural Network Based on Q-learning for Mobile Robot Path Planning,"Path planning is a difficult part of the navigation task for the mobile robot under dynamic and unknown environment. It needs to solve a mapping relationship between the sensing space and the action space. The relationship can be achieved through different ways. But it is difficult to be expressed by an accurate equation. This paper uses multi-layer feedforward artificial neural network (ANN) to construct a path-planning controller by its powerful nonlinear functional approximation. Then the path planning task is simplified to a classified problem which are five state-action mapping relationship. One reinforcement learning method, Q-learning, is used to collect training samples for the ANN controller. At last the trained controller runs in the simulation environment and retrains itself furthermore combining the reinforcement signal during the interaction with the environment. Strategy based on the Combination of ANN and Q-learning is better than using only one of the two methods. The simulation result also shows that the strategy can find the optimal path than using Q-learning only.",
Rate-Constrained Beamforming for Collaborating Hearing Aids,"Hearing aids are audio capture devices which aim at providing the hearing impaired with better audibility. Most of the state-of-the-art systems involve sensing devices that work independently. However, the availability of a wireless communication link between two hearing aids would allow to perform spatial beamforming such as to provide better rejection of interfering signals. In this paper, we identify and study the above scenario from an information-theoretic viewpoint. We explore the gain provided by collaborating hearing aids as a function of the communication rate. In particular, we derive a closed-form gain-rate formula in the case where a sound source has to be extracted from ambient noise. A similar analysis is provided in the presence of an interfering point source and the corresponding optimal rate allocation is discussed","Array signal processing,
Collaboration,
Hearing aids,
Microphone arrays,
Sensor arrays,
Auditory system,
Collaborative work,
Wireless communication,
Acoustic noise,
Loudspeakers"
An Integration of Ontology-based and Policy-based Network Management for Automation,"As the Internet continues to grow, the tasks of network management are becoming more and more difficult. Over the past few years, much effort has been given to implement the automation of network management. Ontology seems to be a promising technology for that purpose. However, only ontology alone may not easily achieve the goal of automatic network management. Thus under this background, policies may be added to enhance ontology-based network management. This paper just aims in the integration of ontology-based and policy-based approaches to automate network management. A common model for this integrated automatic network management is then proposed in the paper. To check the feasibility of this model, the paper provides a possible scenario.","Ontologies,
Automation,
Information management,
Computer network management,
Object oriented modeling,
OWL,
IP networks,
Technology management,
Computational intelligence,
Computer science"
Some Reflections on Model Predictive Control of Transmission Voltages,"This paper deals with the application of algorithms inspired by model predictive control to solve voltage-related power system control problems in both normal and emergency operating conditions. In the first part of the paper, we identify critical issues for a practical implementation of this methodology, and analyze how far these requirements have been met so far. In the second part, we outline a voltage control scheme that hopefully addresses the above issues. The central idea of this scheme is a static optimization to determine target control values, followed by a dynamic optimization to produce a feasible transition, both carried out in the closed-loop mode of model predictive control.","Reflection,
Predictive models,
Predictive control,
Voltage control,
Power system modeling,
Open loop systems,
Control systems,
Power system control,
Safety,
Power system reliability"
Statistical Cone-Beam CT Image Reconstruction using the Cell Broadband Engine,"CT images can be reconstructed analytically or iteratively. The analytic methods, e.g. filtered backprojection, are known to be computationally inexpensive and highly accurate. Iterative reconstruction seems of high interest since better dose usage is expected. However, iterative methods are computationally extremely expensive and therefore have been applied to modalities with low amounts of data (e.g. PET) only. A promising algorithm for CT is the ordered subset convex (OSC) whose initial design has recently been significantly improved and now achieves high image quality. Recently, a novel general purpose architecture optimized for distributed computing became available: the Cell Broadband Engine (CBE). Its eight synergistic processing elements (SPEs) currently allow for a theoretical performance of 192 GFlops. We aim at maximizing the OSC image reconstruction speed for flatpanel-based cone-beam CT such as micro-CT or C-arm-CT. For this geometry highly optimized perspective CBE-based cone-beam forward-and backprojection algorithms were designed and implemented. Performance was assessed by reconstructing a 5123 volume from 512 cone-beam projections of size 10242. In combination with a preceding Feldkamp-type initialization, four OSC iterations turned out to be sufficient to achieve high image quality. Using both CBEs of our dual Cell-based blade (Mercury Computer Systems) allows to reconstruct the whole volume in about one minute.",
Distributed Simulation in Industry - A Survey Part 2 - Experts on Distributed Simulation,"Distributed simulation is used very little in industry, especially when compared with the interest in distributed simulation from research and from the military domain. In order to answer the question why industry lags behind, the authors have carried out an extensive survey, using a questionnaire and interviews, with users, vendors, and developers of distributed simulation products, as well as with vendors of non-distributed simulation software. This paper reports on the second part of the survey, namely a series of open ended interviews. We report on the responses we obtained indicating the discrepancies between the different ""worlds"". A categorization of these responses is given using which it is possible to formulate clear guidelines for further developments of standards for distributed simulation","Computational modeling,
Computer simulation,
Defense industry,
Computer industry,
Data analysis,
Packaging,
Mathematics,
Computer science,
Technology management,
Engineering management"
Exploring Large-Scale Video News via Interactive Visualization,"In this paper, we have developed a novel visualization framework to enable more effective visual analysis of large-scale news videos, where keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness measurement to help audiences rind news stories of interest at first glance. A computational approach is also developed to quantify the interestingness measurement of video clips. Our experimental results have shown that our techniques for intelligent news video analysis have the capacity to enable more effective visualization of large-scale news videos. Our news video visualization system is very useful for security applications and for general audiences to quickly find news topics of interest from among many channels","Large-scale systems,
Visualization,
Computer science,
Information analysis,
Computer industry,
Decision making,
Investments,
Databases,
Informatics,
Computational intelligence"
Generating compact redundancy-free XML documents from conceptual-model hypergraphs,"As XML data becomes more and more prevalent and as larger quantities of data find their way into XML documents, the need for quality XML data organization only increase. One standard way of structuring data well is to reduce and, if possible, eliminate redundancy, while at the same time making the storage structures as compact as possible. In this paper, we present a methodology to generate XML storage structures where conforming XML documents are redundancy-free, and for most practical cases, are also fully compact. Our methodology assumes the input is a conceptual-model hypergraph. For the special case that every edge in the hypergraph is binary, we present a simple algorithm, guaranteed to always generate redundancy-free storage structures. We show, however, that generating a minimum number of redundancy-free storage structures is NP-hard. We therefore provide heuristics to guide the process and observe that these heuristics result in satisfactory solutions, which are often optimal. We then present a general algorithm for n-ary edges and show that it generates redundancy-free storage structures. The general algorithm must overcome several problems that do not arise in the special case","XML,
Storage automation,
Proposals,
Feedback,
Information systems,
Computer science"
Online strategies for high-performance power-aware thread execution on emerging multiprocessors,"Granularity control is an effective means for trading power consumption with performance on dense shared memory multiprocessors, such as multi-SMT and multi-CMP systems. With granularity control, the number of threads used to execute an application, or part of an application, is changed, thereby also changing the amount of work done by each active thread. In this paper, we analyze the energy/performance trade-off of varying thread granularity in parallel benchmarks written for shared memory systems. We use physical experimentation on a real multi-SMT system and a power estimation model based on the die areas of processor components and component activity factors obtained from a hardware event monitor. We also present HPPATCH, a runtime algorithm for live tuning of thread granularity, which attempts to simultaneously reduce both execution time and processor power consumption",
Efficiency Study of TCP Protocols in InfrastructuredWireless Networks,"Traditional TCP protocols treat all packet loss as a sign of congestion. Their inability to recognize non-congestion related packet loss has significant effects on the communication efficiency in the wireless networks. Recently proposed protocols such as Freeze-TCP, TCP-Probing, TCP Westwood/Westwood+, TCP Veno, TCP-Jersey, and JTCP, all improve over the traditional TCP protocols. This paper reports a quantitative comparison of recent protocols against the currently most often used namely, TCP SACK, TCP NewReno, and TCP Vegas. Simulation tests were designed for various network layouts, and with differing external interferences in an attempt to most accurately simulate real-life scenarios. To carry out these comparisons, the performance of each protocol was measured based on three benchmark metrics: throughput, average congestion window, and time to complete a file transfer","Intelligent networks,
Wireless networks,
Wireless application protocol,
Transport protocols,
Testing,
Interference,
Telecommunication network reliability,
Propagation losses,
Computer science,
Time measurement"
"Secure Bit: Transparent, Hardware Buffer-Overflow Protection","We propose a minimalist, architectural approach, Secure Bit (patent pending), to protect against buffer overflow attacks on control data (return-address and function-pointer attacks in particular). Secure Bit provides a hardware bit to protect the integrity of addresses for the purpose of preventing such buffer-overflow attacks. Secure Bit is transparent to user software: it provides backward compatibility with legacy user code. It can detect and prevent all address-corrupting buffer-overflow attacks with little runtime performance penalty. Addresses passed in buffers between processes are marked insecure, and control instructions using those addresses as targets will raise an exception. An important differentiating aspect of our protocol is that, once an address has been marked as insecure, there is no instruction to remark it as secure. Robustness and transparency are demonstrated by emulating the hardware, booting Linux on the emulator, running application software on that Linux, and performing known attacks","Hardware,
Protection,
Kernel,
Protocols,
Buffer overflow,
Linux,
Costs,
Runtime,
Robustness,
Application software"
Software-Change Prediction: Estimated+Actual,The authors advocate that combining the estimated change sets computed from impact analysis techniques with the actual change sets that can be recovered from version histories will result in improved software-change prediction. An overview of both impact analysis (IA) and mining software repositories (MSR) is given. These are compared and a discussion of their expressiveness and effectiveness is presented. A framework is proposed to integrate these two approaches for software-change prediction,"Information analysis,
Performance analysis,
Unified modeling language,
State estimation,
Software systems,
Computer science,
History,
Retirement,
Software maintenance,
Conferences"
Study of Network Security Situation Awareness Model Based on Simple Additive Weight and Grey Theory,"Network security situation awareness is a new technology to monitor network security, and it is one of hot research domains in information security. The research situation of situation awareness all over the world is first analyzed. Network security situation awareness model (NSAM) based on simple additive weight and grey theory is presented. The construction of NSAM is divided into two stages: current network security situation evaluation modeling and future network security situation prediction modeling. The model of current network security situation evaluation using simple additive weight is established by the threat degree of various services attacked. The model of future network security situation prediction adopting grey theory is built by past and current network security situation. Test results show that NSAM is feasible and reasonable",
Dependency channel modeling for a LDPC-based Wyner-Ziv video compression scheme,"Research in distributed video coding for low complexity encoding has shown that without knowledge of the correlation between source and side information (i.e. the behavior of the dependency channel), the performance is substantially below that of well known state-of-the-art video coders. In a practical system the decoder needs to estimate the statistics in this dependency channel. In this paper we investigate the relation between the compression ratio and the sensitivity of the estimated channel model parameter at the decoder side. We observe that this is a hard task, but not unrealistic. We show that the tolerable parameter range is very dependent on the compression ratio and the (actual) statistics in the dependency channel.","Video compression,
Decoding,
Error correction codes,
Video coding,
Statistical distributions,
Parameter estimation,
Parity check codes,
Error correction,
Bit error rate,
Mathematical model"
Autotuning Autopilots for Micro-ROVs,"Underwater vehicles are highly nonlinear and complex systems, that makes designing autopilots extremely difficult. This paper presents autotuning as a method for tuning parameters of a micro-ROV autopilot. The main benefit of this procedure is that the model of the process does not have to be known. Autotuning is often used for industrial processes but not on marine vessels. This procedure, which is performed in closed-loop, is completely automated and enables the operator to retune an autopilot whenever ROV performance is degraded (due to different operating points, tether influence, currents, etc.). In this article we use already known different autotuning recommendations (primarily designed for type 0 processes) with some modifications which we recommend for micro-ROVs. We also give results of using different types of PID controllers, whose parameters are being tuned. A real life demonstration on a VideoRay Pro II micro-ROV is provided","Automatic control,
Underwater vehicles,
Payloads,
Control systems,
Remotely operated vehicles,
Three-term control,
Nonlinear control systems,
Employee welfare,
Control engineering computing,
Nonlinear systems"
An Enhanced Wireless LAN Positioning Algorithm based on the Fingerprint Approach,"As ubiquitous computing gained much attention in recent years, location estimation in wireless LAN becomes a hot topic. Previous research work suggests the use of the averaged received signal strength (RSS) as fingerprint can achieve high accuracy for location estimation. In a library environment, however, the accuracy of such traditional approach is barely acceptable. It is because library contains considerably large number of metal bookshelves, and limited number of access points. Worse yet, the layout of these access points in the library is fixed for connection to the Internet, and therefore it is hard to change the environment to adapt for location estimation system. In this paper, we introduce an enhanced fingerprint (EFP) algorithm, and tested it in a library environment. The experiment result showed that the proposed EFP algorithm can have more than 30% of improvement in accuracy over traditional approaches without changing anything in the library environment","Wireless LAN,
Fingerprint recognition,
Libraries,
Testing,
Databases,
Personal digital assistants,
Radar,
Computer science,
Ubiquitous computing,
Internet"
A New Algorithm for Maintaining Closed Frequent Itemsets in Data Streams by Incremental Updates,"Online mining of closed frequent itemsets over streaming data is one of the most important issues in mining data streams. In this paper, we propose an efficient one-pass algorithm, NewMoment to maintain the set of closed frequent itemsets in data streams with a transaction-sensitive sliding window. An effective bit-sequence representation of items is used in the proposed algorithm to reduce the time and memory needed to slide the windows. Experiments show that the proposed algorithm not only attain highly accurate mining results, but also run significant faster and consume less memory than existing algorithm Moment for mining closed frequent itemsets over recent data streams","Itemsets,
Data mining,
Algorithm design and analysis,
Computer science,
Electronic mail,
Size control,
Control systems,
Data analysis,
Character generation,
Error correction"
UAV Assisted Disruption Tolerant Routing,"In order to provide network connectivity in highly partitioned ad-hoc networks, we propose a routing strategy that incorporates an existing ad-hoc routing protocol, ad hoc on demand distance vector (AODV), with disruption tolerant networking (DTN) a la store-carry-forward mechanisms using unmanned aerial vehicles (UAVs) as carriers. This paper focuses on the design, implementation, and evaluation of the routing strategy. The major contribution of this work is the implementation of our DTN aware routing protocol on top of existing and mostly unmodified AODV. We show the advantage of the DTN protocol through simulation using ns-2","Unmanned aerial vehicles,
Routing protocols,
Disruption tolerant networking,
Bandwidth,
Ad hoc networks,
Motion control,
Computer science,
Network interfaces,
Communication system control,
Mobile communication"
Interactive Visualization and Analysis of Network and Sensor Data on Mobile Devices,"Mobile devices are rapidly gaining popularity due to their small size and their wide range of functionality. With the constant improvement in wireless network access, they are an attractive option not only for day to day use. but also for in-field analytics by first responders in widespread areas. However, their limited processing, display, graphics and power resources pose a major challenge in developing effective applications. Nevertheless, they are vital for rapid decision making in emergencies when combined with appropriate analysis tools. In this paper, we present an efficient, interactive visual analytic system using a PDA to visualize network information from Purdue's Ross-Ade Stadium during football games as an example of in-held data analytics combined with text and video analysis. With our system, we can monitor the distribution of attendees with mobile devices throughout the stadium through their access of information and association/disassociation from wireless access points, enabling the detection of crowd movement and event activity. Through correlative visualization and analysis of synchronized video (instant replay video) and text information (play statistics) with the network activity, we can provide insightful information to network monitoring personnel, safety personnel and analysts. This work provides a demonstration and testbed for mobile sensor analytics that will help to improve network performance and provide safety personnel with information for better emergency planning and guidance","Data visualization,
Information analysis,
Personnel,
Monitoring,
Safety,
Displays,
Graphics,
Decision making,
Visual analytics,
Game theory"
Gene Selection for Cancer Classification using Wilcoxon Rank Sum Test and Support Vector Machine,"Gene selection is an important problem in microarray data processing. A new gene selection method based on Wilcoxon rank sum test and support vector machine (SVM) is proposed in this paper. First, Wilcoxon rank sum test is used to select a subset. Then each selected gene is trained and tested using SVM classifier with linear kernel separately, and genes with high testing accuracy rates are chosen to form the final reduced gene subset. Leave-one-out cross validation (LOOCV) classification results on two datasets: breast cancer and ALL/AML leukemia, demonstrate the proposed method can get 100% success rate with final reduced subset. The selected genes are listed and their expression levels are sketched, which show that the selected genes can make clear separation between two classes",
OS-multicast: On-demand Situation-aware Multicasting in Disruption Tolerant Networks,"Disruption tolerant networks (DTNs) are emerging solutions to networks that experience frequent network partitions and large end-to-end delays. In this paper, we study how to provide high-performance multicasting service in DTNs. We develop a multicasting mechanism based on on-demand path discovery and overall situation awareness of link availability (OS-multicast) to address the challenges of opportunistic link connectivities in DTNs. Simulation results show that OS-multicast can achieve a better message delivery ratio than existing approaches, e.g. DTBR (a dynamic tree-based routing), with similar delay performance. OS-multicast also achieves better efficiency performance when the probability of link unavailability is high and the duration of link downtime is large","Intelligent networks,
Disruption tolerant networking,
Delay,
Routing,
Data communication,
Mobile ad hoc networks,
Computer science,
Availability,
Military communication,
Monitoring"
Overall Blocking Behavior Analysis of General Banyan-Based Optical Switching Networks,"Banyan networks are attractive for serving as the optical switch architectures due to their nice properties of small depth and absolutely signal loss uniformity. Combining the horizontal expansion and vertical stacking of optical banyan networks is a general scheme for constructing banyan-based optical switching networks. The resulting horizontally expanded and vertically stacked optical banyan (HVOB) networks usually take either a high hardware cost or a large network depth to guarantee the nonblocking property. Blocking behavior analysis is an effective approach to studying network performance and finding a graceful compromise among hardware cost, network depth, and blocking probability; however, little has been done to analyze the blocking behavior of general HVOB networks. In this paper, we study the overall blocking behavior of general HVOB networks, where an upper bound on the blocking probability of a HVOB network is developed with respect to the number of planes (stacked copies) and the number of stages. The upper bound accurately depicts the overall blocking behavior of a HVOB network as verified by an extensive simulation study, and it agrees with the strictly nonblocking condition of the network. The derived upper bound is significant because it reveals the inherent relationship among blocking probability, network depth, and network hardware cost, so that a desirable tradeoff can be made among them. In particular, our bound gives network developers an effective tool to estimate the maximum blocking probability of a HVOB network, in which different routing strategies can be applied with a guaranteed performance in terms of blocking probability, hardware cost and network depth. Our upper bound model predicts some unobvious qualitative behaviors of HVOB networks, and it draws an important conclusion that a very low blocking probability (e.g., less than 0.001 percent) can be achieved in a HVOB network without introducing either a significantly high hardware cost or a large network depth","Optical fiber networks,
Hardware,
Costs,
Upper bound,
Performance analysis,
Optical switches,
Optical losses,
Stacking,
Routing,
Predictive models"
Incremental Mining of Sequential Patterns over a Stream Sliding Window,"Incremental mining of sequential patterns from data streams is one of the most challenging problems in mining data streams. However, previous work of mining sequential patterns from data streams is almost focused on mining of patterns from stream of item-sequences, not stream of itemset-sequences. In this paper, we propose an efficient single-pass algorithm, called IncSPAM, to maintain the set of sequential patterns from itemset-sequence streams with a transaction-sensitive sliding window. An effective bit-sequence representation of items is used in the proposed algorithm to reduce the time and memory needed to slide the windows. Experiments show that the proposed IncSPAM algorithm is efficient for mining sequential patterns over data streams","Data mining,
Databases,
Sensor phenomena and characterization,
Algorithm design and analysis,
Computer science,
Electronic mail,
Indexing,
Unsolicited electronic mail,
Batteries,
Size control"
Advantages of inset PM machines for zero-speed sensorless position detection,"The aim of this paper is to analyze the behavior of a sensorless control system based on a high-frequency signal injection combined with an inset PM motor. The rotor position of a synchronous permanent magnet (PM) motor with anisotropic rotor can be detected by the control system which superimposes a high-frequency signal to the steady-state stator voltage. The corresponding high-frequency current is modulated by the rotor anisotropy and used to determine the rotor position. These techniques are effective at zero and at low motor speed. Both saturation and cross-coupling have a heavy influence on the correct rotor position detection. In order to highlight the effectiveness of the sensorless technique, the tests are carried out at various operating conditions",
A New Routing Algorithm in Triple-Based Hierarchical Interconnection Network,"Triple-based hierarchical interconnection network (THIN) is not only a new kind of direct networks but also a kind of HIN's. Efficient routing algorithm is very essential to the performance of the interconnection network and the parallel computing system. This paper presents DDRA (distributed deterministic routing algorithm) routing algorithm in triple-based hierarchical interconnection network. Fully applying the hierarchical characteristic of the network, DDRA routing algorithm just uses the node address to determine an approximate minimal path between source and destination node, without constructing the route table on each node. The analysis based on the simulation of DDRA shows it is not only very simple and easy to be implemented in hardware, but has high efficiency",
Fast and Robust Feature-based Recognition of Multiple Objects,"In robotics, one crucial requirement to a visual system is robust and efficient recognition of multiple objects. While in many available systems the focus is on tracking, the main problem still is to recognize objects in an arbitrary scene within a database of multiple objects. For any tracking system, recognition is needed for initialization and therefore always built in. However, the task of recognition becomes considerably harder, when learning and recognizing multiple objects. In this paper, we present a system, which accomplishes this task for textured objects robustly and efficiently. Our system is based on texture features, combining principal component analysis, k-means clustering and kd-tree search with best-bin-first strategy. We evaluated our system in several real-word scenarios, and present experimental results in a kitchen environment. Within a database of 20 objects, our system can analyze an arbitrary scene in less than 350 ms on a 3 GHz CPU","Robustness,
Layout,
Principal component analysis,
Image recognition,
Service robots,
Object recognition,
Image segmentation,
Robot vision systems,
Cameras,
Humanoid robots"
Photorealistic Attention-Based Gaze Animation,"We apply a neurobiological model of visual attention and gaze control to the automatic animation of a photorealistic virtual human head. The attention model simulates biological visual processing along the occipito-parietal pathway of the primate brain. The gaze control model is derived from motion capture of human subjects, using high-speed video-based eye and head tracking apparatus. Given an arbitrary video clip, the model predicts visual locations most likely to attract an observer's attention, and simulates the dynamics of eye and head movements towards these locations. Tested on 85 video clips including synthetic stimuli, video games, TV news, sports, and outdoor scenes, the model demonstrates a strong ability at saccading towards and tracking salient targets. The resulting autonomous virtual human animation is of photorealistic quality",
Cooperative Mutual 3D Laser Mapping and Localization,"A 3D laser scanner is built by adding a rotating mirror to a conventional 2D scanner. The scanners are deployed on four robots to build full 3D representations of an indoor environment. An original representation mechanism referred to as occupancy lists, rather than standard 2D free space grids, is used to maintain the 3D map. Localization is done by extracting horizontal sub-ceiling cross-sections. Taking cross-sections from near the ceiling in this way results in more reliable and time invariant maps. Experimental results show inter-robot sightings and the sharing of map data aid mapping by improving the reliability of localization. Mapping with four robots reduced the average position error from 0.35 m for single robot operation to 0.1 m when cooperating.",
Analysis of BGP prefix origins during Google's May 2005 outage,"Google went down for 15 to 60 minutes around 22:10, May 07, 2005 UTC. This was explained by Google as having been caused by internal DNS misconfigurations. Another vulnerable protocol which could have caused such service outage is BGP. To pursue the latter possibility further, we explore how BGP was functioning during that period of time using the RouteViews BGP data set. Interestingly, our investigation reveals that one autonomous system (i.e., AS 174 operated by Cogent), which is apparently independent from Google, mysteriously originated routes for one of the IP prefixes assigned to Google (134.233.161.0/24) immediately prior to the service outage. As a result, 49.1% of ASes re-advertising routes for 64.233.161.0/24 switched to the incorrect path. Those poisoned ASes directly serve 1500 IP prefixes, and span a broad range of geographic locations. Since this erroneous prefix origination apparently has not occurred previously, or after this specific instance, we consider that it might have been the result of malicious activity (e.g., compromise of one or more BGP speakers) and contributed at least partially to Google's service outage.",
FPGA core watermarking based on power signature analysis,"This paper introduces a new method to watermark FPGA cores where the signature (watermark) is detected at the power supply pins of the FPGA. This is the first watermarking method, where the signature is extracted in this way. The authors were able to sign cores at the netlist as well as the bitfile level, so a wide spectrum of cores can be protected. The power watermarking method works with all types of FPGAs, but with Xilinx FPGAs, the watermarking algorithms and the signature can be integrated into the functionality of the watermarked core. So it is very hard to remove the watermark without destroying the core. A detection algorithm was introduced which can decode the signature from a voltage trace with high probability. Additionally, a second algorithm is introduced which improves the detection probability in case of considerable noise sources. Using this algorithm, it is possible to decode the signature even if other cores operate on the same device at the same time",
A Steering Environment for Online Parallel Visualization of Legacy Parallel Simulations,"In the context of scientific computing, the computational steering consists in the coupling of numerical simulations with 3D visualization systems through the network. This allows scientists to monitor online the intermediate results of their computations in a more interactive way than the batch mode, and allows them to modify the simulation parameters on-the-fly. While most of existing computational steering environments support parallel simulations, they are often limited to sequential visualization systems. This may lead to an important bottleneck and increased rendering time. To achieve the required performance for online visualization, we have designed the EPSN framework, a computational steering environment that enables to interconnect legacy parallel simulations with parallel visualization systems. For this, we have introduced a redistribution algorithm for unstructured data, that is well adapted to the context of M times N computational steering. Then, we focus on the design of our parallel viewer and present some experimental results obtained with a particle-based simulation in astrophysics","Computational modeling,
Data visualization,
Rendering (computer graphics),
Concurrent computing,
Graphics,
Numerical simulation,
Clustering algorithms,
Displays,
Analytical models,
Context modeling"
Partner Assignment Algorithm for Cooperative Diversity in Mobile Communication Systems,"Most work on cooperative diversity has assumed that the cooperating group (source and partners) and the associated average channel conditions between terminals (source, partners, and destination) are predetermined. In practical situations, however, it is important to develop the efficient algorithms for assigning the terminals with good inter-user channels for cooperating groups. In this paper, we propose the partner assignment algorithm for cooperative diversity in mobile communication systems. The proposed partner assignment algorithm is investigated by using the path loss model for mobile communication systems. Numerical results show that the proposed partner assignment algorithm provides the comparable probability of cooperative transmission to the partner assignment algorithm using exhaustive search. The probability of cooperative transmission increases with the number of users, which gives potential benefits of practical implementation to user cooperation in mobile communication systems. The proposed partner assignment algorithm also provides the tradeoff between probability of cooperative transmission, cooperative diversity gain, and the amount of average SNR information of inter-user channels",
Real time Pressure-Volume loops in mice using complex admittance: measurement and implications,"Real time left ventricular (LV) pressure-volume (P-V) loops have provided a framework for understanding cardiac mechanics in experimental animals and humans. Conductance measurements have been used for the past 25 years to generate an instantaneous left ventricular (LV) volume signal. The standard conductance method yields a combination of blood and ventricular muscle conductance; however, only the blood signal is used to estimate LV volume. The state of the art techniques like hypertonic saline injection and IVC occlusion, determine only a single steady-state value of the parallel conductance of the cardiac muscle. This is inaccurate, since the cardiac muscle component should vary instantaneously throughout the cardiac cycle as the LV contracts and fills, because the distance from the catheter to the muscle changes. The capacitive nature of cardiac muscle can be used to identify its contribution to the combined conductance signal. This method, in contrast to existing techniques, yields an instantaneous estimate of the parallel admittance of cardiac muscle that can be used to correct the measurement in real time. The corrected signal consists of blood conductance alone. We present the results of real time in vivo measurements of pressure-admittance and pressure-phase loops inside the murine left ventricle. We then use the magnitude and phase angle of the measured admittance to determine pressure volume loops inside the LV on a beat by beat basis. These results may be used to achieve a substantial improvement in the state of the art in this measurement method by eliminating the need for hypertonic saline injection",
Closed Multidimensional Sequential Pattern Mining,"We propose a new method, called closed multidimensional sequential pattern mining, for mining multidimensional sequential patterns. The new method is an integration of closed sequential pattern mining and closed itemset pattern mining. Based on this method, we show that (1) the number of complete closed multidimensional sequential patterns is not larger than the number of complete multidimensional sequential patterns (2) the set of complete closed multidimensional sequential patterns covers the complete resulting set of multidimensional sequential patterns. In addition, mining using closed itemset pattern mining on multidimensional information would mine only multidimensional information associated with mined closed sequential patterns, and mining using closed sequential pattern mining on sequences would mine only sequences associated with mined closed itemset patterns",
JOSHUA: Symmetric Active/Active Replication for Highly Available HPC Job and Resource Management,"Most of today's HPC systems employ a single head node for control, which represents a single point of failure as it interrupts an entire HPC system upon failure. Furthermore, it is also a single point of control as it disables an entire HPC system until repair. One of the most important HPC system services running on the head node is the job and resource management. If it goes down, all currently running jobs loose the service they report back to. They have to be restarted once the head node is up and running again. With this paper, we present a generic approach for providing symmetric active/active replication for highly available HPC job and resource management. The JOSHUA solution provides a virtually synchronous environment for continuous availability without any interruption of service and without any loss of state. Replication is performed externally via the PBS service interface without the need to modify any service code. Test results as well as availability analysis of our proof-of-concept prototype implementation show that continuous availability can be provided by JOSHUA with an acceptable performance trade-off",
Privacy-Preserving Data Linkage and Geocoding: Current Approaches and Research Directions,"Data linkage is the task of matching and aggregating records that relate to the same entity from one or more data sets. A related technique is geocoding, the matching of addresses to their geographic locations. As data linkage is often based on personal information (like names and addresses), privacy and confidentiality are of paramount importance. In this paper we present an overview of current approaches to privacy-pre serving data linkage, and discuss their limitations. Using real-world scenarios we illustrate the significance of developing improved techniques for automated, large scale and distributed privacy-pre serving linking and geocoding. We then discuss four core research areas that need to be addressed in order to make linking and geocoding of large confidential data collections feasible","Couplings,
Joining processes,
Ores,
Computer science,
Government,
Soil,
Terrorism,
Information retrieval,
Data mining,
Conferences"
An Integrated High-Level On-Line Test Synthesis Tool,"Several researchers have recently implemented on-line testability in the form of duplication-based self-checking digital system design, early in the design process. The authors consider the on-line testability within the optimization phase of iterative, cost function-driven high-level synthesis, such that self-checking resources are inserted automatically without any modification of the source behavioral hardware description language code. This is enabled by introducing a metric for the on-line testability. A new variation of duplication (namely inversion testing) is also proposed and used, providing the system with an additional degree of freedom for minimizing hardware overheads associated with test resource insertion. Considering the on-line testability within the synthesis process facilitates fast and painless design space exploration, resulting in a versatile high-level-synthesis process, capable of producing alternative realizations according to the designer's directions, for alternative target technologies. Finally, the fault escape probability of the overall scheme is discussed theoretically and evaluated experimentally","Circuit faults,
Hardware,
Automatic testing,
System testing,
High level synthesis,
Design automation,
Computer science,
Circuit testing,
Adders,
Digital systems"
Policy-directed data movement in grids,"One of the guiding principles of the grid is local (site) autonomy. Resource owners maintain control over their resources even when those resources are part of a larger grid. In other words, resource sharing in grids must be subject to the policies of the (local) resource owners. To date, not enough attention has been paid to describing, manipulating or enforcing explicit resource usage policies. Most existing grid systems have either implicit resource usage policies (with ad-hoc enforcement mechanisms) or support only limited types of policies (e.g., security policies). Systems that do provide some support for resource usage policies typically consider only CPU resources, leaving the provisioning of other resources on the grid unconstrained. This paper focuses on policies for grid storage resources. We have identified classes of policies for grid storage resource providers and have implemented an explicit policy-based architecture to manage and enforce them. This architecture consists of two components, MyPolMan, a service to express and manage policies, and .NET GridFTP, an enforcement mechanism for policy-based grid data movement. We show how this system allows grid users to access storage resources through a familiar API while allowing local system administrators to control resource utilization",
Control of Rectangular Multi-Affine Hybrid Systems,"We study the problem of feedback control for a class of non-linear hybrid systems characterized by rectangular invariants and multi-affine dynamics, which we call rectangular multi-affine hybrid systems. The goal is to find initial states and feedback control strategies so that all trajectories of the closed loop system satisfy arbitrary specifications given as temporal logic formulas over the set of discrete states of the system. Sufficient conditions for solvability are obtained in terms of sets of linear inequalities. If these conditions are satisfied, a control strategy is automatically constructed. The computation consists of polyhedral set operations, construction of Buchi automata from linear temporal logic formulas, and searches on graphs","Control systems,
Vehicle dynamics,
Automatic control,
Feedback control,
Sufficient conditions,
Automata,
Logic circuits,
Control system synthesis,
Genetics,
Biological system modeling"
Interaction Testing in Model-Based Development: Effect on Model-Coverage,"Model-based software development is gaining interest in domains such as avionics, space, and automotives. The model serves as the central artifact for the development efforts (such as, code generation), therefore, it is crucial that the model be extensively validated. Automatic generation of interaction test suites is a candidate for partial automation of this model validation task. Interaction testing is a combinatorial approach that systematically tests all t-way combinations of inputs for a system. In this paper, we report how well interaction test suites (2-way through 5-way interaction test suites) structurally cover a model of the mode- logic of a flight guidance system. We conducted experiments to (1) compare the coverage achieved with interaction test suites to that of randomly generated tests and (2) determine if interaction test suites improve the coverage of black-box test suites derived from system requirements. The experiments show that the interaction test suites provide little benefit over the randomly generated tests and do not improve coverage of the requirements-based tests. These findings raise questions on the application of interaction testing in this domain.","System testing,
Switches,
Software testing,
Automatic testing,
Aerospace electronics,
Automation,
Computer science,
Programming,
Automotive engineering,
Software systems"
On Mining Moving Patterns for Object Tracking Sensor Networks,"In this paper, we propose a heterogeneous tracking model, referred to as HTM, to efficiently mine object moving patterns and track objects. Specifically, we use a variable memory Markov model to exploit the dependencies among object movements. Furthermore, due to the hierarchical nature of HTM, multi-resolution object moving patterns are provided. The proposed HTM is able to accurately predict the movements of objects and thus reduces the energy consumption for object tracking. Simulation results show that HTM not only is able to effectively mine object moving patterns but also save energy in tracking objects.","Object detection,
Computer science,
Tracking,
Energy conservation,
Humans,
Animals,
Collaboration,
Magnetic heads,
Energy consumption,
Data mining"
Using P2P Networks for Error Recovery in MBMS Applications,"The wireless networks are notoriously error prone and all errors cannot be prevented in real-time communications. The problem of error correction becomes even more challenging in mobile multicast/broadcast applications. The mobile devices are being equipped with multiple modems that could work simultaneously; for example, devices with both GSM (WAN) and WLAN networks such as WiFi. These multi-modal devices can use the second network to improve their error resilience. We propose a P2P approach to establish and utilize an error recovery channel on a secondary network for multi-user video applications. The mobile devices within the vicinity can utilize the WLAN network to form a P2P network for error recovery purpose. We developed and evaluated three error recovery models for error recovery over secondary networks. The proposed models balance the response time, bandwidth utilization, fairness, and unnecessary data received",
Efficient Group Mobility for Heterogeneous Sensor Networks,"Mobility management protocols allow wireless devices to move between networks. These protocols have traditionally supported the mobility of individual nodes and are therefore not optimized to support the migration of groups. Accordingly, the time required to re-establish connectivity, frequency of dropped packets and contention for the air interface increase significantly for mobile groups. We propose a protocol for mobile groups that reduces all of the above by allowing a single node to perform handoffs on behalf of all group members. This ""gateway"" node eliminates the need for multiple handoff messages by obscuring group membership to external parties. Through extensive simulation and implementation, we show significant reduction in handoff times, message complexity and packet loss for groups of heterogeneous, mobile sensors running AODV and DSDV. By leveraging the naturally occurring hierarchy, we demonstrate that it is possible for groups to efficiently use traditional mobility protocols to support their collective movements.","Protocols,
Wireless sensor networks,
Network address translation,
Mobile radio mobility management,
Delay,
Computer security,
Information security,
Computer science,
Information science,
Laboratories"
Fault-tolerant power assignment and backbone in wireless networks,"Given a wireless network, we need to assign transmission power to each of the nodes, that will enable communication between any two nodes (via other nodes). Moreover, due to possible faults, we would like to have at least k vertex-disjoint paths from any node to any other node, where k is some fixed integer, depending on the reliability of the nodes. The goal is to achieve this directed k-connectivity with minimal overall power consumption. The problem is NP-Hard for any k ges 1, already for planar networks. Here we develop an O(k)-approximation algorithm for the planar case. We also address the problem of constructing a connected backbone, for which we present an efficient constant-factor approximation algorithm","Fault tolerance,
Spine,
Intelligent networks,
Wireless networks,
Transceivers,
Costs,
Computer science,
Ad hoc networks,
Fault tolerant systems,
Power engineering and energy"
The control system for the front-end electronics of the ALICE time projection chamber,"The ALICE detector is a dedicated heavy-ion detector currently built at the Large Hadron Collider (LHC) at CERN. The detector control system (DCS) covers the task of controlling, configuring and monitoring the detector. One sub-system is the control system for the Front-end electronics of the time projection chamber (TPC). It controls in total 216 readout systems with 4356 Front-End Cards serving roughly 560 000 channels. The system consists of a large number of distributed nodes in a layer-structured hierarchy. The low-level node controlling the Front-end electronics is an embedded computer system, the DCS board, which provides the opportunity to run a light-weight Linux system on the card. The board interfaces to the Front-end electronics via a dedicated hardware interface and connects to the higher DCS-layers via the DIM communication framework over Ethernet. Since the experiment will be running in a radiation environment, fault tolerance, error correction and system stability in general are major concerns. Already the low level devices carry out intelligent error handling and act automatically upon several conditions. This paper presents the architecture of the system, the application of the DCS board and experiences from integration tests.","Control systems,
Detectors,
Distributed control,
Large Hadron Collider,
Monitoring,
Lighting control,
Communication system control,
Embedded computing,
Linux,
Hardware"
Rule-based tracking of multiple lanes using particle filters,"Tracking of lanes is essential for intelligent vehicles in order to drive autonomously. A system is presented which allows tracking of multiple lanes. The system is based on a clear modelling of a lane and the parameter set of each lane is estimated using a particle filter which fuses different cues. A finite-state machine then decides whether or not a lane is really tracked. For each lane, a separate tracker is used and a set of rules controls the life-cycle of all trackers and keeps track of all the estimated lanes","Particle tracking,
Particle filters,
Roads,
Intelligent vehicles,
Life estimation,
Probability density function,
Intelligent systems,
Fuses,
Knowledge based systems,
Topology"
Multiple flows of control in migratable parallel programs,"Many important parallel applications require multiple flows of control to run on a single processor. In this paper, we present a study of four flow-of-control mechanisms: processes, kernel threads, user-level threads and event-driven objects. Through experiments, we demonstrate the practical performance and limitations of these techniques on a variety of platforms. We also examine migration of these flows-of-control with focus on thread migration, which is critical for application-independent dynamic load balancing in parallel computing applications. Thread migration, however, is challenging due to the complexity of both user and system state involved. In this paper, we present several techniques to support migratable threads and compare the performance of these techniques",
XML syntax conscious compression,"XML is the standard format of content representation and sharing on the Web. XML is a highly verbose language, especially regarding the duplication of meta-data in the form of elements and attributes. As XML content is becoming more widespread so is the demand to compress XML data volume. The paper presents the best XML compression ratios reported to date. Its advantage over other XML compression techniques is that it uses syntactic information to enhance compression. Therefore, it is a fully syntactic based XML compression. The syntactic information is parsed from XML documents by an innovative XML parser. We developed a new XML parser-generator for that purpose. Our parser-generator is based on a syntactic dictionary (DTD, XML-Schema, etc.) of the XML in order to create an efficient and compact XML parsers. This XML parser-generator is adopted to streaming technologies and can be used in a wide variety of XML applications such as validators, converters, gateways, routers, browsers editors etc. The parsers' symbols are encoded by a partial prediction matching (PPM) codec. We compare between the performance of our algorithm and other existing XML compression techniques. The proposed compression algorithm achieves better compression ratio in comparison to other XML compression techniques that do not utilize syntactic structure. The superiority of our compression technique is more evident when it is tested on XML data sets that contain only tags and not free text","XML,
Dictionaries,
Production,
Uniform resource locators,
Computer science,
Codecs,
Compression algorithms,
Testing,
Data compression,
Tagging"
"Design, Implementation and Evaluation of a Remote Laboratory System for Electrical Engineering Courses","This paper describes an Internet-based laboratory, named Remote Monitored and Controlled Laboratory (RMCLab) developed at University of Patras for electrical engineering laboratory education. The key feature of this remote laboratory is the utilization of real experiments, in terms of instrumentation and lab circuits, rather than simulation or virtual reality environment. RMCLab is able to provide affordably and simultaneously its services to many users through the Internet. A wide variety of lab experiments are supported, including not only fixed circuits but also students' custom circuit designs; thus contributing to students' authentic understanding of theory subjects. RMCLab can be accessed via the Web through http://www.apel.ee.upatras.gr/rmclab",
The Multimessage Unicast Capacity Region for Bidirectional Ring Networks,"The capacity region for multiple unicast sessions on a bidirectional ring network is established and is shown to be achieved by routing. The proof uses recently developed progressive d-separating edge set bounds, new extensions of these bounds, as well as tools from the literature on the multicommodity flow problem","Unicast,
Clocks,
Telecommunication traffic,
Routing,
SONET,
Network coding,
Internet telephony,
Computer science,
Communication industry,
Optical fibers"
Biodiversity World: a problem-solving environment for analysing biodiversity patterns,"In the Biodiversity World (BDW) project we have created a flexible and extensible Web services-based grid environment for biodiversity researchers to solve problems in biodiversity and analyse biodiversity patterns. In this environment, heterogeneous and globally distributed biodiversity-related resources such as data sets and analytical tools are made available to be accessed and assembled by users into workflows to perform complex scientific experiments. One such experiment is bioclimatic modelling of the geographical distribution of individual species using climate variables in order to predict past and future climate-related changes in species distribution. Data sources and analytical tools required for such analysis of species distribution are widely dispersed, available on heterogeneous platforms, present data in different formats and lack interoperability. The BDW system brings all these disparate units together so that the user can combine tools with little thought as to their availability, data formats and interoperability. We describe the architecture of the BDW problem solving environment (PSE) consisting of a number of components providing uniform access to heterogeneous resources and analytical tools. Architectural components of the BDW system include a workflow management tool, resource wrappers, a communications layer, BDW datatypes and a metadata repository.","Biodiversity,
Problem-solving,
Pattern analysis,
Data analysis,
Biological system modeling,
Predictive models,
Performance analysis,
Availability,
Computer science,
History"
A Memory Model for Scientific Algorithms on Graphics Processors,"We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C's model to analyze the cache misses. Moreover, we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications - sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30-50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on high-end processors. In practice, we are able to achieve 2-5x performance improvement","Graphics,
Algorithm design and analysis,
Performance analysis,
Sorting,
Bandwidth,
Hardware,
Computer architecture,
Fast Fourier transforms,
Throughput,
Permission"
Capturing the Elusive Poissonity in Web Traffic,"Numerous studies have shown that the process of packet arrivals from Web traffic exhibits strong long-range dependence, which makes it not amenable to be described using the convenient but necessarily short-range dependent framework of Poisson modeling. However, Web traffic is ultimately driven by independent human behavior, so it seems natural to search for an underlying ""seed process"", consistent with Poissonity, indirectly driving the packet arrivals of Web traffic. Our study examines Web traffic at different levels of packet aggregation, using powerful statistical analysis tools for identifying the finest level that can be effectively modeled using a homogeneous Poisson process. We show that the arrivals of HTTP responses, TCP connections and Web pages do not provide a satisfactory seed process. However, we find Poissonity in the arrivals of ""navigation bursts"". A navigation burst is a tightly-spaced sequence of Web pages downloaded by the same Web client, which can be explained by fast navigation through several pages before reaching relevant content. Our analysis suggests that the start times of such navigation bursts, which we identify by detecting user think times between 12 and 30 seconds, can be effectively modeled as a homogeneous Poisson process. We believe that our methodology can be extended to other complex modeling problems where finding Poissonity can greatly simplify parsimonious modeling.",
Active User-Based and Ontology-Based Web Log Data Preprocessing for Web Usage Mining,"User identification and session identification are two major steps in preprocessing Web log data for Web usage mining. This paper introduces a fast active user-based user identification algorithm with time complexity O(n). The algorithm uses both an IP address and a finite users' inactive time to identify different users in the Web log. Web site ontology is useful for identifying Web site structure and break points for browsing behavior. For session identification, we present an ontology-based method that utilizes the Web site structure and functionalities to identify different sessions","Ontologies,
Data preprocessing,
HTML,
Data engineering,
Computer science,
Navigation,
Application software,
Cleaning,
Web mining,
Data security"
Automatic Segmentation of the Papilla in a Fundus Image Based on the C-V Model and a Shape Restraint,"For computer aided Glaucoma diagnostics it is essential to robustly and automatically detect and segment the main regions, e.g. the papilla (optic nerve head), in a fundus image. In this paper an effective method for automatic papilla segmentation based on the C-V model and a shape restraint is proposed. The method is a combination between the C-V model using level sets and the elliptic shape restraint for papilla segmentation. The combination of the level set framework with a shape restraint ensures that the evolving curve stays an ellipse. Experiments verify that the method shows a good performance in detecting the papilla shapes and computing the shape feature parameters within a broad variety of fundus images. The experiment results also show that the method is robust to noise and object deformity",
Service oriented pervasive computing for emergency response systems,"Pervasive computing paradigm is characterized by the presence of a diverse variety of computing and communicating devices. The vision of pervasive computing is to enable effective support for user tasks by utilizing the devices carried by the users and those available within the infrastructure around the users. Emergency response is very critical to crisis situations. The process of emergency response can benefit greatly by the application of pervasive computing principles. In this paper, we describe a prototype designed to enhance the existing emergency response mechanism in attending to automobile accident victims. The prototype has been built using our event oriented middleware called pervasive information communities organization (PICO), and is built on top of a popular P2P framework called JXTA. By utilizing the middleware, we model the useful features of devices available within a car as services. Our proposed framework allows the creation, discovery and maintenance of continuous services such as crash detection. Further more, the framework allows the composition of complex services on occurrence of events by utilizing the available basic services with a goal to provide essential support in case of an accident. We present the design and implementation details of the prototype and snapshots of the working prototype","Pervasive computing,
Road accidents,
Prototypes,
Middleware,
Automobiles,
Layout,
Personal digital assistants,
Personnel,
Road transportation,
Computer science"
"Apples, Oranges, and Testbeds","Research into wireless sensor networks is rapidly moving from simulations to realistic testbeds. The widely varying characteristics (e.g., radio hardware, #nodes, and topology) of various testbeds raise concerns about the validity of results across different testbeds. This paper presents empirical data of an experiment involving one application (Surge), two routing protocols (multihop and mintroute), and two testbeds (MoteLab and MistLab). The outcome is somewhat mixed. When increasing the data rate, congestion causes goodput to fall off in a similar fashion on both testbeds, which is good, but only when ignoring mintroute ill-behaving for low rates on MoteLab, which is bad. Accounting for differences in communication hardware is necessary, but even then results should be taken with a grain of salt. This certainly holds for TOSSIM simulation results, since we found that they generally do not match those of physical testbeds","Benchmark testing,
Wireless sensor networks,
Hardware,
Network topology,
Surges,
Routing protocols,
Energy consumption,
Mathematics,
Computer science,
Computational modeling"
Automated Pericardial Fat Quantification in CT Data,"Recent evidence indicates that pericardial fat may be a significant cardiovascular risk factor. Although pericardial fat is routinely imaged during computed tomography (CT) for coronary calcium scoring, it is currently ignored in the analysis of CT images. The primary reason for this is the absence of a tool capable of automatic quantification of pericardial fat. Recent studies on pericardial fat imaging were limited to manually outlined regions-of-interest and preset fat attenuation thresholds, which are subject to inter-observer and inter-scan variability. In this paper, we present a method for automatic pericardial fat burden quantification and classification. We evaluate the performance of our method using data from 23 subjects with very encouraging results",
Tools for the automation of large distributed control systems,"The new LHC experiments at CERN will have very large numbers of channels to operate. In order to be able to configure and monitor such large systems, a high degree of parallelism is necessary. The control system is built as a hierarchy of sub-systems distributed over several computers. A toolkit-SMI++, combining two approaches: finite state machines and rule-based programming, allows for the description of the various sub-systems as decentralized deciding entities, reacting in real-time to changes in the system, thus providing for the automation of standard procedures and for the automatic recovery from error conditions in a hierarchical fashion. In this paper we will describe the principles and features of SMI++ as well as its integration with an industrial SCADA tool for use by the LHC experiments and we will try to show that such tools, can provide a very convenient mechanism for the automation of large scale, high complexity, applications.","Automation,
Distributed control,
Large Hadron Collider,
Computerized monitoring,
Parallel processing,
Automatic control,
Control systems,
Distributed computing,
Automata,
Automatic programming"
DTuples: A Distributed Hash Table based Tuple Space Service for Distributed Coordination,"In this paper, we introduce a Linda (Carriero et al., 1989) like peer-to-peer tuple space middleware build on top of distributed hash table Tuples. With the help of DTuples, the development of the distributed cooperation and coordination task would be simplified. The decoupled style of tuple space (Carriero and Gelemter, 2001) model is useful in peer-to-peer environment. In the application level, the DTuples was used instead of the publish/subscribe model and message-passing model. In low level, the DTuples service can be used as cooperation middleware and context-aware middleware. The tuples in the DTuples are stored in distributed hash table based peer-to-peer tuple storage. In our current work, the in(), rd(), out() and copy-collect() primitives are supported. The eval() primitive will be added to D-Tuples in the future. In this paper, we present the key design concepts of DTuples",
Distributed Decision Making Algorithm for Self-Healing Sensor Networks,"Sensor nodes are unreliable small devices with limited energy supply, and sensors may run out of power or fail over time. A sensor network should have self-healing capability to handle coverage holes, routing voids and disconnections caused by random sensor locations and sensor failures. To provide self-healing capabilities in sensor networks while keeping the cost low, we propose to deploy a small number of mobile sensors in addition to a large number of static sensors. A mobile sensor may receive multiple requests from different areas. In this paper, we present a distributed decision making algorithm based on fuzzy logic. The distributed algorithm has low computation requirement and is robust to incomplete information. Our extensive simulations demonstrate the self-healing capabilities provided by mobile sensors, and the good performance of the distributed decision making algorithm.",
Scheduled video delivery-a scalable on-demand video delivery scheme,"Continuous media, such as digital movies, video clips, and music, are becoming an increasingly common way to convey information, entertain and educate people. However, limited system and network resources have delayed the widespread usage of continuous media. Most existing on-demand services are not scalable for a large content repository. In this paper, we propose a scalable and inexpensive video delivery scheme, named Scheduled Video Delivery (SVD). In the SVD scheme, users submit requests with a specified start time. Incentives are provided so that users will specify the start times that reflect their real needs. The SVD system combines requests to form the multicasting groups and schedules these groups to meet the deadline. SVD scheduling has a different objective from many existing scheduling schemes. Its focus has been shifted from minimizing the waiting time toward meeting deadlines and at the same time combining requests to form multicasting groups. SVD compliments most existing video delivery schemes as it can be combined with them. It requires much less resources than other schemes. Simulation study for the SVD performance and the comparison to other schemes are presented.",
Using constraint hierarchies to support QoS-guided service composition,"A key impediment to the widespread adoption of Web services is the relatively limited set of tools available to deal with quality-of-service (QoS) factors (Ran, 2003). QoS factors pose several difficult challenges in how they may be articulated. While the functional requirements of a service can be represented as predicates to be satisfied by the target system, QoS factors are effectively statements of objectives to be maximized or minimized. QoS requirements occur naturally as local specifications of preference. Dealing with QoS factors is therefore a multi-objective optimization problem. In effect, these objectives are never fully satisfied, but satisfied to varying degrees. In evaluating alternative design decisions, we need to trade-off varying degrees of satisfaction of potentially mutually contradictory non-functional requirements. One key contribution of this paper is the use of the constraint hierarchies framework from hierarchical constraint logic programming framework in dealing with quality of service (QoS) factors . We show how QoS factors can be formulated as soft constraints and how the machinery associated with constraint hierarchies can be used to evaluate the alternative trade-offs involved in seeking to satisfy a set of QoS factors that might pull in different directions. We apply also this approach to the problem of reasoning about Web service selection and composition, and establish that significant value can be derived from such an exercise",
A multi-agent based framework for collaborative testing on Web services,"Web services (WS) is currently the major implementation of service-oriented architecture (SOA). It defines a framework for agile and flexible integration among autonomous services based on Internet open standards. However, testing has been a challenge due the dynamic and collaborative nature of WS. This paper introduces an on-going project on a multiagent based framework to coordinate distributed test agents to generate, plan, execute, monitor and communicate tests on WS. Test agents are classified into different roles which communicate through XML-based agent test protocols. Test master accepts test cases from test generator, generates test plans and distributed them to various test groups. A set of test agents that implement a test plan are organized into a test group, which is coordinated by a test coordinator. Test runners execute the test scripts, collect test results and forward the results to test analyzer for quality and reliability analysis. The status of the test agents are monitored by the test monitor. Test agents are dynamically created, deployed and organized. Through the monitoring and coordinating mechanism, the agents can re-adjust the test plan and their behavior at runtime to be adaptive to the changing environment","Collaboration,
Testing,
Web services,
Service oriented architecture,
Application software,
Monitoring,
Runtime,
Computer science,
Web and internet services,
Context-aware services"
Robot Middleware and its Standardization in OMG - Report on OMG Technical Meetings in St. Louis and Boston,"In this paper, we report our activities of standardization in OMG, especially the OMG St. Louis Technical Meeting held in April, and the OMG Boston Technical Meeting held in June. The goal of this paper is to start a technical discussion in the robotics and mechatronics community to promote RT-middleware technology (robot systems designed by reusable building blocks) that results in efficient development of complicated robot systems. As a vehicle for the rapid development and diffusion of this RT-middleware technology, we've just started up an international standardization activity on robot technologies. In order to increase interoperability, compatibility and reusability between the various robotic common components, Object Managing Group (OMG) established Robotics Domain Task Force (Robotics-DTF) in December 2005. The Robotics-DTF is looking to begin a dialog with vendors, end users, researchers, robotics organizations and other interested parties to lay the groundwork for a common platform-independent model of robotics software development",
Optical Clocks Based on Quantum Emitters,"In this report, we discuss the new feature of optical clocks based on quantum emitters, including clock operating above threshold, which is a laser essentially used as an active optical clock, and clock operating under threshold. Comparing with the active microwave clock, hydrogen maser, excellent performance of optical clock based on quantum emitter is expected. Comparison between configurations of atomic beam and lattice trapped atoms is analyzed. Moreover, we also present the discussion on the possibility of Ramsey laser, a laser operating at the configuration of Ramsey separated oscillating fields","Stimulated emission,
Clocks,
Atom optics,
Ultrafast optics,
Optical pumping,
Atomic beams,
Laser excitation,
Masers,
Pump lasers,
Doppler effect"
A Hierarchical Global Path Planning Approach for AUV Based on Genetic Algorithm,"Global path planning is one of the key technologies of AUV (autonomous underwater vehicle), and it represents the intelligence level of AUV in some ways. This paper proposes a hierarchical global path planning approach for autonomous underwater vehicle in ocean. This approach consists of successive decomposition of the robot's workspace and searching for a path at each level of decomposition. To get a global shortest path, the genetic algorithm is used in the algorithm for its efficient on optimization. The approach proves to be flexible and practicable, and can reduce memory consumption significantly",
Off-Line Signature Recognition and Verification by Kernel Principal Component Self-Regression,"Automatic signature verification is an active area of research with numerous applications such as bank check verification, ATM access, etc. In this research, a kernel principal component self-regression (KPCSR) model is proposed for offline signature verification and recognition problems. Developed from the kernel principal component regression (KPCR), the self-regression model selects a subset of the principal components from the kernel space for the input variables to accurately characterize each user's signature, thus offering good verification and recognition performance. The model directly works on bitmap images in the preliminary experiments, showing satisfactory performance. A modular scheme with subject-specific KPCSR structure proves very efficient, from which each user is assigned an independent KPCSR model for coding the corresponding visual information. Experimental results obtained on public benchmarking signature databases demonstrate the superiority of the proposed method","Kernel,
Handwriting recognition,
Forgery,
Feature extraction,
Support vector machines,
Image databases,
Support vector machine classification,
Principal component analysis,
Computer science,
Mathematics"
Proximity based access control in smart-emergency departments,"In this paper, we propose a proximity based automated access control (PBAC) model for smart-ED environments which improves the existing ED work-flow by automating mundane administrative procedure for secure information access, allowing caregivers to focus on the treatment of patients. The proposed access model builds on the traditional role-based access control model and extends it by including multiple authentication levels for preventing unauthorized access. We also provide a semi-formal specification for the PBAC model. We further validate it using ultra-wide-band (UWB) based prototype which was tested in ED","Access control,
Authentication,
Information systems,
Medical treatment,
Hospitals,
Computer science,
Ultra wideband technology,
Prototypes,
Testing,
Information retrieval"
Data-Centric Routing in Sensor Networks using Biased Walk,"We present spiral, a data-centric routing algorithm for short-term communication in unstructured sensor networks. Conventional data-centric routing algorithms are based on flooding or random walk. Flooding returns the shortest route but has a high search cost; random walk has a lower search cost but returns a sub-optimal route. Spiral offers a compromise between these two extremes - it has a lower search cost than flooding and returns better routes than random walk. Spiral is a biased walk that visits nodes near the source before more distant nodes. This results in a spiral-like search path that is not only more likely to find a closer copy of the desired data than random walk, but is also able to compute a shorter route because the network around the source is more thoroughly explored. Our experiments show that in a 500-node network with an average degree of 20 and two copies of every data object, for a short-term communication of 40 packets the total communication cost by spiral is only 72% of that by flooding, 81% of ERS, 74% of random walk, and 73% of DFS","Routing,
Spirals,
Costs,
Floods,
USA Councils,
Energy consumption,
Computer science,
Peer to peer computing,
Wireless sensor networks,
Communications Society"
Power-Constrained SOC Test Schedules through Utilization of Functional Buses,"In this paper, we are proposing a core-based test methodology that utilizes the functional bus for test stimuli and response transportation. An efficient algorithm for the generation of a complete test schedule that efficiently utilizes the functional bus under a power constraint is described. The test schedule is composed of a set of test vector delivery sequences in small chunks, denoted as packets. The utilization of small packet sizes optimizes the functional bus utilization. The experimental results show that the methodology is highly effective compared to previous approaches that do not use the functional bus. The strong results of the proposed approach are particularly highlighted when small bus widths are considered, an important consideration in current SOC designs where increasingly larger bus widths pose routing and reliability challenges.","Circuit testing,
Automatic testing,
Processor scheduling,
Performance evaluation,
Communication networks,
Software testing,
Built-in self-test,
Information science,
Cities and towns,
Computer science"
Fuzzy-Rough Set Aided Sentence Extraction Summarization,"In this paper, a novel method is proposed to extract key sentences of a document as its summary by estimating the relevance of sentences through the use of fuzzy-rough sets. This method uses senses rather than raw words to lessen the problem that sentences of the same or similar semantic meaning but written in synonyms are treated differently. Also included is semantic clustering, used to avoid selecting redundant key sentences. A prototype of this automatic text summarization scheme is constructed and an intrinsic method with criteria widely used in information-retrieval systems is employed for measuring the summary quality. The results of applying the prototype to datasets with manually-generated summaries are shown",
Genetic Algorithm Based Optimal Block Mapping Method for LSB Substitution,"The least-significant-bit (LSB) substitution which is commonly used in steganography would degrade the quality of host image significantly when embedding high-capacity information. To overcome this drawback, we present an optimal block mapping LSB method based on genetic algorithm, which is out of researchers¿ concern but fundamentally important. Additionally we discuss the rule to select the best block size. The main idea is to minimize the degradation of the stego image by finding a best mapping function between host and secret image blocks at global scope. The experimental result is promising. The advantage of our method over most commonly used LSB strategy is obvious in enhancing PSNR value while having a secluded statistic peculiarity.",
Impact of Node Density and Sensing Range on Intrusion Detection in Wireless Sensor Networks,"Wireless sensor networks present a feasible and economic solution to some of the most challenging problems such as intrusion detection. In this paper, we establish relationship between node density, sensing range, and the possible intrusion distance before the intruder is detected by any of the sensors. We also extend our model to the multi-sensor joint detection case where an event can only be detected by k (k > 1) sensors simultaneously. Furthermore, we consider the effect of node heterogeneity on the intrusion detection in wireless sensor network, where two types of sensors with different sensing ranges are deployed. All the analytical results are validated via simulations. Our analysis can provide very useful insights in choosing network design parameters of sensor networks so that specified performance requirements can be met, thereby enhancing the acceptance of sensor networks.","Intrusion detection,
Wireless sensor networks,
Intelligent sensors,
Computer networks,
Distributed computing,
Event detection,
Information security,
Monitoring,
Mobile computing,
Computer science"
Indoor Mapping and Positioning Using Impulse Radios,,
An Investigation of Bad Smells in Object-Oriented Design,"Bad smells are used to identify problematic classes in object-oriented design. Although intuitively making sense, the promise that bad smells can indicate the quality of design has not been validated by empirical evidence. This paper presents the results from an investigation that explored the relationship between the bad smells and the errors in an object-oriented system. The investigation found that some bad smells are positively associated with class errors","Surgery,
Computer errors,
Software engineering,
Computer architecture,
Design engineering,
Computer science,
Process design,
Java,
Statistical analysis,
Information technology"
2DOF Control-Based Fast and Precise Positioning Using Disturbance Observer,"The paper presents a practical compensator design approach for the fast-response and high-precision positioning using a two degrees-of-freedom (2DOF) controller. In the controller design for mechatronic positioning devices, effects of resonant vibrations and nonlinear friction on the positioning performance are especially paid attention. A feedforward control based on a coprime factorization description successfully suppresses the mechanical vibrations and provides the fast response. A feedback control with a disturbance observer, on the other hand, allows the plant system to behave a nominal one with robust stability, and compensates for effects of nonlinear friction on the positioning performance. The disturbance observer is especially designed in order to improve the servo characteristic in positioning, where an initial value compensation based on a rolling friction model is adopted at the starting motion. The proposed positioning controller has been verified by experiments using a prototype which simulates the vibratory mechanism with the nonlinear friction and the dead time components","Friction,
Servomechanisms,
Mechatronics,
Vibration control,
Servomotors,
Fasteners,
Prototypes,
Resonance,
Virtual prototyping,
Digital signal processing"
Scalable Grid Application Scheduling via Decoupled Resource Selection and Scheduling,"Over the past years grid infrastructures have been deployed at larger and larger scales, with envisioned deployments incorporating tens of thousands of resources. Therefore, application scheduling algorithms can become unscalable (albeit polynomial) and thus unusable in large-scale environments. One reason for unscalability is that these algorithms perform implicit resource selection. One can achieve better scalability by performing explicit resource selection independently from scheduling in a ""decoupled' approach. Furthermore, we hypothesize that one can achieve similar or even better performance as with the non-decoupled approach, which we call the ""one step"" approach, by selecting resources judiciously. Leveraging the Virtual Grid abstraction, we demonstrate that the decoupled approach is indeed both scalable and effective in large-scale and highly heterogeneous resource environments.",
Communication-Aware Job Placement Policies for the KOALA Grid Scheduler,"In multicluster systems, and more generally, in grids, parallel applications may require co-allocation, i.e., the simultaneous allocation of resources such as processors in multiple clusters. Although co-allocation enables the allocation of more processors than available on a single cluster, depending on the applications' communication characteristics, it has the potential disadvantage of increased execution times due to relatively slow wide-area communication. In this paper, we present two job placement policies, the Cluster Minimization and the Flexible Cluster Minimization policies which take into account the wide-area communication overhead when co-allocating applications across the clusters. We have implemented these policies in our grid scheduler called KOALA in order to serve different job request types. To assess the performance of the policies, we perform experiments in a real multicluster testbed using communication-intensive parallel applications.",
Acquiring High-Rate Neural Spike Data with Hardware-Constrained Embedded Sensors,"In an effort to enable embedded sensors that are hardware and bandwidth constrained to acquire high- frequency neural signals, signal-filtering and signal- compression algorithms have been implemented and tested on a commercial-off-the-shelf embedded-system platform. The sensor modules have been programmed to acquire, filter, and transmit raw biological signals at a rate of 32 kbps. Furthermore, on-board signal processing enables one channel sampled at a rate of 4 kS/s at 12-bit resolution to be compressed via ADPCM and transmitted in real time. In addition, the sensors can be configured to only transmit individual time-referenced ""spike"" waveforms, or only the spike parameters for alleviating network traffic and increasing battery life",
A Simple and Efficient Solution to Remote User Authentication Using Smart Cards,"The password based authentication schemes are commonly used for authenticating remote users. Many password based schemes both with and without smart card have been proposed; each scheme has its merits and demerits. In this paper we propose an efficient scheme for remote user authentication which does not maintain verifier table and allows the user to freely choose and change their passwords. Also the proposed scheme achieves mutual authentication which is essential for many applications. The scheme is secure against id theft, guessing attack, insider attack, stolen verifier attack, replay attack, impersonation attack, and reflection attack",
Internet Time-delay Prediction Based on Autoregressive and Neural Network Model,"Internet time-delay prediction plays a key role in improving dynamic performances for many real-time applications, especially for Internet-based control systems. Therefore, ideal prediction approach for Internet time-delay must be investigated emphatically. In this paper, after analyzing Internet communication features, the autoregressive model and adaptive linear neural network model are adopted to predict the uncertain Internet time-delay. The simulation results for time-delay prediction illustrate that the autoregressive model and the adaptive linear neural network provide two kind of promising ways to predict the Internet time-delay. The resulting statistics of simulation experiments also illustrates that the adaptive linear neural network has a better prediction performance than autoregressive model","IP networks,
Neural networks,
Predictive models,
Internet,
Communication system control,
Control systems,
Adaptive systems,
Delay,
Bandwidth,
Random processes"
"Rural Microfinance Service Delivery: Gaps, Inefficiencies and Emerging Solutions","Microfinance, the provision of financial services to poor and under-served communities, has emerged as one of the most promising avenues for stimulating rural economic development through local enterprise. In this paper we will discuss some of the major technology gaps faced by rural microfinance institutions, focusing on areas that are most important for the future growth of the industry. This work builds upon six months of field research, including field studies with eight different microfinance organizations located across Latin America and Asia, and discussions with many other organizations worldwide. Historically it has proved difficult to provide sustainable micro-financial services to remote rural clients. As formal financial institutions begin to look seriously at this market, the microfinance industry faces significant challenges in maturing and scaling to sustainability. We will look at three of the major tasks faced by rural microfinance service providers today - 1) the exchange of information with remote clients, 2) management and processing of data at the institutional level and 3) the collection and delivery of money to remote rural areas. Each of these has been a difficult problem to solve for microfinance institutions worldwide, and may offer opportunities for information technology-based solutions. For each of these ""gaps"" we will look at current best practices, examine the role information technology has (or has not) played in overcoming these obstacles, and discuss promising future directions. In this context, we will discuss the use of handheld technologies for rural data collection, experiences in the implementation of MIS systems at the institutional level and current strategies for introducing electronic banking to remote rural areas. For each of these, we will look at the results obtained thus far and the potential ramifications for the long-term growth and sustainability of the sector",
Microarchitecture Parameter Selection To Optimize System Performance Under Process Variation,"Design variability due to within-die and die-to-die process variations has the potential to significantly reduce the maximum operating frequency and the effective yield of high-performance microprocessors in future process technology generations. This variability manifests itself by increasing the number and criticality of long delay paths. To quantify this impact, we use an architectural process variation model that is appropriate for the analysis of system performance in the early-stages of the design process. We propose a method of selecting microarchitectural parameters to mitigate the frequency impact due to process variability for distinct structures, while minimizing IPC (instructions-per-cycle) loss. We propose an optimization procedure to be used for system-level design decisions, and we find that joint architecture and statistical timing analysis can be more advantageous than pure circuit level optimization. Overall, the technique can improve the 90% yield frequency by about 14% with 3% IPC loss for a baseline machine with a 20FO4 logic depth per pipestage. This approach is sensitive to the selection of processor pipeline depth, and we demonstrate that machines with aggressive pipelines will experience greater challenges in coping with process variability","Microarchitecture,
System performance,
Frequency,
Design optimization,
Pipelines,
Microprocessors,
Delay,
Performance analysis,
Process design,
System-level design"
Fast barrier synchronization for InfiniBand/spl trade/,The MPI_Barrier() call can be crucial for several applications and has been target of different optimizations since several decades. The best solution to the barrier problem scales with O(log2N) and uses the dissemination principle. A new method using an enhanced dissemination principle and inherent network parallelism is demonstrated in this paper. The new approach was able to speedup the barrier performance by 40% in relation to the best published algorithm. It is shown that it is possible to leverage the inherent hardware parallelism inside the InfiniBandtrade network to lower the latency of the MPI-Barrier() operation without additional costs. The principle of sending multiple messages in (pseudo-) parallel can be implemented into a well known algorithm to decrease the number of rounds and speed the overall operation up,"Hardware,
Delay,
Clustering algorithms,
Parallel processing,
Chemical technology,
Computer science,
Application software,
Costs,
Bandwidth,
Counting circuits"
A Probabilistic Ensemble Pruning Algorithm,"An ensemble is a group of learners that work together as a committee to solve a problem. However, the existing ensemble training algorithms sometimes generate unnecessary large ensembles, which consume extra computational resource and may degrade the performance. Ensemble pruning algorithm aims to find a good subset of ensemble members to constitute a small ensemble, which saves the computational resource and performs as well as, or better than, the non-pruned ensemble. This paper introduces a probabilistic ensemble pruning algorithm by choosing a set of ""sparse"" combination weights, most of which are zero, to prune the large ensemble. In order to obtain the set of sparse combination weights and satisfy the non-negative restriction of the combination weights, a left-truncated, non-negative, Gaussian prior is adopted over every combination weight. Expectation-maximization algorithm is employed to obtain maximum a posterior (MAP) estimation of weight vector. Four benchmark regression problems and another four benchmark classification problems have been employed to demonstrate the effectiveness of the method","Machine learning,
Degradation,
Bayesian methods,
Gaussian distribution,
Computer science,
Expectation-maximization algorithms,
Bagging,
Boosting,
Evolutionary computation,
Machine learning algorithms"
A Differential Approach to Analog Viterbi Decoding,"In this paper we present a differential approach to analog current-mode Viterbi decoding. Such an approach is suited for current and future communication systems which take advantage of analog processing together with conventional digital logic. With the increased data rates, the elimination of the high-speed and power consuming A/D-converter has made an analog Viterbi decoder a promising alternative compared with its digital counterparts. The novel differential structure enables path memory management to be excluded from the implementation. In the proposed design decoding is performed after seven consecutive stages, which results in a high speed and savings in power consumption and silicon area. An example decoder was implemented for a (2,1,7) convolutional code having coding rate of 1/2 and constraint length of 7.","Viterbi algorithm,
Decoding,
Convolutional codes,
Memory management,
Energy consumption,
Silicon,
Modems,
Analog-digital conversion,
Quantization,
CMOS technology"
A Low Power 16-bit RISC with Lossless Compression Accelerator for Body Sensor Network System,A low power 16-bit RISC is proposed for body sensor network system. The proposed IPEEP scheme provides zero overhead for the wakeup operation. The lossless compression accelerator is embedded in the RISC to support the low energy data compression. The accelerator consists of 16times16-bit storage array which has vertical and horizontal access path. By using the accelerator the energy consumption of the lossless compression operation is reduced by 93.8%. The RISC is implemented by 1-poly 6-metal 0.18 um CMOS technology with 16 k gates. It operates at 4 MHz and consumes 24.2 uW at 0.6 V supply voltage.,"Reduced instruction set computing,
Body sensor networks,
Base stations,
Energy consumption,
Registers,
Data compression,
Sensor systems,
CMOS technology,
Compression algorithms,
Wireless sensor networks"
Model-based determination of QT intervals,"A method is presented to determine the QT interval by fitting a nonlinear artificial ECG model to segmented regions of a human ECG. The model consists of a set of temporally Gaussian functions with different widths and heights. These parameters are fitted to a given ECG (segmented around the QRS complex to include the P and T wave) using a nonlinear least squares optimization routine. The Q onset and T offset can be determined precisely (in a statistical sense) from the parameters of the Gaussian. Since the fitted waveform contains no noise, the differential is smooth. Waveform boundaries can also be determined by searching for the minimum of a differential. Furthermore, the residual error provides an estimate of the confidence in the fit, and hence, the derived QT interval. Using the human expert-annotated PhysioNet QT database, various QT interval estimation schemes were compared using the model-fitted ECG to find an optimal marker of the QT interval. It was found that humans are inconsistent and almost always under-estimate the T-offset (if defined to be the end of any repolarization). This is probably due to the truncation of any human estimation when the T wave tail is consumed by noise. We therefore propose an alternative QT end point. Finally, an entry based on the most favourable technique was submitted in the PhysioNet / Computers in Cardiology Challenge 2006; QT Interval Measurement, which is intended to produce a comparison of several automatic and human annotators on the Physikalisch-Technische Bundesanstalt diagnostic ECG database. A follow-up paper to address differences between those generated by our method and the consensus of the other entries will be submitted shortly.",
Semantic Multimedia Retrieval using Lexical Query Expansion and Model-Based Reranking,"We present methods for improving text search retrieval of visual multimedia content by applying a set of visual models of semantic concepts from a lexicon of concepts deemed relevant for the collection. Text search is performed via queries of words or fully qualified sentences, and results are returned in the form of ranked video clips. Our approach involves a query expansion stage, in which query terms are compared to the visual concepts for which we independently build classifier models. We leverage a synonym dictionary and WordNet similarities during expansion. Results over each query are aggregated across the expanded terms and ranked. We validate our approach on the TRECVID 2005 broadcast news data with 39 concepts specifically designed for this genre of video. We observe that concept models improve search results by nearly 50% after model-based re-ranking of text-only search. We also observe that purely model-based retrieval significantly outperforms text-based retrieval on non-named entity queries",
Distributed Energy-Efficient Scheduling Approach for K-Coverage in Wireless Sensor Networks,"In sensor networks, it is desired to conserve energy so that the network lifetime can be maximized. An efficient approach to prolong the network lifetime is to identify a schedule for all the sensors, indicating which subset of the sensors can be active during the current time slot. Furthermore, to ensure the quality of surveillance, some applications require k-coverage of the monitored area. In this paper, we first define the sensor energy-efficient scheduling for k-coverage (SESK) problem. We further resolve it by proposing a scheduling approach named distributed energy-efficient scheduling for k-coverage DESK such that the energy consumption among all the sensors is balanced while still assuring the k-coverage requirement. This approach is completely distributed as well as localized, which is more practical than centralized ones in wireless sensor networks","Energy efficiency,
Wireless sensor networks,
Fault tolerance,
Temperature sensors,
Chemical sensors,
Processor scheduling,
Surveillance,
Energy consumption,
Sensor systems,
Condition monitoring"
"Multilevel models in model-driven engineering, product lines, and metaprogramming","Model-driven engineering (MDE) aims to raise the level of abstraction in program specification and increase automation in program development. These are also the goals of product lines (a family of related programs) and metaprogramming (programming as computation). We show that the confluence of MDE, product lines, and metaprogramming exposes a multilevel paradigm of program development, and further, we can use object-oriented design techniques to represent programs, the metaprograms that produced these programs, and the meta-metaprograms that produced these metaprograms, recursively. The paradigm is based on a small number of simple and well-known ideas, scales to the synthesis of applications of substantial size, and helps clarify concepts of MDE.",
Call Stack Coverage for GUI Test-Suite Reduction,"Graphical user interfaces (GUIs) are used as front-ends to most of today's software applications; testing GUIs for functional correctness is needed to ensure the overall correctness of these applications. The event-driven nature of GUIs presents new challenges for testing. One important challenge is test suite reduction. Conventional reduction techniques/tools based on static analysis are not easily applicable due to the increased use of multi-language GUI implementations, callbacks for event handlers, virtual function calls, reflection, and multi-threading. Moreover, many existing techniques ignore event handlers from libraries, and fail to consider the context in which a handler executes. Consequently, they yield GUI test suites with seriously impaired fault-detection ability. This paper presents a new reduction technique based on the call stack coverage criterion. Call stacks may be collected for any executing program with very little overhead. An empirical study involving three large GUI-based applications shows that call stack based reduction provides an excellent tradeoff between reduction in test suite size and loss of fault-detection effectiveness",
Distinguishing Lesions from Posterior Acoustic Shadowing in Breast Ultrasound via Non-Linear Dimensionality Reduction,"Breast ultrasound (US) in conjunction with digital mammography has come to be regarded as the gold standard for breast cancer diagnosis. While breast US has certain advantages over digital mammography, it suffers from image artifacts such as posterior acoustic shadowing (PAS), presence of which often obfuscates lesion margins. Since classification of lesions as either malignant or benign is largely dictated by lesion's shape and margin characteristics, it is important to distinguish lesion area from PAS. This paper represents the first attempt to extract and identify those image features that can help distinguish between lesion and PAS. Our methodology comprises of extracting over 100 statistical, gradient, and Gabor features at multiple scales and orientations at every pixel in the breast US image. Adaboost, a powerful feature ensemble technique is used to discriminate between lesions and PAS by combining the different image features. A non-linear dimensionality reduction method called Graph Embedding is then used to visualize separation and inter-class dependencies between lesions and PAS in a lower dimensional space. Results of quantitative evaluation on a database of 45 breast US images indicate that our methodology allows for greater discriminability between the lesion and PAS classes compared to that achievable by any individual image texture or intensity feature","Lesions,
Nonlinear acoustics,
Shadow mapping,
Ultrasonic imaging,
Mammography,
Gold,
Breast cancer,
Shape,
Pixel,
Visualization"
Fully Automated Software for Polar-Map Registration and Sampling from PET Images,"Left ventricle myocardial activity (LV) from volumetric images is commonly presented in a 2D format called a polar map. Few applications offer complete automation of the registration and sampling process, which may lead to biasing of the results by the user and increased variability. A fully automated process is described which includes two steps: reorientation of the data to a common LV reference frame, and registration of the LV using a deformable spline model. The software implementation of this algorithm consists of a graphical user interface (GUI) that enables the user to verify the process, and to intervene in cases where automation may fail. 135 PET images (consisting of several tracers, in rest and stress, and in humans and dogs) were processed and visually validated. All but one case (99.3%) were successfully handled by the automated process.","Image sampling,
Positron emission tomography,
Automation,
Graphical user interfaces,
Myocardium,
Application software,
Spline,
Deformable models,
Software algorithms,
Stress"
Practical Byzantine Group Communication,"This paper presents an adaptation of the JazzEnsemble group communication system that enables it to tolerate Byzantine failures. The work emphasizes scalability and good performance in the normal case, i.e., when there are no failures, while providing strong semantics to the application. The paper presents the main concepts and protocols that enable the Byzantine tolerant version of JazzEnsemble to obtain these goals. In particular, this includes fuzzy mute and fuzzy verbose failure detectors, an efficient Byzantine vector consensus protocol, and a novel Byzantine uniform broadcast protocol, as well as modifications at each layer of the system to overcome potential Byzantine attacks. Additionally, high-level protocols only rely on the oral messages model, and thus messages need to be signed only once at a low level of the system. Finally, the paper presents an extensive performance evaluation, which demonstrates the system’s scalability and efficiency, and is used to analyze the sources of performance degradation associated with various aspects of overcoming Byzantine failures.",
An Edge Preserving Locally Adaptive Anti-aliasing Zooming Algorithm with Diffused Interpolation,"In this paper the problem of producing an enlarged image from a given digital image is addressed (zooming). Different image interpolation techniques are used for image enlargement. During interpolation, preserving details and smoothing data at the same time for not introducing spurious artifacts (i.e. Aliasing) is difficult. A complete and a definitive solution to this problem is still an open issue. Although there are some well known methods in the market Parket [14], Sakamote [16], the paper proposes a method that considers discontinuities and luminance variations in a sequence of non linear iterations steps. All the pixels present near the edges are diffused into the edge in a way that aliasing is reduced to a greater extent. Hence the proposed method is completed in limited computational resources. The proposed method preserves edges and brings smoothness and at the same time controls the aliasing effect.",
Moving Object Localization in Thermal Imagery by Forward-backward MHI,"Detecting moving objects automatically is a key component of an automatic visual surveillance and tracking system. In airborne thermal video, the moving objects may be small, color information is not available, and even intensity appearance may be camouflaged. Previous motionbased moving object detection approaches often use background subtraction, inter-frame difference or three-frame difference. In this paper, we describe a detection and localization method based on forward-backward motion history images (MHI). This method can accurately detect location and shape of moving objects for initializing a tracker. Using long and varied video sequences, we quantify the effectiveness of this method.",
Genetic algorithm based independent component analysis to separate noise from Electrocardiogram signals,"A technique is proposed to reduce additive noise from biomedical signals that have high kurtosis values using genetic algorithm (GA). The technique is applied to reduce multiple linear additive noises from electrocardiogram (ECG) signals, which have high kurtosis values due to the presence of R peaks. This GA method uses the basic principles of independent component analysis (ICA) and could also be used to reduce additive noise from other signals that have high kurtosis values. The method is simpler compared to neural learning algorithms and does not require any prior statistical knowledge of the signals. An additional advantage of the method compared to other ICA methods is that only the ECG signal will be extracted thus avoiding extraction of all independent components and manual inspection to determine the ECG signal","Genetic algorithms,
Independent component analysis,
Electrocardiography,
Additive noise,
Noise reduction,
Signal processing,
Signal analysis,
Algorithm design and analysis,
Biomedical computing,
Rhythm"
Nanowire Addressing with Randomized-Contact Decoders,"Methods for assembling crossbars from nanowires (NWs) have been designed and implemented. Methods for controlling individual NWs within a crossbar have also been proposed, but implementation remains a challenge. A NW decoder is a device that controls many NWs with a much smaller number of lithographically produced mesoscale wires (MWs). Unlike traditional demultiplexers, all proposed NW decoders are assembled stochastically. In a randomized-contact decoder (RCD) (Hogg et al., 2006), for example, field-effect transistors are randomly created at about half of the NW/MW junctions. In this paper, we tightly bound the number of MWs required to produce a correctly functioning RCD with high probability. We show that the number of MWs is logarithmic in the number of NWs, even when errors occur. We also analyze the overhead associated with controlling a stochastically assembled decoder. As we explain, lithographically-produced control circuitry must store information regarding which MWs control which NWs. This requires more area than the MWs themselves, but has received little attention elsewhere","Decoding,
Assembly,
Programmable logic arrays,
Wires,
Circuits,
Logic arrays,
Diodes,
Permission,
Ohmic contacts,
Voltage"
Combining WS-Policy and Aspect-Oriented Programming,"Web Service technologies offer a successful way for interoperability among applications. However, although current approaches are beginning to address how to specify non-functional capabilities in Web Services at description level, they have so far failed to propose an acceptable method to decouple the named capabilities from Web Service implementations, resulting in a large amount of code scattered and tangled all over the application. Considering it is our desire to minimize middleware participation in nonfunctional properties management, it is the aim of this paper to describe how aspect-oriented techniques can be used in conjunction with WS-Policy in order to allow the mentioned properties to be completely decoupled at description and implementation level, thus improving their reusability and service maintenance and evolution.","Web services,
Application software,
Middleware,
Software engineering,
Computer science,
Computer architecture,
Service oriented architecture,
Scattering,
Cryptography,
Encapsulation"
QoS-Oriented Load Balancing for WLANs,"The load balancing (LB) is essential for improving the WLAN efficiency, especially for VoIP over WLAN. Although 802.11v forms an important step to standardize LB in WLAN, nevertheless the concept, algorithms, and system model of LB are still required. In that respect, this paper proposes a novel concept for LB according to the graph representation of WLAN environment. In relation with WLAN features, a load distribution graph is constructed as the basis of the presented LB concept and algorithms in local scope. Accordingly, a QoS-oriented LB method and a system model are developed. Based on that, a system prototype, called Lobaq, is implemented according to the centralized architecture. The experimental results present the major functionalities of system behavior and provide an improvement of network balance in relation with the test scenario from lower than 37% to more than 73%. Moreover, these results confirm the LB according to the QoS requirements","Load management,
Wireless LAN,
Quality of service,
Load modeling,
Land mobile radio cellular systems,
Joining processes,
Routing,
Protocols,
Standards organizations,
Admission control"
Gridding spot centers of smoothly distorted microarray images,"We use an optimization technique to accurately locate a distorted grid structure in a microarray image. By assuming that spot centers deviate smoothly from a checkerboard grid structure, we show that the process of gridding spot centers can be formulated as a constrained optimization problem. The constraint is equal to the variations of the transform parameter. We demonstrate the accuracy of our algorithm on two sets of microarray images. One set consists of some images from the Stanford Microarray Database; we compare our centers with those annotated in the Database. The other set consists of oligonucleotide images, and we compare our results with those obtained by GenePix Pro 5.0. Our experiments were performed completely automatically.","Gene expression,
Image databases,
Probes,
Genomics,
Bioinformatics,
Fluorescence,
Image analysis,
Information science,
Constraint optimization"
Rough Sets on Intuitionistic Fuzzy Approximation Spaces,The notion of intuitionistic fuzzy approximation space is introduced. Rough sets on such spaces are defined and some of their properties are studied,
"Truels, or Survival of the Weakest","A duel is simply a game with two players, each having a probability of winning that game and each having an intrinsic marksmanship, or ability, associated with his or her performance. The mathematical treatment of this game confirms our simple expectations. If more than two players participate, a series of duels might be needed to determine the absolute winner. This is the case in sports tournaments, which pair teams in all possible ways until the ultimate winner emerges. A truel is a generalization of a duel involving three players and slightly different rules",Thermodynamics
Image Complexity and Feature Extraction for Steganalysis of LSB Matching Steganography,"In this paper, we present a scheme for steganalysis of LSB matching steganography based on feature extraction and pattern recognition techniques. Shape parameter of generalized Gaussian distribution (GGD) in the wavelet domain is introduced to measure image complexity. Several statistical pattern recognition algorithms are applied to train and classify the feature sets. Comparison of our method and others indicates our method is highly competitive. It is highly efficient for color image steganalysis. It is also efficient for grayscale steganalysis in the low image complexity domain","Feature extraction,
Steganography,
Pattern recognition,
Shape measurement,
Wavelet domain,
Pattern matching,
Gaussian distribution,
Gray-scale,
Maximum likelihood estimation,
Computer science"
"Lower Bounds for Additive Spanners, Emulators, and More","An additive spanner of an unweighted undirected graph G with distortion d is a subgraph H such that for any two vertices u,v isin G, we have deltaH(u, v) les deltaG(u, v) + d. For every k = O((ln n)/( ln ln n)), we construct a graph G on vertices for which any additive spanner of G with distortion 2k - 1 has Omega((1/k)n1 + 1k/) edges. This matches the lower bound previously known only to hold under a 1963 conjecture of Erdos. We generalize our lower bound in a number of ways. First, we consider graph emulators introduced by Dor, Halperin, and Zwick (FOCS, 1996), where an emulator of an unweighted undirected graph G with distortion d is like an additive spanner except H may be an arbitrary weighted graph such that deltaG(u, v) ges deltaG(u, v) ges deltaG(u, v) + d. We show a lower bound of Omega((1/k2)n1 + 1k/) edges for distortion- (2k - 1) emulators. These are the first non-trivial bounds for k > 3. Second, we parameterize our bounds in terms of the minimum degree of the graph. Namely, for minimum degree n1k + c/ for any c ges 0, we prove a bound of Omega((1/k)n1 + 1k - c(1 + 2/(k - 1))/) for additive spanners and Omega((1/k2)n1 + 1k - c(1 + 2/(k - 1))/) for emulators. For k = 2 these can be improved to OmegaQ(n(3/2) - c). This partially answers a question of Baswana et al. (SODA, 2005) for additive spanners. Finally, we continue the study of pair-wise and source-wise distance preservers defined by Coppersmith and Elkin (SODA, 2005) by considering their approximate variants and their relaxation to emulators. We prove the first lower bounds for such graphs",
Optimization of Electrode Contour for Improvement of Insulation Performance of High Voltage Vacuum Circuit Breaker,"For the improvement of the electrical insulation performance of the vacuum circuit breakers (VCB) and vacuum interrupters (VI), the authors develop the computer-aided optimization technique based on the electric field calculation. For this background, the authors developed a computer-aided optimization tool for VCB and VI electrical insulation design. Using this tool, in this paper, the authors evaluated the several design variables necessary for optimizing the electrode contour of main contactor and center shield in VI. The influences of the various factors like area effect of breakdown, arrangement of VIs, etc. to the optimization result were investigated. The optimization results revealed that the electric field distribution in VI was greatly influenced by floating potential of the center shielding electrode. Finally it is concluded that the arrangement of VI in the VCB grounded wall should be carefully considered in the optimization and design procedure",
A Method for Dynamic Configuration of a Cognitive Radio,"Cognitive radios offer a broad range of opportunities for improving the use and utilization of radio frequency spectrum, and they also offer a host of exciting prospects in networking research. This includes the creation of radio networks that can reconfigure their operation based on application requirements, policy updates and environmental conditions. Such reconfiguration requires an understanding of cross-layer interactions within the network protocol stack. It also requires the development of algorithms to determine when such reconfigures should be made, and additionally, the potential impacts of these changes on the radio network. In this article, we describe how cognitive radios can be used to create dynamic wireless networks. Such networks can quickly adapt to the needs of users as well as to changes in the environment. We describe how parameters at the application, physical, data link and network layers interact and how desirable configurations of these parameters can be determined. We then describe a technique that uses these configurations in the creation of an adaptive model for a cognitive radio. ""The views expressed in this article are those of the author and do not reflect the official policy or position of the United States Air Force, Department of Defense, or the U.S. Government.""","Cognitive radio,
Radio network,
Radio frequency,
Protocols,
Wireless networks,
Predictive models,
US Department of Energy,
Computer science,
Government,
Software radio"
TOPCASED Combining Formal Methods with Model-Driven Engineering,"This paper briefly presents the TOPCASED project which gathers industrialists, researchers, universities and SMEs, aiming at producing a free/open-source system/software/hardware-engineering toolkit, implemented over the Eclipse platform, using only standard components. An important aspect of TOPCASED is that it enables researchers to plug in their tools easily. TOPCASED is meant to be used on actual industrial projects and may therefore be considered as an important target by researchers working on formal methods and foundations of software engineering for critical systems","Model driven engineering,
Aerospace industry,
Open source software,
Space technology,
Computer industry,
Educational institutions,
Software tools,
Software systems,
Software engineering,
Automotive engineering"
A Generic Mobility Model for Resource Prediction in Mobile Grids,"Grid Computing has emerged as an efficient problem solution paradigm since the last decade. Most of the research and implementation of grid computing environments collaborate and share heterogeneous resources in static manner. Scheduling is the most challenging and primitive issue that should be addressed effectively to achieve minimized resource utilization and maximum performance. This way reduced average execution time is achieved. To realize the above mentioned goal, many prediction models have been presented based on different parameters such as network bandwidth, computing capability and replica management. Recent years have seen a keen interest of research community and practitioners for the development and deployment of mobile grids. This paper aims to present a generic mobility model to extend the conventional prediction models. The model visualizes best resources and hence improves scheduler performance in mobile grid environments. A sample implementation mechanism and two possible scenarios have been demonstrated.",
LEARN: localized energy aware restricted neighborhood routing for ad hoc networks,"In this paper, we address the problem of energy efficient localized routing in wireless ad hoc networks. Numerous energy aware routing protocols were proposed to seek the power efficiency of routes. Among them, several geographical localized routing protocols were proposed to help making smarter routing decision using only local information and reduce the routing overhead. However, most of the proposed localized routing methods cannot theoretically guarantee the power efficiency of their routes. In this paper, we give the first localized routing algorithm, called localized energy aware restricted neighborhood routing (LEARN), which can guarantee the power efficiency of its route asymptotically almost sure. Given destination node t, an intermediate node v will only select a certain neighbor v such that < vut les alpha for a parameter alpha < pi/3 in our LEARN method. We theoretically prove that for a network, formed by nodes that are produced by a Poisson distribution with rate n over a compact and convex region O with unit area, when the transmission range rn = radicbetalnl/pin for some beta > pi/alpha, our LEARN routing protocol will find the route for any pair of nodes asymptotically almost sure. When the transmission range rn = radicbetalnl/pin for some beta < pi/alpha, the LEARN routing protocol will not be able to find the route for any pair of nodes asymptotically almost sure. We also conducted simulations to study the performance of LEARN and compare it with a typical localized routing protocol (GPSR) and a global ad hoc routing protocol (DSR)","Ad hoc networks,
Routing protocols,
Energy efficiency,
Mobile ad hoc networks,
Computer science,
USA Councils,
Peer to peer computing,
Scalability,
Communications Society,
Power engineering and energy"
A Distributed Information Services Architecture to Support Biomarker Discovery in Early Detection of Cancer,"Informatics in biomedicine is becoming increasingly interconnected via distributed information services, interdisciplinary correlation, and crossinstitutional collaboration. Partnering with NASA, the Early Detection Research Network (EDRN), a program managed by the National Cancer Institute, has been defining and building an informatics architecture to support the discovery of biomarkers in their earliest stages. The architecture established by EDRN serves as a blueprint for constructing a set of services focused on the capture, processing, management and distribution of information through the phases of biomarker discovery and validation.","Biomarkers,
Cancer detection,
Computer architecture,
Knowledge based systems,
Biomedical informatics,
Propulsion,
Laboratories,
Bioinformatics,
Portals,
Computer science"
Vehicle Tracking using Template Matching based on Feature Points,"We propose a new vehicle tracking system which can detect and monitor vehicles as they break traffic lane rules. Our proposed tracking scheme is based on characteristics of both traffic scene and vehicle. These include: background information, local position, and the size of a moving vehicle. The initial size and position of the vehicle are obtained using a 4-directional contour tracking method. The object contour is made less sensitive to luminosity changes by incorporating a frame differencing operation. Each vehicle can be described with four feature points. The template region is estimated by means of a minimum distance approach with respect to center position. Our experimental results confirm that extraction of feature points from each frame of the scene improves the efficiency of vehicle tracking systems. Furthermore, adjustment of the vehicle region with matched points of template resolves the previous problem of occlusion",
Identification of Overlapping Functional Modules in Protein Interaction Networks: Information Flow-based Approach,"Recent computational analyses of protein interaction networks have attempted to understand cellular organizations, processes and functions. Various topology-based clustering methods have been applied to the protein interaction networks. However, they have been in difficulties due to unreliable interaction data and the specific features of the networks such as small-world and scale-free properties. In this paper, we present an information flow-based approach for analyzing the weighted protein interaction networks, which are integrated with other biological knowledge. Our approach is designed to identify overlapping functional modules. The algorithm selects a small number of informative proteins based on the weighted connectivity, and simulates the information flow through the network from each informative protein. Our experimental results show that the modules generated by our algorithm correspond to real functional associations of proteins. Furthermore, we demonstrate that our approach outperforms other previous methods in terms of accuracy",
Silence Based Communication for Sensor Networks,"We consider a power-efficient communication model for wireless sensor networks where silence is used to convey information. We study the average-case and worst-case complexities of symmetric functions under this model and describe protocols that achieve them. For binary-input functions, we determine the average complexity. For ternary-input functions, we consider a special type of protocols and provide close lower and upper bounds for their worst-case complexity. We also describe the protocol that achieves the average complexity","Timing,
Protocols,
Context,
Monitoring,
Satellites,
Temperature measurement,
Temperature sensors,
Costs,
Computer networks,
Wireless sensor networks"
Embedded controllers for local board-control,"The LHCb experiment at CERN has a large number of custom electronics boards performing high-speed data-processing. As in any large experiment the control and monitoring of these crate-mounted boards must be integrated into the overall control-system. Traditionally this has been done by using buses on the back-plane of the crates, such as VME. LHCb has chosen to equip every board with an embedded micro-controller and connecting them in a large Local Area Network. The intelligence of these devices allows complex (soft) real-time control and monitoring, required for modern field programmable gate array (FPGA) driven electronics. Moreover, each board has its own, isolated control access path, which increases the robustness of the entire system. The system is now in pre-production at several sites and will go into full production during next year. The hardware and software will be discussed and experience from the R&D and pre-production will be reviewed, with an emphasis on the advantages and difficulties of this approach to board-control.","Hardware,
Monitoring,
Local area networks,
Jacobian matrices,
Field programmable gate arrays,
Microprocessors,
Computer architecture,
Joining processes,
Control systems,
Robust control"
A Stable On-Demand Routing Protocol for Mobile Ad Hoc Networks with Weight-Based Strategy,"A mobile ad hoc network (MANET) consists of a set of mobile hosts that can communicate with each other without the assistance of base stations. In MANETs, the high mobility of mobile nodes is a major reason for link failures. In this paper, we propose a stable weight-based on-demand routing protocol (SWORP) for MANETs. The proposed scheme uses the weight-based route strategy to select a stable route in order to enhance system performance. The weight of a route is decided by three factors: the route expiration time, the error count, and the hop count. Route discovery usually find multiple routes from the source node to the destination node. Then we select the path with the largest weight value for routing. Experimental results show that the proposed SWORP outperforms DSR and AODV especially in the high mobility environment","Routing protocols,
Mobile ad hoc networks,
Stability,
Computer science,
Base stations,
System performance,
Chaotic communication,
Broadcasting,
Network topology,
Concurrent computing"
An Efficient FPGA Implementation of Gaussian Mixture Models-Based Classifier Using Distributed Arithmetic,"Gaussian mixture models (GMM)-based classifiers have shown increased attention in many pattern recognition applications. Improved performances have been demonstrated in many applications but using such classifiers can require large storage and complex processing units due to exponential calculations and large number of coefficients involved. This poses a serious problem for portable real-time pattern recognition applications. In this paper, first the performance of GMM and its hardware complexity are analyzed and compared with a number of benchmark algorithms. Next, an efficient digital hardware implementation based on distributed arithmetic (DA) is proposed. A novel exponential calculation circuit based on linear piecewise approximation is also developed to reduce hardware complexity. Implementation is carried out on the Celoxica-RC1000 board equipped with the Virtex-E FPGA. Maximum optimization has been achieved by means of manual placement and routing in order to achieve a compact core footprint. A detailed evaluation of the performance metrics of the GMM core is also presented.","Field programmable gate arrays,
Hardware,
Pattern recognition,
Design engineering,
Signal processing algorithms,
Digital arithmetic,
Application software,
Costs,
Distributed computing,
Performance analysis"
Parallel Evolutionary Asymmetric Subsethood Product Fuzzy-Neural Inference System with Applications,"This paper introduces PEASuPFuNIS, a parallel evolutionary asymmetric subsethood product fuzzy neural network as an extension of ASuPFuNIS, which is implemented using a high performance LAM/MPI cluster. EASuPFuNlS employs differential evolution learning which is parallelized using a master-slave model, and the implementation is facilitated through the use of derived data-types. Parallelization of EASuPFuNIS using DE learning leads to super-linear speedups concomitant with high performance as is shown through instrumentation using two problems: the Hang function approximation problem, and the Mackey-Glass time series prediction problem. Parallelization and ran-time speedup of the EASuPFuNIS model opens up the possibility of applying this class of models to real world problem domains which was hitherto not possible with the serial version due to the requirement of large computation time.",
The Use of UML Sequence Diagram for System-on-Chip System Level Transaction-based Functional Verification,"An important problem faced by system-on-chip transaction-based verification is how to design the complex transaction test sequence. Scenario-based sequence diagram is a good way to capture the system level functional specification. In the paper, we propose a method to support transaction level verification of SoC based on UML sequence diagram. We use UML sequence diagram to capture the communication and collaboration behaviour among IP cores in system-on-chip and build high level specification for transaction level verification. Then these sequence diagrams will be used to guide the generation of transaction test sequence. We develop a component-based transaction verification environment named SoC-CBTVE. In the SoC-CBTVE, based on the method, we verify a typical SoC design. Experimental results show that UML sequence diagram can capture the complex communication behaviour among IP cores in SoC design, and efficiently supports SoC system level functional verification",
Feature Selection based on the Bhattacharyya Distance,"This paper presents a Bhattacharyya distance based feature selection method, which utilizes a recursive algorithm to obtain the optimal dimension reduction matrix in terms of the minimum upper bound of classification error under normal distribution for multi-class classification problem. In our scheme, PCA is incorporated as a pre-processing to reduce the intractably heavy computation burden of the recursive algorithm. The superior experimental results on the handwritten-digit recognition with the MNIST database and the steganalysis applications have demonstrated the effectiveness of our proposed method","Upper bound,
Error probability,
Gaussian distribution,
Principal component analysis,
Handwriting recognition,
Spatial databases,
Error analysis,
Scattering,
Computer science,
Distributed computing"
Trust Assessment Using Provenance in Service Oriented Applications,"Workflow forms a key part of many existing Service Oriented applications, involving the integration of services that may be made available at distributed sites. It is possible to distinguish between an ""abstract"" workflow description outlining which services must be involved in a workflow execution and a ""physical"" workflow description outlining the particular instances of services that were used in a particular enactment. Provenance information provides a useful way to capture the physical workflow description automatically especially if this information is captured in a standard format. Subsequent analysis on this provenance information may be used to evaluate whether the abstract workflow description has been adhered to, and to enable a user executing a workflow-based application to establish ""trust"" in the outcome.",
On Computing the Canonical Features of Software Systems,"Software applications typically have many features that vary in their similarity. We define a measurement of similarity between pairs of features based on their underlying implementations and use this measurement to compute a set of canonical features. The canonical features set (CFS) consists of a small number of features that are as dissimilar as possible to each other, yet are most representative of the features that are not in the CFS. The members of the CFS are distinguishing features and understanding their implementation provides the engineer with an overview of the system undergoing scrutiny. The members of the CFS can also be used as cluster centroids to partition the entire set of features. Partitioning the set of features can simplify the understanding of large and complex software systems. Additionally, when a specific feature must undergo maintenance, it is helpful to know which features are most closely related to it. We demonstrate the utility of our method through the analysis of the Jext, Firefox, and Gaim software systems",
Hardness of Learning Halfspaces with Noise,"Learning an unknown halfspace (also called a perceptron) from, labeled examples is one of the classic problems in machine learning. In the noise-free case, when a half-space consistent with all the training examples exists, the problem can be solved in polynomial time using linear programming. However, under the promise that a halfspace consistent with a fraction (1 - epsiv) of the examples exists (for some small constant epsiv > 0), it was not known how to efficiently find a halfspace that is correct on even 51% of the examples. Nor was a hardness result that ruled out getting agreement on more than 99.9% of the examples known. In this work, we close this gap in our understanding, and prove that even a tiny amount of worst-case noise makes the problem of learning halfspaces intractable in a strong sense. Specifically, for arbitrary epsiv,delta > 0, we prove that given a set of examples-label pairs from the hypercube a fraction (1 - epsiv) of which can be explained by a halfspace, it is NP-hard to find a halfspace that correctly labels a fraction (frac12 + delta) of the examples. The hardness result is tight since it is trivial to get agreement on frac12 the examples. In learning theory parlance, we prove that weak proper agnostic learning of halfspaces is hard. This settles a question that was raised by Blum et. al in their work on learning halfspaces in the presence of random classification noise (A. Blum et. al, 1996), and in some more recent works as well. Along the way, we also obtain a strong hardness for another basic computational problem: solving a linear system over the rationals",
Robust B+-Tree-Based Indexing of Moving Objects,"With the emergence of an infrastructure that enables the geo-positioning of on-line, mobile users, the management of so-called moving objects has emerged as an active area of research. Among the indexing techniques for efficiently answering predictive queries on moving-object positions, the recent Bx-tree is based on the B+-tree and is relatively easy to integrate into an existing DBMS. However, the Bx-tree is sensitive to data skew. This paper proposes a new query processing algorithm for the B^x-tree that fully exploits the available data statistics to reduce the query enlargement that is needed to guarantee perfect recall, thus significantly improving robustness. The new technique is empirically evaluated and compared with four other approaches and with the TPR-tree, a competitor that is based on the R*-tree. The results indicate that the new index is indeed more robust than its predecessor-it significantly reduces the number of I/O operations per query for the workloads considered. In many settings, the TPR-tree is outperformed as well.",
Tone Mapping for HDR Image using Optimization A New Closed Form Solution,"This paper studies an optimization approach for designing tone reproduction curve (TRC) based tone mapping operators for the display of high dynamic range (HDR) images in low dynamic range (LDR) reproduction media. Previous work has shown that the tone mapping problem can be formulated as that of optimizing a two-term cost function where adjusting the relative weightings of the two terms allows users to interactively control the appearance of the output image. However, only heuristic solutions to the tone mapping objective function have been found in past research. The main contribution of this paper is the re-formulation of the objective function to allow the introduction of a closed-form solution to the two-term tone mapping objective function. The new solution has simplified previous heuristic solutions and made this approach mathematically more elegant, computationally faster and practically easier to implement","Closed-form solution,
Dynamic range,
Layout,
Cameras,
Humans,
Computer displays,
Visual system,
Brightness,
Computer science,
Information technology"
Performance analysis of admission control for integrated services with minimum rate guarantees,"In this paper, we focus on an admission control strategy for streaming and elastic users that enforces a minimum rate guarantee for each elastic user through pre-emptive capacity reservation. We propose approximations to estimate the performance of this strategy. We apply time-scale decomposition for the limiting regimes, and for non-limiting regimes we propose a novel weighted approximation. Simulation results suggest that the performance is almost insensitive to traffic parameter distributions, and is well estimated by our proposed approximations. Our work is motivated by the integration of services in 3rd generation wireless systems such as UMTS and CDMA 2000","Performance analysis,
Admission control,
Intserv networks,
Streaming media,
Traffic control,
Videos,
Mathematics,
Computer science,
Telecommunication traffic,
3G mobile communication"
Max-Min Central Vein Detection in Retinal Fundus Images,"This paper describes a new framework for the automated tracking of the central retinal vein in retinal images. The procedure first computes a binary image of the retinal vasculature, then obtains the skeleton (medial axis) of the vascular network. Terminal and branching points of the network are then located, and the network converted into a graph representation including length and thickness information for all vessels. Finally, a maxmin approach is used to locate the central vein: the candidates central vein are the minimal paths from the optic disk to all terminal nodes found using Dijkstra algorithm. The actual central vein is selected among the all candidates by maximizing a merit function estimating the total vessel area in the image. Results are presented and compared with those provided by a manual classification on 20 images of the DRIVE set. An overall performance ratio of 92% is achieved.","Veins,
Retina,
Skeleton,
Optical filters,
Arteries,
Image databases,
Geometrical optics,
Physics computing,
Computer networks,
Image converters"
Structure-Based Ontology Evaluation,"In order to build high quality ontologies, ontology evaluation technologies are needed. Now, most evaluation methods focus on the syntax evaluation which guarantees the correctness and completeness of ontology. The primary goal of these evaluation methods is to prevent applications from using inconsistent or incorrect ontologies. During last few years, some studies pay attention to the common meaningful structures among ontologies, which are analogous to each other. However, fewer studies concentrate on the internal structure of the ontology. Ontology, as the representation of knowledge, should have similar structure with domain knowledge. Furthermore, the ontology with well-organized structure will make it easy to understand, learn, apply, and reuse. Therefore, based on the statistics and the graph theory, this paper puts forward an improved ontology evaluation method, in which six properties are proposed to describe the characteristics of ontology structure. After discussing the algorithm of these six properties, we introduce an ontology development tool, in which the evaluation method has been integrated. This tool supports the entire ontology development process, and can help ontology developers refine the structure of ontology, and easily make a decision that which aspect of the ontology should be revised or improved",
Mapping Recursive Functions to Reconfigurable Hardware,"Reconfigurable computing devices such as FPGAs offer application developers the ability to create solutions with a performance comparable to that of a hardware solution, but with the flexibility of software. Development tools that attempt to support popular software development languages such as C and Java have been developed to reduce the need for the FPGA developer to be trained in hardware design practices, however the tools have not been successful in mirroring all of the languages functionality. In particular most tools do not support programming with recursive functions. Previous research on mapping recursive functions to reconfigurable hardware has built a stack on the device which does not take full advantage of the massive amount of parallel resources on the reconfigurable device. This paper describes a method for mapping recursive functions to reconfigurable hardware without the use of a stack. It does this by unrolling the function on the device as it is executing. The results presented in this paper show that using this method can result in a significant performance increase when compared to a stack based implementation.","Hardware,
Field programmable gate arrays,
Space technology,
Parallel processing,
Pipeline processing,
Software packages,
Packaging,
Australia,
Computer science,
Software tools"
"Experiences with Honeypot Systems: Development, Deployment, and Analysis","This paper presents a summary of university research performed on honeypot techniques and summarizes the results Honeypots are computing resources that serve no other purpose on a network than to be a target for attackers, and log data about the attacks. Low-interaction and high-interaction honeypots were implemented and deployed on a university network, and data was logged and analyzed about attacks that occurred during the honeypots’ deployment. Current efforts in the ""black hat"" attacker community focusing on detecting and subverting honeypots are discussed, and recommendations are made to improve the usage of honeypots as an attack profiling tool by improving the control measures that make a honeypot easy to detect.",
Accuracy Enhancement for UWB Indoor Positioning Using Ray Tracing,,
Optimal Utility-Lifetime Trade-off in Self-regulating Wireless Sensor Networks: A Distributed Approach,"The performance of wireless sensor network applications is typically a function of the amount of data collected by the individual sensors and delivered to a set of sinks through multi-hop routing within the network. However, the energy-constrained nature of the nodes limits the operational lifetime of the network since energy is dissipated both in sensing and in communicating data across the network. There is thus an inherent trade-off in simultaneously maximizing the application performance (characterized here by a network utility function) and the network lifetime. In this paper, we characterize this trade-off by considering a cross-layer design problem in a wireless sensor network with orthogonal link transmissions. We compute an optimal set of source rates, network flows, and radio resources at the transport, network, and radio resource layers respectively, while jointly maximizing the network utility and lifetime. Through the framework of ""layering as optimization decomposition"", we show that the cross-layer optimization problem decomposes both horizontally (across nodes) and vertically (across different layers in the protocol stack) into simpler subproblems allowing a fully distributed solution.",
Using Patterns to Understand and Compare Web Services Security Products and Standards,"Web services are becoming an important way for enterprises to interoperate. Many security standards have been developed for web services, but they are still vulnerable to a variety of attacks; lack of security is one of the main reasons given by people who are reluctant to use web services even knowing of their advantages. A problem with web services security standards is that several organizations are involved in developing them and as a result there are many, and they may overlap. We are developing a catalog of architectural security patterns for web services corresponding to security standards and mechanisms. In this paper we explore another aspect: how to compare standards using patterns. By expressing standards as patterns, we can compare them and understand them better. For example, we can discover potentially overlapping and inconsistent aspects between them.","Web services,
Standards development,
Standards organizations,
Service oriented architecture,
Computer security,
Computer science,
XML,
Virtual private networks,
Identity management systems,
Pattern analysis"
SAS: A Simple Anonymity Scheme for Clustered Wireless Sensor Networks,"In this paper, we propose a simple and efficient scheme for establishing anonymity in clustered wireless sensor networks. This scheme is applied to a clustered sensor network in which the nodes in a neighborhood share pairwise keys for authentic and confidential communication. The scheme, named Simple Anonymity Scheme (SAS), uses a range of pseudonyms as identifiers for a node in the network, to ensure concealment of its true identifier (ID). After deployment, neighboring nodes in the network share their individual pseudonyms and use them to ensure that the communication is anonymous and that a node's true ID is kept private. Even when many nodes in a given neighborhood of the network are compromised and are colluding, our scheme ensures that non-compromised nodes are still guaranteed complete anonymity. The compromised nodes cannot identify the sender or the receiver of communication happening between non-compromised nodes. Our scheme requires reasonably low memory and has very low computation cost, needing no change in other protocols of the network stack. It can be embedded into any wireless sensor network routing protocol to ensure anonymity and privacy during node discovery and routing in the network.","Synthetic aperture sonar,
Wireless sensor networks,
Telecommunication traffic,
Privacy,
Mobile ad hoc networks,
Protocols,
Base stations,
Aggregates,
Intelligent networks,
Intelligent sensors"
Using Cross-Document Random Walks for Topic-Focused Multi-Document,Graph-ranking based methods have been developed for generic multi-document summarization in recent years and they make uniform use of the relationships between sentences to extract salient sentences. This paper proposes to integrate the relevance of the sentences to the specified topic into the graph-ranking based method for topic-focused multi-document summarization. The cross-document relationships and the within-document relationships between sentences are differentiated and we apply the graph-ranking based method using each individual kind of sentence relationships and explore their relative importance for topic-focused multi-document summarization. Experimental results on DUC2003 and DUC2005 demonstrate the great importance of the cross-document relationships between sentences for topic-focused multi-document summarization. Even the approach based only on the cross-document sentence relationships can perform better than or at least as well as the approaches based on both kinds of sentence relationships,"Computer science,
Data mining"
Handling High Speed Traffic Measurement Using Network Processors,"Traffic measurement management, accounting, worm detection, etc. It becomes more and more difficult today and cannot keep pace with the increasing speed of today's Internet. Many new algorithms and hardware have been proposed to solve this problem. Network Processors, known for its nice tradeoff between performance and programming flexibility, are chosen as the platform. The goal of this paper is to find a suitable algorithm based on NPs to handle high speed network traffic measurement. Four algorithms, Raw measurement, Sampling, Multi-stage Filter and Multi-Resolution Space Code Bloom Filter (MRSCBF) are implemented and evaluated on Intel's IXP 2400 network processor. The results reveal that Sampling and Multi-stage Filters can fully exploit the parallel and heterogeneous architecture of network processor, so are suitable for high speed network traffic measurement on the network processor platform. MRSCBF, on the other hand, is not so efficient on network processors because of its complex process and frequent memory access.",
DHT-based unicast for mobile ad hoc networks,"As mobile ad hoc networks (MANETs) become ever more popular, it also becomes more and more interesting to build distributed network applications (e.g. data storage, etc.) that have been successfully built using DHTs on the Internet in such MANETs. For this purpose, DHT substrates especially designed for the use in MANETs have been recently proposed. In this paper, we demonstrate that such a DHT substrate (MADPastry) can be efficiently used to not only provide indirect, key-based overlay routing, but also conventional direct unicasting. Our simulation results show that DHT-based unicasting can markedly outperform conventional reactive ad hoc routing (AODV) as well as proactive ad hoc routing (OLSR). Therefore, MANET nodes that are already running a DHT substrate for application purposes will no longer have to maintain a separate ad hoc routing protocol in parallel but can use their DHT substrate for conventional point-to-point ad hoc routing as well",
