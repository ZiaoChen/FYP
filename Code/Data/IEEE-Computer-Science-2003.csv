Title,Abstract,Keywords
Chord: a scalable peer-to-peer lookup protocol for Internet applications,"A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: Communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.","Peer to peer computing,
Protocols,
Internet,
Routing,
Application software,
Computer science,
Analytical models,
Costs,
Centralized control,
Network servers"
The vision of autonomic computing,"A 2001 IBM manifesto observed that a looming software complexity crisis -caused by applications and environments that number into the tens of millions of lines of code - threatened to halt progress in computing. The manifesto noted the almost impossible difficulty of managing current and planned computing systems, which require integrating several heterogeneous environments into corporate-wide computing systems that extend into the Internet. Autonomic computing, perhaps the most attractive approach to solving this problem, creates systems that can manage themselves when given high-level objectives from administrators. Systems manage themselves according to an administrator's goals. New components integrate as effortlessly as a new cell establishes itself in the human body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems.",
Mutual-information-based registration of medical images: a survey,"An overview is presented of the medical image processing literature on mutual-information-based registration. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application. Methods are classified according to the different aspects of mutual-information-based registration. The main division is in aspects of the methodology and of the application. The part on methodology describes choices made on facets such as preprocessing of images, gray value interpolation, optimization, adaptations to the mutual information measure, and different types of geometrical transformations. The part on applications is a reference of the literature available on different modalities, on interpatient registration and on different anatomical objects. Comparison studies including mutual information are also considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on past achievements and some future challenges.",
Directed diffusion for wireless sensor networking,"Advances in processor, memory, and radio technology enable small and cheap nodes capable of sensing, communication, and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. We explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed-diffusion-based network are application aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network (e.g., data aggregation). We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network analytically and experimentally. Our evaluation indicates that directed diffusion can achieve significant energy savings and can outperform idealized traditional schemes (e.g., omniscient multicast) under the investigated scenarios.","Wireless sensor networks,
Humans,
Robustness,
Computer science,
Vehicle detection,
Sensor phenomena and characterization,
Random access memory,
Routing,
Wireless communication,
Large-scale systems"
Image denoising using scale mixtures of Gaussians in the wavelet domain,"We describe a method for removing noise from digital images, based on a statistical model of the coefficients of an overcomplete multiscale oriented basis. Neighborhoods of coefficients at adjacent positions and scales are modeled as the product of two independent random variables: a Gaussian vector and a hidden positive scalar multiplier. The latter modulates the local variance of the coefficients in the neighborhood, and is thus able to account for the empirically observed correlation between the coefficient amplitudes. Under this model, the Bayesian least squares estimate of each coefficient reduces to a weighted average of the local linear estimates over all possible values of the hidden multiplier variable. We demonstrate through simulations with images contaminated by additive white Gaussian noise that the performance of this method substantially surpasses that of previously published methods, both visually and in terms of mean squared error.",
Laplacian Eigenmaps for Dimensionality Reduction and Data Representation,"One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.",
"The CMU pose, illumination, and expression database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",
Basic mechanisms and modeling of single-event upset in digital microelectronics,"Physical mechanisms responsible for nondestructive single-event effects in digital microelectronics are reviewed, concentrating on silicon MOS devices and integrated circuits. A brief historical overview of single-event effects in space and terrestrial systems is given, and upset mechanisms in dynamic random access memories, static random access memories, and combinational logic are detailed. Techniques for mitigating single-event upset are described, as well as methods for predicting device and circuit single-event response using computer simulations. The impact of technology trends on single-event susceptibility and future areas of concern are explored.","Microelectronics,
Space technology,
Single event upset,
Circuits,
Logic devices,
Laboratories,
Silicon,
MOS devices,
DRAM chips,
SRAM chips"
"Sprite: a simple, cheat-proof, credit-based system for mobile ad-hoc networks","Mobile ad hoc networking has been an active research area for several years. How to stimulate cooperation among selfish mobile nodes, however, is not well addressed yet. In this paper, we propose Sprite, a simple, cheat-proof, credit-based system for stimulating cooperation among selfish nodes in mobile ad hoc networks. Our system provides incentive for mobile nodes to cooperate and report actions honestly. Compared with previous approaches, our system does not require any tamper-proof hardware at any node. Furthermore, we present a formal model of our system and prove its properties. Evaluations of a prototype implementation show that the overhead of our system is small. Simulations and analysis show that mobile nodes can cooperate and forward each other's messages, unless the resource of each node is extremely low.","Sprites (computer),
Ad hoc networks,
Mobile ad hoc networks,
Mobile computing,
Computer science,
Military computing,
Spread spectrum communication,
Protocols,
Fault diagnosis,
Costs"
Learning a classification model for segmentation,"We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.",
Gossip-based computation of aggregate information,"Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.","Aggregates,
Peer to peer computing,
Protocols,
Large-scale systems,
Temperature sensors,
Distributed computing,
Computer science,
Scalability,
Fault tolerant systems,
Stress"
A shape-based approach to the segmentation of medical imagery using level sets,"We propose a shape-based approach to curve evolution for the segmentation of medical images containing known object types. In particular, motivated by the work of Leventon, Grimson, and Faugeras (2000), we derive a parametric model for an implicit representation of the segmenting curve by applying principal component analysis to a collection of signed distance representations of the training data. The parameters of this representation are then manipulated to minimize an objective function for segmentation. The resulting algorithm is able to handle multidimensional data, can deal with topological changes of the curve, is robust to noise and initial contour placements, and is computationally efficient. At the same time, it avoids the need for point correspondences during the training phase of the algorithm. We demonstrate this technique by applying it to two medical applications; two-dimensional segmentation of cardiac magnetic resonance imaging (MRI) and three-dimensional segmentation of prostate MRI.","Image segmentation,
Biomedical imaging,
Level set,
Magnetic resonance imaging,
Parametric statistics,
Principal component analysis,
Training data,
Multidimensional systems,
Noise robustness,
Magnetic noise"
LANDMARC: indoor location sensing using active RFID,"Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of ""context-aware"" applications, where location is the most essential context. We present LANDMARC, a location sensing prototype system that uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, we demonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market.",
Inside the Slammer worm,"The Slammer worm spread so quickly that human response was ineffective. In January 2003, it packed a benign payload, but its disruptive capacity was surprising. Why was it so effective and what new challenges do this new breed of worm pose?.","Computer worms,
Payloads,
Internet,
Data analysis,
Silicon,
Bandwidth,
Probes,
Humans,
Computer science"
IMPORTANT: a framework to systematically analyze the Impact of Mobility on Performance of Routing Protocols for Adhoc Networks,"A mobile ad hoc network (MANET) is a collection of wireless mobile nodes forming a temporary network without using any existing infrastructure. Since not many MANETs are currently deployed, research in this area is mostly simulation based. Random waypoint is the commonly used mobility model in these simulations. Random waypoint is a simple model that may be applicable to some scenarios. However, we believe that it is not sufficient to capture some important mobility characteristics of scenarios in which MANETs may be deployed. Our framework aims to evaluate the impact of different mobility models on the performance of MANET routing protocols. We propose various protocol independent metrics to capture interesting mobility characteristics, including spatial and temporal dependence and geographic restrictions. In addition, a rich set of parameterized mobility models is introduced including random waypoint, group mobility, freeway and Manhattan models. Based on these models several 'test-suite' scenarios are chosen carefully to span the metric space. We demonstrate the utility of our test-suite by evaluating various MANET routing protocols, including DSR, AODV and DSDV. Our results show that the protocol performance may vary drastically across mobility models and performance rankings of protocols may vary with the mobility models used. This effect can be explained by the interaction of the mobility characteristics with the connectivity graph properties. Finally, we attempt to decompose the routing protocols into mechanistic ""building blocks"" to gain a deeper insight into the performance variations across protocols in the face of mobility.",
Human-robot interactions during the robot-assisted urban search and rescue response at the World Trade Center,"The World Trade Center (WTC) rescue response provided an unfortunate opportunity to study the human-robot interactions (HRI) during a real unstaged rescue for the first time. A post-hoc analysis was performed on the data collected during the response, which resulted in 17 findings on the impact of the environment and conditions on the HRI: the skills displayed and needed by robots and humans, the details of the Urban Search and Rescue (USAR) task, the social informatics in the USAR domain, and what information is communicated at what time. The results of this work impact the field of robotics by providing a case study for HRI in USAR drawn from an unstaged USAR effort. Eleven recommendations are made based on the findings that impact the robotics, computer science, engineering, psychology, and rescue fields. These recommendations call for group organization and user confidence studies, more research into perceptual and assistive interfaces, and formal models of the state of the robot, state of the world, and information as to what has been observed.",
Fast pose estimation with parameter-sensitive hashing,"Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images.",
Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels,"We describe an automated method to locate the optic nerve in images of the ocular fundus. Our method uses a novel algorithm we call fuzzy convergence to determine the origination of the blood vessel network. We evaluate our method using 31 images of healthy retinas and 50 images of diseased retinas, containing such diverse symptoms as tortuous vessels, choroidal neovascularization, and hemorrhages that completely obscure the actual nerve. On this difficult data set, our method achieved 89% correct detection. We also compare our method against three simpler methods, demonstrating the performance improvement. All our images and data are freely available for other researchers to use in evaluating related methods.",
Algebraic soft-decision decoding of Reed-Solomon codes,"A polynomial-time soft-decision decoding algorithm for Reed-Solomon codes is developed. This list-decoding algorithm is algebraic in nature and builds upon the interpolation procedure proposed by Guruswami and Sudan(see ibid., vol.45, p.1757-67, Sept. 1999) for hard-decision decoding. Algebraic soft-decision decoding is achieved by means of converting the probabilistic reliability information into a set of interpolation points, along with their multiplicities. The proposed conversion procedure is shown to be asymptotically optimal for a certain probabilistic model. The resulting soft-decoding algorithm significantly outperforms both the Guruswami-Sudan decoding and the generalized minimum distance (GMD) decoding of Reed-Solomon codes, while maintaining a complexity that is polynomial in the length of the code. Asymptotic analysis for alarge number of interpolation points is presented, leading to a geo- metric characterization of the decoding regions of the proposed algorithm. It is then shown that the asymptotic performance can be approached as closely as desired with a list size that does not depend on the length of the code.","Decoding,
Reed-Solomon codes,
Interpolation,
Maintenance,
Algorithm design and analysis,
Information theory,
Computer science,
Wireless communication,
Modulation coding,
Constellation diagram"
Toward an affect-sensitive multimodal human-computer interaction,"The ability to recognize affective states of a person we are communicating with is the core of emotional intelligence. Emotional intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for successful interpersonal social interaction. This paper argues that next-generation human-computer interaction (HCI) designs need to include the essence of emotional intelligence - the ability to recognize a user's affective states-in order to become more human-like, more effective, and more efficient. Affective arousal modulates all nonverbal communicative cues (facial expressions, body movements, and vocal and physiological reactions). In a face-to-face interaction, humans detect and interpret those interactive signals of their communicator with little or no effort. Yet design and development of an automated system that accomplishes these tasks is rather difficult. This paper surveys the past work in solving these problems by a computer and provides a set of recommendations for developing the first part of an intelligent multimodal HCI-an automatic personalized analyzer of a user's nonverbal affective feedback.","Human computer interaction,
Emotion recognition,
Face detection,
Animation,
Anthropometry,
Neuroscience,
Psychology,
Cognitive science,
Knowledge based systems,
Feedback"
A versatile wavelet domain noise filtration technique for medical imaging,"We propose a robust wavelet domain method for noise filtering in medical images. The proposed method adapts itself to various types of image noise as well as to the preference of the medical expert; a single parameter can be used to balance the preservation of (expert-dependent) relevant details against the degree of noise reduction. The algorithm exploits generally valid knowledge about the correlation of significant image features across the resolution scales to perform a preliminary coefficient classification. This preliminary coefficient classification is used to empirically estimate the statistical distributions of the coefficients that represent useful image features on the one hand and mainly noise on the other. The adaptation to the spatial context in the image is achieved by using a wavelet domain indicator of the local spatial activity. The proposed method is of low complexity, both in its implementation and execution time. The results demonstrate its usefulness for noise suppression in medical ultrasound and magnetic resonance imaging. In these applications, the proposed method clearly outperforms single-resolution spatially adaptive algorithms, in terms of quantitative performance measures as well as in terms of visual quality of the images.","Wavelet domain,
Filtration,
Biomedical imaging,
Noise reduction,
Noise robustness,
Filtering,
Spatial resolution,
Image resolution,
Statistical distributions,
Magnetic noise"
ZIGZAG: an efficient peer-to-peer scheme for media streaming,"A peer-to-peer technique called ZIGZAG for single-source media streaming is designed . ZIGZAG allows the media server to distribute content to many clients by organizing them into an appropriate tree rooted at the server. This application-layer multicast tree has a height logarithmic with the number of clients and a node degree bounded by a constant. This helps reduce the number of processing hops on the delivery path to a client while avoiding network bottleneck. Consequently, the end-to-end delay is kept small. Although one could build a tree satisfying such properties easily, an efficient control protocol between the nodes must be in place to maintain the tree under the effects of network dynamics and unpredictable client behaviors. ZIGZAG handles such situations gracefully requiring a constant amortized control overhead. Especially, failure recovery can be done regionally with little impact on the existing clients and mostly no burden on the server.",
Recovering documentation-to-source-code traceability links using latent semantic indexing,"An information retrieval technique, latent semantic indexing, is used to automatically identify traceability links from system documentation to program source code. The results of two experiments to identify links in existing software systems (i.e., the LEDA library, and Albergate) are presented. These results are compared with other similar type experimental results of traceability link identification using different types of information retrieval techniques. The method presented proves to give good results by comparison and additionally it is a low cost, highly flexible method to apply with regards to preprocessing and/or parsing of the source code and documentation.",
Evaluation and characterization of available bandwidth probing techniques,"The packet pair mechanism has been shown to be a reliable method to measure the bottleneck link capacity on a network path, but its use for measuring available bandwidth is more challenging. In this paper, we use modeling, measurements, and simulations to better characterize the interaction between probing packets and the competing network traffic. We first construct a simple model to understand how competing traffic changes the probing packet gap for a single-hop network. The gap model shows that the initial probing gap is a critical parameter when using packet pairs to estimate available bandwidth. Based on this insight, we present two available bandwidth measurement techniques, the initial gap increasing (IGI) method and the packet transmission rate (PTR) method. We use extensive Internet measurements to show that these techniques estimate available bandwidth faster than existing techniques such as Pathload, with comparable accuracy. Finally, using both Internet measurements and ns simulations, we explore how the measurement accuracy of active probing is affected by factors such as the probing packet size, the length of probing packet train, and the competing traffic on links other than the tight link.",
Model-driven development: a metamodeling foundation,"Metamodeling is an essential foundation for MDD, but there's little consensus on the precise form it should take and role it should play. The authors analyze the underlying motivation for MDD and then derive a concrete set of requirements that a supporting infrastructure should satisfy. They discuss why the traditional ""language definition"" interpretation of metamodeling isn't a sufficient foundation and explain how it can be extended to unlock MDD's full potential.","Metamodeling,
Productivity,
Object oriented modeling,
Programming profession,
Personnel,
Humans,
Program processors,
Computer science,
Production,
Investments"
Multiclass spectral clustering,"We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported.","Discrete transforms,
Karhunen-Loeve transforms,
Image segmentation,
Computer vision,
Robots,
Information science,
Singular value decomposition,
Robustness,
Image converters,
Clustering methods"
Single-ISA heterogeneous multi-core architectures: the potential for processor power reduction,"This paper proposes and evaluates single-ISA heterogeneous multi-core architectures as a mechanism to reduce processor power dissipation. Our design incorporates heterogeneous cores representing different points in the power/performance design space; during an application's execution, system software dynamically chooses the most appropriate core to meet specific performance and power requirements. Our evaluation of this architecture shows significant energy benefits. For an objective function that optimizes for energy efficiency with a tight performance threshold, for 14 SPEC benchmarks, our results indicate a 39% average energy reduction while only sacrificing 3% in performance. An objective function that optimizes for energy-delay with looser performance bounds achieves, on average, nearly a factor of three improvements in energy-delay product while sacrificing only 22% in performance. Energy savings are substantially more than chip-wide voltage/frequency scaling.",
Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel,"Support vector machines (SVMs) with the gaussian (RBF) kernel have been popular for practical use. Model selection in this class of SVMs involves two hyper parameters: the penalty parameter C and the kernel width σ. This letter analyzes the behavior of the SVM classifier when these hyper parameters take very small or very large values. Our results help in understanding the hyperparameter space that leads to an efficient heuristic method of searching for hyperparameter values with small generalization errors. The analysis also indicates that if complete model selection using the gaussian kernel has been conducted, there is no need to consider linear SVM.",
Steganalysis using image quality metrics,"We present techniques for steganalysis of images that have been potentially subjected to steganographic algorithms, both within the passive warden and active warden frameworks. Our hypothesis is that steganographic schemes leave statistical evidence that can be exploited for detection with the aid of image quality features and multivariate regression analysis. To this effect image quality metrics have been identified based on the analysis of variance (ANOVA) technique as feature sets to distinguish between cover-images and stego-images. The classifier between cover and stego-images is built using multivariate regression on the selected quality metrics and is trained based on an estimate of the original image. Simulation results with the chosen feature set and well-known watermarking and steganographic techniques indicate that our approach is able with reasonable accuracy to distinguish between cover and stego images.",
SPEED: a stateless protocol for real-time communication in sensor networks,"In this paper, we present a real-time communication protocol for sensor networks, called SPEED. The protocol provides three types of real-time communication services, namely, real-time unicast, real-time area-multicast and real-time area-anycast. SPEED is specifically tailored to be a stateless, localized algorithm with minimal control overhead End-to-end soft real-time communication is achieved by maintaining a desired delivery speed across the sensor network through a novel combination of feedback control and non-deterministic geographic forwarding. SPEED is a highly efficient and scalable protocol for sensor networks where the resources of each node are scarce. Theoretical analysis, simulation experiments and a real implementation on Berkeley motes are provided to validate our claims.","Intelligent networks,
Intelligent sensors,
Routing protocols,
Computer science,
Unicast,
Feedback control,
Large-scale systems,
Base stations,
Communication system control,
Analytical models"
Design and analysis of an MST-based topology control algorithm,"In this paper, we present a minimum spanning tree (MST) based topology control algorithm, called local minimum spanning tree (LMST), for wireless multi-hop networks. In this algorithm, each node builds its local minimum spanning tree independently and only keeps on-tree nodes that are one-hop away as its neighbors in the final topology. We analytically prove several important properties of LMST: (1) the topology derived under LMST preserves the network connectivity; (2) the node degree of any node in the resulting topology is bounded by 6; and (3) the topology can be transformed into one with bidirectional links (without impairing the network connectivity) after removal of all uni-directional links. These results are corroborated in the simulation study.","Algorithm design and analysis,
Network topology,
Spread spectrum communication,
Bidirectional control,
Interference,
Wireless networks,
Power control,
Communication system control,
Computer science,
Energy management"
Adaptive computing on the Grid using AppLeS,"Ensembles of distributed, heterogeneous resources, also known as computational grids, have emerged as critical platforms for high-performance and resource-intensive applications. Such platforms provide the potential for applications to aggregate enormous bandwidth, computational power, memory, secondary storage, and other resources during a single execution. However, achieving this performance potential in dynamic, heterogeneous environments is challenging. Recent experience with distributed applications indicates that adaptivity is fundamental to achieving application performance in dynamic grid environments. The AppLeS (Application Level Scheduling) project provides a methodology, application software, and software environments for adaptively scheduling and deploying applications in heterogeneous, multiuser grid environments. We discuss the AppLeS project and outline our findings.","Grid computing,
Application software,
Distributed computing,
Processor scheduling,
Computer networks,
Concurrent computing,
Dynamic scheduling,
Computer Society,
Adaptive scheduling,
Computer science"
Independent component analysis of Gabor features for face recognition,"We present an independent Gabor features (IGFs) method and its application to face recognition. The novelty of the IGF method comes from 1) the derivation of independent Gabor features in the feature extraction stage and 2) the development of an IGF features-based probabilistic reasoning model (PRM) classification method in the pattern recognition stage. In particular, the IGF method first derives a Gabor feature vector from a set of downsampled Gabor wavelet representations of face images, then reduces the dimensionality of the vector by means of principal component analysis, and finally defines the independent Gabor features based on the independent component analysis (ICA). The independence property of these Gabor features facilitates the application of the PRM method for classification. The rationale behind integrating the Gabor wavelets and the ICA is twofold. On the one hand, the Gabor transformed face images exhibit strong characteristics of spatial locality, scale, and orientation selectivity. These images can, thus, produce salient local features that are most suitable for face recognition. On the other hand, ICA would further reduce redundancy and represent independent features explicitly. These independent features are most useful for subsequent pattern discrimination and associative recall. Experiments on face recognition using the FacE REcognition Technology (FERET) and the ORL datasets, where the images vary in illumination, expression, pose, and scale, show the feasibility of the IGF method. In particular, the IGF method achieves 98.5% correct face recognition accuracy when using 180 features for the FERET dataset, and 100% accuracy for the ORL dataset using 88 features.",
Automatic grasp planning using shape primitives,"Automatic grasp planning for robotic hands is a difficult problem because of the huge number of possible hand configurations. However, humans simplify the problem by choosing an appropriate prehensile posture appropriate for the object and task to be performed. By modeling an object as a set of shape primitives, such as spheres, cylinders, cones and boxes, we can use a set of rules to generate a set of grasp starting positions and pregrasp shapes that can then be tested on the object model. Each grasp is tested and evaluated within our grasping simulator ""GraspIt!"", and the best grasps are presented to the user. The simulator can also plan grasps in a complex environment involving obstacles and the reachability constraints of a robot arm.","Shape,
Testing,
Fingers,
Grasping,
Robots,
Humans,
Computer science,
Technology planning,
Robotics and automation,
Wrist"
Computing geodesics and minimal surfaces via graph cuts,"Geodesic active contours and graph cuts are two standard image segmentation techniques. We introduce a new segmentation method combining some of their benefits. Our main intuition is that any cut on a graph embedded in some continuous space can be interpreted as a contour (in 2D) or a surface (in 3D). We show how to build a grid graph and set its edge weights so that the cost of cuts is arbitrarily close to the length (area) of the corresponding contours (surfaces) for any anisotropic Riemannian metric. There are two interesting consequences of this technical result. First, graph cut algorithms can be used to find globally minimum geodesic contours (minimal surfaces in 3D) under arbitrary Riemannian metric for a given set of boundary conditions. Second, we show how to minimize metrication artifacts in existing graph-cut based methods in vision. Theoretically speaking, our work provides an interesting link between several branches of mathematics -differential geometry, integral geometry, and combinatorial optimization. The main technical problem is solved using Cauchy-Crofton formula from integral geometry.",
Fault localization with nearest neighbor queries,"We present a method for performing fault localization using similar program spectra. Our method assumes the existence of a faulty run and a larger number of correct runs. It then selects according to a distance criterion the correct run that most resembles the faulty run, compares the spectra corresponding to these two runs, and produces a report of ""suspicious"" parts of the program. Our method is widely applicable because it does not require any knowledge of the program input and no more information from the user than a classification of the runs as either ""correct"" or ""faulty"". To experimentally validate the viability of the method, we implemented it in a tool, Whither, using basic block profiling spectra. We experimented with two different similarity measures and the Siemens suite of 132 programs with injected bugs. To measure the success of the tool, we developed a generic method for establishing the quality of a report. The method is based on the way an ""ideal user"" would navigate the program using the report to save effort during debugging. The best results obtained were, on average, above 50%, meaning that our ideal user would avoid looking half of the program.","Nearest neighbor searches,
Computer bugs,
Testing,
Programming profession,
Computer science,
Electronic mail,
Navigation,
Debugging,
Runtime,
Software engineering"
Energy efficient schemes for wireless sensor networks with multiple mobile base stations,"One of the main design issues for a sensor network is conservation of the energy available at each sensor node. We propose to deploy multiple, mobile base stations to prolong the lifetime of the sensor network. We split the lifetime of the sensor network into equal periods of time known as rounds. Base stations are relocated at the start of a round. Our method uses an integer linear program to determine new locations for the base stations and a flow-based routing protocol to ensure energy efficient routing during each round. We propose four metrics and evaluate our solution using these metrics. Based on the simulation results we show that employing multiple, mobile base stations in accordance with the solution given by our schemes would significantly increase the lifetime of the sensor network.","Energy efficiency,
Wireless sensor networks,
Base stations,
Sensor phenomena and characterization,
Routing protocols,
Media Access Protocol,
Mobile computing,
Computer science,
Computer network management,
Energy management"
The impact of multihop wireless channel on TCP throughput and loss,"This paper studies TCP performance over multihop wireless networks that use the IEEE 802.11 protocol as the access method. Our analysis and simulations show that, given a specific network topology and flow patterns, there exists a TCP window size W*, at which TCP achieves best throughput via improved spatial channel reuse. However, TCP does not operate around W*, and typically grows its average window size much larger; this leads to decreased throughput and increased packet loss. The TCP throughput reduction can be explained by its loss behavior. Our results show that network overload is mainly signified by wireless link contention in multihop wireless networks. As long as the buffer size at each node is reasonably large (say, larger than 10 packets), buffer overflow-induced packet loss is rare and packet drops due to link-layer contention dominate. Link-layer drops offer the first sign for network overload. We further show that multihop wireless links collectively exhibit graceful drop behavior: as the offered load increases, the link contention drop probability also increases, but saturates eventually. In general, the link drop probability is insufficient to stabilize the average TCP window size around W*. Consequently, TCP suffers from reduced throughput due to reduced spatial reuse. We further propose two techniques, link RED and adaptive pacing, through which we are able to improve TCP throughput by 5% to 30% in various simulated topologies. Some simulation results are also validated by real hardware experiments.","Throughput,
Spread spectrum communication,
Wireless networks,
Buffer overflow,
Analytical models,
Network topology,
Computer science,
Performance loss,
Wireless application protocol,
Access protocols"
Real-time communication and coordination in embedded sensor networks,"Sensor networks can be considered distributed computing platforms with many severe constraints, including limited CPU speed, memory size, power, and bandwidth. Individual nodes in sensor networks are typically unreliable and the network topology dynamically changes, possibly frequently. Sensor networks also differ because of their tight interaction with the physical environment via sensors and actuators. Because of this interaction, we find that sensor networks are very data-centric. Due to all of these differences, many solutions developed for general distributed computing platforms and for ad-hoc networks cannot be applied to sensor networks. After discussing several motivating applications, this paper first discusses the state of the art with respect to general research challenges, then focuses on more specific research challenges that appear in the networking, operating system, and middleware layers. For some of the research challenges, initial solutions or approaches are identified.","Intelligent networks,
Wireless sensor networks,
Computer science,
National security,
Distributed computing,
Network topology,
Actuators,
Operating systems,
Communication system control,
Space technology"
Automatic construction of 3-D statistical deformation models of the brain using nonrigid registration,"In this paper, we show how the concept of statistical deformation models (SDMs) can be used for the construction of average models of the anatomy and their variability. SDMs are built by performing a statistical analysis of the deformations required to map anatomical features in one subject into the corresponding features in another subject. The concept of SDMs is similar to statistical shape models (SSMs) which capture statistical information about shapes across a population, but offers several advantages over SSMs. First, SDMs can be constructed directly from images such as three-dimensional (3-D) magnetic resonance (MR) or computer tomography volumes without the need for segmentation which is usually a prerequisite for the construction of SSMs. Instead, a nonrigid registration algorithm based on free-form deformations and normalized mutual information is used to compute the deformations required to establish dense correspondences between the reference subject and the subjects in the population class under investigation. Second, SDMs allow the construction of an atlas of the average anatomy as well as its variability across a population of subjects. Finally, SDMs take the 3-D nature of the underlying anatomy into account by analysing dense 3-D deformation fields rather than only information about the surface shape of anatomical structures. We show results for the construction of anatomical models of the brain from the MR images of 25 different subjects. The correspondences obtained by the nonrigid registration are evaluated using anatomical landmark locations and show an average error of 1.40 mm at these anatomical landmark positions. We also demonstrate that SDMs can be constructed so as to minimize the bias toward the chosen reference subject.","Brain modeling,
Deformable models,
Anatomy,
Shape,
Statistical analysis,
Magnetic resonance,
Tomography,
Image segmentation,
Mutual information,
Information analysis"
Fuzzy risk analysis based on similarity measures of generalized fuzzy numbers,"In this paper, we present a new method for fuzzy risk analysis based on similarity measures of generalized fuzzy numbers. Firstly, we present a method called the simple center of gravity method (SCGM) to calculate the center-of-gravity (COG) points of generalized fuzzy numbers. Then, we use the SCGM to propose a new method to measure the degree of similarity between generalized fuzzy numbers. The proposed similarity measure uses the SCGM to calculate the COG points of trapezoidal or triangular generalized fuzzy numbers and then to calculate the degree of similarity between generalized fuzzy numbers. We also prove some properties of the proposed similarity measure and use an example to compare the proposed method with the existing similarity measures. The proposed similarity measure can overcome the drawbacks of the existing methods. We also apply the proposed similarity measure to develop a new method to deal with fuzzy risk analysis problems. The proposed fuzzy risk analysis method is more flexible and more intelligent than the existing methods due to the fact that it considers the degrees of confidence of decisionmakers' opinions.","Risk analysis,
Gravity,
Vectors,
Equations,
Decision making,
Councils,
Computer science,
Arithmetic"
The adaptive bases algorithm for intensity-based nonrigid image registration,"Nonrigid registration of medical images is important for a number of applications such as the creation of population averages, atlas-based segmentation, or geometric correction of functional magnetic resonance imaging (IMRI) images to name a few. In recent years, a number of methods have been proposed to solve this problem, one class of which involves maximizing a mutual information (Ml)-based objective function over a regular grid of splines. This approach has produced good results but its computational complexity is proportional to the compliance of the transformation required to register the smallest structures in the image. Here, we propose a method that permits the spatial adaptation of the transformation's compliance. This spatial adaptation allows us to reduce the number of degrees of freedom in the overall transformation, thus speeding up the process and improving its convergence properties. To develop this method, we introduce several novelties: 1) we rely on radially symmetric basis functions rather than B-splines traditionally used to model the deformation field; 2) we propose a metric to identify regions that are poorly registered and over which the transformation needs to be improved; 3) we partition the global registration problem into several smaller ones; and 4) we introduce a new constraint scheme that allows us to produce transformations that are topologically correct. We compare the approach we propose to more traditional ones and show that our new algorithm compares favorably to those in current use.",
Linear and nonlinear methods for brain-computer interfaces,"At the recent Second International Meeting on Brain-Computer Interfaces (BCIs) held in June 2002 in Rensselaerville, NY, a formal debate was held on the pros and cons of linear and nonlinear methods in BCI research. Specific examples applying EEG data sets to linear and nonlinear methods are given and an overview of the various pros and cons of each approach is summarized. Overall, it was agreed that simplicity is generally best and, therefore, the use of linear methods is recommended wherever possible. It was also agreed that nonlinear methods in some applications can provide better results, particularly with complex and/or other very large data sets.","Brain computer interfaces,
Feature extraction,
Electroencephalography,
Computer science,
Brain modeling,
Mathematical programming,
Support vector machines,
Councils,
Counting circuits,
Signal processing"
Dictionary Learning Algorithms for Sparse Representation,"Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial “25 words or less”), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).",
CBSA: content-based soft annotation for multimodal image retrieval using Bayes point machines,"We propose a content-based soft annotation (CBSA) procedure for providing images with semantical labels. The annotation procedure starts with labeling a small set of training images, each with one single semantical label (e.g., forest, animal, or sky). An ensemble of binary classifiers is then trained for predicting label membership for images. The trained ensemble is applied to each individual image to give the image multiple soft labels, and each label is associated with a label membership factor. To select a base binary-classifier for CBSA, we experiment with two learning methods, support vector machines (SVMs) and Bayes point machines (BPMs), and compare their class-prediction accuracy. Our empirical study on a 116-category 25K-image set shows that the BPM-based ensemble provides better annotation quality than the SVM-based ensemble for supporting multimodal image retrievals.","Image retrieval,
Content based retrieval,
Support vector machines,
Labeling,
Animals,
Learning systems,
Support vector machine classification,
Shape,
Engineering profession,
Computer science"
Power laws and the AS-level Internet topology,"We study and characterize the topology of the Internet at the autonomous system (AS) level. First, we show that the topology can be described efficiently with power laws. The elegance and simplicity of the power laws provide a novel perspective into the seemingly uncontrolled Internet structure. Second, we show that power laws have appeared consistently over the last five years. We also observe that the power laws hold even in the most recent and more complete topology with correlation coefficient above 99% for the degree-based power law. In addition, we study the evolution of the power-law exponents over the five-year interval and observe a variation for the degree-based power law of less than 10%. Thirdly, we provide relationships between the exponents and other topological metrics.","Internet,
Computer science,
Network topology,
Protocols,
Parameter estimation,
Power measurement,
Engineering profession"
"Fast, iterative image reconstruction for MRI in the presence of field inhomogeneities","In magnetic resonance imaging, magnetic field inhomogeneities cause distortions in images that are reconstructed by conventional fast Fourier transform (FFT) methods. Several noniterative image reconstruction methods are used currently to compensate for field inhomogeneities, but these methods assume that the field map that characterizes the off-resonance frequencies is spatially smooth. Recently, iterative methods have been proposed that can circumvent this assumption and provide improved compensation for off-resonance effects. However, straightforward implementations of such iterative methods suffer from inconveniently long computation times. This paper describes a tool for accelerating iterative reconstruction of field-corrected MR images: a novel time-segmented approximation to the MR signal equation. We use a min-max formulation to derive the temporal interpolator. Speedups of around 60 were achieved by combining this temporal interpolator with a nonuniform fast Fourier transform with normalized root mean squared approximation errors of 0.07%. The proposed method provides fast, accurate, field-corrected image reconstruction even when the field map is not smooth.",
Designing a super-peer network,"A super-peer is a node in a peer-to-peer network that operates both as a server to a set of clients, and as an equal in a network of super-peers. Super-peer networks strike a balance between the efficiency of centralized search, and the autonomy, load balancing and robustness to attacks provided by distributed search. Furthermore, they take advantage of the heterogeneity of capabilities (e.g., bandwidth, processing power) across peers, which recent studies have shown to be enormous. Hence, new and old P2P systems like KaZaA and Gnutella are adopting super-peers in their design. Despite their growing popularity, the behavior of super-peer networks is not well understood. For example, what are the potential drawbacks of super-peer networks? How can super-peers be made more reliable? How many clients should a super-peer take on to maximize efficiency? we examine super-peer networks in detail, gaming an understanding of their fundamental characteristics and performance tradeoffs. We also present practical guidelines and a general procedure for the design of an efficient super-peer network.","Peer to peer computing,
Network servers,
Bandwidth,
Costs,
Computer science,
Load management,
Robustness,
Guidelines,
File servers,
Fault tolerant systems"
The dual-bootstrap iterative closest point algorithm with application to retinal image registration,"Motivated by the problem of retinal image registration, this paper introduces and analyzes a new registration algorithm called Dual-Bootstrap Iterative Closest Point (Dual-Bootstrap ICP). The approach is to start from one or more initial, low-order estimates that are only accurate in small image regions, called bootstrap regions. In each bootstrap region, the algorithm iteratively: 1) refines the transformation estimate using constraints only from within the bootstrap region; 2) expands the bootstrap region; and 3) tests to see if a higher order transformation model can be used, stopping when the region expands to cover the overlap between images. Steps 1): and 3), the bootstrap steps, are governed by the covariance matrix of the estimated transformation. Estimation refinement [Step 2)] uses a novel robust version of the ICP algorithm. In registering retinal image pairs, Dual-Bootstrap ICP is initialized by automatically matching individual vascular landmarks, and it aligns images based on detected blood vessel centerlines. The resulting quadratic transformations are accurate to less than a pixel. On tests involving approximately 6000 image pairs, it successfully registered 99.5% of the pairs containing at least one common landmark, and 100% of the pairs containing at least one common landmark and at least 35% image overlap.","Iterative closest point algorithm,
Retina,
Image registration,
Iterative algorithms,
Testing,
Image analysis,
Algorithm design and analysis,
Covariance matrix,
Robustness,
Blood vessels"
An adaptive spatial fuzzy clustering algorithm for 3-D MR image segmentation,"An adaptive spatial fuzzy c-means clustering algorithm is presented in this paper for the segmentation of three-dimensional (3-D) magnetic resonance (MR) images. The input images may be corrupted by noise and intensity nonuniformity (INU) artifact. The proposed algorithm takes into account the spatial continuity constraints by using a dissimilarity index that allows spatial interactions between image voxels. The local spatial continuity constraint reduces the noise effect and the classification ambiguity. The INU artifact is formulated as a multiplicative bias field affecting the true MR imaging signal. By modeling the log bias field as a stack of smoothing B-spline surfaces, with continuity enforced across slices, the computation of the 3-D bias field reduces to that of finding the B-spline coefficients, which can be obtained using a computationally efficient two-stage algorithm. The efficacy of the proposed algorithm is demonstrated by extensive segmentation experiments using both simulated and real MR images and by comparison with other published algorithms.","Clustering algorithms,
Image segmentation,
Pixel,
Magnetic resonance imaging,
Magnetic noise,
Noise reduction,
Spline,
Nonuniform electric fields,
Information technology,
Magnetic resonance"
Self-adaptive fitness formulation for constrained optimization,"A self-adaptive fitness formulation is presented for solving constrained optimization problems. In this method, the dimensionality of the problem is reduced by representing the constraint violations by a single infeasibility measure. The infeasibility measure is used to form a two-stage penalty that is applied to the infeasible solutions. The performance of the method has been examined by its application to a set of eleven test cases from the specialized literature. The results have been compared with previously published results from the literature. It is shown that the method is able to find the optimum solutions. The proposed method requires no parameter tuning and can be used as a fitness evaluator with any evolutionary algorithm. The approach is also robust in its handling of both linear and nonlinear equality and inequality constraint functions. Furthermore, the method does not require an initial feasible solution.","Constraint optimization,
Decoding,
Genetic algorithms,
Testing,
Evolutionary computation,
Robustness,
Councils,
Computer science,
Biological cells,
Emulation"
ICDAR 2003 robust reading competitions,,"Robustness,
Layout,
Text recognition,
Optical character recognition software,
Character recognition,
Computer science,
Packaging machines,
Image converters,
Image databases,
Visual databases"
Reducing the run-time complexity of multiobjective EAs: The NSGA-II and other algorithms,"The last decade has seen a surge of research activity on multiobjective optimization using evolutionary computation and a number of well performing algorithms have been published. The majority of these algorithms use fitness assignment based on Pareto-domination: Nondominated sorting, dominance counting, or identification of the nondominated solutions. The success of these algorithms indicates that this type of fitness is suitable for multiobjective problems, but so far the use of Pareto-based fitness has lead to program run times in O(GMN/sup 2/), where G is the number of generations, M is the number of objectives, and N is the population size. The N/sup 2/ factor should be reduced if possible, since it leads to long processing times for large population sizes. This paper presents a new and efficient algorithm for nondominated sorting, which can speed up the processing time of some multiobjective evolutionary algorithms (MOEAs) substantially. The new algorithm is incorporated into the nondominated sorting genetic algorithm II (NSGA-II) and reduces the overall run-time complexity of this algorithm to O(GN log/sup M-1/N), much faster than the O(GMN/sup 2/) complexity published by Deb et al. (2002). Experiments demonstrate that the improved version of the algorithm is indeed much faster than the previous one. The paper also points out that multiobjective EAs using fitness based on dominance counting and identification of nondominated solutions can be improved significantly in terms of running time by using efficient algorithms known from computer science instead of inefficient O(MN/sup 2/) algorithms.",
From visual data exploration to visual data mining: a survey,"We survey work on the different uses of graphical mapping and interaction techniques for visual data mining of large data sets represented as table data. Basic terminology related to data mining, data sets, and visualization is introduced. Previous work on information visualization is reviewed in light of different categorizations of techniques and systems. The role of interaction techniques is discussed, in addition to work addressing the question of selecting and evaluating visualization techniques. We review some representative work on the use of information visualization techniques in the context of mining data. This includes both visual data exploration and visually expressing the outcome of specific mining algorithms. We also review recent innovative approaches that attempt to integrate visualization into the DM/KDD process, using it to enhance user interaction and comprehension.",
The Concave-Convex Procedure,"The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.",
Recognizing objects in adversarial clutter: breaking a visual CAPTCHA,"In this paper we explore object recognition in clutter. We test our object recognition techniques on Gimpy and EZ-Gimpy, examples of visual CAPTCHAs. A CAPTCHA (""Completely Automated Public Turing test to Tell Computers and Humans Apart"") is a program that can generate and grade tests that most humans can pass, yet current computer programs can't pass. EZ-Gimpy, currently used by Yahoo, and Gimpy are CAPTCHAs based on word recognition in the presence of clutter. These CAPTCHAs provide excellent test sets since the clutter they contain is adversarial; it is designed to confuse computer programs. We have developed efficient methods based on shape context matching that can identify the word in an EZ-Gimpy image with a success rate of 92%, and the requisite 3 words in a Gimpy image 33% of the time. The problem of identifying words in such severe clutter provides valuable insight into the more general problem of object recognition in scenes. The methods that we present are instances of a framework designed to tackle this general problem.","Humans,
Artificial intelligence,
Object recognition,
Automatic testing,
Computer science,
Shape,
Layout,
Internet,
Postal services,
Dictionaries"
User-level performance of channel-aware scheduling algorithms in wireless data networks,"Channel-aware scheduling strategies, such as the Proportional Fair algorithm for the CDMA 1xEV-DO system, provide an effective mechanism for improving throughput performance in wireless data networks by exploiting channel fluctuations. The performance of channel-aware scheduling algorithms has mostly been explored at the packet level for a static user population, often assuming infinite backlogs. In the present paper, we focus on the performance at the flow level in a dynamic setting with random finite-size service demands. We show that in certain cases the user-level performance may be evaluated by means of a multiclass Processor-Sharing model where the total service rate varies with the total number of users. The latter model provides explicit formulas for the distribution of the number of active users of the various classes, the mean response times, the blocking probabilities, and the mean throughput. In addition we show that, in the presence of channel variations, greedy, myopic strategies which maximize throughput in a static scenario, may result in sub-optimal throughput performance for a dynamic user configuration and cause potential instability effects.","Scheduling algorithm,
Intelligent networks,
Throughput,
Delay,
Processor scheduling,
Multiaccess communication,
Telecommunication traffic,
Laboratories,
Mathematics,
Computer science"
Computational techniques for the verification of hybrid systems,"Hybrid system theory lies at the intersection of the fields of engineering control theory and computer science verification. It is defined as the modeling, analysis, and control of systems that involve the interaction of both discrete state systems, represented by finite automata, and continuous state dynamics, represented by differential equations. The embedded autopilot of a modern commercial jet is a prime example of a hybrid system: the autopilot modes correspond to the application of different control laws, and the logic of mode switching is determined by the continuous state dynamics of the aircraft, as well as through interaction with the pilot. To understand the behavior of hybrid systems, to simulate, and to control these systems, theoretical advances, analyses, and numerical tools are needed. In this paper, we first present a general model for a hybrid system along with an overview of methods for verifying continuous and hybrid systems. We describe a particular verification technique for hybrid systems, based on two-person zero-sum game theory for automata and continuous dynamical systems. We then outline a numerical implementation of this technique using level set methods, and we demonstrate its use in the design and analysis of aircraft collision avoidance protocols and in verification of autopilot logic.","Control system synthesis,
Automatic control,
Automata,
Aircraft,
Control theory,
Computer science,
Control system analysis,
Differential equations,
Control systems,
Logic"
An efficient fastSLAM algorithm for generating maps of large-scale cyclic environments from raw laser range measurements,"The ability to learn a consistent model of its environment is a prerequisite for autonomous mobile robots. A particularly challenging problem in acquiring environment maps is that of closing loops; loops in the environment create challenging data association problems [J.-S. Gutman et al., 1999]. This paper presents a novel algorithm that combines Rao-Blackwellized particle filtering and scan matching. In our approach scan matching is used for minimizing odometric errors during mapping. A probabilistic model of the residual errors of scan matching process is then used for the resampling steps. This way the number of samples required is seriously reduced. Simultaneously we reduce the particle depletion problem that typically prevents the robot from closing large loops. We present extensive experiments that illustrate the superior performance of our approach compared to previous approaches.","Large-scale systems,
Robot sensing systems,
Mobile robots,
Simultaneous localization and mapping,
Computer science,
Particle filters,
Laser theory,
Filtering,
Error correction,
Laser modes"
Security for Grid services,"Grid computing is concerned with the sharing and coordinated use of diverse resources in distributed ""virtual organizations."" The dynamic and multiinstitutional nature of these environments introduces challenging security issues that demand new technical approaches. In particular, one must deal with diverse local mechanisms, support dynamic creation of services, and enable dynamic creation of trust domains. We describe how these issues are addressed in two generations of the Globus Toolkit/spl reg/. First, we review the Globus Toolkit version 2 (GT2) approach; then we describe new approaches developed to support the Globus Toolkit version 3 (GT3) implementation of the Open Grid Services Architecture, an initiative that is recasting Grid concepts within a service-oriented framework based on Web services. GT3's security implementation uses Web services security mechanisms for credential exchange and other purposes, and introduces a tight least-privilege model that avoids the need for any privileged network service.","Web services,
Grid computing,
Computer science,
Resource management,
National security,
Mathematics,
Computer security,
Information security,
Service oriented architecture,
Computer network management"
Clustering appearances of objects under varying illumination conditions,"We introduce two appearance-based methods for clustering a set of images of 3D (three-dimensional) objects, acquired under varying illumination conditions, into disjoint subsets corresponding to individual objects. The first algorithm is based on the concept of illumination cones. According to the theory, the clustering problem is equivalent to finding convex polyhedral cones in the high-dimensional image space. To efficiently determine the conic structures hidden in the image data, we introduce the concept of conic affinity, which measures the likelihood of a pair of images belonging to the same underlying polyhedral cone. For the second method, we introduce another affinity measure based on image gradient comparisons. The algorithm operates directly on the image gradients by comparing the magnitudes and orientations of the image gradient at each pixel. Both methods have clear geometric motivations, and they operate directly on the images without the need for feature extraction or computation of pixel statistics. We demonstrate experimentally that both algorithms are surprisingly effective in clustering images acquired under varying illumination conditions with two large, well-known image data sets.","Lighting,
Clustering algorithms,
Computer vision,
Computer science,
Pixel,
Image databases,
Feature extraction,
Statistics,
Image converters,
Cameras"
Mean shift based clustering in high dimensions: a texture classification example,"Feature space analysis is the main module in many computer vision tasks. The most popular technique, k-means clustering, however, has two inherent limitations: the clusters are constrained to be spherically symmetric and their number has to be known a priori. In nonparametric clustering methods, like the one based on mean shift, these limitations are eliminated but the amount of computation becomes prohibitively large as the dimension of the space increases. We exploit a recently proposed approximation technique, locality-sensitive hashing (LSH), to reduce the computational complexity of adaptive mean shift. In our implementation of LSH the optimal parameters of the data structure are determined by a pilot learning procedure, and the partitions are data driven. As an application, the performance of mode and k-means based textons are compared in a texture classification study.","Computer vision,
Space technology,
Computational complexity,
Clustering algorithms,
Robustness,
Computer science,
Industrial engineering,
Technology management,
Engineering management,
Clustering methods"
Fuzzy association rules: general model and applications,"The theory of fuzzy sets has been recognized as a suitable tool to model several kinds of patterns that can hold in data. In this paper, we are concerned with the development of a general model to discover association rules among items in a (crisp) set of fuzzy transactions. This general model can be particularized in several ways; each particular instance corresponds to a certain kind of pattern and/or repository of data. We describe some applications of this scheme, paying special attention to the discovery of fuzzy association rules in relational databases.","Association rules,
Fuzzy sets,
Data mining,
Relational databases,
Dairy products,
Fuzzy set theory,
Pattern recognition,
Humans,
Computer science,
Artificial intelligence"
A robust method for registration of three-dimensional knee implant models to two-dimensional fluoroscopy images,"A method was developed for registering three-dimensional knee implant models to single plane X-ray fluoroscopy images. We use a direct image-to-image similarity measure, taking advantage of the speed of modern computer graphics workstations to quickly render simulated (predicted) images. As a result, the method does not require an accurate segmentation of the implant silhouette in the image (which can be prone to errors). A robust optimization algorithm (simulated annealing) is used that can escape local minima and find the global minimum (true solution). Although we focus on the analysis of total knee arthroplasty (TKA) in this paper, the method can be (and has been) applied to other implanted joints, including, but not limited to, hips, ankles, and temporomandibular joints. Convergence tests on an in vivo image show that the registration method can reliably find poses that are very close to the optimal (i.e., within 0.4/spl deg/ and 0.1 mm), even from starting poses with large initial errors. However, the precision of translation measurement in the Z (out-of-plane) direction is not as good. We also show that the method is robust with respect to image noise and occlusions. However, a small amount of user supervision and intervention is necessary to detect cases when the optimization algorithm falls into a local minimum. Intervention is required less than 5% of the time when the initial starting pose is reasonably close to the correct answer, but up to 50% of the time when the initial starting pose is far away. Finally, extensive evaluations were performed on cadaver images to determine accuracy of relative pose measurement. Comparing against data derived from an optical sensor as a ""gold standard,"" the overall root-mean-square error of the registration method was approximately 1.5/spl deg/ and 0.65 mm (although Z translation error was higher). However, uncertainty in the optical sensor data may account for a large part of the observed error.","Robustness,
Knee,
Implants,
Computational modeling,
Optical sensors,
X-ray imaging,
Velocity measurement,
Computer graphics,
Workstations,
Rendering (computer graphics)"
Statistical timing analysis considering spatial correlations using a single PERT-like traversal,"We present an efficient statistical timing analysis algorithm that predicts the probability distribution of the circuit delay while incorporating the effects of spatial correlations of intra-die parameter variations, using a method based on principal component analysis. The method uses a PERT-like circuit graph traversal, and has a run-time that is linear in the number of gates and interconnects, as well as the number of grid partitions used to model spatial correlations. On average, the mean and standard deviation values computed by our method have errors of 0.2% and 0.9%, respectively, in comparison with a Monte Carlo simulation.","Timing,
Circuit analysis,
Integrated circuit interconnections,
Circuit optimization,
Delay estimation,
Distributed computing,
Permission,
Computer science,
Algorithm design and analysis,
Prediction algorithms"
Noise reduction for magnetic resonance images via adaptive multiscale products thresholding,"Edge-preserving denoising is of great interest in medical image processing. This paper presents a wavelet-based multiscale products thresholding scheme for noise suppression of magnetic resonance images. A Canny edge detector-like dyadic wavelet transform is employed. This results in the significant features in images evolving with high magnitude across wavelet scales, while noise decays rapidly. To exploit the wavelet interscale dependencies we multiply the adjacent wavelet subbands to enhance edge structures while weakening noise. In the multiscale products, edges can be effectively distinguished from noise. Thereafter, an adaptive threshold is calculated and imposed on the products, instead of on the wavelet coefficients, to identify important features. Experiments show that the proposed scheme better suppresses noise and preserves edges than other wavelet-thresholding denoising methods.","Noise reduction,
Magnetic resonance,
Magnetic resonance imaging,
Magnetic noise,
Rician channels,
Image edge detection,
Signal to noise ratio,
Gaussian noise,
Additive white noise,
Wavelet transforms"
FAME-a flexible appearance modeling environment,"Combined modeling of pixel intensities and shape has proven to be a very robust and widely applicable approach to interpret images. As such the active appearance model (AAM) framework has been applied to a wide variety of problems within medical image analysis. This paper summarizes AAM applications within medicine and describes a public domain implementation, namely the flexible appearance modeling environment (FAME). We give guidelines for the use of this research platform, and show that the optimization techniques used renders it applicable to interactive medical applications. To increase performance and make models generalize better, we apply parallel analysis to obtain automatic and objective model truncation. Further, two different AAM training methods are compared along with a reference case study carried out on cross-sectional short-axis cardiac magnetic resonance images and face images. Source code and annotated data sets needed to reproduce the results are put in the public domain for further investigation.","Active appearance model,
Biomedical imaging,
Pixel,
Shape,
Robustness,
Image analysis,
Guidelines,
Rendering (computer graphics),
Medical services,
Biomedical equipment"
Trust and reputation model in peer-to-peer networks,"It is important to enable peers to represent and update their trust in other peers in open networks for sharing files, and especially services. We propose a Bayesian network-based trust model and a method for building reputation based on recommendations in peer-to-peer networks. Since trust is multifaceted, peers need to develop differentiated trust in different aspects of other peers' capability. The peer's needs are different in different situations. Depending on the situation, a peer may need to consider its trust in a specific aspect of another peer's capability or in multiple aspects. Bayesian networks provide a flexible method to present differentiated trust and combine different aspects of trust. The evaluation of the model using a simulation shows that the system where peers communicate their experiences (recommendations) outperforms the system where peers do not share recommendations with each other and that a differentiated trust adds to the performance in terms of percentage of successful interactions.","Intelligent networks,
Peer to peer computing,
Bayesian methods,
Computer science,
Aggregates,
IP networks,
Ad hoc networks,
Costs,
Monitoring,
Authorization"
Data clustering using particle swarm optimization,"This paper proposes two new approaches to using PSO to cluster data. It is shown how PSO can be used to find the centroids of a user specified number of clusters. The algorithm is then extended to use K-means clustering to seed the initial swarm. This second algorithm basically uses PSO to refine the clusters formed by K-means. The new PSO algorithms are evaluated on six data sets, and compared to the performance of K-means clustering. Results show that both PSO clustering techniques have much potential.","Particle swarm optimization,
Clustering algorithms,
Machine learning algorithms,
Computer science,
Data mining,
Topology,
Data analysis,
Image segmentation,
Scalability,
Machine learning"
A unifying framework for partial volume segmentation of brain MR images,"Accurate brain tissue segmentation by intensity-based voxel classification of magnetic resonance (MR) images is complicated by partial volume (PV) voxels that contain a mixture of two or more tissue types. In this paper, we present a statistical framework for PV segmentation that encompasses and extends existing techniques. We start from a commonly used parametric statistical image model in which each voxel belongs to one single tissue type, and introduce an additional downsampling step that causes partial voluming along the borders between tissues. An expectation-maximization approach is used to simultaneously estimate the parameters of the resulting model and perform a PV classification. We present results on well-chosen simulated images and on real MR images of the brain, and demonstrate that the use of appropriate spatial prior knowledge not only improves the classifications, but is often indispensable for robust parameter estimation as well. We conclude that general robust PV segmentation of MR brain images requires statistical models that describe the spatial distribution of brain tissues more accurately than currently available models.","Image segmentation,
Brain modeling,
Biomedical imaging,
Hospitals,
Magnetic resonance,
Parameter estimation,
Robustness,
Markov random fields,
Biomedical engineering,
Monte Carlo methods"
Three-dimensional segmentation and growth-rate estimation of small pulmonary nodules in helical CT images,"Small pulmonary nodules are a common radiographic finding that presents an important diagnostic challenge in contemporary medicine. While pulmonary nodules are the major radiographic indicator of lung cancer, they may also be signs of a variety of benign conditions. Measurement of nodule growth rate over time has been shown to be the most promising tool in distinguishing malignant from nonmalignant pulmonary nodules. In this paper, we describe three-dimensional (3-D) methods for the segmentation, analysis, and characterization of small pulmonary nodules imaged using computed tomography (CT). Methods for the isotropic resampling of anisotropic CT data are discussed. 3-D intensity and morphology-based segmentation algorithms are discussed for several classes of nodules. New models and methods for volumetric growth characterization based on longitudinal CT studies are developed. The results of segmentation and growth characterization methods based on in vivo studies are described. The methods presented are promising in their ability to distinguish malignant from nonmalignant pulmonary nodules and represent the first such system in clinical use.","Image segmentation,
Computed tomography,
Cancer,
Diagnostic radiography,
Biomedical imaging,
Medical diagnostic imaging,
Lungs,
Time measurement,
Image analysis,
Anisotropic magnetoresistance"
Construction of an efficient overlay multicast infrastructure for real-time applications,"This paper presents an overlay architecture where service providers deploy a set of service nodes (called MSNs) in the network to efficiently implement media-streaming applications. These MSNs are organized into an overlay and act as application-layer multicast forwarding entities for a set of clients. We present a decentralized scheme that organizes the MSNs into an appropriate overlay structure that is particularly beneficial for real-time applications. We formulate our optimization criterion as a ""degree-constrained minimum average-latency problem"" which is known to be NP-hard. A key feature of this formulation is that it gives a dynamic priority to different MSNs based on the size of its service set. Our proposed approach iteratively modifies the overlay tree using localized transformations to adapt with changing distribution of MSNs, clients, as well as network conditions. We show that a centralized greedy approach to this problem does not perform quite as well, while our distributed iterative scheme efficiently converges to near-optimal solutions.",
Understanding TCP fairness over wireless LAN,"As local area wireless networks based on the IEEE 802.11 standard see increasing public deployment, it is important to ensure that access to the network by different users remains fair. While fairness issues in 802.11 networks have been studied before, this paper is the first to focus on TCP fairness in 802.11 networks in the presence of both mobile senders and receivers. In this paper, we evaluate extensively through analysis, simulation, and experimentation the interaction between the 802.11 MAC protocol and TCP. We identify four different regions of TCP unfairness that depend on the buffer availability at the base station, with some regions exhibiting significant unfairness of over 10 in terms of throughput ratio between upstream and downstream TCP flows. We also propose a simple solution that can be implemented at the base station above the MAC layer that ensures that different TCP flows share the 802.11 bandwidth equitably irrespective of the buffer availability at the base station.","Wireless LAN,
Base stations,
Bandwidth,
Media Access Protocol,
Wireless networks,
Analytical models,
Throughput,
Femtocell networks,
Computer science,
USA Councils"
Efficient mining of frequent subgraphs in the presence of isomorphism,"Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.","Databases,
Tree graphs,
Testing,
Data mining,
Bioinformatics,
Indexing,
Computer science,
Itemsets,
Performance gain,
Tree data structures"
Fault-tolerant clustering of wireless sensor networks,"During the past few years distributed wireless sensor networks have been the focus of considerable research for both military and civil applications. Sensors are generally constrained in on-board energy supply therefore efficient management of the network is crucial to extend the life of the system. Sensors' energy cannot support long haul communication to reach a remote command site, thus they require multi-tier architecture to forward data. An efficient way to enhance the lifetime of the system is to partition the network into distinct clusters with a high-energy node called a gateway as cluster-head. Failures are inevitable in sensor networks due to the inhospitable environment and unattended deployment. However, failures in higher level of hierarchy e.g. cluster-head cause more damage to the system because they also limit accessibility to the nodes that are under their supervision. In this paper we propose an efficient mechanism to recover sensors from a failed cluster. Our approach avoids a full-scale re-clustering and does not require deployment of redundant gateways.","Fault tolerance,
Wireless sensor networks,
Sensor systems,
Sensor fusion,
Military computing,
Computer science,
Energy management,
Application software,
Power generation economics,
Environmental economics"
Texture-based classification of atherosclerotic carotid plaques,"There are indications that the morphology of atherosclerotic carotid plaques, obtained by high-resolution ultrasound imaging, has prognostic implications. The objective of this study was to develop a computer-aided system that will facilitate the characterization of carotid plaques for the identification of individuals with asymptomatic carotid stenosis at risk of stroke. A total of 230 plaque images were collected which were classified into two types: symptomatic because of ipsilateral hemispheric symptoms, or asymptomatic because they were not connected with ipsilateral hemispheric events. Ten different texture feature sets were extracted from the manually segmented plaque images using the following algorithms: first-order statistics, spatial gray level dependence matrices, gray level difference statistics, neighborhood gray tone difference matrix, statistical feature matrix, Laws texture energy measures, fractal dimension texture analysis, Fourier power spectrum and shape parameters. For the classification task a modular neural network composed of self-organizing map (SOM) classifiers, and combining techniques based on a confidence measure were used. Combining the classification results of the ten SOM classifiers inputted with the ten feature sets improved the classification rate of the individual classifiers, reaching an average diagnostic yield (DY) of 73.1%. The same modular system was implemented using the statistical k-nearest neighbor (KNN) classifier. The combined DY for the KNN system was 68.8%. The results of this paper show that it is possible to identify a group of patients at risk of stroke based on texture features extracted from ultrasound images of carotid plaques. This group of patients may benefit from a carotid endarterectomy whereas other patients may be spared from an unnecessary operation.","Ultrasonic imaging,
Feature extraction,
Statistical analysis,
Shape measurement,
Morphology,
High-resolution imaging,
Image segmentation,
Energy measurement,
Power measurement,
Fractals"
A new accurate and precise 3-D segmentation method for skeletal structures in volumetric CT data,"We developed a highly automated three-dimensionally based method for the segmentation of bone in volumetric computed tomography (CT) datasets. The multistep approach starts with three-dimensional (3-D) region-growing using local adaptive thresholds followed by procedures to correct for remaining boundary discontinuities and a subsequent anatomically oriented boundary adjustment using local values of cortical bone density. We describe the details of our approach and show applications in the proximal femur, the knee, and the skull. The accuracy of the determination of geometrical parameters was analyzed using CT scans of the semi-anthropomorphic European spine phantom. Depending on the settings of the segmentation parameters cortical thickness could be determined with an accuracy corresponding to the side length of 1 to 2.5 voxels. The impact of noise on the segmentation was investigated by artificially adding noise to the CT data. An increase in noise by factors of two and five changed cortical thickness corresponding to the side length of one voxel. Intraoperator and interoperator precision was analyzed by repeated analysis of nine pelvic CT scans. Precision errors were smaller than 1% for trabecular and total volumes and smaller than 2% for cortical thickness. Intraoperator and interoperator precision errors were not significantly different. Our segmentation approach shows: 1) high accuracy and precision and is 2) robust to noise, 3) insensitive to user-defined thresholds, 4) highly automated and fast, and 5) easy to initialize.","Computed tomography,
Bones,
Image segmentation,
Biomedical imaging,
Magnetic resonance imaging,
Medical diagnostic imaging,
Physics,
Image edge detection,
Surface morphology,
Knee"
On the privacy preserving properties of random data perturbation techniques,"Privacy is becoming an increasingly important issue in many data mining applications. This has triggered the development of many privacy-preserving data mining techniques. A large fraction of them use randomized data distortion techniques to mask the data for preserving the privacy of sensitive data. This methodology attempts to hide the sensitive data by randomly modifying the data values often using additive noise. We question the utility of the random value distortion technique in privacy preservation. We note that random objects (particularly random matrices) have ""predictable"" structures in the spectral domain and it develops a random matrix-based spectral filtering technique to retrieve original data from the dataset distorted by adding random values. We present the theoretical foundation of this filtering method and extensive experimental results to demonstrate that in many cases random data distortion preserve very little data privacy. We also point out possible avenues for the development of new privacy-preserving data mining techniques like exploiting multiplicative and colored noise for preserving privacy in data mining applications.","Data privacy,
Perturbation methods,
Data mining,
Filtering,
Computer science,
Application software,
Additive noise,
Information retrieval,
Colored noise,
Telecommunication traffic"
Globally convergent image reconstruction for emission tomography using relaxed ordered subsets algorithms,"We present two types of globally convergent relaxed ordered subsets (OS) algorithms for penalized-likelihood image reconstruction in emission tomography: modified block sequential regularized expectation-maximization (BSREM) and relaxed OS separable paraboloidal surrogates (OS-SPS). The global convergence proof of the existing BSREM (De Pierro and Yamagishi, 2001) required a few a posteriori assumptions. By modifying the scaling functions of BSREM, we are able to prove the convergence of the modified BSREM under realistic assumptions. Our modification also makes stepsize selection more convenient. In addition, we introduce relaxation into the OS-SPS algorithm (Erdogan and Fessler, 1999) that otherwise would converge to a limit cycle. We prove the global convergence of diagonally scaled incremental gradient methods of which the relaxed OS-SPS is a special case; main results of the proofs are from (Nedic and Bertsekas, 2001) and (Correa and Lemarechal, 1993). Simulation results showed that both new algorithms achieve global convergence yet retain the fast initial convergence speed of conventional unrelaxed ordered subsets algorithms.","Image converters,
Image reconstruction,
Tomography,
Convergence,
Iterative algorithms,
Subspace constraints,
Gradient methods,
Maximum likelihood estimation,
Reconstruction algorithms,
Limit-cycles"
IP traceback with deterministic packet marking,"We propose a new approach for IP traceback which is scalable and simple to implement, and introduces no bandwidth and practically no processing overhead. It is backward compatible with equipment which does not implement it. The approach is capable of tracing back attacks, which are composed of just a few packets. In addition, a service provider can implement this scheme without revealing its internal network topology.","Encoding,
Bandwidth,
Network topology,
Computer crime,
Computer science education,
Educational technology,
Computer networks,
Telecommunication computing,
Electronic mail,
Proposals"
"Combining low-, high-level and empirical domain knowledge for automated segmentation of ultrasonic breast lesions","Breast cancer is the most frequently diagnosed malignancy and the second leading cause of mortality in women . In the last decade, ultrasound along with digital mammography has come to be regarded as the gold standard for breast cancer diagnosis. Automatically detecting tumors and extracting lesion boundaries in ultrasound images is difficult due to their specular nature and the variance in shape and appearance of sonographic lesions. Past work on automated ultrasonic breast lesion segmentation has not addressed important issues such as shadowing artifacts or dealing with similar tumor like structures in the sonogram. Algorithms that claim to automatically classify ultrasonic breast lesions, rely on manual delineation of the tumor boundaries. In this paper, we present a novel technique to automatically find lesion margins in ultrasound images, by combining intensity and texture with empirical domain specific knowledge along with directional gradient and a deformable shape-based model. The images are first filtered to remove speckle noise and then contrast enhanced to emphasize the tumor regions. For the first time, a mathematical formulation of the empirical rules used by radiologists in detecting ultrasonic breast lesions, popularly known as the ""Stavros Criteria"" is presented in this paper. We have applied this formulation to automatically determine a seed point within the image. Probabilistic classification of image pixels based on intensity and texture is followed by region growing using the automatically determined seed point to obtain an initial segmentation of the lesion. Boundary points are found on the directional gradient of the image. Outliers are removed by a process of recursive refinement. These boundary points are then supplied as an initial estimate to a deformable model. Incorporating empirical domain specific knowledge along with low and high-level knowledge makes it possible to avoid shadowing artifacts and lowers the chance of confusing similar tumor like structures for the lesion. The system was validated on a database of breast sonograms for 42 patients. The average mean boundary error between manual and automated segmentation was 6.6 pixels and the normalized true positive area overlap was 75.1%. The algorithm was found to be robust to 1) variations in system parameters, 2) number of training samples used, and 3) the position of the seed point within the tumor. Running time for segmenting a single sonogram was 18 s on a 1.8-GHz Pentium machine.","Lesions,
Breast neoplasms,
Ultrasonic imaging,
Sonogram,
Breast cancer,
Image segmentation,
Shadow mapping,
Deformable models,
Mammography,
Gold"
Energy-aware partitioning for multiprocessor real-time systems,"In this paper, we address the problem of partitioning periodic real-time tasks in a multiprocessor platform by considering both feasibility and energy-awareness perspectives: our objective is to compute the feasible partitioning that results in minimum energy consumption on multiple identical processors by using variable voltage earliest-deadline-first scheduling. We show that the problem is NP-hard in the strong sense on m /spl ges/ 2 processors even when feasibility is guaranteed a priori. Then, we develop our framework where load balancing plays a major role in producing energy-efficient partitionings. We evaluate the feasibility and energy-efficiency performances of partitioning heuristics experimentally.","Real time systems,
Processor scheduling,
Dynamic voltage scaling,
Energy consumption,
Computer science,
Energy efficiency,
Dynamic scheduling,
Runtime,
Load management,
Performance evaluation"
Validation of nonrigid image registration using finite-element methods: application to breast MR images,"Presents a novel method for validation of nonrigid medical image registration. This method is based on the simulation of physically plausible, biomechanical tissue deformations using finite-element methods. Applying a range of displacements to finite-element models of different patient anatomies generates model solutions which simulate gold standard deformations. From these solutions, deformed images are generated with a range of deformations typical of those likely to occur in vivo. The registration accuracy with respect to the finite-element simulations is quantified by co-registering the deformed images with the original images and comparing the recovered voxel displacements with the biomechanically simulated ones. The functionality of the validation method is demonstrated for a previously described nonrigid image registration technique based on free-form deformations using B-splines and normalized mutual information as a voxel similarity measure, with an application to contrast-enhanced magnetic resonance mammography image pairs. The exemplar nonrigid registration technique is shown to be of subvoxel accuracy on average for this particular application. The validation method presented here is an important step toward more generic simulations of biomechanically plausible tissue deformations and quantification of tissue motion recovery using nonrigid image registration. It will provide a basis for improving and comparing different nonrigid registration techniques for a diversity of medical applications, such as intrasubject tissue deformation or motion correction in the brain, liver or heart.","Image registration,
Finite element methods,
Breast,
Deformable models,
Medical simulation,
Biomedical imaging,
Anatomy,
Gold,
Image generation,
In vivo"
Load-balanced clustering of wireless sensor networks,"Wireless sensor networks have potential to monitor environments for both military and civil applications. Due to inhospitable conditions these sensors are not always deployed uniformly ion the area of interest. Since sensors are generally constrained in on-board energy supply, efficient management of the network is crucial to extend the life of the sensors. Sensors' energy cannot support long haul communication to reach a remote command site and thus requires many levels of hops or a gateway to forward the data on behalf of the sensor. In this paper, we propose an algorithm to network these sensors in to well define clusters with less energy-constrained gateway nodes acting as cluster-heads, and balance load among these gateways. Simulation results show how our approach can balance the load and improve the lifetime of the system.","Wireless sensor networks,
Sensor systems,
Sensor phenomena and characterization,
Sensor fusion,
Computer science,
Energy management,
Computerized monitoring,
Military computing,
Application software,
Clustering algorithms"
Selective removal of impulse noise based on homogeneity level information,"We propose a decision-based, signal-adaptive median filtering algorithm for removal of impulse noise. Our algorithm achieves accurate noise detection and high SNR measures without smearing the fine details and edges in the image. The notion of homogeneity level is defined for pixel values based on their global and local statistical properties. The cooccurrence matrix technique is used to represent the correlations between a pixel and its neighbors, and to derive the upper and lower bound of the homogeneity level. Noise detection is performed at two stages: noise candidates are first selected using the homogeneity level, and then a refining process follows to eliminate false detections. The noise detection scheme does not use a quantitative decision measure, but uses qualitative structural information, and it is not subject to burdensome computations for optimization of the threshold values. Empirical results indicate that our scheme performs significantly better than other median filters, in terms of noise suppression and detail preservation.",
Decimal floating-point: algorism for computers,"Decimal arithmetic is the norm in human calculations, and human centric applications must use a decimal floating point arithmetic to achieve the same results. Initial benchmarks indicate that some applications spend 50% to 90% of their time in decimal processing, because software decimal arithmetic suffers a 100/spl times/ to 1000/spl times/ performance penalty over hardware. The need for decimal floating point in hardware is urgent. Existing designs, however, either fail to conform to modern standards or are incompatible with the established rules of decimal arithmetic. We introduce a new approach to decimal floating point which not only provides the strict results which are necessary for commercial applications but also meets the constraints and requirements of the IEEE 854 standard. A hardware implementation of this arithmetic is in development, and it is expected that this will significantly accelerate a wide variety of applications.","Hardware,
Digital arithmetic,
Floating-point arithmetic,
Application software,
Humans,
Computer science,
Software performance,
Acceleration,
Business,
Data processing"
Measuring tortuosity of the intracerebral vasculature from MRA images,"The clinical recognition of abnormal vascular tortuosity, or excessive bending, twisting, and winding, is important to the diagnosis of many diseases. Automated detection and quantitation of abnormal vascular tortuosity from three-dimensional (3-D) medical image data would, therefore, be of value. However, previous research has centered primarily upon two-dimensional (2-D) analysis of the special subset of vessels whose paths are normally close to straight. This report provides the first 3-D tortuosity analysis of clusters of vessels within the normally tortuous intracerebral circulation. We define three different clinical patterns of abnormal tortuosity. We extend into 3-D two tortuosity metrics previously reported as useful in analyzing 2-D images and describe a new metric that incorporates counts of minima of total curvature. We extract vessels from MRA data, map corresponding anatomical regions between sets of normal patients and patients with known pathology, and evaluate the three tortuosity metrics for ability to detect each type of abnormality within the region of interest. We conclude that the new tortuosity metric appears to be the most effective in detecting several types of abnormalities. However, one of the other metrics, based on a sum of curvature magnitudes, may be more effective in recognizing tightly coiled, ""corkscrew"" vessels associated with malignant tumors.","Diseases,
Biomedical imaging,
Two dimensional displays,
Image analysis,
Malignant tumors,
Shape measurement,
Associate members,
Image recognition,
Medical diagnostic imaging,
Data mining"
Platelets: a multiscale approach for recovering edges and surfaces in photon-limited medical imaging,"The nonparametric multiscale platelet algorithms presented in this paper, unlike traditional wavelet-based methods, are both well suited to photon-limited medical imaging applications involving Poisson data and capable of better approximating edge contours. This paper introduces platelets, localized functions at various scales, locations, and orientations that produce piecewise linear image approximations, and a new multiscale image decomposition based on these functions. Platelets are well suited for approximating images consisting of smooth regions separated by smooth boundaries. For smoothness measured in certain Holder classes, it is shown that the error of m-term platelet approximations can decay significantly faster than that of m-term approximations in terms of sinusoids, wavelets, or wedgelets. This suggests that platelets may outperform existing techniques for image denoising and reconstruction. Fast, platelet-based, maximum penalized likelihood methods for photon-limited image denoising, deblurring and tomographic reconstruction problems are developed. Because platelet decompositions of Poisson distributed images are tractable and computationally efficient, existing image reconstruction methods based on expectation-maximization type algorithms can be easily enhanced with platelet techniques. Experimental results suggest that platelet-based methods can outperform standard reconstruction methods currently in use in confocal microscopy, image restoration, and emission tomography.","Biomedical imaging,
Piecewise linear approximation,
Image reconstruction,
Image denoising,
Piecewise linear techniques,
Image decomposition,
Single photon emission computed tomography,
Distributed computing,
Reconstruction algorithms,
Microscopy"
ERPs evoked by different matrix sizes: implications for a brain computer interface (BCI) system,"A brain-computer interface (BCI) system may allow a user to communicate by selecting one of many options. These options may be presented in a matrix. Larger matrices allow a larger vocabulary, but require more time for each selection. In this study, subjects were asked to perform a target detection task using matrices appropriate for a BCI. The study sought to explore the relationship between matrix size and EEG measures, target detection accuracy, and user preferences. Results indicated that larger matrices evoked a larger P300 amplitude, and that matrix size did not significantly affect performance or preferences.","Enterprise resource planning,
Brain computer interfaces,
Vocabulary,
Object detection,
Size measurement,
Robustness,
Electroencephalography,
Real time systems,
Parametric study,
Cognitive science"
"Service overlay networks: SLAs, QoS, and bandwidth provisioning","We advocate the notion of service overlay network (SON) as an effective means to address some of the issues, in particular, end-to-end quality of service (QoS), plaguing the current Internet, and to facilitate the creation and deployment of value-added Internet services such as VoIP, Video-on-Demand, and other emerging QoS-sensitive services. The SON purchases bandwidth with certain QoS guarantees from the individual network domains via bilateral service level agreement (SLA) to build a logical end-to-end service delivery infrastructure on top of the existing data transport networks. Via a service contract, users directly pay the SON for using the value-added services provided by the SON. In this paper, we study the bandwidth provisioning problem for a SON which buys bandwidth from the underlying network domains to provide end-to-end value-added QoS sensitive services such as VoIP and Video-on-Demand. A key problem in the SON deployment is the problem of bandwidth provisioning, which is critical to cost recovery in deploying and operating the value-added services over the SON. The paper is devoted to the study of this problem. We formulate the bandwidth provisioning problem mathematically, taking various factors such as SLA, service QoS, traffic demand distributions, and bandwidth costs. Analytical models and approximate solutions are developed for both static and dynamic bandwidth provisioning. Numerical studies are also performed to illustrate the properties of the proposed solutions and demonstrate the effect of traffic demand distributions and bandwidth costs on SON bandwidth provisioning.","Bandwidth,
Quality of service,
Web and internet services,
IP networks,
Costs,
Telecommunication traffic,
Traffic control,
Computer science,
Contracts,
Analytical models"
A policy language for a pervasive computing environment,"We describe a policy language designed for pervasive computing applications that is based on deontic concepts and grounded in a semantic language. The pervasive computing environments under consideration are those in which people and devices are mobile and use various wireless networking technologies to discover and access services and devices in their vicinity. Such pervasive environments lend themselves to policy-based security due to their extremely dynamic nature. Using policies allows the security functionality to be modified without changing the implementation of the entities involved. However, along with being extremely dynamic, these environments also tend to span several domains and be made up of entities of varied capabilities. A policy language for environments of this sort needs to be very expressive but lightweight and easily extensible. We demonstrate the feasibility of our policy language in pervasive environments through a prototype used as part of a secure pervasive system.","Pervasive computing,
Information security,
Mobile computing,
Communication system security,
Handheld computers,
Personal digital assistants,
Natural languages,
Computer science,
Application software,
Prototypes"
Segmenting foreground objects from a dynamic textured background via a robust Kalman filter,"The algorithm presented aims to segment the foreground objects in video (e.g., people) given time-varying, textured backgrounds. Examples of time-varying backgrounds include waves on water, clouds moving, trees waving in the wind, automobile traffic, moving crowds, escalators, etc. We have developed a novel foreground-background segmentation algorithm that explicitly accounts for the nonstationary nature and clutter-like appearance of many dynamic textures. The dynamic texture is modeled by an autoregressive moving average model (ARMA). A robust Kalman filter algorithm iteratively estimates the intrinsic appearance of the dynamic texture, as well as the regions of the foreground objects. Preliminary experiments with this method have demonstrated promising results.","Robustness,
Vehicle dynamics,
Autoregressive processes,
Object detection,
Iterative algorithms,
Statistical distributions,
Layout,
Computer science,
Clouds,
Automobiles"
An Atlas framework for scalable mapping,"This paper describes Atlas, a hybrid metrical/topological approach to SLAM that achieves efficient mapping of large-scale environments. The representation is a graph of coordinate frames, with each vertex in the graph representing a local frame, and each edge representing the transformation between adjacent frames. In each frame, we build a map that captures the local environment and the current robot pose along with the uncertainties of each. Each map's uncertainties are modeled with respect to its own frame. Probabilities of entities with respect to arbitrary frames are generated by following a path formed by the edges between adjacent frames, computed via Dijkstra's shortest path algorithm. Loop closing is achieved via an efficient map matching algorithm. We demonstrate the technique running in real-time in a large indoor structured environment (2.2 km path length) with multiple nested loops using laser or ultrasonic ranging sensors.","Uncertainty,
Simultaneous localization and mapping,
Marine technology,
Paper technology,
Large-scale systems,
Robot kinematics,
Computer science,
Oceans,
Communications technology,
Underwater communication"
Fitness-distance-ratio based particle swarm optimization,"This paper presents a modification of the particle swarm optimization algorithm (PSO) intended to combat the problem of premature convergence observed in many applications of PSO. The proposed new algorithm moves particles towards nearby particles of higher fitness, instead of attracting each particle towards just the best position discovered so far by any particle. This is accomplished by using the ratio of the relative fitness and the distance of other particles to determine the direction in which each component of the particle position needs to be changed. The resulting algorithm (FDR-PSO) is shown to perform significantly better than the original PSO algorithm and some of its variants, on many different benchmark optimization problems. Empirical examination of the evolution of the particles demonstrates that the convergence of the algorithm does not occur at an early phase of particle evolution, unlike PSO. Avoiding premature convergence allows FDR-PSO to continue search for global optima in difficult multimodal optimization problems.","Convergence,
Cognition,
Particle swarm optimization,
Performance analysis,
Application software,
Problem-solving,
Power engineering and energy,
Computer science,
Animals,
Evolutionary computation"
Localized minimum-energy broadcasting in ad-hoc networks,"In the minimum energy broadcasting problem, each node can adjust its transmission power in order to minimize total energy consumption but still enable a message originated from a source node to reach all the other nodes in an ad-hoc wireless network. In all existing solutions each node requires global network information (including distances between any two neighboring nodes in the network) in order to decide its own transmission radius. We describe a new localized protocol where each node requires only the knowledge of its distance to all neighboring nodes and distances between its neighboring nodes (or, alternatively, geographic position of itself and its neighboring nodes). In addition to using only local information, our protocol is shown experimentally to be comparable to the best known globalized BIP solution. Our solutions are based on the use of relative neighborhood graph, which preserves connectivity and is defined in localized manner.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Protocols,
Network topology,
Energy consumption,
Wireless sensor networks,
Computer science,
Wireless networks,
Batteries"
Segmentation of prostate boundaries from ultrasound images using statistical shape model,"Presents a statistical shape model for the automatic prostate segmentation in transrectal ultrasound images. A Gabor filter bank is first used to characterize the prostate boundaries in ultrasound images in both multiple scales and multiple orientations. The Gabor features are further reconstructed to be invariant to the rotation of the ultrasound probe and incorporated in the prostate model as image attributes for guiding the deformable segmentation. A hierarchical deformation strategy is then employed, in which the model adaptively focuses on the similarity of different Gabor features at different deformation stages using a multiresolution technique, i.e., coarse features first and fine features later. A number of successful experiments validate the algorithm.","Image segmentation,
Ultrasonic imaging,
Shape,
Deformable models,
Prostate cancer,
Biopsy,
Biomedical imaging,
Image analysis,
Radiology,
Biomedical computing"
A study on reduced support vector machines,"Recently the reduced support vector machine (RSVM) was proposed as an alternate of the standard SVM. Motivated by resolving the difficulty on handling large data sets using SVM with nonlinear kernels, it preselects a subset of data as support vectors and solves a smaller optimization problem. However, several issues of its practical use have not been fully discussed yet. For example, we do not know if it possesses comparable generalization ability as the standard SVM. In addition, we would like to see for how large problems RSVM outperforms SVM on training time. In this paper we show that the RSVM formulation is already in a form of linear SVM and discuss four RSVM implementations. Experiments indicate that in general the test accuracy of RSVM are a little lower than that of the standard SVM. In addition, for problems with up to tens of thousands of data, if the percentage of support vectors is not high, existing implementations for SVM is quite competitive on the training time. Thus, from this empirical study, RSVM will be mainly useful for either larger problems or those with many support vectors. Experiments in this paper also serve as comparisons of: 1) different implementations for linear SVM and 2) standard SVM using linear and quadratic cost functions.","Support vector machines,
Kernel,
Testing,
Cost function,
Matrix decomposition,
Training data,
Quadratic programming,
Computer science,
Large-scale systems,
Convergence"
Dynamic weighted majority: a new ensemble method for tracking concept drift,"Algorithms for tracking concept drift are important for many applications. We present a general method based on the weighted majority algorithm for using any online learner for concept drift. Dynamic weighted majority (DWM) maintains an ensemble of base learners, predicts using a weighted-majority vote of these ""experts"", and dynamically creates and deletes experts in response to changes in performance. We empirically evaluated two experimental systems based on the method using incremental naive Bayes and incremental tree inducer [ITI] as experts. For the sake of comparison, we also included Blum's implementation of weighted majority. On the STAGGER concepts and on the SEA concepts, results suggest that the ensemble method learns drifting concepts almost as well as the base algorithms learn each concept individually. Indeed, we report the best overall results for these problems to date.","Application software,
Voting,
Target tracking,
Data mining,
Algorithm design and analysis,
Computer science,
Noise robustness,
Training data,
Computer security,
User interfaces"
A transformation based algorithm for reversible logic synthesis,"A digital combinational logic circuit is reversible if it maps each input pattern to a unique output pattern. Such circuits are of interest in quantum computing, optical computing, nanotechnology and low-power CMOS design. Synthesis approaches are not well developed for reversible circuits even for small numbers of inputs and outputs. In this paper, a transformation based algorithm for the synthesis of such a reversible circuit in terms of n /spl times/ n Toffoli gates is presented. Initially, a circuit is constructed by a single pass through the specification with minimal look-ahead and no back-tracking. Reduction rules are then applied by simple template matching. The method produces very good results for larger problems.","Circuit synthesis,
Quantum computing,
Optical computing,
Computer science,
Niobium,
Nanotechnology,
Optical design,
Logic design,
Permission,
Combinational circuits"
A subspace identification extension to the phase correlation method [MRI application],"The phase correlation method (PCM) is known to provide straightforward estimation of rigid translational motion between two images. It is often claimed that the original method is best suited to identify integer pixel displacements, which has prompted the development of numerous subpixel displacement identification methods. However, the fact that the phase correlation matrix is rank one for a noise-free rigid translation model is often overlooked. This property leads to the low complexity subspace identification technique presented here. The combination of noninteger pixel displacement identification without interpolation, robustness to noise, and limited computational complexity make this approach a very attractive extension of the PCM. In addition, this approach is shown to be complementary with other subpixel phase correlation based techniques.","Correlation,
Phase change materials,
Magnetic resonance imaging,
Fourier transforms,
Motion estimation,
Interpolation,
Image registration,
Biomedical imaging,
Degradation,
Phase estimation"
Event-based traceability for managing evolutionary change,"Although the benefits of requirements traceability are widely recognized, the actual practice of maintaining a traceability scheme is not always entirely successful. The traceability infrastructure underlying a software system tends to erode over its lifetime, as time-pressured practitioners fail to consistently maintain links and update impacted artifacts each time a change occurs, even with the support of automated systems. This paper proposes a new method of traceability based upon event-notification and is applicable even in a heterogeneous and globally distributed development environment. Traceable artifacts are no longer tightly coupled but are linked through an event service, which creates an environment in which change is handled more efficiently, and artifacts and their related links are maintained in a restorable state. The method also supports enhanced project management for the process of updating and maintaining the system artifacts.",
Characterization of pinhole SPECT acquisition geometry,"A method is presented to estimate the acquisition geometry of a pinhole single photon emission computed tomography (SPECT) camera with a circular detector orbit. This information is needed for the reconstruction of tomographic images. The calibration uses the point source projection locations of a tomographic acquisition of three point sources located at known distances from each other. It is shown that this simple phantom provides the necessary and sufficient information for the proposed calibration method. The knowledge of two of the distances between the point sources proves to be essential. The geometry is estimated by fitting analytically calculated projections to the measured ones, using a simple least squares Powell algorithm. Some mild a priori knowledge is used to constrain the solutions of the fit. Several of the geometrical parameters are however highly correlated. The effect of these correlations on the reconstructed images is evaluated in simulation studies and related to the estimation accuracy. The highly correlated detector tilt and electrical shift are shown to be the critical parameters for accurate image reconstruction. The performance of the algorithm is finally demonstrated by phantom measurements. The method is based on a single SPECT scan of a simple calibration phantom, executed immediately after the actual SPECT acquisition. The method is also applicable to cone-beam SPECT and X-ray CT.","Image reconstruction,
Calibration,
Imaging phantoms,
Detectors,
Extraterrestrial measurements,
Computational geometry,
Single photon emission computed tomography,
Cameras,
Algorithm design and analysis,
Least squares approximation"
Sentiment analyzer: extracting sentiments about a given topic using natural language processing techniques,"We present sentiment analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by relationship analysis. SA utilizes two linguistic resources for the analysis: the sentiment lexicon and the sentiment pattern database. The performance of the algorithms was verified on online product review articles (""digital camera"" and ""music"" reviews), and more general documents including general Webpages and news articles.","Natural language processing,
Data mining,
Spatial databases,
Computer science,
Feature extraction,
Pattern analysis,
Web pages,
Product development,
Marketing management,
Text analysis"
Properties of an adaptive archiving algorithm for storing nondominated vectors,"Search algorithms for Pareto optimization are designed to obtain multiple solutions, each offering a different trade-off of the problem objectives. To make the different solutions available at the end of an algorithm run, procedures are needed for storing them, one by one, as they are found. In a simple case, this may be achieved by placing each point that is found into an ""archive"" which maintains only nondominated points and discards all others. However, even a set of mutually nondominated points is potentially very large, necessitating a bound on the archive's capacity. But with such a bound in place, it is no longer obvious which points should be maintained and which discarded; we would like the archive to maintain a representative and well-distributed subset of the points generated by the search algorithm, and also that this set converges. To achieve these objectives, we propose an adaptive archiving algorithm, suitable for use with any Pareto optimization algorithm, which has various useful properties as follows. It maintains an archive of bounded size, encourages an even distribution of points across the Pareto front, is computationally efficient, and we are able to prove a form of convergence. The method proposed here maintains evenness, efficiency, and cardinality, and provably converges under certain conditions but not all. Finally, the notions underlying our convergence proofs support a new way to rigorously define what is meant by ""good spread of points"" across a Pareto front, in the context of grid-based archiving schemes. This leads to proofs and conjectures applicable to archive sizing and grid sizing in any Pareto optimization algorithm maintaining a grid-based archive.","Pareto optimization,
Convergence,
Distributed computing,
Evolutionary computation,
Algorithm design and analysis,
Stochastic processes,
Computer science"
An infrastructure for adaptive dynamic optimization,"Dynamic optimization is emerging as a promising approach to overcome many of the obstacles of traditional static compilation. But while there are a number of compiler infrastructures for developing static optimizations, there are very few for developing dynamic optimizations. We present a framework for implementing dynamic analyses and optimizations. We provide an interface for building external modules, or clients, for the DynamoRIO dynamic code modification system. This interface abstracts away many low-level details of the DynamoRIO runtime system while exposing a simple and powerful, yet efficient and lightweight API. This is achieved by restricting optimization units to linear streams of code and using adaptive levels of detail for representing instructions. The interface is not restricted to optimization and can be used for instrumentation, profiling, dynamic translation, etc. To demonstrate the usefulness and effectiveness of our framework, we implemented several optimizations. These improve the performance of some applications by as much as 40% relative to native execution. The average speedup relative to base DynamoRIO performance is 12%.","Optimizing compilers,
Runtime,
Laboratories,
Computer science,
Abstracts,
Instruments,
Application software,
Performance analysis,
Software performance,
Software libraries"
Computing aggregates for monitoring wireless sensor networks,"Wireless sensor networks involve very large numbers of small, low-power, wireless devices. Given their unattended nature, and their potential applications in harsh environments, we need a monitoring infrastructure that indicates system failures and resource depletion. We describe an architecture for sensor network monitoring, then focus on one aspect of this architecture: continuously computing aggregates (sum, average, count) of network properties (loss rates, energy-levels etc., packet counts). Our contributions are two-fold. First, we propose a novel tree construction algorithm that enables energy-efficient computation of some classes of aggregates. Second, we show through actual implementation and experiments that wireless communication artifacts in even relatively benign environments can significantly impact the computation of these aggregate properties. In some cases, without careful attention to detail, the relative error in the computed aggregates can be as much as 50%. However, by carefully discarding links with heavy packet loss and asymmetry, we can improve accuracy by an order of magnitude.","Computer networks,
Aggregates,
Wireless sensor networks,
Condition monitoring,
Computerized monitoring,
Energy efficiency,
Protocols,
Computer science,
Energy states,
Application software"
Segmentation and analysis of the human airway tree from three-dimensional X-ray CT images,"The lungs exchange air with the external environment via the pulmonary airways. Computed tomography (CT) scanning can be used to obtain detailed images of the pulmonary anatomy, including the airways. These images have been used to measure airway geometry, study airway reactivity, and guide surgical interventions. Prior to these applications, airway segmentation can be used to identify the airway lumen in the CT images. Airway tree segmentation can be performed manually by an image analyst, but the complexity of the tree makes manual segmentation tedious and extremely time-consuming. We describe a fully automatic technique for segmenting the airway tree in three-dimensional (3-D) CT images of the thorax. We use grayscale morphological reconstruction to identify candidate airways on CT slices and then reconstruct a connected 3-D airway tree. After segmentation, we estimate airway branchpoints based on connectivity changes in the reconstructed tree. Compared to manual analysis on 3-mm-thick electron-beam CT images, the automatic approach has an overall airway branch detection sensitivity of approximately 73%.","Image segmentation,
Image analysis,
Humans,
X-ray imaging,
Computed tomography,
Image reconstruction,
Lungs,
Anatomy,
Geometry,
Surgery"
Provisioning of survivable multicast sessions against single link failures in optical WDM mesh networks,"In this paper, we investigate approaches and algorithms for establishing a multicast session in a mesh network while protecting the session against any single link failure, e.g., a fiber cut in an optical network. First, we study these approaches and algorithms to protect a single multicast tree in a mesh network and then extend it to dynamically provision survivable multicast connections (where connections come and go) in an optical wavelength-division multiplexing (WDM) network. We propose two new and efficient approaches for protecting a multicast session: 1) segment protection in which we protect each segment in the primary tree separately (rather than the entire tree) and allow these backup segments to share edges with the other existing primary and backup segments and 2) the path-pair protection in which we find a path-pair (disjoint primary and backup paths) to each destination and allow a new path pair to share edges with already-found path pairs. Unlike previous schemes, such as finding link-disjoint trees and arc-disjoint trees, our new schemes 1) guarantee a solution where previous schemes fail and 2) find an efficient solution requiring less network resources. We study these approaches and algorithms systematically, starting with the existing approaches such as fully link-disjoint and arc-disjoint trees and then presenting our new and efficient proposed approaches, such as segment-disjoint and path-disjoint schemes for protecting multicast connections. Our most efficient algorithm, based on the path-pair protection scheme, called optimal path-pair-based shared disjoint paths (OPP-SDP) algorithm, finds a solution if such a solution exists and outperforms all the other schemes in terms of network cost. We also show that OPP-SDP performs close to the optimal solution obtained by solving a mathematical formulation of the problem expressed as an integer linear program. Building upon the study on protecting a single tree, we perform simulations, employing the above protection schemes, to study dynamic provisioning of survivable multicast sessions (where sessions come and go) in a WDM mesh network. Our simulations show that the most efficient scheme, OPP-SDP, has minimum blocking probability.","Intelligent networks,
Optical fiber networks,
Wavelength division multiplexing,
WDM networks,
Mesh networks,
Protection,
Optical fiber communication,
Multicast algorithms,
Cost function,
Computer science"
The ICSI Meeting Corpus,"We have collected a corpus of data from natural meetings that occurred at the International Computer Science Institute (ICSI) in Berkeley, California over the last three years. The corpus contains audio recorded simultaneously from head-worn and table-top microphones, word-level transcripts of meetings, and various metadata on participants, meetings, and hardware. Such a corpus supports work in automatic speech recognition, noise robustness, dialog modeling, prosody, rich transcription, information retrieval, and more. We present details on the contents of the corpus, as well as rationales for the decisions that led to its configuration. The corpus were delivered to the Linguistic Data Consortium (LDC).","Speech recognition,
Speech processing,
Audio recording,
Microphones"
Support vector learning for fuzzy rule-based classification systems,"To design a fuzzy rule-based classification system (fuzzy classifier) with good generalization ability in a high dimensional feature space has been an active research topic for a long time. As a powerful machine learning approach for pattern recognition problems, the support vector machine (SVM) is known to have good generalization ability. More importantly, an SVM can work very well on a high- (or even infinite) dimensional feature space. This paper investigates the connection between fuzzy classifiers and kernel machines, establishes a link between fuzzy rules and kernels, and proposes a learning algorithm for fuzzy classifiers. We first show that a fuzzy classifier implicitly defines a translation invariant kernel under the assumption that all membership functions associated with the same input variable are generated from location transformation of a reference function. Fuzzy inference on the IF-part of a fuzzy rule can be viewed as evaluating the kernel function. The kernel function is then proven to be a Mercer kernel if the reference functions meet a certain spectral requirement. The corresponding fuzzy classifier is named positive definite fuzzy classifier (PDFC). A PDFC can be built from the given training samples based on a support vector learning approach with the IF-part fuzzy rules given by the support vectors. Since the learning process minimizes an upper bound on the expected risk (expected prediction error) instead of the empirical risk (training error), the resulting PDFC usually has good generalization. Moreover, because of the sparsity properties of the SVMs, the number of fuzzy rules is irrelevant to the dimension of input space. In this sense, we avoid the ""curse of dimensionality."" Finally, PDFCs with different reference functions are constructed using the support vector learning approach. The performance of the PDFCs is illustrated by extensive experimental results. Comparisons with other methods are also provided.","Fuzzy systems,
Kernel,
Input variables,
Support vector machines,
Support vector machine classification,
Computer science,
Machine learning,
Pattern classification,
Parameter estimation,
Pattern recognition"
Toward secure key distribution in truly ad-hoc networks,"Ad-hoc networks - and in particular wireless mobile ad-hoc networks $have unique characteristics and constraints that make traditional cryptographic mechanisms and assumptions inappropriate. In particular it may not be warranted to assume pre-existing shared secrets between members of the network or the presence of a common PKI. Thus, the issue of key distribution in ad-hoc networks represents an important problem. Unfortunately, this issue has been largely ignored; as an example, most protocols for secure ad-hoc routing assume that key distribution has already taken place. Traditional key distribution schemes either do not apply in an ad-hoc scenario or are not efficient enough for small, resource-constrained devices. We propose to combine efficient techniques from identity-based (ID-based) and threshold cryptography to provide a mechanism that enables flexible and efficient key distribution while respecting the constraints of ad-hoc networks. We also discuss the available mechanisms and their suitability for the proposed task.",
Estimating 3D hand pose from a cluttered image,"A method is proposed that can generate a ranked list of plausible three-dimensional hand configurations that best match an input image. Hand pose estimation is formulated as an image database indexing problem, where the closest matches for an input hand image are retrieved from a large database of synthetic hand images. In contrast to previous approaches, the system can function in the presence of clutter, thanks to two novel clutter-tolerant indexing methods. First, a computationally efficient approximation of the image-to-model chamfer distance is obtained by embedding binary edge images into a high-dimensional Euclidean space. Second, a general-purpose, probabilistic line matching method identifies those line segment correspondences between model and input images that are the least likely to have occurred by chance. The performance of this clutter tolerant approach is demonstrated in quantitative experiments with hundreds of real hand images.","Image databases,
Indexing,
Impedance matching,
Image retrieval,
Image segmentation,
Information retrieval,
Computer vision,
Computer science,
Embedded computing,
Humans"
Retrospective evaluation of intersubject brain registration,"Although numerous methods to register brains of different individuals have been proposed, no work has been done, as far as we know, to evaluate and objectively compare the performances of different nonrigid (or elastic) registration methods on the same database of subjects. In this paper, we propose an evaluation framework, based on global and local measures of the relevance of the registration. We have chosen to focus more particularly on the matching of cortical areas, since intersubject registration methods are dedicated to anatomical and functional normalization, and also because other groups have shown the relevance of such registration methods for deep brain structures. Experiments were conducted using 6 methods on a database of 18 subjects. The global measures used show that the quality of the registration is directly related to the transformation's degrees of freedom. More surprisingly, local measures based on the matching of cortical sulci did not show significant differences between rigid and non rigid methods.","Brain,
Photometry,
Databases,
Data mining,
Performance evaluation,
Magnetic resonance imaging,
Councils,
Geographic Information Systems,
Cities and towns"
Dynamic virtual clusters in a grid site manager,"This paper presents new mechanisms for dynamic resource management in a cluster manager called Cluster-on-Demand (COD). COD allocates servers from a common pool to multiple virtual clusters (vclusters), with independently configured software environments, name spaces, user access controls, and network storage volumes. We present experiments using the popular Sun GridEngine batch scheduler to demonstrate that dynamic virtual clusters are an enabling abstraction for advanced resource management in computing utilities and grids. In particular, they support dynamic, policy-based cluster sharing between local users and hosted Grid services, resource reservation and adaptive provisioning, scavenging of the idle resources, and dynamic instantiation of Grid services. These goals are achieved in a direct and general way through a new set of fundamental cluster management functions, with minimal impact on the Grid middleware itself.","Resource management,
Grid computing,
Middleware,
Access control,
Sun,
Processor scheduling,
Dynamic scheduling,
Application software,
Computer science,
Network servers"
Identifying important features for intrusion detection using support vector machines and neural networks,"Intrusion detection is a critical component of secure information systems. This paper addresses the issue of identifying important input features in building an intrusion detection system (IDS). Since elimination of the insignificant and/or useless inputs leads to a simplification of the problem, faster and more accurate detection may result. Feature ranking and selection, therefore, is an important issue in intrusion detection. We apply the technique of deleting one feature at a time to perform experiments on SVMs and neural networks to rank the importance of input features for the DARPA collected intrusion data. Important features for each of the 5 classes of intrusion patterns in the DARPA data are identified. It is shown that SVM-based and neural network based IDSs using a reduced number of features can deliver enhanced or comparable performance. An IDS for class-specific detection based on five SVMs is proposed.","Computer vision,
Intrusion detection,
Support vector machines,
Neural networks,
Computer crime,
TCPIP,
Local area networks,
Computer science,
Information systems,
Telecommunication traffic"
Adaptive probabilistic search for peer-to-peer networks,"Peer-to-peer networks are gaining increasing attention from both the scientific and the large Internet user community. Popular applications utilizing this new technology offer many attractive features to a growing number of users. At the heart of such networks lies the search algorithm. Proposed methods either depend on the network-disastrous flooding and its variations or utilize various indices too expensive to maintain. We describe an adaptive, bandwidth-efficient algorithm for search in unstructured peer-to-peer networks, the adaptive probabilistic search method (APS). Our scheme utilizes feedback from previous searches to probabilistically guide future ones. It performs efficient object discovery while inducing zero overhead over dynamic network operations. Extensive simulation results show that APS achieves high success rates, increased number of discovered objects, very low bandwidth consumption and adaptation to changing topologies.",
H/sub /spl infin// state-feedback controller design for discrete-time fuzzy systems using fuzzy weighting-dependent Lyapunov functions,"For discrete-time Takagi-Sugeno (TS) fuzzy systems, we propose an H/sub /spl infin// state-feedback fuzzy controller associated with a fuzzy weighting-dependent Lyapunov function. The controller, which is designed via parameterized linear matrix inequalities (PLMIs), employs not only the current-time but also the one-step-past information on the time-varying fuzzy weighting functions. Appropriately selecting the structures of variables in the PLMIs allows us to find an LMI formulation as a special case.","Fuzzy systems,
Control systems,
Fuzzy control,
Lyapunov method,
Linear matrix inequalities,
Stability analysis,
Control system synthesis,
Takagi-Sugeno model,
Computer science education,
Control engineering education"
Detection and handling of MAC layer misbehavior in wireless networks,,"Intelligent networks,
Wireless networks,
Media Access Protocol,
Access protocols,
Counting circuits,
Data communication,
Wireless application protocol,
Bandwidth,
Computer science,
Distributed computing"
Documenting software architectures: views and beyond,"This lecture maps the concepts and templates explored in this tutorial with well-known architectural prescriptions, including the 4+1 approach of the Rational Unified Process, the Siemens Four Views approach, and the ANSI/IEEE-1471-2000 recommended best practice for documenting architectures for software-intensive systems. The lecture concludes by re-capping the highlights of the tutorial, and asking for feedback.",
Building text classifiers using positive and unlabeled examples,"We study the problem of building text classifiers using positive and unlabeled examples. The key feature of this problem is that there is no negative example for learning. Recently, a few techniques for solving this problem were proposed in the literature. These techniques are based on the same idea, which builds a classifier in two steps. Each existing technique uses a different method for each step. We first introduce some new methods for the two steps, and perform a comprehensive evaluation of all possible combinations of methods of the two steps. We then propose a more principled approach to solving the problem based on a biased formulation of SVM, and show experimentally that it is more accurate than the existing techniques.","Text categorization,
Support vector machines,
Support vector machine classification,
Labeling,
Niobium,
Iterative algorithms,
Performance evaluation,
Computer science,
Biomedical engineering,
Sun"
Multiprocessor EDF and deadline monotonic schedulability analysis,"Schedulability tests are presented for preemptive earlier-deadline-first and deadline-monotonic scheduling of periodic or sporadic real-time tasks on a single-queue m-server system, in which the deadline of a task may be less than or equal to the task period. These results subsume and generalize several known utilization-based multiprocessor schedulability tests, and are derived via an independent proof.","Processor scheduling,
System testing,
Real time systems,
Multiprocessing systems,
Scheduling algorithm,
Computer science,
Sufficient conditions,
Delta modulation,
Writing"
Spacetime stereo: shape recovery for dynamic scenes,"This paper extends the traditional binocular stereo problem into the spacetime domain, in which a pair of video streams is matched simultaneously instead of matching pairs of images frame by frame. Almost any existing stereo algorithm may be extended in this manner simply by replacing the image matching term with a spacetime term. By utilizing both spatial and temporal appearance variation, this modification reduces ambiguity and increases accuracy. Three major applications for spacetime stereo are proposed in this paper. First, spacetime stereo serves as a general framework for structured light scanning and generates high quality depth maps for static scenes. Second, spacetime stereo is effective for a class of natural scenes, such as waving trees and flowing water, which have repetitive textures and chaotic behaviors and are challenging for existing stereo algorithms. Third, the approach is one of very few existing methods that can robustly reconstruct objects that are moving and deforming over time, achieved by use of oriented spacetime windows in the matching procedure. Promising experimental results in the above three scenarios are demonstrated.","Shape,
Layout,
Stereo vision,
Pixel,
Robustness,
Image reconstruction,
Computer science,
Streaming media,
Image matching,
Chaos"
Bayesian network-based trust model,"We propose a Bayesian network-based trust model. Since trust is multifaceted, even in the same context, agents still need to develop differentiated trust in different aspects of other agents' behaviors. The agent's needs are different in different situations. Depending on the situation, an agent may need to consider its trust in a specific aspect of another agent's capability or in a combination of multiple aspects. Bayesian networks provide a flexible method to present differentiated trust and combine different aspects of trust. A Bayesian network-based trust model is presented for a file sharing peer-to-peer application.","Bayesian methods,
Peer to peer computing,
Computer science,
Electronic commerce,
Distributed computing,
Authorization,
Authentication,
Protection,
Intelligent agent"
Convergence studies on iterative algorithms for image reconstruction,"We introduce a general iterative scheme for image reconstruction based on Landweber's method. In our configuration, a sequential block-iterative (SeqBI) version can be readily formulated from a simultaneous block-iterative (SimBI) version, and vice versa. This provides a mechanism to derive new algorithms from known ones. It is shown that some widely used iterative algorithms, such as the algebraic reconstruction technique (ART), simultaneous ART (SART), Cimmino's, and the recently designed diagonal weighting and component averaging algorithms, are special examples of the general scheme. We prove convergence of the general scheme under conditions more general than assumed in earlier studies, for its SeqBI and SimBI versions in the consistent and inconsistent cases, respectively. Our results suggest automatic relaxation strategies for the SeqBI and SimBI versions and characterize the dependence of the limit image on the initial guess. It is found that in all cases the limit is the sum of the minimum norm solution of a weighted least-squares problem and an oblique projection of the initial image onto the null space of the system matrix.","Convergence,
Iterative algorithms,
Image reconstruction,
Subspace constraints,
Computed tomography,
Magnetic resonance imaging,
Iterative methods,
Biomedical imaging,
Radiology,
Cities and towns"
Periodic resource model for compositional real-time guarantees,"We address the problem of providing compositional hard real-time guarantees in a hierarchy of schedulers. We first propose a resource model to characterize a periodic resource allocation and present exact schedulability conditions for our proposed resource model under the EDF and RM algorithms. Using the exact schedulability conditions, we then provide methods to abstract the timing requirements that a set of periodic tasks demands under the EDF and RM algorithms as a single periodic task. With these abstraction methods, for a hierarchy of schedulers, we introduce a composition method that derives the timing requirements of a parent scheduler from the timing requirements of its child schedulers in a compositional manner such that the timing requirement of the parent scheduler is satisfied, if and only if the timing requirements of its child schedulers are satisfied.","Processor scheduling,
Timing,
Scheduling algorithm,
Resource management,
Information science,
Algorithm design and analysis,
Computer interfaces,
Real time systems,
Partitioning algorithms"
The bridge test for sampling narrow passages with probabilistic roadmap planners,"Probabilistic roadmap (PRM) planners have been successful in path planning of robots with many degrees of freedom, but narrow passages in a robot's configuration space create significant difficulty for PRM planners. This paper presents a hybrid sampling strategy in the PRM framework for finding paths through narrow passages. A key ingredient of the new strategy is the bridge test, which boosts the sampling density inside narrow passages. The bridge test relies on simple tests of local geometry and can be implemented efficiently in high-dimensional configuration spaces. The strengths of the bridge test and uniform sampling complement each other naturally and are combined to generate the final hybrid sampling strategy. Our planner was tested on point robots and articulated robots in planar workspaces. Preliminary experiments show that the hybrid sampling strategy enables relatively small roadmaps to reliably capture the connectivity of configuration spaces with difficult narrow passages.","Bridges,
Testing,
Sampling methods,
Orbital robotics,
Path planning,
Computer science,
Road accidents,
Computational geometry,
Application software,
Virtual prototyping"
Implementation of a content-scanning module for an Internet firewall,A module has been implemented in Field Programmable Gate Array (FPGA) hardware that scans the content of Internet packets at Gigabits/second rates. All of the packet processing operations are performed using reconfigurable hardware within a single Xilinx Virtex XCV2000E FPGA. A set of layered protocol wrappers is used to parse the headers and payloads of packets for Internet protocol data. A content matching server automatically generates the Finite State Machines (FSMs) to search for regular expressions. The complete system is operated on the Field-programmable Port Extender (FPX) platform.,"Internet,
Field programmable gate arrays,
Hardware,
Protocols,
Payloads,
Automata,
Computer science,
Network servers,
Web server,
Intrusion detection"
Constructing test suites for interaction testing,"Software system faults are often caused by unexpected interactions among components. Yet the size of a test suite required to test all possible combinations of interactions can be prohibitive in even a moderately sized project. Instead, we may use pairwise or t-way testing to provide a guarantee that all pairs or t-way combinations of components are tested together This concept draws on methods used in statistical testing for manufacturing and has been extended to software system testing. A covering array, CA(N; t, k, v), is an N/spl times/k array on v symbols such that every N x t sub-array contains all ordered subsets from v symbols of size t at least once. The properties of these objects, however do not necessarily satisfy real software testing needs. Instead we examine a less studied object, the mixed level covering array and propose a new object, the variable strength covering array, which provides a more robust environment for software interaction testing. Initial results are presented suggesting that heuristic search techniques are more effective than some of the known greedy methods for finding smaller sized test suites. We present a discussion of an integrated approach for finding covering arrays and discuss how application of these techniques can be used to construct variable strength arrays.",
Modeling peer-peer file sharing systems,"Peer-peer networking has recently emerged as a new paradigm for building distributed networked applications. We develop simple mathematical models to explore and illustrate fundamental performance issues of peer-peer file sharing systems. The modeling framework introduced and the corresponding solution methods are flexible enough to accommodate different characteristics of such systems. Through the specification of model parameters, we apply our framework to three different peer-peer architectures: centralized indexing, distributed indexing with flooded queries, and distributed indexing with hashing directed queries. Using our model, we investigate the effects of system scaling, freeloaders, file popularity and availability on system performance. In particular, we observe that a system with distributed indexing and flooded queries cannot exploit the full capacity of peer-peer systems. We further show that peer-peer file sharing systems can tolerate a significant number of freeloaders without suffering much performance degradation. In many cases, freeloaders can benefit from the available spare capacity of peer-peer systems and increase overall system throughput. Our work shows that simple models coupled with efficient solution methods can be used to understand and answer questions related to the performance of peer-peer file sharing systems.","Peer to peer computing,
Indexing,
Network servers,
System performance,
Degradation,
Throughput,
Computer science,
Application software,
Mathematical model,
Availability"
"Hierarchical active shape models, using the wavelet transform","Active shape models (ASMs) are often limited by the inability of relatively few eigenvectors to capture the full range of biological shape variability. This paper presents a method that overcomes this limitation, by using a hierarchical formulation of active shape models, using the wavelet transform. The statistical properties of the wavelet transform of a deformable contour are analyzed via principal component analysis, and used as priors in the contour's deformation. Some of these priors reflect relatively global shape characteristics of the object boundaries, whereas, some of them capture local and high-frequency shape characteristics and, thus, serve as local smoothness constraints. This formulation achieves two objectives. First, it is robust when only a limited number of training samples is available. Second, by using local statistics as smoothness constraints, it eliminates the need for adopting ad hoc physical models, such as elasticity or other smoothness models, which do not necessarily reflect true biological variability. Examples on magnetic resonance images of the corpus callosum and hand contours demonstrate that good and fully automated segmentations can be achieved, even with as few as five training samples.","Active shape model,
Wavelet transforms,
Biological system modeling,
Wavelet analysis,
Principal component analysis,
Robustness,
Statistics,
Elasticity,
Magnetic resonance,
Image segmentation"
A special-purpose peer-to-peer file sharing system for mobile ad hoc networks,"Establishing peer-to-peer (P2P) file sharing for mobile ad hoc networks (MANET) requires the construction of a search algorithm for transmitting queries and search results as well as the development of a transfer protocol for downloading files matching a query. In this paper, we present a special-purpose system for searching and file transfer tailored to both the characteristics of MANET and the requirements of peer-to-peer file sharing. Our approach is based on an application layer overlay network. As innovative feature, overlay routes are set up on demand by the search algorithm, closely matching network topology and transparently aggregating redundant transfer paths on a per-file basis. The transfer protocol guarantees low transmission overhead and a high fraction of successful downloads by utilizing overlay routes. In a detailed ns-2 simulation study, we show that both the search algorithm and the transfer protocol outperform off-the-shelf approaches based on a P2P file sharing system for the wireline Internet, TCP and a MANET routing protocol.","Peer to peer computing,
Mobile ad hoc networks,
Network topology,
Routing protocols,
IP networks,
Computer science,
Internet,
Wireless networks,
Telecommunication traffic,
Mobile communication"
Traffic grooming for survivable WDM networks - shared protection,"We investigate the survivable traffic-grooming problem for optical mesh networks employing wavelength-division multiplexing (WDM). In the dynamic provisioning context, a typical connection request may require bandwidth less than that of a wavelength channel, and it may also require protection from network failures, typically fiber cuts. Based on a generic grooming-node architecture, we propose three approaches for grooming a connection request with shared protection: protection-at-lightpath level (PAL); mixed protection-at-connection level (MPAC); separate protection-at-connection level (SPAC). In shared-mesh protection, backup paths can share resources as long as their corresponding working paths are unlikely to fail simultaneously. These three schemes explore different ways of backup sharing, and they trade-off between wavelengths and grooming ports. Since the existing version of the problem for provisioning one connection request with shared protection is NP-complete, we propose effective heuristics. Under today's typical connection-bandwidth distribution where lower bandwidth connections outnumber higher bandwidth connections, we find the following: 1) it is beneficial to groom working paths and backup paths separately, as in PAL and SPAC; 2) separately protecting each individual connection, i.e., SPAC, yields the best performance when the number of grooming ports is sufficient; 3) protecting each specific lightpath, i.e., PAL, achieves the best performance when the number of grooming ports is moderate or small.","Telecommunication traffic,
Wavelength division multiplexing,
WDM networks,
Protection,
Bandwidth,
Optical fiber networks,
SONET,
Mesh networks,
Computer science,
Lead"
Wavelet coding of volumetric medical datasets,"Several techniques based on the three-dimensional (3-D) discrete cosine transform (DCT) have been proposed for volumetric data coding. These techniques fail to provide lossless coding coupled with quality and resolution scalability, which is a significant drawback for medical applications. This paper gives an overview of several state-of-the-art 3-D wavelet coders that do meet these requirements and proposes new compression methods exploiting the quadtree and block-based coding concepts, layered zero-coding principles, and context-based arithmetic coding. Additionally, a new 3-D DCT-based coding scheme is designed and used for benchmarking. The proposed wavelet-based coding algorithms produce embedded data streams that can be decoded up to the lossless level and support the desired set of functionality constraints. Moreover, objective and subjective quality evaluation on various medical volumetric datasets shows that the proposed algorithms provide competitive lossy and lossless compression results when compared with the state-of-the-art.","Image coding,
Biomedical imaging,
Discrete cosine transforms,
Scalability,
Medical diagnostic imaging,
Transform coding,
Medical services,
Biomedical equipment,
Decoding,
Propagation losses"
GUI ripping: reverse engineering of graphical user interfaces for testing,,"Graphical user interfaces,
Reverse engineering,
Automatic testing,
Software testing,
Humans,
Computer science,
Educational institutions,
Data mining,
Software engineering,
Application software"
JAM: a jammed-area mapping service for sensor networks,"Preventing denial-of-service attacks in wireless sensor networks is difficult primarily because of the limited resources available to network nodes and the ease with which attacks are perpetrated. Rather than jeopardize design requirements which call for simple, inexpensive, mass-producible devices, we propose a coping strategy that detects and maps jammed regions. We describe a mapping protocol for nodes that surround a jammer which allows network applications to reason about the region as an entity, rather than as a collection of broken links and congested nodes. This solution is enabled by a set of design principles: loose group semantics, eager eavesdropping, supremacy of local information, robustness to packet loss and failure, and early use of results. Performance results show that regions can be mapped in 1-5 seconds, fast enough for real-time response. With a moderately connected network, the protocol is robust to failure rates as high as 25 percent.","Jamming,
Wireless sensor networks,
Computer crime,
Protocols,
Robustness,
Intelligent sensors,
Military computing,
Batteries,
Computer science,
Distributed computing"
Counting people in crowds with a real-time network of simple image sensors,"Estimating the number of people in a crowded environment is a central task in civilian surveillance. Most vision-based counting techniques depend on detecting individuals in order to count, an unrealistic proposition in crowded settings. We propose an alternative approach that directly estimates the number of people. In our system, groups of image sensors segment foreground objects from the background, aggregate the resulting silhouettes over a network, and compute a planar projection of the scene's visual hull. We introduce a geometric algorithm that calculates bounds on the number of persons in each region of the projection, after phantom regions have been eliminated. The computational requirements scale well with the number of sensors and the number of people, and only limited amounts of data are transmitted over the network. Because of these properties, our system runs in real-time and can be deployed as an untethered wireless sensor network. We describe the major components of our system, and report preliminary experiments with our first prototype implementation.","Intelligent networks,
Image sensors,
Computer networks,
Computer science,
Prototypes,
Resource management,
Bandwidth,
Sensor systems,
Research and development,
Surveillance"
Hipikat: recommending pertinent software development artifacts,"A newcomer to a software project must typically come up-to-speed on a large, varied amount of information about the project before becoming productive. Assimilating this information in the open-source context is difficult because a newcomer cannot rely on the mentoring approach that is commonly used in traditional software developments. To help a newcomer to an open-source project become productive faster, we propose Hipikat, a tool that forms an implicit group memory from the information stored in a project's archives, and that recommends artifacts from the archives that are relevant to a task that a newcomer is trying to perform. To investigate this approach, we have instantiated the Hipikat tool for the Eclipse open-source project. In this paper we describe the Hipikat tool, we report on a qualitative study conducted with a Hipikat mock-up on a medium-sized in-house project, and we report on a case study in which Hipikat recommendations were evaluated for a task on Eclipse.",
Picking statistically valid and early simulation points,"Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to months to complete. To address this issue we have recently proposed using simulation points (found by only examining basic block execution frequency profiles) to increase the efficiency and accuracy of simulation. Simulation points are a small set of execution samples that when combined represent the complete execution of the program. We present a statistically driven algorithm for forming clusters from which simulation points are chosen, and examine algorithms for picking simulation points earlier in a program's execution-in order to significantly reduce fast-forwarding time during simulation. In addition, we show that simulation points can be used independent of the underlying architecture. The points are generated once for a program/input pair by only examining the code executed. We show the points accurately track hardware metrics (e.g., performance and cache miss rates) between different architecture configurations. They can therefore be used across different architecture configurations to allow a designer to make accurate trade-off decisions between different configurations.","Computational modeling,
Computer architecture,
Clustering algorithms,
Vectors,
Frequency,
Computer science,
Pipelines,
Computer simulation,
Hardware,
Application software"
Peculiarity oriented multidatabase mining,"Peculiarity rules are a new class of rules which can be discovered by searching relevance among a relatively small number of peculiar data. Peculiarity oriented mining in multiple data sources is different from, and complementary to, existing approaches for discovering new, surprising, and interesting patterns hidden in data. A theoretical framework for peculiarity oriented mining is presented. Within the proposed framework, we give a formal interpretation and comparison of three classes of rules, namely, association rules, exception rules, and peculiarity rules, as well as describe how to mine interesting peculiarity rules in multiple databases.","Data mining,
Association rules,
Transaction databases,
Object oriented databases,
Data analysis,
Performance analysis,
Computer science"
Cooperative peer groups in NICE,"A distributed scheme for trust inference in peer-to-peer networks is presented. Our work is in context of the NICE system, which is a platform for implementing cooperative applications over the Internet. We describe a technique for efficiently storing user reputation information in a completely decentralized manner, and show how this information can be used to efficiently identify noncooperative users in NICE. We present a simulation based study of our algorithms, in which we show our scheme scales to thousands of users using modest amounts of storage, processing, and bandwidth at any individual node. Lastly, we show that our scheme is robust and can form cooperative groups in systems where the vast majority of users are malicious.","Peer to peer computing,
Internet,
Cooperative systems,
Inference algorithms,
Bandwidth,
Resource management,
Streaming media,
Intelligent networks,
Computer science,
Educational institutions"
A game theoretic framework for incentives in P2P systems,"Peer-to-peer (P2P) networks are self-organizing, distributed systems, with no centralized authority or infrastructure. Because of the voluntary participation, the availability of resources in a P2P system can be highly variable and unpredictable. We use ideas from game theory to study the interaction of strategic and rational peers, and propose a differential service-based incentive scheme to improve the system's performance.","Game theory,
Peer to peer computing,
Distributed computing,
Economic forecasting,
Intelligent networks,
Computer science,
Availability,
Incentive schemes,
Centralized control,
Control systems"
3-D/2-D registration of CT and MR to X-ray images,"A crucial part of image-guided therapy is registration of preoperative and intraoperative images, by which the precise position and orientation of the patient's anatomy is determined in three dimensions. This paper presents a novel approach to register three-dimensional (3-D) computed tomography (CT) or magnetic resonance (MR) images to one or more two-dimensional (2-D) X-ray images. The registration is based solely on the information present in 2-D and 3-D images. It does not require fiducial markers, intraoperative X-ray image segmentation, or timely construction of digitally reconstructed radiographs. The originality of the approach is in using normals to bone surfaces, preoperatively defined in 3-D MR or CT data, and gradients of intraoperative X-ray images at locations defined by the X-ray source and 3-D surface points. The registration is concerned with finding the rigid transformation of a CT or MR volume, which provides the best match between surface normals and back projected gradients, considering their amplitudes and orientations. We have thoroughly validated our registration method by using MR, CT, and X-ray images of a cadaveric lumbar spine phantom for which ""gold standard"" registration was established by means of fiducial markers, and its accuracy assessed by target registration error. Volumes of interest, containing single vertebrae L1-L5, were registered to different pairs of X-ray images from different starting positions, chosen randomly and uniformly around the ""gold standard"" position. CT/X-ray (MR/X-ray) registration, which is fast, was successful in more than 91% (82% except for Ll) of trials if started from the ""gold standard"" translated or rotated for less than 6 mm or 17/spl deg/ (3 mm or 8.6/spl deg/), respectively. Root-mean-square target registration errors were below 0.5 mm for the CT to X-ray registration and below 1.4 mm for MR to X-ray registration.","Computed tomography,
X-ray imaging,
Gold,
Surface reconstruction,
Spine,
Medical treatment,
Anatomy,
Registers,
Magnetic resonance,
Two dimensional displays"
An efficient clustering-based heuristic for data gathering and aggregation in sensor networks,"The rapid advances in processor, memory, and radio technology have enabled the development of distributed networks of small, inexpensive nodes that are capable of sensing, computation, and wireless communication. Sensor networks of the future are envisioned to revolutionize the paradigm of collecting and processing information in diverse environments. However, the severe energy constraints and limited computing resources of the sensors, present major challenges for such a vision to become a reality. We consider a network of energy-constrained sensors that are deployed over a region. Each sensor periodically produces information as it monitors its vicinity. The basic operation in such a network is the systematic gathering and transmission of sensed data gathering and transmission of sensed data to a base station for further processing. During data gathering, sensors have the ability to perform in-network aggregation (fusion) of data packets enroute to the base station. The lifetime of such a sensor system is the time during which we can gather information from all the sensors to the base station. A key challenge in data gathering is to maximize the system lifetime, given the energy constraints of the sensors. Given the location of sensors and the base station and the available energy at each sensor, we are interested in finding an efficient manner in which data should be collected from all the sensors and transmitted to the base station, such that the system lifetime is maximized. This is the maximum lifetime data-gathering problem. In this paper, we describe a heuristic to solve the data-gathering problem with aggregation in sensor networks. Our experimental results demonstrate that the proposed algorithm significantly outperform previous methods, in terms of system lifetime.","Intelligent networks,
Base stations,
Sensor systems,
Wireless sensor networks,
Acoustic sensors,
Sensor fusion,
Sensor phenomena and characterization,
Computer science,
Computer networks,
Distributed computing"
Mutual information-based CT-MR brain image registration using generalized partial volume joint histogram estimation,"Mutual information (MI)-based image registration has been found to be quite effective in many medical imaging applications. To determine the MI between two images, the joint histogram of the two images is required. In the literature, linear interpolation and partial volume interpolation (PVI) are often used while estimating the joint histogram for registration purposes. It has been shown that joint histogram estimation through these two interpolation methods may introduce artifacts in the MI registration function that hamper the optimization process and influence the registration accuracy. In this paper, we present a new joint histogram estimation scheme called generalized partial volume estimation (GPVE). It turns out that the PVI method is a special case of the GPVE procedure. We have implemented our algorithm on the clinically obtained brain computed tomography and magnetic resonance image data furnished by Vanderbilt University. Our experimental results show that, by properly choosing the kernel functions, the GPVE algorithm significantly reduces the interpolation-induced artifacts and, in cases that the artifacts clearly affect registration accuracy, the registration accuracy is improved.","Brain,
Image registration,
Histograms,
Interpolation,
Mutual information,
Biomedical imaging,
Optimization methods,
Computed tomography,
Magnetic resonance,
Kernel"
Computational geometry for patient-specific reconstruction and meshing of blood vessels from MR and CT angiography,"Investigation of three-dimensional (3-D) geometry and fluid-dynamics in human arteries is an important issue in vascular disease characterization and assessment. Thanks to recent advances in magnetic resonance (MR) and computed tomography (CT), it is now possible to address the problem of patient-specific modeling of blood vessels, in order to take into account interindividual anatomic variability of vasculature. Generation of models suitable for computational fluid dynamics is still commonly performed by semiautomatic procedures, in general based on operator-dependent tasks, which cannot be easily extended to a significant number of clinical cases. In this paper, we overcome these limitations making use of computational geometry techniques. In particular, 3-D modeling was carried out by means of 3-D level sets approach. Model editing was also implemented ensuring harmonic mean curvature vectors distribution on the surface, and model geometric analysis was performed with a novel approach, based on solving Eikonal equation on Voronoi diagram. This approach provides calculation of central paths, maximum inscribed sphere estimation and geometric characterization of the surface. Generation of adaptive-thickness boundary layer finite elements is finally presented. The use of the techniques presented here makes it possible to introduce patient-specific modeling of blood vessels at clinical level.","Computational geometry,
Blood vessels,
Angiography,
Computed tomography,
Solid modeling,
Humans,
Arteries,
Diseases,
Magnetic resonance,
Computational modeling"
A system for volumetric robotic mapping of abandoned mines,"This paper describes two robotic systems developed for acquiring accurate volumetric maps of underground mines. One system is based on a cart instrumented by laser range finders, pushed through a mine by people. Another is a remotely controlled mobile robot equipped with laser range finders. To build consistent maps of large mines with many cycles, we describe an algorithm for estimating global correspondences and aligning robot paths. This algorithm enables us to recover consistent maps several hundreds of meters in diameter, without odometric information. We report results obtained in two mines, a research mine in Bruceton, PA, and an abandoned coal mine in Burgettstown, PA.","Simultaneous localization and mapping,
Iterative algorithms,
Computer science,
Laser theory,
Robot sensing systems,
Safety,
Accidents,
Navigation,
Remotely operated vehicles,
Computer errors"
Learning how to inpaint from global image statistics,"Inpainting is the problem of filling-in holes in images. Considerable progress has been made by techniques that use the immediate boundary of the hole and some prior information on images to solve this problem. These algorithms successfully solve the local inpainting problem but they must, by definition, give the same completion to any two holes that have the same boundary, even when the rest of the image is vastly different. We address a different, more global inpainting problem. How can we use the rest of the image in order to learn how to inpaint? We approach this problem from the context of statistical learning. Given a training image we build an exponential family distribution over images that is based on the histograms of local features. We then use this image specific distribution to inpaint the hole by finding the most probable image given the boundary and the distribution. The optimization is done using loopy belief propagation. We show that our method can successfully complete holes while taking into account the specific image statistics. In particular it can give vastly different completions even when the local neighborhoods are identical.",
Whole program path-based dynamic impact analysis,"Impact analysis, determining when a change in one part of a program affects other parts of the program, is time-consuming and problematic. Impact analysis is rarely used to predict the effects of a change, leaving maintainers to deal with consequences rather than working to a plan. Previous approaches to impact analysis involving analysis of call graphs, and static and dynamic slicing, exhibit several tradeoffs involving computational expense, precision, and safety, require access to source code, and require a relatively large amount of effort to re-apply as software evolves. This paper presents a new technique for impact analysis based on whole path profiling, that provides a different set of cost-benefits tradeoffs - a set which can potentially be beneficial for an important class of predictive impact analysis tasks. The paper presents the results of experiments that show that the technique can predict impact sets that are more accurate than those computed by call graph analysis, and more precise (relative to the behavior expressed in a program's profile) than those computed by static slicing.",
Stochastic assembly of sublithographic nanoscale interfaces,"We describe a technique for addressing individual nanoscale wires with microscale control wires without using lithographic-scale processing to define nanoscale dimensions. Such a scheme is necessary to exploit sublithographic nanoscale storage and computational devices. Our technique uses modulation doping to address individual nanowires and self-assembly to organize them into nanoscale-pitch decoder arrays. We show that if coded nanowires are chosen at random from a sufficiently large population, we can ensure that a large fraction of the selected nanowires have unique addresses. For example, we show that N lines can be uniquely addressed over 99% of the time using no more than /spl lceil/2.2log/sub 2/(N)/spl rceil/+11 address wires. We further show a hybrid decoder scheme that only needs to address N=O(W/sub litho-pitch//W/sub nano-pitch/) wires at a time through this stochastic scheme; as a result, the number of unique codes required for the nanowires does not grow with decoder size. We give an O(N/sup 2/) procedure to discover the addresses which are present. We also demonstrate schemes that tolerate the misalignment of nanowires which can occur during the self-assembly process.",
Gradient-based 2-D/3-D rigid registration of fluoroscopic X-ray to CT,"We present a gradient-based method for rigid registration of a patient preoperative computed tomography (CT) to its intraoperative situation with a few fluoroscopic X-ray images obtained with a tracked C-arm. The method is noninvasive, anatomy-based, requires simple user interaction, and includes validation. It is generic and easily customizable for a variety of routine clinical uses in orthopaedic surgery. Gradient-based registration consists of three steps: 1) initial pose estimation; 2) coarse geometry-based registration on bone contours, and; 3) fine gradient projection registration (GPR) on edge pixels. It optimizes speed, accuracy, and robustness. Its novelty resides in using volume gradients to eliminate outliers and foreign objects in the fluoroscopic X-ray images, in speeding up computation, and in achieving higher accuracy. It overcomes the drawbacks of intensity-based methods, which are slow and have a limited convergence range, and of geometry-based methods, which depend on the image segmentation quality. Our simulated, in vitro, and cadaver experiments on a human pelvis CT, dry vertebra, dry femur, fresh lamb hip, and human pelvis under realistic conditions show a mean 0.5-1.7 mm (0.5-2.6 mm maximum) target registration accuracy.","X-ray imaging,
Computed tomography,
Orthopedic surgery,
Humans,
Pelvis,
Bones,
Ground penetrating radar,
Robustness,
Convergence,
Image segmentation"
An Eulerian PDE approach for computing tissue thickness,"We outline an Eulerian framework for computing the thickness of tissues between two simply connected boundaries that does not require landmark points or parameterizations of either boundary. Thickness is defined as the length of correspondence trajectories, which run from one tissue boundary to the other, and which follow a smooth vector field constructed in the region between the boundaries. A pair of partial differential equations (PDEs) that are guided by this vector field are then solved over this region, and the sum of their solutions yields the thickness of the tissue region. Unlike other approaches, this approach does not require explicit construction of any correspondence trajectories. An efficient, stable, and computationally fast solution to these PDEs is found by careful selection of finite differences according to an up-winding condition. The behavior and performance of our method is demonstrated on two simulations and two magnetic resonance imaging data sets in two and three dimensions. These experiments reveal very good performance and show strong potential for application in tissue thickness visualization and quantification.","Partial differential equations,
Finite difference methods,
Alzheimer's disease,
Computational modeling,
Magnetic resonance imaging,
Data visualization,
Biomedical imaging,
Image analysis,
Myocardium,
Cardiac disease"
Registration and fusion of retinal images-an evaluation study,"We present the results of a study on the application of registration and pixel-level fusion techniques to retinal images. The images are of different modalities (color, fluorescein angiogram), different resolutions, and taken at different times (from a few minutes during an angiography examination to several years between two examinations). We propose a new registration method based on global point mapping with blood vessel bifurcations as control points and a search for control point matches that uses local structural information of the retinal network. Three transformation types (similarity, affine, and second-order polynomial) are evaluated on each image pair. Fourteen pixel-level fusion techniques have been tested and classified according to their qualitative and quantitative performance. Four quantitative fusion performance criteria are used to evaluate the gain obtained with the grayscale fusion.","Retina,
Pixel,
Image resolution,
Angiography,
Blood vessels,
Biomedical imaging,
Bifurcation,
Polynomials,
Testing,
Performance gain"
Obstruction-free synchronization: double-ended queues as an example,"We introduce obstruction-freedom, a new nonblocking property for shared data structure implementations. This property is strong enough to avoid the problems associated with locks, but it is weaker than previous nonblocking properties-specifically lock-freedom and wait-freedom-allowing greater flexibility in the design of efficient implementations. Obstruction-freedom admits substantially simpler implementations, and we believe that in practice it provides the benefits of wait-free and lock-free implementations. To illustrate the benefits of obstruction-freedom, we present two obstruction-free CAS-based implementations of double-ended queues (deques); the first is implemented on a linear array, the second on a circular array. To our knowledge, all previous nonblocking deque implementations are based on unrealistic assumptions about hardware support for synchronization, have restricted functionality, or have operations that interfere with operations at the opposite end of the deque even when the deque has many elements in it. Our obstruction-free implementations have none of these drawbacks, and thus suggest that it is much easier to design obstruction-free implementations than lock-free and wait-free ones. We also briefly discuss other obstruction-free data structures and operations that we have implemented.","Yarn,
Content addressable storage,
Data structures,
Delay,
Computer science,
Sun,
Laboratories,
Drives,
Hardware,
Software engineering"
Finding and tracking people from the bottom up,"We describe a tracker that can track moving people in long sequences without manual initialization. Moving people are modeled with the assumption that, while configuration can vary quite substantially from frame to frame, appearance does not. This leads to an algorithm that firstly builds a model of the appearance of the body of each individual by clustering candidate body segments, and then uses this model to find all individuals in each frame. Unusually, the tracker does not rely on a model of human dynamics to identify possible instances of people; such models are unreliable, because human motion is fast and large accelerations are common. We show our tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net. Experiments on video of real scenes demonstrate that this tracker can (a) count distinct individuals; (b) identify and track them; (c) recover when it loses track, for example, if individuals are occluded or briefly leave the view; (d) identify the configuration of the body largely correctly; and (e) is not dependent on particular models of human motion.","Biological system modeling,
Humans,
Predictive models,
Particle filters,
Clothing,
Computer science,
Clustering algorithms,
Acceleration,
Tracking loops,
Inference algorithms"
Combining multiple weak clusterings,"A data set can be clustered in many ways depending on the clustering algorithm employed, parameter settings used and other factors. Can multiple clusterings be combined so that the final partitioning of data provides better clustering? The answer depends on the quality of clusterings to be combined as well as the properties of the fusion method. First, we introduce a unified representation for multiple clusterings and formulate the corresponding categorical clustering problem. As a result, we show that the consensus function is related to the classical intra-class variance criterion using the generalized mutual information definition. Second, we show the efficacy of combining partitions generated by weak clustering algorithms that use data projections and random data splits. A simple explanatory model is offered for the behavior of combinations of such weak clustering components. We analyze the combination accuracy as a function of parameters controlling the power and resolution of component partitions as well as the learning dynamics vs. the number of clusterings involved. Finally, some empirical studies compare the effectiveness of several consensus functions.","Clustering algorithms,
Partitioning algorithms,
Mutual information,
Robustness,
Data mining,
Training data,
Computer science,
Fusion power generation,
Taxonomy,
Classification algorithms"
Measurement and analysis of brain deformation during neurosurgery,"Recent studies have shown that the surface of the brain is deformed by up to 20 mm after the skull is opened during neurosurgery, which could lead to substantial error in commercial image-guided surgery systems. We quantitatively analyze the intraoperative brain deformation of 24 subjects to investigate whether simple rules can describe or predict the deformation. Interventional magnetic resonance images acquired at the start and end of the procedure are registered nonrigidly to obtain deformation values throughout the brain. Deformation patterns are investigated quantitatively with respect to the location an magnitude of deformation, and to the distribution and principal direction of the displacements. We also measure the volume change of the lateral ventricles by manual segmentation. Our study indicates that brain shift occurs predominantly in the hemisphere ipsi-lateral to the craniotomy, and that there is more brain deformation during resection procedures than during biopsy or functional procedures. However, the brain deformation patterns are extremely complex in this group of subjects. This paper quantitatively demonstrates that brain deformation occurs not only at the surface, but also in deeper brain structure, and that the principal direction of displacement does not always correspond with the direction of gravity. Therefore, simple computational algorithms that utilize limited intraoperative information (e.g., brain surface shift) will not always accurately predict brain deformation at the lesion.","Neurosurgery,
Brain,
Skull,
Surgery,
Magnetic analysis,
Magnetic resonance,
Volume measurement,
Biopsy,
Gravity,
Lesions"
Ultrasonic liver tissues classification by fractal feature vector based on M-band wavelet transform,"Describes the feasibility of selecting a fractal feature vector based on M-band wavelet transform to classify ultrasonic liver images - normal liver, cirrhosis, and hepatoma. The proposed feature extraction algorithm is based on the spatial-frequency decomposition and fractal geometry. Various classification algorithms based on respective texture measurements and filter banks are presented and tested. Classifications for the three sets of ultrasonic liver images reveal that the fractal feature vector based on M-band wavelet transform is trustworthy. A hierarchical classifier, which is based on the proposed feature extraction algorithm is at least 96.7% accurate in the distinction between normal and abnormal liver images and is at least 93.6% accurate in the distinction between cirrhosis and hepatoma liver images. Additionally, the criterion for feature selection is specified and employed for performance comparisons herein.","Liver,
Fractals,
Wavelet transforms,
Ultrasonic imaging,
Image texture analysis,
Radio frequency,
Humans,
Neural networks,
Feature extraction,
Ultrasonic variables measurement"
Prophet address allocation for large scale MANETs,"A mobile device in a MANET must be assigned a free IP address before it may participate in unicast communication. This is a fundamental and difficult problem in the practical use of any MANET. Several solutions have been proposed. However, these approaches have different drawbacks. A new IP address allocation algorithm, namely prophet allocation, is proposed in the paper. The proposed scheme may be applied to large scale MANETs with low complexity, low communication overhead, even address distribution, and low latency. Both theoretical analysis and simulation experiments are conducted to demonstrate the superiority of the proposed algorithm over other known algorithms. Moreover, the proposed prophet allocation is able to solve the problem of network partition and merger efficiently.","Large-scale systems,
Mobile ad hoc networks,
Mobile communication,
Computer science,
Unicast,
Partitioning algorithms,
Mobile computing,
Delay,
Algorithm design and analysis,
Analytical models"
Probabilistic tracking in joint feature-spatial spaces,"In this paper, we present a probabilistic framework for tracking regions based on their appearance. We exploit the feature-spatial distribution of a region representing an object as a probabilistic constraint to track that region over time. The tracking is achieved by maximizing a similarity-based objective function over transformation space given a nonparametric representation of the joint feature-spatial distribution. Such a representation imposes a probabilistic constraint on the region feature distribution coupled with the region structure, which yields an appearance tracker that is robust to small local deformations and partial occlusion. We present the approach for the general form of joint feature-spatial distributions and apply it to tracking with different types of image features including row intensity, color and image gradient.","Target tracking,
Computer science,
Educational institutions,
Robustness,
Computer vision,
Region 6,
Deformable models,
Random variables,
Kernel,
Measurement uncertainty"
Mechanism of high luminous efficient discharges with high pressure and high Xe-content in AC PDP,The mechanism of high luminous efficiency discharges with high Xe content in an AC plasma display panel was analyzed by computer simulation using a two-dimensional fluid model. The model has reproduced well the experimental results. The high luminous efficiency with high Xe content is attributed to high electron heating efficiency as well as high excitation efficiency by electron. The electron heating efficiency is increased with increasing the sustaining voltage under high Xe content and this phenomenon was analyzed by investigating the cathode sheath and secondary electron emission characteristics.,"Voltage,
Plasma displays,
Heating,
Electron emission,
Laboratories,
Computer simulation,
Cathodes,
Technological innovation,
Image quality,
Costs"
Cortical surface registration for image-guided neurosurgery using laser-range scanning,"In this paper, a method of acquiring intraoperative data using a laser range scanner (LRS) is presented within the context of model-updated image-guided surgery. Registering textured point clouds generated by the LRS to tomographic data is explored using established point-based and surface techniques as well as a novel method that incorporates geometry and intensity information via mutual information (SurfaceMI). Phantom registration studies were performed to examine accuracy and robustness for each framework. In addition, an in vivo registration is performed to demonstrate feasibility of the data acquisition system in the operating room. Results indicate that SurfaceMI performed better in many cases than point-based (PBR) and iterative closest point (ICP) methods for registration of textured point clouds. Mean target registration error (TRE) for simulated deep tissue targets in a phantom were 1.0 /spl plusmn/ 0.2,2.0 /spl plusmn/ 0.3, and 1.2 /spl plusmn/ 0.3 mm for PBR, ICP, and SurfaceMI, respectively. With regard to in vivo registration, the mean TRE of vessel contour points for each framework was 1.9 /spl plusmn/ 1.0, 0.9 /spl plusmn/ 0.6, and 1.3 /spl plusmn/ 0.5 for PBR, ICP, and SurfaceMI, respectively. The methods discussed in this paper in conjunction with the quantitative data provide impetus for using LRS technology within the model-updated image-guided surgery framework.","Surface emitting lasers,
Neurosurgery,
Surface texture,
Bioreactors,
Iterative closest point algorithm,
Laser modes,
Surgery,
Clouds,
Imaging phantoms,
In vivo"
Lossy-to-lossless compression of medical volumetric data using three-dimensional integer wavelet transforms,"We study lossy-to-lossless compression of medical volumetric data using three-dimensional (3-D) integer wavelet transforms. To achieve good lossy coding performance, it is important to have transforms that are unitary. In addition to the lifting approach, we first introduce a general 3-D integer wavelet packet transform structure that allows implicit bit shifting of wavelet coefficients to approximate a 3-D unitary transformation. We then focus on context modeling for efficient arithmetic coding of wavelet coefficients. Two state-of-the-art 3-D wavelet video coding techniques, namely, 3-D set partitioning in hierarchical trees (Kim et al., 2000) and 3-D embedded subband coding with optimal truncation (Xu et al., 2001), are modified and applied to compression of medical volumetric data, achieving the best performance published so far in the literature-both in terms of lossy and lossless compression.","Wavelet transforms,
Image coding,
Biomedical imaging,
Video coding,
Performance loss,
Wavelet coefficients,
Context modeling,
Medical diagnostic imaging,
Decoding,
Transform coding"
Sampling biases in IP topology measurements,"Considerable attention has been focused on the properties of graphs derived from Internet measurements. Router-level topologies collected via traceroute-like methods have led some to conclude that the router graph of the Internet is well modeled as a power-law random graph. In such a graph, the degree distribution of nodes follows a distribution with a power-law tail. We argue that the evidence to date for this conclusion is at best insufficient We show that when graphs are sampled using traceroute-like methods, the resulting degree distribution can differ sharply from that of the underlying graph. For example, given a sparse Erdos-Renyi random graph, the subgraph formed by a collection of shortest paths from a small set of random sources to a larger set of random destinations can exhibit a degree distribution remarkably like a power-law. We explore the reasons for how this effect arises, and show that in such a setting, edges are sampled in a highly biased manner. This insight allows us to formulate tests for determining when sampling bias is present. When we apply these tests to a number of well-known datasets, we find strong evidence for sampling bias.","Sampling methods,
Internet,
Testing,
Network topology,
Frequency,
Optical reflection,
Computer science,
Probability distribution,
Assembly,
Probes"
Design of two-dimensional recursive filters using genetic algorithms,"In this paper, we examine a new design method for two-dimensional (2-D) recursive digital filters using genetic algorithms (GAs). The design of the 2-D filter is reduced to a constrained minimization problem the solution of which is achieved by the convergence of an appropriate GA. Theoretical results are illustrated by a numerical example. Also, comparison with the results of some previous design methods is attempted.","Algorithm design and analysis,
Filters,
Genetic algorithms,
Two dimensional displays,
Design methodology,
Image processing,
Signal processing algorithms,
Stability,
Computer science,
Military computing"
Dynamic texture segmentation,"We address the problem of segmenting a sequence of images of natural scenes into disjoint regions that are characterized by constant spatio-temporal statistics. We model the spatio-temporal dynamics in each region by Gauss-Markov models, and infer the model parameters as well as the boundary of the regions in a variational optimization framework. Numerical results demonstrate that - in contrast to purely texture-based segmentation schemes - our method is effective in segmenting regions that differ in their dynamics even when spatial statistics are identical.","Image segmentation,
Statistics,
Layout,
Vehicle dynamics,
Gaussian processes,
Marine vehicles,
Remotely operated vehicles,
Mobile robots,
Statistical distributions,
Computer science"
A primal sketch of the cortex mean curvature: a morphogenesis based approach to study the variability of the folding patterns,"In this paper, we propose a new representation of the cortical surface that may be used to study the cortex folding process and to recover some putative stable anatomical landmarks called sulcal roots usually buried in the depth of adult brains. This representation is a primal sketch derived from a scale space computed for the mean curvature of the cortical surface. This scale-space stems from a diffusion equation geodesic to the cortical surface. The primal sketch is made up of objects defined from mean curvature minima and saddle points. The resulting sketch aims first at highlighting significant elementary cortical folds, second at representing the fold merging process during brain growth. The relevance of the framework is illustrated by the study of central sulcus sulcal roots from antenatal to adult age. Some results are proposed for ten different brains. Some preliminary results are also provided for superior temporal sulcus.","Brain,
Psychiatry,
Hospitals,
Anatomy,
Neuroscience,
Geophysics computing,
Equations,
Merging,
Image databases"
Unsupervised robust nonparametric estimation of the hemodynamic response function for any fMRI experiment,"This paper deals with the estimation of the blood oxygen level-dependent response to a stimulus, as measured in functional magnetic resonance imaging (fMRI) data. A precise estimation is essential for a better understanding of cerebral activations. The most recent works have used a nonparametric framework for this estimation, considering each brain region as a system characterized by its impulse response, the so-called hemodynamic response function (HRF). However, the use of these techniques has remained limited since they are not well-adapted to real fMRI data. Here, we develop a threefold extension to previous works. We consider asynchronous event-related paradigms account for different trial types and integrate several fMRI sessions into the estimation. These generalizations are simultaneously addressed through a badly conditioned observation model. Bayesian formalism is used to model temporal prior information of the underlying physiological process of the brain hemodynamic response. By this way, the HRF estimate results from a tradeoff between information brought by the data and by our prior knowledge. This tradeoff is modeled with hyperparameters that are set to the maximum-likelihood estimate using an expectation conditional maximization algorithm. The proposed unsupervised approach is validated on both synthetic and real fMRI data, the latter originating from a speech perception experiment.","Robustness,
Hemodynamics,
Magnetic resonance imaging,
Spatial resolution,
Data acquisition,
Protocols,
Blood,
Bayesian methods,
Brain modeling,
Maximum likelihood estimation"
Multielement synthetic transmit aperture imaging using temporal encoding,"A new method to increase the signal-to-noise ratio (SNR) of synthetic transmit aperture imaging is investigated. The approach utilizes multiple elements to emulate a spherical wave, and the conventional short excitation pulse is replaced by a linear frequency-modulated (FM) signal. The approach is evaluated in terms of image quality parameters in comparison to linear array imaging. Field II simulations using an 8.5-MHz linear array transducer with 128 elements show an improvement in lateral resolution of up to 30% and up to 10.75% improvement in contrast resolution for the new approach. Measurements are performed using our experimental multichannel ultrasound scanning system, RASMUS. The designed linear FM signal obtains temporal sidelobes below -55 dB, and SNR investigations show improvements of 4-12 dB. A 30 mm (/spl ap/45%) increase in penetration depth is obtained on a multitarget phantom with 0.5 dB/[cm MHz] attenuation. Furthermore, in vivo images of the abdomen are presented, which demonstrate the clinical application of the new approach.","Encoding,
Ultrasonic imaging,
Signal resolution,
Signal to noise ratio,
Frequency,
Image quality,
Ultrasonic transducers,
Ultrasonic variables measurement,
Performance evaluation,
Signal design"
Broadcasting in ad hoc networks based on self-pruning,"We propose a general framework for broadcasting in ad hoc networks through self-pruning. The approach is based on selecting a small subset of hosts (also called nodes) to form a forward node set to carry out a broadcast process. Each node, upon receiving a broadcast packet, determines whether to forward the packet based on two neighborhood coverage conditions. These coverage conditions depend on neighbor connectivity and history of visited nodes, and in general, resort to global network information. Using local information such as k-hop neighborhood information, the forward node set is selected through a distributed and local pruning process. The forward node set can be constructed and maintained through either a proactive process (i.e., ""up-to-date"") or a reactive process (i.e., ""on-the-fly""). Several existing broadcast algorithms can be viewed as special cases of the coverage conditions with k-hop neighborhood information. Simulation results show that new algorithms, which are more efficient than existing ones, can be derived from the coverage conditions, and self-pruning based on 2- or 3-hop neighborhood information is relatively cost-effective.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Mobile computing,
Computer networks,
Wireless networks,
Floods,
Computer science,
Electronic mail,
History"
Beyond Moore's Law: the interconnect era,"Reversing early limitations on Moore's low, interconnectors have replaced transistors as the main determinants of chip performance. This ""tyranny of interconnectors"" will only escalate in the future, and thus the nanoelectronics that follow silicon must be interconnect-centric. This new technology will likely use ""transistors"" that approach, if not surpass, the 0.1 ps latency of 10 nm generation silicon transistors. Consequently, if we optimistically assume that the interconnects of this post-Moore's Law nanotechnology will be superconductive, their latency will exceed that of the transistors for interconnect lengths greater than 30 /spl mu/m, while long, on-chip interconnect lengths will be 1,000 times greater at 30 mm. Consequently, mainstream electronics will have an interconnect era beyond Moore's law.","Moore's Law,
Integrated circuit interconnections,
Transistors,
Silicon,
Copper,
Wires,
Delay,
Energy dissipation,
Logic devices,
Electron tubes"
Applying video sensor networks to nearshore environment monitoring,"Environmental monitoring is an important emerging application area for pervasive computing. We describe shore-based sensing using standard video cameras and measurement techniques for important variables such as wave and ocean current conditions. We apply networked sensors for monitoring environmentally sensitive beaches and nearshore coastal oceans. We give some steps to improve the Argus video sensor network's functionality to quantify the time-space characteristics of the imaged world. We also discuss future system architectures, on the basis of the experience with a sensor network that have already deployed.","Monitoring,
Sensor phenomena and characterization,
Oceans,
Pervasive computing,
Measurement standards,
Cameras,
Measurement techniques,
Sea measurements,
Image sensors,
Sensor systems"
Mediators of the effectiveness of online courses,"A three-year field study of 17 courses, part of an undergraduate degree in information systems, compared the process and outcomes of three modes of delivery: totally online via asynchronous learning networks, traditional face-to-face courses, and sections using a mix of traditional and online activities. There were no significant differences in perceived learning by students associated with mode of delivery. Group collaboration and access to professors was perceived to be highest in mixed-mode sections, while convenience was rated highest in the distance sections. For online courses, there was generally a significant relationship between the hypothesized mediators (active participation, motivation, collaboration, access to the professor, and convenience) and perceived learning. Overall, the results of this study show that outcomes of online courses improved when professors structured them to support the growth of a learning community, by being available online to interact with students, and by using collaborative learning strategies.","Information systems,
Collaborative work,
Online Communities/Technical Collaboration,
Computer mediated communication,
Education,
Educational activities,
Internet,
Information science,
Computational Intelligence Society"
Caches and hash trees for efficient memory integrity verification,"We study the hardware cost of implementing hash-tree based verification of untrusted external memory by a high performance processor. This verification could enable applications such as certified program execution. A number of schemes are presented with different levels of integration between the on-processor L2 cache and the hash-tree machinery. Simulations show that for the best of our methods, the performance overhead is less than 25%, a significant decrease from the 10/spl times/ overhead of a naive implementation.","Machinery,
Costs,
Bandwidth,
Hardware,
Application software,
Laboratories,
Computer science,
Coprocessors,
Security,
Protection"
Palantir: raising awareness among configuration management workspaces,"Current configuration management systems promote workspaces that isolate developers from each other. This isolation is both good and bad It is good, because developers make their changes without any interference from changes made concurrently by other developers. It is bad, because not knowing which artifacts are changing in parallel regularly leads to problems when changes are promoted from workspaces into a central configuration management repository. Overcoming the bad isolation, while retaining the good isolation, is a matter of raising awareness among developers, an issue traditionally ignored by the discipline of configuration management. To fill this void, we have developed Palantir, a novel workspace awareness tool that complements existing configuration management systems by providing developers with insight into other workspaces. In particular, the tool informs a developer of which other developers change which other artifacts, calculates a simple measure of severity of those changes, and graphically displays the information in a configurable and generally non-obtrusive manner. To illustrate the use of Palantir, we demonstrate how it integrates with two representative configuration management systems.","Displays,
Project management,
Visualization,
Computer science,
Interference,
Particle measurements,
Merging,
Joining processes,
Filtering"
Effective intrusion detection using multiple sensors in wireless ad hoc networks,"In this paper we propose a distributed intrusion detection system for ad hoc wireless networks based on mobile agent technology. Wireless networks are particularly vulnerable to intrusion, as they operate in open medium, and use cooperative strategies for network communications. By efficiently merging audit data from multiple network sensors, we analyze the entire ad hoc wireless network for intrusions and try to inhibit intrusion attempts. In contrast to many intrusion detection systems designed for wired networks, we implement an efficient and bandwidth-conscious framework that targets intrusion at multiple levels and takes into account distributed nature of ad hoc wireless network management and decision policies.","Wireless sensor networks,
Intrusion detection,
Intelligent networks,
Ad hoc networks,
Wireless networks,
Routing,
Monitoring,
Mobile ad hoc networks,
Computer science,
Mobile agents"
Merging parametric active contours within homogeneous image regions for MRI-based lung segmentation,"Inhaled hyperpolarized helium-3 (/sup 3/He) gas is a new magnetic resonance (MR) contrast agent that is being used to study lung functionality. To evaluate the total lung ventilation from the hyperpolarized /sup 3/He MR images, it is necessary to segment the lung cavities. This is difficult to accomplish using only the hyperpolarized /sup 3/He MR images, so traditional proton (/sup 1/H) MR images are frequently obtained concurrent with the hyperpolarized /sup 3/He MR examination. Segmentation of the lung cavities from traditional proton (/sup 1/H) MRI is a necessary first step in the analysis of hyperpolarized /sup 3/He MR images. In this paper, we develop an active contour model that provides a smooth boundary and accurately captures the high curvature features of the lung cavities from the /sup 1/H MR images. This segmentation method is the first parametric active contour model that facilitates straightforward merging of multiple contours. The proposed method of merging computes an external force field that is based on the solution of partial differential equations with boundary condition defined by the initial positions of the evolving contours. A theoretical connection with fluid flow in porous media and the proposed force field is established. Then by using the properties of fluid flow we prove that the proposed method indeed achieves merging and the contours stop at the object boundary as well. Experimental results involving merging in synthetic images are provided. The segmentation technique has been employed in lung /sup 1/H MR imaging for segmenting the total lung air space. This technology plays a key role in computing the functional air space from MR images that use hyperpolarized /sup 3/He gas as a contrast agent.",
Secret-key agreement over unauthenticated public channels .I. Definitions and a completeness result,"This is the first part of a three-part paper on secret-key agreement secure against active adversaries. In all three parts, we address the question whether two parties, knowing some correlated pieces of information X and Y, respectively, can generate a string S about which an adversary, knowing some information Z and having read and write access to the communication channel used by the legitimate partners, is almost completely ignorant. Whether such key agreement is possible, and if yes at which rate, is an inherent property of the joint probability distribution P/sub XYZ/. In this part, we first prove a number of general impossibility results. We then consider the important special case where the legitimate partners as well as the adversary have access to the outcomes of many independent repetitions of a fixed tripartite random experiment. In this case, the result characterizing the possibility of secret-key agreement secure against active adversaries is of all-or-nothing nature: either a secret key can be generated at the same rate as in the (well-studied) passive-adversary case, or such secret-key agreement is completely impossible. The exact condition characterizing the two cases is presented.","Communication channels,
Quantum computing,
Probability distribution,
Joining processes,
Computational modeling,
Random variables,
Character generation,
Security,
Public key cryptography,
Computer science"
Efficient memory integrity verification and encryption for secure processors,"Secure processors enable new sets of applications such as commercial grid computing, software copy-protection, and secure mobile agents by providing security from both physical and software attacks. This paper proposes new hardware mechanisms for memory integrity verification and encryption, which are two key primitives required in single-chip secure processors. The integrity verification mechanism offers significant performance advantages over existing ones when the checks are infrequent as in grid computing applications. The encryption mechanism improves the performance in all cases.","Cryptography,
Application software,
Grid computing,
Mobile agents,
Tellurium,
Computer security,
Hardware,
Read-write memory,
Computer science,
Artificial intelligence"
Graph wavelets for spatial traffic analysis,"A number of problems in network operations and engineering call for new methods of traffic analysis. While most existing traffic analysis methods are fundamentally temporal, there is a clear need for the analysis of traffic across multiple network links - that is, for spatial traffic analysis. In this paper we give examples of problems that can be addressed via spatial traffic analysis. We then propose a formal approach to spatial traffic analysis based on the wavelet transform. Our approach (graph wavelets) generalizes the traditional wavelet transform so that it can be applied to data elements connected via an arbitrary graph topology. We explore the necessary and desirable properties of this approach and consider some of its possible realizations. We then apply graph wavelets to measurements from an operating network. Our results show that graph wavelets are very useful for our motivating problems; for example, they can be used to form highly summarized views of an entire network's traffic load, to gain insight into a network's global traffic response to a link failure, and to localize the extent of a failure event within the network.","Wavelet analysis,
Telecommunication traffic,
Pattern analysis,
Discrete wavelet transforms,
Wavelet transforms,
Network topology,
Wavelet domain,
Computer science,
Statistical analysis,
Capacity planning"
Recognition of group activities using dynamic probabilistic networks,"Dynamic Probabilistic Networks (DPNs) are exploited for modeling the temporal relationships among a set of different object temporal events in the scene for a coherent and robust scene-level behaviour interpretation. In particular, we develop a Dynamically Multi-Linked Hidden Markov Model (DML-HMM) to interpret group activities involving multiple objects captured in an outdoor scene. The model is based on the discovery of salient dynamic interlinks among multiple temporal events using DPNs. Object temporal events are detected and labeled using Gaussian Mixture Models with automatic model order selection. A DML-HMM is built using Schwarz's Bayesian Information Criterion based factorisation resulting in its topology being intrinsically determined by the underlying causality and temporal order among different object events. Our experiments demonstrate that its performance on modelling group activities in a noisy outdoor scene is superior compared to that of a Multi-Observation Hidden Markov Model (MOHMM), a Parallel Hidden Markov Model (PaHMM) and a Coupled Hidden Markov Model (CHMM).","Hidden Markov models,
Layout,
State-space methods,
Bayesian methods,
Character recognition,
Network topology,
Computer science,
Robustness,
Object detection,
Event detection"
A comparison of coverage-based and distribution-based techniques for filtering and prioritizing test cases,"This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distribution-based techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.","Filtering,
Computer aided software engineering,
Costs,
Automatic testing,
Sampling methods,
System testing,
Software testing,
Computer science,
Java,
Program processors"
On setting TCP's congestion window limit in mobile ad hoc networks,"Improving TCP performance has long been the focus of many research efforts in mobile ad hoc networks (MANET). In this paper, we address one aspect of this endeavor: how to properly set TCP's congestion window limit (CWL) to achieve optimal performance. Past research has shown that using a small CWL improves TCP performance in certain scenarios [M. Gerla et al., Feb. 1999], [Z. Fu et al., Apr. 2003], however, no comprehensive study has been given. To this end, we turn the problem of setting TCP's optimal CWL into identifying the bandwidth-delay product (BDP) of a path in MANET. We first show and prove that, independent of the MAC layer protocol being used, the BDP of a path in MANET cannot exceed the round-trip hop-count (RTHC) of the path. We further refine this upper bound based on the IEEE 802.11 MAC layer protocol, and show that in a chain topology, a tighter upper bound exists, which is approximately 1/5 of the RTHC of the path. Based on this tighter bound, we propose an adaptive CWL setting strategy to dynamically adjust TCP's CWL according to the current RTHC of its path. Using ns-2 simulations, we show that our simple strategy improves TCP performance by 8% to 16% in a dynamic MANET environment.",
Consistency management with repair actions,"Comprehensive consistency management requires a strong mechanism for repair once inconsistencies have been detected In this paper we present a repair framework for inconsistent distributed documents. The core piece of the framework is a new method for generating interactive repairs from full first order logic formulae that constrain these documents. We present a full implementation of the components in our repair framework, as well as their application to the UML and related heterogeneous documents such as EJB deployment descriptors. We describe how our approach can be used as an infrastructure for building higher-level, domain specific frameworks and provide an overview of related work in the database and software development environment community.","Logic,
Unified modeling language,
Application software,
XML,
Computer science,
Educational institutions,
Databases,
Programming,
Data engineering,
Engineering management"
Firewall Policy Advisor for anomaly discovery and rule editing,"Firewalls are core elements in network security. However, managing firewall rules, especially for enterprize networks, has become complex and error-prone. Firewall filtering rules have to be carefully written and organized in order to correctly implement the security policy. In addition, inserting or modifying a filtering rule requires thorough analysis of the relationship between this rule and other rules in order to determine the proper order of this rule and commit the updates. In this paper, we present a set of techniques and algorithms that provide (1) automatic discovery of firewall policy anomalies to reveal rule conflicts and potential problems in legacy firewalls, and (2) anomaly-free policy editing for rule insertion, removal and modification. This is implemented in a user-friendly tool called ""Firewall Policy Advisor"". The Firewall Policy Advisor significantly simplifies the management of any generic firewall policy written as filtering rules, while minimizing network vulnerability due to firewall rule misconfiguration.",
ICR: in-cache replication for enhancing data cache reliability,,"Delay,
Hardware,
Clocks,
Error correction,
Computer science,
Reliability engineering,
Error correction codes,
Protection,
Costs,
Alpha particles"
Weighted and robust incremental method for subspace learning,"Visual learning is expected to be a continuous and robust process, which treats input images and pixels selectively. In this paper, we present a method for subspace learning, which takes these considerations into account. We present an incremental method, which sequentially updates the principal subspace considering weighted influence of individual images as well as individual pixels within an image. This approach is further extended to enable determination of consistencies in the input data and imputation of the values in inconsistent pixels using the previously acquired knowledge, resulting in a novel incremental, weighted and robust method for subspace learning.","Robustness,
Principal component analysis,
Pixel,
Layout,
Information science,
Humans,
Visual system,
Machine learning,
Computer vision,
Singular value decomposition"
Improving web application testing with user session data,"Web applications have become critical components of the global information infrastructure, and it is important that they be validated to ensure their reliability. Therefore, many techniques and tools for validating web applications have been created. Only a few of these techniques, however, have addressed problems of testing the functionality of web applications, and those that do have not fully considered the unique attributes of web applications. In this paper we explore the notion that user session data gathered as users operate web applications can be successfully employed in the testing of those applications, particularly as those applications evolve and experience different usage profiles. We report results of an experiment comparing new and existing test generation techniques for web applications, assessing both the adequacy of the generated tests and their ability to detect faults on a point-of-sale web application. Our results show that user session data can produce test suites as effective overall as those produced by existing white-box techniques, but at less expense. Moreover the classes of faults detected differ somewhat across approaches, suggesting that the techniques may be complimentary.","Application software,
Computer science,
Fault detection,
Software systems,
System testing,
Automatic testing,
Reliability engineering,
Marketing and sales,
Proposals,
Medical diagnostic imaging"
Texture segmentation by multiscale aggregation of filter responses and shape elements,"Texture segmentation is a difficult problem, as is apparent from camouflage pictures. A textured region can contain texture elements of various sizes, each of which can itself be textured. We approach this problem using a bottom-up aggregation framework that combines structural characteristics of texture elements with filter responses. Our process adaptively identifies the shape of texture elements and characterize them by their size, aspect ratio, orientation, brightness, etc., and then uses various statistics of these properties to distinguish between different textures. At the same time our process uses the statistics of filter responses to characterize textures. In our process the shape measures and the filter responses crosstalk extensively. In addition, a top-down cleaning process is applied to avoid mixing the statistics of neighboring segments. We tested our algorithm on real images and demonstrate that it can accurately segment regions that contain challenging textures.","Statistics,
Image segmentation,
Filter bank,
Shape measurement,
Brightness,
Crosstalk,
Computer vision,
Computer science,
Cleaning,
Testing"
Context-awareness on mobile devices - the hydrogen approach,"Information about the user's environment offers new opportunities and exposes new challenges in terms of time-aware, location-aware, device-aware and personalized applications. Such applications constantly need to monitor the environment - called context - to allow the application to react accordingly to this context. Context-awareness is especially interesting in mobile scenarios where the context of the application is highly dynamic and allows the application to deal with the constraints of mobile devices in terms of presentation and interaction abilities and communication restrictions. Current context-aware applications often realize sensing of context information in an ad hoc manner. The application programmer needs to deal with the supply of the context information including the sensing of the environment, its interpretation and its disposal for further processing in addition to the primary purpose of the application. The close interweavement of device specific context handling with the application obstructs its reuse with other hardware configurations. Recently, architectures providing support for context-aware applications have been developed. Up to now such architectures are not trimmed to the special requirements of mobile devices regarding particularly the limitations of network connections, limited computing power and the characteristics of mobile users. This paper proposes an architecture and a software framework - the hydrogen context framework -which support context-awareness for considering these constraints. It is extensible to consider all kind of context information and comprises a layered architecture. To prove the feasibility the framework has been implemented to run on mobile devices. A context-aware postbox is realized to demonstrate the capabilities of the framework.",
Wavelet-based estimation of a semiparametric generalized linear model of fMRI time-series,"Addresses the problem of detecting significant changes in fMRI time series that are correlated to a stimulus time course. This paper provides a new approach to estimate the parameters of a semiparametric generalized linear model of the fMRI time series. The fMRI signal is described as the sum of two effects: a smooth trend and the response to the stimulus. The trend belongs to a subspace spanned by large scale wavelets. The wavelet transform provides an approximation to the Karhunen-Loeve transform for the long memory noise and we have developed a scale space regression that permits one to carry out the regression in the wavelet domain while omitting the scales that are contaminated by the trend. In order to demonstrate that our approach outperforms the state-of-the art detrending technique, we evaluated our method against a smoothing spline approach. Experiments with simulated data and experimental fMRI data, demonstrate that our approach can infer and remove drifts that cannot be adequately represented with splines.","Spline,
Smoothing methods,
Brain modeling,
Polynomials,
Wavelet domain,
Wavelet transforms,
Blood,
Biochemistry,
Instruments,
Signal processing"
Modeling malware spreading dynamics,"In this paper we present analytical techniques that can be used to better understand the behavior of malware, a generic term that refers to all kinds of malicious software programs propagating on the Internet, such as e-mail viruses and worms. We develop a modeling methodology based on Interactive Markov Chains that is able to capture many aspects of the problem, especially the impact of the underlying topology on the spreading characteristics of malware. We propose numerical methods to obtain useful bounds and approximations in the case of very large systems, validating our results through simulation. An analytic methodology represents a fundamentally important step in the development of effective countermeasures for future malware activity. Furthermore, we believe our approach can help to understand a wide range of ""dynamic interactions on networks"", such as routing protocols and peer-to-peer applications.","Internet,
Electronic mail,
Computer viruses,
Computer worms,
Topology,
Viruses (medical),
Computer science,
Routing protocols,
Peer to peer computing,
Application software"
Multi-robot task allocation: analyzing the complexity and optimality of key architectures,"Important theoretical aspects of multi-robot coordination mechanisms have, to date, been largely ignored. To address part of this negligence, we focus on the problem of multi-robot task allocation. We give a formal, domain-independent, statement of the problem and show it to be an instance of another, well-studied, optimization problem. In this light, we analyze several recently proposed approaches to multi-robot task allocation, describing their fundamental characteristics in such a way that they can be objectively studied, compared, and evaluated.",
Handhelds go to school: lessons learned,"Working in conjunction with teachers, researchers have developed a series of projects exploring the potential for using wireless handheld devices to enhance K-12 classroom instruction.","Handheld computers,
Portable computers,
Calculators,
Computer simulation,
Computational modeling,
Educational institutions,
Physics computing,
Computer science education,
Educational programs,
Mobile computing"
Three-dimensional motion tracking of coronary arteries in biplane cineangiograms,"A three-dimensional (3-D) method for tracking the coronary arteries through a temporal sequence of biplane X-ray angiography images is presented. A 3-D centerline model of the coronary vasculature is reconstructed from a biplane image pair at one time frame, and its motion is tracked using a coarse-to-fine hierarchy of motion models. Three-dimensional constraints on the length of the arteries and on the spatial regularity of the motion field are used to overcome limitations of classical two-dimensional vessel tracking methods, such as tracking vessels through projective occlusions. This algorithm was clinically validated in five patients by tracking the motion of the left coronary tree over one cardiac cycle. The root mean square reprojection errors were found to be submillimeter in 93% (54/58) of the image pairs. The performance of the tracking algorithm was quantified in three dimensions using a deforming vascular phantom. RMS 3-D distance errors were computed between centerline models tracked in the X-ray images and gold-standard centerline models of the phantom generated from a gated 3-D magnetic resonance image acquisition. The mean error was 0.69(/spl plusmn/0.06) mm over eight temporal phases and four different biplane orientations.",
Enhancing image and video retrieval: learning via equivalence constraints,"The paper is about learning using partial information in the form of equivalence constraints. Equivalence constraints provide relational information about the labels of data points, rather than the labels themselves. Our work is motivated by the observation that in many real life applications partial information about the data can be obtained with very little cost. For example, in video indexing we may want to use the fact that a sequence of faces obtained from successive frames in roughly the same location is likely to contain the same unknown individual. Learning using equivalence constraints is different from learning using labels and poses new technical challenges. In this paper we present three novel methods for clustering and classification, which use equivalence constraints. We provide results of our methods on a distributed image querying system that works on a large facial image database, and on the clustering and retrieval of surveillance data. Our results show that we can significantly improve the performance of image retrieval by taking advantage of such assumptions as temporal continuity in the data. Significant improvement is also obtained by making the users of the system take the role of distributed teachers, which reduces the need for expensive labeling by paid human labor.","Image retrieval,
Information retrieval,
Indexing,
Surveillance,
Humans,
Training data,
Data mining,
Computer science,
Costs,
Image databases"
A formal model for trust in dynamic networks,"We propose a formal model of trust informed by the Global Computing scenario and focusing on the aspects of trust formation, evolution, and propagation. The model is based on a novel notion of trust structures which, building on concepts from trust management and domain theory, feature at the same time a trust and an information partial order.","Intelligent networks,
Collaboration,
Engines,
Computer science,
Information security,
Humans,
Pervasive computing,
Distributed computing,
Buildings,
Internet"
Coupling dense and landmark-based approaches for nonrigid registration,"We investigate the introduction of cortical constraints for non rigid intersubject brain registration. We extract sulcal patterns with the active ribbon method, presented by Le Goualher et al. (1997). An energy based registration method (Hellier et al., 2001), which will be called photometric registration method in this paper, makes it possible to incorporate the matching of cortical sulci. The local sparse similarity and the photometric similarity are, thus, expressed in a unified framework. We show the benefits of cortical constraints on a database of 18 subjects, with global and local assessment of the registration. This new registration scheme has also been evaluated on functional magnetoencephalography data. We show that the anatomically constrained registration leads to a substantial reduction of the intersubject functional variability.","Photometry,
Magnetoencephalography,
Deformable models,
Anatomy,
Databases,
Robustness,
Labeling,
Anatomical structure,
Biomedical imaging,
Medical treatment"
"A penalized-likelihood image reconstruction method for emission tomography, compared to postsmoothed maximum-likelihood with matched spatial resolution","Regularization is desirable for image reconstruction in emission tomography. A powerful regularization method is the penalized-likelihood (PL) reconstruction algorithm (or equivalently, maximum a posteriori reconstruction), where the sum of the likelihood and a noise suppressing penalty term (or Bayesian prior) is optimized. Usually, this approach yields position-dependent resolution and bias. However, for some applications in emission tomography, a shift-invariant point spread function would be advantageous. Recently, a new method has been proposed, in which the penalty term is tuned in every pixel to impose a uniform local impulse response. In this paper, an alternative way to tune the penalty term is presented. We performed positron emission tomography and single photon emission computed tomography simulations to compare the performance of the new method to that of the postsmoothed maximum-likelihood (ML) approach, using the impulse response of the former method as the postsmoothing filter for the latter. For this experiment, the noise properties of the PL algorithm were not superior to those of postsmoothed ML reconstruction.","Image reconstruction,
Spatial resolution,
Positron emission tomography,
Bayesian methods,
Single photon emission computed tomography,
Noise level,
Reconstruction algorithms,
Optimization methods,
Computational modeling,
Filters"
Theory and evaluation of human robot interactions,"Human-robot interaction (HRI) for mobile robots is still in its infancy. Most user interactions with robots have been limited to tele-operation capabilities where the most common interface provided to the user has been the video feed from the robotic platform and some way of directing the path of the robot. For mobile robots with semiautonomous capabilities, the user is also provided with a means of setting way points. More importantly, most HRI capabilities have been developed by robotics experts for use by robotics experts. As robots increase in capabilities and are able to perform more tasks in an autonomous manner we need to think about the interactions that humans will have with robots and what software architecture and user interface designs can accommodate the human in-the-loop. We also need to design systems that can be used by domain experts but not robotics experts. This paper outlines a theory of human-robot interaction and proposes the interactions and information needed by both humans and robots for the different levels of interaction, including an evaluation methodology based on situational awareness.","Human robot interaction,
Robot sensing systems,
Mobile robots,
Software architecture,
NIST,
Feeds,
User interfaces,
Computer architecture,
Human computer interaction,
Man machine systems"
Establishing pairwise keys for secure communication in ad hoc networks: a probabilistic approach,"A prerequisite for a secure communication between two nodes in an ad hoc network is that the nodes share a key to bootstrap their trust relationship. In this paper, we present a scalable and distributed protocol that enables two nodes to establish a pairwise shared key on the fly, without requiring the use of any on-line key distribution center. The design of our protocol is based on a novel combination of two techniques - probabilistic key sharing and threshold secret sharing. Our protocol is scalable since every node only needs to possess a small number of keys, independent of the network size, and it is computationally efficient because it only relies on symmetric key cryptography based operations. We show that a pairwise key established between two nodes using our protocol is secure against a collusion attack by up to a certain number of compromised nodes. We also show through a set of simulations that our protocol can be parameterized to meet the desired levels of performance, security and storage for the application under consideration.","Intelligent networks,
Ad hoc networks,
Peer to peer computing,
Cryptographic protocols,
Public key cryptography,
Network servers,
Computer science,
Computer networks,
Secure storage,
Information systems"
Indra: a peer-to-peer approach to network intrusion detection and prevention,"While the spread of the Internet has made the network ubiquitous, it has also rendered networked systems vulnerable to malicious attacks orchestrated from anywhere. These attacks or intrusions typically start with attackers infiltrating a network through a vulnerable host and then launching further attacks on the local network or Intranet. Attackers rely on increasingly sophisticated techniques like using distributed attack sources and obfuscating their network addresses. On the other hand, software that guards against them remains rooted in traditional centralized techniques, presenting an easily-targeted single point of failure. Scalable, distributed network intrusion prevention techniques are sorely needed. We propose Indra - a distributed scheme based on sharing information between trusted peers in a network to guard the network as a whole against intrusion attempts. We present initial ideas for running Indra over a peer-to-peer infrastructure to distribute up-to-date rumors, facts, and trust information in a scalable manner.","Peer to peer computing,
Intrusion detection,
Computer science,
Laboratories,
IP networks,
Real time systems,
Humans,
Computer crime,
Centralized control,
Control systems"
Cooperative packet caching and shortest multipath routing in mobile ad hoc networks,"A mobile ad hoc network is an autonomous system of infrastructureless, multihop wireless mobile nodes. Reactive routing protocols perform well in such an environment due to their ability to cope quickly against topological changes. In this paper, we propose a new routing protocol called Caching and Multipath (CHAMP) Routing Protocol. CHAMP uses cooperative packet caching and shortest multipath routing to reduce packet loss due to frequent route breakdowns. Simulation results reveal that by using a five-packet data cache, CHAMP exhibits excellent improvement in packet delivery, outperforming AODV and DSR by at most 30% in stressful scenarios. Furthermore, end-to-end delay is significantly reduced while routing overhead is lower at high mobility rates.",
Retargetable and reconfigurable software dynamic translation,"Software dynamic translation (SDT) is a technology that permits the modification of an executing program's instructions. In recent years, SDT has received increased attention, from both industry and academia, as a feasible and effective approach to solving a variety of significant problems. Despite this increased attention, the task of initiating a new project in software dynamic translation remains a difficult one. To address this concern, and in particular, to promote the adoption of SDT technology into an even wider range of applications, we have implemented Strata, a cross-platform infrastructure for building software dynamic translators. This paper describes Strata's architecture, our experience retargeting it to three different processors, and our use of Strata to build two novel SDT systems - one for safe execution of untrusted binaries and one for fast prototyping of architectural simulators.","Virtual manufacturing,
Monitoring,
Computer science,
Emulation,
Switches,
Design optimization,
Programming,
Buildings,
Computer architecture,
Code standards"
A performance study of monitoring and information services for distributed systems,"Monitoring and information services form a key component of a distributed system, or Grid. A quantitative study of such services can aid in understanding the performance limitations, advise in the deployment of the monitoring system, and help evaluate future development work. To this end, we study the performance of three monitoring and information services for distributed systems: the Globus Toolkit/spl reg/ Monitoring and Discovery Service (MDS2), the European Data Grid Relational Grid Monitoring Architecture (R-GMA) and Hawkeye, part of the Condor project. We perform experiments to test their scalability with respect to number of users, number of resources and amount of data collected. Our study shows that each approach has different behaviors, often due to their different design goals. In the four sets of experiments we conducted to evaluate the performance of the service components under different circumstances, we found a strong advantage to caching or pre-fetching the data, as well as the need to have primary components at well-connected sites because of the high load seen by all systems.","Computerized monitoring,
Computer science,
Testing,
Scalability,
Relational databases,
Computer architecture,
Mathematics,
Laboratories,
Performance evaluation,
File systems"
Verity: a QoS metric for selecting Web services and providers,"With the proliferation of Web services, quality of service serves as a benchmark to differentiate the services and their providers. As of today, a wide spectrum of attributes have been identified to account for the quality of a service like availability, reliability, servability, performance, reputation and so on. Reputation has been measured as an average user rating and we argue that the user perception alone is not sufficient to indicate the reputation. It is necessary to measure how trustworthy the provider has been in complying with the agreed levels in the SLA (service level agreement). To quantify the consistency in compliance levels, we introduce a new QoS attribute termed verity and propose an architecture to quantify it. We argue that verity should be taken into account for a quality driven selection and composition of Web services. Reputation, when expressed as a vector of user rating, compliance and verity is a more intuitive indicator of the provider's trustworthiness.","Web services,
Quality of service,
Availability,
Network servers,
Computer science,
Software engineering,
World Wide Web,
Web server,
Internet,
Context-aware services"
Efficient channel scheduling algorithms in optical burst switched networks,"Optical burst switching (OBS) is a promising paradigm for the next-generation Internet. In OBS, a key problem is to schedule bursts on wavelength channels whose bandwidth may become fragmented with the so-called void (or idle) intervals with both fast and bandwidth efficient algorithms so as to reduce burst loss. To date, only two scheduling algorithms, called Horizon and LAUC-VF, have been proposed, which trade off bandwidth efficiency for fast running time and vice versa, respectively. In this paper, we propose several novel algorithms for scheduling bursts in OBS networks with and without fiber delay lines (FDLs). In networks without FDLs, our proposed Min-SV algorithm can schedule a burst successfully in O(logm) time, where m is the total number of void intervals, as long as there is a suitable void interval. Simulation results suggest that our algorithm achieves a loss rate which is at least as low as the best previously known algorithm LAUC-VF, but can run much faster. In fact, its speed can be almost the same as Horizon (which has a much higher loss rate). In networks with FDLs, our proposed batching FDL algorithm considers a batch of FDLs simultaneously to find a suitable FDL to delay a burst which would otherwise be discarded due to contention, instead of considering the FDLs one by one. The average search time of this algorithm is therefore significantly reduced from that of the existing sequential search algorithms.","Scheduling algorithm,
Intelligent networks,
Optical fiber networks,
Optical packet switching,
Bandwidth,
Optical buffering,
Switching circuits,
Processor scheduling,
Delay lines,
Computer science"
A Metadata Catalog Service for Data Intensive Applications,"Advances in computational, storage and network technologies as well as middle ware such as the Globus Toolkit allow scientists to expand the sophistication and scope of data-intensive applications. These applications produce and analyze terabytes and petabytes of data that are distributed in millions of files or objects. To manage these large data sets efficiently, metadata or descriptive information about the data needs to be managed. There are various types of metadata, and it is likely that a range of metadata services will exist in Grid environments that are specialized for particular types of metadata cataloguing and discovery. In this paper, we present the design of a Metadata Catalog Service (MCS) that provides a mechanism for storing and accessing descriptive metadata and allows users to query for data items based on desired attributes. We describe our experience in using the MCS with several applications and present a scalability study of the service.","Permission,
Computer networks,
Grid computing,
Analytical models,
Marine technology,
Scalability,
Bandwidth,
Data analysis,
Performance analysis,
USA Councils"
"Dynamic integrated scheduling of hard real-time, soft real-time, and non-real-time processes","Real-time systems are growing in complexity and real-time and soft real-time applications are becoming common in general-purpose computing environments. Thus, there is a growing need for scheduling solutions that simultaneously support processes with a variety of different timeliness constraints. Toward this goal we have developed the resource allocation/dispatching (RAD) integrated scheduling model and the rate-based earliest deadline (RBED) integrated multi-class real-time scheduler based on this model. We present RAD and the RBED scheduler and formally prove the correctness of the operations that RBED employs. We then describe our implementation of RBED and present results demonstrating how RBED simultaneously and seamlessly supports hard real-time, soft real-time, and best-effort processes.","Dynamic scheduling,
Resource management,
Processor scheduling,
Dispatching,
Real time systems,
Computer science,
Timing,
Application software,
Embedded computing,
Hierarchical systems"
Secret-key agreement over unauthenticated public channels .II. Privacy amplification,"For pt. II see ibid., vol.49, no.4, p.832-38 (2003). Here, we consider the special case where the legitimate partners already share a mutual string which might, however, be partially known to the adversary. The problem of generating a secret key in this case has been well studied in the passive-adversary model - for instance, in the context of quantum key agreement - under the name of privacy amplification. We consider the same problem with respect to an active adversary and propose two protocols, one based on universal hashing and one based on extractors, allowing for privacy amplification secure against an adversary whose knowledge about the initial partially secret string is limited to one third of the length of this string. Our results are based on novel techniques for authentication secure even against adversaries knowing a substantial amount of the ""secret"" key.","Privacy,
Authentication,
Cryptography,
Entropy,
Information security,
Context modeling,
Cryptographic protocols,
Computer science,
Materials science and technology,
Complexity theory"
DSP-based multiple peak power tracking for expandable power system,"A DSP-based improved maximum power point tracking (MPPT) approach for multiple solar array application is presented. It incorporates a ""shared bus"" current sharing method that can regulate many paralleled current mode DC/DC converters. The modular architecture eases the expansion of system power. The current sharing and MPPT performance of the proposed system is validated and evaluated by a 500 W prototype with two solar arrays.","Power systems,
Control systems,
DC-DC power converters,
Space vehicles,
Propulsion,
Power system control,
Voltage control,
Industrial power systems,
Computer science,
Application software"
UDDIe: an extended registry for Web services,"The Universal Description, Discovery and Integration (UDDI) is a specification for distributed Web-based information registries for Web Services. UDDI allows HTTP-enabled business services to be published, and subsequently searched, based on their interface. UDDI consists of three components: ""white pages"" to hold basic contact information and identifiers for a company, ""yellow pages"" to enable companies to be listed based on their industry categories (using standard taxonomies), and ""green pages"" to record interface details of how a Web service is to be invoked. UDDI is however limited in scope - allowing white, yellow or green pages to be searched based on a few attributes, and does not provide an automatic mechanism for updating the registry as services (and service providers) change. We implement UDDIe -an extension to UDDI, which supports the notion of ""blue pages"", to record user defined properties associated with a service - and to enable discovery of services based on these. UDDIe enables a registry to be more dynamic, by allowing services to hold a lease - a time period describing how long a service description should remain in the registry. UDDIe can co-exist with existing UDDI - and has been implemented as open-source software.","Web services,
Companies,
Java,
Computer science,
Computer industry,
Software standards,
Standards development,
Standards publication,
Taxonomy,
Mechanical factors"
Direct dynamics simulations,"With today's improved computers, scientists can obtain the potential energy gradient for a classical trajectory by solving the time-independent Schrodinger equation at each numerical integration step. The practicality of this approach-called a direct dynamics simulation-is enhanced by its use of linear scaling and semiempirical electronic structure methods.","Computational modeling,
Potential energy,
Quantum computing,
Schrodinger equation,
Computer simulation,
Nonlinear dynamical systems,
Nonlinear equations,
Solid modeling,
Chemical analysis,
Biological system modeling"
Wavelength converter placement under different RWA algorithms in wavelength-routed all-optical networks,"Sparse wavelength conversion and appropriate routing and wavelength assignment (RWA) algorithms are the two key factors in improving the blocking performance in wavelength-routed all-optical networks. It has been shown that the optimal placement of a limited number of wavelength converters in an arbitrary mesh network is an NP-complete problem. There have been various heuristic algorithms proposed in the literature, in which most of them assume that a static routing and random-wavelength assignment RWA algorithm is employed. However, the existing work shows that fixed-alternate routing and dynamic routing RWA algorithms can achieve much better blocking performance. Our study further demonstrates that the wavelength converter placement and RWA algorithms are closely related in the sense that a well-designed wavelength converter placement mechanism for a particular RWA algorithm might not work well with a different RWA algorithm. Therefore, the wavelength converter placement and the RWA have to be considered jointly. The objective of this paper is to investigate the wavelength converter placement problem under the fixed-alternate routing (FAR) algorithm and least-loaded routing (LLR) algorithm. Under the FAR algorithm, we propose a heuristic algorithm called minimum blocking probability first for wavelength converter placement. Under the LLR algorithm, we propose another heuristic algorithm called weighted maximum segment length. The objective of the converter placement algorithms is to minimize the overall blocking probability. Extensive simulation studies have been carried out over three typical mesh networks, including the 14-node NSFNET, 19-node EON, and 38-node CTNET. We observe that the proposed algorithms not only outperform existing wavelength converter placement algorithms by a large margin, but they also can achieve almost the same performance compared with full wavelength conversion under the same RWA algorithm.",
An XML-based lightweight C++ fact extractor,"A lightweight fact extractor is presented that utilizes XML tools, such as XPath and XSLT to extract static information from C++ source code programs. The source code is first converted into an XML representation, srcML, to facilitate the use of a wide variety of XML tools. The method is deemed lightweight because only a partial parsing of the source is done. Additionally, the technique is quite robust and can be applied to incomplete and noncompilable source code. The trade off to this approach is that queries on some low level details cannot be directly addressed. This approach is applied to a fact extractor benchmark as comparison with other, heavier weight, fact extractors. Fact extractors are widely used to support understanding tasks associated with maintenance, reverse engineering and various other software engineering tasks.","Data mining,
XML,
Robustness,
Software engineering,
Computer science,
Reverse engineering,
Software testing,
System testing,
Software systems,
White spaces"
Location determination of a mobile device using IEEE 802.11b access point signals,"Wireless LANs are becoming increasingly popular today, particularly those based on IEEE 802.11b standard. We study the problem of determining the location of a mobile device, which is communicating through a WLAN. We exploit the fact that the strength of the signals that a device will receive from different access points will vary with location. We build a database of signal strength information for various locations, and use this information to determine which location a given test data comes from. The problem is complicated because RF signals are affected by the noise, interference, multi-path effect, and random movement in the environment. We find that in spite of this randomness, the signal information is sufficient to detect the position of mobile device with certain error margin.","Wireless LAN,
Hardware,
Transmitters,
Testing,
Pulse measurements,
Time measurement,
Computer science,
Databases,
Working environment noise,
Radiofrequency interference"
Integrated wavelets for enhancement of microcalcifications in digital mammography,"This paper presents a new algorithm for enhancement of microcalcifications in mammograms. The main novelty is the application of techniques we have developed for construction of filterbanks derived from the continuous wavelet transform. These discrete wavelet decompositions, called integrated wavelets, are optimally designed for enhancement of multiscale structures in images. Furthermore, we use a model based approach to refine existing methods for general enhancement of mammograms resulting in a more specific enhancement of microcalcifications. We present results of our method and compare them with known algorithms. Finally, we want to indicate how these techniques can also be applied to the detection of microcalcifications. Our algorithm was positively evaluated in a clinical study. It has been implemented in a mammography workstation designed for soft-copy reading of digital mammograms developed by IMAGETOOL, Germany.","Mammography,
Continuous wavelet transforms,
Discrete wavelet transforms,
Wavelet coefficients,
Cancer detection,
Breast cancer,
Image analysis,
Image reconstruction,
Image enhancement,
Workstations"
"Bounded geometries, fractals, and low-distortion embeddings","The doubling constant of a metric space (X, d) is the smallest value /spl lambda/ such that every ball in X can be covered by /spl lambda/ balls of half the radius. The doubling dimension of X is then defined as dim (X) = log/sub 2//spl lambda/. A metric (or sequence of metrics) is called doubling precisely when its doubling dimension is bounded. This is a robust class of metric spaces which contains many families of metrics that occur in applied settings. We give tight bounds for embedding doubling metrics into (low-dimensional) normed spaces. We consider both general doubling metrics, as well as more restricted families such as those arising from trees, from graphs excluding a fixed minor, and from snowflaked metrics. Our techniques include decomposition theorems for doubling metrics, and an analysis of a fractal in the plane according to T. J. Laakso (2002). Finally, we discuss some applications and point out a central open question regarding dimensionality reduction in L/sub 2/.",
"Bounded geometries, fractals, and low-distortion embeddings",,
"Quantitative comparison of FBP, EM, and Bayesian reconstruction algorithms for the IndyPET scanner","We quantitatively compare filtered backprojection (FBP), expectation-maximization (EM), and Bayesian reconstruction algorithms as applied to the IndyPET scanner-a dedicated research scanner which has been developed for small and intermediate field of view imaging applications. In contrast to previous approaches that rely on Monte Carlo simulations, a key feature of our investigation is the use of an empirical system kernel determined from scans of line source phantoms. This kernel is incorporated into the forward model of the EM and Bayesian algorithms to achieve resolution recovery. Three data sets are used, data collected on the IndyPET scanner using a bar phantom and a Hoffman three-dimensional brain phantom, and simulated data containing a hot lesion added to a uniform background. Reconstruction quality is analyzed quantitatively in terms of bias-variance measures (bar phantom) and mean square error (lesion phantom). We observe that without use of the empirical system kernel, the FBP, EM, and Bayesian algorithms give similar performance. However, with the inclusion of the empirical kernel, the iterative algorithms provide superior reconstructions compared with FBP, both in terms of visual quality and quantitative measures. Furthermore, Bayesian methods outperform EM. We conclude that significant improvements in reconstruction quality can be realized by combining accurate models of the system response with Bayesian reconstruction algorithms.","Bayesian methods,
Reconstruction algorithms,
Image reconstruction,
Kernel,
Imaging phantoms,
Lesions,
Biomedical measurements,
Iterative algorithms,
Tomography,
Biomedical imaging"
CAMPOUT: a control architecture for tightly coupled coordination of multirobot systems for planetary surface exploration,"Exploration of high risk terrain areas such as cliff faces and site construction operations by autonomous robotic systems on Mars requires a control architecture that is able to autonomously adapt to uncertainties in knowledge of the environment. We report on the development of a software/hardware framework for cooperating multiple robots performing such tightly coordinated tasks. This work builds on our earlier research into autonomous planetary rovers and robot arms. Here, we seek to closely coordinate the mobility and manipulation of multiple robots to perform examples of a cliff traverse for science data acquisition, and site construction operations including grasping, hoisting, and transport of extended objects such as large array sensors over natural, unpredictable terrain. In support of this work we have developed an enabling distributed control architecture called control architecture for multirobot planetary outposts (CAMPOUT) wherein integrated multirobot mobility and control mechanisms are derived as group compositions and coordination of more basic behaviors under a task-level multiagent planner. CAMPOUT includes the necessary group behaviors and communication mechanisms for coordinated/cooperative control of heterogeneous robotic platforms. In this paper, we describe CAMPOUT, and its application to ongoing physical experiments with multirobot systems at the Jet Propulsion Laboratory in Pasadena, CA, for exploration of cliff faces and deployment of extended payloads.","Control systems,
Multirobot systems,
Robot kinematics,
Sensor arrays,
Communication system control,
Robot sensing systems,
Mars,
Computer architecture,
Uncertainty,
Software performance"
"A practical, decision-theoretic approach to multi-robot mapping and exploration","An important assumption underlying virtually all approaches to multi-robot exploration is prior knowledge about their relative locations. This is due to the fact that robots need to merge their maps so as to coordinate their exploration strategies. The key step in map merging is to estimate the relative locations of the individual robots. This paper presents a novel approach to multi-robot map merging under global uncertainty about the robot's relative locations. Our approach uses an adapted version of particle filters to estimate the position of one robot in the other robot's partial map. The risk of false-positive map matches is avoided by verifying match hypotheses using a rendezvous approach. We show how to seamlessly integrate this approach into a decision-theoretic multi-robot coordination strategy. The experiments show that our sample-based technique can reliably find good hypotheses for map matches. Furthermore, we present results obtained with two robots successfully merging their maps using the decision-theoretic rendezvous strategy.","Robot kinematics,
Merging,
Particle filters,
Robustness,
Computer science,
Knowledge engineering,
Artificial intelligence,
Uncertainty"
Reliable multicast MAC protocol for wireless LANs,"Reliable multicast in wireless applications is gaining importance with the development in technology. Applications like multicast file transfer, distributed computing, chat and whiteboard applications need reliability. However, due to mobility and wireless channel characteristics, obtaining reliability in data transfer is a difficult and challenging task. IEEE 802.11 does not support reliable multicast due to its inability to exchange RTS/CTS and ACKS with multiple recipients. However, several MAC layer protocols have been proposed that provide reliable multicast. For example, J. Kuri et al. [July 2001] have proposed the leader-based, probability-based, and delay-based protocols. These protocols work around the problem of multiple CTSs/ACKs colliding by providing ways to have only one of the multicast recipient nodes respond with a CTS or an ACK. These protocols perform well in low mobility wireless LANs but the performance degenerates as the mobility of nodes increases. In this paper, we discuss the inherent drawbacks of these protocols and provide an alternative approach. We present an extension to the IEEE 802.11 MAC layer protocol to provide the link level reliability to both unicast as well as multicast data communications. The extension is NAK based and uses tones, instead of conventional packets, to signal a NAK. We also incorporate dual tones, proposed by J. Deng et al. [Oct. 1998], to prevent an incoming mobile node from interrupting an ongoing transmission. Simulation results suggest that our MAC performs better than those proposed by J. Kuri et al. [July 2001] in terms of both data throughput as well as reliability.",
DIFS: a distributed index for features in sensor networks,"Sensor networks pose new challenges in the collection and distribution of data. Much attention has been focused on standing queries that use in-network aggregation of time series data to return data statistics in a communication-efficient manner. In this work, rather than consider searches over time series data, we consider searches over semantically rich high-level events, and present the design, analysis, and numerical simulations of a spatially distributed index that provides for efficient index construction and range searches. The scheme provides load balanced communication over index nodes by using the governing property that the wider the spatial extent known to an index node, the more constrained is the value range covered by that node.","Intelligent networks,
Sensor phenomena and characterization,
Costs,
Computer science,
Animals,
Floods,
Routing,
Temperature sensors,
Feeds,
Time series analysis"
Set reconciliation with nearly optimal communication complexity,"We consider the problem of efficiently reconciling two similar sets held by different hosts while minimizing the communication complexity, which we call the set reconciliation problem. We describe an approach to set reconciliation based on a polynomial encoding of sets. The resulting protocols exhibit tractable computational complexity and nearly optimal communication complexity when the sets being reconciled are sparse. Also, these protocols can be adapted to work over a broadcast channel, allowing many clients to reconcile with one host based on a single broadcast, even if each client is missing a different subset.","Complexity theory,
Protocols,
Polynomials,
Broadcasting,
Laboratories,
Computational complexity,
Distributed databases,
Information theory,
Writing,
Computer science"
What test oracle should I use for effective GUI testing?,"Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. In this paper we show that the test oracle used during testing also contributes significantly to test effectiveness and cost. A test oracle is a mechanism that determines whether software executed correctly for a test case. We define a test oracle to contain two essential parts: oracle information that represents expected output; and an oracle procedure that compares the oracle information with the actual output. By varying the level of detail of oracle information and changing the oracle procedure, a test designer can create different types of test oracles. We design 11 types of test oracles and empirically compare them on four software systems. We seed faults in software to create 100 faulty versions, execute 600 test cases on each version, for all 11 types of oracles. In all, we report results of 660,000 test runs on software. We show (1) the time and space requirements of the oracles, (2) that faults are detected early in the testing process when using detailed oracle information and complex oracle procedures, although at a higher cost per test case, and (3) that employing expensive oracles results in detecting a large number of faults using relatively smaller number of test cases.","Graphical user interfaces,
Software testing,
Costs,
Software engineering,
Fault detection,
Computer science,
Educational institutions,
System testing,
Software systems,
Application software"
Putting the 'I' in 'team': an ego-centric approach to cooperative localization,"This paper describes a cooperative method for relative localization of mobile robot teams; that is, it describes a method whereby every robot in the team can estimate the pose of every other robot, relative to itself. This robot does not require the use of GPS, landmarks, or maps of any kind; instead, robots make direct measurement of the relative pose of nearby robots, and broadcast this information to the team as a whole. Each robot processes this information independently to generate ego-centric estimate for the pose of other robots. Our method uses Bayesian formalism with a particle filter implementation, and is, as a consequence, very robust. It is also completely distributed, yet requires relatively little communication between robots. This paper describes the basic ego-centric formalism, sketches the implementation, and presents experimental results obtained using a team of four mobile robots.","Robot kinematics,
Robot sensing systems,
Mobile robots,
Broadcasting,
Global Positioning System,
Laboratories,
Computer science,
Mobile computing,
Bayesian methods,
Particle filters"
A Taxonomy for Artificial Embryogeny,"A major challenge for evolutionary computation is to evolve phenotypes such as neural networks, sensory systems, or motor controllers at the same level of complexity as found in biological organisms. In order to meet this challenge, many researchers are proposing indirect encodings, that is, evolutionary mechanisms where the same genes are used multiple times in the process of building a phenotype. Such gene reuse allows compact representations of very complex phenotypes. Development is a natural choice for implementing indirect encodings, if only because nature itself uses this very process. Motivated by the development of embryos in nature, we define artificial embryogeny (AE) as the subdiscipline of evolutionary computation (EC) in which phenotypes undergo a developmental phase. An increasing number of AE systems are currently being developed, and a need has arisen for a principled approach to comparing and contrasting, and ultimately building, such systems. Thus, in this paper, we develop a principled taxonomy for AE. This taxonomy provides a unified context for long-term research in AE, so that implementation decisions can be compared and contrasted along known dimensions in the design space of embryogenic systems. It also allows predicting how the settings of various AE parameters affect the capacity to efficiently evolve complex phenotypes.",
Achieving faster failure detection in OSPF networks,"A network running OSPF takes several tens of seconds to recover from a failure, using the current default parameter settings. The main component of this delay is the time required to detect a failure using the hello protocol. Reducing the value of the hellointerval can speed up the failure detection time. However, too small a value of the hellointerval can result in an increase in network congestion, potentially causing multiple consecutive hellos to be lost. This can lead to a false breakdown of adjacencies between routers. Such false alarms not only disrupt network traffic by causing unnecessary routing changes, but also increase the processing load on the routers, which may potentially lead to routing instability. In this paper, we investigate the following question - what is the optimal value for the hellointerval that will lead to fast failure detection in the network, while keeping occurrences of false alarms within acceptable limits? We examine the impact of both network congestion and the network topology on the optimal value for the hellointerval. Additionally, we investigate the effectiveness of faster failure detection in achieving fast failure recovery in OSPF networks.","Intelligent networks,
Network topology,
Routing protocols,
Floods,
Propagation delay,
Computational Intelligence Society,
Computer science,
Delay effects,
Electric breakdown,
Telecommunication traffic"
A bidding protocol for deploying mobile sensors,"In some harsh environments, manually deploying sensors is impossible. Alternative methods may lead to imprecise placement resulting in coverage holes. To provide the required high coverage in these situations, we propose to deploy sensor networks composed of a mixture of mobile and static sensors in which mobile sensors can move from dense areas to sparse areas to improve the overall coverage. This paper presents a bidding protocol to assist the movement of mobile sensors. In the protocol, static sensors detect coverage holes locally by using Voronoi diagrams, and bid for mobile sensors based on the size of the detected hole. Mobile sensors choose coverage holes to heal based on the bid. Simulation results show that our algorithm provides suitable tradeoff between coverage and sensor cost.",
Using deformations for browsing volumetric data,"Many traditional techniques for ""looking inside"" volumetric data involve removing portions of the data, for example using various cutting tools, to reveal the interior. This allows the user to see hidden parts of the data, but has the disadvantage of removing potentially important surrounding contextual information. We explore an alternate strategy for browsing that uses deformations, where the user can cut into and open up, spread apart, or peel away parts of the volume in real time, making the interior visible while still retaining surrounding context. We consider various deformation strategies and present a number of interaction techniques based on different metaphors. Our designs pay special attention to the semantic layers that might compose a volume (e.g. the skin, muscle, bone in a scan of a human). Users can apply deformations to only selected layers, or apply a given deformation to a different degree to each layer, making browsing more flexible and facilitating the visualization of relationships between layers. Our interaction techniques are controlled with direct, ""in place"" manipulation, using pop-up menus and 3D widgets, to avoid the divided attention and awkwardness that would come with panels of traditional widgets. Initial user feedback indicates that our techniques are valuable, especially for showing portions of the data spatially situated in context with surrounding data.","Data visualization,
Computer science,
Cutting tools,
Skin,
Muscles,
Bones,
Humans,
Feedback,
Chromium,
Computer graphics"
A finite-element approach for Young's modulus reconstruction,"Modulus imaging has great potential in soft-tissue characterization since it reveals intrinsic mechanical properties. A novel Young's modulus reconstruction algorithm that is based on finite-element analysis is reported here. This new method overcomes some limitations in other Young's modulus reconstruction methods. Specifically, it relaxes the force boundary condition requirements so that only the force distribution at the compression surface is necessary, thus making the new method more practical. The validity of the new method is demonstrated and the performance of the algorithm with noise in the input data is tested using numerical simulations. Details of how to apply this method under clinical conditions is also discussed.","Finite element methods,
Stress,
Capacitive sensors,
Elasticity,
Strain measurement,
Biological materials,
Image reconstruction,
Mechanical factors,
Biological tissues,
Biomedical imaging"
Visualization methods for time-dependent data - an overview,"Visualization has been successfully applied to analyse time-dependent data for a long time now. Lately, a number of new approaches have been introduced, promising more effective graphs especially for large datasets and multi-parameter data. In this paper, we give an overview on the visualization of time-series data and the available techniques. We provide a taxonomy and discuss general aspects of time-dependent data. After an overview on conventional techniques we discuss techniques for analysing time-dependent multivariate data sets in more detail. After this, we give an overview on dynamic presentation techniques and event-based visualization.","Data visualization,
Data analysis,
Taxonomy,
Time series analysis,
Computer science,
Data engineering,
Mathematics,
Sensor phenomena and characterization,
Humans,
History"
Generative representations for the automated design of modular physical robots,"The field of evolutionary robotics has demonstrated the ability to automatically design the morphology and controller of simple physical robots through synthetic evolutionary processes. However, it is not clear if variation-based search processes can attain the complexity of design necessary for practical engineering of robots. Here, we demonstrate an automatic design system that produces complex robots by exploiting the principles of regularity, modularity, hierarchy, and reuse. These techniques are already established principles of scaling in engineering design and have been observed in nature, but have not been broadly used in artificial evolution. We gain these advantages through the use of a generative representation, which combines a programmatic representation with an algorithmic process that compiles the representation into a detailed construction plan. This approach is shown to have two benefits: it can reuse components in regular and hierarchical ways, providing a systematic way to create more complex modules from simpler ones; and the evolved representations can capture intrinsic properties of the design space, so that variations in the representations move through the design space more effectively than equivalent-sized changes in a nongenerative representation. Using this system, we demonstrate for the first time the evolution and construction of modular, three-dimensional, physically locomoting robots, comprising many more components than previous work on body-brain evolution.",
"Recognition of cursive Roman handwriting: past, present and future","This paper reviews the state of the art in off-line Roman cursive handwriting recognition. The input provided to an off-line handwriting recognition system is an image of a digit, a word, or - more generally -some text, and the system produces, as output, an ASCII transcription of the input. This task involves a number of processing steps, some of which are quite difficult. Typically, preprocessing, normalization, feature extraction, classification, and postprocessing operations are required. We'll survey the state of the art, analyze recent trends, and try to identify challenges for future research in this field.","Handwriting recognition,
Character recognition,
Humans,
Text recognition,
Face recognition,
Computer science,
Feature extraction,
Testing,
Pattern recognition,
Vocabulary"
Slice-to-volume registration and its potential application to interventional MRI-guided radio-frequency thermal ablation of prostate cancer,"In this study, we registered live-time interventional magnetic resonance imaging (iMRI) slices with a previously obtained high-resolution MRI volume that in turn can be registered with a variety of functional images, e.g., PET, SPECT, for tumor targeting. We created and evaluated a slice-to-volume (SV) registration algorithm with special features for its potential use in iMRI-guided radio-frequency (RF) thermal ablation of prostate cancer. The algorithm features included a multiresolution approach, two similarity measures, and automatic restarting to avoid local minima. Imaging experiments were performed on volunteers using a conventional 1.5-T MR scanner and a clinical 0.2-T C-arm iMRI system under realistic conditions. Both high-resolution MR volumes and actual iMRI image slices were acquired from the same volunteers. Actual and simulated iMRI images were used to test the dependence of SV registration on image noise, receive coil inhomogeneity, and RF needle artifacts. To quantitatively assess registration, we calculated the mean voxel displacement over a volume of interest between SV registration and volume-to-volume registration, which was previously shown to be quite accurate. More than 800 registration experiments were performed. For transverse image slices covering the prostate, the SV registration algorithm was 100% successful with an error of <2 mm, and the average and standard deviation was only 0.4 mm /spl plusmn/ 0.2 mm. Visualizations such as combined sector display and contour overlay showed excellent registration of the prostate and other organs throughout the pelvis. Error was greater when an image slice was obtained at other orientations and positions, mostly because of inconsistent image content such as that from variable rectal and bladder filling. These preliminary experiments indicate that MR SV registration is sufficiently accurate to aid image-guided therapy.","Radio frequency,
Prostate cancer,
Magnetic resonance imaging,
Positron emission tomography,
Neoplasms,
High-resolution imaging,
Testing,
Coils,
Needles,
Data visualization"
Face recognition under variable lighting using harmonic image exemplars,"We propose a new approach for face recognition under arbitrary illumination conditions, which requires only one training image per subject (if there is no pose variation) and no 3D shape information. Our method is based on the result of Basri and Jacobs (2001), which demonstrated that the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace. In this paper, we show that we can recover basis images spanning this space from just one image taken under arbitrary illumination conditions. First, using a bootstrap set consisting of 3D face models, we compute a statistical model for each basis image. During training, given a novel face image under arbitrary illumination, we recover a set of images for this face. We prove that these images are the set of basis images with maximum probability. During testing, we recognize the face for which there exists a weighted combination of basis images that is the closest to the test face image. We provide a series of experiments that achieve high recognition rates, under a wide range of illumination conditions, including multiple sources of illumination. Our method achieves comparable levels of accuracy with methods that have much more onerous training data requirements.","Face recognition,
Lighting,
Shape,
Image recognition,
Testing,
Computer science,
Probability,
Training data,
Image reconstruction,
Image texture analysis"
Fast gradient methods based on global motion estimation for video compression,"This paper presents a fast global motion estimation (GME) algorithm based on gradient methods (GM), which can be used for real-time applications, such as in MPEG4 video compression. This approach improves the existing state-of-the-art GME algorithms by introducing two major modifications: first, only a small subset (down to 3%) of the original image pixels is used in the estimation process. Second, an interpolation-free formulation of the basic GM is derived, further decreasing the computational complexity. Experimental results show no loss of GME accuracy and compression efficiency compared to the MPEG-4 verification model, while reducing the computational complexity of the GME by a factor of 20.",
Challenges in location-aware computing,"The availability of massive amounts of high-quality data as well as advances in computing and communications suggest the possibility of powerful new applications in science, commerce, environment, and government-but many research challenges remain. Research in geospatial information systems is inherently multidisciplinary, and research efforts are likely to be most successful if conducted by teams that combine expertise in applications as well as information technology. Advances in location-aware computing, in particular, could have important implications not just for how geospatial data are acquired but also for how and with what quality they can be delivered and how mobile and geographically distributed systems are designed.","Mobile computing,
Computer networks,
Pervasive computing,
Councils,
Wireless sensor networks,
Personal digital assistants,
Mobile communication,
Context,
Hardware,
Handheld computers"
Improving test suites via operational abstraction,"This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text. The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications)from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults. This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.","Software testing,
Automatic testing,
Formal specifications,
Fault detection,
System testing,
Computer science,
Stacking,
Software tools,
Runtime,
Detectors"
OP-cluster: clustering by tendency in high dimensional space,"Clustering is the process of grouping a set of objects into classes of similar objects. Because of unknownness of the hidden patterns in the data sets, the definition of similarity is very subtle. Until recently, similarity measures are typically based on distances, e.g Euclidean distance and cosine distance. We propose a flexible yet powerful clustering model, namely OP-cluster (Order Preserving Cluster). Under this new model, two objects are similar on a subset of dimensions if the values of these two objects induce the same relative order of those dimensions. Such a cluster might arise when the expression levels of (coregulated) genes can rise or fall synchronously in response to a sequence of environment stimuli. Hence, discovery of OP-Cluster is essential in revealing significant gene regulatory networks. A deterministic algorithm is designed and implemented to discover all the significant OP-Clusters. A set of extensive experiments has been done on several real biological data sets to demonstrate its effectiveness and efficiency in detecting coregulated patterns.","Euclidean distance,
Computer science,
Clustering algorithms,
Algorithm design and analysis,
Data analysis,
Pattern analysis,
Databases,
Statistical analysis,
Machine learning,
Pattern recognition"
Compiler optimization-space exploration,"To meet the demands of modern architectures, optimizing compilers must incorporate an ever larger number of increasingly complex transformation algorithms. Since code transformations may often degrade performance or interfere with subsequent transformations, compilers employ predictive heuristics to guide optimizations by predicting their effects a priori. Unfortunately, the unpredictability of optimization interaction and the irregularity of today's wide-issue machines severely limit the accuracy of these heuristics. As a result, compiler writers may temper high variance optimizations with overly conservative heuristics or may exclude these optimizations entirely. While this process results in a compiler capable of generating good average code quality across the target benchmark set, it is at the cost of missed optimization opportunities in individual code segments. To replace predictive heuristics, researchers have proposed compilers which explore many optimization options, selecting the best one a posteriori. Unfortunately, these existing iterative compilation techniques are not practical for reasons of compile time and applicability. We present the Optimization-Space Exploration (OSE) compiler organization, the first practical iterative compilation strategy applicable to optimizations in general-purpose compilers. Instead of replacing predictive heuristics, OSE uses the compiler writer's knowledge encoded in the heuristics to select a small number of promising optimization alternatives for a given code segment. Compile time is limited by evaluating only these alternatives for hot code segments using a general compile-time performance estimator An OSE-enhanced version of Intel's highly-tuned, aggressively optimizing production compiler for IA-64 yields a significant performance improvement, more than 20% in some cases, on Itanium for SPEC codes.","Optimizing compilers,
Computer architecture,
Resource management,
Microarchitecture,
Computer science,
Degradation,
Cost function,
Production,
Parallel processing,
Performance gain"
A survey of camera self-calibration,"The paper surveys the developments of the last 10 years in the area of camera self-calibration. Self-calibration is an attempt to calibrate camera by finding intrinsic parameters that are consistent with the underlying projective geometry of a sequence of images. In order to solve this problem, the camera intrinsic constraints have been used separately and in conjunction with camera motion constraints or scene constraints. Most self-calibration algorithms are concerned with unknown but constant intrinsic camera parameters. Recently, camera self-calibration in the case of varying intrinsic camera parameters was also studied. We present the basic theories behind the different self-calibration techniques and discuss the ideas behind most of the self-calibration algorithms.","Cameras,
Layout,
Geometry,
Solid modeling,
Equations,
Calibration,
Computer science,
Focusing,
Augmented reality,
H infinity control"
Performance evaluation of load-balanced clustering of wireless sensor networks,"Wireless sensor networks have received increasing attention in recent few years. In many military and civil applications of sensor networks, sensors are constrained in onboard energy supply and are left unattended. Energy, size and cost constraints of such sensors limit the communication range. Therefore, multi-hop wireless connectivity is required to forward data on their behalf to a remote command site. In this paper, we investigate the performance of an algorithm to network these sensors into well defined clusters with less-energy-constrained gateway nodes acting as clusterheads as well as to balance the load among these gateways. Load balanced clustering increases the system stability and improves the communication between different nodes in the system. To evaluate the efficiency of this approach, we studied the performance of sensor networks by applying various different routing protocols. Simulation results shows that irrespective of the routing protocol used, this approach improves the lifetime of the system.","Wireless sensor networks,
Sensor systems,
Sensor fusion,
Routing protocols,
Temperature sensors,
Energy management,
Signal processing,
Batteries,
Computer science,
Military computing"
Wireless sensor placement for reliable and efficient data collection,"Sensors can be paired with radio units and deployed to form a wireless ad-hoc sensor network. Actual deployments must consider the coverage that can be achieved with a given number of sensors: this coverage varies with the range of the radios and the maximum allowable distance between any point in the area and the nearest sensor. Deployments must also preserve connectivity in spite of possible failure or energy depletion in a subset of the units. This paper presents and analyzes a variety of regular deployment topologies, including circular and star deployments as well as deployments in square, triangular, and hexagonal grids.","Wireless sensor networks,
Biosensors,
Wireless communication,
Temperature sensors,
Relays,
Base stations,
Robot sensing systems,
Human robot interaction,
Temperature measurement,
Computer network reliability"
Observational determinism for concurrent program security,"Noninterference is a property of sequential programs that is useful for expressing security policies for data confidentiality and integrity. However, extending noninterference to concurrent programs has proved problematic. In this paper we present a relatively expressive secure concurrent calculi, provides first-class channels, high-order functions, and an unbounded number of threads. Well-typed programs obey a generalization of noninterference that ensures immunity to internal timing attacks and to attacks that exploit information about the thread scheduler. Elimination of these refinement attacks is possible because the enforced security property extends noninterference with observational determinism. Although the security property is strong, it also avoids some of the restrictiveness imposed on previous security-typed concurrent languages.",
Linear scaling electronic structure methods in chemistry and physics,Calculating the electronic structure of large atomistic systems requires algorithms that scale linearly with system size. Efficient implementations of these emerging algorithms provide scientists in various fields with powerful software tools to address challenging problems.,"Chemistry,
Quantum mechanics,
Physics computing,
Schrodinger equation,
Quantum computing,
Atomic measurements,
Mathematics,
Electrons,
Mechanical factors,
Chemicals"
Towards an autonomic computing environment,"Autonomic Computing is a promising new concept in system development. It aims to (i) increase reliability by designing systems to be self-protecting and self-healing; and (ii) increase autonomy and performance by enabling systems to adapt to changing circumstances, using self-configuring and self-optimizing mechanisms. This paper discusses the type of system architecture needed to support such objectives.","Computer architecture,
Humans,
Conferences,
Databases,
Expert systems,
Mathematics,
Reliability engineering,
Informatics,
Autonomic nervous system,
Computer science"
A linear wavelet filter for parametric imaging with dynamic PET,"Describes a new filter for parametric images obtained from dynamic positron emission tomography (PET) studies. The filter is based on the wavelet transform following the heuristics of a previously published method that are here developed into a rigorous theoretical framework. It is shown that the space-time problem of modeling a dynamic PET sequence reduces to the classical one of estimation of a normal multivariate vector of independent wavelet coefficients that, under least-squares risk, can be solved by straightforward application of well established theory. From the study of the distribution of wavelet coefficients of PET images, it is inferred that a James-Stein linear estimator is more suitable for the problem than traditional nonlinear procedures that are incorporated in standard wavelet filters. This is confirmed by the superior performance of the James-Stein filter in simulation studies compared to a state-of-the-art nonlinear wavelet filter and a nonstationary filter selected from literature. Finally, the formal framework is interpreted for the practitioner's point of view and advantages and limitations of the method are discussed.","Nonlinear filters,
Positron emission tomography,
Kinetic theory,
Image reconstruction,
Hospitals,
Wavelet transforms,
Wavelet coefficients,
Vectors,
Wavelet analysis,
In vivo"
Automatic particle detection through efficient Hough transforms,"Manual selection of single particles in images acquired using cryo-electron microscopy (cryoEM) will become a significant bottleneck when a very large number of images are required to achieve three-dimensional reconstructions at near atomic resolution. Investigation of fast, accurate approaches for automatic particle detection has become one of the current challenges in the cryoEM community. At the same time, the investigation is hampered by the fact that few benchmark particles or image datasets exist in the community. The unavailability of such data makes it difficult to evaluate newly developed algorithms and to leverage expertise from other disciplines. The paper presents our recent contribution to this effort. It also describes our newly developed computational framework for particle detection, through the application of edge detection and a sequence of ordered Hough transforms. Experimental results using keyhole limpet hemocyanin (KLH) as a model particle are very promising. In addition, it introduces a newly established web site, designed to support the investigation of automatic particle detection by providing an annotated image dataset of KLH available to the general scientific community.","Transmission electron microscopy,
Image reconstruction,
Image edge detection,
Image resolution,
Image analysis,
Three dimensional displays,
Concurrent computing,
Sequences,
Biological system modeling,
Web page design"
Human-like motion of a humanoid robot arm based on a closed-form solution of the inverse kinematics problem,"Humanoid robotics is a new challenging field. To cooperate with human beings, humanoid robots not only have to feature human-like form and structure but, more importantly, they must possess human-like characteristics regarding motion, communication and intelligence. In this paper, we propose an algorithm for solving the inverse kinematics problem associated with the redundant robot arm of the humanoid robot ARMAR. The formulation of the problem is based on the decomposition of the workspace of the arm and on the analytical description of the redundancy of the arm. The solution obtained is characterized by its accuracy and low cost of computation. The algorithm is enhanced in order to generate human-like manipulation motions from object trajectories.","Humanoid robots,
Closed-form solution,
Humans,
Redundancy,
Shoulder,
Robot kinematics,
Computer industry,
Application software,
Informatics,
Computer science"
An improvement of rotation invariant 3D-shape based on functions on concentric spheres,"In this paper, we consider 3D-shape descriptors generated by using functions on a sphere. The descriptors are engaged for retrieving polygonal mesh models. Invariance of descriptors with respect to rotation of a model can be achieved either by using the principle component analysis (PCA) or defining features in which the invariance exists. The contribution of the paper is twofold: firstly, we define a new rotation invariant feature vector based on functions on concentric spheres, which outperforms a recently proposed descriptor; secondly, we compare the two approaches for achieving rotation invariance as well as options to use a single function or several functions on concentric spheres to generate feature vectors. We conclude that descriptors, which use the PCA, outperform others, while capturing the internal structure of a 3D-model with functions on concentric spheres can improve retrieval effectiveness.","Principal component analysis,
Information science,
Computer vision,
Shape,
Internet,
Feature extraction,
Research initiatives,
Fourier transforms,
Reflection"
How to misuse AODV: a case study of insider attacks against mobile ad-hoc routing protocols,"We present a systematic analysis of insider attacks against mobile ad-hoc routing protocols, using the Ad hoc On-Demand Distance Vector (AODV) protocol as an example. It identifies a number of attack goals and then studies how to achieve these goals through misuses of the routing messages. To facilitate the analysis, we classify the insider attacks into two categories: atomic misuses and compound misuses. Atomic misuses are performed by manipulating a single routing message, which cannot be further divided; compound misuses are composed of combinations of atomic misuses and possibly normal uses of the routing protocol. The analysis results reveal several classes of insider attacks, including route disruption, route invasion, node isolation, and resource consumption. We also describe simulation results that demonstrate the impact of these attacks.",
Discovering clusters in motion time-series data,"An approach is proposed for clustering time-series data. The approach can be used to discover groupings of similar object motions that were observed in a video collection. A finite mixture of hidden Markov models (HMMs) is fitted to the motion data using the expectation maximization (EM) framework. Previous approaches for HMM-based clustering employ a k-means formulation, where each sequence is assigned to only a single HMM. In contrast, the formulation presented in this paper allows each sequence to belong to more than a single HMM with some probability, and the hard decision about the sequence class membership can be deferred until a later time when such a decision is required. Experiments with simulated data demonstrate the benefit of using this EM-based approach when there is more ""overlap"" in the processes generating the data. Experiments with real data show the promising potential of HMM-based motion clustering in a number of applications.","Hidden Markov models,
Sequences,
Computer vision,
Computer science,
Explosives,
Vehicles,
Humans,
Animals,
Tracking,
Organizing"
"Chisel: a policy-driven, context-aware, dynamic adaptation framework","We argue that the software user, the developer, the designer and indeed the application logic itself all possess invaluable intelligence to gear how software should adapt itself to changing requirements and changing context. We present Chisel, an open framework for dynamic adaptation of services using reflection in a policy-driven, context-aware manner. The system is based on decomposing the particular aspects of a service object that do not provide its core functionality into multiple possible behaviours. As the execution environment, user context and application context change, the service object will be adapted to use different behaviours, driven by a human-readable declarative adaptation policy script. To demonstrate this framework we will provide a dynamically adaptive middleware for mobile computing. The framework will allow users and applications to make mobile-aware dynamic changes to the behaviour of various services of the middleware, and allow the addition of new unanticipated behaviours at run-time, without changing or stopping the middleware or an application that may be using it. This is achieved by implementing the behaviours as metatypes in Iguana/J, which supports non-invasive dynamic associations of metatypes to service objects without any requirement to interrupt, change or access the object's source code.","Reflection,
Context-aware services,
Middleware,
Computer science,
Educational institutions,
Application software,
Logic design,
Gears,
Mobile computing,
Runtime"
Approximation by Fully Complex Multilayer Perceptrons,"We investigate the approximation ability of a multi layer perceptron (MLP) network when it is extended to the complex domain. The main challenge for processing complex data with neural networks has been the lack of bounded and analytic complex nonlinear activation functions in the complex domain, as stated by Liouville's theorem. To avoid the conflict between the boundedness and the analyticity of a nonlinear complex function in the complex domain, a number of ad hoc MLPs that include using two real-valued MLPs, one processing the real part and the other processing the imaginary part, have been traditionally employed. However, since nonanalytic functions do not meet the Cauchy-Riemann conditions, they render themselves into degenerative backpropagation algorithms that compromise the efficiency of nonlinear approximation and learning in the complex vector field. A number of elementary transcendental functions (ETFs) derivable from the entire exponential functionez that are analytic are defined as fully complex activation functions and are shown to provide a parsimonious structure for processing data in the complex domain and address most of the shortcomings of the traditional approach. The introduction of ETFs, however, raises a new question in the approximation capability of this fully complex MLP. In this letter, three proofs of the approximation capability of the fully complex MLP are provided based on the characteristics of singularity among ETFs. First, the fully complex MLPs with continuous ETFs over a compact set in the complex vector field are shown to be the universal approximator of any continuous complex mappings. The complex universal approximation theorem extends to bounded measurable ETFs possessing a removable singularity. Finally, it is shown that the output of complex MLPs using ETFs with isolated and essential singularities uniformly converges to any nonlinear mapping in the deleted annulus of singularity nearest to the origin.",
Hardness of approximating the minimum distance of a linear code,"We show that the minimum distance d of a linear code is not approximable to within any constant factor in random polynomial time (RP), unless nondeterministic polynomial time (NP) equals RP. We also show that the minimum distance is not approximable to within an additive error that is linear in the block length n of the code. Under the stronger assumption that NP is not contained in random quasi-polynomial time (RQP), we show that the minimum distance is not approximable to within the factor 2/sup log1-/spl epsi//(n), for any /spl epsi/>0. Our results hold for codes over any finite field, including binary codes. In the process, we show that it is hard to find approximately nearest codewords even if the number of errors exceeds the unique decoding radius d/2 by only an arbitrarily small fraction /spl epsi/d. We also prove the hardness of the nearest codeword problem for asymptotically good codes, provided the number of errors exceeds (2/3)d. Our results for the minimum distance problem strengthen (though using stronger assumptions) a previous result of Vardy (1997) who showed that the minimum distance cannot be computed exactly in deterministic polynomial time (P), unless P = NP. Our results are obtained by adapting proofs of analogous results for integer lattices due to Ajtai (1998) and Micciancio (see SIAM J. Computing, vol.30, no.6, p.2008-2035, 2001). A critical component in the adaptation is our use of linear codes that perform better than random (linear) codes.","Linear code,
Vectors,
Galois fields,
Decoding,
Computational complexity,
Error correction codes,
Engineering profession,
Computer science,
Hamming distance,
Additives"
On constant-composition codes over Z/sub q/,"A constant-composition code is a special constant-weight code under the restriction that each symbol should appear a given number of times in each codeword. In this correspondence, we give a lower bound for the maximum size of the q-ary constant-composition codes with minimum distance at least 3. This bound is asymptotically optimal and generalizes the Graham-Sloane bound for binary constant-weight codes. In addition, three construction methods of constant-composition codes are presented, and a number of optimum constant-composition codes are obtained by using these constructions.","Mathematics,
Concatenated codes,
Computer science education,
Educational programs,
Error correction,
Error correction codes,
Upper bound,
Modular construction,
Modulation coding,
Computer science"
A theory of multiplexed illumination,"Imaging of objects under variable lighting directions is an important and frequent practice in computer vision and image-based rendering. We introduce an approach that significantly improves the quality of such images. Traditional methods for acquiring images under variable illumination directions use only a single light source per acquired image. In contrast, our approach is based on a multiplexing principle, in which multiple light sources illuminate the object simultaneously from different directions. Thus, the object irradiance is much higher. The acquired images are then computationally demultiplexed. The number of image acquisitions is the same as in the single-source method. The approach is useful for imaging dim object areas. We give the optimal code by which the illumination should be multiplexed to obtain the highest quality output. For n images corresponding to n light sources, the noise is reduced by /spl radic/(n)/2 relative to the signal. This noise reduction translates to a faster acquisition time or an increase in density of illumination direction samples. It also enables one to use lighting with high directional resolution using practical setups, as we demonstrate in our experiments.","Lighting,
Light sources,
Computer vision,
Rendering (computer graphics),
Noise reduction,
Object recognition,
Demultiplexing,
Stress,
Computer science,
Image resolution"
Using benchmarking to advance research: a challenge to software engineering,"Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.","Software engineering,
Collaboration,
Computer science,
Information retrieval,
Software algorithms,
Databases,
Reverse engineering,
Computer languages,
Books,
Guidelines"
Automated feature-based range registration of urban scenes of large scale,"We are building a system that can automatically acquire 3D range scans and 2D images to build geometrically and photometrically correct 3D models of urban environments. A major bottleneck in the process is the automated registration of a large number of geometrically complex 3D range scans in a common frame of reference. In this paper we provide a method for the accurate and efficient registration of a large number of complex range scans. The method utilizes range segmentation and feature extraction algorithms. Our algorithm automatically computes pairwise registrations between individual scans, builds a topological graph, and places the scans in the same frame of reference. We present results for building large scale 3D models of historic sites and urban structures.","Layout,
Large-scale systems,
Solid modeling,
Photometry,
Image reconstruction,
Laser modes,
Data mining,
Computer science,
Educational institutions,
Feature extraction"
Kernel independent component analysis,"We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.","Kernel,
Independent component analysis,
Vectors,
Graphical models,
Computer science,
Unsupervised learning,
Yield estimation,
Hilbert space,
Robustness,
Computational modeling"
A normal distribution for tensor-valued random variables: applications to diffusion tensor MRI,"Diffusion tensor magnetic resonance imaging (DT-MRI) provides a statistical estimate of a symmetric, second-order diffusion tensor of water, D, in each voxel within an imaging volume. We propose a new normal distribution, p(D) /spl prop/ exp(-1/2 D : A : D), which describes the variability of D in an ideal DT-MRI experiment. The scalar invariant, D : A : D, is the contraction of a positive definite symmetric, fourth-order precision tensor, A, and D. A correspondence is established between D : A : D and the elastic strain energy density function in continuum mechanics-specifically between D and the second-order infinitesimal strain tensor, and between A and the fourth-order tensor of elastic coefficients. We show that A can be further classified according to different classical elastic symmetries (i.e., isotropy, transverse isotropy, orthotropy, planar symmetry, and anisotropy). When A is an isotropic fourth-order tensor, we derive an explicit analytic expression for p(D), and for the distribution of the three eigenvalues of D, p(/spl gamma//sub 1/, /spl gamma//sub 2/, /spl gamma//sub 3/), which are confirmed by Monte Carlo simulations. We show how A can be estimated from either real or synthetic DT-MRI data for any given experimental design. Here we propose a new criterion for an optimal experimental design: that A be an isotropic fourth-order tensor. This condition ensures that the statistical properties of D (and quantities derived from it) are rotationally invariant. We also investigate the degree of isotropy of several DT-MRI experimental designs. Finally, we show that the univariate and multivariate distributions are special cases of the more general tensor-variate normal distribution, and suggest how to generalize p(D) to treat normal random tensor variables that are of third- (or higher) order. We expect that this new distribution, p(D), should be useful in feature extraction; in developing a hypothesis testing framework for segmenting and classifying noisy, discrete tensor data; and in designing experiments to measure tensor quantities.","Gaussian distribution,
Random variables,
Tensile stress,
Magnetic resonance imaging,
Diffusion tensor imaging,
Design for experiments,
Capacitive sensors,
Density functional theory,
Anisotropic magnetoresistance,
Eigenvalues and eigenfunctions"
A complexity reducing transformation in algebraic list decoding of Reed-Solomon codes,"The main computational steps in algebraic soft decoding, as well as Sudan-type list decoding, of Reed-Solomon codes are interpolation and factorization. A series of transformations is given for the interpolation problem that arises in these decoding algorithms. These transformations reduce the space and time complexity to a small fraction of the complexity of the original interpolation problem. A factorization procedure that applies directly to the reduced interpolation problem is also presented.","Decoding,
Reed-Solomon codes,
Interpolation,
Polynomials,
Error correction codes,
Digital communication,
Memory,
Error correction,
Galois fields,
Computer science"
Practical aspects of a data-driven motion correction approach for brain SPECT,"Patient motion can cause image artifacts in single photon emission computed tomography despite restraining measures. Data-driven detection and correction of motion can be achieved by comparison of acquired data with the forward projections. This enables the brain locations to be estimated and data to be correctly incorporated in a three-dimensional (3-D) reconstruction algorithm. Digital and physical phantom experiments were performed to explore practical aspects of this approach. Noisy simulation data modeling multiple 3-D patient head movements were constructed by projecting the digital Hoffman brain phantom at various orientations. Hoffman physical phantom data incorporating deliberate movements were also gathered. Motion correction was applied to these data using various regimes to determine the importance of attenuation and successive iterations. Studies were assessed visually for artifact reduction, and analyzed quantitatively via a mean registration error (MRE) and mean square difference measure (MSD). Artifacts and distortion in the motion corrupted data were reduced to a large extent by application of this algorithm. MRE values were mostly well within 1 pixel (4.4 mm) for the simulated data. Significant MSD improvements (>2) were common. Inclusion of attenuation was unnecessary to accurately estimate motion, doubling the efficiency and simplifying implementation. Moreover, most motion-related errors were removed using a single iteration. The improvement for the physical phantom data was smaller, though this may be due to object symmetry. In conclusion, these results provide the basis of an implementation protocol for clinical validation of the technique.","Imaging phantoms,
Brain modeling,
Attenuation,
Single photon emission computed tomography,
Motion measurement,
Motion detection,
Reconstruction algorithms,
Head,
Distortion measurement,
Motion estimation"
3D object modeling and recognition using affine-invariant patches and multi-view spatial constraints,"This paper presents a representation for three-dimensional objects in terms of affine-invariant image patches and their spatial relationships. Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction, allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint. The proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes. Preliminary modeling and recognition results are presented.","Object recognition,
Image recognition,
Layout,
Shape,
Geometry,
Computer science,
Image reconstruction,
Image segmentation,
Brightness,
Pattern matching"
Level-set-based artery-vein separation in blood pool agent CE-MR angiograms,"Blood pool agents (BPAs) for contrast-enhanced (CE) magnetic-resonance angiography (MRA) allow prolonged imaging times for higher contrast and resolution. Imaging is performed during the steady state when the contrast agent is distributed through the complete vascular system. However, simultaneous venous and arterial enhancement in this steady state hampers interpretation. In order to improve visualization of the arteries and veins from steady-state BPA data, a semiautomated method for artery-vein separation is presented. In this method, the central arterial axis and central venous axis are used as initializations for two surfaces that simultaneously evolve in order to capture the arterial and venous parts of the vasculature using the level-set framework. Since arteries and veins can be in close proximity of each other, leakage from the evolving arterial (venous) surface into the venous (arterial) part of the vasculature is inevitable. In these situations, voxels are labeled arterial or venous based on the arrival time of the respective surface. The evolution is steered by external forces related to feature images derived from the image data and by internal forces related to the geometry of the level sets. In this paper, the robustness and accuracy of three external forces (based on image intensity, image gradient, and vessel-enhancement filtering) and combinations of them are investigated and tested on seven patient datasets. To this end, results with the level-set-based segmentation are compared to the reference-standard manually obtained segmentations. Best results are achieved by applying a combination of intensity- and gradient-based forces and a smoothness constraint based on the curvature of the surface. By applying this combination to the seven datasets, it is shown that, with minimal user interaction, artery-vein separation for improved arterial and venous visualization in BPA CE-MRA can be achieved.","Blood,
Steady-state,
High-resolution imaging,
Arteries,
Veins,
Image segmentation,
Magnetic separation,
Angiography,
Image resolution,
Data visualization"
Homeostatic and tendency-based CPU load predictions,"The dynamic nature of a resource-sharing environment means that applications must be able to adapt their behavior in response to changes in system status. Predictions of future system performance can be used to guide such adaptations. In this paper, we present and evaluate several new one-step-ahead and low-overhead time series prediction strategies that track recent trends by giving more weight to recent data. We present results that show that a dynamic tendency prediction model with different ascending and descending behavior performs best among all strategies studied. A comparative study conducted on a set of 38 machine load traces shows that this new predictor achieves average prediction errors that are between 2% and 55% less (36% less on average) than those incurred by the predictors used within the popular Network Weather Service system.","Predictive models,
Weather forecasting,
Biomedical measurements,
Sea measurements,
Distributed computing,
Processor scheduling,
History,
Computer science,
System performance,
Time measurement"
Three-dimensional database of subcortical electrophysiology for image-guided stereotactic functional neurosurgery,"We present a method of constructing a database of intraoperatively observed human subcortical electrophysiology. In this approach, patient electrophysiological data are standardized using a multiparameter coding system, annotated to their respective magnetic resonance images (MRIs), and nonlinearly registered to a high-resolution MRI reference brain. Once registered, we are able to demonstrate clustering of like interpatient physiologic responses within the thalamus, globus pallidus, subthalamic nucleus, and adjacent structures. These data may in turn be registered to a three-dimensional patient MRI within our image-guided visualization program enabling prior to surgery the delineation of surgical targets, anatomy with high probability of containing specific cell types, and functional borders. The functional data were obtained from 88 patients (106 procedures) via microelectrode recording and electrical stimulation performed during stereotactic neurosurgery at the London Health Sciences Centre. Advantages of this method include the use of nonlinear registration to accommodate for interpatient anatomical variability and the avoidance of digitized versions of printed atlases of anatomy as a common database coordinate system. The resulting database is expandable, easily searched using a graphical user interface, and provides a visual representation of functional organization within the deep brain.","Image databases,
Neurosurgery,
Magnetic resonance imaging,
Surgery,
Anatomy,
Visual databases,
Humans,
Electrophysiology,
Image coding,
Magnetic resonance"
"Grids, the TeraGrid and beyond","The correlation, combination, and statistical analysis of large data volumes derived from multiple sources depend on joining a new generation of high-resolution scientific instruments, high-performance computing systems, and large-scale scientific data archives via high-speed networks and a software infrastructure that enables resource and data sharing by collaborating groups of distributed researchers. Scheduled for completion in 2003, the National Science Foundation's TeraGrid, a massive research computing infrastructure, will combine five large computing and data management facilities and support many additional academic institutions and research laboratories in just such endeavors. When operational, the TeraGrid will help researchers solve problems in fields such as genomics, biology, and high-energy physics.","Collaborative software,
Biology computing,
High energy physics instrumentation computing,
Statistical analysis,
Instruments,
Computer networks,
Distributed computing,
Large-scale systems,
High-speed networks,
Processor scheduling"
Multi-scale phase-based local features,"Local feature methods suitable for image feature based object recognition and for the estimation of motion and structure are composed of two steps, namely the 'where' and 'what' steps. The 'where' step (e.g., interest point detector) must select image points that are robustly localizable under common image deformations and whose neighborhoods are relatively informative. The 'what' step (e.g., local feature extractor) then provides a representation of the image neighborhood that is semi-invariant to image deformations, but distinctive enough to provide model identification. We present a quantitative evaluation of both the 'where' and the 'what' steps for three recent local feature methods: a) phase-based local features (Carneiro and Jepson, 2002), b) differential invariants (Schmid and Mohr, 1997), and c) the scale invariant feature transform (SIFT) (Lowe, 1999). Moreover, in order to make the phase-based approach more comparable to the other two approaches, we also introduce a new form of multi-scale interest point detector to be used for its 'where' step. The results show that the phase-based local features lead to better performance than the other two approaches when dealing with common illumination changes, 2D rotation, and sub-pixel translation. On the other hand, the phase-based local features are somewhat more sensitive to scale and large shear changes than the other two methods. Finally, we demonstrate the viability of the phase-based local feature in a simple object recognition system.","Detectors,
Robustness,
Feature extraction,
Object recognition,
Motion estimation,
Data mining,
Principal component analysis,
Computer science,
Deformable models,
Phase detection"
Helical pinhole SPECT for small-animal imaging: a method for addressing sampling completeness,"Pinhole collimators are widely used to image small organs and small animals because sensitivity and resolution improve as the distance between the aperture and the object decreases. Axial blurring is present in reconstruction of SPECT projection data when pinhole apertures follow a circular orbit because the object is incompletely sampled. For an object with constant axial extent, the blurring worsens as the radius of rotation (ROR) decreases. In contrast, helical orbits of pinhole collimators can give complete sampling at small ROR, where sensitivity and resolution are improved. Herein, a metric of sampling completeness is introduced. It is used to evaluate the sampling of an object as a function of ROR, axial position, and radial position for circular orbits. The metric is also used to determine the completely sampled volume for a helical orbit of a pinhole aperture. Experimental and computer-simulated projections of circular orbits and helical orbits are reconstructed, yielding similar results; helical orbits reduce axial blurring because of their sampling properties.","Sampling methods,
Collimators,
Apertures,
Biomedical imaging,
Computed tomography,
Image sampling,
Animals,
Image reconstruction,
Single photon emission computed tomography,
Biomedical engineering"
Edgelens: an interactive method for managing edge congestion in graphs,"An increasing number of tasks require people to explore, navigate and search extremely complex data sets visualized as graphs. Examples include electrical and telecommunication networks, Web structures, and airline routes. The problem is that graphs of these real world data sets have many interconnected nodes, ultimately leading to edge congestion: the density of edges is so great that they obscure nodes, individual edges, and even the visual information beneath the graph. To address this problem we developed an interactive technique called EdgeLens. An EdgeLens interactively curves graph edges away for a person's focus attention without changing the node positions. This opens up sufficient space to disambiguate node and edge relationships and to see underlying information while still preserving node layout. Initially two methods of creating this interaction were developed and compared in a user study. The results of this study were used in the selection of a basic approach and the subsequent development of the EdgeLens. We then improved the EdgeLens through use of transparency and colour and by allowing multiple lenses to appear on the graph.","Data visualization,
Computer science,
Cities and towns,
Navigation,
Lenses,
Telecommunication congestion control,
Chromium,
Layout,
Telephony,
Power grids"
Electrical conductivity imaging via contactless measurements: an experimental study,A data-acquisition system has been developed to image electrical conductivity of biological tissues via contactless measurements. This system uses magnetic excitation to induce currents inside the body and measures the resulting magnetic fields. The data-acquisition system is constructed using a PC-controlled lock-in amplifier instrument. A magnetically coupled differential coil is used to scan conducting phantoms by a computer controlled scanning system. A 10000-turn differential coil system with circular receiver coils of radii 15 mm is used as a magnetic sensor. The transmitter coil is a 100-turn circular coil of radius 15 mm and is driven by a sinusoidal current of 200 mA (peak). The linearity of the system is 7.2% full scale. The sensitivity of the system to conducting tubes when the sensor-body distance is 0.3 cm is 21.47 mV/(S/m). It is observed that it is possible to detect a conducting tube of average conductivity (0.2 S/m) when the body is 6 cm from the sensor. The system has a signal-to-noise ratio of 34 dB and thermal stability of 33.4 mV//spl deg/C. Conductivity images are reconstructed using the steepest-descent algorithm. Images obtained from isolated conducting tubes show that it is possible to distinguish two tubes separated 17 mm from each other. The images of different phantoms are found to be a good representation of the actual conductivity distribution. The field profiles obtained by scanning a biological tissue show the potential of this methodology for clinical applications.,"Contacts,
Conductivity measurement,
Electric variables measurement,
Coils,
Magnetic field measurement,
Biological tissues,
Magnetic separation,
Imaging phantoms,
Current measurement,
Amplifiers"
How history justifies system architecture (or not),"The revision history of a software system conveys important information about how and why the system evolved in time. The revision history can also tell us which parts of the system are coupled by common changes: ""whenever the database schema was changed, the sqlquery() method was altered, too."" This ""evolutionary"" coupling can be compared with the coupling as imposed by the system architecture; differences indicate anomalies which may be subject to restructuring. Our ROSE prototype analyzes fine-grained coupling between software entities as indicated by common changes. It turns out that common changes are a good indicator for modularity, that evolutionary coupling should be determined between syntactical entities (rather than files or modules), and that common changes can indicate coupling between software entities and nonprogram artifacts that is unavailable to the analysis of a single version.","History,
Computer architecture,
Computer science,
Software systems,
Software prototyping,
Software design,
Visual databases,
Visualization,
Program processors,
Prototypes"
Learning dynamics for exemplar-based gesture recognition,"This paper addresses the problem of capturing the dynamics for exemplar-based recognition systems. Traditional HMM provides a probabilistic tool to capture system dynamics and in exemplar paradigm, HMM states are typically coupled with the exemplars. Alternatively, we propose a non-parametric HMM approach that uses a discrete HMM with arbitrary states (decoupled from exemplars) to capture the dynamics over a large exemplar space where a nonparametric estimation approach is used to model the exemplar distribution. This reduces the need for lengthy and non-optimal training of the HMM observation model. We used the proposed approach for view-based recognition of gestures. The approach is based on representing each gesture as a sequence of learned body poses (exemplars). The gestures are recognized through a probabilistic framework for matching these body poses and for imposing temporal constraints between different poses using the proposed non-parametric HMM.","Hidden Markov models,
Humans,
Motion analysis,
Computer vision,
Biological system modeling,
Training data,
Graphical models,
Computer science,
Laboratories,
Educational institutions"
ROC analysis of ultrasound tissue characterization classifiers for breast cancer diagnosis,"Breast cancer diagnosis through ultrasound tissue characterization was studied using receiver operating characteristic (ROC) analysis of combinations of acoustic features, patient age, and radiological findings. A feature fusion method was devised that operates even if only partial diagnostic data are available. The ROC methodology uses ordinal dominance theory and bootstrap resampling to evaluate A/sub z/ and confidence intervals in simple as well as paired data analyses. The combined diagnostic feature had an A/sub z/ of 0.96 with a confidence interval of [0.93, 0.99] at a significance level of 0.05. The combined features show statistically significant improvement over prebiopsy radiological findings. These results indicate that ultrasound tissue characterization, in combination with patient record and clinical findings, may greatly reduce the need to perform biopsies of benign breast lesions.","Ultrasonic imaging,
Breast cancer,
Breast biopsy,
Lesions,
Data analysis,
Image analysis,
Costs,
Inspection,
Councils,
Radiology"
Watermarking 2D vector maps in the mesh-spectral domain,"The paper proposes a digital watermarking algorithm for 2D vector digital maps. The watermark is a robust, informed-detection watermark to be used to prevent such abuses as an intellectual property rights violation. The algorithm proposed in the paper embeds watermarks in the frequency-domain representation of a 2D vector digital map. Our method treats vertices in the map as a point set, and imposes connectivity among the points by using Delaunay triangulation. The method then computes the mesh-spectral coefficients (Karni, 2000) from the mesh created. Modifications of the coefficients according to the message bits, and inverse transforming the coefficients back into the coordinate domain produces the watermarked map. Our evaluation experiments showed that the watermark produced by the method is resistant against additive random noise, similarity transformation, vertex insertion and removal. It is also resistant, to some extent, against cropping. Compared to our previous algorithm (Ohbuchi, 2002), the algorithm described in this paper showed significantly improved attack resiliency.","Watermarking,
Roads,
Geographic Information Systems,
Robustness,
Additive noise,
Global Positioning System,
Companies,
Shape,
Computer science,
Intellectual property"
Qualitative image based localization in indoors environments,"Man made indoor environments possess regularities, which can be efficiently exploited in automated model acquisition by means of visual sensing. In this context we propose an approach for inferring a topological model of an environment from images or the video stream captured by a mobile robot during exploration. The proposed model consists of a set of locations and neighborhood relationships between them. Initially each location in the model is represented by a collection of similar, temporally adjacent views, with the similarity defined according to a simple appearance based distance measure. The sparser representation is obtained in a subsequent learning stage by means of learning vector quantization (LVQ). The quality of the model is tested in the context of qualitative localization scheme by means of location recognition: given a new view, the most likely location where that view came from is determined.","Indoor environments,
Mobile robots,
Context modeling,
Principal component analysis,
Robot sensing systems,
Topology,
Navigation,
Computer science,
Drives,
Streaming media"
Routability-driven white space allocation for fixed-die standard-cell placement,"The use of white space in fixed-die standard-cell placement is an effective way to improve routability. In this paper, we present a white space allocation approach that dynamically assigns white space according to the congestion distribution of the placement. In the top-down placement flow, white space is assigned to congested regions using smooth allocating functions. A post-allocation optimization step is taken to further improve placement quality. Experimental results show that the proposed allocation approach, combined with a multilevel placement flow, significantly improves placement routability and layout quality. A set of approaches for white space allocation has been presented and compared in this paper. All of them are based on routability-driven methods. However, these approaches vary in the allocation function and allocation aggressiveness. All the placement results are investigated by feeding them into a widely used industrial router (Warp Route of Cadence). Comparisons have been made between: 1) placement with or without white space allocation; 2) different white space allocation approaches; and 3) our placement flow, industrial placement tool, and the other state-of-the-art academic placement tool.","White spaces,
Routing,
Aerospace industry,
Simulated annealing,
Analytical models,
Terrorism,
Computer science,
Computational modeling,
Wires,
Data structures"
A contribution to convergence theory of fuzzy c-means and derivatives,"In this paper, we revisit the convergence and optimization properties of fuzzy clustering algorithms, in general, and the fuzzy c-means (FCM) algorithm, in particular. Our investigation includes probabilistic and (a slightly modified implementation of) possibilistic memberships, which will be discussed under a unified view. We give a convergence proof for the axis-parallel variant of the algorithm by Gustafson and Kessel, that can be generalized to other algorithms more easily than in the usual approach. Using reformulated fuzzy clustering algorithms, we apply Banach's classical contraction principle and establish a relationship between saddle points and attractive fixed points. For the special case of FCM we derive a sufficient condition for fixed points to be attractive, allowing identification of them as (local) minima of the objective function (excluding the possibility of a saddle point).","Convergence,
Clustering algorithms,
Prototypes,
Sufficient conditions,
Partitioning algorithms,
Phase change materials,
Computer science"
A stateful intrusion detection system for World-Wide Web servers,"Web servers are ubiquitous, remotely accessible, and often misconfigured. In addition, custom Web-based applications may introduce vulnerabilities that are overlooked even by the most security-conscious server administrators. Consequently, Web servers are a popular target for hackers. To mitigate the security exposure associated with Web servers, intrusion detection systems are deployed to analyze and screen incoming requests. The goal is to perform early detection of malicious activity and possibly prevent more serious damage to the protected site. Even though intrusion detection is critical for the security of Web servers, the intrusion detection systems available today only perform very simple analyses and are often vulnerable to simple evasion techniques. In addition, most systems do not provide sophisticated attack languages that allow a system administrator to specify custom, complex attack scenarios to be detected. We present WebSTAT, an intrusion detection system that analyzes Web requests looking for evidence of malicious behavior. The system is novel in several ways. First of all, it provides a sophisticated language to describe multistep attacks in terms of states and transitions. In addition, the modular nature of the system supports the integrated analysis of network traffic sent to the server host, operating system-level audit data produced by the server host, and the access logs produced by the Web server. By correlating different streams of events, it is possible to achieve more effective detection of Web-based attacks.","Intrusion detection,
Web server,
Network servers,
Event detection,
Performance analysis,
Application software,
Computer hacking,
Telecommunication traffic,
Computer science,
Protection"
Verification of Web services using an enhanced UDDI server,"The UDDI (Universal Description Discovery and Integration) provides classification to find the distributed Web services (WS) by keyword matching. The UDDI version 3 allows searching WS using digital signatures. However, it still needs systematic verification to ensure WS quality in a timely fashion. This paper proposes adding verification mechanism to the UDDI servers including check-in and checkout of WS. The key idea is that test scripts should be attached to WS, and both WS providers and clients use these test scripts. Before accepting a new WS into the service directory. The associated test scripts must test the new WS, and they will be accepted only if the test was successful. Before using a specific WS, a client can use the appropriate test scripts to test the WS and it will be used only if the test was successful. While the code for WS may not be available, but the associated test scripts test script specification techniques and distributed test execution techniques to perform testing with a UDDI server.","Web services,
Testing,
Digital signatures,
Computer architecture,
Service oriented architecture,
Middleware,
Web and internet services,
Simple object access protocol,
Runtime,
Computer science"
Resolving multiple occluded layers in augmented reality,"A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the ""Superman's X-ray vision"" problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm's reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints.","Augmented reality,
Visualization,
Laboratories,
Displays,
Navigation,
Context awareness,
Human factors,
Switches,
Virtual reality,
Computer science"
Comparing evolutionary algorithms on binary constraint satisfaction problems,"Constraint handling is not straightforward in evolutionary algorithms (EAs) since the usual search operators, mutation and recombination, are 'blind' to constraints. Nevertheless, the issue is highly relevant, for many challenging problems involve constraints. Over the last decade, numerous EAs for solving constraint satisfaction problems (CSP) have been introduced and studied on various problems. The diversity of approaches and the variety of problems used to study the resulting algorithms prevents a fair and accurate comparison of these algorithms. This paper aligns related work by presenting a concise overview and an extensive performance comparison of all these EAs on a systematically generated test suite of random binary CSPs. The random problem instance generator is based on a theoretical model that fixes deficiencies of models and respective generators that have been formerly used in the evolutionary computing field.","Evolutionary computation,
Constraint optimization,
Genetic mutations,
System testing,
Guidelines,
Mathematics,
Computer science,
Software algorithms,
Software libraries"
Multi-resolution real-time stereo on commodity graphics hardware,"In this paper a stereo algorithm suitable for implementation on commodity graphics hardware is presented. This is important since it allows freeing up the main processor for other tasks including high-level interpretation of the stereo results. Our algorithm relies on the traditional sum-of-square-differences (SSD) dissimilarity measure between correlation windows. To achieve good results close to depth discontinuities as well as on low texture areas, a multi-resolution approach is used. The approach efficiently combines SSD measurements for windows of different sizes. Our implementation running on an NVIDIA GeForce4 graphics card achieves 50-70M disparity evaluations per second including all the overhead to download images and read-back the disparity map, which is equivalent to the fastest commercial CPU implementations available. An important advantage of our approach is that rectification is not necessary so that correspondences can just as easily be obtained for images that contain the epipoles. Another advantage is that this approach can easily be extended to multi-baseline stereo.","Hardware,
Stereo vision,
Computer vision,
Computer graphics,
Size measurement,
Application software,
Multiresolution analysis,
Computer science,
Casting,
Optimization methods"
Visual correspondence using energy minimization and mutual information,"We address visual correspondence problems without assuming that scene points have similar intensities in different views. This situation is common, usually due to nonLambertian scenes or to differences between cameras. We use maximization of mutual information, a powerful technique for registering images that requires no a priori model of the relationship between scene intensities in different views. However, it has proven difficult to use mutual information to compute dense visual correspondence. Comparing fixed-size windows via mutual information suffers from the well-known problems of fixed windows, namely poor performance at discontinuities and in low-texture regions. In this paper, we show how to compute visual correspondence using mutual information without suffering from these problems. Using a simple approximation, mutual information can be incorporated into the standard energy minimization framework used in early vision. The energy can then be efficiently minimized using graph cuts, which preserve discontinuities and handle low-texture regions. The resulting algorithm combines the accurate disparity maps that come from graph cuts with the tolerance for intensity changes that comes from mutual information.","Mutual information,
Layout,
Cameras,
Pixel,
Stereo vision,
Reflectivity,
Brightness,
Computer science,
Computer vision,
Minimization methods"
The Lutonium: a sub-nanojoule asynchronous 8051 microcontroller,"We describe the Lutonium, an asynchronous 8051 microcontroller designed for low Et/sup 2/. In 0.18 /spl mu/m CMOS, at nominal 1.8 V, we expect a performance of 0.5 nJ per instruction at 200 MIPS. At 0.5 V, we expect 4 MIPS and 40 pJ/instruction, corresponding to 25,000 MIPS/Watt. We describe the structure of a fine-grain pipeline optimized for Et/sup 2/ efficiency, some of the peripherals implementation, and the advantages of an asynchronous implementation of a deep-sleep mechanism.","Microcontrollers,
Energy efficiency,
Delay,
Pipelines,
Energy consumption,
Microprocessors,
Threshold voltage,
Circuits,
Decoding,
Computer science"
Region-based wavelet coding methods for digital mammography,"Spatial resolution and contrast sensitivity requirements for some types of medical image techniques, including mammography, delay the implementation of new digital technologies, namely, computer-aided diagnosis, picture archiving and communications systems, or teleradiology. In order to reduce transmission time and storage cost, an efficient data-compression scheme to reduce digital data without significant degradation of medical image quality is needed. In this study, we have applied two region-based compression methods to digital mammograms. In both methods, after segmenting the breast region, a region-based discrete wavelet transform is applied, followed by an object-based extension of the set partitioning in hierarchical trees (OB-SPIHT) coding algorithm in one method, and an object-based extension of the set partitioned embedded block (OB-SPECK) coding algorithm in the other. We have compared these specific implementations against the original SPIHT and the new standard JPEG 2000, both using reversible and irreversible filters, on five digital mammograms compressed at rates ranging from 0.1 to 1.0 bit per pixel (bbp). Distortion was evaluated for all images and compression rates by the peak signal-to-noise ratio. For all images, OB-SPIHT and OB-SPECK performed substantially better than the traditional SPIHT and JPEG 2000, and a slight difference in performance was found between them. A comparison applying SPIHT and the standard JPEG 2000 to the same set of images with the background pixels fixed to zero was also carried out, obtaining similar implementation as region-based methods. For digital mammography, region-based compression methods represent an improvement in compression efficiency from full-image methods, also providing the possibility of encoding multiple regions of interest independently.","Mammography,
Image coding,
Transform coding,
Biomedical imaging,
Medical diagnostic imaging,
Image storage,
Discrete wavelet transforms,
Partitioning algorithms,
Spatial resolution,
Delay"
Small-scale compensation for WLAN location determination systems,"To limit the radio map size and the time required to build the radio map, current WLAN location determination system do not handle small-scale variations. This contributes to most of the estimation error in the current systems. We propose a general technique, the perturbation technique, to handle the small-scale variations problem. The system uses user history to detect small-scale variations and then perturbs the signal strength vector entries to overcome it. The results obtained show that the accuracy can be increased by more than 8%. Moreover, the worst-case error is enhanced by more than 60%. We also show that the perturbation technique can help in enhancing the accuracy due to temporal variations in case of change in the environment conditions, thus increasing the accuracy of the current WLAN location determination systems beyond their limits.","Wireless LAN,
Working environment noise,
Signal processing,
Computer science,
Educational institutions,
Estimation error,
Perturbation methods,
Radio frequency,
History,
Context-aware services"
Generalized processor sharing with light-tailed and heavy-tailed input,"We consider a queue fed by a mixture of light-tailed and heavy-tailed traffic. The two traffic flows are served in accordance with the generalized processor sharing (GPS) discipline. GPS-based scheduling algorithms, such as weighted fair queueing, have emerged as an important mechanism for achieving service differentiation in integrated networks. We derive the asymptotic workload behavior of the light-tailed traffic flow under the assumption that its GPS weight is larger than its traffic intensity. The GPS mechanism ensures that the workload is bounded above by that in an isolated system with the light-tailed flow served in isolation at a constant rate equal to its GPS weight. We show that the workload distribution is in fact asymptotically equivalent to that in the isolated system, multiplied with a certain pre-factor, which accounts for the interaction with the heavy-tailed flow. Specifically, the pre-factor represents the probability that the heavy-tailed flow is backlogged long enough for the light-tailed flow to reach overflow. The results provide crucial qualitative insight in the typical overflow scenario.","Traffic control,
Telecommunication traffic,
Global Positioning System,
Mathematics,
Computer science,
Processor scheduling,
Scheduling algorithm,
Delay,
Video sharing,
Streaming media"
Leader election algorithms for wireless ad hoc networks,"We consider the problem of secure leader election and propose two cheat-proof election algorithms: Secure Extrema Finding Algorithm (SEFA) and Secure Preference-based Leader Election Algorithm (SPLEA). Both algorithms assume a synchronous distributed system in which the various rounds of election proceed in a lock-step fashion. SEFA assumes that all elector-nodes share a single common evaluation function that returns the same value at any elector-node when applied to a given candidate-node. When elector-nodes can have different preferences for a candidate-node, the scenario becomes more complicated. Our Secure Preference-based Leader Election Algorithm (SPLEA) deals with this case. Here, individual utility functions at each elector-node determine an elector-node's preference for a given candidate-node. We relax the assumption of a synchronous distributed system in our Asynchronous Extrema Finding Algorithm (AEFA) and also allow the topology to change during the election process. In AEFA, nodes can start the process of election at different times, but eventually after topological changes stop long enough for the algorithm to terminate, all nodes agree on a unique leader. Our algorithm has been proven to be ""weakly"" self-stabilizing.","Nominations and elections,
Ad hoc networks,
Communication system security,
Mobile ad hoc networks,
Wireless sensor networks,
Computer science,
Reactive power,
Cost accounting,
Topology,
Wireless application protocol"
A metrics suite for measuring reusability of software components,"In component-based software development, it is necessary to measure the reusability of components in order to realize the reuse of components effectively. There are some product metrics for measuring the reusability of object-oriented software. However, in application development with reuse, it is difficult to use conventional metrics because the source codes of components cannot be obtained, and these metrics require analysis of source codes. We propose a metrics suite for measuring the reusability of such black-box components based on limited information that can be obtained from the outside of components without any source codes. We define five metrics for measuring a component's understandability, adaptability, and portability, with confidence intervals that were set by statistical analysis of a number of JavaBeans components. Moreover, we provide a reusability metric by combining these metrics based on a reusability model. As a result of evaluation experiments, it is found that our metrics can effectively identify black-box components with high reusability.","Software measurement,
Software reusability,
Programming,
Software metrics,
Logic,
Object oriented modeling,
Software systems,
Computer science,
Computer industry,
Cities and towns"
A node-centric load balancing algorithm for wireless sensor networks,"By spreading the workload across a sensor network, load balancing reduces hot spots in the sensor network and increases the energy lifetime of the sensor network. In this paper, we design a node-centric algorithm that constructs a load-balanced tree in sensor networks of asymmetric architecture. We utilize a Chebyshev Sum metric to evaluate via simulation the balance of the routing trees produced by our algorithm. We find that our algorithm achieves routing trees that are more effectively balanced than the routing based on breadth-first search (BFS) and shortest-path obtained by Dijkstra's algorithm.","Load management,
Wireless sensor networks,
Routing,
Base stations,
Biomedical monitoring,
Computer science,
Algorithm design and analysis,
Chebyshev approximation,
Collaborative work,
Energy consumption"
Test-driven development as a defect-reduction practice,"Test-driven development is a software development practice that has been used sporadically for decades. With this practice, test cases (preferably automated) are incrementally written before production code is implemented. Test-driven development has recently re-emerged as a critical enabling practice of the extreme programming software development methodology. We ran a case study of this practice at IBM. In the process, a thorough suite of automated test cases was produced after UML design. In this case study, we found that the code developed using a test-driven development practice showed, during functional verification and regression tests, approximately 40% fewer defects than a baseline prior product developed in a more traditional fashion. The productivity of the team was not impacted by the additional focus on producing automated test cases. This test suite aids in future enhancements and maintenance of this code. The case study and the results are discussed in detail.","Programming,
Automatic testing,
Software testing,
Production,
Computer science,
Radio access networks,
Unified modeling language,
Productivity,
NASA,
Writing"
Mining strong affinity association patterns in data sets with skewed support distribution,"Existing association-rule mining algorithms often rely on the support-based pruning strategy to prune its combinatorial search space. This strategy is not quite effective for data sets with skewed support distributions because they tend to generate many spurious patterns involving items from different support levels or miss potentially interesting low-support patterns. To overcome these problems, we propose the concept of hyperclique pattern, which uses an objective measure called h-confidence to identify strong affinity patterns. We also introduce the novel concept of cross-support property for eliminating patterns involving items with substantially different support levels. Our experimental results demonstrate the effectiveness of this method for finding patterns in dense data sets even at very low support thresholds, where most of the existing algorithms would break down. Finally, hyperclique patterns also show great promise for clustering items in high dimensional space.","Dairy products,
Computer science,
Cities and towns,
Data mining,
Clustering algorithms,
Frequency,
Pairwise error probability,
Association rules,
Degradation"
Utilizing solar power in wireless sensor networks,"Sensor networks are designed especially for deployment in adverse and non-accessible areas without a fixed infrastructure. Therefore, energy conservation plays a crucial role for these networks. We propose to utilize solar power in wireless sensor networks, establishing a topology where, changing over time, some nodes can receive and transmit packets without consuming the limited battery resources. We propose and evaluate two protocols that perform solar-aware routing. The presented simulation results show that both protocols provide significant energy savings when utilizing solar power. The paper shows that incorporating the solar status of nodes in the routing decision is feasible and results in reduced overall battery consumption.","Solar energy,
Intelligent networks,
Wireless sensor networks,
Routing protocols,
Batteries,
Energy efficiency,
Hardware,
Computer science,
Energy conservation,
Network topology"
"Degeneracies, dependencies and their implications in multi-body and multi-sequence factorizations",The body of work on multi-body factorization separates between objects whose motions are independent. In this work we show that in many cases objects moving with different 3D motions will be captured as a single object using these approaches. We analyze what causes these degeneracies between objects and suggest an approach for overcoming some of them. We further show that in the case of multiple sequences linear dependencies can supply information for temporal synchronization of sequences and for spatial matching of points across sequences.,"Layout,
Character generation,
Cameras,
Shape,
Bismuth,
Computer science,
Subspace constraints,
Video sequences,
Stacking"
Mixtures of general linear models for functional neuroimaging,"We set out a new general framework for making inferences from neuroimaging data, which includes a standard approach to neuroimaging analysis, statistical parametric mapping (SPM), as a special case. The model offers numerous conceptual and statistical advantages that derive from analyzing data at the ""cluster level"" rather than the ""voxel level"" and from explicit modeling of the shape and position of clusters of activation. This provides a natural and principled way to pool data from nearby voxels for parameter and variance-component estimation. The model can also be viewed as performing a spatio-temporal cluster analysis. The parameters of the model are estimated using an expectation maximization (EM) algorithm.","Neuroimaging,
Data analysis,
Scanning probe microscopy,
Active shape model,
Parameter estimation,
Clustering algorithms,
Hemodynamics,
Neuroscience,
Performance analysis,
Magnetic resonance imaging"
Rate-monotonic scheduling on uniform multiprocessors,"The rate-monotonic algorithm is arguably one of the most popular algorithms for scheduling systems of periodic real-time tasks. The rate-monotonic scheduling of systems of periodic tasks on uniform multiprocessor platforms is considered here. A simple, sufficient test is presented for determining whether a given periodic task system will be successfully scheduled by this algorithm upon a particular uniform multiprocessor platform-this test generalizes earlier results concerning rate-monotonic scheduling upon identical multiprocessor platforms.","Processor scheduling,
Multiprocessing,
Real time systems"
"Toward a scalable, silicon-based quantum computing architecture","Advances in quantum devices have brought scalable quantum computation closer to reality. We focus on the system-level issues of how quantum devices can be brought together to form a scalable architecture. In particular, we examine promising silicon-based proposals. We discover that communication of quantum data is a critical resource in such proposals. We find that traditional techniques using quantum SWAP gates are exponentially expensive as distances increase and propose quantum teleportation as a means to communicate data over longer distances on a chip. Furthermore, we find that realistic quantum error-correction circuits use a recursive structure that benefits from using teleportation for long-distance communication. We identify a set of important architectural building blocks necessary for constructing scalable communication and computation. Finally, we explore an actual layout scheme for recursive error correction, and demonstrate the exponential growth in communication costs with levels of recursion, and that teleportation limits those costs.","Quantum computing,
Computer architecture,
Proposals,
Teleportation,
Costs,
Silicon,
Engineering profession,
Computer science,
Solid state circuits,
Error correction"
Radius Margin Bounds for Support Vector Machines with the RBF Kernel,"An important approach for efficient support vector machine (SVM) model selection is to use differentiable bounds of the leave-one-out (loo) error. Past efforts focused on finding tight bounds of loo (e.g., radius margin bounds, span bounds). However, their practical viability is still not very satisfactory. Duan, Keerthi, and Poo (2003) showed that radius margin bound gives good prediction for L2-SVM, one of the cases we look at. In this letter, through analyses about why this bound performs well for L2-SVM, we show that finding a bound whose minima are in a region with small loo values may be more important than its tightness. Based on this principle, we propose modified radius margin bounds for L1-SVM (the other case) where the original bound is applicable only to the hard-margin case. Our modification for L1-SVM achieves comparable performance to L2-SVM. To study whether L1-or L2-SVM should be used, we analyze other properties, such as their differentiability, number of support vectors, and number of free support vectors. In this aspect, L1-SVM possesses the advantage of having fewer support vectors. Their implementations are also different, so we discuss related issues in detail.",
Joint semantics and feature based image retrieval using relevance feedback,"Relevance feedback is a powerful technique for image retrieval and has been an active research direction for the past few years. Various ad hoc parameter estimation techniques have been proposed for relevance feedback. In addition, methods that perform optimization on multilevel image content model have been formulated. However, these methods only perform relevance feedback on low-level image features and fail to address the images' semantic content. In this paper, we propose a relevance feedback framework to take advantage of the semantic contents of images in addition to low-level features. By forming a semantic network on top of the keyword association on the images, we are able to accurately deduce and utilize the images' semantic contents for retrieval purposes. We also propose a ranking measure that is suitable for our framework. The accuracy and effectiveness of our method is demonstrated with experimental results on real-world image collections.","Image retrieval,
Feedback,
Information retrieval,
Content based retrieval,
Computer science,
Digital images,
Image databases,
Spatial databases,
Parameter estimation,
Optimization methods"
Guide-wire tracking during endovascular interventions,"A method is presented to extract and track the position of a guide wire during endovascular interventions under X-ray fluoroscopy. The method can be used to improve guide-wire visualization in low-quality fluoroscopic images and to estimate the position of the guide wire in world coordinates. A two-step procedure is utilized to track the guide wire in subsequent frames. First, a rough estimate of the displacement is obtained using a template-matching procedure. Subsequently, the position of the guide wire is determined by fitting a spline to a feature image. The feature images that have been considered enhance line-like structures on: 1) the original images; 2) subtraction images; and 3) preprocessed images in which coherent structures are enhanced. In the optimization step, the influence of the scale at which the feature is calculated and the additional value of using directional information is investigated. The method is evaluated on 267 frames from ten clinical image sequences. Using the automatic method, the guide wire could be tracked in 96% of the frames, with a similar accuracy to three observers, although the position of the tip was estimated less accurately.","Wire,
Biomedical imaging,
Visualization,
Spline,
Radiology,
Blood vessels,
Image quality,
Data mining,
Catheters,
X-ray imaging"
A comparison of methods for mammogram registration,"Mammogram registration is an important technique to optimize the display of cases on a digital viewing station, and to find corresponding regions in temporal pairs of mammograms for computer-aided diagnosis algorithms. Four methods for mammogram registration were tested and results were compared. The performance of all registration methods was measured by comparing the distance between annotations of abnormalities in the previous and current view before and after registration. Registration by mutual information outperformed alignment based on nipple location, alignment based on center of mass of breast tissue, and warping.","Breast cancer,
Breast tissue,
Cancer detection,
Mammography,
Computer displays,
Computer aided diagnosis,
Mutual information,
X-ray imaging,
Muscles,
Biomedical imaging"
Using IEEE 802.11e MAC for QoS over wireless,"We study the behavior of the new MAC protocols for QoS in the proposed IEEE 802.11e draft standard and analyze them for their ability to fulfill their goals of better QoS and higher channel efficiency. We study the response of these mechanisms to various choices in available protocol parameters. We show that HCF reduces channel contention and allows better channel utilization. However, both the proposed MAC coordination functions, EDCF and HCF, are highly sensitive to protocol parameters. We believe that the effectiveness of these functions also depends on the scheduling algorithms. The effects of the various policy choices need to be understood and validated before the draft becomes a standard.","Media Access Protocol,
Ethernet networks,
Counting circuits,
Multiaccess communication,
Computer science,
Scheduling algorithm,
Explosions,
Wireless LAN,
Physical layer,
Bandwidth"
TCP performance re-visited,"Detailed measurements and analyses for the Linux-2.4 TCP stack on current adapters and processors are presented. We describe the impact of CPU scaling and memory bus loading on TCP performance. As CPU speeds outstrip I/O and memory speeds, many generally accepted notions of TCP performance begin to unravel. In-depth examinations and explanations of previously held TCP performance truths are provided, and we expose cases where these assumptions and rules of thumb no longer hold in modern-day implementations. We conclude that unless major architectural changes are adopted, we would be hard-pressed to continue relying on the 1 GHz/1 Gbps rule of thumb.","Bandwidth,
Protocols,
Current measurement,
Thumb,
Performance analysis,
Costs,
TCPIP,
Computer science,
Engines,
Network servers"
Distance-preserving mappings from binary vectors to permutations,"Mappings of the set of binary vectors of a fixed length to the set of permutations of the same length are useful for the construction of permutation codes. In this article, several explicit constructions of such mappings preserving or increasing the Hamming distance are given. Some applications are given to illustrate the usefulness of the construction. In particular, a new lower bound on the maximal size of permutation arrays (PAs) is given.","Hamming distance,
Convolutional codes,
Modulation coding,
Councils,
Information management,
Computer science,
Informatics"
Voronoi tracking: location estimation using sparse and noisy sensor data,"Tracking the activity of people in indoor environments has gained considerable attention in the robotics community over the last years. Most of the existing approaches are based on sensors, which allow to accurately determining the locations of people but do not provide means to distinguish between different persons. In this paper we propose a novel approach to tracking moving objects and their identity using noisy, sparse information collected by id-sensors such as infrared and ultrasound badge systems. The key idea of our approach is to use particle filters to estimate the locations of people on the Voronoi graph of the environment. By restricting particles to a graph, we make use of the inherent structure of indoor environments. The approach has two key advantages. First, it is by far more efficient and robust than unconstrained particle filters. Second, the Voronoi graph provides a natural discretization of human motion, which allows us to apply unsupervised learning techniques to derive typical motion patterns of the people in the environment. Experiments using a robot to collect ground-truth data indicate the superior performance of Voronoi tracking. Furthermore, we demonstrate that EM-based learning of behavior patterns increases the tracking performance and provides valuable information for high-level behavior recognition.","Indoor environments,
Particle filters,
Humans,
Working environment noise,
State-space methods,
Computer science,
Robot sensing systems,
Ultrasonic imaging,
Robustness,
Pattern recognition"
A new algorithm for border description of polarized light surface microscopic images of pigmented skin lesions,"The aim of the study was to provide mathematical descriptors for the border of pigmented skin lesion images and to assess their efficacy for distinction among different lesion groups. New descriptors such as lesion slope and lesion slope regularity are introduced and mathematically defined. A new algorithm based on the Catmull-Rom spline method and the computation of the gray-level gradient of points extracted by interpolation of normal direction on spline points was employed. The efficacy of these new descriptors was tested on a data set of 510 pigmented skin lesions, composed by 85 melanomas and 425 nevi, by employing statistical methods for discrimination between the two populations.","Optical polarization,
Microscopy,
Pigmentation,
Skin,
Lesions,
Spline,
Data mining,
Interpolation,
Testing,
Malignant tumors"
An efficient algorithm for finding the k longest testable paths through each gate in a combinational circuit,,
Real time data association for FastSLAM,"The ability to simultaneously localise a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. This paper presents a real-world implementation of FastSLAM, an algorithm that recursively estimates the full posterior distribution of both robot pose and landmark locations. In particular, we present an extension to FastSLAM that addresses the data association problem using a nearest neighbor technique. Building on this, we also present a novel multiple hypotheses tracking implementation (MHT) to handle uncertainty in the data association. Finally an extension to the multi-robot case is introduced. Our algorithm has been run successfully using a number of data sets obtained in outdoor environments. Experimental results are presented that demonstrate the performance of the algorithms when compared with standard Kalman filter-based approaches.","Simultaneous localization and mapping,
Robot sensing systems,
Bayesian methods,
Uncertainty,
Partitioning algorithms,
Particle filters,
Australia,
Computer science,
Recursive estimation,
Kalman filters"
PCA-based face recognition in infrared imagery: baseline and comparative studies,"Techniques for face recognition generally fall into global and local approaches, with the principal component analysis (PCA) being the most prominent global approach. We use the PCA algorithm to study the comparison and combination of infrared and typical visible-light images for face recognition. We examine the effects of lighting change, facial expression change and passage of time between the gallery image and probe image. Experimental results indicate that when there is substantial passage of time (greater than one week) between the gallery and probe images, recognition from typical visible-light images may outperform that from infrared images. Experimental results also indicate that the combination of the two generally outperforms either one alone. This is the only study that we know of to focus on the issue of how passage of time affects infrared face recognition.","Face recognition,
Infrared imaging,
Probes,
Principal component analysis,
Image recognition,
Image databases,
Control systems,
Testing,
Computer science,
Robustness"
Free riding: a new challenge to peer-to-peer file sharing systems,"Most of the research in the field of peer-to-peer file sharing systems has concentrated on performance issues such as efficient file lookup, replicating files to improve file download speeds etc. However there is a new challenge that questions the very existence and usefulness of such systems in the form of ""free riding"". This paper studies the seriousness of the negative impact that free riding can have in a P2P file sharing system. We introduce the concept of utility function to measure the usefulness of peers to the system as a whole, and describe a scheme based on this concept to control free riding. A simple utility function is described to illustrate the scheme. We design and develop a simulation model to study various patterns of sharing behaviors among the peers in a file sharing community and their impact on the system. A set of experimental results is reported. The experiments indicate that the utility based free riding control can increase the lifetime of the system by 10 times, even with a simple utility function.","Peer to peer computing,
Educational institutions,
Control systems,
Quality of service,
Internet"
A multi-commodity flow based approach to virtual network resource allocation,"The virtual network (VN) concept has been studied as a useful mean in supporting rapid service creation and deployment. This paper proposes a scheme for allocating resources to VNs with the objective of maximizing the number of VNs that can be accommodated into a network. In our scheme, resources are pre-allocated for each pair of edge nodes, using the solution to the multi-commodity flow problem. A VN creation request consists of a set of edge node pairs and the bandwidth requirements between each pair. A request is satisfied or accepted by utilizing the pre-allocated resource and possibly the residual resource pool after pre-allocation. Extensive simulation studies show that the proposed scheme accepts more VN requests and yields better network resource utilization over traditional approaches. Service providers may potentially boost revenue by a simple switch to a more intelligent resource allocation scheme.","Resource management,
Switches,
Communication system control,
Computer science,
Bandwidth,
Videoconference,
Resource virtualization,
Performance analysis,
Context,
Virtual private networks"
Identifying high performance ERP projects,"Learning from high performance projects is crucial for software process improvement. Therefore, we need to identify outstanding projects that may serve as role models. It is common to measure productivity as an indicator of performance. It is vital that productivity measurements deal correctly with variable returns to scale and multivariate data. Software projects generally exhibit variable returns to scale, and the output from ERP projects is multivariate. We propose to use data envelopment analysis variable returns to scale (DEA VRS) to measure the productivity of software projects. DEA VRS fulfills the two requirements stated above. The results from this empirical study of 30 ERP projects extracted from a benchmarking database in Accenture identified six projects as potential role models. These projects deserve to be studied and probably copied as part of a software process improvement initiative. The results also suggest that there is a 50 percent potential for productivity improvement, on average. Finally, the results support the assumption of variable returns to scale in ERP projects. We recommend DEA VRS be used as the default technique for appropriate productivity comparisons of individual software projects. Used together with methods for hypothesis testing, DEA VRS is also a useful technique for assessing the effect of alleged process improvements.","Enterprise resource planning,
Productivity,
Software performance,
Data envelopment analysis,
Software measurement,
Best practices,
Software engineering,
Project management,
Performance evaluation,
Data mining"
Kinematic and deformation analysis of 4-D coronary arterial trees reconstructed from cine angiograms,"In the cardiovascular arena, percutaneous catheter-based interventional (i.e., therapeutic) procedures include a variety of coronary and other vascular system interventions. These procedures use two-dimensional (2-D) X-ray-based imaging as the sole or the major imaging modality for procedure guidance and quantification of key parameters. Coronary vascular curvilinearity is one key parameter that requires a four-dimensional (4-D) format, i.e., three-dimensional (3-D) anatomical representation that changes during the cardiac cycle. A new method has been developed for reconstruction and analysis of these patient-specific 4-D datasets utilizing routine cine angiograms. The proposed method consists of three major processes: 1) reconstruction of moving coronary arterial tree throughout the cardiac cycle; 2) establishment of temporal correspondence with smoothness constraints; and 3) kinematic and deformation analysis of the reconstructed 3-D moving coronary arterial trees throughout the cardiac cycle.","Kinematics,
Image reconstruction,
Optical imaging,
Cardiology,
Arteries,
X-ray imaging,
Three dimensional displays,
Two dimensional displays,
Geometry,
Biomedical imaging"
On the topology of multicast trees,"The benefit derived from using multicast is seemingly dependent upon the shape of the distribution tree. We attempt to model interdomain multicast trees accurately. We measure a number of key parameters, such as depth, degree frequency, and average degree, for a number of real and synthetic data sets. We find that interdomain multicast trees actually do share a common shape at both the router and autonomous system levels. Furthermore, we develop a characterization of multicast efficiency which reveals that group sizes as small as 20 to 40 receivers offer a 55%-70% reduction in the total number of links traversed when compared to separately delivered unicast streams. A final contribution of our work consists in a number of data sets, compiled from multicast group membership and path data, that can be used to generate large sample trees, representative of the current multicast infrastructure.","Shape,
Unicast,
Bandwidth,
Costs,
Network topology,
Multicast protocols,
Frequency measurement,
Assembly,
Computer science,
IP networks"
Fluorescence-enhanced optical tomography using referenced measurements of heterogeneous media,"A three-dimensional image reconstruction for fluorescence-enhanced frequency-domain photon migration (FDPM) measurements in turbid media is developed and investigated for three different simulated measurement types: 1) absolute emission measurement, or emission measurements of phase and amplitude attenuation made for a given incident point source of excitation light; 2) referenced emission measurements made relative to an excitation measurement conducted at a single reference point away from the incident source; and 3) referenced emission measurements made relative to the excitation measurement conducted at identical points of detection. The image reconstruction algorithm employs a gradient-based constrained truncated Newton (CONTN) method which implements a bounding parameter, which can be used to govern the level of contrast used to discriminate tissue volumes from heterogeneous background tissues. Reverse differentiation technique is used to calculate the gradients. Using simulated data with superimposed noise to achieve a signal-to-noise ratio of 55 and 35 dB to mimic experimental excitation and emission FDPM measurements, respectively, we show the robustness of emission measurements referenced to excitation light. We investigate the performance of algorithm CONTN using these measurement techniques and show that the absorption coefficients due to fluorophore are reconstructed by CONTN accurately and efficiently. Furthermore, we demonstrate the performance of the bounding parameter for rejection of background artifacts owing to background tissue heterogeneity.","Fluorescence,
Tomography,
Nonhomogeneous media,
Attenuation measurement,
Frequency measurement,
Phase measurement,
Image reconstruction,
Optical attenuators,
Signal to noise ratio,
Noise measurement"
Power delivery and locomotion of untethered microactuators,"The ability for a device to locomote freely on a surface requires the ability to deliver power in a way that does not restrain the device's motion. This paper presents a MEMS actuator that operates free of any physically restraining tethers. We show how a capacitive coupling can be used to deliver power to untethered MEMS devices, independently of the position and orientation of those devices. Then, we provide a simple mechanical release process for detaching these MEMS devices from the fabrication substrate once chemical processing is complete. To produce these untethered microactuators in a batch-compatible manner while leveraging existing MEMS infrastructure, we have devised a novel postprocessing sequence for a standard MEMS multiproject wafer process. Through the use of this sequence, we show how to add, post hoc , a layer of dielectric between two previously deposited polysilicon films. We have demonstrated the effectiveness of these techniques through the successful fabrication and operation of untethered scratch drive actuators. Locomotion of these actuators is controlled by frequency modulation, and the devices achieve maximum speeds of over 1.5 mm/s.","Microactuators,
Actuators,
Micromechanical devices,
Self-assembly,
Microelectromechanical devices,
Fabrication,
Computer science,
Assembly systems,
Robotic assembly,
Robots"
A low-latency and energy-efficient algorithm for convergecast in wireless sensor networks,"In wireless sensor networks (WSN) the process of dissemination of data among various sensors (broadcast) and collection of data from all sensors (convergecast or data aggregation) are common communication operations. With increasing demands on efficient use of battery power, many efficient broadcast tree construction and channel allocation algorithms have been proposed. Generally convergecast is preceded by broadcast. Hence the tree used for broadcast is also used for convergecast. Our research shows that this approach is inefficient in terms of latency and energy consumption. In this paper we propose a heuristic solution for the problem of minimum energy convergecast which also works toward minimizing data latency. This algorithm constructs a tree using a greedy approach where new nodes are added to the tree such that weight on the branch to which it is added is less. The algorithm then allocates direct sequence spread spectrum or frequency hopping spread spectrum codes. Simulation results show that energy consumed and communication latency of our approach is lower than some of the existing approaches for convergecast. We have then used our algorithm to perform broadcast. Surprisingly our results show that this algorithms performance for broadcasting is better compared to other broadcast techniques.","Energy efficiency,
Intelligent networks,
Wireless sensor networks,
Broadcasting,
Delay,
Batteries,
Energy consumption,
Spread spectrum communication,
Base stations,
Computer science"
Power law distributions in class relationships,"Power law distributions have been found in many natural and social phenomena, and more recently in the source code and run-time characteristics of Object-Oriented (OO) systems. A power law implies that small values are extremely common, whereas large values are extremely rare. We identify twelve new power laws relating to the static graph structures of Java programs. The graph structures analyzed represented different forms of OO coupling, namely, inheritance, aggregation, interface, parameter type and return type. Identification of these new laws provides the basis for predicting likely features of classes in future developments. The research ties together work in object-based coupling and World Wide Web structures.","Java,
Web sites,
Topology,
Earthquakes,
Frequency,
Computer science,
Information systems,
Educational institutions,
Runtime,
Cities and towns"
Jini meets UPnP: an architecture for Jini/UPnP interoperability,"A service discovery framework provides a collection of protocols for developing dynamic client/server applications, allowing clients to find and use services without any previous knowledge of the locations or characteristics of the services. There are currently many service discovery, technologies available or in development, including Jini, UPnP, SLP, Salutation, Bluetooth SDP, and Ninja. These have similar high-level goals, but quite different architectures. Each software or hardware product utilizing service discovery will typically use only one of these protocols, meaning that clients and services using different technologies will not be able to cooperate. Since it is likely that several protocols will be widely used, there is a need for interoperability frameworks that allow clients and services written using different service discovery technologies to cooperate. This paper presents a Jini/UPnP interoperability framework that allows Jini clients to use UPnP services and UPnP clients to use Jini services, without modification to service or client implementations. As service specifications are typically developed independently for each protocol, a fully automatic interoperability solution is not currently practical, so we introduce service-specific proxies to bridge Jini and UPnP. Our goal is to reduce the amount of effort required to support new service types and our framework includes a substantial amount of support for rapid proxy development. A modest development effort is required to support each new service type, and our initial (and highly unscientific) measurements reveal that the level of effort is typically on the order of one day by a member of our team.",
Saudi Arabian license plate recognition system,"A license plate recognition (LPR) system is one kind of intelligent transport systems and is of considerable interest because of its potential applications to areas such as highway electronic toll collection, traffic monitoring system and so on. This paper proposes an automatic license plate recognition system for Saudi Arabian license plates. The system captures the images of the vehicles with a digital camera. An algorithm for the extraction of license plate has been designed and an algorithm for segmentation of characters is proposed. The performance of the system has been investigated on real images of about 610 vehicles captured under various illumination conditions. Recognition of about 95% shows that the system is quite effective.","Licenses,
Image segmentation,
Vehicles,
Application software,
Character recognition,
Image edge detection,
Digital cameras,
Image recognition,
Charge-coupled image sensors,
Computer science"
Chromosome image enhancement using multiscale differential operators,"Chromosome banding patterns are very important features for karyotyping, based on which cytogenetic diagnosis procedures are conducted. Due to cell culture, staining, and imaging conditions, image enhancement is a desirable preprocessing step before performing chromosome classification. In this paper, we apply a family of differential wavelet transforms (Wang and Lee, 1998), (Wang, 1999) for this purpose. The proposed differential filters facilitate the extraction of multiscale geometric features of chromosome images. Moreover, desirable fast computation can be realized. We study the behavior of both banding edge pattern and noise in the wavelet transform domain. Based on the fact that image geometrical features like edges are correlated across different scales in the wavelet representation, a multiscale point-wise product (MPP) is used to characterize the correlation of the image features in the scale-space. A novel algorithm is proposed for the enhancement of banding patterns in a chromosome image. In order to compare objectively the performance of the proposed algorithm against several existing image-enhancement techniques, a quantitative criteria, the contrast improvement ratio (CIR), has been adopted to evaluate the enhancement results. The experimental results indicate that the proposed method consistently outperforms existing techniques in terms of the CIR measure, as well as in visual effect. The effect of enhancement on cytogenetic diagnosis is further investigated by classification tests conducted prior to and following the chromosome image enhancement. In comparison with conventional techniques, the proposed method leads to better classification results, thereby benefiting the subsequent cytogenetic diagnosis.","Biological cells,
Image enhancement,
Cities and towns,
Wavelet transforms,
Filters,
Digital images,
Cells (biology),
Wavelet domain,
Visual effects,
Testing"
Performance utility-analysis of multi-state systems,"This paper defines a new utility importance of a state of a component in multi-state systems. This utility importance overcomes some drawbacks of a well-known importance measure suggested by William S. Griffith (J. Applied Probability, 1980). The relationship between this new utility importance and the Griffith importance is studied and their difference is illustrated with examples. The contribution of an individual component to the performance utility of a multi-state system is discussed. Examples show that a meaningful index for measuring the performance of individual components in a multi-state system can hardly be defined in general, without considering the actual values of the utility levels and the distributions of the component-states in the system. An example illustrates how genetic algorithm, simulated annealing, and tabu search can be used in selecting components and defining the position order of components so that the performance utility of a multi-state system is optimized.","Reliability,
Genetic algorithms,
Modeling,
Simulated annealing,
Power generation,
Random variables,
Computer science,
Manufacturing industries,
Manufacturing systems,
Systems engineering and theory"
The qualitative impact of using LEGO MINDSTORMS robots to teach computer engineering,Teaching introductory programming skills and embedded systems design to undergraduate engineering students in a variety of disciplines requires delivering the teaching content according to their learning styles. This paper describes how the LEGO MINDSTORMS robots were introduced as hands-on educational technology for computer engineering and provides student feedback measurements on the effectiveness of using the LEGO MINDSTORMS robots.,"Robots,
Computer science education,
Control engineering education"
Masquerade detection using enriched command lines,,"History,
Laboratories,
Computer science,
Testing,
Costs,
Error correction,
Pathology,
Detection algorithms,
Keyboards,
Computer security"
New families of binary sequences with low correlation,"For a positive integer, n, new families, S and U, of binary sequences of period 2/sup n/-1 with low correlations are proposed, where for some positive integer, e, S is defined for odd n/e and U for even n/e. The family S has four-valued correlations and is a generalization of the family of Gold-like sequences introduced by S. Boztas and P.V. Kumar (see ibid., vol.40, p.532-7, 1994). The family U, which is also a generalization of the sequence family defined by P. Udaya (""Polyphase and frequency hopping sequences obtained from finite rings"", Ph.D. dissertation, Dept. Elec. Eng., Indian Inst. Technol., Kanpur, 1992), has six-valued correlations. The relationship between Gold-like sequences and Gold sequences is the same as the relationship between the family S and the family constructed from the binary sequences partially contributed by R. Gold (see ibid., vol.IT-14, p.154-6, 1968), T. Kasami (see Coordinated Sci. Lab., Univ. of Illinois,Urbana-Champaign, Tech. Rep. R-285, AD 632574, 1966), and Welch. Using a lifting idea (No, J.-S. and Kumar, P.V., ibid., vol.35, p.371-9, 1989) for the families S and U, families of binary sequences with the same correlation distributions and large linear span are also constructed.","Binary sequences,
Gold,
Autocorrelation,
Information theory,
Computer science"
TSP: mining top-K closed sequential patterns,"Sequential pattern mining has been studied extensively in data mining community. Most previous studies require the specification of a minimum support threshold to perform the mining. However, it is difficult for users to provide an appropriate threshold in practice. To overcome this difficulty, we propose an alternative task: mining top-k frequent closed sequential patterns of length no less than min-l, where k is the desired number of closed sequential patterns to be mined, and minl, is the minimum length of each pattern. We mine closed patterns since they are compact representations of frequent patterns. We developed an efficient algorithm, called TSP, which makes use of the length constraint and the properties of top-k closed sequential patterns to perform dynamic support-raising and projected database-pruning. Our extensive performance study shows that TSP outperforms the closed sequential pattern mining algorithm even when the latter is running with the best tuned minimum support threshold.","Data mining,
Databases,
Itemsets,
Testing,
Computer science,
Frequency"
Camera calibration using spheres: a semi-definite programming approach,"Vision algorithms utilizing camera networks with a common field of view are becoming increasingly feasible and important. Calibration of such camera networks is a challenging and cumbersome task. The current approaches for calibration using planes or a known 3D target may not be feasible as these objects may not be simultaneously visible in all the cameras. In this paper, we present a new algorithm to calibrate cameras using occluding contours of spheres. In general, an occluding contour of a sphere projects to an ellipse in the image. Our algorithm uses the projection of the occluding contours of three spheres and solves for the intrinsic parameters and the locations of the spheres. The problem is formulated in the dual space and the parameters are solved for optimally and efficiently using semidefinite programming. The technique is flexible, accurate and easy to use. In addition, since the contour of a sphere is simultaneously visible in all the cameras, our approach can greatly simplify calibration of multiple cameras with a common field of view. Experimental results from computer simulated data and real world data, both for a single camera and multiple cameras, are presented.","Cameras,
Calibration,
Computer vision,
Geometry,
Layout,
Costs,
Machine vision,
Computer science,
Educational institutions,
Computational modeling"
"Comparing naive Bayes, decision trees, and SVM with AUC and accuracy","Predictive accuracy has often been used as the main and often only evaluation criterion for the predictive performance of classification or data mining algorithms. In recent years, the area under the ROC (receiver operating characteristics) curve, or simply AUC, has been proposed as an alternative single-number measure for evaluating performance of learning algorithms. We proved that AUC is, in general, a better measure (defined precisely) than accuracy. Many popular data mining algorithms should then be reevaluated in terms of AUC. For example, it is well accepted that Naive Bayes and decision trees are very similar in accuracy. How do they compare in AUC? Also, how does the recently developed SVM (support vector machine) compare to traditional learning algorithms in accuracy and AUC? We will answer these questions. Our conclusions will provide important guidelines in data mining applications on real-world datasets.","Decision trees,
Support vector machines,
Data mining,
Machine learning algorithms,
Accuracy,
Machine learning,
Support vector machine classification,
Computer science,
Area measurement,
Guidelines"
Fast path-based neural branch prediction,"Microarchitectural prediction based on neural learning has received increasing attention in recent years. However, neural prediction remains impractical because its superior accuracy over conventional predictors is not enough to offset the cost imposed by its high latency. We present a new neural branch predictor that solves the problem from both directions: it is both more accurate and much faster than previous neural predictors. Our predictor improves accuracy by combining path and pattern history to overcome limitations inherent to previous predictors. It also has much lower latency than previous neural predictors. The result is a predictor with accuracy for superior to conventional predictors but with latency comparable to predictors from industrial designs. Our simulations show that a path-based neural predictor improves the instructions-per-cycle (IPC) rate of an aggressively clocked microarchitecture by 16% over the original perceptron predictor.",
Positional adaptation of processors: application to energy reduction,"Although adaptive processors can exploit application variability to improve performance or save energy, effectively managing their adaptivity is challenging. To address this problem, we introduce a new approach to adaptivity: the positional approach. In this approach, both the testing of configurations and the application of the chosen configurations are associated with particular code sections. This is in contrast to the currently-used temporal approach to adaptation, where both the testing and application of configurations are tied to successive intervals in time. We propose to use subroutines as the granularity of code sections in positional adaptation. Moreover, we design three implementations of subroutine-based positional adaptation that target energy reduction in three different workload environments: embedded or specialized server, general purpose, and highly dynamic. All three implementations of positional adaptation are much more effective than temporal schemes. On average, they boost the energy savings of applications by 50% and 84% over temporal schemes in two experiments.",
Feedback control with queueing-theoretic prediction for relative delay guarantees in web servers,"The use of feedback control theory for performance guarantees in QoS-aware systems has gained much attention in recent years. In this paper, we investigate merging, within a single framework, the predictive power of queueing theory with the reactive power of feedback control to produce software systems with a superior ability to achieve QoS specifications in highly unpredictable environments. The approach is applied to the problem of achieving relative delay guarantees in high-performance servers. Experimental evaluation of this approach on an Apache web server shows that the combined schemes perform significantly better in terms of keeping the relative delay on target compared to feedback control or queueing prediction alone.","Feedback control,
Web server,
Computer science,
Queueing analysis,
Internet,
System performance,
Adaptive control,
Degradation,
Delay effects,
Delay estimation"
Modeling TTL-based Internet caches,"This paper presents a way of modeling the hit rates of caches that use a time-to-live (TTL)-based consistency policy. TTL-based consistency, as exemplified by DNS and Web caches, is a policy in which a data item, once retrieved, remains valid for a period known as the ""time-to-live"". Cache systems using large TTL periods are known to have high hit rates and scale well, but the effects of using shorter TTL periods are not well understood. We model hit rate as a function of request arrival times and the choice of TTL, enabling us to better understand cache behavior for shorter TTL periods. Our formula for the hit rate is closed form and relies upon a simplifying assumption about the interarrival times of requests for the data item in question: that these requests can be modeled as a sequence of independent and identically distributed random variables. Analyzing extensive DNS traces, we find that the results of the formula match observed statistics surprisingly well; in particular, the analysis is able to adequately explain the somewhat counterintuitive empirical finding of Jung et al. that the cache hit rate for DNS accesses rapidly increases as a function of TTL, exceeding 80% for a TTL of 15 minutes.",
Higher quality requirements specifications through natural language patterns,"In most current industrial software engineering projects, the majority of requirements documents are written almost entirely in natural language. However, specifying the requirements in natural language has one major drawback, namely the inherent imprecision, i.e., ambiguity, incompleteness, and inaccuracy, of natural language. Since the requirements document forms the basis of the whole development process, such defects can have severe consequences for the whole project. Therefore, it is important to deal with these defects in a requirements specification right from the start. We present an approach for reducing the problem of imprecision in natural language requirements specifications with the use of natural language patterns, which allow formulating requirements sentences in a less ambiguous, more complete, and more accurate way. To ensure the applicability of our approach we based our patterns on a metamodel for requirements statements for embedded systems. With this metamodel, we ensure that all forms of requirements statements are described with the patterns. We validated the effectiveness of the patterns by using them to rewrite a substantial, previously written, requirements specification to eliminate its imprecisions.","Natural languages,
Embedded system,
Software systems,
Software engineering,
Costs,
Computer science,
Systems engineering and theory,
Computer industry,
Delay,
Proposals"
Computation-aware scheme for software-based block motion estimation,"Many fast block-matching algorithms (BMAs) reduce the computational complexity of motion estimation by sophisticatedly inspecting a subset of checking points, and stop only when all those checking points have been examined. This means that the searching process for each current block cannot be interrupted, even when it is performed in a software-based computation environment. Our main goal is to allow the searching process to stop once a specified amount of computation has been performed. A novel computation-aware scheme is proposed, which first dynamically determines the target amount of computation power allocated to a frame, and then allocates this to each block in a computation-distortion-optimized manner. We propose a rate-control-like procedure and a predicted computation-distortion benefit heuristic to realize this scheme. Conventional BMAs, such as full-search block matching, three-step search, new three-step search, four-step search, and diamond search, can be transformed into their corresponding computation-aware BMA versions. In our simulations, the resulting computation-aware BMAs not only exhibit higher efficiency than conventional BMAs, but also allow the motion estimation to terminate after any specified amount of computation has been performed (in units of checking points).","Motion estimation,
Computer science,
Computational complexity,
Codecs,
Application software,
High performance computing,
Videos,
ISO standards,
PSNR,
Computational modeling"
Robotics in education: Low-cost platforms for teaching integrated systems,,"Educational robots,
Robot sensing systems,
Robotics and automation,
Service robots,
Educational institutions,
Orbital robotics,
Computer science,
Computer science education,
Consumer electronics,
Robot control"
Icasso: software for investigating the reliability of ICA estimates by clustering and visualization,"A major problem in application of independent component analysis (ICA) is that the reliability of the estimated independent components is not known. Firstly, the finite sample size induces statistical errors in the estimation. Secondly, as real data never exactly follows the ICA model, the contrast function used in the estimation may have many local minima which are all equally good, or the practical algorithm may not always perform properly, for example getting stuck in local minima with strongly suboptimal values of the contrast function. We present an explorative visualization method for investigating the relations between estimates from FastICA. The algorithmic and statistical reliability is investigated by running the algorithm many times with different initial values or with differently bootstrapped data sets, respectively. Resulting estimates are compared by visualizing their clustering according to a suitable similarity measure. Reliable estimates correspond to tight clusters, and unreliable ones to points which do not belong to any such cluster. We have developed a software package called Icasso to implement these operations. We also present results of this method when applying Icasso on biomedical data.","Independent component analysis,
Clustering algorithms,
Application software,
Data analysis,
Data visualization,
Bioinformatics,
Erbium,
Neural networks,
Information technology,
Computer science"
Control of stress and microstructure in cathodic arc deposited films,"The almost fully ionized cathodic arc plasma is a versatile source for the deposition of thin films. Ion energies impinging on the growth surface can easily be controlled by applying substrate bias. The natural energy of the depositing ions is moderate (tens of electron volts) and generates substantial compressive stress in most materials. In hard materials (such as tetrahedral-carbon and titanium nitride), the high-yield stress makes the problem particularly severe. Recent work has shown that stress relaxation can be achieved by pulses of high ion-energy bombardment (/spl sim/10 keV) applied to the substrate during growth. In this paper, we describe the variation of intrinsic stress as a function of applied pulsed bias voltage (V) and pulse frequency (f) for deposition of carbon and titanium nitride films. We found that stress relaxation depends on the parameter Vf, so it is possible to achieve the same level of stress relief for a range of voltages by selecting appropriate pulsing frequencies. With the right choice of parameters, it is possible to almost completely eliminate the intrinsic stress and deposit very thick coatings. Our experimental results showed correlations between intrinsic stress and film microstructures, such as the preferred orientation. This leads to the possibility of controlling microstructure with high energy ion pulsing during growth. Molecular dynamics computer simulations of isolated impacts provide insight into the atomic-scale processes at work. Using the results of such simulations, we describe a model for how stress relief might take place, based on relaxation in thermal spikes occurring around impact sites of the high-energy ions.","Stress control,
Microstructure,
Compressive stress,
Substrates,
Titanium,
Voltage,
Frequency,
Thermal stresses,
Plasma materials processing,
Plasma sources"
Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm,"A key element of bioinformatics research is the extraction of meaningful information from large experimental data sets. Various approaches, including statistical and graph theoretical methods, data mining, and computational pattern recognition, have been applied to this task with varying degrees of success. Using a novel classifier based on the Bayes discriminant function, we present a hybrid algorithm that employs feature selection and extraction to isolate salient features from large medical and other biological data sets. We have previously shown that a genetic algorithm coupled with a k-nearest-neighbors classifier performs well in extracting information about protein-water binding from X-ray crystallographic protein structure data. The effectiveness of the hybrid EC-Bayes classifier is demonstrated to distinguish the features of this data set that are the most statistically relevant and to weight these features appropriately to aid in the prediction of solvation sites.","Evolutionary computation,
Data mining,
Feature extraction,
Protein engineering,
Bioinformatics,
Pattern recognition,
Genetic algorithms,
Computer science,
Solvents,
Biology computing"
Optimal linear estimation fusion - part VI: sensor data compression,,"Sensor fusion,
Data compression,
Image coding,
Mean square error methods,
Data engineering,
Computer science,
Application software,
Algorithm design and analysis,
Image sensors,
Image processing"
What does motion reveal about transparency?,"The perception of transparent objects from images is known to be a very hard problem in vision. Given a single image, it is difficult to even detect the presence of transparent objects in the scene. In this paper, we explore what can be said about transparent objects by a moving observer. We show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene. We present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion. The objects can be complex in that they may be composed of multiple layers with different refractive indices. We have conducted numerous simulations to verify the practical feasibility of our algorithm. We have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy.","Layout,
Painting,
Optical refraction,
Optical control,
Shape measurement,
Computer science,
Object detection,
Graphics,
Rendering (computer graphics),
Interpolation"
Slicing of state-based models,"System modeling is a widely used technique to model state-based systems. Several state-based languages are used to model such systems, e.g., EFSM (extended finite state machine), SDL (specification description language) and state charts. Although state-based modeling is very useful, system models are frequently large and complex and are hard to understand and modify. Slicing is a well-known reduction technique. Most of the research on slicing is code-based. There has been limited research on specification-based slicing and model-based slicing. In this paper, we present an approach to slicing state-based models, in particular EFSM models. Our approach automatically identifies the parts of the model that affect an element of interest using EFSM dependence analysis. Slice reduction techniques are then used to reduce the size of the EFSM slice. Our experience with the presented slicing approach showed that significant reduction of state-based models could be achieved.","Software systems,
Modeling,
Automata,
Decision support systems,
Software maintenance,
Computer science,
Technological innovation,
Programming,
Debugging,
Software testing"
Particle swarm optimization for traveling salesman problem,"This paper proposes a new application of particle swarm optimization for traveling salesman problem. We have developed some special methods for solving TSP using PSO. We have also proposed the concept of swap operator and swap sequence, and redefined some operators on the basis of them, in this way the paper has designed a special PSO. The experiments show that it can achieve good results.","Particle swarm optimization,
Traveling salesman problems,
Space exploration,
Local area networks,
Educational institutions,
Computer science,
Application software,
Genetics,
Evolutionary computation,
Production"
Dynamic bandwidth management for single-hop ad hoc wireless networks,"Distributed weighted fair scheduling schemes for QoS support in wireless networks have not yet become standard. In this paper we propose an admission control and dynamic bandwidth management scheme that provides fairness in the absence of distributed link level weighted fair scheduling. In case weighted fair scheduling becomes available, our system assists it by supplying the scheduler with weights and adjusting them dynamically as network and traffic characteristics vary. To obtain these weights, we convert the bandwidth requirement of the application into a channel time requirement. Our bandwidth manager then allots each flow a share of the channel time depending on its requirement relative to the requirements of other flows in the network. It uses a max-min fairness algorithm with minimum guarantees. The flow controls its packet transmission rate so it only occupies the channel for the fraction of time allotted to it by the bandwidth manager. As available bandwidth in the network and the traffic characteristics of various flows change, the channel time proportion allotted also dynamically varies. Our experiments show that, at the cost of a very low overhead, there is a high probability that every flow in the network will receive at least its minimum requested share of the network bandwidth.","Bandwidth,
Wireless networks,
Dynamic scheduling,
Media Access Protocol,
Throughput,
Admission control,
Communication system traffic control,
Computer network management,
Computer science,
Processor scheduling"
Multitime scale Markov decision processes,"This paper proposes a simple analytical model called M time scale Markov decision process (MMDPs) for hierarchically structured sequential decision making processes, where decisions in each level in the M-level hierarchy are made in M different discrete time scales. In this model, the state-space and the control-space of each level in the hierarchy are nonoverlapping with those of the other levels, respectively, and the hierarchy is structured in a ""pyramid"" sense such that a decision made at level m (slower time scale) state and/or the state will affect the evolutionary decision making process of the lower level m+1 (faster time scale) until a new decision is made at the higher level but the lower level decisions themselves do not affect the transition dynamics of higher levels. The performance produced by the lower level decisions will affect the higher level decisions. A hierarchical objective function is defined such that the finite-horizon value of following a (nonstationary) policy at level m+1 over a decision epoch of level m plus an immediate reward at level m is the single-step reward for the decision making process at level m. From this we define ""multi-level optimal value function"" and derive ""multi-level optimality equation."" We discuss how to solve MMDPs exactly and study some approximation methods, along with heuristic sampling-based schemes, to solve MMDPs.","Decision making,
Analytical models,
Equations,
Approximation methods,
Process control,
Context modeling,
Computer science"
Bounded archiving using the lebesgue measure,"Many modern multiobjective evolutionary algorithms (MOEAs) store the points discovered during optimization in an external archive, separate from the main population, as a source of innovation and/or for presentation at the end of a run. Maintaining a bound on the size of the archive may be desirable or necessary for several reasons, but choosing which points to discard and which to keep in the archive, as they are discovered, is not trivial. We briefly review the state-of-the-art in bounded archiving, and present a new method based on locally maximizing the hyper-volume dominated by the archive. The new archiver is shown to outperform existing methods, on several problem instances, with respect to the quality of the archive obtained when judged using three distinct quality measures.","Computer science,
Chemistry,
Physics,
Technological innovation,
Modems,
Convergence,
Data structures,
Genetics,
Systems engineering and theory,
Educational institutions"
Initial observations of the simultaneous multithreading Pentium 4 processor,"We analyze an Intel Pentium 4 hyper-threading processor. The focus is to understand its performance and the underlying reasons behind that performance. Particular attention is paid to putting the processor in context with prior published research in simultaneous multithreading-validating and reevaluating, where appropriate, how this processor performs relative to expectations. Results include multiprogrammed speedup, parallel speedup, as well as synchronization and communication throughput. The processor is also evaluated in the context of prior work on the interaction of multithreading with the operating system and compilation.","Multithreading,
Switches,
Surface-mount technology,
Hardware,
Operating systems,
Microprocessors,
Context modeling,
Computer science,
Throughput,
Marine vehicles"
AODV-PA: AODV with path accumulation,"Ad hoc networks meet the demands of spontaneous network set-up. They are characterized by the use of wireless links, dynamically changing topology, multi-hop connectivity and decentralized routing mechanisms and decision-making. AODV and DSR are the two most widely studied on-demand ad hoc routing protocols. Previous studies have shown limitations of these protocols in certain network scenarios. To improve the performance of AODV, we modify AODV to include the source route accumulation feature of DSR. We call this AODV with path accumulation. This protocol optimizes AODV to perform effectively in terms of routing overhead and delay during high load. The performance of the protocol is evaluated by a simulation model under a variety of network conditions. We also compare its performance with that of unmodified AODV and DSR. We demonstrate how a small change to the AODV protocol can lead to significantly improved performance results.","Routing protocols,
Ad hoc networks,
Spread spectrum communication,
Delay effects,
Computer science,
Laboratories,
Network topology,
Decision making,
Proposals,
Floods"
"The impact of pair programming on student performance, perception and persistence","This study examined the effectiveness of pair programming in four lecture sections of a large introductory programming course. We were particularly interested in assessing how the use of pair programming affects student performance and decisions to pursue computer science related majors. We found that students who used pair programming produced better programs, were more confident in their solutions, and enjoyed completing the assignments more than students who programmed alone. Moreover, pairing students were significantly more likely than non-pairing students to complete the course, and consequently to pass it. Among those who completed the course, pairers performed as well on the final exam as non-pairers, were significantly more likely to be registered as computer science related majors one year later, and to have taken subsequent programming courses. Our findings suggest that not only does pairing not compromise students' learning, but that it may enhance the quality of their programs and encourage them to pursue computer science degrees.",
Base-station repositioning for optimized performance of sensor networks,"Most of the energy aware routing approaches for unattended wireless sensor networks pursue multi-hop paths in order to minimize the total transmission power. Since almost in all sensor networks data are routed towards a single sink (base-station), hops close to that sink become heavily involved in packet forwarding and thus their batteries get depleted rather quickly. In this paper we investigate the potential of base-station repositioning for enhanced network performance. We address issues related to when should the base-station be relocated, where it would be moved to and how to handle its motion without any effect on data traffic. Our approach tracks the distance from the closest hops to the base-station and the traffic density through these hops. When a hop that forward high traffic is further than a threshold the base-station qualifies the impact of the relocation on the network performance and moves if the overhead is justified. The presented approach is validated in a simulated environment.","Telecommunication traffic,
Wireless sensor networks,
Spread spectrum communication,
Routing,
Traffic control,
Portable computers,
Computer science,
Batteries,
Communication system security,
Monitoring"
Tractable conservative constraint satisfaction problems,"In a constraint satisfaction problem (CSP), the aim is to find an assignment of values to a given set of variables, subject to specified constraints. The CSP is known to be NP-complete in general. However, certain restrictions on the form of the allowed constraints can lead to problems solvable in polynomial time. Such restrictions are usually imposed by specifying a constraint language. The principal research direction aims to distinguish those constraint languages, which give rise to tractable CSPs from those which do not. We achieve this goal for the widely used variant of the CSP, in which the set of values for each individual variable can be restricted arbitrarily. Restrictions of this type can be expressed by including in a constraint language all possible unary constraints. Constraint languages containing all unary constraints will be called conservative. We completely characterize conservative constraint languages that give rise to CSP classes solvable in polynomial time. In particular, this result allows us to obtain a complete description of those (directed) graphs H for which the List H-Coloring problem is polynomial time solvable.","Polynomials,
Laboratories,
Constraint theory,
Logic,
Computer science"
Algorithms for spatial outlier detection,"A spatial outlier is a spatially referenced object whose non-spatial attribute values are significantly different from the values of its neighborhood. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and useful spatial patterns for further analysis. One drawback of existing methods is that normal objects tend to be falsely detected as spatial outliers when their neighborhood contains true spatial outliers. We propose a suite of spatial outlier detection algorithms to overcome this disadvantage. We formulate the spatial outlier detection problem in a general way and design algorithms which can accurately detect spatial outliers. In addition, using a real-world census data set, we demonstrate that our approaches can not only avoid detecting false spatial outliers but also find true spatial outliers ignored by existing methods.","Computer science,
Performance analysis,
Graphics,
Testing,
Scattering,
Iterative algorithms,
Biometrics,
Pattern analysis,
Object detection,
Detection algorithms"
A new convex edge-preserving median prior with applications to tomography,"In a Bayesian tomographic maximum a posteriori (MAP) reconstruction, an estimate of the object f is computed by iteratively minimizing an objective function that typically comprises the sum of a log-likelihood (data consistency) term and prior (or penalty) term. The prior can be used to stabilize the solution and to also impose spatial properties on the solution. One such property, preservation of edges and locally monotonic regions, is captured by the well-known median root prior (MRP), an empirical method that has been applied to emission and transmission tomography. We propose an entirely new class of convex priors that depends on f and also on m, an auxiliary field in register with f. We specialize this class to our median prior (MP). The approximate action of the median prior is to draw, at each iteration, an object voxel toward its own local median. This action is similar to that of MRP and results in solutions that impose the same sorts of object properties as does MRP. Our MAP method is not empirical, since the problem is stated completely as the minimization of a joint (on f and m) objective. We propose an alternating algorithm to compute the joint MAP solution and apply this to emission tomography, showing that the reconstructions are qualitatively similar to those obtained using MRP.","Tomography,
Materials requirements planning,
Image reconstruction,
Iterative algorithms,
Radiology,
Biomedical engineering,
Bayesian methods,
Minimization methods,
Image quality,
Spatial resolution"
Decompression hardware determination for test volume and time reduction through unified test pattern compaction and compression,A methodology for the determination of decompression hardware that guarantees complete fault coverage for a unified compaction/compression scheme is proposed. Test cube information is utilized for the determination of a near optimal decompression hardware. The proposed scheme attains simultaneously high compression levels and reduced pattern counts through a linear decompression hardware. Significant test volume and test application time reductions are delivered through the scheme we propose while a highly cost effective hardware implementation is retained.,"Testing,
Hardware,
Compaction,
Costs,
Test pattern generators,
Sun,
Computer science,
Moore's Law,
Sequential analysis,
Pins"
Unresponsive flows and AQM performance,"Routers handle data packets from sources unresponsive to TCP's congestion avoidance feedback. We are interested in the impact these sources have on active queue management (AQM) control of long-lived TCP traffic. In this paper, we combine models of TCP/AQM dynamics with models of unresponsive traffic to analyze the effects on AQM performance.","Traffic control,
Feedback,
Performance analysis,
Computer science,
Fluid dynamics,
Internet,
Transient response,
Computer networks,
Contracts,
US Department of Defense"
Image registration,"In order to demonstrate the growth of the medical image registration field over the past decades, this paper presents the number of journal publications on this topic since 1988 until 2002. In a similar manner, trends in topics within the field of medical image registration are detected. Publications on computed tomography (CT) and magnetic resonance imaging (MRI) are rather constant through the years. Positron emission tomography (PET) and single photon emission computed tomography (SPECT), on the other hand, seem to loose ground to newly emerging functional imaging techniques, such as functional MRI (fMRI) whereas an increase in interest in registration of ultrasound (US) images was observed. Two topics in image registration that are currently considered hot are intraoperative and elastic registration. Although the interest in intraoperative registration strongly increased in the late 1990s, there seems to be a slight relative decrease in recent years. On the other hand, elastic registration has become a popular topic, reaching the highest numbers so far in 2002.","Biomedical signal processing,
Image registration,
Positron emission tomography,
Single photon emission computed tomography"
Detecting insider threats by monitoring system call activity,One approach to detecting insider misbehavior is to monitor system call activity and watch for danger signs or unusual behavior. We describe an experimental system designed to test this approach. We tested the system's ability to detect common insider misbehavior by examining file system and process-related system calls. Our results show that this approach can detect many such activities.,
Efficient data mining for maximal frequent subtrees,"A new type of tree mining is defined, which uncovers maximal frequent induced subtrees from a database of unordered labeled trees. A novel algorithm, PathJoin, is proposed. The algorithm uses a compact data structure, FST-Forest, which compresses the trees and still keeps the original tree structure. PathJoin generates candidate subtrees by joining the frequent paths in FST-Forest. Such candidate subtree generation is localized and thus substantially reduces the number of candidate subtrees. Experiments with synthetic data sets show that the algorithm is effective and efficient.","Data mining,
Tree graphs,
Tree data structures,
Computer science,
Databases,
Association rules,
Educational institutions,
Data engineering,
Bioinformatics,
Pattern analysis"
Information assurance measures and metrics - state of practice and proposed taxonomy,"The term ""assurance"" has been used for decades in trusted system development as an expression of confidence that one has in the strength of mechanisms or countermeasures. One of the unsolved problems of security engineering is the adoption of measures or metrics that can reliably depict the assurance associated with a specific hardware and software system. This paper reports on a recent attempt to focus requirements in this area by examining those currently in use. It then suggests a categorization of information assurance (IA) metrics that may be tailored to an organization's needs. We believe that the provision of security mechanisms in systems is a subset of the systems engineering discipline having a large software-engineering correlation. There is general agreement that no single system metric or any ""one-prefect"" set of IA metrics applies across all systems or audiences. The set most useful for an organization largely depends on their IA goals, their technical, organizational and operational needs, and the financial, personnel, and technical resources that are available.","Taxonomy,
Information security,
Computer security,
Conferences,
Computer science,
Application software,
Government,
Reliability engineering,
Software measurement,
Hardware"
3D ultrasonic tagging system for observing human activity,"This paper describes an ultrasonic tagging system developed for robustly observing human activity in a living area. Using ultrasonic transmitter tags with unique identifiers, the system is shown through experimental application to be able to track the three-dimensional motion of tagged objects in real time with high accuracy, resolution and robustness to occlusion. The use of an ultrasonic system is desirable because of its low cost and use of commercial components, and the proposed system achieves high accuracy and robustness through the use of many redundant sensors. The system employs multilateration to locate tagged objects using one of two estimation algorithms, a least-squares optimization method or a random sample consensus method.","Tagging,
Humans,
Pattern recognition,
Real time systems,
Object recognition,
Noise robustness,
Costs,
Equations,
Computer science,
Transmitters"
Hybrid evolutionary algorithms based on PSO and GA,"Inspired by the idea of genetic algorithm, we propose two hybrid evolutionary algorithms based on PSO and GA methods through crossing over the PSO and GA algorithms. The main ideas of the two proposed methods are to integrate PSO and GA methods in parallel and series forms respectively. Simulations for a series of benchmark test functions show that both of the two proposed methods possess better ability to find the global optimum than that of the standard PSO algorithm.","Evolutionary computation,
Genetic algorithms,
Computational modeling,
Particle swarm optimization,
Computer science,
Benchmark testing,
Genetic mutations,
Algorithm design and analysis,
Educational institutions,
High performance computing"
VEST: an aspect-based composition tool for real-time systems,"Building distributed embedded systems from scratch is not cost-effective. Instead, designing and building these systems by using domain specific components has promise. However, in using components, the most difficult issues are ensuring that hidden dependencies won't cause failures and that non-functional properties such as real-time performance are being met. We have built the VEST toolkit whose aim is to provide a rich set of dependency checks based on the concept of aspects to support distributed embedded system development via components. We describe the toolkit and its novelty. We also use VEST on two case studies of a CORBA-based middleware for avionics. Data collected shows that VEST can significantly reduce the time it takes to build a distributed real-time embedded system by over 50%. Key ""lessons learned"" from our experience with using VEST on these case studies are also highlighted.","Real time systems,
Embedded system,
Software libraries,
Buildings,
Yarn,
Hardware,
Middleware,
Embedded software,
Computer science,
Aerospace electronics"
Fuzzy ants as a clustering concept,We present a swarm intelligence approach to data clustering. Data is clustered without initial knowledge of the number of clusters. Ant based clustering is used to initially create raw clusters and then these clusters are refined using the Fuzzy C Means algorithm. Initially the ants move the individual objects to form heaps. The centroids of these heaps are taken as the initial cluster centers and the Fuzzy C Means algorithm is used to refine these clusters. In the second stage the objects obtained from the Fuzzy C Means algorithm are hardened according to the maximum membership criteria to form new heaps. These new heaps are then sometimes moved and merged by the ants. The final clusters formed are refined by using the Fuzzy C Means algorithm. Results from three small data sets show that the partitions produced are competitive with those obtained from FCM.,"Clustering algorithms,
Particle swarm optimization,
Iterative algorithms,
Partitioning algorithms,
Feedback,
Sorting,
Cadaver,
Computer science,
Data engineering,
Fuzzy logic"
The modified group delay function and its application to phoneme recognition,"We explore a new spectral representation of speech signals through group delay functions. The group delay functions by themselves are noisy and difficult to interpret owing to zeroes that are close to the unit circle in the z-domain and these clutter the spectra. A new modified group delay function (Yegnanarayan, B. and Murthy, H.A., IEEE Trans. Sig. Processing, vol.40, p.2281-9, 1992) that reduces the effects of zeroes close to the unit circle is used. Assuming that this new function is minimum phase, the modified group delay spectrum is converted to a sequence of cepstral coefficients. A preliminary phoneme recogniser is built using features derived from these cepstra. Results are compared with those obtained from features derived from the traditional mel frequency cepstral coefficients (MFCC). The baseline MFCC performance is 34.7%, while that of the best modified group delay cepstrum is 39.2%. The performance of the composite MFCC feature, which includes the derivatives and double derivatives, is 60.7%, while that of the composite modified group delay feature is 57.3%. When these two composite features are combined, /spl sim/2% improvement in performance is achieved (62.8%). When this new system is combined with linear frequency cepstra (LFC) (Gadde, V.R.R. et al., The SRI SPINE 2001 Evaluation System. http://elazar.itd.nrl.navy.mil/spine/sri2/presentation/sri2001.html, 2001), the system performance results in another /spl sim/0.8% improvement (63.6%).","Delay,
Cepstral analysis,
Mel frequency cepstral coefficient,
Fourier transforms,
Speech analysis,
Speech recognition,
Phase estimation,
Working environment noise,
Application software,
Computer science"
Communicating centrality in policy network drawings,"We introduce a network visualization technique that supports an analytical method applied in the social sciences. Policy network analysis is an approach to study policy making structures, processes, and outcomes, thereby concentrating on relations between policy actors. An important operational concept for the analysis of policy networks is the notion of centrality, i.e., the distinction of actors according to their importance in a relational structure. We integrate this measure in a layout model for networks by mapping structural to geometric centrality. Thus, centrality values and network data can be presented simultaneously and explored interactively.","Intelligent networks,
Social network services,
Public policy,
Data visualization,
Solid modeling,
Information analysis,
Drugs,
Instruments,
Collaboration,
Multidimensional systems"
Computer forensics education,The application of science and education to computer-related crime forensics is still largely limited to law enforcement organizations. Building a suitable workforce development program could support the rapidly growing field of computer and network forensics.,"Forensics,
Computer science education,
Educational programs,
Computer security,
Information security,
Computer crime,
Law enforcement,
Computer networks,
Layout,
Information systems"
A new recursive construction for optical orthogonal codes,"We present a new recursive construction for (n,/spl omega/,/spl lambda//sub a/,/spl lambda//sub c/) optical orthogonal codes. For the case of /spl lambda//sub a/ = /spl lambda//sub c/ = /spl lambda/, this recursive construction enlarges the original family with /spl lambda/ unchanged, and produces a new family of asymptotically optimal codes, if the original family is asymptotically optimal. We call a code asymptotically optimal, following the definition of O. Moreno et al. (see ibid., vol.41, p.448-55, 1995), if, as n, the length of code, goes to infinity, the ratio of the number of codewords to the corresponding Johnson bound approaches unity.","Autocorrelation,
H infinity control,
Multiaccess communication,
Computer science,
Upper bound"
Predicting maintainability with object-oriented metrics -an empirical comparison,,"Software maintenance,
Size measurement,
Software measurement,
Software systems,
Software quality,
History,
Programming,
Software engineering,
Computer science,
Joining processes"
EMPOWER: a network emulator for wireline and wireless networks,"The increasing need of protocol development environments and network performance evaluation tools gives rise to the research of flexible, scalable, and accurate network emulators. The desired network emulator should be able to facilitate the emulation of either wireline or wireless networks. In the case when network topology is critical to the underlying network protocol, the emulator should provide specific mechanisms to emulate network topology. In this paper, we present a distributed network emulation system EMPOWER, which not only can fulfill those requirements, but also can generate user-defined network conditions and traffic dynamics at packet level. EMPOWER is highly scalable in that each emulator node could be configured to emulate multiple network nodes. Some significant research issues such as topology mapping scheme and scalability of the emulator are discussed and addressed. Preliminary emulation results show that EMPOWER is capable of assisting the study of both wireless and wireline network protocols and applications.","Wireless networks,
Emulation,
Network topology,
Wireless application protocol,
Computer science,
Telecommunication traffic,
Testing,
Communication system traffic control,
Traffic control,
Bit error rate"
Twin binary sequences: a nonredundant representation for general nonslicing floorplan,"The efficiency and effectiveness of many floorplanning methods depend very much on the representation of the geometrical relationship between the modules. A good representation can shorten the searching process so that more accurate estimations on area and interconnect costs can be performed. Nonslicing floorplan is the most general kind of floorplan that is commonly used. Unfortunately, there is not yet any complete and nonredundant topological representation for nonslicing structure. In this paper, we propose the first representation of this kind. Like some previous work (Zhou et al. 2001), we have also made use of a mosaic floorplan as an intermediate step. However, instead of including a more than sufficient number of extra dummy blocks in the set of modules (that will increase the size of the solution space significantly), our representation allows us to insert an exact number of irreducible empty rooms to a mosaic floorplan such that every nonslicing floorplan can be obtained uniquely from one and only one mosaic floorplan. The size of the solution space is only O(n!2/sup 3n//n/sup 1.5/), which is the size without empty room insertion, but every nonslicing floorplan can be generated uniquely and efficiently in linear time without any redundant representation.","Binary sequences,
Integrated circuit interconnections,
Very large scale integration,
Costs,
Design optimization,
Shape,
Circuit optimization,
Minimization,
Delay,
Computer science"
Design of a motion-compensation OSEM list-mode algorithm for resolution-recovery reconstruction for the HRRT,"The HRRT PET system has the potential to produce human brain images with resolution better than 3 mm. To achieve the best possible accuracy and precision, we have designed MOLAR, a motion-compensation OSEM list-mode algorithm for resolution-recovery reconstruction on a computer cluster with the following features: direct use of list mode data with dynamic motion information (Polaris); exact reprojection of each line-of- response (LOR); system matrix computed from voxel-to-LOR distances (radial and axial); spatially varying resolution model implemented for each event by selection from precomputed line spread functions based on factors including detector obliqueness, crystal layer, and block detector position; distribution of events to processors and to subsets based on order of arrival; removal of voxels and events outside a reduced field-of-view defined by the attenuation map; no pre-corrections to Poisson data, i.e., all physical effects are defined in the model; randoms estimation from singles; model-based scatter simulation incorporated into the iterations; and component-based normalization. Preliminary computation estimates suggest that reconstruction of a single frame in one hour is achievable. Careful evaluation of this system will define which factors play an important role in producing high resolution, low-noise images with quantitative accuracy.","Algorithm design and analysis,
Image reconstruction,
Clustering algorithms,
Image resolution,
Spatial resolution,
Distributed computing,
Physics computing,
Event detection,
Motion detection,
Detectors"
Factors in automatic musical genre classification of audio signals,"Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.","Computer science,
Peer to peer computing,
Music information retrieval,
Humans,
Feature extraction,
Performance analysis,
Machine learning,
Audio compression,
Hard disks,
Bandwidth"
An analysable bus-guardian for event-triggered communication,"We present a guardian-based approach to detecting 'babbling idiots', faulty nodes which erroneously consume extra resource in an event triggered system. In general, one cannot detect all babbling idiots, but the maximum effect of undetected faults is bounded and small, and therefore can be taken into account in worst case response time analysis to guarantee that a babbling idiot cannot cause a timing failure elsewhere in the system. The approach is applied specifically to the CAN protocol to protect against faulty nodes transmitting message frames too often. We show that the overhead of including the effect of undetected frames into the worst case response time analysis is small enough to be of practical value.","Fault detection,
Real time systems,
Delay,
Timing,
Failure analysis,
Protocols,
Protection,
Bandwidth,
Computer science,
Event detection"
Zero-knowledge sets,"We show how a polynomial-time prover can commit to an arbitrary finite set S of strings so that, later on, he can, for any string x, reveal with a proof whether x /spl isin/ S or x /spl notin/ S, without revealing any knowledge beyond the verity of these membership assertions. Our method is non interactive. Given a public random string, the prover commits to a set by simply posting a short and easily computable message. After that, each time it wants to prove whether a given element is in the set, it simply posts another short and easily computable proof, whose correctness can be verified by any one against the public random string. Our scheme is very efficient; no reasonable prior way to achieve our desiderata existed. Our new primitive immediately extends to providing zero-knowledge databases.","Laboratories,
Upper bound,
Security,
Polynomials,
Mathematics,
Computer science,
National electric code,
Modular construction"
Reliable identification of bounded-length viruses is NP-complete,"A virus is a program that replicates itself by copying its code into other files. A common virus-protection mechanism involves scanning files to detect code patterns of known viruses. We prove that the problem of reliably identifying a bounded-length mutating virus is NP-complete by showing that a virus detector for a certain virus strain can be used to solve the satisfiability problem. The implication of this result is that virus identification methods will be facing increasing strain as virus mutation and hosting strategies mature, and that different protection methods should be developed and employed.","Viruses (medical),
Computer science,
Decoding,
Random sequences,
Binary sequences,
Polynomials,
Binary trees,
Algorithm design and analysis,
Computational complexity,
Upper bound"
UMR: a multi-round algorithm for scheduling divisible workloads,"In this paper we present an algorithm for scheduling parallel divisible workload applications. Our algorithm uses multiple rounds to overlap communication and computation between a master and several workers. We use ""uniform"" rounds, i.e. a fixed amount of work is sent out to all workers at each round. This restriction makes it possible to compute an approximately optimal number of rounds, which was not possible for previously proposed algorithms. In addition, we use more realistic platform models than those used in previous works. We provide an analysis of our algorithm both for homogeneous and heterogeneous platforms and present simulation results.","Scheduling algorithm,
Processor scheduling,
Algorithm design and analysis,
Partitioning algorithms,
Computer science,
Supercomputers,
Application software,
Computational modeling,
Analytical models,
Read-write memory"
An incentive compatible reputation mechanism,"Traditional centralized approaches to security are difficult to apply to large, distributed marketplaces in which software agents operate. Developing a notion of trust that is based on the reputation of agents can provide a softer notion of security that is sufficient for many multi-agent applications. We address the issue of incentive-compatibility (i.e. how to make it optimal for agents to share reputation information truthfully), by introducing a side-payment scheme, organized through a set of broker agents, that makes it rational for software agents to truthfully share the reputation information they have acquired in their past experience. We also show how to use a cryptographic mechanism to protect the integrity of reputation information and to achieve a tight bounding between the identity and reputation of an agent.","Software agents,
Artificial intelligence,
Laboratories,
Computer science,
Computer security,
Information security,
Application software,
Cryptography,
Protection,
Environmental management"
Blind deblurring of spiral CT images,"To discriminate fine anatomical features in the inner ear, it has been desirable that spiral computed tomography (CT) may perform beyond their current resolution limits with the aid of digital image processing techniques. In this paper, we develop a blind deblurring approach to enhance image resolution retrospectively without complete knowledge of the underlying point spread function (PSF). An oblique CT image can be approximated as the convolution of an isotropic Gaussian PSF and the actual cross section. Practically, the parameter of the PSF is often unavailable. Hence, estimation of the parameter for the underlying PSF is crucially important for blind image deblurring. Based on the iterative deblurring theory, we formulate an edge-to-noise ratio (ENR) to characterize the image quality change due to deblurring. Our blind deblurring algorithm estimates the parameter of the PSF by maximizing the ENR, and deblurs images. In the phantom studies, the blind deblurring algorithm reduces image blurring by about 24%, according to our blurring residual measure. Also, the blind deblurring algorithm works well in patient studies. After fully automatic blind deblurring, the conspicuity of the submillimeter features of the cochlea is substantially improved.","Spirals,
Computed tomography,
Image resolution,
Parameter estimation,
Iterative algorithms,
Ear,
Digital images,
Convolution,
Image restoration,
Image quality"
Robust estimation of ultrasound pulses using outlier-resistant de-noising,"A different approach to the problem of estimation of the ultrasound pulse spectrum, which usually arises as a part of ultrasound image restoration algorithms, is presented. It is shown that this estimation problem can be reformulated in terms of a de-noising problem. In this formulation, the log-spectrum of a radio-frequency line (RF-line) is viewed as a noisy measurement of the signal that needs to be estimated, i.e., the ultrasound pulse log-spectrum. The log-spectrum of the tissue reflectivity function (i.e., tissue response) is considered as the noise to be rejected. The contribution of the paper is twofold. First, it provides statistical description of the reflectivity function log-spectrum for the case, when the samples of the reflectivity function are independent identically distributed (i.i.d.) Gaussian random variables. Moreover, it is shown that the problem of the pulse spectrum recovery is essentially a de-noising problem. Consequently, it is suggested to solve the problem within the framework of the de-noising by wavelet shrinkage. Second, a computationally efficient algorithm is proposed for the pulse-spectrum estimation, which can be viewed as a modified version of the classical Donoho's three-step de-noising procedure. This modification is necessary, because of specific properties of the noise to be rejected. It is shown, that whenever the samples of the reflectivity function can be assumed to be i.i.d. Gaussian random variables, the samples of its log-spectrum obey the Fisher-Tippet distribution. For this type of noise, straightforward implementation of the standard de-noising can cause serious estimation errors. In order to overcome this difficulty, an outlier-resistant de-noising is performed. The unique properties of this modified de-noising algorithm allow estimating the pulse spectrum adaptively to its properties, as they are continuously influenced by the frequency-dependent attenuation process. The performance of the proposed algorithm is examined in a series of computer-simulations. It is shown that this algorithm, developed on the assumption of the ""Gaussian"" reflectivity function, remains applicable for broader classes of distributions. The results obtained in a series of in vivo experiments reveal superior performance of the novel approach over some of alternative estimation techniques, e.g., cepstrum-based estimation.","Robustness,
Ultrasonic imaging,
Noise reduction,
Reflectivity,
Pulse measurements,
Random variables,
Frequency estimation,
Image restoration,
Radio frequency,
Ultrasonic variables measurement"
Learning a locality preserving subspace for visual recognition,"We have demonstrated that the face recognition performance can be improved significantly in low dimensional linear subspaces. Conventionally, principal component analysis (PCA) and linear discriminant analysis (LDA) are considered effective in deriving such a face subspace. However, both of them effectively see only the Euclidean structure of face space. We propose a new approach to mapping face images into a subspace obtained by locality preserving projections (LPP) for face analysis. We call this Laplacianface approach. Different from PCA and LDA, LPP finds an embedding that preserves local information, and obtains a face space that best detects the essential manifold structure. In this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced. We compare the proposed Laplacianface approach with eigenface and fisherface methods on three test datasets. Experimental results show that the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition.","Principal component analysis,
Linear discriminant analysis,
Face recognition,
Face detection,
Testing,
Computer vision,
Helium,
Asia,
Computer science,
Image analysis"
Near-optimal lower bounds on the multi-party communication complexity of set disjointness,"We study the communication complexity of the set disjointness problem in the general multiparty model. For t players, each holding a subset of a universe of size n, we establish a near-optimal lower bound of /spl Omega/(n/(t log t)) on the communication complexity of the problem of determining whether their sets are disjoint. In the more restrictive one-way communication model, in which the players are required to speak in a predetermined order, we improve our bound to an optimal /spl Omega/(n/t). These results improve upon the earlier bounds of /spl Omega/(n/t/sup 2/) in the general model, and /spl Omega/((/spl epsiv//sup 2/n)/t/sup 1+/spl epsiv//) in the one-way model, due to Bar-Yossef, Jayram, Kumar, and Sivakumar (2002). As in the case of earlier results, our bounds apply to the unique intersection promise problem. This communication problem is known to have connections with the space complexity of approximating frequency moments in the data stream model. Our results lead to an improved space complexity lower bound of /spl Omega/(n/sup 1-2/k//log n) for approximating the k/sup th/ frequency moment with a constant number of passes over the input, and a technical improvement to /spl Omega/(n/sup 1-2/k/) if only one pass over the input is permitted. Our proofs rely on the information theoretic direct sum decomposition paradigm of Bar-Yossef et al. [2002]. Our improvements stem from novel analytical techniques, as opposed to earlier techniques based on Hellinger and related distances, for estimating the information cost of protocols for one-bit functions.","Complexity theory,
Mathematics,
Frequency,
Computer science,
Sun,
Mathematical model,
Cost function,
Protocols,
Probes,
Books"
Augmenting simulated annealing to build interaction test suites,"Component based software development is prone to unexpected interaction faults. The goal is to test as many-potential interactions as is feasible within time and budget constraints. Two combinatorial objects, the orthogonal array and the covering array, can be used to generate test suites that provide a guarantee for coverage of all t-sets of component interactions in the case when the testing of all interactions is not possible. Methods for construction of these types of test suites have focused on two main areas. The first is finding new algebraic constructions that produce smaller test suites. The second is refining computational search algorithms to find smaller test suites more quickly. In this paper we explore one method for constructing covering arrays of strength three that combines algebraic constructions with computational search. This method leverages the computational efficiency and optimality of size obtained through algebraic constructions while benefiting from the generality of a heuristic search. We present a few examples of specific constructions and provide some new bounds for some strength three covering arrays.","Simulated annealing,
Testing,
Operating systems,
Linux,
Computer science,
Control systems,
Programming,
Software systems,
Hardware,
Printers"
More on average case vs approximation complexity,"We consider the problem to determine the maximal number of satisfiable equations in a linear system chosen at random. We make several plausible conjectures about the average case hardness of this problem for some natural distributions on the instances, and relate them to several interesting questions in the theory of approximation algorithms and in cryptography. Namely we show that our conjectures imply the following facts: (1) Feige's hypothesis about the hardness of refuting a random 3CNF is true, which in turn implies inapproximability within a constant for several combinatorial problems, for which no NP-hardness of approximation is known. (2) It is hard to approximate the nearest codeword within factor n/sup 1 - /spl epsi//. (3) It is hard to estimate the rigidity of a matrix. More exactly, it is hard to distinguish between matrices of low rigidity and random ones. (4) There exists a secure public-key (probabilistic) cryptosystem, based on the intractability of decoding of random binary codes. Our conjectures are strong in that they assume cryptographic hardness: no polynomial algorithm can solve the problem on any non-negligible fraction of inputs. Nevertheless, to the best of our knowledge no efficient algorithms are currently known that refute any of our hardness conjectures.","Computer aided software engineering,
Cryptography,
Decoding,
Equations,
Linear systems,
Approximation algorithms,
Polynomials,
Computer science,
NP-complete problem,
Public key"
Beyond the Personal Software Process: Metrics collection and analysis for the differently disciplined,"Pedagogues such as the Personal Software Process (PSP) shift metrics definition, collection, and analysis from the organizational level to the individual level. While case study research indicates that the PSP can provide software engineering students with empirical support for improving estimation and quality assurance, there is little evidence that many students continue to use the PSP when no longer required to do so. Our research suggests that this ""PSP adoption problem"" may be due to two problems: the high overhead of PSP-style metrics collection and analysis, and the requirement that PSP users ""context switch"" between product development and process recording. This paper overviews our initial PSP experiences, our first attempt to solve the PSP adoption problem with the LEAP system, and our current approach called Hackystat. This approach fully automates both data collection and analysis, which eliminates overhead and context switching. However, Hackystat changes the kind of metrics data that is collected, and introduces new privacy-related adoption issues of its own.","Software engineering,
Data analysis,
Quality assurance,
Software tools,
Character generation,
Context-aware services,
Collaborative software,
Laboratories,
Information analysis,
Product development"
Simultaneous pose and correspondence determination using line features,"We present a new robust line matching algorithm for solving the model-to-image registration problem. Given a model consisting of 3D lines and a cluttered perspective image of this model, the algorithm simultaneously estimates the pose of the model and the correspondences of model lines to image lines. The algorithm combines softassign for determining correspondences and POSIT for determining pose. Integrating these algorithms into a deterministic annealing procedure allows the correspondence and pose to evolve from initially uncertain values to a joint local optimum. This research extends to line features the SoftPOSIT algorithm proposed recently for point features. Lines detected in images are typically more stable than points and are less likely to be produced by clutter and noise, especially in man-made environments. Experiments on synthetic and real imagery with high levels of clutter, occlusion, and noise demonstrate the robustness of the algorithm.",
An exploratory study into deception detection in text-based computer-mediated communication,"Deception is an everyday occurrence across all communication media. The expansion of the Internet has significantly increased the amount of textual communication received and stored by individuals and organizations. Inundated with massive amounts of textual information transmitted through computer-mediated communication, CMC, people remain largely unsuccessful and inefficient in detecting those messages that may be deceptive. Creating an automated tool that could help people flag the possible deceptive messages in CMC is desirable, but first it is necessary to understand cues used to deceive in textual instances. This study focuses on the identification of deceptive cues deceivers use in a textual CMC environment. 30 dyads (n =14 truthful, n = 16 deceptive) were able to complete the desert survival problem. Findings have demonstrated significant differences between the content within truthful and deceptive messages. Several cues were also found to be significantly more present when deceivers write messages.","Computer mediated communication,
Internet,
Costs,
Switches,
Information systems,
Research initiatives,
Information filtering,
Information filters,
Pressing,
National security"
LightFlood: an efficient flooding scheme for file search in unstructured peer-to-peer systems,"""Flooding"" is a fundamental operation in unstructured peer-to-peer (P2P) file sharing systems, such as Gnutella. Although it is effective in content search, flooding is very inefficient because it results in a great amount of redundant messages. Our study shows that more than 70% of the generated messages are redundant for a flooding with a TTL of 7 in a moderately connected network. Existing efforts to address this problem have been focused on limiting the use of flooding operations. We propose LightFlood, an efficient flooding scheme, with the objective of minimizing the number of redundant messages and retaining the same message propagating scope as that of standard flooding. By constructing a tree-like suboverlay within the existing P2P overlay called FloodNet, the flooding operation in LightFlood is divided into two stages. In the first stage, a message is propagated by using the standard flooding scheme with three or four TTL hops, through which the message can be spread to a sufficiently large scope with a small number of redundant messages. In the second stage, the message propagating is only conducted across the FloodNet, significantly reducing the number of redundant messages. Our analysis and simulation results show that the LightFlood scheme provides a low overhead broadcasting facility that can be effectively used in P2P searching. Compared with standard flooding used in Gnutella, we show that the LightFlood scheme with an additional 2 to 3 hops can reduce up to more than 69% of flooding messages, and retain the same flooding scope","Floods,
Peer to peer computing,
Broadcasting,
Optical propagation,
Web server,
Delay,
Computer science,
Educational institutions,
Analytical models,
Web and internet services"
The Medical Image Display and Analysis Group at the University of North Carolina: Reminiscences and philosophy,"The period of the Medical Image Display and Analysis Group (MIDAG) so far is 1974-2002: more than 27 years. We began with a focus on two-dimensional (2-D) display: contrast enhancement, display scale choice, and display device standardization. We co-invented adaptive histogram equalization and later improved it to contrast-limited AHE, and we were perhaps the first to show that adaptive contrast enhancement, i.e., care in the mapping between recorded and displayed intensity and variation of that mapping with the local properties of the image, could significantly affect diagnostic or therapeutic decisions. MIDAG prides itself in having affected medical practice and, thus, the lives of patients. Despite the fact that bringing research from conception to actual medical use is a process sometimes taking a decade, the largest fraction, perhaps all, of our graduate students and faculty are attracted to these applications of computers by this altruism. Areas in which MIDAG research has come to this fruition are the uses of color display in nuclear medicine, the standardization of CRT display and the realization of how many bits of intensity are needed, and the use of tested contrast enhancement methods in areas of medical image use where subtle changes must be detected. Medical areas where we have had an effect are mammography, a major target area for both the standardization and contrast enhancement ends, and portal imaging in radiotherapy, a target area for contrast enhancement. In the 1980s, some of MIDAG's attention moved to image analysis. Also beginning in the 1980s we began to make some contributions to the notions of scale space description of images. With emphasis on the development of segmentation by deformable models and our aforementioned principle that validation is a critical part of research developing image analysis and display methods, we have begun to seriously face the issues of how to validate segmentation and how to choose the parameters of a segmentation method. Our experimental design and analysis techniques involve a variety of new methods for repeated variables designs.","Biomedical imaging,
Image analysis,
Medical diagnostic imaging,
Two dimensional displays,
Standardization,
Image segmentation,
Computer displays,
Image color analysis,
Histograms,
Adaptive equalizers"
On the capability of an SOM based intrusion detection system,"An approach to network intrusion detection is investigated, based purely on a hierarchy of Self-Organizing Feature Maps. Our principle interest is to establish just how far such an approach can be taken in practice. To do so, the KDD benchmark dataset from the International Knowledge Discovery and Data Mining Tools Competition is employed. This supplies a connection-based description of a factitious computer network in which each connection is described in terms of 41 features. Unlike previous approaches, only 6 of the most basic features are employed. The resulting system is capable of detection (false positive) rates of 89% (4.6%), where this is at least as good as the alternative data-mining approaches that require all 41 features.","Intrusion detection,
Data mining,
Neurons,
Testing,
Computer science,
Computer networks,
Computer vision,
Internet,
Knowledge based systems,
Monitoring"
The structure of tail-biting trellises: minimality and basic principles,"Basic structural properties of tail-biting trellises are investigated. We start with rigorous definitions of various types of minimality for tail-biting trellises. We then show that biproper and/or nonmergeable tail-biting trellises are not necessarily minimal, even though every minimal tail-biting trellis is biproper. Next, we introduce the notion of linear (or group) trellises and prove, by example, that a minimal tail-biting trellis for a binary linear code need not have any linearity properties whatsoever. We observe that a trellis - either tail-biting or conventional - is linear if and only if it factors into a product of elementary trellises. Using this result, we show how to construct, for any given linear code /spl Copf/, a tail-biting trellis that minimizes the product of state-space sizes among all possible linear tail-biting trellises. We also prove that every minimal linear tail-biting trellis for /spl Copf/ arises from a certain n/spl times/n characteristic matrix, and show how to compute this matrix in time O(n/sup 2/) from any basis for /spl Copf/. Furthermore, we devise a linear-programming algorithm that starts with the characteristic matrix and produces a linear tail-biting trellis for /spl Copf/; which minimizes the maximum state-space size. Finally, we consider a generalized product construction for tail-biting trellises, and use it to prove that a linear code /spl Copf/ and its dual /spl Copf//sup /spl perp///spl Copf//sup /spl perp///spl Copf//sup /spl perp///spl Copf//sup /spl perp// have the same state-complexity profiles.","Linear code,
Block codes,
Iterative decoding,
Linearity,
Information theory,
Convolutional codes,
Parity check codes,
Conferences,
Computer science,
Wireless communication"
Object-specific figure-ground segregation,"We consider the problem of segmenting an image into foreground and background, with foreground containing solely objects of interest known a priori. We propose an integration model that incorporates both edge detection and object part detection results. It consists of two parallel processes: low-level pixel grouping and high-level patch grouping. We seek a solution that optimizes a joint grouping criterion in a reduced space enforced by grouping correspondence between pixels and patches. Using spectral graph partitioning, we show that a near global optimum can be found by solving a constrained eigenvalue problem. We report promising experimental results on a dataset of 15 objects under clutter and occlusion.","Image segmentation,
Image edge detection,
Object detection,
Object segmentation,
Labeling,
Testing,
Cognitive robotics,
Cognition,
Information science,
Eigenvalues and eigenfunctions"
Factors affecting the correlation coefficient template matching algorithm with application to real-time 2-D coronary artery MR imaging,"This paper characterizes factors affecting the accuracy of the correlation coefficient (CC) template matching algorithm, as applied to motion tracking from two-dimensional real-time coronary artery magnetic resonance images. The performance of this algorithm is analyzed in the presence of both random and systematic error. In the presence of random error, it is shown that a necessary and sufficient condition for accurate motion tracking is a large CC difference-to-noise ratio (CCDNR). The CCDNR itself is in turn affected by five factors: image and template size, image and template structure, and the magnitude of the noise. Techniques are introduced for manipulating some of these factors in order to increase the CCDNR for greater motion tracking accuracy. In the presence of superimposed systematic error it is shown that, while large CCDNR is necessary, it alone is not sufficient to ensure accurate motion tracking. Techniques are developed for improving motion tracking accuracy that minimize the effects of systematic error, while maintaining an adequate CCDNR level. The ability of these techniques to improve motion tracking accuracy is demonstrated both in phantoms and in coronary artery images.","Arteries,
Tracking,
Magnetic resonance imaging,
Biomedical imaging,
Two dimensional displays,
Magnetic resonance,
Performance analysis,
Algorithm design and analysis,
Biophysics,
Magnetic analysis"
Design patterns and change proneness: an examination of five evolving systems,"Design patterns are recognized, named solutions to common design problems. The use of the most commonly referenced design patterns should promote adaptable and reusable program code. When a system evolves, changes to code involving a design pattern should, in theory, consist of creating new concrete classes that are extensions or subclasses of previously existing classes. Changes should not, in theory, involve direct modifications to the classes in prior versions that play roles in a design patterns. We studied five systems, three proprietary systems and two open source systems, to identify the observable effects of the use of design patterns in early versions on changes that occur as the systems evolve. In four of the five systems, pattern classes are more rather than less change prone. Pattern classes in one of the systems were less change prone. These results held up after normalizing for the effect of class size - larger classes are more change prone in two of the five systems. These results provide insight into how design patterns are actually used, and should help us to learn to develop software designs that are more easily adapted.","Concrete,
Software design,
Software maintenance,
Pattern analysis,
Programming,
Laboratories,
Computer science,
Educational institutions,
Pattern recognition,
Size measurement"
Soft Learning Vector Quantization,"Learning vector quantization (LVQ) is a popular class of adaptive nearest prototype classifiers for multiclass classification, but learning algorithms from this family have so far been proposed on heuristic grounds. Here, we take a more principled approach and derive two variants of LVQ using a gaussian mixture ansatz. We propose an objective function based on a likelihood ratio and derive a learning rule using gradient descent. The new approach provides a way to extend the algorithms of the LVQ family to different distance measure and allows for the design of “soft” LVQ algorithms. Benchmark results show that the new methods lead to better classification performance than LVQ 2.1. An additional benefit of the new method is that model assumptions are made explicit, so that the method can be adapted more easily to different kinds of problems.",
SLA-driven management of distributed systems using the common information model,"We present a novel approach of using CIM for the SLA-driven management of distributed systems and discuss our implementation experiences. Supported by the growing acceptance of the Web Services Architecture, an emerging trend in application service delivery is to move away from tightly coupled systems towards structures of loosely coupled, dynamically bound systems to support both long and short term business relationships across different service provider boundaries. Such dynamic structures will only be successful if the obligations of different providers with respect to the quality of the offered services can be unambiguously specified and enforced by means of dynamic Service Level Agreements (SLAs). In other words, the management of SLAs needs to become as dynamic as the underlying infrastructure for which they are defined. Our previous work has shown that Web Services, as a typical example for a service-oriented architecture, can be extended in a straightforward way for defining and monitoring SLAs. However, SLAs defined for a Web Services environment need to take into account the underlying managed resources whose management interfaces are defined based on traditional management architectures, such as SNMP-based management or the Common Information Model (CIM). As a solution to this problem, the approach presented in this paper addresses the integration problem of how to transform a Web Services SLA so that it can be understood and enforced by a service provider whose management system is based on a traditional management architecture, such as CIM.",
Multiversion scheduling in rechargeable energy-aware real-time systems,"In the context of battery-powered real-time systems three constraints need to be addressed: energy; deadlines; and task rewards. Many future real-time systems will count on different software versions, each with different rewards, time and energy requirements, to achieve a variety of QoS-aware tradeoffs. We propose a solution that allows the device to run the most valuable task versions while still meeting all deadlines and without depleting the energy. Assuming that the battery is rechargeable, we also propose: (a) a static solution that maximizes the system value assuming a worst-case scenario (i.e., worst-case task execution times); and (b) a dynamic scheme that takes advantage of the extra energy in the system when worst-case scenarios do not happen. Three dynamic policies are shown to make better use of the recharging energy while improving the system value.","Real time systems,
Voltage,
Energy consumption,
Frequency,
Performance loss,
Processor scheduling,
Computer science,
Battery management systems,
Energy management,
Power system management"
Extraction and tracking of the license plate using Hough transform and voted block matching,"In recent years, the method of license plate tracking has been applied for obtaining the position of the nearby vehicles from a vehicle. In this paper, we propose a new method of license plate extracting and tracking from time sequential images taken by a video camera on a vehicle. For extracting and tracking the plate correctly, we use Hough transform and Voted Block Matching. This method enables the extracting and tracking even when the distance between the camera on the vehicle and the license plate of another vehicle is changing or the plate is occluded by some objects.","Licenses,
Image edge detection,
Filters,
Image processing,
Robustness,
Lighting,
Computer science,
Smart cameras,
Intelligent vehicles,
Image sensors"
Learning benefits of structural example-based adaptive tutoring systems,"This paper illustrates and evaluates a generic adaptive tutoring environment, structural example-based adaptive tutoring system (SEATS), based on the theory of cognitive knowledge acquisition. The system teaches by presenting side-by-side examples and highlighting their common structural components. This technique assists the process of generalization and reduces mapping by surface features, allowing students to apply their newly gained knowledge to different sets of problems. SEATS also implements adaptive presentation based on a straightforward model of student/user performance. SEATS was evaluated with a recursion tutorial used by 117 students in a 1-hour tutorial session. Results indicate that using adaptation in combination with the structural example-based feature produces an effect on rate and extent of learning significantly greater than when the features are used alone, or when both are absent. The study further points out that future evaluations will have to take students' curiosity into account, since many of them are likely to give at least some incorrect answers on purpose, to test the response of the system. The results indicate that building SEATS with simple adaptive mechanisms is an efficient way of teaching electronically.","Intelligent tutoring systems,
Learning systems,
Decision support systems"
TCP performance over multipath routing in mobile ad hoc networks,"In this paper, we investigate TCP performance over a multipath routing protocol. Multipath routing can improve the path availability in mobile environment. Thus, it has a great potential to improve TCP performance in ad hoc networks under mobility. Previous research on multipath routing mostly used UDP traffic for performance evaluation. When TCP is used, we find that most times, using multiple paths simultaneously may actually degrade TCP performance. This is partly due to frequent out-of-order packet delivery via different paths. We then test another multipath routing strategy called backup path routing. Under the backup path routing scheme, TCP is able to gain improvements against mobility. We then further study related issues to backup path routing, which can affect TCP performance. Some important discoveries are reported in the paper and simulation results show that by careful selection of the multipath routing strategies, we can improve TCP performance by more than 30% even under very high mobility.","Intelligent networks,
Mobile ad hoc networks,
Routing protocols,
Degradation,
Availability,
Telecommunication traffic,
Out of order,
Switches,
Computer science,
Ad hoc networks"
Reversible logic synthesis for minimization of full-adder circuit,"Reversible logic is of the growing importance to many future technologies. A reversible circuit maps each output vector, into a unique input vector, and vice versa. This paper introduces an approach to synthesise the generalized multi-rail reversible cascades and minimizing the ""garbage bit"" and number of reversible gates, which is the main challenge of reversible logic synthesis. This proposed full-adder circuit contains only three gates and two garbage outputs whereas earlier full-adder circuit by M. Perkowski et al. (2001) requires four gates and produces two garbage outputs and another existing full-adder circuit by Md. H. H Azad Khan (2002) requires three gates but produces three garbage outputs. Thus, the proposed full-adder circuit is efficient in terms of number of gates with compared to M. Perkowski et al. (2001) as well as in terms of number of garbage outputs with compared to Md. H. H Azad Khan (2002).","Logic circuits,
Circuit synthesis,
Minimization,
Signal synthesis,
Computer science,
DH-HEMTs,
Combinational circuits,
Equations,
Paper technology,
Hardware"
A choice relation framework for supporting category-partition test case generation,"We describe in this paper a choice relation framework for supporting category-partition test case generation. We capture the constraints among various values (or ranges of values) of the parameters and environment conditions identified from the specification, known formally as choices. We express these constraints in terms of relations among choices and combinations of choices, known formally as test frames. We propose a theoretical backbone and techniques for consistency checks and automatic deductions of relations. Based on the theory, algorithms have been developed for generating test frames from the relations. These test frames can then be used as the basis for generating test cases. Our algorithms take into consideration the resource constraints specified by software testers, thus maintaining the effectiveness of the test frames (and hence test cases) generated.","Computer aided software engineering,
Software testing,
Software maintenance,
Spine,
Software algorithms,
Costs,
Humans,
Information technology,
Australia,
Computer science"
Tracking objects using density matching and shape priors,"We present a novel method for tracking objects by combining density matching with shape priors. Density matching is a tracking method which operates by maximizing the Bhattacharyya similarity measure between the photometric distribution from an estimated image region and a model photometric distribution. Such trackers can be expressed as PDE-based curve evolutions, which can be implemented using level sets. Shape priors can be combined with this level-set implementation of density matching by representing the shape priors as a series of level sets; a variational approach allows for a natural, parametrization-independent shape term to be derived. Experimental results on real image sequences are shown.",
Corner sequence - a P-admissible floorplan representation with a worst case linear-time packing scheme,"Floorplanning/placement allocates a set of modules into a chip so that no two modules overlap and some specified objective is optimized. To facilitate floorplanning/placement, we need to develop an efficient and effective representation to model the geometric relationship among modules. In this paper, we present a P-admissible representation, called corner sequence (CS), for nonslicing floorplans. CS consists of two tuples that denote the packing sequence of modules and the corners to which the modules are placed. CS is very effective and simple for implementation. Also, it supports incremental update during packing. In particular, it induces a generic worst case linear-time packing scheme that can also be applied to other representations. Experimental results show that CS achieves very promising results for a set of commonly used MCNC benchmark circuits.","Computer aided software engineering,
Very large scale integration,
Solid modeling,
Integrated circuit interconnections,
Intellectual property,
Delay effects,
Circuit optimization,
Information science,
Binary trees,
Costs"
Estimating the principal curvatures and the Darboux frame from real 3-D range data,"Principal curvatures and the local Darboux frame are natural tools to be used during processes which involve extraction of geometric properties from three-dimensional (3-D) range data. As second-order features their estimations are highly sensitive to noise and therefore, until recent years, it was almost impractical to extract reliable results from real 3-D data. Since the use of more accurate 3-D range imaging equipment has become more popular, as well as the use of polyhedral meshes to approximate surfaces, evaluation of existing algorithms for curvature estimation is again relevant. The work presented here, makes some subtle but very important modifications to two such algorithms, originally suggested by Taubin (1995) and Chen and Schmitt (1992). The algorithms have been adjusted to deal with real discrete noisy range data, given as a cloud of sampled points, lying on surfaces of free-form objects. The results of this linear time (and space) complexity implementation were evaluated in a series of tests on synthetic and real input. We also present one of many possible uses for these extracted features in an efficient and robust application for the recovery of 3-D geometric primitives from range data of complex scenes. The application combines the segmentation, classification and fitting processes in a single process which advances monotonously through the recovery procedure. It is also very robust and does not use any least-squares fittings. The conclusion of this study is that with current scanning technology and the algorithms presented here, reliable estimates of the principal curvatures and Darboux frame can be extracted from real data and used in a large variety of tasks.","Data mining,
Testing,
Layout,
Feature extraction,
Robustness,
Clouds,
Libraries,
Registers,
Computer science"
Non-stationary problem optimization using the primal-dual genetic algorithm,"Genetic algorithms (GAs) have been widely used for stationary optimization problems where the fitness landscape does not change during the computation. However, the environments of real world problems may change over time, which puts forward serious challenge to traditional GAs. In this paper, we introduce the application of a new variation of GA called the primal-dual genetic algorithm (PDGA) for problem optimization in nonstationary environments. Inspired by the complementarity and dominance mechanisms in nature, PDGA operates on a pair of chromosomes that are primal-dual to each other in the sense of maximum distance in genotype in a given distance space. This paper investigates an important aspect of PDGA, its adaptability to dynamic environments. A set of dynamic problems are generated from a set of stationary benchmark problems using a dynamic problem generating technique proposed in this paper. Experimental study over these dynamic problems suggests that PDGA can solve complex dynamic problems more efficiently than traditional GA and a peer GA, the dual genetic algorithm. The experimental results show that PDGA has strong viability and robustness in dynamic environments.","Genetic algorithms,
Biological cells,
Organisms,
Computer science,
Robustness,
Evolutionary computation,
Trajectory,
DNA,
Encoding,
Hamming distance"
Fractional cut: improved recursive bisection placement,"In this paper, we present improvements to recursive bisection based placement. In contrast to prior work, our horizontal cut lines are not restricted to row boundaries; this avoids a ""narrow region"" problem. To support these new cut line positions, a dynamic programming based legalization algorithm has been developed. The combination of these has improved the stability and lowered the wire lengths produced by our Feng Shui placement tool. On benchmarks derived from industry partitioning examples, our results are close to those of the annealing based tool Dragon, while taking only a fraction of the run time. On synthetic benchmarks, our wire lengths are nearly 23% better than those of Dragon. For both benchmark suites, our results are substantially better than those of the recursive bisection based tool Capo and the analytic placement tool Kraftwerk.",
Isosurfaces as deformable models for magnetic resonance angiography,"Vascular disease produces changes in lumenal shape evident in magnetic resonance angiography (MRA). However, quantification of vascular shape from MRA is problematic due to image artifacts. Prior deformable models for vascular surface reconstruction primarily resolve problems of initialization of the surface mesh. However, initialization can be obtained in a trivial manner for MRA using isosurfaces. We propose a methodology for deforming the isosurface to conform to the boundaries of objects in the image with minimal a priori assumptions of object shape. As in conventional methods, external forces attract the surface toward edges in the image. However, smoothing is produced by a moment that aligns the normals of adjacent surface triangles. Notably, the moment produces no translational motion of surface triangles. The deformable isosurface was applied to a digital phantom of a stenotic artery, to MRA of three renal arteries with atherosclerotic disease and MRA of one carotid artery with atherosclerotic disease. Results of the surface reconstruction from the deformable model were compared with conventional X-ray angiography for the renal arteries. Measurement of the degree of stenosis of the renal arteries was within 12% /spl plusmn/ 6%. The deformable model provided improvements over the isosurface in all cases in terms of measurement of the degree of stenosis or improving the surface smoothness.","Isosurfaces,
Deformable models,
Magnetic resonance,
Angiography,
Surface reconstruction,
Arteries,
Diseases,
Shape,
Image reconstruction,
Smoothing methods"
Clustering with qualitative information,"We consider the problem of clustering a collection of elements based on pairwise judgments of similarity and dissimilarity. N. Bansal et al. (2002) cast the problem thus: given a graph G whose edges are labeled ""+"" (similar) or ""-"" (dissimilar), partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the input labeling is maximized (resp. minimized). Complete graphs, where the classifier labels every edge, and general graphs, where some edges are not labeled, are both worth studying. We answer several questions left open by N. Bansal et al. (2002) and provide a sound overview of clustering with qualitative information. We give a factor 4 approximation for minimization on complete graphs, and a factor O(log n) approximation for general graphs. For the maximization version, a PTAS for complete graphs is shown by N. Bansal et al. (2002); we give a factor 0.7664 approximation for general graphs, noting that a PTAS is unlikely by proving APX-hardness. We also prove the APX-hardness of minimization on complete graphs.","Engineering profession,
Labeling,
Clustering algorithms,
US Department of Energy"
"Verification, validation, and certification of modeling and simulation applications","Certifying that a large-scale complex modeling and simulation (M&S) application can be used for a set of specific purposes is an onerous task, which involves complex evaluation processes throughout the entire M&S development life cycle. The evaluation processes consist of verification and validation activities, quality assurance, assessment of qualitative and quantitative elements, assessments by subject matter experts, and integration of disparate measurements and assessments. Planning, managing, and conducting the evaluation processes require a disciplined life-cycle approach and should not be performed in an ad hoc manner. The purpose of this tutorial paper is to present structured evaluation processes throughout the entire M&S development life cycle. Engineers, analysts, and managers can execute the evaluation processes presented herein to be able to formulate a certification decision for a large-scale complex M&S application.","Certification,
Accreditation,
Application software,
Large-scale systems,
Predictive models,
Computer science,
Multilevel systems,
Quality assurance,
Process planning,
Performance evaluation"
Sociophysics simulations,The article summarizes some of the more recent simulations in sociophysics. It considers hierarchical and consensus models concentrating on simple models that take about one page of Fortran.,
Visualizing fuzzy points in parallel coordinates,"Exploratory data analysis heavily relies on methods to visualize data and models in a user friendly and interpretable manner. We show how models consisting of a collection of fuzzy points can be visualized in parallel coordinates. In contrast to existing techniques that display only lines representing centroids or shaded areas showing the general variance of cluster centers, the proposed technique shows the spread of the fuzzy membership in each dimension in detail. This allows for a better interpretation of overlap in fuzzy rules.",
Enhanced DCF of IEEE 802.11e to support QoS,"We introduce the emerging IEEE 802.11e standard to support quality of service at medium access control level. One of the most important functions in 802.11e is the contention-based channel access mechanism called enhanced distributed coordination function (EDCF), which provides a priority scheme by differentiating the inter-frame space and the initial window size. We propose an analytical model to evaluate the EDCF priority scheme. Saturation throughput and saturation delay are derived analytically. Simulations are also conducted to validate analytical results. Our study shows that differentiating the initial window size is better than differentiating the inter-frame space in terms of total throughput and delay.","Quality of service,
Throughput,
Performance analysis,
Analytical models,
Media Access Protocol,
Delay,
Access protocols,
Centralized control,
Multiaccess communication,
Computer science"
ESPDA: Energy-efficient and Secure Pattern-based Data Aggregation for wireless sensor networks,"Secure data transmission and data aggregation are critical in designing cluster-based sensor networks. This paper presents an Energy-efficient and Secure Pattern-based Data Aggregation protocol (ESPDA) for wireless sensor networks. ESPDA is energy and bandwidth efficient because cluster-heads prevent the transmission of redundant data from sensor nodes. ESPDA is also secure because it does not require the encrypted data to be decrypted by cluster-heads to perform data aggregation. In ESPDA, cluster-head first requests sensor nodes to send the corresponding pattern code for the sensed data. If multiple sensor nodes send the same pattern code to the cluster-head, then only one of them is permitted to send the data to the cluster-head. Hence, ESPDA has advantages over the conventional data aggregation techniques with respect to energy, bandwidth efficiency and security. Simulations results show that as data redundancy increases, the amount of data transmitted from sensor nodes to cluster-head decreases up to 45% when compared to conventional algorithms.","Energy efficiency,
Wireless sensor networks,
Data communication,
Data security,
Base stations,
Wireless application protocol,
Clustering algorithms,
Bandwidth,
Cryptography,
Computer science"
Weighted fairness in buffered crossbar scheduling,"The crossbar is the most popular packet switch architecture. By adding small buffers at the crosspoints, important advantages can be obtained: (1) crossbar scheduling is simplified; (2) high throughput is achievable; (3) weighted scheduling becomes feasible. We study the fairness properties of a buffered crossbar with weighted fair schedulers. We show by means of simulation that, under heavy demand, the system allocates throughput in a weighted max-min fair manner. We study the impact of the size of the crosspoint buffers in approximating the weighted max-min fair rates and we find that a small amount of buffering per crosspoint (3-8 cells) suffices for the maximum percentage discrepancy to fall below 5% for 32/spl times/32 switches.","Switches,
Throughput,
Quality of service,
Processor scheduling,
Costs,
Aggregates,
Computer science,
Buffer storage,
Packet switching,
Computer architecture"
Multistability Analysis for Recurrent Neural Networks with Unsaturating Piecewise Linear Transfer Functions,"Multistability is a property necessary in neural networks in order to enable certain applications (e.g., decision making), where monostable networks can be computationally restrictive. This article focuses on the analysis of multistability for a class of recurrent neural networks with unsaturating piecewise linear transfer functions. It deals fully with the three basic properties of a multistable network: boundedness, global attractivity, and complete convergence. This article makes the following contributions: conditions based on local inhibition are derived that guarantee boundedness of some multistable networks, conditions are established for global attractivity, bounds on global attractive sets are obtained, complete convergence conditions for the network are developed using novel energy-like functions, and simulation examples are employed to illustrate the theory thus developed.",
Seven principles of efficient human robot interaction,"Advances in robot technology and artificial intelligence have increased the range of robot applications as well as the importance of supporting human interaction with robots and robot teams. Previous work by the authors has highlighted the importance of creating neglect tolerant autonomy and efficient interfaces. In this paper, lessons learned from evaluating neglect tolerance and interface efficiency are compiled into a set of principles for efficient interaction. Emphasis is placed on designing efficient interfaces, but many of the principles require autonomy levels that support the principles. Each principle is illustrated by an example and motivated by citing relevant factors from cognitive information processing.",
An experience developing an IDS stimulator for the black-box testing of network intrusion detection systems,"Signature-based intrusion detection systems use a set of attack descriptions to analyze event streams, looking for evidence of malicious behavior. If the signatures are expressed in a well-defined language, it is possible to analyze the attack signatures and automatically generate events or series of events that conform to the attack descriptions. This approach has been used in tools whose goal is to force intrusion detection systems to generate a large number of detection alerts. The resulting ""alert storm"" is used to desensitize intrusion detection system administrators and hide attacks in the event stream. We apply a similar technique to perform testing of intrusion detection systems. Signatures from one intrusion detection system are used as input to an event stream generator that produces randomized synthetic events that match the input signatures. The resulting event stream is then fed to a number of different intrusion detection systems and the results are analyzed. This paper presents the general testing approach and describes the first prototype of a tool, called Mucus, that automatically generates network traffic using the signatures of the Snort network-based intrusion detection system. The paper describes preliminary cross-testing experiments with both an open-source and a commercial tool and reports the results. An evasion attack that was discovered as a result of analyzing the test results is also presented.","Intrusion detection,
System testing,
Application software,
Telecommunication traffic,
Performance analysis,
Computer network reliability,
Computer science,
Performance evaluation,
Impedance matching,
Automatic testing"
An optimization model for placement of wavelength converters to minimize blocking probability in WDM networks,"The introduction of wavelength converters in wavelength division multiplexing (WDM) networks can reduce the blocking probabilities of calls. In this paper, we study the problem of placing a given number of converters in a general topology WDM network such that the overall system blocking probability is minimized. The original contributions of this work are the following: 1) formulation of success probability in a network as a polynomial function of the locations of converters; 2) proposal of an optimization model of the converter placement problem as the minimization of a polynomial function of 0-1 variables under a linear constraint, so that standard optimization tools can be employed to solve the problem; and 3) design of a search algorithm that can efficiently find the optimal solution to the converter placement problem. Experiments have been conducted to demonstrate the effectiveness of the proposed model and the efficiency of the algorithm.","Intelligent networks,
WDM networks,
Optical wavelength conversion,
Wavelength converters,
Wavelength division multiplexing,
Polynomials,
Computer science,
Constraint optimization,
Design optimization,
Wavelength routing"
Probabilistic worst-case response-time analysis for the controller area network,"This paper presents a novel approach for calculating a probabilistic worst-case response-time for messages in the Controller Area Network (CAN). CAN uses a bit-stuffing mechanism to exclude forbidden bit-patterns within a message frame. The added bits eliminate the forbidden patterns but cause an increase in frame length. How much the length is increased depends on the bit-pattern of the original message frame. Traditional response-time analysis methods assume that all frames have a worst-case number of stuff-bits. This introduces pessimism in the analysis. In this paper we introduce an analysis approach based on using probability distributions to model the number of stuff-bits. The new analysis additionally opens tip for making trade-offs between reliability and timeliness, in the sense that the analysis will provide a certain probability for missing deadlines, which in the reliability analysis can be treated as a probability of failure. We evaluate the performance of our method using a subset of the SAE/sup 1/ benchmark.","Job shop scheduling,
Failure analysis,
Telecommunication network reliability,
Cyclic redundancy check,
Protocols,
Real time systems,
Hardware,
Computer science,
Probability distribution,
Distribution functions"
Multiview paraperspective projection model for diminished reality,"This paper introduces a ""diminished reality"" technique for removing an object or collection of objects and replacing it with an appropriate background image. Diminished reality can be considered an important part of many mixed and augmented reality applications. Our target application is the use of augmented reality (AR) to revamp procedures in industrial plants. An object or a region of interest is delineated on a single reference image. A paraperspective projection model is used to generate the correct background from multiple calibrated views of the scene. We propose methods to deal with approximately planar backgrounds with different orientations. We also propose a multi-resolution approach to deal with non-planar backgrounds. Different sets of experimental results demonstrate the success and limits of the algorithms. Results on real data from water treatment and power plants show the usefulness of this method for industrial applications.","Rendering (computer graphics),
Augmented reality,
Layout,
Power generation,
Computer science,
Pipelines,
Image reconstruction,
Industrial plants,
Medical services,
Biomedical equipment"
Rapid registration for wide field of view freehand three-dimensional ultrasound,"A freehand scanning protocol is the only way to acquire arbitrary large volumes of three-dimensional ultrasound (US) data. For some applications, multiple freehand sweeps are required to cover the area of interest. Aligning these multiple sweeps is difficult, typically requiring nonrigid image-based registration as well as the readings from the spatial locator attached to the US probe. Conventionally, nonrigid warps are achieved through general elastic spline deformations, which are expensive to compute and difficult to constrain. This paper presents an alternative registration technique, where the warp's degrees of freedom are carefully linked to the mechanics of the freehand scanning process. The technique is assessed through an extensive series of in vivo experiments, which reveal a registration precision of a few pixels with comparatively little computational load.",
A unified scheme for resource protection in automated trust negotiation,"Automated trust negotiation is an approach to establishing trust between strangers through iterative disclosure of digital credentials. In automated trust negotiation, access control policies play a key role in protecting resources from unauthorized access. Unlike in traditional trust management systems, the access control policy for a resource is usually unknown to the party requesting access to the resource, when trust negotiation starts. The negotiating parties can rely on policy disclosures to learn each other's access control requirements. However a policy itself may also contain sensitive information. Disclosing policies' contents unconditionally may leak valuable business information or jeopardize individuals' privacy. In this paper we propose UniPro, a unified scheme to model protection of resources, including policies, in trust negotiation. UniPro improves on previous work by modeling policies as first-class resources, protecting them in the same way as other resources, providing fine-grained control over policy disclosure, and clearly distinguishing between policy disclosure and policy satisfaction, which gives users more flexibility in expressing their authorization requirements. We also show that UniPro can be used with practical negotiation strategies without jeopardizing autonomy in the choice of strategy, and present criteria under which negotiations using UniPro are guaranteed to succeed in establishing trust.","Protection,
Access control,
Computer science,
Information security,
Web and internet services,
Iterative methods,
Resource management,
Authorization,
Computer security,
Open systems"
U-measure: a quality measure for multiobjective programming,"A multiobjective programming algorithm may find multiple nondominated solutions. If these solutions are scattered more uniformly over the Pareto frontier in the objective space, they are more different choices and so their quality is better. In this paper, we propose a quality measure called U-measure to measure the uniformity of a given set of nondominated solutions over the Pareto frontier. This frontier is a nonlinear hyper-surface. We measure the uniformity over this hyper-surface in three main steps: 1) determine the domains of the Pareto frontier over which uniformity is measured, 2) determine the nearest neighbors of each solution in the objective space, and 3) compute the discrepancy among the distances between nearest neighbors. The U-measure is equal to this discrepancy where a smaller discrepancy indicates a better uniformity. We can apply the U-measure to complement the other quality measures so that we can evaluate and compare multiobjective programming algorithms from different perspectives.","Functional programming,
Time measurement,
Nearest neighbor searches,
Scattering,
Computer science,
Mathematics,
Cost accounting,
Humans"
Wireless LAN based indoor positioning system WiPS and its simulation,"The wireless LAN (Wi-Fi) infrastructure is widely used and many location-aware systems and services are researched. In this paper, wireless LAN based indoor positioning system (WiPS) is proposed. WiPS uses wireless LAN technology to measure the location of each mobile terminal. Mobile terminals equip only wireless LAN device to communicate and measure its location, so it can be made without any additional devices for location sensing. Existing wireless LAN based location systems measure the signal strength by only access points. On WiPS, each mobile terminal also measures the signal strength of neighboring terminals. Thereby WiPS can achieve more precise location estimation than existing systems, where there are many mobile terminals. We simulate the case that distance measurement has probabilistic error. The result shows improvement of accuracy in WiPS.","Wireless LAN,
Global Positioning System,
Costs,
Information science,
Large scale integration,
Wireless sensor networks,
Computational modeling,
Ubiquitous computing,
Sensor systems,
Density measurement"
Efficient structured data access in parallel file systems,"Parallel scientific applications store and retrieve very large, structured datasets. Directly supporting these structured accesses is an important step in providing high-performance I/O solutions for these applications. High-level interfaces such as HDF5 and Parallel netCDF provide convenient APIs for accessing structured datasets, and the MPI-IO interface also supports efficient access to structured data. However, parallel file systems do not traditionally support such access. In this work we present an implementation of structured data access support in the context of the parallel virtual file system (PVFS). We call this support ""datatype I/O"" because of its similarity to MPI datatypes. This support is built by using a reusable datatype-processing component from the MPICH2 MPI implementation. We describe how this component is leveraged to efficiently process structured data representations resulting from MPI-IO operations. We quantitatively assess the solution using three test applications. We also point to further optimizations in the processing path that could be leveraged for even more efficient operation.","Data structures,
Database systems,
Message passing,
Network operating systems,
Distributed database management systems"
Optimal binary communication with nonequal probabilities,"Optimal signal energies are derived for optimal binary digital communication systems with arbitrary signal probabilities and correlation with both coherent and noncoherent detection. The resulting bit-error probability (BEP) is computed and compared with the BEP of the same systems with equal signal energies. One of the conclusions is that for the coherent system with nonnegative correlation, and for the noncoherent system with arbitrary correlation, the optimal signals are on-off keying (OOK), i.e., the signal with probability p/spl les/0.5 has energy E/p, while the second signal has zero energy, where E is the average signal energy. The proposed system is also better than a system with source coding and equiprobable signals.","Digital communication,
Source coding,
Gaussian noise,
Signal design,
Communications Society,
Australia,
Computer science,
Image coding"
Model-based calibration for sensor networks,"Calibration is the process of mapping raw sensor readings into corrected values by identifying and correcting systematic bias. Calibration is important from both off-line and on-line perspectives. Major objectives of calibration procedure include accuracy, resiliency against random errors, ability to be applied in various scenarios, and to address a variety of error models. In addition, a compact mapping function is attractive in terms of both storage and robustness. We start by introducing the nonparametric statistical approach for conducting off-line calibration. After that, we present the non-parametric statistical percentile method for establishing the confidence interval for a particular mapping function. Furthermore, we propose the first model-based on-line procedure for calibration. The calibration problem is formulated as an instance of nonlinear function minimization and solved using the standard conjugate gradient approach. A number of trade-offs between the effectiveness of calibration and noise level, latency, size of network and the complexity of phenomena are analyzed in a quantitative way. As a demonstration example, we use a system consisting of photovoltaic optical sensors.","Calibration,
Sensor systems,
Optical sensors,
Sensor phenomena and characterization,
Wireless sensor networks,
Delay,
Computer science,
Computer errors,
Robustness,
Noise level"
A new predictive search area approach for fast block motion estimation,"According to the observation on the distribution of motion differentials among the motion vector of any block and those of its four neighboring blocks from six real video sequences, this paper presents a new predictive search area approach for fast block motion estimation. Employing our proposed simple predictive search area approach into the full search (FS) algorithm, our improved FS algorithm leads to 93.83% average execution-time improvement ratio, but only has a small estimation accuracy degradation. We also investigate the advantages of computation and estimation accuracy of our improved FS algorithm when compared to the edge-based search algorithm of Chan and Siu (see IEEE Trans. Image Processing, vol.10, p.1223-1238, Aug. 2001); experimental results reveal that our improved FS algorithm has 74.33% average execution-time improvement ratio and has a higher estimation accuracy. Finally, we further compare the performance among our improved FS algorithm, the three-step search algorithm, and the block-based gradient descent search algorithm.","Motion estimation,
Degradation,
Video sequences,
Video coding,
Councils,
Computer science,
Information management,
Pixel,
Statistical distributions"
Building pair programming knowledge through a family of experiments,"Pair programming is a practice in which two programmers work collaboratively at one computer on the same design, algorithm, code, or test. Pair programming is becoming increasingly popular in industry and in university curricula. A family of experiments was run with over 1200 students at two US universities, North Carolina State University and the University of California Santa Cruz, to assess the efficacy of pair programming as an alternative learning technique in introductory programming courses. Students who used the pair programming technique were at least as likely to complete the introductory course with a grade of C or better when compared with students who used the solo programming technique. Paired students earned exam and project scores equal to or better than solo students. Paired students had a positive attitude toward collaboration and were significantly more likely to be registered as computer science-related majors one year later. Our findings also suggest that students in paired classes continue to be successful in subsequent programming classes continue to be successful in subsequent programming classes that require solo programming.","Programming profession,
Computer science,
Collaborative work,
Algorithm design and analysis,
Testing,
Software engineering,
Psychology,
Navigation,
Watches,
Costs"
Three-dimensional encoding/two-dimensional decoding of medical data,"We propose a fully three-dimensional (3-D) wavelet-based coding system featuring 3-D encoding/two-dimensional (2-D) decoding functionalities. A fully 3-D transform is combined with context adaptive arithmetic coding; 2-D decoding is enabled by encoding every 2-D subband image independently. The system allows a finely graded up to lossless quality scalability on any 2-D image of the dataset. Fast access to 2-D images is obtained by decoding only the corresponding information thus avoiding the reconstruction of the entire volume. The performance has been evaluated on a set of volumetric data and compared to that provided by other 3-D as well as 2-D coding systems. Results show a substantial improvement in coding efficiency (up to 33%) on volumes featuring good correlation properties along the z axis. Even though we did not address the complexity issue, we expect a decoding time of the order of one second/image after optimization. In summary, the proposed 3-D/2-D multidimensional layered zero coding system provides the improvement in compression efficiency attainable with 3-D systems without sacrificing the effectiveness in accessing the single images characteristic of 2-D ones.","Encoding,
Decoding,
Image coding,
Biomedical imaging,
Signal processing algorithms,
Two dimensional displays,
Computed tomography,
Positron emission tomography,
Partitioning algorithms,
Picture archiving and communication systems"
Identifying communities of practice through ontology network analysis,"This article describes Ontocopi, a tool for identifying communities of practice by analyzing ontologies of relevant working domains. Ontocopi spots patterns in ontological formal relations, traversing the ontology from instance to instance via selected relations.","Ontologies,
Computer displays,
Memory management,
Best practices,
Knowledge management,
Qualifications,
Social network services,
Algorithm design and analysis,
Plugs,
Computer science"
Content based file type detection algorithms,"Identifying the true type of a computer file can be a difficult problem. Previous methods of file type recognition include fixed file extensions, fixed ""magic numbers"" stored with the files, and proprietary descriptive file wrappers. All of these methods have significant limitations. This paper proposes algorithms for automatically generating ""fingerprints"" of file types based on a set of known input files, then using the fingerprints to recognize the true type of unknown files based on their content, rather than metadata associated with them. Recognition is performed by three different algorithms based on: byte frequency analysis, byte frequency cross-correlation analysis, and file header/trailer analysis. Tests were run to measure the accuracy of these algorithms. The accuracy varied from 23% to 96% depending upon which algorithm was used. These algorithms could be used by virus scanning packages, firewalls, intrusion detection systems, forensic analyses of computer hard drives, Web browsers, or any other program that needs to identify the types of files for proper operation. File type detection is also important to the operating systems for correct identification and handling of files regardless of file extension.","Detection algorithms,
Algorithm design and analysis,
Fingerprint recognition,
Performance analysis,
Frequency,
Testing,
Packaging,
Intrusion detection,
Forensics,
Drives"
Highlight sound effects detection in audio stream,"This paper addresses the problem of highlight sound effects detection in audio stream, which is very useful in fields of video summarization and highlight extraction. Unlike researches on audio segmentation and classification, in this domain, it just locates those highlight sound effects in audio stream. An extensible framework is proposed and in current system three sound effects are considered: laughter, applause and cheer, which are tied up with highlight events in entertainments, sports, meetings and home videos. HMMs are used to model these sound effects and a log-likelihood scores based method is used to make final decision. A sound effect attention model is also proposed to extend general audio attention model for highlight extraction and video summarization. Evaluations on a 2-hours audio database showed very encouraging results.","Streaming media,
Hidden Markov models,
TV,
Asia,
Frequency estimation,
Computer science,
Audio databases,
Speech,
Layout,
Cepstral analysis"
Noncontiguous I/O accesses through MPI-IO,"I/O performance remains a weakness of parallel computing systems today. While this weakness is partly attributed to rapid advances in other system components, I/O interfaces available to programmers and the I/O methods supported by file systems have traditionally not matched efficiently with the types of I/O operations that scientific applications perform, particularly noncontiguous accesses. The MPI-IO interface allows for rich descriptions of the I/O patterns desired for scientific applications and implementations such as ROMIO have taken advantage of this ability while remaining limited by underlying file system methods. A method of noncontiguous data access, list I/O, was recently implemented in the Parallel Virtual File System (PVFS). We implement support for this interface in the ROMIO MPI-IO implementation. Through a suite of noncontiguous I/O tests we compared ROMIO list I/O to current methods of ROMIO noncontiguous access and found that the list I/O interface provides performance benefits in many noncontiguous cases.","File systems,
Testing,
Distributed computing,
Mathematics,
Computer science,
Laboratories,
Parallel processing,
Programming profession,
Tiles,
Checkpointing"
The virtual data grid: a new model and architecture for data-intensive collaboration,"It is increasingly common to encounter communities engaged in the collaborative analysis and transformation of large quantities of data over extended periods of time. I argue that these communities require a scalable system for managing, tracing, exploring and communicating the derivation and analysis of diverse data objects. Such a system could bring significant productivity increases facilitating discovery, understanding, assessment, and sharing of both data and transformation resources for computation, storage, and collaboration. I define a model and architecture for a virtual data grid capable of addressing these requirements. I define a broadly applicable model of a ""typed dataset"" as the unit of derivation tracking, and simple constructs for describing how datasets are derived from transformations and from other datasets. I also define mechanisms for integrating with, and adapting to, existing data management systems and transformation and analysis tools, as well as grid mechanisms for distributed resource management and computation planning. Finally, I report on successful application results obtained with a prototype implementation called Chimera, involving challenging analysis of high-energy physics and astronomy data.","Collaboration,
Resource management,
Data analysis,
Productivity,
Computer architecture,
Distributed computing,
Grid computing,
Prototypes,
Physics,
Astronomy"
Robust data clustering,"We address the problem of robust clustering by combining data partitions (forming a clustering ensemble) produced by multiple clusterings. We formulate robust clustering under an information-theoretical framework; mutual information is the underlying concept used in the definition of quantitative measures of agreement or consistency between data partitions. Robustness is assessed by variance of the cluster membership, based on bootstrapping. We propose and analyze a voting mechanism on pairwise associations of patterns for combining data partitions. We show that the proposed technique attempts to optimize the mutual information based criteria, although the optimality is not ensured in all situations. This evidence accumulation method is demonstrated by combining the well-known K-means algorithm to produce clustering ensembles. Experimental results show the ability of the technique to identify clusters with arbitrary shapes and sizes.","Robustness,
Mutual information,
Clustering algorithms,
Partitioning algorithms,
Shape,
Computer science,
Data engineering,
Pattern analysis,
Voting,
Analysis of variance"
A road sign recognition system based on dynamic visual model,"We propose a computational model motivated by human cognitive processes for detecting changes of driving environments. The model, called dynamic visual model, consists of three major components: sensory, perceptual, and conceptual components. The proposed model is used as the underlying framework in which a system for detecting and recognizing road signs is developed.","Humans,
Visual system,
Video sequences,
Computer science,
Computational modeling,
Vehicle dynamics,
Traffic control,
Road vehicles,
Data mining,
Computer science education"
Data caches in multitasking hard real-time systems,"Data caches are essential in modern processors, bridging the widening gap between main memory and processor speeds. However, they yield very complex performance models, which make it hard to bound execution times tightly. This paper contributes a new technique to obtain predictability in preemptive multitasking systems in the presence of data caches. We explore the use of cache partitioning, dynamic cache locking, and static cache analysis to provide worst-case performance estimates in a safe and tight way. Cache partitioning divides the cache among tasks to eliminate inter-task cache interferences. We combine static cache analysis and cache locking mechanisms to ensure that all intra-task conflicts, and consequently, memory access times, are exactly predictable. To minimize the performance degradation due to cache partitioning and locking, two strategies are employed. First, the cache is loaded with data likely to be accessed so that their cache utilization is maximized. Second, compiler optimizations such as tiling and padding are applied in order to reduce cache replacement misses. Experimental results show that this scheme is fully predictable, without compromising the performance of the transformed programs. Our method outperforms static cache locking for all analyzed task sets under various cache architectures, with a CPU utilization reduction ranging between 3.8 and 20.0 times for a high performance system.","Multitasking,
Real time systems,
Performance analysis,
Costs,
Computer science,
Data engineering,
Australia,
Interference elimination,
Degradation,
Optimizing compilers"
A variable strength interaction testing of components,"Complete interaction testing of components is too costly in all but the smallest systems. Yet component interactions are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing to guarantee a minimum coverage of all t-way interactions across components. However, t is always fixed. This paper examines the need to vary the size of t in an individual test suite and defines a new object, the variable strength covering array that has this property. We present some computational methods to find variable strength arrays and provide initial bounds for a group of these objects.",
Optical investigations of dynamic vacuum arc mode changes with different axial magnetic field contacts,"By using a high-speed charge-coupled device (CCD) video technique, three different axial magnetic field contact systems (i.e., unipolar, bipolar, and quadrupolar systems) are investigated at an arc current of 10 kA. Video recordings were compared to computer simulations of light emission emitted at the side-on of diffuse and diffuse columnar arcs. The computer images reproduced typical trends, such as stronger light intensities in front of the cathode caused by higher-plasma densities in this region. A low-current dc vacuum arc was initiated by contact separation before the high current was injected at a fixed contact distance of 10 mm. Videos were taken from two directions perpendicular to each other to localize the vacuum arc properly. From these investigations, the transient development of vacuum arc under different axial magnetic field profiles can be visualized. The results were interpreted with respect to the behavior of the vacuum arc in the second half cycle after an eventual reignition.",
An algorithm for fast adaptive image binarization with applications in radiotherapy imaging,"Locally adaptive image binarization with a sliding-window threshold can be an effective tool for various image processing tasks. We have used the method for the detection of bone ridges in radiotherapy portal images. However, a straight-forward implementation of sliding-window processing is too time consuming for routine use. Therefore, we have developed a new thresholding criterion suitable for incremental update within the sliding window, and we show that our algorithm gives better results on difficult portal images than various publicly available adaptive thresholding routines. For small windows, the routine is also faster than an adaptive implementation of the Otsu algorithm that uses interpolation between fixed tiles, and the resulting images are equally good.",
Cone-beam reprojection using projection-matrices,"This paper addresses reprojection of three-dimensional (3-D) reconstructions obtained from cone-beam scans using a C-arm imaging equipment assisted by a pose-determining system. The emphasis is on reprojecting without decomposing the estimated projection matrix (P-matrix) associated with a pose. Both voxel- and ray-driven methods are considered. The voxel-driven reprojector follows the algorithm for backprojection using a P-matrix. The ray-driven reprojector is derived by extracting from the P-matrix the equation of the line joining a detector-pixel and the X-ray source position. This reprojector can be modified to a ray-driven backprojector. When the geometry is specified explicitly in terms of the physical parameters of the imaging system, the projection matrices can be constructed. The resulting ""projection-matrix method"" is advantageous, especially when the scanning trajectory is irregular. The algorithms presented are useful in iterative methods of image reconstruction and enhancement procedures, apart from their well-known role in visualization and volume rendering. Reprojections of 3-D patient data compare favorably with the original X-ray projections obtained from a prototype C-arm system. The algorithms for reprojection can be modified to compute perspective maximum intensity projection.","Image reconstruction,
X-ray imaging,
Iterative algorithms,
Matrix decomposition,
Equations,
X-ray detection,
X-ray detectors,
Geometry,
Optical imaging,
Iterative methods"
"Paths, trees, and minimum latency tours","We give improved approximation algorithms for a variety of latency minimization problems. In particular, we give a 3.59-approximation to the minimum latency problem, improving on previous algorithms by a multiplicative factor of 2. Our techniques also give similar improvements for related problems like k-traveling repairmen and its multiple depot variant. We also observe that standard techniques can be used to speed up the previous and this algorithm by a factor of O/sup /spl tilde//(n).","Delay,
Approximation algorithms,
Computer science,
Cost function,
Minimization methods,
Space exploration,
Tree graphs,
Educational institutions,
Equations"
Computational aspects of diffuse optical tomography,"Diffuse optical tomography (DOT) is a novel functional imaging modality for visualizing and continuously monitoring tissue and blood oxygenation levels, which is useful for brain imaging and tumor detection. Because of the nonlinearity of infrared light propagation in tissue, developing fast and robust reconstruction methods is the main challenge in making DOT a viable tool for clinical diagnostics.",
Learning object intrinsic structure for robust visual tracking,"In this paper, a novel method to learn the intrinsic object structure for robust visual tracking is proposed. The basic assumption is that the parameterized object state lies on a low dimensional manifold and can be learned from training data. Based on this assumption, firstly we derived the dimensionality reduction and density estimation algorithm for unsupervised learning of object intrinsic representation, the obtained non-rigid part of object state reduces even to 2 dimensions. Secondly the dynamical model is derived and trained based on this intrinsic representation. Thirdly the learned intrinsic object structure is integrated into a particle-filter style tracker. We will show that this intrinsic object representation has some interesting properties and based on which the newly derived dynamical model makes particle-filter style tracker more robust and reliable. Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as fish twisting with self-occlusion and large inter-frame lip motion. The proposed method also has the potential to solve other type of tracking problems.",
Proxy-assisted techniques for delivering continuous multimedia streams,"We present a proxy-assisted video delivery architecture that can simultaneously reduce the resources requirements at the central server and the service latency experienced by clients (i.e., end users). Under the proposed video delivery architecture, we develop and analyze two novel proxy-assisted video streaming techniques for on-demand delivery of video objects to a large number of clients. By taking advantage of the resources available at the proxy servers, these techniques not only significantly reduce the central server and network resource requirements, but are also capable of providing near-instantaneous service to a large number of clients. We optimize the performance of our video streaming architecture by carefully selecting video delivery techniques for videos of various popularity and intelligently allocating resources between proxy servers and the central server. Through empirical studies, we demonstrate the efficacy of the proposed proxy-assisted video streaming techniques.",
Towards a characterization of truthful combinatorial auctions,"This paper analyzes incentive compatible (truthful) mechanisms over restricted domains of preferences, the leading example being combinatorial auctions. Our work generalizes the characterization of Roberts (1979) who showed that truthful mechanisms over unrestricted domains with at least 3 possible outcomes must be ""affine maximizers"". We show that truthful mechanisms for combinatorial auctions (and related restricted domains) must be ""almost affine maximizers"" if they also satisfy an additional requirement of ""independence of irrelevant alternatives"". This requirement is without loss of generality for unrestricted domains as well as for auctions between two players where all goods must be allocated. This implies unconditional results for these cases, including a new proof of Roberts' theorem. The computational implications of this characterization are severe, as reasonable ""almost affine maximizers"" are shown to be as computationally hard as exact optimization. This implies the near-helplessness of such truthful polynomial-time auctions in all cases where exact optimization is computationally intractable.",
Polyphonic audio matching and alignment for music retrieval,"We describe a method that aligns polyphonic audio recordings of music to symbolic score information in standard MIDI files without the difficult process of polyphonic transcription. By using this method, we can search through a MIDI database to find the MIDI file corresponding to a polyphonic audio recording.","Multiple signal classification,
Music information retrieval,
Audio recording,
Audio databases,
Computer science,
Digital audio players,
Timing,
Spatial databases,
Error analysis,
Heuristic algorithms"
Distributed dominant pruning in ad hoc networks,"Efficient routing among mobile hosts is an important function in ad hoc networks. Routing based on a connected dominating set is a promising approach, where the search space for a route is reduced to the hosts in the set. A set is dominating if all the hosts are either in the set or neighbors of hosts in the set. The efficiency of dominating-set-based routing mainly depends on the overhead introduced in the formation of the dominating set and the size of the dominating set. In this paper, we first review a distributed formation of a connected dominating set called marking process and dominating-set-based routing. Then a generalization of two existing rules (called rules 1 and 2). We prove that the vertex set derived by applying rule k is still a connected dominating set. When implemented with local neighborhood information. Rule k is more effective in reducing the dominating set derived from the marking process than the combination of rules 1 and 2, and has the same communication complexity and less computation complexity. Simulation results confirm that rule k outperforms rules 1 and 2, especially in relatively dense networks with unidirectional links.",
Information hiding using steganography,"Due to advances in ICT, most information is kept electronically. Consequently, the security of information has become a fundamental issue. Besides cryptography, steganography can be employed to secure information. Steganography is a technique of hiding information in digital media. In contrast to cryptography, the message or encrypted message is embedded in a digital host before passing it through the network, thus the existence of the message is unknown. Besides hiding data for confidentiality, this approach of information hiding can be extended to copyright protection for digital media: audio, video, and images.","Steganography,
Cryptography,
Information security,
Copyright protection,
Data security,
Publishing,
Watermarking,
Fingerprint recognition,
Writing,
Computer science"
A framework for fuzzy quantification models analysis,"A framework for description of fuzzy quantification models is presented. Within this framework, the fuzzy quantified statements evaluation problem is described as the compatibility between the fuzzy quantifier and a fuzzy cardinality or a fuzzy aggregation measure. A list of desirable properties for quantification models is presented and those models that fit the framework are confronted with it.",
Processor Power Reduction Via Single-ISA Heterogeneous Multi-Core Architectures,"This paper proposes a single-ISA heterogeneousmulti-core architecture as a mechanism to reduce processorpower dissipation. It assumes a single chip containing a diverseset of cores that target different performance levels and consumedifferent levels of power. During an application’s execution,system software dynamically chooses the most appropriate core tomeet specific performance and power requirements. It describesan example architecture with five cores of varying performanceand complexity. Initial results demonstrate a five-fold reductionin energy at a cost of only 25% performance.","Computer architecture,
Application software,
Energy consumption,
Power dissipation,
System software,
Costs,
Computer science,
Power engineering and energy,
Packaging,
Fans"
Inversion for refractivity parameters from radar sea clutter,"This paper describes estimation of low-altitude atmospheric refractivity from radar sea clutter observations. The vertical structure of the refractive environment is modeled using five parameters, and the horizontal structure is modeled using six parameters. The refractivity model is implemented with and without an a priori constraint on the duct strength, as might be derived from soundings or numerical weather-prediction models. An electromagnetic propagation model maps the refractivity structure into a replica field. Replica fields are compared to the observed clutter using a squared-error objective function. A global search for the 11 environmental parameters is performed using genetic algorithms. The inversion algorithm is implemented on S-band radar sea-clutter data from Wallops Island, Virginia. Reference data are from range-dependent refractivity profiles obtained with a helicopter. The inversion is assessed (1) by comparing the propagation predicted from the radar-inferred refractivity profiles and from the helicopter profiles, (2) by comparing the refractivity parameters from the helicopter soundings to those estimated, and (3) by examining the fit between observed clutter and optimal replica field. This technique could provide near-real-time estimation of ducting effects. In practical implementations it is unlikely that range-dependent soundings would be available. A single sounding is used for evaluating the radar-inferred environmental parameters. When the unconstrained environmental model is used, the “refractivity-from-clutter,” the propagation loss generated and the loss from this single sounding, is close within the duct; however, above the duct they differ. Use of the constraint on the duct strength leads to a better match also above the duct.",
Learning Bayesian network classifiers for facial expression recognition both labeled and unlabeled data,"Understanding human emotions is one of the necessary skills for the computer to interact intelligently with human users. The most expressive way humans display emotions is through facial expressions. In this paper, we report on several advances we have made in building a system for classification of facial expressions from continuous video input. We use Bayesian network classifiers for classifying expressions from video. One of the motivating factor in using the Bayesian network classifiers is their ability to handle missing data, both during inference and training. In particular, we are interested in the problem of learning with both labeled and unlabeled data. We show that when using unlabeled data to learn classifiers, using correct modeling assumptions is critical for achieving improved classification performance. Motivated by this, we introduce a classification driven stochastic structure search algorithm for learning the structure of Bayesian network classifiers. We show that with moderate size labeled training sets and large amount of unlabeled data, our method can utilize unlabeled data to improve classification performance. We also provide results using the Naive Bayes (NB) and the Tree-Augmented Naive Bayes (TAN) classifiers, showing that the two can achieve good performance with labeled training sets, but perform poorly when unlabeled data are added to the training set.",
Multi-output Galois Field Sum of Products synthesis with new quantum cascades,"Galois Field Sum of Products (GFSOP) leads to efficient multi-valued reversible circuit synthesis using quantum gates. In this paper, we propose a new generalization of ternary Toffoli gate and another new generalized reversible ternary gale with discussion of their quantum realizations. Algorithms for synthesizing ternary GFSOP using quantum cascades of these gates are proposed In both the synthesis methods, 5 ternary shift operators and ternary swap gate are used We also propose quantum realizations of 5 ternary shift operators and ternary swap gate. In the cascades of the new ternary gates, local mirrors, variable ordering, and product ordering techniques are used to reduce the circuit cost. Experimental results show that the cascade of the new ternary gates is more efficient than the cascade of ternary Toffoli gates.","Galois fields,
Circuit synthesis,
Multivalued logic,
Quantum computing,
Computer science,
DH-HEMTs,
Mirrors,
Costs,
Rain,
Error correction codes"
Self-aligned 1.14-GHz vibrating radial-mode disk resonators,"A new fabrication methodology that allows self-alignment of a micromechanical structure to its anchor(s) has been utilized to achieve vibrating radial-contour mode micromechanical disk resonators with record resonance frequencies up to 1.14 GHz and measured Q's at this frequency >1,500 in both vacuum and air. In addition, 733-MHz versions have been demonstrated with Q's of 7,330 and 6,100 in vacuum and air, respectively. For these resonators, self-alignment of the stem to exactly the center of the disk it supports allows balancing of the resonator far superior to that achieved by previous versions (where separate masks were used to define the disk and stem), allowing the present devices to retain high Q while achieving frequencies in the GHz range for the first time.","Electrodes,
Resonance,
Resonant frequency,
Frequency measurement,
Micromechanical devices,
Vibration measurement,
Computer science,
Fabrication,
Electric variables measurement,
System-on-a-chip"
Comparing PSO structures to learn the game of checkers from zero knowledge,This paper investigates the effectiveness of various particle swarm optimiser structures to learn how to play the game of checkers. Co-evolutionary techniques are used to train the game playing agents. Performance is compared against a player making moves at random. Initial experimental results indicate definite advantages in using certain information sharing structures and swarm size configurations to successfully learn the game of checkers.,
Manifold of facial expression,"We propose the concept of manifold of facial expression based on the observation that images of a subject's facial expressions define a smooth manifold in the high dimensional image space. Such a manifold representation can provide a unified framework for facial expression analysis. We first apply active wavelet networks (AWN) on the image sequences for facial feature localization. To learn the structure of the manifold in the feature space derived by AWN, we investigated two types of embeddings from a high dimensional space to a low dimensional space: locally linear embedding (LLE) and Lipschitz embedding. Our experiments show that LLE is suitable for visualizing expression manifolds. After applying Lipschitz embedding, the expression manifold can be approximately considered as a super-spherical surface in the embedding space. For manifolds derived from different subjects, we propose a nonlinear alignment algorithm that keeps the semantic similarity of facial expression from different subjects on one generalized manifold. We also show that nonlinear alignment outperforms linear alignment in expression classification.",
Feature selection for unsupervised and supervised inference: the emergence of sparsity in a weighted-based approach,"The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science including - examples in computer vision, text processing and more recently bioinformatics are abundant. We present a definition of ""relevancy"" based on spectral properties of the Affinity (or Laplacian) of the features' measurement matrix. The feature selection process is then based on a continuous ranking of the features defined by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a ""biased nonnegativity"" of a key matrix in the process. As a result, a simple least-squares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maxima over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence shows that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.","Computer vision,
Face recognition,
Engines,
Text processing,
Bioinformatics,
Sparse matrices,
Speech recognition,
Support vector machines,
Support vector machine classification,
Testing"
Enabling autonomic behavior in systems software with hot swapping,"Autonomic computing systems are designed to be self-diagnosing and self-healing, such that they detect performance and correctness problems, identify their causes, and apply the appropriate remedy. These abilities can improve performance, uptime, and security, while simultaneously reducing the effort and skills required of system administrators. One way that systems can support these abilities is by allowing monitoring code, diagnostic code, and function implementations to be dynamically inserted and removed in live systems. This “hot swapping” avoids the requisite prescience and additional complexity inherent in creating systems that have all possible configurations built in ahead of time. For already-complex pieces of code such as operating systems, hot swapping provides a simpler, higher performance, and more maintainable method of achieving autonomic behavior. In this paper we discuss hot swapping as a technique for enabling autonomic computing in systems software. First we discuss its advantages and describe the required system structure. Then, for experimental K42 operating system which explicitly supports interposition and replacement of active operating system code, we describe its infrastructure for hot swapping and several instances of its use demonstrating autonomic behavior.",
Non-uniform information dissemination for sensor networks,"Future smart environments are characterized by multiple nodes that sense, collect, and disseminate information about environmental phenomena through a wireless network. In this paper, we define a set of applications that require a new form of distributed knowledge about the environment, referred to as non-uniform information granularity. By non-uniform information granularity we mean that the required accuracy or precision of information is proportional to the distance between a source node (information producer) and current sink node (information consumer). That is, as the distance between the source node and sink node increases, loss in information precision is acceptable. Applications that can benefit from this type of knowledge range from battlefield scenarios to rescue operations. The main objectives of this paper are two-fold: first, we precisely define non-uniform information granularity, and second we describe the different protocols that achieve non-uniform information dissemination and analyze these protocols based on complexity, energy consumption, and accuracy of information.","Protocols,
Magnetic sensors,
Intelligent sensors,
Sensor phenomena and characterization,
Information analysis,
Fires,
Wireless sensor networks,
Landmine detection,
Computer science,
Application software"
Assessing uncertainty in reliability of component-based software systems,"Many architecture-based software reliability models were proposed in the past. Regardless of the accuracy of these models, if a considerable uncertainty exists in the estimates of the operational profile and components reliabilities then a significant uncertainty exists in calculated software reliability. Therefore, the traditional way of estimating software reliability by plugging point estimates of unknown parameters into the model may not be appropriate since it discards any variance due to uncertainty of the parameters. In this paper we propose a methodology for uncertainty analysis of architecture-based software reliability models suitable for large complex component based applications and applicable throughout the software life cycle. First, we describe different approaches to build the architecture based software reliability model and to estimate parameters. Then, we perform uncertainty analysis using the method of moments and Monte Carlo simulation which enable us to study how the uncertainty of parameters propagates in the reliability estimate. Both methods are illustrated on two case studies and compared using several criteria.",
Optimization of wavelet decomposition for image compression and feature preservation,"A neural-network-based framework has been developed to search for an optimal wavelet kernel that can be used for a specific image processing task. In this paper, a linear convolution neural network was employed to seek a wavelet that minimizes errors and maximizes compression efficiency for an image or a defined image pattern such as microcalcifications in mammograms and bone in computed tomography (CT) head images. We have used this method to evaluate the performance of tap-4 wavelets on mammograms, CTs, magnetic resonance images, and Lena images. We found that the Daubechies wavelet or those wavelets with similar filtering characteristics can produce the highest compression efficiency with the smallest mean-square-error for many image patterns including general image textures as well as microcalcifications in digital mammograms. However, the Haar wavelet produces the best results on sharp edges and low-noise smooth areas. We also found that a special wavelet (whose low-pass filter coefficients are 0.32252136, 0.85258927, 0.38458542, and -0.14548269) produces the best preservation outcomes in all tested microcalcification features including the peak signal-to-noise ratio, the contrast and the figure of merit in the wavelet lossy compression scheme. Having analyzed the spectrum of the wavelet filters, we can find the compression outcomes and feature preservation characteristics as a function of wavelets. This newly developed optimization approach can be generalized to other image analysis applications where a wavelet decomposition is employed.","Image coding,
Low pass filters,
Wavelet analysis,
Computed tomography,
Kernel,
Image processing,
Convolution,
Neural networks,
Bones,
Head"
Breeding software test cases with genetic algorithms,"Faulty software is usually costly and possibly life threatening as software becomes an increasingly critical component in a wide variety of systems. Thorough software testing by both developers and dedicated quality assurance staff is one way to uncover flaws. Automated test generation techniques can be used to augment the process, free of the cognitive biases that have been found in human testers. This paper focuses on breeding software test cases using genetic algorithms as part of a software testing cycle. An evolving fitness function that relies on a fossil record of organisms results in interesting search behaviours, based on the concepts of novelty, proximity, and severity. A case study that uses a simple, but widely studied program is used to illustrate the approach. Several visualization techniques are also introduced to analyze particular fossil records, as well as the overall search process.",
Signal nonlinearity in fMRI: a comparison between BOLD and MION,"In this paper, we introduce a methodology for comparing the nonlinearities present in sets of time series using four different nonlinearity measures, one of which, the ""delay vector variance"" method, is a novel approach to the characterization of a time series. It is then applied to examine the difference in nonlinearity between functional magnetic resonance imaging (fMRI) signals that have been recorded using different contrast agents. Recently, an exogenous contrast agent, monocrystalline iron oxide particle (MION), has been introduced for fMRI, which has been shown to increase the functional sensitivity compared with the traditional blood oxygen level dependent (BOLD) technique. The resulting fMRI signals are influenced by cerebral blood volume, whereas the more traditionally recorded BOLD signals are influenced not only by cerebral blood volume, but also by the cerebral blood flow and the metabolic rate of oxygen. The proposed methodology is applied to address the question whether this difference in the number of physiological variables is reflected in a difference in the degree of nonlinearity. We therefore analyze two sets of fMRI signals, one from a BOLD and the other from a MION monkey study with similar experimental designs. In the neuroimaging context, the proposed nonlinearity analyses are different from those described in the literature, since no a priori model is assumed: rather than pinpointing the source(s) of nonlinearity, nonparametric analyses are performed on BOLD and MION fMRI signals. Furthermore, we introduce a strategy for analyzing a population of fMRI signals, rather than focusing the analysis on one signal, as is traditionally done in the domain of nonlinear signal processing. Our results show that, overall, the BOLD signals are more nonlinear in nature than the MION ones, which is in agreement with current hypotheses.","Signal analysis,
Signal processing,
Performance analysis,
Time measurement,
Delay effects,
Magnetic resonance imaging,
Iron,
Blood flow,
Design for experiments,
Neuroimaging"
A new clutter rejection algorithm for Doppler ultrasound,"Several strategies, known as clutter or wall Doppler filtering, were proposed to remove the strong echoes produced by stationary or slow moving tissue structures from the Doppler blood flow signal. In this study, the matching pursuit (MP) method is proposed to remove clutter components. The MP method decomposes the Doppler signal into wavelet atoms that are selected in a decreasing energy order. Thus, the high-energy clutter components are extracted first. In the present study, the pulsatile Doppler signal s(n) was simulated by a sum of random-phase sinusoids. Two types of high-amplitude clutter signals were then superimposed on s(n): time-varying low-frequency components, covering systole and early diastole, and short transient clutter signals, distributed within the whole cardiac cycle. The Doppler signals were modeled with the MP method and the most dominant atoms were subtracted from the time-domain signal s(n) until the signal-to-clutter (S/C) ratio reached a maximum. For the low-frequency clutter signal, the improvement in S/C ratio was 19.0 /spl plusmn/ 0.6 dB, and 72.0 /spl plusmn/ 4.5 atoms were required to reach this performance. For the transient clutter signal, ten atoms were required and the maximum improvement in S/C ratio was 5.5 /spl plusmn/ 0.5 dB. The performance of the MP method was also tested on real data recorded over the common carotid artery of a normal subject. Removing 15 atoms significantly improved the appearance of the Doppler sonogram contaminated with low-frequency clutter. Many more atoms (over 200) were required to remove transient clutter components. These results suggest the possibility of using this signal processing approach to implement clutter rejection filters on ultrasound commercial instruments.",
Re-integrating the research record,"Describes the Scientific Annotation Middleware, a set of components and services that support the creation and use of annotation metadata describing data objects and the semantic relationships among them. It captures aspects of data processing history and the research process and federates the results into a coherent human- and machine-interpretable research record.","Standardization,
Middleware,
Logic,
Collaboration,
Laboratories,
Collaborative software,
Databases,
Information analysis,
Protocols,
Costs"
Logic in access control,"Access control is central to security in computer systems. Over the years, there have been many efforts to explain and improve access control, sometimes with logical ideas and tools. This paper is a partial survey and discussion of the role of logic in access control. It considers logical foundations for access control and their applications, in particular in languages for programming security policies.",
"Criticality-based analysis and design of unstructured peer-to-peer networks as ""Complex systems""","Due to enormous complexity of the unstructured peer-to-peer networks as large-scale, self-configure, and dynamic systems, the models used to characterize these systems are either inaccurate, because of oversimplification, or analytically inapplicable, due to their high complexity. By recognizing unstructured peer-to-peer networks as ""complex systems "", we employ statistical models used before to characterize complex systems for formal analysis and efficient design of peer-to-peer networks. We provide two examples of application of this modeling approach that demonstrate its power. For instance, using this approach we have been able to formalize the main problem with normal flooding search, propose a remedial approach with our probabilistic flooding technique, and find the optimal operating point for probabilistic flooding rigorously, such that it improves scalability of the normal flooding by 99%.",
Three-dimensional guide-wire reconstruction from biplane image sequences for integrated display in 3-D vasculature,"Using three-dimensional rotational X-ray angiography (3DRA), three-dimensional (3-D) information of the vasculature can be obtained prior to endovascular interventions. However, during interventions, the radiologist has to rely on fluoroscopy images to manipulate the guide wire. In order to take full advantage of the 3-D information from 3DRA data during endovascular interventions, a method is presented that yields an integrated display of the position of the guide wire and vasculature in 3-D. The method relies on an automated method that tracks the guide wire simultaneously in biplane fluoroscopy images. Based on the calibrated geometry of the C-arm, the 3-D guide-wire position is determined and visualized in the 3-D coordinate system of the vasculature. The method is evaluated in an intracranial anthropomorphic vascular phantom. The influence of the angle between projections, distortion correction of the projection images, and accuracy of geometry knowledge on the accuracy of 3-D guide-wire reconstruction from biplane images is determined. If the calibrated geometry information is used and the images are corrected for distortion, a mean distance to the reference standard of 0.42 mm and a tip distance of 0.65 mm is found, which means that accurate guide-wire reconstruction from biplane images can be performed.","Image reconstruction,
Image sequences,
Three dimensional displays,
Wire,
Geometry,
X-ray imaging,
Angiography,
Visualization,
Anthropomorphism,
Imaging phantoms"
On the maximum tolerable noise of k-input gates for reliable computation by formulas,We determine the precise threshold of component noise below which formulas composed of odd degree components can reliably compute all Boolean functions.,
Detection of spectral signatures in multispectral MR images for classification,"Presents a new spectral signature detection approach to magnetic resonance (MR) image classification. It is called constrained energy minimization (CEM) method, which is derived from the minimum variance distortionless response in passive sensor array processing. It considers a bank of spectral channels as an array of sensors where each spectral channel represents a sensor and object spectral signature in multispectral MR images are viewed as signals impinging upon the array. The strength of the CEM lies on its ability in detection of spectral signatures of interest without knowing image background. The detected spectral signatures are then used for classification. The CEM makes use of a finite impulse response (FIR) filter to linearly constrain a desired object while minimizing interfering effects caused by other unknown signal sources. Unlike most spatial-based classification techniques, the proposed CEM takes advantage of spectral characteristics to achieve object detection and classification. A series of experiments is conducted and compared with the commonly used c-means method for performance evaluation. The results show that the CEM method is a promising and effective spectral technique for MR image classification.",
LDA/SVM driven nearest neighbor classification,"Nearest neighbor (NN) classification relies on the assumption that class conditional probabilities are locally constant. This assumption becomes false in high dimensions with finite samples due to the curse of dimensionality. The NN rule introduces severe bias under these conditions. We propose a locally adaptive neighborhood morphing classification method to try to minimize bias. We use local support vector machine learning to estimate an effective metric for producing neighborhoods that are elongated along less discriminant feature dimensions and constricted along most discriminant ones. As a result, the class conditional probabilities can be expected to be approximately constant in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other competing techniques using a number of datasets.",
A type system for statically detecting spreadsheet errors,"We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking. We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships. With these, we develop a set of rules to assign units to cells in the spreadsheet. We check for errors by ensuring that every cell has a well-formed unit. We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets. We have run our system on practical examples, and even found errors in published spreadsheets.","Spreadsheet programs,
Computer errors,
Software engineering,
Computer science,
Computer languages,
Spatial databases,
Scientific computing,
Educational programs,
Programming profession,
Application software"
Synchronization of multiple levels of data fusion in wireless sensor networks,"In wireless sensor networks, in-network data fusion is needed for energy-efficient information flow from a plurality of sensors to a central server or sink. As data (either raw or fused) is propagated towards the sink, multiple levels of data fusion are likely. The data fusion at various levels should be synchronized in order to fuse data effectively. It is important that information from as many sensors as possible to be fused in order to increase the credibility of the aggregated report. However, there are trade-offs between fusing a large number of sensor reports and the latency incurred in the aggregation process. The paths taken by the data towards the sink determine where data can be fused, and thus, have an effect on the efficiency of the aggregation process. In this work, we propose a methodology by which the various levels of fusion are synchronized to ensure that the aggregated report has a desired trade-off between credibility and latency, regardless of the topology of the structure created by the integration of the paths on which data traverses towards the sink.","Intelligent networks,
Wireless sensor networks,
Sensor fusion,
Delay,
Network servers,
Protocols,
Computer science,
Data engineering,
Power engineering and energy,
Energy efficiency"
Using temporal coherence to build models of animals,"We describe a system that can build appearance models of animals automatically from a video sequence of the relevant animal with no explicit supervisory information. The video sequence need not have any form of special background. Animals are modeled as a 2D kinematic chain of rectangular segments, where the number of segments and the topology of the chain are unknown. The system detects possible segments, clusters segments whose appearance is coherent over time, and then builds a spatial model of such segment clusters. The resulting representation of the spatial configuration of the animal in each frame can be seen either as a track - in which case the system described should be viewed as a generalized tracker, that is capable of modeling objects while tracking them - or as the source of an appearance model which can be used to build detectors for the particular animal. This is because knowing a video sequence is temporally coherent - i.e. that a particular animal is present through the sequence - is a strong supervisory signal. The method is shown to be successful as a tracker on video sequences of real scenes showing three different animals. For the same reason it is successful as a tracker, the method results in detectors that can be used to find each animal fairly reliably within the Corel collection of images.","Coherence,
Animals,
Object detection,
Video sequences,
Detectors,
Computer science,
Kinematics,
Topology,
Layout,
Region 4"
Distributed tracking in wireless ad hoc sensor networks,,"Wireless sensor networks,
Intelligent networks,
Target tracking,
Radar tracking,
Signal processing algorithms,
Electromechanical sensors,
Inference algorithms,
Computer science,
Collaboration,
Sensor systems"
Model checking for probability and time: from theory to practice,"Probability features increasingly often in software and hardware systems: it is used in distributed coordination and routing problems, to model fault-tolerances and performance, and to provide adaptive resource management strategies. Probabilistic model checking is an automatic procedure for establishing if a desired property holds in a probabilistic specifications such as ""leader election is eventually resolved with probability 1"", ""the chance of shutdown occurring is at most 0.01%"", and ""the probability that a message will be delivered within 30ms is at least 0.75"". A probabilistic model checker calculates the probability of a given temporal logic property being satisfied, as opposed to validity. In contrast to conventional model checkers, which rely on reachability analysis of the underlying transition system graph, probabilistic model checking additionally involves numerical solutions of linear equations and linear programming problems. This paper reports our experience with implementing PRISM (www.cs.bham.ac.uk//spl sim/dxp/prism), a probabilistic symbolic model checker, demonstrates its usefulness in analyzing real-world probabilistic protocols, and outlines future challenges for this research direction.",
Applications of hidden Markov models to detecting multi-stage network attacks,"This paper describes an approach using hidden Markov models (HMM) to detect complex Internet attacks. These attacks consist of several steps that may occur over an extended period of time. Within each step, specific actions may be interchangeable. A perpetrator may deliberately use a choice of actions within a step to mask the intrusion. In other cases, alternate action sequences may be random (due to noise) or because of lack of experience on the part of the perpetrator. For an intrusion detection system to be effective against complex Internet attacks, it must be capable of dealing with the ambiguities described above. We describe research results concerning the use of HMMs as a defense against complex Internet attacks. We describe why HMMs are particularly useful when there is an order to the actions constituting the attack (that is, for the case where one action must precede or follow another action in order to be effective). Because of this property, we show that HMMs are well suited to address the multi-step attack problem. In a direct comparison with two other classic machine learning techniques, decision trees and neural nets, we show that HMMs perform generally better than decision trees and substantially better than neural networks in detecting these complex intrusions.","Hidden Markov models,
Internet,
Intrusion detection,
Machine learning,
Decision trees,
Neural networks,
IP networks,
Laboratories,
Computer networks,
Reconnaissance"
Multi-slider linkage mechanism for endoscopic forceps manipulator,"This paper proposes a new endoscopic hand-held forceps manipulator for endoscopic surgery using two bending mechanisms by multi-slider linkage mechanisms to achieve high mechanical performance and applicability. A bending mechanism consisted of three outer frames, two rotating joints and two sliding linkages for drive and restraint. Two pin-joints could rotate /spl plusmn/45 degrees respectively, enabling rotation of /spl plusmn/90 degrees. The rotation of the joint was available by pulling/pushing the adjacent element by sliding linkage in order. We connected two bending mechanisms, one was for the horizontal plane bending and the other was for the vertical plane bending, enabling 2-DOFs independent motions between -90 degrees and +90 degrees. The 2-DOFs bending mechanism and 1-DOF forceps mechanism were driven by three brushless DC-servomotors. A personal computer-based control unit determined displacement of sliding two linkages and of pulling one wire by the target angle of each DOF mechanism. We examined the actual angle of 2-DOFs bending mechanism, obtaining repeatability of /spl plusmn/0.87 degrees in the horizontal plane bending and /spl plusmn/0.91 degrees in the vertical plane bending. In vivo experiment, this manipulator performed endoscopic surgical tasks under pneumoperitoneum. In conclusion, we were sure of a usefulness of multi-slider linkage mechanism for the new handheld forceps manipulator for clinical application, which showed high repeatability of less 1.0 mm and large workspace with sufficient holding power of 0.55 kgf.",
Mobile robot labs,"There has been much interest in achieving educational and research objectives through the use of small, low-cost robot platforms. While our initial experiences with these platforms were similarly positive, we questioned whether these platforms could be pushed beyond their early uses and transitioned towards achieving substantial educational and research goals. This article reports initial results of this investigation - the construction and implementation of a series of detailed lab assignments using these platforms to tackle basic computer science, AI, robotics, and engineering problems. We first provide detailed descriptions of the labs we have developed and then discuss the robot platforms, including the progression of hardware issues encountered. Finally, we share what we have learned from this endeavor.","Mobile robots,
Artificial intelligence,
Robot sensing systems,
Buildings,
Sonar detection,
Microcontrollers,
Educational robots,
Robotics and automation,
Educational institutions,
Sonar navigation"
"The extended quadratic residue code is the only (48,24,12) self-dual doubly-even code","An extremal self-dual doubly-even binary (n,k,d) code has a minimum weight d=4/spl lfloor/n/24/spl rfloor/+4. Of such codes with length divisible by 24, the Golay code is the only (24,12,8) code, the extended quadratic residue code is the only known (48,24,12) code, and there is no known (72,36,16) code. One may partition the search for a (48,24,12) self-dual doubly-even code into three cases. A previous search assuming one of the cases found only the extended quadratic residue code. We examine the remaining two cases. Separate searches assuming each of the remaining cases found no codes and thus the extended quadratic residue code is the only doubly-even self-dual (48,24,12) code.",
Stochastic programming models for general redundancy-optimization problems,"This paper provides a unified modeling idea for both parallel and standby redundancy optimization problems. A spectrum of redundancy stochastic programming models is constructed to maximize the mean system-lifetime, /spl alpha/-system lifetime, or system reliability. To solve these models, a hybrid intelligent algorithm is presented. Some numerical examples illustrate the effectiveness of the proposed algorithm. This paper considers both parallel redundant systems and standby redundant systems whose components are connected with each other in a logical configuration with a known system structure function. Three types of system performance-expected system lifetime, /spl alpha/-system lifetime and system reliability-are introduced. A stochastic simulation is designed to estimate these system performances. In order to model general redundant systems, a spectrum of redundancy stochastic programming models is established. Stochastic simulation, NN and GA are integrated to produce a hybrid intelligent algorithm for solving the proposed models. Finally, the effectiveness of the hybrid intelligent algorithm is illustrated by some numerical examples.",
Improved SVD systolic array and implementation on FPGA,"This paper presents an efficient systolic array for the computation of the Singular Value Decomposition (SVD). The proposed architecture is three times more efficient and faster than the Brent, Luk, Van Loan (BLV) SVD systolic array. The architecture has been implemented efficiently on FPGA using a high level language for hardware design ""Handel-C"".",
Interference aware (IA) MAC: an enhancement to IEEE802.11b DCF,"The IEEE802.11 has been devised explicitly for low mobility and single access point scenarios, so its effectiveness is impaired in conditions with not negligible interference in ad hoc and/or in infrastructure modes. Well known problems typical of these scenarios are the so called hidden and exposed terminals. In this paper we propose and test a novel MAC layer for wireless LANs, able to improve the performance of IEEE802.11 DCF (distributed coordination function) in environments with high interference levels. The key point is to insert information about received power and interference levels into MAC control packets. By computing an estimation of the interference increasing due to an eventual transmission (forbidden with IEEE802.11 because blocked by the virtual carrier sensing), the number of parallel transmissions can grow significantly.","Interference,
Physical layer,
Spread spectrum communication,
Wireless sensor networks,
Media Access Protocol,
Quadrature phase shift keying,
Access protocols,
Computer science,
Testing,
Wireless LAN"
Multi-channel MAC protocol for mobile ad hoc networks,"This paper introduces a new MAC scheme operating on multiple channels that maximizes network performance and provides differentiated services in mobile ad hoc networks (MANETs). Specifically, the IEEE 802.11 in ad hoc mode, the most popular MAC protocol in mobile ad hoc networks, is extended from single channel to multiple channels operation. The current standard allows the practical use of three channels in 802.11b and eight in 802.11a, but multiple channel operation is not supported in ad hoc mode. The proposed protocol ensures maximum performance, low delay, reliability, efficiency and fairness, while allowing transmission priorities to be set on a per-channel basis. In addition, a solution is provided for the hidden multi-channel problem, which arises when only one network interface card is used. Basic considerations for mobility management in a multi-channel environment is also given. We show through simulation that the multi-channel MAC protocol greatly outperforms the original IEEE 802.11 MAC protocol.","Media Access Protocol,
Mobile ad hoc networks,
Access protocols,
Frequency,
Network interfaces,
Interference,
Computer science,
Delay,
Mobile radio mobility management,
Femtocell networks"
KNIGHT/spl trade/: a real time surveillance system for multiple and non-overlapping cameras,"In this paper, we present a wide area surveillance system that detects, tracks and classifies moving objects across multiple cameras. At the single camera level, tracking is performed using a voting based approach that utilizes color and shape cues to establish correspondence. The system uses the single camera tracking results along with the relationship between camera field of view (FOV) boundaries to establish correspondence between views of the same object in multiple cameras. To this end, a novel approach is described to find the relationships between the FOV lines of cameras. The proposed approach combines tracking in cameras with overlapping and/or non-overlapping FOVs in a unified framework, without requiring explicit calibration. The proposed algorithm has been implemented in a real time system. The system uses a client-server architecture and runs at 10 Hz with three cameras.",
Probabilistic cooperative localization and mapping in practice,"In this paper we present a probabilistic framework for the reduction in the uncertainty of a moving robot pose during exploration by using a second robot to assist. A Monte Carlo Simulation technique (specifically, a Particle Filter) is employed in order to model and reduce the accumulated odometric error. Furthermore, we study the requirements to obtain an accurate yet timely pose estimate. A team of two robots is employed to explore an indoor environment in this paper, although several aspects of the approach have been extended to larger groups. The concept behind our exploration strategy has been presented previously and is based on having one robot carry a sensor that acts as a ""robot tracker"" to estimate the position of the other robot. By suitable use of the tracker as an appropriate motion-control mechanism we can sweep areas of free space between the stationary and the moving robot and generate an accurate graph-based description of the environment. This graph is used to guide the exploration process. Complete exploration without any overlaps is guaranteed as a result of the guidance provided by the dual graph of the spatial decomposition (triangulation) of the environment. We present experimental results from indoor experiments in our laboratory and from more complex simulated experiments.",
Fuzzy clustering for intrusion detection,"The newly formed Department of Homeland Security has been mandated to reduce America's vulnerability to terrorism. In addition to being charged with physical protection, this newly formed department is also responsible for protecting the nation's critical infrastructure. Protecting computer systems from intrusions is an important aspect of securing the nation's infrastructure. We are exploring how fuzzy data mining and concepts introduced by the semantic Web can operate in synergy to perform distributed intrusion detection. The underlying premise of our intrusion detection model is to describe attacks as instances of an ontology using a semantically rich language, reason over them and subsequently classify them as instances of an attack of a specific type. However, before an abnormality can be specified as an instance of the ontology, it first needs to be detected. Hence, our intrusion detection model is two phased, where the first phase uses data mining techniques to analyze low level data streams that capture process, system and network states and to detect anomalous behavior. The second phase reasons over instances of anomalous behavior specified according to our ontology. This paper focuses on the initial phase of our model: outlier detection within low level data streams. Accordingly, we present the preliminary results of the use of fuzzy clustering to detect anomalies within low level kernel data streams.","Intrusion detection,
Terrorism,
Protection,
Detectors,
Ontologies,
Filters,
Event detection,
Computer science,
Semantic Web,
Data mining"
"Intruder deductions, constraint solving and insecurity decision in presence of exclusive or","We present decidability results for the verification of cryptographic protocols in the presence of equational theories corresponding to xor and Abelian groups. Since the perfect cryptography assumption is unrealistic for cryptographic primitives with visible algebraic properties such as xor, we extend the conventional Dolev-Yao model by permitting the intruder to exploit these properties. We show that the ground reachability problem in NP for the extended intruder theories in the cases of xor and Abelian groups. This result follows from a normal proof theorem. Then, we show how to lift this result in the xor case: we consider a symbolic constraint system expressing the reachability (e.g., secrecy) problem for a finite number of sessions. We prove that such a constraint system is decidable, relying in particular on an extension of combination algorithms for unification procedures. As a corollary, this enables automatic symbolic verification of cryptographic protocols employing xor for a fixed number of sessions.","Cryptographic protocols,
Cryptography,
Equations,
Constraint theory,
Authentication,
Logic,
Computer science,
Context modeling,
Algebra,
Polynomials"
Speech-gesture driven multimodal interfaces for crisis management,"Emergency response requires strategic assessment of risks, decisions, and communications that are time critical while requiring teams of individuals to have fast access to large volumes of complex information and technologies that enable tightly coordinated work. The access to this information by crisis management teams in emergency operations centers can be facilitated through various human-computer interfaces. Unfortunately, these interfaces are hard to use, require extensive training, and often impede rather than support teamwork. Dialogue-enabled devices, based on natural, multimodal interfaces, have the potential of making a variety of information technology tools accessible during crisis management. This paper establishes the importance of multimodal interfaces in various aspects of crisis management and explores many issues in realizing successful speech-gesture driven, dialogue-enabled interfaces for crisis management. This paper is organized in five parts. The first part discusses the needs of crisis management that can be potentially met by the development of appropriate interfaces. The second part discusses the issues related to the design and development of multimodal interfaces in the context of crisis management. The third part discusses the state of the art in both the theories and practices involving these human-computer interfaces. In particular, it describes the evolution and implementation details of two representative systems, Crisis Management (XISM) and Dialog Assisted Visual Environment for Geoinformation (DAVE/spl I.bar/G). The fourth part speculates on the short-term and long-term research directions that will help addressing the outstanding challenges in interfaces that support dialogue and collaboration. Finally, the fifth part concludes the paper.","Crisis management,
Computer science,
Speech recognition,
Computer aided instruction,
Paper technology,
Management training,
Impedance,
Teamwork,
Information technology,
Technology management"
Smooth multirate multicast congestion control,"A significant impediment to deployment of multicast services is the daunting technical complexity of developing, testing and validating congestion control protocols fit for wide-area deployment. Protocols such as pgmcc and TFMCC have recently made considerable progress on the single rate case, i.e. where one dynamic reception rate is maintained for all receivers in the session. However, these protocols have limited applicability, since scaling to session sizes beyond tens of participants necessitates the use of multiple rate protocols. Unfortunately, while existing multiple rate protocols exhibit better scalability, they are both less mature than single rate protocols and suffer from high complexity. We propose a new approach to multiple rate congestion control that leverages proven single rate congestion control methods by orchestrating an ensemble of independently controlled single rate sessions. We describe SMCC, a new multiple rate equation-based congestion control algorithm for layered multicast sessions that employs TFMCC as the primary underlying control mechanism for each layer. SMCC combines the benefits of TFMCC (smooth rate control, equation-based TCP friendliness) with the scalability and flexibility of multiple rates to provide a sound multiple rate multicast congestion control policy.","Multicast protocols,
Testing,
Scalability,
Equations,
Buildings,
Centralized control,
Computer science,
Impedance,
Multicast algorithms,
Control design"
A performance oriented migration framework for the grid,"At least three factors in the existing migration frameworks make them less suitable in Grid systems especially when the goal is to improve the response times for individual applications. These factors are the separate policies for suspension and migration of executing applications employed by these migration frameworks, the use of pre-defined conditions for suspension and migration and the lack of knowledge of the remaining execution time of the applications. In this paper we describe a migration framework for performance oriented Grid systems that implements tightly coupled policies for both suspension and migration of executing applications and takes into account both system load and application characteristics. The main goal of our migration framework is to improve the response times for individual applications. We also present some results that demonstrate the usefulness of our migration framework.","Contracts,
Delay,
Grid computing,
Processor scheduling,
Lifting equipment,
Computer science,
Variable structure systems,
Application software,
Fault tolerant systems,
Predictive models"
Simple estimators for relational Bayesian classifiers,"We present the relational Bayesian classifier (RBC), a modification of the simple Bayesian classifier (SBC) for relational data. There exist several Bayesian classifiers that learn predictive models of relational data, but each uses a different estimation technique for modelling heterogeneous sets of attribute values. The effects of data characteristics on estimation have not been explored. We consider four simple estimation techniques and evaluate them on three real-world data sets. The estimator that assumes each multiset value is independently drawn from the same distribution (INDEPVAL) achieves the best empirical results. We examine bias and variance tradeoffs over a range of data sets and show that INDEPVAL's ability to model more multiset information results in lower bias estimates and contributes to its superior performance.","Bayesian methods,
Motion pictures,
Predictive models,
Laboratories,
Computer science,
Drives,
Data mining"
"Portability, extensibility and robustness in iROS","The dynamism and heterogeneity in ubicomp environments on both short and long time scales implies that middleware platforms for these environments need to be designed ground up for portability, extensibility and robustness. In this paper, we describe how we met these requirements in iROS, a middleware platform for a class of ubicomp environments, through the use of three guiding principles - economy of mechanism, client simplicity and levels of indirection. Apart from theoretical arguments and experimental results, experience through several deployments with a variety of apps, in most cases not done by the original designers of the system, provides some validation in practice that the design decisions have in fact resulted in the intended portability, extensibility and robustness. A retrospective examination of the system leads the authors to the following lesson: A logically-centralized design and physically-centralized implementation enables the best behavior in terms of extensibility and portability along with ease of administration, and sufficient behavior in terms of scalability and robustness.","Robustness,
Pervasive computing,
Middleware,
Application software,
Hardware,
Computer science,
Scalability,
Ubiquitous computing,
Mobile computing,
Distributed computing"
An improved schedulability test for uniprocessor periodic task systems,"We present a sufficient linear-time schedulability test for preemptable, asynchronous, periodic task systems with arbitrary relative deadlines, scheduled on a uniprocessor by an optimal scheduling algorithm. We show that this test is more accurate than the commonly-used density condition. We also present and discuss the results of our test with that of a pseudo-polynomial-time schedulability test presented previously for a restricted class of task systems in which utilization is strictly less than one.","System testing,
Optimal scheduling,
Processor scheduling,
Scheduling algorithm,
Polynomials,
Gold,
Sequential analysis,
Delay,
Computer science,
Costs"
Probabilistic user behavior models,"We present a mixture model based approach for learning individualized behavior models for the Web users. We investigate the use of maximum entropy and Markov mixture models for generating probabilistic behavior models. We first build a global behavior model for the entire population and then personalize this global model for the existing users by assigning each user individual component weights for the mixture model. We then use these individual weights to group the users into behavior model clusters. We show that the clusters generated in this manner are interpretable and able to represent dominant behavior patterns. We conduct offline experiments on around two months worth of data from CiteSeer, an online digital library for computer science research papers currently storing more than 470,000 documents. We show that both maximum entropy and Markov based personal user behavior models are strong predictive models. We also show that maximum entropy based mixture model outperforms Markov mixture models in recognizing complex user behavior patterns.",
Spatial domain filtering for fast modification of the tradeoff between image sharpness and pixel noise in computed tomography,"In computed tomography (CT), selection of a convolution kernel determines the tradeoff between image sharpness and pixel noise. For certain clinical applications it is desirable to have two or more sets of images with different settings. So far, this typically requires reconstruction of several sets of images. We present an alternative approach using default reconstruction of sharp images and online filtering in the spatial domain allowing modification of the sharpness-noise tradeoff in real time. A suitable smoothing filter function in the frequency domain is the ratio of smooth and original (sharp) kernel. Efficient implementation can be achieved by a Fourier transform of this ratio to the spatial domain. Separating the two-dimensional spatial filtering into two subsequent one-dimensional filtering stages in the x and y directions using a Gaussian approximation for the convolution kernel further reduces computational complexity. Due to efficient implementation, interactive modification of the filter settings becomes possible, which can completely replace the variety of different reconstruction kernels.","Filtering,
Pixel,
Computed tomography,
Kernel,
Image reconstruction,
Convolution,
Filters,
Smoothing methods,
Frequency domain analysis,
Fourier transforms"
Particle swarm optimization recommender system,"Recommender systems are new types of Internet-based software tools, designed to help users find their way through today's complex on-line shops and entertainment Web sites. This paper describes a new recommender system, which employs a particle swarm optimization (PSO) algorithm to learn personal preferences of users and provide tailored suggestions. Experiments are carried out to observe the performance of the system and results are compared to those obtained from the genetic algorithm (GA) recommender system and a standard, non-adaptive system based on the Pearson algorithm.",
Shape-similarity search of 3D models by using enhanced shape functions,"We propose a pair of shape features for shape-similarity search of 3D (three-dimensional) polygonal-mesh models. The shape features are an extension of the D2 shape functions proposed by Osada et al. (2001). Our proposed shape features are tolerant of topological variations and geometrical degeneracies. Our shape feature is also invariant to similarity transformation. Experiments showed that, with only a modest increase in computational cost, our shape feature achieved a significant performance improvement over Osada's D2.",
Using multilevel call matrices in large software projects,"Traditionally, node link diagrams are the prime choice when it comes to visualizing software architectures. However, node link diagrams often fall short when used to visualize large graph structures. In this paper we investigate the use of call matrices as visual aids in the management of large software projects. We argue that call matrices have a number of advantages over traditional node link diagrams when the main object of interest is the link instead of the node. Matrix visualizations can provide stable and crisp layouts of large graphs and are inherently well suited for large multilevel visualizations because of their recursive structure. We discuss a number of visualization issues, using a very large software project currently under development at Philips Medical Systems as a running example.","Visualization,
Software engineering,
Software systems,
Programming profession,
Biomedical imaging,
Mathematics,
Computer science,
Software architecture,
Project management,
Chromium"
Regularization parameter estimation for feedforward neural networks,"Under the framework of the Kullback-Leibler (KL) distance, we show that a particular case of Gaussian probability function for feedforward neural networks (NNs) reduces into the first-order Tikhonov regularizer. The smooth parameter in kernel density estimation plays the role of regularization parameter. Under some approximations, an estimation formula is derived for estimating regularization parameters based on training data sets. The similarity and difference of the obtained results are compared with other work. Experimental results show that the estimation formula works well in sparse and small training sample cases.",
Broadcasting algorithms in radio networks with unknown topology,"In this paper we present new randomized and deterministic algorithms for the classical problem of broadcasting in radio networks with unknown topology. We consider directed n-node radio networks with specified eccentricity D (maximum distance from the source node to any other node). Our first main result closes the gap between the lower and upper bound: we describe an optimal randomized broadcasting algorithm whose running time complexity is O(D log(n/D) + log/sup 2/n), with high probability. In particular, we obtain a randomized algorithm that completes broadcasting in any n-node radio network in time O(n), with high probability. The main source of our improvement is a better ""selecting sequence"" used by the algorithm that brings some stronger property and improves the broadcasting time. Next, we demonstrate how to apply our approach to deterministic broadcasting, and describe a deterministic oblivious algorithm that completes broadcasting in almost optimal time O(n log/sup 2/D). Finally, we show how our randomized broadcasting algorithm can be used to improve the randomized complexity of the gossiping problem.",
The role of /spl epsi/-dominance in multi objective particle swarm optimization methods,"In this paper, the influence of /spl epsi/-dominance on multi-objective particle swarm optimization (MOPSO) methods is studied. The most important role of /spl epsi/-dominance is to bound the number of non-dominated solutions stored in the archive (archive size), which has influences on computational time, convergence and diversity of solutions. Here, /spl epsi/-dominance is compared with the existing clustering technique for fixing the archive size and the solutions are compared in terms of computational time, convergence and diversity. A new diversity metric is also suggested. The results show that the /spl epsi/-dominance method can find solutions much faster than the clustering technique with comparable and even in some cases better convergence and diversity.","Particle swarm optimization,
Data structures,
Testing,
Optimization methods,
Computer science,
Diversity methods"
Physiologically based modeling of 3-D vascular networks and CT scan angiography,"In this paper, a model-based approach to medical image analysis is presented. It is aimed at understanding the influence of the physiological (related to tissue) and physical (related to image modality) processes underlying the image content. This methodology is exemplified by modeling first, the liver and its vascular network, and second, the standard computed tomography (CT) scan acquisition. After a brief survey on vascular modeling literature, a new method, aimed at the generation of growing three-dimensional vascular structures perfusing the tissue, is described. A solution is proposed in order to avoid intersections among vessels belonging to arterial and/or venous trees, which are physiologically connected. Then it is shown how the propagation of contrast material leads to simulate time-dependent sequences of enhanced liver CT slices.",
Problems and Programmers: an educational software engineering card game,"Problems and Programmers is an educational card game that we have developed to help teach software engineering. It is based on the observation that students, in a typical software engineering course, gain little practical experience in issues regarding the software process. The underlying problem is time: any course faces the practical constraint of only being able to involve students in at most a few small software development projects. Problems and Programmers overcomes this limitation by providing a simulation of the software process. In playing the game, students become aware of not only general lessons, such as the fact that they must continuously make tradeoffs among multiple potential next steps, but also specific issues such as the fact that inspections improve the quality of code but delay its delivery time. We describe game play of Problems and Programmers, discuss its underlying design, and report on the results of a small experiment in which twenty-eight students played the game.","Software engineering,
Programming profession,
Educational programs,
Computer science,
Inspection,
Delay effects,
Software testing,
Employment,
Computational modeling,
Computer simulation"
A new approach to automatic speech summarization,"This paper proposes a new automatic speech summarization method. In this method, a set of words maximizing a summarization score is extracted from automatically transcribed speech. This extraction is performed according to a target compression ratio using a dynamic programming (DP) technique. The extracted set of words is then connected to build a summarization sentence. The summarization score consists of a word significance measure, a confidence measure, linguistic likelihood, and a word concatenation probability. The word concatenation score is determined by a dependency structure in the original speech given by stochastic dependency context free grammar (SDCFG). Japanese broadcast news speech transcribed using a large-vocabulary continuous-speech recognition (LVCSR) system is summarized using our proposed method and compared with manual summarization by human subjects. The manual summarization results are combined to build a word network. This word network is used to calculate the word accuracy of each automatic summarization result using the most similar word string in the network. Experimental results show that the proposed method effectively extracts relatively important information by removing redundant and irrelevant information.","Data mining,
Speech recognition,
Automatic speech recognition,
Indexing,
Speech processing,
Computer science,
Information science,
Dynamic programming,
Stochastic processes,
Broadcasting"
The Penn-Lehman automated trading project,"The Penn-Lehman automated trading project is a broad investigation of algorithms and strategies for automated trading in financial markets. The PLAT project's centerpiece is the Penn exchange simulator (PXS), a software simulator for automated stock trading that merges automated client orders for shares with real-world, real-time order data. PXS automatically computes client profits and losses, volumes traded, simulator and external prices, and other quantities of interest. To test the effectiveness of PXS and of various trading strategies, we've held three formal competitions between automated clients.","Computational modeling,
Consumer electronics,
Microstructure,
Automatic testing,
Stock markets,
Artificial intelligence,
Computer science,
Algorithm design and analysis,
Safety,
Veins"
On supporting distributed collaboration in sensor networks,"In sensor networks, nodes may malfunction due to the hostile environment. Therefore, dealing with node failure is a very important research issue. In this paper, we study distributed cooperative failure detection techniques. In the proposed techniques, the nodes around a suspected node collaborate with each other to reach an agreement on whether the suspect is faulty or malicious. We first formalize the problem as how to construct a dominating tree to cover all the neighbors of the suspect and give the lower bound of the message complexity. Two tree-based propagation collection protocols are proposed to construct dominating trees and collect information via the tree structure. Instead of using the traditional flooding technique, we propose a coverage-based heuristic to improve the system performance. Theoretical analysis and simulation results show that the heuristic can help achieve a higher tree coverage with lower message complexity, lower delay and lower energy consumption.",
User-centered modeling and evaluation of multimodal interfaces,"Historically, the development of computer interfaces has been a technology-driven phenomenon. However, new multimodal interfaces are composed of recognition-based technologies that must interpret human speech, gesture, gaze, movement patterns, and other complex natural behaviors, which involve highly automatized skills that are not under full conscious control. As a result, it now is widely acknowledged that multimodal interface design requires modeling of the modality-centered behavior and integration patterns upon which multimodal systems aim to build. This paper summarizes research on the cognitive science foundations of multimodal interaction, and on the essential role that user-centered modeling has played in prototyping, guiding, and evaluating the design of next-generation multimodal interfaces. In particular, it discusses the properties of different modalities and the information content they carry, the unique features of multimodal language and its processability, as well as when users are likely to interact multimodally and how their multimodal input is integrated and synchronized. It also reviews research on typical performance and linguistic efficiencies associated with multimodal interaction, and on the user-centered reasons why multimodal interaction minimizes errors and expedites error handling. In addition, this paper describes the important role that selective methodologies and evaluation metrics have played in shaping next-generation multimodal systems, and it concludes by highlighting future directions for designing a new class of adaptive multimodal-multisensor interfaces.",
Channel assignment for initial and handoff calls to improve the call-completion probability,"The paper focuses on how to assign channels for initial and handoff calls. Previous schemes give priority to handoff calls by queuing handoff calls, reserving some channels for handoff calls, or subrating existing calls for handoff calls. We queue both initial and handoff calls. We take this idea from derivations of the optimal value for an approximation to the call-completion probability. Our goal is to have higher call-completion probability and still keep forced-termination probability low. We propose four schemes: SFTT (single-queue, FIFO, timeout, average timeout), SPTT (single-queue, priority, timeout, average timeout), DFTS (dual-queues, FIFO, timeout, statistical TDM), and DPTS (dual-queues, priority, timeout, statistical TDM). The four schemes, along with the NPS and FIFO schemes, were simulated and compared. For the SFTT scheme, we also simulated different average timeouts for initial calls. All four proposed schemes have better call-completion probabilities than the NPS and FIFO schemes. Call-completion probabilities can be improved by implementing a priority scheme which serves the waiting call with the least remaining time first. The implementation of statistical multiplexing also has the effect of increasing call-completion probability when the average new-call arrival rates are high. However, both the priority scheme and statistical multiplexing may increase forced-termination probability.","Probability,
Base stations,
Personal communication networks,
Mobile communication,
Computer science,
Degradation,
Mathematical model"
The WHaT: a wireless haptic texture sensor,"We describe the WHaT, a wireless device for haptic texture measurement and interaction. The WHaT is designed for simultaneously measuring contact force and acceleration in a hand-held probe. The probe is small and can be comfortably held, like a pen. It transmits contact measurements to a host computer over a wireless link, with low latency. We discuss the design and initial evaluation of the device.",
Modeling of Web services flow,"Services such as automatic purchasing, automatic updating of prices, or getting latest information etc, can be provided on the Internet using Web services technology. A client can access these services using the Internet. Web services infrastructure includes some standards, such as simple object access protocol (SOAP), Web services description language (WSDL) and universal description, discovery and integration (UDDI). In this paper we represent distributed Web services by modeling the flow of messages and methods in a Web service transaction. Such a model assists the Web services designer to ensure the correctness of Web flows in terms of deadlock and correct termination of the Web services transaction. WSDL and methods are modeled using Petri Nets. A software tool is implemented for extracting the model from the WSDL description of the Web services flow.","Web services,
Simple object access protocol,
System recovery,
Web and internet services,
Petri nets,
XML,
Mathematical model,
Merging,
Computer science,
Software tools"
Understanding change-proneness in OO software through visualization,"During software evolution, adaptive, and corrective maintenance are common reasons for changes. Often such changes cluster around key components. It is therefore important to analyze the frequency of changes to individual classes, but, more importantly, to also identify and show related changes in multiple classes. Frequent changes in clusters of classes may be due to their importance, due to the underlying architecture or due to chronic problems. Knowing where those change-prone clusters are can help focus attention, identify targets for re-engineering and thus provide product-based information to steer maintenance processes. This paper describes a method to identify and visualize classes and class interactions that are the most change-prone. The method was applied to a commercial embedded, real-time software system. It is object-oriented software that was developed using design patterns.",
An autonomous robot photographer,"We describe a complete, end-to-end system for taking well-composed photographs using a mobile robot. The general scenario is a reception, or other event, where people are roaming around talking to each other. The robot serves as an ""event photographer"", roaming around the same space as the participants, periodically taking photographs. These images are then sent to a workstation where participants can print the photographs out, or email them.","Robot vision systems,
Orbital robotics,
Photography,
Navigation,
Digital cameras,
Workstations,
Mobile robots,
Intelligent robots,
Firewire,
Computer science"
Meshed multipath routing: an efficient strategy in sensor networks,"Due to limited functionalities and potentially large number of sensors, conventional routing strategies proposed for distributed control applications (such as mobile ad hoc networks) are not directly applicable in wireless sensor networks. In this paper, we propose a novel mesh multipath routing (M-MPR) with selective forwarding of packets. Our evaluation shows that M-MPR achieves much improved throughput performance over conventional disjoint multipath routing, with comparable power consumption and receiver complexity. We also show that for comparable throughput, M-MPR achieves better load distribution and requires lesser route maintenance overhead with respect to packet forwarding along a preferred route.",
"Towards autonomic computing: agent-based modelling, dynamical systems analysis, and decentralised control","Autonomic computing aims to deal with the complexity of today systems by letting the system handle the complexity autonomously. This is a very hard and challenging domain because current systems are complex, distributed, interconnected and rapidly changing systems. We firmly believe that a main challenge in conquering autonomic systems is the integration of three existing research communities: the multiagent systems community allows natural modelling of the system and explicitly considers autonomous behaviour and distributed interaction, dynamical systems theory allows analysis of the dynamics of these models and the decentralised control community can use insights gathered from the analysis to create decentralised control mechanisms to control the dynamics of autonomic systems. We describe this generic perspective on autonomic computing, give an overview of the relevant work done in each community and describe the contribution of each community in conquering autonomic computing.",
SAFE: a data dissemination protocol for periodic updates in sensor networks,"In sensor networks, it is crucial to design and employ energy-efficient communication protocols, since nodes are battery-powered and thus their lifetimes are limited. This paper studies data dissemination in two-tiered networks comprised of stationary sensor nodes and mobile data users who request periodic sensor data updates. We propose a protocol called SAFE (sinks accessing data from environments) which attempts to save energy through data dissemination path sharing among multiple data sinks. Simulation results show that the proposed protocol is energy-efficient as well as scalable to a large data sink population.","Intelligent networks,
Access protocols,
Energy efficiency,
Mobile communication,
Sensor systems,
Computer science,
Scattering,
Batteries,
Design engineering,
Power engineering and energy"
Design and evaluation of a wide-area event notification service,,
A general data fusion architecture,,"Doped fiber amplifiers,
Uncertainty,
Mathematical model,
Computer science,
Unified modeling language,
Bandwidth,
Sensor fusion,
Sensor systems,
Computer architecture,
Application software"
"A unified framework for modeling TCP-Vegas, TCP-SACK, and TCP-Reno","A general analytical framework for the modeling and analysis of TCP variations is presented. The framework allows the modeling of multiple variations of TCP, including TCP-Vegas, TCP-SACK, and TCP-Reno, under general network situations. In particular, the framework allows us to propose the first analytical model of TCP-Vegas for arbitrary on-off traffic that is able to predict the operating point of the network. The analysis provided by our framework leads to many interesting observations with respect to both the behavior of bottleneck links that are shared by TCP sources and the effectiveness of the design decisions in TCP-SACK and TCP-Vegas.","Analytical models,
Protocols,
Delay,
Computer science,
Traffic control,
Queueing analysis,
Mathematics,
Mathematical model,
Telecommunication traffic,
Performance evaluation"
On things to happen during a TCP handover,"This paper gives a survey on the impact of handover between heterogeneous links to the performance of TCP and elaborates on the effects TCP is challenged at the moment of a handover. Moreover, we propose several approaches to improve the performance of TCP in these situations and present an evaluation of these approaches by means of simulation.","TCPIP,
Land mobile radio cellular systems,
Bandwidth,
Delay,
Computer science,
Mobile radio mobility management,
4G mobile communication,
Radio network,
Appropriate technology,
Transport protocols"
A multiagent system for optimizing urban traffic,"For the purposes of managing an urban traffic system, a hierarchical multiagent system that consists of several locally operating agents each representing an intersection of a traffic system is proposed. Local traffic agents (LTAs) are concerned with the optimal performance of their assigned intersection; however, the resulting traffic light patterns may result in the failure of the system when examined at a global level. Therefore, supervision is required and achieved with the use of a coordinator traffic agent (CTA). A CTA provides a means by which the optimal local light pattern can be compared against the global concerns. The pattern can then be slightly modified to accommodate the global environment, while maintaining the local concerns of the intersection. Functionality of the proposed system is examined using two traffic scenarios: traffic accident and morning rush hour. For both scenarios, the proposed multiagent system efficiently managed the gradual congestion of the traffic.","Multiagent systems,
Telecommunication traffic,
Traffic control,
Computer science,
Niobium,
Road accidents,
Automobiles,
Transportation,
Urban areas,
Pattern recognition"
The hapticon editor: a tool in support of haptic communication research,"We define haptic icons, or ""hapticons"", as brief programmed forces applied to a user through a haptic interface, with the role of communicating a simple idea in manner similar to visual or auditory icons. In this paper we present the design and implementation of an innovative software tool and graphical interface for the creation and editing of hapticons. The tool's features include various methods for creating new icons including direct recording of manual trajectories and creation from a choice of basis waveforms; novel direct-manipulation icon editing mechanisms, integrated playback and convenient storage of icons to file. We discuss some ways in which the tool has aided our research in the area of haptic iconography and present an innovative approach for generating and rendering simple textures on a low degree of freedom haptic device using what we call terrain display.","Haptic interfaces,
Audio user interfaces,
Shape control,
Computer graphics,
Computer science,
Software tools,
Rendering (computer graphics),
Computer displays,
Auditory displays,
Computer interfaces"
Structured superpeers: leveraging heterogeneity to provide constant-time lookup,"Peer-to-peer (P2P) systems are typically divided into those that centralize lookup functionality in a single location and those that distribute the lookup operation across the set of participating hosts. The former approach can offer constant time lookup latency, but is more expensive to scale and suffers from single points of failure. In contrast, the fully distributed approach is easier to scale and can be more resilient to failures, but the lookup latency scales as a function of the total number of participants. While the research community has made great progress in improving the latency of distributed lookup, these systems, exemplified by Chord[I. Stoica et al., (2001)] typically require O(logN) hops to locate an object in a system with N hosts. We explore the costs and benefits of a new hybrid approach that partially distributes lookup information among a dynamically adjusted set of high-capacity ""superpeers"". This design exploits the resource heterogeneity inherent in existing P2P systems to provide many of the advantages of a centralized system, even while avoiding most of the problems associated with such systems. Lookup is performed using superpeers in constant-time, and the system performs well even in the event of simultaneous super-peer failures. Finally, while our gain in performance is potentially at the expense of scalability, we will show that a straightforward implementation should be able to scale to over one million peers with reasonable lookup rates.","Peer to peer computing,
Delay,
Costs,
Bandwidth,
Computer science,
Performance gain,
Scalability,
Routing,
Conferences,
Internet"
Cultural swarms: modeling the impact of culture on social interaction and problem solving,"In this paper we investigate how diverse knowledge sources interact to direct individuals in a swarm population. We identify three basic phases of problem solving that are generated by the swarm population in the solution of real valued function optimization problems. The question that we are interested in answering is how these phases derive from the interaction of various sources of cultural knowledge present in the belief space of a population. We map the central tendency of the subset of individuals that are influences by each knowledge source over time at the meta-level. The resultant patterns suggest the presence of ""cultural swarms"" where various knowledge sources take turns at leading and following in the exploration and exploitation of the problem space. This suggests that the social interaction of individuals coupled with their interaction with a culture within which they are embedded provides a powerful vehicle for the solution of complex problems.","Cultural differences,
Problem-solving,
Computer science,
History,
Vehicles,
Power system modeling,
Humans,
Decision making,
Genetic programming,
Character generation"
The future of electrical and computer engineering education,"The authors will briefly describe how some of today's innovations and advancements might provide potential for improving the efficiency, effectiveness, and quality of contemporary teaching methods. A model curriculum proposed in this paper merges the disciplines of mathematics, science, engineering, and computing. It also addresses the growing need for exposing aspiring engineers to the human, cultural, and professional aspects of their emerging careers.",
Measuring bottleneck bandwidth of targeted path segments,"Accurate measurement of network bandwidth is crucial for network management applications as well as flexible Internet applications and protocols which actively manage and dynamically adapt to changing utilization of network resources. Extensive work has focused on two approaches to measuring bandwidth: measuring it hop-by-hop, and measuring it end-to-end along a path. Unfortunately, best-practice techniques for the former are inefficient, and techniques for the latter are only able to observe bottlenecks visible at end-to-end scope. In this paper, we develop end-to-end probing methods which can measure bottleneck bandwidth along arbitrary, targeted subpaths of a path in the network, including subpaths shared by a set of flows. We evaluate our technique through extensive ns simulations, then provide a comparative Internet performance evaluation against hop-by-hop techniques. We also describe a number of applications which we foresee as standing to benefit from solutions to this problem, ranging from network troubleshooting and capacity provisioning to optimizing the layout of application-level overlay networks to optimized replica placement.",
Quotient FCMs-a decomposition theory for fuzzy cognitive maps,"In this paper, we introduce a decomposition theory for fuzzy cognitive maps (FCM). First, we partition the set of vertices of an FCM into blocks according to an equivalence relation, and by regarding these blocks as vertices we construct a quotient FCM. Second, each block induces a natural sectional FCM of the original FCM, which inherits the topological structure as well as the inference from the original FCM. In this way, we decompose the original FCM into a quotient FCM and some sectional FCM. As a result, the analysis of the original FCM is reduced to the analysis of the quotient and sectional FCM, which are often much smaller in size and complexity. Such a reduction is important in analyzing large-scale FCM. We also propose a causal algebra in the quotient FCM, which indicates that the effect that one vertex influences another in the quotient depends on the weights and states of the vertices along directed paths from the former to the latter. To illustrate the process involved, we apply our decomposition theory to university management networks. Finally, we discuss possible approaches to partitioning an FCM and major concerns in constructing quotient FCM. The results represented in this paper provide an effective framework for calculating and simplifying causal inference patterns in complicated real-world applications.","Fuzzy cognitive maps,
Computer science,
Software engineering,
Large-scale systems,
Algebra,
Intelligent networks,
Australia Council,
Mathematics,
Statistics,
State-space methods"
A generalized target-driven cache replacement policy for mobile environments,"Caching frequently accessed data items on the client side is an effective technique to improve system performance in a mobile environment. Due to cache size limitations, cache replacement algorithms are used to find a suitable subset of items for eviction from the cache. In this paper, we propose a generalized cost function for cache replacement algorithms for mobile environment. The distinctive feature of our cost function is that it is general and it can be used for various performance metrics by making the necessary changes. To demonstrate the practical effectiveness of the general cost function, we derive two specific functions to be evaluated by setting two different targets: minimizing the query delay and minimizing the downlink traffic. Detailed experiments are carried out to evaluate the proposed methodology. Compared to previous schemes, our algorithm significantly improves the performance in terms of query delay or in terms of bandwidth utilization depending on the targets.",
Protein folding in silico: an overview,"Understanding the mechanism that drives a protein into its unique, biologically active structure, and predicting this structure and the protein's corresponding function from knowledge about its amino acid sequence, is called the protein-folding problem. The inherent difficulties in solving experimentally a protein's tertiary structure only amplify the problem. Whereas it takes only hours to days to determine an amino acid sequence, for example, it would take months to years to discover its corresponding 3D shape by X-ray crystallography or nuclear magnetic resonance experiments. Equally challenging are experiments that explore folding process kinetics and dynamics. In short, efficient computational methods could help us tackle the protein-folding problem.","Proteins,
Biological system modeling,
Computational modeling,
Computer simulation,
Lattices,
Amino acids,
Humans,
Muscles,
Kinetic theory,
Monte Carlo methods"
Smart identification frameworks for ubiquitous computing applications,"We present our results in the conceptual design and the implementation of ubiquitous computing applications using smart identification technologies. First, we describe such technologies and their potential application areas, followed by an overview of some applications we have developed. Based on the experiences we gained from the development of these systems, we point out design concepts that we find useful for structuring and implementing such applications. Building upon these concepts, we have created two frameworks based on Jini (i.e., distributed Java objects) and Web services to support the development of ubiquitous computing applications that make use of smart identification technology. We describe our prototype frameworks, discuss the underlying concepts and present some lessons learned.","Ubiquitous computing,
Radiofrequency identification,
Application software,
Space technology,
Pervasive computing,
Joining processes,
Active RFID tags,
Tagging,
Passive RFID tags,
Computer science"
Deployment and dynamic reconfiguration planning for distributed software systems,"Initial deployment and subsequent dynamic reconfiguration of a software system is difficult because of the interplay of many interdependent factors, including cost, time, application state, and system resources. As the size and complexity of software systems increases, procedures (manual or automated) that assume a static software architecture and environment are becoming untenable. We have developed a novel technique for carrying out the deployment and reconfiguration planning processes that leverages recent advances in the field of temporal planning. We describe a tool called Planit, which manages the deployment and reconfiguration of a software system utilizing a temporal planner. Given a model of the structure of a software system, the network upon which the system should be hosted, and a goal configuration, Planit will use the temporal planner to devise possible deployments of the system. Given information about changes in the state of the system, network and a revised goal, Planit will use the temporal planner to devise possible reconfigurations of the system. We present the results of a case study in which Planit is applied to a system consisting of various components that communicate across an application-level overlay network.",
Camera calibration with known rotation,"We address the problem of using external rotation information with uncalibrated video sequences. The main problem addressed is, what is the benefit of the orientation information for camera calibration? It is shown that in case of a rotating camera the camera calibration problem is linear even in the case that all intrinsic parameters vary. For arbitrarily moving cameras the calibration problem is also linear but underdetermined for the general case of varying all intrinsic parameters. However, if certain constraints are applied to the intrinsic parameters the camera calibration can be computed linearly. It is analyzed which constraints are needed for camera calibration of freely moving cameras. Furthermore we address the problem of aligning the camera data with the rotation sensor data in time. We give an approach to align these data in case of a rotating camera.",
On the intrinsic locality properties of Web reference streams,"There has been considerable work done in the study of Web reference streams: sequences of requests for Web objects. In particular, many studies have looked at the locality properties of such streams, because of the impact of locality on the design and performance of caching and prefetching systems. However, a general framework for understanding why reference streams exhibit given locality properties has not yet emerged. In this paper we take a first step in this direction. We propose a framework for describing how reference streams are transformed as they pass through the Internet, based on three operations: aggregation, disaggregation, and filtering. We also propose metrics to capture the temporal locality of reference streams in this framework. We argue that these metrics (marginal entropy and interreference coefficient of variation) are more natural and more useful than previously proposed metrics for temporal locality; and we show that these metrics provide insight into the nature of reference stream transformations in the Web.",
Teaching requirements engineering through role playing: lessons learnt,"Requirements engineering (RE) has attracted a great deal of attention from researchers and practitioners in recent years. This increasing interest demands academia to provide software engineering students with a solid foundation in the subject matter. RE is a multi disciplinary and communication rich activity in software development. The RE tools, methods and techniques to date have drawn upon a variety of disciplines, and the requirements engineers are increasingly expected to be well versed with these disciplines. In RE education, it is thus imperative to cover a wide range of topics and teach a variety of skills that one needs to know and acquire to be able to perform effective requirements engineering. We argue that in teaching RE, we must cover three fundamental topics: the traditional analysis and modelling skills, interviewing and groupware skills for requirements elicitation, and writing skills for specifying requirements. We report on our experiences gained in teaching such a requirements engineering subject for the first time to students at University of Technology, Sydney. We used role playing as a pedagogical tool to give students a greater appreciation of the range of issues and problems associated with requirements engineering in real settings. We believe that the lessons learnt from this exercise will be valuable for those interested in advancing requirements engineering education and training.",
Constrained component deployment in wide-area networks using AI planning techniques,"Wide-area network applications are increasingly being built using component-based models, which enable integration of diverse functionality in modules distributed across the network. In such models, dynamic component selection and deployment enables an application to flexibly adapt to changing client and network characteristics, achieve load-balancing, and satisfy QoS requirements. Unfortunately, the problem of finding a valid component deployment is hard because one needs to decide on the set of components while satisfying various constraints resulting from application semantic requirements, network resource limitations, and interactions between the two. In this paper, we propose a general model for the component placement problem and present an algorithm for it, which is based on AI planning algorithms. We validate the effectiveness of our algorithm by demonstrating its scalability with respect to network size and number of components in the context of deployments generated for two example applications - a security-sensitive mail service, and a Webcast service - in a variety of network environments.","Intelligent networks,
Artificial intelligence,
Bandwidth,
Concurrent computing,
Distributed computing,
Computer science,
Application software,
Mathematical model,
Scalability,
Context-aware services"
Modeling scan chain modifications for scan-in test power minimization,,
Classification of Web documents using a graph model,In this paper we describe work relating to classification of Web documents using a graph-based model instead of the traditional vector-based model for document representation. We compare the classification accuracy of the vector model approach using the k-nearest neighbor (k-NN) algorithm to a novel approach which allows the use of graphs for document representation in the k-NN algorithm. The proposed method is evaluated on three different Web document collections using the leave-one-out approach for measuring classification accuracy. The results show that the graph-based k-NN approach can outperform traditional vector-based k-NN methods in terms of both accuracy and execution time.,
An enhanced multilevel algorithm for circuit placement,"This paper presents several important enhancements to the recently published multilevel placement package mPL. The improvements include (i) unconstrained quadratic relaxation on small, noncontiguous subproblems at every level of the hierarchy; (ii) improved interpolation (declustering) based on techniques from algebraic multigrid (AMG), and (iii) iterated V-cycles with additional geometric information for aggregation in subsequent V-cycles. The enhanced version of mPL, named mPL2, improves the total wirelength result by about 12% compared to the original version. The attractive scalability properties of the mPL run time have been largely retained, and the overall run time remains very competitive. Compared to GORDIAN-L-DOMINO on uniform-cell-size IBM/ISPD98 benchmarks, a speed-up of well over 8/spl times/ on large circuits (/spl ges/100,000 cells or nets) is obtained along with an average improvement in total wirelength of about 2%. Compared to Dragon on the same benchmarks, a speed-up of about 5/spl times/ is obtained at the cost of about 4% increased wirelength. On the recently published PEKO synthetic benchmarks, mPL2 generates surprisingly high-quality placements-roughly 60% closer to the optimal than those produced by Capo 8.5 and Dragon-in run time about twice as long as Capo's and about 1/10th of Dragon's.","Algorithm design and analysis,
Scalability,
Circuit analysis computing,
Timing,
Computer science,
Mathematics,
Design automation,
Packaging,
Interpolation,
Costs"
"Heart-surface reconstruction and ECG electrodes localization using fluoroscopy, epipolar geometry and stereovision: application to noninvasive imaging of cardiac electrical activity","To date there is no imaging modality for cardiac arrhythmias which remain the leading cause of sudden death in the United States (>300,000/yr.). Electrocardiographic imaging (ECGI), a noninvasive modality that images cardiac arrhythmias from body surface potentials, requires the geometrical relationship between the heart surface and the positions of body surface ECG electrodes. A photographic method was validated in a mannequin and used to determine the three-dimensional coordinates of body surface ECG electrodes to within 1 mm of their actual positions. Since fluoroscopy is available in the cardiac electrophysiology (EP) laboratory where diagnosis and treatment of cardiac arrhythmias is conducted, a fluoroscopic method to determine the heart surface geometry was developed based on projective geometry, epipolar geometry, point reconstruction, b-spline interpolation and visualization. Fluoroscopy-reconstructed hearts in a phantom and a human subject were validated using high-resolution computed tomography (CT) imaging. The mean absolute distance error for the fluoroscopy-reconstructed heart relative to the CT heart was 4 mm (phantom) and 10 mm (human). In the human, ECGI images of normal cardiac electrical activity on the fluoroscopy-reconstructed heart showed close correlation with those obtained on the CT heart. Results demonstrate the feasibility of this approach for clinical noninvasive imaging of cardiac arrhythmias in the interventional EP laboratory.",
An improved genetic algorithm with variable population-size and a PSO-GA based hybrid evolutionary algorithm,"This paper presents an improved genetic algorithm with variable population-size (VPGA) inspired by the natural features of the variable size of the population. Based on the VPGA and the particle swarm optimization (PSO) algorithms, this paper also proposes a novel hybrid approach called PSO-GA based hybrid evolutionary algorithm (PGBHEA). Simulations show that both VPGA and PGBHEA are effective for the optimization problem.","Genetic algorithms,
Evolutionary computation,
Particle swarm optimization,
Computational modeling,
Biological cells,
Educational institutions,
Computer science,
High performance computing,
Mathematics,
Data structures"
Regularization in tomographic reconstruction using thresholding estimators,"In tomographic medical devices such as single photon emission computed tomography or positron emission tomography cameras, image reconstruction is an unstable inverse problem, due to the presence of additive noise. A new family of regularization methods for reconstruction, based on a thresholding procedure in wavelet and wavelet packet (WP) decompositions, is studied. This approach is based on the fact that the decompositions provide a near-diagonalization of the inverse Radon transform and of prior information in medical images. A WP decomposition is adaptively chosen for the specific image to be restored. Corresponding algorithms have been developed for both two-dimensional and full three-dimensional reconstruction. These procedures are fast, noniterative, and flexible. Numerical results suggest that they outperform filtered back-projection and iterative procedures such as ordered-subset-expectation-maximization.",
"FEAT a tool for locating, describing, and analyzing concerns in source code","Developers working on existing programs repeatedly have to address concerns, or aspects, that are not well modularized in the source code comprising a system. In such cases, a developer has to first locate the implementation of the concern in the source code comprising the system, and then document the concern sufficiently to be able to understand it and perform the actual change task.",
"Grid Harvest Service: a system for long-term, application-level task scheduling","With the emergence of Grid computing environment, performance measurement, analysis and prediction of non-dedicated distributed systems have become increasingly important. In this study, we put forward a novel performance model for non-dedicated network computing. Based on this model, a performance prediction and task scheduling system called Grid Harvest Service (GHS), has been designed and implemented. GHS consists of a performance measurement component, a prediction component and a scheduling component. Different scheduling algorithms are proposed for different situations. Experimental results show that the GHS system provides satisfactory solution for performance prediction and scheduling of large applications and that GHS has a real potential.",
Introducing abuse frames for analysing security requirements,We are developing an approach using Jackson's Problem Frames to analyse security problems in order to determine security vulnerabilities. We introduce the notion of an anti-requirement as the requirement of a malicious user that can subvert an existing requirement. We incorporate anti-requirements into so-called abuse frames to represent the notion of a security threat imposed by malicious users in a particular problem context. We suggest how abuse frames can provide a means for bounding the scope of security problems in order to analyse security threats and derive security requirements.,
Multi-resolution state retrieval in sensor networks,"Large-scale dense sensor networks require mechanisms to extract topology information that can be used for various aspects of sensor network management. It is critical for any topology discovery algorithm in dense networks not only to adhere to the resource constraints of bandwidth and energy but also to provide several views of the network. Due to factors of density, redundancy and failures it may not be possible or practical to get a complete view of the topology. We describe a distributed parameterized algorithm for Sensor Topology Retrieval at Multiple Resolutions (STREAM), which makes a tradeoff between topology details and resources expended. The algorithm retrieves network state at multiple resolutions at a proportionate communication cost. We also define various classes of topology queries and show how the parameters in the algorithm can be used to support queries specific to sensor networks. We show that topology determined at different resolutions is sufficient for approximating different network properties. We also show that STREAM can be used for general-purpose multi-resolution information retrieval in sensor networks.",
An analytical formula for the mean differential group delay of randomly birefringent spun fibers,"Polarization-mode dispersion (PMD) is a serious impairment for high-bit-rate optical telecommunication systems. It is known that spinning the fiber during the drawing process drastically reduces the PMD. However, the analysis of pulse propagation through a randomly birefringent spun fiber is still at an early stage. In this paper, we derive an analytical formula for the mean differential group delay of a periodically spun fiber with random birefringence. We model the birefringence with fixed modulus and a random orientation under the condition that the spin period is shorter than the beat length. Finally, we numerically compare the analytical results with those given by the random-modulus model of birefringence, and we obtain good agreement as long as the short-period assumption is satisfied.","Delay,
Birefringence,
Optical fiber polarization,
Spinning,
Computer science,
Polarization mode dispersion,
Optical pulses,
Optical propagation,
Measurement techniques"
Detecting denial of service attacks using support vector machines,"The complexity, openness, and increasing accessibility of the Internet have all greatly increased the risk of information system security availability. A serious type of network attacks is Denial of Service (DoS), which is performed against an information system to prevent legitimate users from accessing the compromised system for service. This paper concerns detecting DoS attacks using Support Vector Machines (SVMs). The key idea is to train SVMs using already discovered patterns (signatures) that represent DoS attacks. Using a benchmark data from a KDD competition designed by DARPA (U.S. Defense Advanced Research Projects Agency), we demonstrate that highly efficient and accurate classifiers can be constructed by using SVMs to detect DoS attacks. Further, we also perform feature ranking of the DARPA intrusion data to identify the key features that are important to DoS detection.",
Regularization of flow streamlines in multislice phase-contrast MR imaging,"Magnetic resonance angiography (MRA) has become an important tool for the clinical evaluation of vascular disease. Flow measurement with phase-contrast (PC) magnetic resonance (MR) imaging provides a powerful method for evaluation of blood velocity information inside vessels. However, image artifacts from complex flow patterns including slow flow, recirculation zone, and pulsatile flow can adversely affect accuracy of results. In this paper, we introduce a new numerical formulation for improving the accuracy of PC velocity fields and corresponding streamlines, based on a physical constraint from fluid dynamics, within a regularization framework. The formulation which makes use of a stream function, automatically enforces continuity constraint of incompressible flow and reconstructs the flow streamlines from PC images. We applied the algorithm to complex MR imaging flow velocities obtained in a flow phantom of an axisymmetric abdominal aortic aneurysm. The algorithm significantly improved streamline results especially inside the recirculation zone, where artifacts are more pronounced. A velocity reconstruction method in primitive variable form is also presented and results are compared with the stream function method. In order to validate flow characteristics derived from PC MR images, we used the FLUENT computational fluid dynamics software package, to simulate flow patterns within the same geometry as our phantom. There was a good agreement between the numerical simulations and recovered PC streamline results. Processed streamlines, in both stream function and primitive variable methods, were more realistic and provided more precise flow patterns than unprocessed PC data. Additionally, the feasibility of the method was demonstrated in the aorta of a normal volunteer.","Streaming media,
Magnetic resonance imaging,
Magnetic resonance,
Imaging phantoms,
Angiography,
Diseases,
Fluid flow measurement,
Velocity measurement,
Phase measurement,
Blood"
An efficient filter for denial-of-service bandwidth attacks,"In this paper, we present an efficient method for detecting and filtering denial-of-service bandwidth attacks. Our system called TOPS (tabulated online packet statistics) can monitor a large number of network addresses in a compact, fixed-size structure using several effective heuristics. We demonstrate that TOPS can detect bandwidth attacks in a standard benchmark dataset with a high accuracy and a low false alarm rate. A key benefit of TOPS is that it uses few computational resources and does not slow down during an attack.","Computer crime,
Bandwidth,
Monitoring,
Information filtering,
Information filters,
Telecommunication traffic,
Statistics,
Network servers,
Protocols,
Computer science"
SVMSVM: support vector machine speaker verification methodology,Support vector machines with the Fisher and score-space kernels are used for text independent speaker verification to provide direct discrimination between complete utterances. This is unlike approaches such as discriminatively trained Gaussian mixture models or other discriminative classifiers that discriminate at the frame-level only. Using the sequence-level discrimination approach we are able to achieve error-rates that are significantly better than the current state-of-the-art on the PolyVar database.,
Source-end DDoS defense,"A successful source-end DDoS (distributed denial-of-service) defense enables early suppression of the attack and minimizes collateral damage. However, such an approach faces many challenges: (a) distributing the attack hinders detection; (b) defense systems must guarantee good service to legitimate traffic during the attack; and (c) deployment costs and false alarm levels must be sufficiently small and effectiveness must be high to provide deployment incentive. We discuss each of the challenges and describe one successful design of a source-end DDoS defense system-the D-WARD system. D-WARD was implemented in a Linux router. We include experimental results to illustrate D-WARD's performance.",
Precise dynamic slicing algorithms,"Dynamic slicing algorithms can greatly reduce the debugging effort by focusing the attention of the user on a relevant subset of program statements. In this paper we present the design and evaluation of three precise dynamic slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic slices. Our experiments show that the LP algorithm is a fast and practical precise slicing algorithm. In fact we show that while precise slices can be orders of magnitude smaller than imprecise dynamic slices, for small number of slicing requests, the LP algorithm is faster than an imprecise dynamic slicing algorithm proposed by Agrawal and Horgan.","Heuristic algorithms,
Debugging,
Computer science,
Costs,
Algorithm design and analysis,
Timing,
Sequential analysis,
Data analysis,
Computer aided instruction"
An immuno-fuzzy approach to anomaly detection,"This paper presents a new technique for generating a set of fuzzy rules that can characterize the non-self space (abnormal) using only self (normal) samples. Because, fuzzy logic can provide a better characterization of the boundary between normal and abnormal, it can increase the accuracy in solving the anomaly detection problem. Experiments with synthetic and real data sets are performed in order to show the applicability of the proposed approach and also to compare with other works reported in the literature.",
Elastic 3-D alignment of rat brain histological images,"A three-dimensional wavelet-based algorithm for nonlinear registration of an elastic body model of the brain is developed. Surfaces of external and internal anatomic brain structures are used to guide alignment. The deformation field is represented with a multiresolution wavelet expansion and is modeled by the partial differential equations of linear elasticity. A progressive estimation of the registration parameters and the usage of an adaptive distance map reduce algorithm complexity, thereby providing computational flexibility that allows mapping of large, high resolution datasets. The performance of the algorithm was evaluated on rat brains. The wavelet-based registration method yielded a twofold improvement over affine registration.",
A comparison of exact and approximate adjoint sensitivities in fluorescence tomography,"Many approaches to fluorescence tomography utilize some form of regularized nonlinear least-squares algorithm for data inversion, thus requiring repeated computation of the Jacobian sensitivity matrix relating changes in observable quantities, such as emission fluence, to changes in underlying optical parameters, such as fluorescence absorption. An exact adjoint formulation of these sensitivities comprises three terms, reflecting the individual contributions of 1) sensitivities of diffusion and decay coefficients at the emission wavelength, 2) sensitivities of diffusion and decay coefficients at the excitation wavelength, and 3) sensitivity of the emission source term. Simplifying linearity assumptions are computationally attractive in that they cause the first and second terms to drop out of the formulation. The relative importance of the three terms is thus explored in order to determine the extent to which these approximations introduce error. Computational experiments show that, while the third term of the sensitivity matrix has the largest magnitude, the second term becomes increasingly significant as target fluorophore concentration or volume increases. Image reconstructions from experimental data confirm that neglecting the second term results in overestimation of sensitivities and consequently overestimation of the value and volume of the fluorescent target, whereas contributions of the first term are so low that they are probably not worth the additional computational costs.",
Face alignment using statistical models and wavelet features,"Active shape model (ASM) is a powerful statistical tool for face alignment by shape. However, it can suffer from changes in illumination and facial expression changes, and local minima in optimization. In this paper, we present a method, W-ASM, in which Gabor wavelet features are used for modeling local image structure. The magnitude and phase of Gabor features contain rich information about the local structural features of face images to be aligned, and provide accurate guidance for search. To a large extent, this repairs defects in gray scale based search. An E-M algorithm is used to model the Gabor feature distribution, and a coarse-to-fine grained search is used to position local features in the image. Experimental results demonstrate the ability of W-ASM to accurately align and locate facial features.","Active shape model,
Active appearance model,
Facial animation,
Facial features,
Asia,
Lighting,
Computer vision,
Computer science,
Face recognition,
Active contours"
Wyner-Ziv coding based on TCQ and LDPC codes,"This paper considers TCQ and LDPC codes for the quadratic Gaussian Wyner-Ziv problem. After TCQ of the source input X, LDPC codes are used to implement Slepian-Wolf coding of the quantized source input Q(X) given the side information Y at the decoder. Assuming ideal Slepian-Wolf coding in the sense of achieving the theoretical limit H (Q(X)|Y), it is shown that Slepian-Wolf coded TCQ (SWC-TCQ) performs 0.2 dB away from the Wyner-Ziv distortion-rate function D/sub WZ/*(R) at high rate. This result mirrors that of entropy-coded TCQ in classic source coding and establishes the connection between performances of high-rate Wyner-Ziv coding and classic source coding. Practical designs with TCQ, irregular LDPC codes (for Slepian-Wolf coding) and optimal estimation at the decoder perform 0.83 dB away from D/sub WZ/*(R) at medium bit rates (e.g., /spl ges/ 2.3 b/s). With 2-D trellis-coded vector quantization, the performance gap to D/sub WZ/*(R) is only 0.66 dB at 1.0 b/s and 0.47 dB at 3.3 b/s.",
Isolated program execution: an application transparent approach for executing untrusted programs,"We present a new approach for safe execution of untrusted programs by isolating their effects from the rest of the system. Isolation is achieved by intercepting file operations made by untrusted processes, and redirecting any change operations to a ""modification cache"" that is invisible to other processes in the system. File read operations performed by the untrusted process are also correspondingly modified, so that the process has a consistent view of system state that incorporates the contents of the file system as well as the modification cache. On termination of the untrusted process, its user is presented with a concise summary of the files modified by the process. Additionally, the user can inspect these files using various software utilities (e.g., helper applications to view multimedia files) to determine if the modifications are acceptable. The user then has the option to commit these modifications, or simply discard them. Essentially, our approach provides ""play"" and ""rewind"" buttons for running untrusted software. Key benefits of our approach are that it requires no changes to the untrusted programs (to be isolated) or the underlying operating system; it cannot be subverted by malicious programs; and it achieves these benefits with acceptable runtime overheads. We describe a prototype implementation of this system for Linux called Alcatraz and discuss its performance and effectiveness.",
Enhancing the freshman and sophomore ECE student experience using a platform for learning/spl trade/,"Outcomes based assessment has shown that introducing a platform for learning/spl trade/ based on a robot referred to as TekBots/spl trade/ into the first two electrical and computer engineering (ECE) courses enhances students' sense of community, innovation capabilities, and troubleshooting skills. At Oregon State University, Corvallis, ECE students enhance their fundamental understanding of ECE concepts as they construct and build upon their individual robot (TekBots). They experience first-hand the fun associated with engineering while gaining a sense of accomplishment. This platform will eventually extend through the four-year curriculum so that, rather than a single point project, the robot serves as a platform that connects and integrates the content from course to course.","Electrical engineering education,
Computer science education,
Robots"
Computational methods for modeling parachute systems,"Using computational models in parachute system development can improve performance. For successful modeling, however, several challenges must be addressed, particularly the interaction between parachute structural dynamics and aerodynamics.",
Template based control of hexapedal running,"In this paper, we introduce a hexapedal locomotion controller that simulation evidence suggests will be capable of driving our RHex robot at speeds exceeding five body lengths per second with reliable stability and rapid maneuverability. We use a low dimensional passively compliant biped as a ""template"" - a control target for the alternating tripod gait of the physical machine. We impose upon the physical machine an approximate inverse dynamics within-stride controller designed to force the true high dimensional system dynamics down onto the lower dimensional subspace corresponding to the template. Numerical simulations suggest the presence of asymptotically stable running gaits with large basins of attraction. Moreover, this controller improves substantially the maneuverability and dynamic range of RHex's running behaviors relative to the initial prototype open-loop algorithms.","Leg,
Robot kinematics,
Morphology,
Control systems,
Prototypes,
Reliability engineering,
Computer science,
Computational modeling,
Computer simulation,
Stability"
Classification rule discovery with ant colony optimization,"Ant-based algorithms or ant colony optimization (ACO) algorithms have been applied successfully to combinatorial optimization problems. More recently, Parpinelli and colleagues applied ACO to data mining classification problems, where they introduced a classification algorithm called Ant/spl I.bar/Miner. In this paper, we present an improvement to Ant/spl I.bar/Miner (we call it Ant/spl I.bar/Miner3). The proposed version was tested on two standard problems and performed better than the original Ant/spl I.bar/Miner algorithm.","Ant colony optimization,
Data mining,
Databases,
Delta modulation,
Humans,
Artificial intelligence,
Particle swarm optimization,
Intelligent agent,
Educational institutions,
Computer science"
A practical approach of teaching Software Engineering,"In today's software industry a software engineer is not only expected to successfully cope with technical challenges, but also to deal with non-technical issues arising from difficult project situations. These issues typically include understanding the customer's domain and requirements, working in a team, organizing the division of work, and coping with time pressure and hard deadlines. Thus, in our opinion teaching Software Engineering, (SE) not only requires studying theory using text books, but also providing students with the experience of typical non-technical issues in a software project. This article reports experiences with the concept of a course focusing on providing practical know-how.","Education,
Software engineering,
Computer industry,
Books,
Software prototyping,
IEEE news,
Organizing,
Programming,
Project management,
Maintenance engineering"
"RRTs for nonlinear, discrete, and hybrid planning and control","In this paper, we describe a planning and control approach in terms of sampling using Rapidly-exploring Random Trees (RRTs), which were introduced by LaValle. We review RRTs for motion planning and show how to use them to solve standard nonlinear control problems. We extend them to the case of hybrid systems and describe our modifications to LaValle's Motion Strategy Library to allow for hybrid motion planning. Finally, we extend them to purely discrete spaces (using heuristic evaluation as a distance metric) and provide computational experiments comparing them to conventional methods, such as A.","Path planning,
Search methods,
State-space methods,
Sampling methods,
Motion control,
Dynamic programming,
Computer science,
Libraries,
Strategic planning,
Extraterrestrial measurements"
"Dynamic, adaptive and reconfigurable systems overview and prospective vision","Systems are more and more expected to work in dynamic environment, to deal with fluctuation of their characteristics and to guaranty functional and nonfunctional requirements. Systems should also keep compliant with the contracted quality of service. Moreover, when necessary, services and aspects should be added or removed on line. In this paper, we overview major approaches to deploy, reconfigure and adapt applications and underlying software platforms. These changes should be realized according to the evolving context and execution environment so that systems can stay compliant with the specifications and requirements of the application. We end this position paper with a vision on future directions and developments.","Adaptive systems,
Fluctuations,
Application software,
Fault tolerant systems,
Safety,
Computer science,
Quality of service,
Software systems,
Distributed processing,
Protocols"
Optimal fractional movement-based scheme for PCS location management,"In this letter, we propose a fractional movement-based location update scheme for personal communication service networks. Similar to the well-known fractional guard channel scheme for channel assignments, in our proposed scheme, the movement threshold is a real number with a fraction instead of an integer. We prove analytically that there is a unique optimal fractional movement threshold that minimizes the total cost of location updates and paging per call arrival.","Personal communication networks,
Databases,
Mobile radio mobility management,
Cost function,
Paging strategies,
Performance analysis,
GSM,
Land mobile radio cellular systems,
Computer science"
A feeling estimation system using a simple electroencephalograph,"This paper proposes a system for feeling estimation. It uses a simple electroencephalograph as an interface. If quantitative feeling presumption with information from a living body is made possible, it can be considered that a computer can appropriately deal with human's natural sensibility. Input data of proposed system is electroencephalogram (EEG) data of several feelings acquired using a simple electroencephalograph. Output of the system is one of some words associated with certain feelings states. One feature of proposed system is performing classification using the neural network. The neural networks have generalization ability and can deal with data including noise. Another feature is a low burden for the user by using a simple electroencephalograph. The proposed system is applicable to the quantitative evaluation replaced with a questionnaire. It is also applicable to handling the non-verbal information that is missing at communication.","Biological neural networks,
Humans,
Electroencephalography,
Neural networks,
Information analysis,
Information processing,
Computer science,
Wavelet transforms,
Intelligent networks,
Information retrieval"
Context attributes: an approach to enable context-awareness for service discovery,"The service discovery problem has attracted a lot of attention from researchers and practitioners. Jini, SLP, and UPnP are among the few emerging service discovery protocols. Although they seem to provide a good solution to the problem, there is an unaddressed need of more sophisticated location and context-aware service selection support. In this paper, we introduce the concept of context attribute as an effective, flexible means to exploit relevant context information during the service discovery process. Context attributes can express context information including service-specific selection logic, client, and network condition. We describe our approach and implementation, and present experimental results of our context-aware service discovery implementation.","Context-aware services,
Mobile computing,
Portable computers,
Personal digital assistants,
Wearable computers,
Access protocols,
Information science,
Engineering drawings,
Logic,
Smart phones"
Limits on the accuracy of 3-D thickness measurement in magnetic resonance images-effects of voxel anisotropy,"Measuring the thickness of sheet-like thin anatomical structures, such as articular cartilage and brain cortex, in three-dimensional (3-D) magnetic resonance (MR) images is an important diagnostic procedure. This paper investigates the fundamental limits on the accuracy of thickness determination in MR images. We defined thickness here as the distance between the two sides of boundaries measured at the subvoxel resolution, which are the zero-crossings of the second directional derivatives combined with Gaussian blurring along the normal directions of the sheet surface. Based on MR imaging and computer postprocessing parameters, characteristics for the accuracy of thickness determination were derived by a theoretical simulation. We especially focused on the effects of voxel anisotropy in MR imaging with variable orientation of sheet-like structure. Improved and stable accuracy features were observed when the standard deviation of Gaussian blurring combined with thickness determination processes was around /spl radic/2/2 times as large as the pixel size. The relation between voxel anisotropy in MR imaging and the range of sheet normal orientation within which acceptable accuracy is attainable was also clarified, based on the dependences of voxel anisotropy and the sheet normal orientation obtained by numerical simulations. Finally, in vitro experiments were conducted using an acrylic plate phantom and a resected femoral head to validate the results of theoretical simulation. The simulated thickness was demonstrated to be well-correlated with the actual in vitro thickness.",
Design a PID controller for active queue management,"As an enhancement mechanism for the end-to-end congestion control, active queue management (AQM) can keep smaller queuing delay and higher throughput by purposefully dropping the packets at the intermediate nodes. Comparing with RED algorithm, although the PI (proportional-integral) controller for AQM designed by C. Hollot improves the stability, the transient performance of the PI controller is not perfect, such as the regulating time is too long. In order to overcome this drawback, in this paper, the PID (proportional-integral-differential) controller is proposed to speed up the responsiveness of AQM system. The controller parameters are tuned based on the determined gain and phase margins. The simulation results show that the integrated performance of the PID controller is obviously superior to that of the PI controller.","Three-term control,
Control systems,
Fluid flow control,
Algorithm design and analysis,
Nonlinear control systems,
Control theory,
Size control,
Communication system control,
Computer science,
Appropriate technology"
Dynamic line integral convolution for visualizing streamline evolution,"The depiction of time-dependent vector fields is a central problem in scientific visualization. This article describes a technique for generating animations of such fields where the motion of the streamlines to be visualized is given by a second ""motion"" vector field. Each frame of our animation is a line integral convolution of the original vector field with a time-varying input texture. The texture is evolved according to the associated motion vector field via an automatically adjusted set of random particles. We demonstrate this technique with examples from electromagnetism.","Aerodynamics,
Convolution,
Visualization,
Animation,
Spatial resolution,
Streaming media,
Electromagnetic fields,
Displays,
Image generation,
Image resolution"
Evaluating IPv4 to IPv6 transition mechanisms,"The next-generation Internet protocol, initially known as IP next generation (IPng), and then later IPv6, has been developed by the Internet Engineering Task Force (IETF) to replace the current Internet protocol (also known as IPv4). To enable the integration of IPv6 into current networks, several transition mechanisms have been proposed by the IETF IPng Transition Working Group. Two transition mechanisms are examined and empirically evaluated, namely 6-over-4, and IPv6 in IPv4 tunneling, as they relate to the performance of IPv6. The impact of these approaches are explored on end-to-end user application performance using metrics such as throughput, latency, host CPU utilization, TCP connection time, and the number of TCP connections per second that a client can establish with a remote server. All experiments were conducted using two duals stack (IPv4/IPv6) routers and two end-stations running Windows 2000, loaded with a dual IPv4/IPv6 stack.","Tunneling,
Protocols,
Encapsulation,
Computer science,
Internet,
Intelligent networks,
Throughput,
Delay,
Performance evaluation,
Testing"
Individuality of handwritten characters,,"Forensics,
Feature extraction,
Text analysis,
Authentication,
Image analysis,
Character recognition,
Handwriting recognition,
Computer science,
Fourier transforms,
Writing"
"A high performance, low complexity algorithm for compile-time job scheduling in homogeneous computing environments","Efficient job scheduling is one of the most important and difficult issues in homogeneous computing environments. List-scheduling is generally accepted as an attractive static approach, since it pairs low complexity with good results. This paper presents a static list-scheduling algorithm with a limited number of processors. The algorithm is called critical nodes parent trees (CNPT). The aim of the algorithm is to give results comparable to or better than the current algorithms, and to achieve very low complexity. The experimental work has shown that the suggested algorithm gave comparable results in a low complexity.","Scheduling algorithm,
Processor scheduling,
High performance computing,
Clustering algorithms,
Computer networks,
Distributed computing,
Costs,
Computer science,
Digital communication,
High-speed networks"
Using specularities for recognition,"Recognition systems have generally treated specular highlights as noise. We show how to use these highlights as a positive source of information that improves recognition of shiny objects. This also enables us to recognize very challenging shiny transparent objects, such as wine glasses. Specifically, we show how to find highlights that are consistent with a hypothesized pose of an object of known 3D shape. We do this using only a qualitative description of highlight formation that is consistent with most models of specular reflection, so no specific knowledge of an object's reflectance properties is needed. We first present a method that finds highlights produced by a dominant compact light source, whose position is roughly known. We then show how to estimate the lighting automatically for objects whose reflection is part specular and part Lambertian. We demonstrate this method for two classes of objects. First, we show that specular information alone can suffice to identify objects with no Lambertian reflectance, such as transparent wine glasses. Second, we use our complete system to recognize shiny objects, such as pottery.",
An NP decision procedure for protocol insecurity with XOR,"We provide a method for deciding the insecurity of cryptographic protocols in presence of the standard Dolev-Yao intruder (with a finite number of sessions) extended with so-called oracle rules, i.e., deduction rules that satisfy certain conditions. As an instance of this general framework, we ascertain that protocol insecurity is in NP for an intruder that can exploit the properties of the XOR operator. This operator is frequently used in cryptographic protocols but cannot be handled in most protocol models. An immediate consequence of our proof is that checking whether a message can be derived by an intruder (using XOR) is in P. We also apply our framework to an intruder that exploits properties of certain encryption modes such as cipher block chaining (CBC).","Cryptographic protocols,
Cryptography,
Computer science,
Security,
Abstracts,
Authentication,
Joining processes,
Algorithm design and analysis,
Equations,
Logic"
Performance modelling of distributed e-business applications using Queuing Petri Nets,"In this paper we show how Queuing Petri Net (QPN) models can be exploited for performance analysis of distributed e-business systems. We study a real-world application, and demonstrate the benefits, in terms of modelling power and expressiveness, that QPN models provide over conventional modelling paradigms such as Queuing Networks and Petri Nets. As shown, QPNs facilitate the integration of both hardware and software aspects of system behavior in the same model. In addition to hardware contention and scheduling strategies, using QPNs one can easily model simultaneous resource possession, synchronization, blocking and contention for software resources. By validating the models presented through measurements, we show that they are not just powerful as a specification mechanism, but are also very powerful as a performance analysis and prediction tool. However, currently available tools and techniques for QPN analysis are limited. Improved solution methods, which enable larger models to be analyzed, need to be developed. By demonstrating the power of QPNs as a modelling paradigm in realistic scenarios, we hope to motivate further research in this area.",
Autonomous replication for high availability in unstructured P2P systems,"We consider the problem of increasing the availability of shared data in peer-to-peer systems. In particular, we conservatively estimate the amount of excess storage required to achieve a practical availability of 99.9% by studying a decentralized algorithm that only depends on a modest amount of loosely synchronized global state. Our algorithm uses randomized decisions extensively together with a novel application of an erasure code to tolerate autonomous peer actions as well as staleness in the loosely synchronized global state. We study the behavior of this algorithm in three distinct environments modeled on previously reported measurements. We show that while peers act autonomously, the community as a whole will reach a stable configuration. We also show that space is used fairly and efficiently, delivering three times availability at a cost of six times the storage footprint of the data collection when the average peer availability is only 24%.","Availability,
Peer to peer computing,
Extraterrestrial measurements,
Computer science,
State estimation,
Costs,
Time sharing computer systems,
Internet,
Video sharing,
File systems"
Flexible ligand docking using differential evolution,"Molecular docking of biomolecules is becoming an increasingly important part in the process of developing new drugs, as well as searching compound databases for promising drug candidates. The docking of ligands to proteins can be formulated as an optimization problem where the task is to find the most favorable energetic conformation among the large space of possible protein-ligand complexes. Stochastic search methods, such as evolutionary algorithms (EAs), can be used to sample large search spaces effectively and is one of the preferred methods for flexible ligand docking. The differential evolution algorithm (DE) is applied to the docking problem using the AutoDock program. The introduced DockDE algorithm is compared with the Lamarckian GA (LGA) provided with AutoDock, and the DockEA previously found to outperform the LGA. The comparison is performed on a suite of six commonly used docking problems. In conclusion, the introduced DockDE outperformed the other algorithms on all problems. Further, the DockDE showed remarkable performance in terms of convergence speed and robustness regarding the found solution.","Proteins,
Drugs,
Evolutionary computation,
Genetic mutations,
Computer science,
Databases,
Stochastic processes,
Evolution (biology),
Robustness,
Biology computing"
Phase tracking and prediction,,"Runtime,
Computer architecture,
Phase detection,
Hardware,
Computer science,
Steady-state,
Design optimization,
Energy management,
Large-scale systems,
Phase measurement"
Optimality and scalability study of existing placement algorithms,"Placement is an important step in the overall IC design process in DSM technologies, as it defines the on-chip interconnects, which have become the bottleneck in determining circuit performance. The rapidly increasing design complexity, combined with the demand for the capability of handling nearly flattened designs for physical hierarchy generation, poses significant challenges to existing placement algorithms. There are very few studies on understanding the optimality and scalability of placement algorithms, due to the limited sizes of existing benchmarks and limited knowledge of optimal solutions. The contribution of this paper includes two parts: (1) we implemented an algorithm for generating synthetic benchmarks that have known optimal wirelengths and can match any given net distribution vector; (2) using benchmarks of 10K to 2M placeable modules with known optimal solutions, we studied the optimality and scalability of three state-of-the-art placers, Dragon (M. Wang et al, Proc. Int. Conf. on CAD, pp. 260-264, 2000), Capo (A.E. Caldwell et al, Proc. Design Automation Conf., pp. 477-482, 2000), mPL (K. Sze, 2002) from academia, and one leading edge industrial placer, QPlace from Cadence. For the first time our study reveals the gap between the results produced by these tools versus true optimal solutions. The wirelengths produced by these tools are 1.66 to 2.53 times the optimal in the worst cases, and are 1.46 to 2.38 times the optimal on the average. As for scalability, the average solution quality of each tool deteriorates by an additional 4% to 25% when the problem size increases by a factor of 10. These results indicate significant room for improvement in existing placement algorithms.","Scalability,
Algorithm design and analysis,
Iterative algorithms,
Process design,
Integrated circuit interconnections,
Circuit optimization,
Partitioning algorithms,
Logic design,
Computer science,
Iterative methods"
Frequent sub-structure-based approaches for classifying chemical compounds,"We study the problem of classifying chemical compound datasets. We present a substructure-based classification algorithm that decouples the substructure discovery process from the classification model construction and uses frequent subgraph discovery algorithms to find all topological and geometric substructures present in the dataset. The advantage of our approach is that during classification model construction, all relevant substructures are available allowing the classifier to intelligently select the most discriminating ones. The computational scalability is ensured by the use of highly efficient frequent subgraph discovery algorithms coupled with aggressive feature selection. Our experimental evaluation on eight different classification problems shows that our approach is computationally scalable and on the average, outperforms existing schemes by 10% to 35%.","Chemical compounds,
Biology computing,
High temperature superconductors,
Computer science,
Classification algorithms,
Solid modeling,
Computational intelligence,
Scalability,
Computer displays,
Drugs"
Characteristics of I/O traffic in personal computer and server workloads,"Understanding the characteristics of I/O traffic is increasingly important as the performance gap between the processor and disk-based storage continues to widen. Moreover, recent advances in technology, coupled with market demands, have led to new and exciting developments in storage systems, particularly network storage, storage utilities, and intelligent self-optimizing storage. In this paper, we empirically examine the physical I/O traffic of a wide range of server and personal computer (PC) workloads, focusing on how these workloads will be affected by the recent developments in storage systems. As part of our analysis, we compare our results with historical data and re-examine some rules of thumb (e.g., one bit of I/O per second for each instruction per second of processing power) that have been widely used for designing computer systems. We find that the I/O traffic is bursty and appears to exhibit self-similar characteristics. Our analysis also indicates that there is little cross-correlation between traffic volumes of server workloads, which suggests that aggregating these workloads will likely help to smooth out the traffic and enable more efficient utilization of resources. We discover that there is significant potential for harnessing “free” system resources to perform background tasks such as optimization of disk block layout. In general, we observe that the characteristics of the I/O traffic are relatively insensitive to the extent of upstream caching, and thus our results still apply, on a qualitative level, when the upstream cache is increased in size.",
Illumination chromaticity estimation using inverse-intensity chromaticity space,"Existing color constancy methods cannot handle both uniform colored surfaces and highly textured surfaces in a single integrated framework. Statistics-based methods require many surface colors, and become error prone when there are only few surface colors. In contrast, dichromatic-based methods can successfully handle uniformly colored surfaces, but cannot be applied to highly textured surfaces since they require precise color segmentation. In this paper, we present a single integrated method to estimate illumination chromaticity from single/multi-colored surfaces. Unlike the existing dichromatic-based methods, the proposed method requires only rough highlight regions, without segmenting the colors inside them. We show that, by analyzing highlights, a direct correlation between illumination chromaticity and image chromaticity can be obtained. This correlation is clearly described in ""inverse-intensity chromaticity space"", a new two-dimensional space we introduce. In addition, by utilizing the Hough transform and histogram analysis in this space, illumination chromaticity can be estimated robustly, even for a highly textured surface. Experimental results on real images show the effectiveness of the method.","Lighting,
Surface texture,
Rough surfaces,
Surface roughness,
Color,
Colored noise,
Computer science,
Computer errors,
Image segmentation,
Image analysis"
Microarchitecture evaluation with physical planning,"Conventionally, microarchitecture designs are mainly guided by the maximum throughput (measured as IPC) and fail to evaluate the impact of architectural decisions on the physical design, and in particular, the impact on the interconnects. In this paper, we propose MEVA, a system to consider both IPC and cycle time in the design space search for a given microarchitectural design. MEVA can consider a variety of user-specified architectural alternatives that trade IPC and cycle time in the design, and performs accurate floorplanning and simulation to fully evaluate each alternative. The resulting solution will maximize the benefit from both IPC and cycle time to provide a better solution than a design space exploration based simply on IPC or cycle time alone. For a sample architectural design, we are able to search a space of 32 architectural configurations with physical planning in less than 2 hours to find a processor configuration that, in terms of BIPS, outperforms the configuration with the best IPC performance by 14%, and the configuration with the fastest clock by 27%. This initial exploration only considers the boundary cases of a much larger design space, but still features substantial IPC and cycle time variation.","Microarchitecture,
Throughput,
Hardware,
Delay,
Space exploration,
Clocks,
Permission,
Time measurement,
Computer science,
Particle measurements"
CLUSEQ: efficient and effective sequence clustering,"Analyzing sequence data has become increasingly important recently in the area of biological sequences, text documents, Web access logs, etc. We investigate the problem of clustering sequences based on their sequential features. As a widely recognized technique, clustering has proven to be very useful in detecting unknown object categories and revealing hidden correlations among objects. One difficulty that prevents clustering from being performed extensively on sequence data (in categorical domain) is the lack of an effective yet efficient similarity measure. Therefore, we propose a novel model (CLUSEQ) for sequence cluster by exploring significant statistical properties possessed by the sequences. The conditional probability distribution (CPD) of the next symbol given a preceding segment is derived and used to characterize sequence behavior and to support the similarity measure. A variation of the suffix tree, namely probabilistic suffix tree, is employed to organize (the significant portion of) the CPD in a concise way. A novel algorithm is devised to efficiently discover clusters with high quality and is able to automatically adjust the number of clusters to its optimal range via a unique combination of successive new cluster generation and cluster consolidation. The performance of CLUSEQ has been demonstrated via extensive experiments on several real and synthetic sequence databases.","Computer science,
Data analysis,
Biological information theory,
Data mining,
Protein sequence,
Amino acids,
Biology,
Object detection,
Performance evaluation,
Probability distribution"
Theories of learning: a computer game perspective,"Computer games provide a good environment for learning. Players learn to play the game without being taught didactically as the learning process takes place naturally in the virtual world. Learning is no longer a process of knowledge transfer from the expert to the novice. Learners need to construct the knowledge themselves by interacting with the environment. It is beneficial to study the theory underpinning computer games: how players learn and respond in the game environment. We elucidate the theories of learning, i.e. behavioural learning theory, cognitive learning theory and motivation theory in the context of computer games. Psychology provides a way to understand the learning that occurs naturally in games and also helps in developing an environment in which the player can learn a particular domain of knowledge extrinsically. By studying the psychology and its relation to computer games, we can understand players more comprehensively, and thus predict their responses. The understanding of psychology offers a framework to developing an educational game that promotes learning while maintaining high player motivation. We also attempt to shed some light on how players learn in computer games based on the theory, and thus infer better techniques in supporting game-based learning.","Game theory,
Psychology,
Knowledge transfer,
Application software,
Computer applications,
Fires,
Software engineering"
The universal virus database ICTVdB,The International Committee on Taxonomy of Viruses database is a universally available taxonomic research tool for understanding relationships among all viruses. ICTVdB's fundamental goals are to provide researchers with precise virus identification and to link the agreed taxonomy to sequence databases.,
E-government: a special case of ICT-enabled business process change,"The literature on business process reengineering (BPR) has evolved into a strand of literature which studies organizational change (OC), and more specifically, business process change (BPC), induced and enabled by information and communication technology (ICT). With the unfolding of electronic government (e-government) changes to the way government works also seem to be imminent. Electronic government increasingly impacts business processes and workflows in the public sector. The BPC/ICT research, hence, has the capacity to directly inform both the research and practice of electronic government. In this paper, the findings of the BPC/ICT literature are reviewed and discussed regarding their applicability to electronic government. Both the theory and preliminary empirical evidence suggest that electronic government must be seen as a special case of ICT-enabled business process change.",
Requirements analysis for customizable software: a goals-skills-preferences framework,"Software customization has been argued to benefit both the productivity of software engineers and end users. However, most customization methods rely on specialists to manually tweak individual applications for a specific user group. Existing software development methods also fail to acknowledge the importance of different kinds of user skills and preferences and how these might be incorporated into a customizable software design. We propose a framework for performing requirements analysis on user goals, skills, and preferences in order to generate a customizable software design. We illustrate our methodology with an email system and review an on-going case study involving users with traumatic brain injury.","Brain injuries,
Computer science,
Application software,
Programming,
Human computer interaction,
Artificial intelligence,
Productivity,
Performance analysis,
Software design,
Telephony"
Kernel-Based Nonlinear Blind Source Separation,"We propose kTDSEP, a kernel-based algorithm for nonlinear blind source separation (BSS). It combines complementary research fields: kernel feature spaces and BSS using temporal information. This yields an efficient algorithm for nonlinear BSS with invertible nonlinearity. Key assumptions are that the kernel feature space is chosen rich enough to approximate the nonlinearity and that signals of interest contain temporal information. Both assumptions are fulfilled for a wide set of real-world applications. The algorithm works as follows: First, the data are (implicitly) mapped to a high (possibly infinite)—dimensional kernel feature space. In practice, however, the data form a smaller submanifold in feature space—even smaller than the number of training data points—a fact that has already been used by, for example, reduced set techniques for support vector machines. We propose to adapt to this effective dimension as a preprocessing step and to construct an orthonormal basis of this submanifold. The latter dimension-reduction step is essential for making the subsequent application of BSS methods computationally and numerically tractable. In the reduced space, we use a BSS algorithm that is based on second-order temporal decorrelation. Finally, we propose a selection procedure to obtain the original sources from the extracted nonlinear components automatically. Experiments demonstrate the excellent performance and efficiency of our kTDSEP algorithm for several problems of nonlinear BSS and for more than two sources.",
A mobile-agent-based PC grid,"This paper proposes a mobile-agent-based middleware that benefits remote computer users who wish to mutually offer their desktop computing resource to other Internet group members while their computers are not being used. Key to this resource exchange grid is the use of mobile agents. Each agent represents a client user, carries his/her job requests, searches for resources available for the request, executes the job at suitable computers, and migrates it to others when the current ones have become unavailable for use. All the features of job migration will be encapsulated in a user program wrapper that is implemented on Java layer between a mobile agent and the corresponding user program. The wrapper maintains the complete execution state of the user program, is carried by the mobile agent upon a job migration, and restores its user program its destination. For this purpose, a user program is preprocessed with JavaCC and ANTLR to include check-pointing functions before its execution. These functions periodically save the execution state of a user program into its corresponding program wrapper, which can thus be carried by an agent smoothly.","Grid computing,
Mobile agents,
Internet,
Computer science,
Middleware,
Java,
Computer networks,
Conferences,
Software systems,
Supercomputers"
An e-whiteboard application to support early design-stage sketching of UML diagrams,"We describe a Unified Modelling Language (UML) diagramming tool that uses an e-whiteboard, pen-based sketching interface to support collaborative design. Our tool allows designers to sketch UML visual modelling language constructs, mixing different UML diagram components, free-hand annotations and hand-written text. A key novelty of our approach is the preservation of hand-drawn diagrams and support for manipulation of the diagrams using pen-based actions. UML sketches can be ""formalized"" to computer-recognised and drawn diagrams, and exported to a 3rd party CASE tool.","Unified modeling language,
Computer aided software engineering,
Collaborative tools,
Collaborative work,
Computer displays,
Software standards,
Software design,
Application software,
Computer science,
Scheduling"
An admission control strategy for differentiated services in IEEE 802.11,"With the provisioning of high-speed wireless LAN (WLAN) environments, traffic classes (e.g., VoIP or video-conference) with different QoS requirements will be introduced in future WLANs. The IEEE 802.11e draft is currently standardizing a distributed access approach, called the enhanced distributed coordination function (EDCF), to support service differentiation in the MAC layer. However, since each mobile station transmits data packets egotistically in a distributed environment, the QoS requirement of each traffic class may not be guaranteed. In this paper, we develop an admission control strategy to guarantee the QoS requirement of each traffic class. In order to provide a criterion for admission decision, we introduce an analytical model for EDCF to evaluate the expected bandwidth and the expected packet delay of each traffic class. The admission control strategy uses the performance measures derived from the analytical model to decide if a new traffic stream is permitted into the system. We validate the accuracy of the analytical model by using the ns-2 simulator. Some performance evaluations are also demonstrated to illustrate the effect of the proposed admission control strategy.","Admission control,
Analytical models,
Traffic control,
Bandwidth,
Delay,
Computer science,
Wireless LAN,
Media Access Protocol,
Access protocols,
Videoconference"
Learning objects on the semantic Web,An important issue in reusing learning objects on the semantic Web is the development of appropriate technology to facilitate the discovery and reuse of learning objects stored in global and local repositories. Another issue is the development of ontologies for marking up the structure of learning objects and ascribing pedagogical meaning to them so that they can be understandable by machines. A third issue is making learning objects smarter so that they can perform a more meaningful role on the semantic Web. We discuss these and other issues as they affect the exploitation of learning objects on the semantic Web.,"Semantic Web,
Ontologies,
XML,
Electronic learning,
Computer science,
Resource description framework,
Packaging,
Mathematics,
Laboratories,
Appropriate technology"
Diverse routing of scheduled lightpath demands in an optical transport network,"This article addresses the problem of defining working and protection paths for scheduled lightpath demands (SLDs) in an optical transport network. An SLD is a demand for a set of lightpaths (connections), defined by a tuple (s, d, n, /spl alpha/, /spl omega/), where s and d are the source and destination nodes of the lightpaths, n is the number of requested lightpaths and /spl alpha/, /spl omega/ are the set-up and tear-down dates of the lightpaths. The problem is formulated as a combinatorial optimization problem where the objective is to minimize the number of channels required to instantiate the lightpaths. Two techniques are used to achieve this goal: channel reuse and backup-multiplexing. The former consists of assigning the same channel (either working or spare) to several lightpaths, provided that these lightpaths are not simultaneous in time. The latter consists of sharing a spare channel among multiple lightpaths. A spare channel cannot be shared if two conditions hold: a) the working paths of these lightpaths have at least one span in common and b) these lightpaths are simultaneous in time. In the other cases, the spare channel can be shared. We propose a simulated annealing (SA) based algorithm to find approximate solutions to this optimization problem since finding exact solutions is computationally intractable. The results show that backup-multiplexing improves the utilization of channels but requires significant computing capacity. Under a fixed computing capacity budget, the technique is useful in cases where there is little time disjointness among SLDs.","Routing,
Intelligent networks,
Optical fiber networks,
Telecommunication traffic,
Superluminescent diodes,
Processor scheduling,
Protection,
Computational modeling,
Simulated annealing,
Computer science"
Volume registration using needle paths and point landmarks for evaluation of interventional MRI treatments,"We created a method for three-dimensional (3-D) registration of medical images (e.g., magnetic resonance imaging (MRI) or computed tomography) to images of physical tissue sections or to other medical images and evaluated its accuracy. Our method proved valuable for evaluation of animal model experiments on interventional-MRI guided thermal ablation and on a new localized drug delivery system. The method computes an optimum set of rigid body registration parameters by minimization of the Euclidean distances between automatically chosen correspondence points, along manually selected fiducial needle paths, and optional point landmarks, using the iterative closest point algorithm. For numerically simulated experiments, using two needle paths over a range of needle orientations, mean voxel displacement errors depended mostly on needle localization error when the angle between needles was at least 20/spl deg/. For parameters typical of our in vivo experiments, the mean voxel displacement error was <0.35 mm. In addition, we determined that the distance objective function was a useful diagnostic for predicting registration quality. To evaluate the registration quality of physical specimens, we computed the misregistration for a needle not considered during the optimization procedure. We registered an ex vivo sheep brain MR volume with another MR volume and tissue section photographs, using various combinations of needle and point landmarks. Mean registration error was always /spl les/0.54 mm for MR-to-MR registrations and /spl les/0.52 mm for MR to tissue section registrations. We also applied the method to correlate MR volumes of radio-frequency induced thermal ablation lesions with actual tissue destruction. In this case, in vivo rabbit thigh volumes were registered to photographs of ex vivo tissue sections using two needle paths. Mean registration errors were between 0.7 and 1.36 mm over all rabbits, the largest error less than two MR voxel widths. We conclude that our method provides sufficient spatial correspondence to facilitate comparison of 3-D image data with data from gross pathology tissue sections and histology.","Needles,
Magnetic resonance imaging,
Biomedical imaging,
Medical diagnostic imaging,
In vivo,
Rabbits,
Computed tomography,
Animals,
Drug delivery,
Minimization methods"
Automatically inferring concern code from program investigation activities,"When performing a program evolution task, developers typically spend a significant amount of effort investigating and reinvestigating source code. To reduce this effort, we propose a technique to automatically infer the essence of program investigation activities as a set of concern descriptions. The concern descriptions produced by our technique list methods and fields of importance in the context of the investigation of an object-oriented system. A developer can rely on this information to perform the change task at hand, or at a later stage for a change that involves the same concerns. The technique involves applying an algorithm to a transcript of a program investigation session. The transcript lists which pieces of source code were accessed by a developer when investigating a program and how the different pieces of code were accessed. We applied the technique to data obtained from program investigation activities for five subjects involved in two different program evolution tasks. The results show that relevant concerns can be identified with a manageable level of noise.","Clustering algorithms,
Costs,
Inference algorithms,
Computer science,
Noise level,
Graphical user interfaces,
Database systems,
Access protocols,
Scattering,
Documentation"
Highly scalable algorithms for rectilinear and octilinear Steiner trees,"The rectilinear Steiner minimum tree (RSMT) problem, which asks for a minimum-length interconnection of a given set of terminals in the rectilinear plane, is one of the fundamental problems in electronic design automation. Recently there has been renewed interest in this problem due to the need for highly scalable algorithms able to handle nets with tens of thousands of terminals. In this paper we give a practical O(n log/sup 2/ n) heuristic for computing near-optimal rectilinear Steiner trees based on a hatched version of the greedy triple contraction algorithm of Zelikovsky (Algorithmica vol. 9, pp. 463-470, 1993). Experiments conducted on both random and industry test cases show that our heuristic matches or exceeds the quality of best known RSMT heuristics, e.g. on random instances with more than 100 terminals our heuristic improves over the rectilinear minimum spanning tree by an average of 11%. Moreover, our heuristic has very good runtime scaling, e.g. it can route a 34k-terminals net extracted from a real design in less than 25 seconds compared to over 86 minutes needed by the O(n/sup 2/) edge-based heuristic of Borah, Owens, and Irwin (Discrete Appl. Math. vol. 90, pp. 51-67, 1999). Since our heuristic is graph-based, it can be easily modified to handle practical considerations such as routing obstacles, preferred directions, via costs, and octilinear routing - indeed, experimental results show only a small factor increase in runtime when switching from rectilinear to octilinear routing.","Routing,
Runtime,
Steiner trees,
Electronic design automation and methodology,
Testing,
Computer science,
Costs,
Tree graphs,
Silicon,
Research and development"
Proving hard-core predicates using list decoding,"We introduce a unifying framework for proving that predicate P is hard-core for a one-way function f, and apply it to a broad family of functions and predicates, reproving old results in an entirely different way as well as showing new hard-core predicates for well known one-way function candidates. Our framework extends the list-coding method of Goldreich and Levin for showing hard-core predicates. Namely, a predicate will correspond to some error correcting code, predicting a predicate will correspond to access to a corrupted codeword, and the task of inverting one-way functions will correspond to the task of list decoding a corrupted codeword. A characteristic of the error correcting codes which emerge and are addressed by our framework is that codewords can be approximated by a small number of heavy coefficients in their Fourier representation. Moreover, as long as corrupted words are close enough to legal codewords, they will share a heavy Fourier coefficient. We list decodes, by devising a learning algorithm applied to corrupted codewords for learning heavy Fourier coefficients. For codes defined over {0, 1}/sup n/ domain, a learning algorithm by Kushilevitz and Mansour already exists. For codes defined over Z/sub N/, which are the codes which emerge for predicates based on number theoretic one-way functions such as the RSA and Exponentiation modulo primes, we develop a new learning algorithm. This latter algorithm may be of independent interest outside the realm of hard-core predicates.","Decoding,
Computer science"
Entropy-based dual-portal-to-3-DCT registration incorporating pixel correlation,"For patient setup verification in external beam radiotherapy (EBRT) of prostate cancer, we developed an information theoretic registration framework, called the minimax entropy registration framework, to simultaneously and iteratively segment portal images and register them to three-dimensional (3-D) computed tomography (CT) image data. The registration framework has two steps, the max step and the min step, and evaluates appropriate entropies to estimate segmentations of the portal images and to rind the transformation parameters. In the initial version of the algorithm (Bansal et al 1999), we assumed image pixels to be independently distributed, an assumption not true in general. Thus, to better segment the portal images and to improve the accuracy of the estimated registration parameters, in this initial formulation of the problem, the correlation among pixel intensities is modeled using a one-dimensional Markov random process. Line processes are incorporated into the model to improve the estimation of segmentation of the portal images. In the max step, the principle of maximum entropy is invoked to estimate the probability distribution on the segmentations. The estimated distribution is then incorporated into the min step to estimate the registration parameters. Performance of the proposed framework is evaluated and compared to that of a mutual information-based registration algorithm using both simulated and real patient data. In the proposed registration framework, registration of the 3-D CT image and the portal images is guided by an estimated segmentation of the pelvic bone. However, as the prostate can move with respect to the pelvic structure, further localization of the prostate using ultrasound image data is required, an issue to be further explored in future.","Image segmentation,
Portals,
Entropy,
Computed tomography,
Pixel,
Parameter estimation,
Prostate cancer,
Minimax techniques,
Registers,
Random processes"
Markov nets: probabilistic models for distributed and concurrent systems,"For distributed systems, i.e., large complex networked systems, there is a drastic difference between a local view and knowledge of the system, and its global view. Distributed systems have local state and time, but do not possess global state and time in the usual sense. In this paper, motivated by the monitoring of distributed systems and in particular of telecommunications networks, we develop a generalization of Markov chains and hidden Markov models for distributed and concurrent systems. By a concurrent system, we mean a system in which components may evolve independently, with sparse synchronizations. We follow a so-called true concurrency approach, in which neither global state nor global time are available. Instead, we use only local states in combination with a partial order model of time. Our basic mathematical tool is that of Petri net unfoldings.","Hidden Markov models,
Petri nets,
Concurrent computing,
Discrete event systems,
Computerized monitoring,
Stochastic systems,
Automata,
Computer science,
Interleaved codes"
State of the art in testing components,"The use of components in development of complex software systems can surely have various benefits. Their testing, however, is still one of the open issues in software engineering. Both the developer of a component and the developer of a system using components often face the problem that information vital for certain development tasks is not available. Such a lack of information has various consequences to both. One of the important consequences is that it might not only obligate the developer of a system to test the components used, it might also complicate these tests. This article gives an overview of component testing approaches that explicitly respect a lack of information in development.","Application software,
Telematics,
Software systems,
System testing,
Computer science,
Software testing,
Software engineering,
Quality assurance,
Software quality"
Extending fingertip grasping to whole body grasping,"Although it is mechanically possible for a robot manipulator to grasp using non-fingertip contacts, there are few examples of this. We call non-fingertip grasping such as grasping with proximal finger phalanges or grasping with the sides of arms ""whole body grasping"". While robotic demonstrations are rare, humans commonly use whole body grasps to interact with the world. One of the distinctive features of whole body grasping is the kinematic coupling among potential contacts. This kinematic coupling introduces extra constraints into the grasp synthesis problem. In this paper, we extend recent grasp control techniques to whole body grasping. We show how the grasp control may be parameterized with a set of contact points on the surface of the robot manipulator that enables the grasp search to handle the extra kinematic coupling constraints and find whole body grasps.",
Deterministic extractors for bit-fixing sources and exposure-resilient cryptography,"We give an efficient deterministic algorithm which extracts /spl Omega/(n/sup 2/spl gamma//) almost-random bits from sources where n/sup 1/2 + /spl gamma// of the n bits are uniformly random and the rest are fixed in advance. This improves on previous constructions which required that at least n/2 of the bits be random. Our construction also gives explicit adaptive exposure-resilient functions and in turn adaptive all-or-nothing transforms. For sources where instead of bits the values are chosen from [d], for d > 2, we give an algorithm which extracts a constant fraction of the randomness. We also give bounds on extracting randomness for sources where the fixed bits can depend on the random bits.","Cryptography,
Computer science,
Data mining,
Protection,
Computational modeling"
The effects of segmentation and feature choice in a translation model of object recognition,"We work with a model of object recognition where words must be placed on image regions. This approach means that large scale experiments are relatively easy, so we can evaluate the effects of various early and midlevel vision algorithms on recognition performance. We evaluate various image segmentation algorithms by determining word prediction accuracy for images segmented in various ways and represented by various features. We take the view that good segmentations respect object boundaries, and so word prediction should be better for a better segmentation. However, it is usually very difficult in practice to obtain segmentations that do not break up objects, so most practitioners attempt to merge segments to get better putative object representations. We demonstrate that our paradigm of word prediction easily allows us to predict potentially useful segment merges, even for segments that do not look similar (for example, merging the black and white halves of a penguin is not possible with feature-based segmentation; the main cue must be ""familiar configuration""). These studies focus on unsupervised learning of recognition. However, we show that word prediction can be markedly improved by providing supervised information for a relatively small number of regions together with large quantities of unsupervised information. This supervisory information allows a better and more discriminative choice of features and breaks possible symmetries.",
A peer-to-peer on-demand streaming service and its performance evaluation,"Providing on-demand video streaming service over the Internet is a challenging task. In this paper, we propose DirectStream, a directory based peer-to-peer video streaming service that efficiently and cost-effectively provides video on-demand service with VCR operation support. We analytically and experimentally examine the system performance, and show that the proposed scheme can significantly reduce the workload posed on the server, and that it scales extremely well as the popularity of the video increases even if participating clients behave non-cooperatively. We propose a QoS parent selection algorithm to construct the appropriate peer-to-peer networks, and discuss how to provide continuous playback in the face of clients' early departures. Our study suggests that peer-to-peer networking is a promising technique to address scalability in on-demand streaming service.",
The value of knowing a demand curve: bounds on regret for online posted-price auctions,"We consider price-setting algorithms for a simple market in which a seller has an unlimited supply of identical copies of some good, and interacts sequentially with a pool of n buyers, each of whom wants at most one copy of the good. In each transaction, the seller offers a price between 0 and 1, and the buyer decides whether or not to buy, by comparing the offered price to his privately-held valuation for the good. The price offered to a given buyer may be influenced by the outcomes of prior transactions, but each individual buyer participates only once. In this setting, what is the value of knowing the demand curve? In other words, how much revenue can an uninformed seller expect to obtain, relative to a seller with prior information about the buyers' valuations? The answer depends on how the buyers' valuations are modeled. We analyze three cases - identical, random, and worst-case valuations - in each case deriving upper and lower bounds which match within a sublogarithmic factor.","Cost accounting,
Mathematics,
Pricing,
Probability distribution,
Image analysis,
Upper bound,
Computer science"
A 3D metaphor for software production visualization,"Software development is difficult because software is complex, the software production process is complex and understanding of software systems is a challenge. We propose a 3D visual approach to depict software production cost related program information to support software maintenance. The information helps us to reduce software maintenance costs, to plan the use of personnel wisely, to appoint experts efficiently and to detect system problems early.","Visualization,
Software maintenance,
Costs,
Reverse engineering,
Software systems,
Peace technology,
Production systems,
Computer science,
Programming,
Personnel"
Knowledge sharing: agile methods vs. Tayloristic methods,"This paper presents a comparative analysis of knowledge sharing approaches of agile and Tayloristic (traditional) software development teams. Issues of knowledge creation, knowledge conversion and transfer, continuous learning, competence management and team composition are discussed. Experience repositories and other tools for knowledge dissemination are examined.","Knowledge management,
Documentation,
Programming,
Management training,
Computer science,
Software engineering,
Software testing,
Project management,
Software development management,
Engineering management"
Comparison of failure detectors and group membership: performance study of two atomic broadcast algorithms,,
Overcoming the limitations of conventional vector processors,"Despite their superior performance for multimedia applications, vector processors have three limitations that hinder their widespread acceptance. First, the complexity and size of the centralized vector register file limits the number of functional units. Second, precise exceptions for vector instructions are difficult to implement. Third, vector processors require an expensive on-chip memory system that supports high bandwidth at low access latency. We introduce CODE, a scalable vector microarchitecture that addresses these three shortcomings. It is designed around a clustered vector register file and uses a separate network for operand transfers across functional units. With extensive use of decoupling, it can hide the latency of communication across functional units and provides 26% performance improvement over a centralized organization. CODE scales efficiently to 8 functional units without requiring wide instruction issue capabilities. A renaming table makes the clustered register file transparent at the instruction set level. Renaming also enables precise exceptions for vector instructions at a performance loss of less than 5%. Finally, decoupling allows CODE to tolerate large increases in memory latency at sublinear performance degradation without using on-chip caches. Thus, CODE can use economical, off-chip, memory systems.","Vector processors,
Registers,
Delay,
System-on-a-chip,
Microarchitecture,
Performance loss,
Computer architecture,
Bandwidth,
CMOS technology,
Computer science"
Timed I/O automata: a mathematical framework for modeling and analyzing real-time systems,"We describe the timed input/output automata (TIOA) framework, a general mathematical framework for modeling and analyzing real-time systems. It is based on timed I/O automata, which engage in both discrete transitions and continuous trajectories. The framework includes a notion of external behavior, and notions of composition and abstraction. We define safety and liveness properties for timed I/O automata, and a notion of receptiveness, and prove basic results about all of these notions. The TIOA framework is defined as a special case of the new hybrid I/O automata (HIOA) modeling framework for hybrid systems. Specifically, a TIOA is an HIOA with no external variables; thus, TIOAs communicate via shared discrete actions only, and do not interact continuously. This restriction is consistent with previous real-time system models, and gives rise to some simplifications in the theory (compared to HIOA). The resulting model is expressive enough to describe complex timing behavior, and to express the important ideas of previous timed automata frameworks.","Automata,
Mathematical model,
Real time systems,
Safety,
Contracts,
Information analysis,
Computer science,
Artificial intelligence,
Laboratories,
Timing"
A fast algorithm for computing hypergraph transversals and its application in mining emerging patterns,Computing the minimal transversals of a hypergraph is an important problem in computer science that has significant applications in data mining. We present a new algorithm for computing hypergraph transversals and highlight their close connection to an important class of patterns known as emerging patterns. We evaluate our technique on a number of large datasets and show that it outperforms previous approaches by a factor of 9-29 times.,
Distributed multi-agent diagnosis and recovery from sensor failures,"This paper presents work extending previous research in sensor fault tolerance, classification, and recovery from a single robot to a heterogeneous team of distributed robots. This approach allows teams of robots to share knowledge about the working environment, sensor and task state, to diagnose failures and also communicate to redistribute tasks in the event that a robot becomes inoperable. Our work presents several novel extensions to prior art: distributed fault handling and task management in a dynamic, distributed Java framework. This research was implemented and demonstrated on robots in a lab environment performing a simplified search operation.",
XR-tree: indexing XML data for efficient structural joins,"XML documents are typically queried with a combination of value search and structure search. While querying by values can leverage traditional database technologies, evaluating structural relationship, specifically parent-child or ancestor-descendant relationship, between XML element sets has imposed a great challenge on efficient XML query processing. We propose XR-tree, namely, XML region tree, which is a dynamic external memory index structure specially designed for strictly nested XML data. The unique feature of XR-tree is that, for a given element, all its ancestors (or descendants) in an element set indexed by an XR-tree can be identified with optimal worst case I/O cost. We then propose a new structural join algorithm that can evaluate the structural relationship between two XR-tree indexed element sets by effectively skipping ancestors and descendants that do not participate in the join. Our extensive performance study shows that the XR-tree based join algorithm significantly outperforms previous algorithms.",
The inequalities of quantum information theory,"Let /spl rho/ denote the density matrix of a quantum state having n parts 1, ..., n. For I/spl sube/N={1, ..., n}, let /spl rho//sub I/=Tr/sub N/spl bsol/I/(/spl rho/) denote the density matrix of the state comprising those parts i such that i/spl isin/I, and let S(/spl rho//sub I/) denote the von Neumann (1927) entropy of the state /spl rho//sub I/. The collection of /spl nu/=2/sup n/ numbers {S(/spl rho//sub I/)}/sub I/spl sube/N/ may be regarded as a point, called the allocation of entropy for /spl rho/, in the vector space R/sup /spl nu//. Let A/sub n/ denote the set of points in R/sup /spl nu// that are allocations of entropy for n-part quantum states. We show that A~/sub n/~ (the topological closure of A/sub n/) is a closed convex cone in R/sup /spl nu//. This implies that the approximate achievability of a point as an allocation of entropy is determined by the linear inequalities that it satisfies. Lieb and Ruskai (1973) have established a number of inequalities for multipartite quantum states (strong subadditivity and weak monotonicity). We give a finite set of instances of these inequalities that is complete (in the sense that any valid linear inequality for allocations of entropy can be deduced from them by taking positive linear combinations) and independent (in the sense that none of them can be deduced from the others by taking positive linear combinations). Let B/sub n/ denote the polyhedral cone in R/sup /spl nu// determined by these inequalities. We show that A~/sub n/~=B/sub n/ for n/spl les/3. The status of this equality is open for n/spl ges/4. We also consider a symmetric version of this situation, in which S(/spl rho//sub I/) depends on I only through the number i=/spl ne/I of indexes in I and can thus be denoted S(/spl rho//sub i/). In this case, we give for each n a finite complete and independent set of inequalities governing the symmetric allocations of entropy {S(/spl rho//sub i/)}/sub 0/spl les/i/spl les/n/ in R/sup n+1/.","Quantum mechanics,
Information theory,
Entropy,
Random variables,
Probability distribution,
Linear matrix inequalities,
Computer science,
Eigenvalues and eigenfunctions"
Heterogeneous mobile sensor net deployment using robot herding and line-of-sight formations,"This paper presents an approach for deploying a team of mobile sensor nodes to form a sensor network in indoor environments. The challenge in this work is that the mobile sensor nodes have no ability for localization or obstacle avoidance. Thus, our approach entails the use of more capable ""helper"" robots that ""herd"" the mobile sensor nodes into their deployment positions. To extensively explore the issues of heterogeneity in multi-robot teams, we employ the use of two types of helper robots-one that acts as a leader and a second that: 1) acts as a follower and 2) autonomously teleoperates the mobile sensor nodes. Due to limited sensing capabilities, neither of these helper robots can herd the mobile sensor nodes alone; instead, our approach enables the team as a whole to successfully accomplish the sensor deployment task. Our approach involves the use of line-of-sight formation keeping, which enables the follower robot to use visual markers to move the group along the path executed by the leader robot. We present results of the implementation of this approach in simulation, as well as results to date in the implementation on physical robot systems. To our knowledge, this is the first implementation of robot herding using such highly heterogeneous robots, in which no single type of robot could accomplish the sensor network deployment task, even if multiple copies of that robot type were available.","Mobile robots,
Robot sensing systems,
Acoustic sensors,
Robot vision systems,
Chemical sensors,
Indoor environments,
Computer science,
Mobile computing,
Sensor phenomena and characterization,
Collaboration"
A generic scheme for building overlay networks in adversarial scenarios,"This paper presents a generic scheme for a central, yet untackled issue in overlay dynamic networks: maintaining stability over long life and against malicious adversaries. The generic scheme maintains desirable properties of the underlying structure including low diameter, and efficient routing mechanism, as well as balanced node dispersal. These desired properties are maintained in a decentralized manner without resorting to global updates or periodic stabilization protocols even against an adaptive adversary that controls the arrival and departure of nodes.",
Meetings about meetings: research at ICSI on speech in multiparty conversations,"In early 2001, we reported (at the Human Language Technology meeting) the early stages of an ICSI (International Computer Science Institute) project on processing speech from meetings (in collaboration with other sites, principally SRI, Columbia, and UW). We report our progress from the first few years of this effort, including: the collection and subsequent release of a 75-meeting corpus (over 70 meeting-hours and up to 16 channels for each meeting); the development of a prosodic database for a large subset of these meetings, and its subsequent use for punctuation and disfluency detection; the development of a dialog annotation scheme and its implementation for a large subset of the meetings; and the improvement of both near-mic and far-mic speech recognition results for meeting speech test sets.",
Data partitioning for maximal scratchpad usage,"The energy consumption for mobile embedded systems is a limiting factor because of today's battery capacities. The memory subsystem consumes a large chunk of the energy, necessitating its efficient utilization. Energy efficient scratchpads are thus becoming common, though unlike caches they require to be explicitly utilized. In this paper, an algorithm integrated into a compiler is presented which analyzes the application, partitions an array variable whenever its beneficial, appropriately modifies the application and selects the best set of variables and program parts to be placed onto the scratchpad. Results show an energy improvement between 5.7% and 17.6% for a variety of applications against a previously known algorithm.",
Counterexample-guided abstraction refinement,"The main practical problem in model checking is the combinatorial explosion of system states commonly known as the state explosion problem. Abstraction methods attempt to reduce the size of the state space by employing knowledge about the system and the specification in order to model only relevant features in the Kripke structure. Counterexample-guided abstraction refinement is an automatic abstraction method where, starting with a relatively small skeletal representation of the system to be verified, increasingly precise abstract representations of the system are computed. The key step is to extract information from false negatives (""spurious counterexamples"") due to over-approximation.",
Human body pose estimation using silhouette shape analysis,We describe a system for human body pose estimation from multiple views that is fast and completely automatic. The algorithm works in the presence of multiple people by decoupling the problems of pose estimation of different people. The pose is estimated based on a likelihood function that integrates information from multiple views and thus obtains a globally optimal solution. Other characteristics that make our method more general than previous work include: (1) no manual initialization; (2) no specification of the dimensions of the 3D structure; (3) no reliance on some learned poses or patterns of activity; (4) insensitivity to edges and clutter in the background and within the foreground. The algorithm has applications in surveillance and promising results have been obtained.,"Humans,
Shape,
Biological system modeling,
Surveillance,
Layout,
Computer science,
Educational institutions,
Bayesian methods,
Filtering,
Cameras"
General composition and universal composability in secure multi-party computation,"Concurrent general composition relates to a setting where a secure protocol is run in a network concurrently with other, arbitrary protocols. Clearly, security in such a setting is what is desired, or even needed, in modern computer networks where many different protocols are executed concurrently. Our main result is a proof that security under concurrent general composition is equivalent to a relaxed variant of universal composability (where the only difference relates to the order of quantifiers in the definition). An important corollary of this theorem is that existing impossibility results for universal composability (or actually its relaxed variant) are inherent in any definition achieving security under concurrent general composition. We stress that the impossibility results obtained are not ""black-box"", and apply even to non-black-box simulation. Our main result also demonstrates that the definition of universal composability is somewhat ""minimal"", in that the composition guarantee provided by universal composability (almost) implies the definition itself. This indicates that the security definition of universal composability is not overly restrictive.","Protocols,
Computer networks,
Intelligent networks,
Concurrent computing,
Computer security,
Stress,
Cryptography,
Random variables,
Privacy,
Polynomials"
Optimal error exponents in hidden Markov models order estimation,"We consider the estimation of the number of hidden states (the order) of a discrete-time finite-alphabet hidden Markov model (HMM). The estimators we investigate are related to code-based order estimators: penalized maximum-likelihood (ML) estimators and penalized versions of the mixture estimator introduced by Liu and Narayan (1994). We prove strong consistency of those estimators without assuming any a priori upper bound on the order and smaller penalties than previous works. We prove a version of Stein's lemma for HMM order estimation and derive an upper bound on underestimation exponents. Then we prove that this upper bound can be achieved by the penalized ML estimator and by the penalized mixture estimator. The proof of the latter result gets around the elusive nature of the ML in HMM by resorting to large-deviation techniques for empirical processes. Finally, we prove that for any consistent HMM order estimator, for most HMM, the overestimation exponent is null.",
Active learning for spoken language understanding,"We describe active learning methods for reducing the labeling effort in a statistical call classification system. Active learning aims to minimize the number of labeled utterances by automatically selecting for labeling the utterances that are likely to be most informative. The first method, inspired by certainty-based active learning, selects the examples that the classifier is least confident about. The second method, inspired by committee-based active learning, selects the examples that multiple classifiers do not agree on. We have evaluated these active learning methods using a call classification system used for AT&T customer care. Our results indicate that it is possible to reduce human labeling effort at least by a factor of two.","Natural languages,
Labeling,
Humans,
Learning systems,
Sampling methods,
Speech,
Cost function,
Computer science,
Sorting,
Training data"
Detecting spoofed packets,"Packets sent using the IP protocol include the IP address of the sending host. The recipient directs replies to the sender using this source address. However, the correctness of this address is not verified by the protocol. The IP protocol specifies no method for validating the authenticity of the packet's source. This implies that an attacker can forge the source address to be any desired. This is almost exclusively done for malicious or at least inappropriate purposes. Given that attackers can exploit this weakness for many attacks, it would be beneficial to know if network traffic has spoofed source addresses. This knowledge can be particularly useful as an adjunct to reduce false positive from intrusion detection systems. This paper discusses attacks using spoofed packets and a wide variety of methods for detecting spoofed packets. These include both active and passive host-based methods as well as the more commonly discussed routing-based methods. Additionally, we present the results of experiments to verify the effectiveness of passive methods.","Protocols,
Telecommunication traffic,
Intrusion detection,
Routing,
Computer science,
Ethernet networks,
Probes"
Fast and accurate vision-based pattern detection and identification,"Fast pattern detection and identification is fundamental problem for many applications of real-time vision systems. The desirable characteristics for a solution are that it requires little computation, localizes a pattern robustly and with high accuracy, and can identify a large number of unique pattern identifiers so that many of these markers can be tracked within a field a view. We will present a system that can accurately track a broad class of patterns both accurately and quickly, when used with a suitable low level vision system that can return calibrated coordinates of regions in an image. Both pattern design and the detection algorithm are considered together to find a solution meeting the above criteria. Along the way, assumptions are verified to make informed choices without relying on guesswork, and allowing similar system to be designed on a solid experimental and statistical basis.",
The information systems research cycle,"What distinguishes information systems from closely aligned disciplines such as computer science, organizational science, management science, economics, or systems engineering? How does IS research balance the demands of relevance and rigor to make effective contributions to both theory and practice? As senior researchers in IS, the authors have engaged in many debates on these questions and have come to some conclusions about what makes this burgeoning field unique and how to properly plan, execute, and evaluate IS research as well as transition it into practice.","Information systems,
Management information systems,
Technology management,
Behavioral science,
Humans,
Economic forecasting,
Information management,
Information analysis,
Computer science,
Engineering management"
Clustering hosts in P2P and global computing platforms,"Being able to identify clusters of nearby hosts among Internet clients provides very useful information for a number of internet and p2p applications. Examples of such applications include web applications, request routing in peer-to-peer overlay network, and distributed computing applications. In this paper, we present and formulate the internet host clustering problem. Leveraging previous work on internet host distance measurement, we propose two hierarchical clustering techniques to solve this problem. The first technique is a marker based hierarchical partitioning approach. The second technique is based on the well known K-means clustering algorithm. We evaluated these two approaches in simulation using a representative Internet topology generated with the GT ITM generator for over 1,000 hosts. Our simulation results demonstrate that our algorithmic clustering approaches effectively identify clusters with arbitrary diameters. Our conclusion is that by leveraging previous work on internet host distance estimation, it is possible to cluster Internet hosts to benefit various applications with various requirements.",
Consistency maintenance in peer-to-peer file sharing networks,"While the current generation of peer-to-peer networks share predominantly static files, future peer-to-peer networks will support sharing of files that are modified frequently by their users. We present techniques to maintain temporal consistency of replicated files in a peer-to-peer network. We consider the Gnutella P2P network and present techniques for maintaining consistency in Gnutella even when peers containing replicated files dynamically join and leave the network. An experimental evaluation of our techniques shows that: (i) a hybrid approach based on push and pull achieves high fidelity in highly dynamic P2P networks and (ii) the run-time overheads of our techniques are small, making them a practical choice for P2P networks.","Peer to peer computing,
Intelligent networks,
Runtime,
Costs,
Protocols,
Computer science,
Video sharing,
Collaboration,
Prototypes,
Conferences"
Compact representations of logic functions using heterogeneous MDDs,"In this paper we propose a compact representation of logic functions using Multi-valued Decision Diagrams (MDDs) called heterogeneous MDDs. In a heterogeneous MDD, each variable may take a different domain. By partitioning binary input variables and representing each partition as a single multi-valued variable, we can produce a heterogeneous MDD with 16% smaller memory size than a Reduced Ordered Binary Decision Diagram (ROBDD), and with as small memory size as the Free Binary Decision Diagrams (FBDDs). We minimized a large number of benchmark functions to show the compactness of heterogeneous MDDs.",
Automated analysis for digital forensic science: semantic integrity checking,"When computer security violations are detected, computer forensic analysts attempting to determine the relevant causes and effects are forced to perform the tedious tasks of finding and preserving useful clues in large networks of operational machines. To augment a computer crime investigator's efforts, we present an expert system with a decision tree that uses predetermined invariant relationships between redundant digital objects to detect semantic incongruities. By analyzing data from a host or network and searching for violations of known data relationships, particularly when an attacker is attempting to hide his presence, an attacker's unauthorized changes may be automatically identified. Examples of such invariant data relationships are provided, as are techniques to identify new, useful ones. By automatically identifying relevant evidence, experts can focus on the relevant files, users, times and other facts first.","Digital forensics,
Computer security,
Computer networks,
Cause effect analysis,
Performance analysis,
Computer crime,
Expert systems,
Decision trees,
Object detection,
Data analysis"
LHAP: a lightweight hop-by-hop authentication protocol for ad-hoc networks,"Most ad hoc networks do not implement any network access control, leaving these networks vulnerable to resource consumption attacks where a malicious node injects packets into the network with the goal of depleting the resources Of the nodes relaying the packets. To thwart or prevent such attacks, it is necessary to employ authentication mechanisms that ensure that only authorized nodes can inject traffic into the network. In this paper we present LHAP a scalable and light-weight authentication protocol for ad hoc networks. LHAP is based on two techniques: (i) hop-by-hop authentication for verifying the authenticity of all the packets transmitted in the network and (ii) one-way key chain and TESLA for packet authentication and for reducing the overhead for establishing trust among nodes. We analyze the security of LHAP and show LHAP is a lightweight security protocol through detailed performance analysis.",
Storing RDF as a graph,"RDF is the first W3C standard for enriching information resources of the Web with detailed meta data. The semantics of RDF data is defined using a RDF schema. The most expressive language for querying RDF is RQL, which enables querying of semantics. In order to support RQL, a RDF storage system has to map the RDF graph model onto its storage structure. Several storage systems for RDF data have been developed, which store the RDF data as triples in a relational database. To evaluate an RQL query on those triple structures, the graph model has to be rebuilt from the triples. We present a new approach to store RDF data as a graph in a object-oriented database. Our approach avoids the costly rebuilding of the graph and efficiently queries the storage structure directly. The advantages of our approach have been shown by performance test on our prototype implementation OO-Store.",
Fast algorithms for GS-model-based image reconstruction in data-sharing Fourier imaging,"Many imaging experiments involve acquiring a time series of images. To improve imaging speed, several ""data-sharing"" methods have been proposed, which collect one (or a few) high-resolution reference(s) and a sequence of reduced data sets. In image reconstruction, two methods, known as ""Keyhole"" and reduced-encoding imaging by generalized-series reconstruction (RIGR), have been used. Keyhole fills in the unmeasured high-frequency data simply with those from the reference data set(s), whereas RIGR recovers the unmeasured data using a generalized series (GS) model, of which the basis functions are constructed based on the reference image(s). This correspondence presents a fast algorithm (and two extensions) for GS-based image reconstruction. The proposed algorithms have the same computational complexity as the Keyhole algorithm, but are more capable of capturing high-resolution dynamic signal changes.","Image reconstruction,
High-resolution imaging,
Encoding,
Computational complexity,
Spatial resolution,
Image resolution,
Radiology,
Biomedical imaging,
Heuristic algorithms,
Signal resolution"
Computer engineering curriculum in the new millennium,"Currently there is a joint activity (referred to as Computing Curricula 2001, shortened to CC2001) involving the Association for Computing Machinery and the IEEE Computer Society, which is producing curriculum guidance for the broad area of computing. Within this activity, a volume on computer engineering is being developed. This volume addresses the important area of the design and development of computers and computer-based systems. Current curricula must be capable of evolving to meet the more immediate needs of students and industry. The purpose of this paper is to look at areas of future development in computer engineering in the next ten years (2013) and beyond and to consider the work of the Computer Engineering volume of CC2001 in this context.",
Cross-layer adaptive video coding to reduce energy on general-purpose processors,"Traditionally, video encoders have been designed assuming that the more redundancy is removed, the better the encoder. However, on current laptops, reducing the compression efficiency of the video encoder by reducing the number of instructions used to perform compression can actually reduce the total energy used to encode and transmit a sequence. The correct balance between computation and compression efficiency may change dynamically, motivating adaptive encoders. At the same time, recent general-purpose processors also employ energy-driven adaptations. For best gains, the adaptations in the hardware and application layers must be coordinated. From a system design viewpoint, this coordination must happen through minimal, well-defined interfaces. This paper develops (1) an adaptive video encoder for general-purpose processors that trades computational complexity for compression efficiency to minimize total system energy, and (2) a method for determining the best configuration for such an encoder when running on a processor that is also adaptive. Our adaptive processor employs recent energy saving techniques of dynamic voltage and frequency scaling and architectural adaptation. Using a detailed simulator, we show that our cross-layer adaptive application algorithm reduces energy significantly, when employed on a fixed or adaptive processor.",
Analysis of the effectiveness of global virtual teams in software engineering projects,"Global software development projects use virtual teams, which are primarily linked through computer and telecommunications technologies across national boundaries. Global virtual teams rarely meet in a face-to-face context and thus face challenging problems not associated with traditional colocated teams. To understand the complex issues in a virtual project environment during the requirements definition phase of the software development cycle, we conducted an exploratory research study, involving 24 virtual teams based in Canada and India, working on defining business requirements for software projects, over a period of 5 weeks. The study indicates that ease of use of technology, trust between the teams and well-defined task structure influence positively the efficiency, effectiveness, and satisfaction level of global virtual teams.","Virtual groups,
Software engineering,
Programming,
Collaborative software,
Project management,
Telecommunication computing,
Collaborative work,
Computer industry,
Software development management,
Technology management"
Flux invariants for shape,"We consider the average outward flux through a Jordan curve of the gradient vector field of the Euclidean distance function to the boundary of a 2D shape. Using an alternate form of the divergence theorem, we show that in the limit as the area of the region enclosed by such a curve shrinks to zero, this measure has very different behaviors at medial points than at non-medial ones, providing a theoretical justification for its use in the Hamilton-Jacobi skeletonization algorithm of Siddiqi et al. (2002). We then specialize to the case of shrinking circular neighborhoods and show that the average outward flux measure also reveals the object angle at skeletal points. Hence, formulae for obtaining the boundary curves, their curvatures, and other geometric quantities of interest, can be written in terms of the average outward flux limit values at skeletal points. Thus this measure can be viewed as a Euclidean invariant for shape description: it can be used to both detect the skeleton from the Euclidean distance function, as well as to explicitly reconstruct the boundary from it. We illustrate our results with several numerical simulations.","Euclidean distance,
Skeleton,
Shape measurement,
Numerical simulation,
Computer vision,
Computer science,
Mathematics,
Area measurement,
Image reconstruction,
Biomedical imaging"
Evaluating individual contribution toward group software engineering projects,"It is widely acknowledged that group or team projects are a staple of undergraduate and graduate software engineering courses. Such projects provide students with experiences that better prepare them for their careers, so teamwork is often required or strongly encouraged by accreditation agencies. While there are a multitude of educational benefits of group projects, they also pose considerable challenge in fairly and accurately discerning individual contribution for evaluation purposes. Issues, approaches, and best practices for evaluating individual contribution are presented from the perspectives of the University of Kentucky, University of Ottawa, University of Southern California, and others. The techniques utilized within a particular course generally are a mix of (1) the group mark is everybody's mark, (2) everybody reports what they personally did, (3) other group members report the relative contributions of other group members, (4) pop quizzes on project details, and (5) cross-validating with the results of individual work.","Software engineering,
Engineering profession,
Teamwork,
Accreditation,
Best practices,
Programming,
Life testing"
Pattern-based control systems engineering,"What is a design pattern? The authors explain this concept and the use of design patterns to document, transfer, and exploit design knowledge.",
Simplification of Toffoli networks via templates,"Reversible logic functions can be realized as networks of Toffoli gates. The synthesis of Toffoli networks can be divided into two steps. First, find a network that realizes the desired junction. Second, transform the network such that it uses fewer gates, while realizing the same function. This paper addresses the second step. Transformations are accomplished via template matching. The basis for a template is a network with m gates that realizes the identity function. If a sequence in the network to be synthesized matches more than half of a template, then a transformation reducing the gate count can be applied. All templates for m/spl les/7 are described in this paper.",
HyperLIC,"We introduce a new method for visualizing symmetric tensor fields. The technique produces images and animations reminiscent of line integral convolution (LIC). The technique is also slightly related to hyperstreamlines in that it is used to visualize tensor fields. However, the similarity ends there. HyperLIC uses a multi-pass approach to show the anisotropic properties in a 2D or 3D tensor field. We demonstrate this technique using data sets from computational fluid dynamics as well as diffusion-tensor MRI.","Tensile stress,
Anisotropic magnetoresistance,
Data visualization,
Animation,
Eigenvalues and eigenfunctions,
Convolution,
Computer science,
Computational fluid dynamics,
Magnetic resonance imaging,
Chromium"
A watermarking infrastructure for enterprise document management,"Digital watermarking is a promising technology to embed copyright information as unperceivable signals in digital contents. Although various watermarking techniques have been employed extensively for protecting rights of multimedia digital contents over the Internet, their applications in the management of enterprise documents have not been studied comprehensively. In this paper, we present a novel watermark-based document distribution protocol, which complements conventional access control techniques, to address a common problem in large enterprises, where document management policies are not properly reinforced. For instance, sensitive documents are often left behind in common areas, printing rooms or public folders in an enterprise. The reinforcement of document management policies requires a concrete support of nonrepudiation in the document distribution protocol. The protocol is adapted from our previous works on watermarking protocols, which defend unethical distribution of digital contents over the Internet. Our protocol makes use of registration certificates to distribute the identity information of the content-end users. We also present an implementation outline summarizing our access policy model, document check-in and check-out mechanism, and watermarking scheme employed.","Watermarking,
Access protocols,
Internet,
Computer science,
Access control,
Printing,
Technology management,
Humans,
Public key,
Protection"
The effect of compiler optimizations on Pentium 4 power consumption,"This paper examines the effect of compiler optimizations on the energy usage and power consumption of the Intel Pentium 4 processor. We measure the effects of different levels of general optimization and specific optimization. We classify general optimizations as those compiler flags which enable a set of compiler optimizations. Specific optimizations are those which can be enabled and disabled individually. The three specific optimizations we study are loop unrolling, loop vectorization, and function inlining. The binaries used in this study are generated using the Intel C++ compiler, which allows fine-grained control over each of these specific optimizations.",
Invisible formal methods for embedded control systems,"Embedded control systems typically comprise continuous control laws combined with discrete mode logic. These systems are modeled using a hybrid automaton formalism, which is obtained by combining the discrete transition system formalism with continuous dynamical systems. This paper develops automated analysis techniques for asserting correctness of hybrid system designs. Our approach is based on symbolic representation of the state space of the system using mathematical formulas in an appropriate logic. Such formulas are manipulated using symbolic theorem proving techniques. It is important that formal analysis should be unobtrusive and acceptable to engineering practice. We motivate a methodology called invisible formal methods that provides a graded sequence of formal analysis technologies ranging from extended typechecking, through approximation and abstraction, to model checking and theorem proving. As an instance of invisible formal methods, we describe techniques to check inductive invariants, or extended types, for hybrid systems and compute discrete finite state abstractions automatically to perform reachability set computation. The abstract system is sound with respect to the formal semantics of hybrid automata. We also discuss techniques for performing analysis on nonstandard semantics of hybrid automata. We also briefly discuss the problem of translating models in Simulink/Stateflow language, which is widely used in practice, into the modeling formalisms, like hybrid automata, for which analysis tools are being developed.",
Automatic video summarization by graph modeling,"We propose a unified approach for summarization based on the analysis of video structures and video highlights. Our approach emphasizes both the content balance and perceptual quality of a summary. Normalized cut algorithm is employed to globally and optimally partition a video into clusters. A motion attention model based on human perception is employed to compute the perceptual quality of shots and clusters. The clusters, together with the computed attention values, form a temporal graph similar to Markov chain that inherently describes the evolution and perceptual importance of video clusters. In our application, the flow of a temporal graph is utilized to group similar clusters into scenes, while the attention values are used as guidelines to select appropriate subshots in scenes for summarization.",
Humanoid arm motion planning using stereo vision and RRT search,"This paper describes an experimental stereo vision based motion planning system for humanoid robots. The goal is to automatically generate arm trajectories that avoid obstacles in unknown environments from high-level task commands. Our system consists of three components: 1) environment sensing using stereo vision with disparity map generation and on-line consistency checking, 2) probabilistic mesh modeling in order to accumulate continuous vision input, and 3) motion planning for the robot arm using RRTs (rapidly exploring random trees). We demonstrate results from experiments using an implementation designed for the humanoid robot H7.","Stereo vision,
Humanoid robots,
Motion planning,
Humans,
Technology planning,
Robot sensing systems,
Orbital robotics,
Kinematics,
Computer science,
Information science"
Effectively visualizing multi-valued flow data using color and texture,"In this paper we offer several new insights and techniques for effectively using color and texture to simultaneously convey information about multiple 2D scalar and vector distributions, in a way that facilitates allowing each distribution to be understood both individually and in the context of one or more of the other distributions. Specifically, we introduce the concepts of: color weaving for simultaneously representing information about multiple co-located color encoded distributions; and texture stitching for achieving more spatially accurate multi-frequency line integral convolution representations of combined scalar and vector distributions. The target application for our research is the definition, detection and visualization of regions of interest in a turbulent boundary layer flow at moderate Reynolds number. In this work, we examine and analyze streamwise-spanwise planes of three-component velocity vectors with the goal of identifying and characterizing spatially organized packets of hairpin vortices.",
Dispersion behaviors for a team of multiple miniature robots,"To safely and efficiently guide search and rescue operations in disaster areas, gathering of relevant information such as the locations of victims, must occur swiftly. Using the concept of repellent virtual pheromones inspired by insect colony coordination behaviors, miniature robots can be quickly dispersed to survey a disaster site. Assisted by visual servoing, dispersion of the miniature robots can quickly cover an area. An external observer such as another robot or an overhead camera is brought into the control loop to provide each miniature robot estimations of the positions of all the other nearby robots in the robotic team. Each robot can then move away from the other nearby robots, resulting in the robot collective swiftly dispersing through the local area. The technique has been implemented using the miniature scout robots, developed by the Center for Distributed Robotics at the University of Minnesota, which are well-suited to surveillance and reconnaissance missions.","Robot kinematics,
Robot sensing systems,
Humans,
Reconnaissance,
Robot vision systems,
Cameras,
Surveillance,
Computer science,
Insects,
Remote sensing"
On /spl Zopf//sub 4/-linear Preparata-like and Kerdock-like codes,"We say that a binary code of length n is additive if it is isomorphic to a subgroup of /spl Zopf//sub 2//sup /spl alpha// /spl times/ /spl Zopf//sub 4//sup /spl beta//, where the quaternary coordinates are transformed to binary by means of the usual Gray map and hence /spl alpha/ + 2/spl beta/ = n. In this paper, we prove that any additive extended Preparata (1968) -like code always verifies /spl alpha/ = 0, i.e., it is always a /spl Zopf//sub 4/-linear code. Moreover, we compute the rank and the dimension of the kernel of such Preparata-like codes and also the rank and the kernel of the /spl Zopf//sub 4/-dual of these codes, i.e., the /spl Zopf//sub 4/-linear Kerdock-like codes.",
On-line laboratories for image and two-dimensional signal processing,"This paper presents innovative on-line Java-based educational DSP software modules that were developed to render possible on-line laboratories for two-dimensional digital signal processing. The developed software modules provide on-line 2-D DSP capabilities, including 2D signal generation, 2D FIR filter design & implementation, and 2D transforms. On-line image processing capabilities are also provided, including image restoration and enhancement. In order to illustrate 2D concepts graphically, contour (2D) and perspective (3D) plots have also been implemented and incorporated as part of the developed software tools. On-line laboratory exercises have been developed in the aforementioned areas for use in the graduate-level multidimensional signal processing and image processing courses at ASU. Statistical and qualitative evaluations are presented to assess the effectiveness of the developed on-line 2D DSP software and laboratories in improving the learning experiences of the students.",
The chaos of software development,"We present a new perspective on the problem of complexity in software, using sound mathematical concepts from information theory such as Shannon's entropy [S. Weaver, (1949)]. We study the complexity of the development process by examining the logs of the source control repository for large software projects. We hypothesize that the process of developing code is a good indicator of the current and future problems in the code and the project. A complex process will have negative affects on its outcome, such as producing a complex system or delaying releases. We validate our work by studying the evolution of six large open source projects (three operating systems, a window manager, an office productivity suite, and a database).","Chaos,
Programming,
Software systems,
Project management,
Operating systems,
Monitoring,
Memory management,
Software architecture,
Computer science,
Information theory"
Comparing particle swarms for tracking extrema in dynamic environments,"This work presents a comparative study of particle swarm models on their abilities to track extrema in dynamic environments. A standard PSO, two randomized PSOs, and a fine-grained PSO are evaluated in non-trivial multimodal dynamic environments involving small constant step changes, different large step changes, and chaotic step changes of the extrema. DF1 proposed by Morrison and De Jong is used to generate these three types of dynamics (1999). Our results indicate that PSO and its variants are able to perform reasonably well in a 2-dimensional variable space, whereas perform well to a less extent in a 10-dimensional variable space. It is also found that the fine-grained PSO is able to outperform all other PSO variants in the 10-dimensional variable space, likely due to its ability in maintaining better population diversity.","Particle tracking,
Computer science,
Information technology,
Australia,
Particle swarm optimization,
Chaos,
Cities and towns,
Evolutionary computation,
Heuristic algorithms,
Testing"
Fast lexicon-based word recognition in noisy index card images,"This paper describes a complete system for reading type-written lexicon words in noisy images - in this case museum index cards. The system is conceptually simple, and straightforward to implement. It involves three stages of processing. The first stage extracts row-regions from the image, where each row is a hypothesized line of text. The next stage scans an OCR classifier over each row image, creating a character hypothesis graph in the process. This graph is then searched using a priority-queue based algorithm for the best matches with a set of words (lexicon). Performance evaluation on a set of museum archive cards indicates competitive accuracy and also reasonable throughput. The priority queue algorithm is over two hundred times faster than using flat dynamic programming on these graphs.","Image recognition,
Optical character recognition software,
Computer science,
Systems engineering and theory,
Throughput,
Dynamic programming,
Packaging machines,
Algorithm design and analysis,
Search methods,
Image segmentation"
Ontology-based approach for information fusion,,
RUMR: robust scheduling for divisible workloads,"Divisible workload applications arise in many fields of science and engineering. They can be parallelized in master-worker fashion and relevant scheduling strategies have been proposed to reduce application markspan. Our goal is to developed a practical divisible workload scheduling strategy. This requires that previous work be revisited as several usual assumptions about the computing platform do not hold in practice. We have partially addressed this concern in a previous paper via an algorithm that achieves high performance with realistic resource latency models. In this paper we extend our approach to account for performance prediction errors, which are expected for most real-world performance and applications. In essence, we combine ideas from multiround divisible workload scheduling, for performance, and from factoring-based scheduling, for robustness. We present simulation results to quantify the benefits of our approach compared to our original algorithm and to other previously proposed algorithms.",
Review of computer vision education,"Computer vision is becoming a mainstream subject of study in computer science and engineering. With the rapid explosion of multimedia and the extensive use of video and image-based communications over the World Wide Web, there is a strong demand for educating students to become knowledgeable in computer imaging and vision. The purpose of this paper is to review the status of computer vision education today.","Machine vision,
Computer science education,
Reviews"
TPC server fault tolerance using connection migration to a backup server,,"Fault tolerance,
Web server,
Computer science,
Virtual prototyping,
Linux,
Operating systems,
Transport protocols,
Internet,
Broadcasting,
Large-scale systems"
Applying the semantic Web layers to access control,"The Semantic Web, also known as the Web of meaning, is considered the new generation of the Web. Its objective is to enable computers and people to work in cooperation. A requisite for this is encoding data in forms that make web contents (meaning, semantics) more understandable by algorithmic means. In this paper, we present the application of semantic Web concepts and technologies to the access control area. The Semantic Access Control Model (SAC) uses different layers of metadata to take advantage of the semantics of the different components relevant for the access decision. We have developed a practical application of this access control model based on a specific language, denominated Semantic Policy Language (SPL), for the description of access criteria. This work demonstrates how the semantic web concepts and its layers infrastructure may play an important role in many relevant fields, such as the case of access control and authorization fields.",
Results for outdoor-SLAM using sparse extended information filters,"In [Thrun, S., et al., 2001], we proposed the sparse extended information filter for efficiently solving the simultaneous localization and mapping (SLAM) problem. In this paper, we extend this algorithm to handle data association problems and report real-world results, obtained with an outdoor vehicle. We find that our approach performs favorably when compared to the extended Kalman filter solution from which it is derived.",
Incorporating a measure of local scale in voxel-based 3-D image registration,"We present a new class of approaches for rigid-body registration and their evaluation in studying multiple sclerosis (MS) via multiprotocol magnetic resonance imaging (MRI). Three pairs of rigid-body registration algorithms were implemented, using cross-correlation and mutual information (MI), operating on original gray-level images, and utilizing the intermediate images resulting from our new scale-based method. In the scale image, every voxel has the local ""scale"" value assigned to it, defined as the radius of the largest ball centered at the voxel with homogeneous intensities. Three-dimensional image data of the head were acquired from ten MS patients for each of six MRI protocols. Images in some of the protocols were acquired in registration. The registered pairs were used as ground truth. Accuracy and consistency of the six registration methods were measured within and between protocols for known amounts of misregistrations. Our analysis indicates that there is no ""best"" method. For medium misregistration, the method using MI, for small and large misregistration the method using normalized cross-correlation performs best. For high-resolution data the correlation method and for low-resolution data the MI method, both using the original gray-level images, are the most consistent. We have previously demonstrated the use of local scale information in fuzzy connectedness segmentation and image filtering. Scale may also have potential for image registration as suggested by this work.","Image registration,
Multiple sclerosis,
Magnetic resonance imaging,
Protocols,
Mutual information,
Magnetic heads,
Correlation,
Image segmentation,
Information filtering,
Information filters"
A three-tier view-based methodology for M-services adaptation,"With recent advances in mobile technologies and infrastructures, there are increasing demands for ubiquitous access to networked services. These services, generally known as m-services, extend supports from Web browsers on personal computers to handheld devices, such as mobile phones and PDAs. However, in general, the capabilities and bandwidth of these devices are significantly inferior to desktop computers over wired connections, which have been assumed by most Internet services. Instead of redesigning or adapting m-services in an ad-hoc manner for multiple platforms available in handheld devices, we propose a methodology for such adaptation based on three tiers: user interface views, data views, and process views. These views provide customization and help balance security and trust. User interface views provide alternative presentations of inputs and outputs. Data views summarize data over limited bandwidth and map heterogeneous data sources. In addition, we introduce a novel approach of applying process views to m-service adaptation, where mobile users may execute a more concise version or modified procedures of the original process. The process view also serves as the key mechanism for integrating user interface views and data views. In addition, we present a formal model on view consistency and integrity in our methodology. We demonstrate the feasibility of our methodology by extending a service negotiation subsystem into an m-service with multi-platform support.","User interfaces,
Mobile computing,
Web and internet services,
Computer science,
Handheld computers,
Bandwidth,
Microcomputers,
Mobile handsets,
Personal digital assistants,
Pervasive computing"
A wearable augmented reality system using positioning infrastructures and a pedometer,,
The analysis of a recombinative hill-climber on H-IFF,"Many experiments have proved that crossover is an essential search operator in evolutionary algorithms, at least for certain functions. However, the rigorous analysis of such algorithms on crossover-friendly functions is still in its infancy. Here, a recombinative hill-climber is analyzed on the crossover-friendly function hierarchical-if-and-only-if (H-IFF) introduced by Watson et al. (1998). The dynamics of this algorithm are investigated and it is proved that the expected optimization time equals /spl Theta/(n log n).",
Engineering fault-tolerant TCP/IP servers using FT-TCP,,
Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics,Achieving and maintaining high software quality is most dependent on how easily the software engineer least familiar with the system can understand the system's code. Understanding attributes of cognitive processes can lead to new software metrics that allow the prediction of human performance in software development and for assessing and improving the understandability of text and code. In this research we present novel metrics based on current understanding of short-term memory performance to predict the location of high frequencies of errors and to evaluate the quality of a software system. We further enhance these metrics by applying static and dynamic program slicing to provide programmers with additional guidance during software inspection and maintenance efforts.,
Mobile agent rendezvous in a ring,"In the rendezvous search problem, two mobile agents must move along the n nodes of a network so as to minimize the time required to meet or rendezvous. When the mobile agents are identical and the network is anonymous, however, the resulting symmetry can make the problem impossible to solve. Symmetry is typically broken by having the mobile agents run either a randomized algorithm or different deterministic algorithms. We investigate the use of identical tokens to break symmetry so that the two mobile agents can run the same deterministic algorithm. After deriving the explicit conditions under which identical tokens can be used to break symmetry on the n node ring, we derive the lower and upper bounds for the time and memory complexity of the rendezvous search problem with various parameter sets. While these results suggest a possible tradeoff between the mobile agents' memory and the time complexity of the rendezvous search problem, we prove that this tradeoff is limited.","Mobile agents,
Search problems,
Intelligent networks,
Computer science,
Mathematics,
Upper bound,
Intrusion detection,
Fault diagnosis,
Algorithm design and analysis,
Clocks"
Learning a discriminative classifier using shape context distances,"For the purpose of object recognition, we learn one discriminative classifier based on one prototype, using shape context distances as the feature vector. From multiple prototypes, the outputs of the classifiers are combined using the method called ""error correcting output codes"". The overall classifier is tested on a benchmark dataset and is shown to outperform existing methods with far fewer prototypes.",
Usability of Web services,"This paper is concerned with the application of Web services to distributed, cross-organizational business processes. Web services provide a platform independent concept of components and composition. Thus, they seem to be a proper technology to cover the heterogeneous structures within distributed business processes. Although the technological basis is given, there are a lot of open questions, such as whether Web services fit together in such a way that the composition yields a deadlock-free system - the question of compatibility; whether one Web service can be exchanged by another within a composed system without running into problems - the question of equivalence; and whether we can reason about the quality of one given Web service without considering the environment it is used in. In this paper, we present the notion of usability - our quality criterion of a Web service. This criterion is intuitive and can be easily proven locally. Moreover, this notion allows to answer the other questions.",
Miss rate prediction across all program inputs,"Improving cache performance requires understanding cache behavior. However, measuring cache performance for one or two data input sets provides little insight into how cache behavior varies across all data input sets. We use our recently published locality analysis to generate a parameterized model of program cache behavior. Given a cache size and associativity, this model predicts the miss rate for arbitrary data input set sizes. This model also identifies critical data input sizes where cache behavior exhibits marked changes. Experiments show this technique is within 2% of the hit rate for set associative caches on a set of integer and floating-point programs.","Space exploration,
Runtime,
Computer science,
Predictive models,
Pattern matching,
Pattern analysis,
Performance analysis,
Sampling methods,
Flow graphs,
Particle measurements"
Graph-based knowledge representation for GIS data,"This paper presents a proposal to create a graph representation for GIS, using both spatial and non-spatial data and also including spatial relations between spatial objects. Because graphs are a powerful and flexible knowledge representation we are able to combine spatial and non-spatial data at the same time and this is one of the strengths of the proposal. We hope to apply this knowledge representation to the data mining process with GIS data including three types of spatial relations: topological, orientation and distance.","Knowledge representation,
Geographic Information Systems,
Data mining,
Spatial databases,
Proposals,
Multidimensional systems,
Humans,
Explosives,
Relational databases,
Data analysis"
The no free lunch theorems: complexity and security,"One of the main challenges for decision scientists in the 21st century will be managing systems of ever increasing complexity. As systems like electrical power grids, computer networks, and the software that controls it all grow increasingly complex, fragility, bugs, and security flaws are becoming increasingly prevalent and problematic. It is natural then to ask what consequences this growing complexity has on our ability to manage these systems. In this paper, we take a first step toward addressing this question with the development of the fundamental matrix, a framework for analyzing the broad qualitative nature of decision making. With the fundamental matrix we explain in a qualitative way many theorems and known results about optimization, complexity, and security. The simplicity of the explanations leads to new insights toward potential research directions. Like other ""theories"" dealing with broad fundamental properties, however, the fundamental matrix has certain limitations that make it largely descriptive. Thus, instead of claiming the last words our goal is to stimulate a dialog and debate that may one day lead to a prescriptive science of complexity.",
Visualizing time-varying volume data,"This article reviews strategies developed so far for enabling interactive visualization of volume data from time-varying simulations with a focus on encoding, feature extraction, and rendering issues. The author also discusses emerging trends in time-varying data visualization research and their potential impact on the scientific research community.","Data visualization,
Rendering (computer graphics),
Encoding,
Feature extraction,
Chemical processes,
Space technology,
Hardware,
Graphics,
Scientific computing,
Chemical technology"
Automatic thresholding of gray-level using multistage approach,"A multistage approach is presented for thresholding document images, along with its application. The proposed method is based on two stages. Global thresholding is used in the first stage to give a preliminary result. A second stage then refines the threshold value based on local spatial characteristics of the regions formed in the first stage. It automatically customizes the thresholding of regions that have specific and consistent characteristics but are different to other regions in the image. This technique works well for both simple images, in which the background and foreground are distinct and separable and complex images containing multiple regions with different shading/textures. A typical application is postal envelope analysis. The results of evaluation show significant improvement compared to several other global and local thresholding techniques.","Image analysis,
Text analysis,
Algorithm design and analysis,
Information analysis,
Computer science,
Australia,
Application software,
Image color analysis,
Gray-scale,
Histograms"
A text watermarking algorithm based on word classification and inter-word space statistics,,
Log correlation for intrusion detection: a proof of concept,"Intrusion detection is an important part of networked-systems security protection. Although commercial products exist, finding intrusions has proven to be a difficult task with limitations under current techniques. Therefore, improved techniques are needed. We argue the need for correlating data among different logs to improve intrusion detection systems accuracy. We show how different attacks are reflected in different logs and argue that some attacks are not evident when a single log is analyzed. We present experimental results using anomaly detection for the virus Yaha. Through the use of data mining tools (RIPPER) and correlation among logs we improve the effectiveness of an intrusion detection system while reducing false positives.","Intrusion detection,
Data mining,
Application software,
Monitoring,
Computer security,
Protection,
Computer science,
Data security,
National security,
Internet"
Cache configuration exploration on prototyping platforms,"We describe cache architecture, intended for prototype-oriented IC platforms, that automatically finds the best cache configuration for a particular application. The cache itself can be configured with respect to the total size, associativity, line size, and way prediction. The cache architecture includes an explorer component that efficiently searches the large space of possible configurations for the set of points representing meaningful tradeoffs between performance and energy - the Pareto-optimal set. We provide results of experiments showing that the architecture effectively finds a good set of Pareto points for numerous Powerstone and MediaBench embedded system benchmarks. Our architecture eliminates the need for time-consuming simulations to determine the best cache configuration, and imposes little power overhead and reasonable size overhead.","Prototypes,
Application specific integrated circuits,
Computer architecture,
Microprocessors,
Memory architecture,
Embedded system,
Embedded computing,
Virtual prototyping,
Testing,
Computer science"
A student-enacted simulation approach to software engineering education,"In some cases, real-world application of software engineering concepts does not effectively map with current undergraduate curriculums. Typically, a student's first ""hands-on"" experience working on large-scale software development projects is via an intern position or his/her first full-time position. However, prior exposure to the corporate project environment would greatly improve a student's performance in industry. In order to develop students for successful careers in software engineering, specifically for software development, they must be immersed not only in the software development lifecycle and paradigms, but also in the workings of large project teams. Currently, most undergraduate software engineering courses are taught by presenting the concepts and methodologies and assigning fragmented three-to-four person group projects. In the Department of Computer Science, Georgetown University, Washington, DC, a two-course approach to undergraduate software engineering education has been developed that incorporates the practical application of coursework in a large team setting. The first course presents a firm software design basis, while the second course demonstrates corporate-level software engineering concepts with a semester-long software development simulation where the entire class is the development team. This paper presents the experiences from offering this software engineering simulation approach.",
Recognition of on-line handwritten mathematical formulas in the E-chalk system,,
Linear complexity over F/sub p/ and trace representation of Lempel-Cohn-Eastman sequences,"In this article, the linear complexity over F/sub p/ of Lempel-Cohn-Eastman (1977) sequences of period p/sup m/-1 for an odd prime p is determined. For p=3,5, and 7, the exact closed-form expressions for the linear complexity over F/sub p/ of LCE sequences of period p/sup m/-1 are derived. Further, the trace representations for LCE sequences of period p/sup m/-1 for p=3 and 5 are found by computing the values of all Fourier coefficients in F/sub p/ for the sequences.","Binary sequences,
Autocorrelation,
Multiaccess communication,
Galois fields,
Closed-form solution,
Councils,
Informatics,
Computer science,
Polynomials"
MoireGraphs: radial focus+context visualization and interaction for graphs with visual nodes,"Graph and tree visualization techniques enable interactive exploration of complex relations while communicating topology. However, most existing techniques have not been designed for situations where visual information such as images is also present at each node and must be displayed. This paper presents MoireGraphs to address this need. MoireGraphs combine a new focus+context radial graph layout with a suite of interaction techniques (focus strength changing, radial rotation, level highlighting, secondary foci, animated transitions and node information) to assist in the exploration of graphs with visual nodes. The method is scalable to hundreds of displayed visual nodes.",
Modeling proportional membership in fuzzy clustering,"To provide feedback from a cluster structure to the data from which it has been determined, we propose a framework for mining typological structures based on a fuzzy clustering model of how the data are generated from a cluster structure. To relate data entities to cluster prototypes, we assume that the observed entities share parts of the prototypes in such a way that the membership of an entity to a cluster expresses the proportion of the cluster's prototype reflected in the entity (proportional membership). In the generic version of the model, any entity may independently relate to any prototype, which is similar to the assumption underlying the fuzzy c-means criterion. The model is referred to as fuzzy clustering with proportional membership (FCPM). Several versions of the model relaxing the generic assumptions are presented and alternating minimization techniques for them are developed. The results of experimental studies of FCPM versions and the fuzzy c-means algorithm are presented and discussed, especially addressing the issues of fitting the underlying clustering model. An example is given with data in the medical field in which our approach is shown to suit better than more conventional methods.","Prototypes,
Feedback,
Clustering algorithms,
Computer science,
Biomedical imaging,
Image processing,
Pattern recognition,
Shape,
Phase change materials,
Partitioning algorithms"
Image registration with global and local luminance alignment,"Inspired by tensor voting, we present luminance voting, a novel approach for image registration with global and local luminance alignment. The key to our modeless approach is the direct estimation of replacement function, by reducing the complex estimation problem to the robust 2D tensor voting in the corresponding voting spaces. No model for replacement function is assumed. Luminance data are first encoded into 2D ball tensors. Subject to the monotonic constraint only, we vote for an optimal replacement function by propagating the smoothness constraint using a dense tensor field. Our method effectively infers missing curve segments and rejects image outliers without assuming any simplifying or complex curve model. The voted replacement functions are used in our iterative registration algorithm for computing the best warping matrix. Unlike previous approaches, our robust method corrects exposure disparity even if the two overlapping images are initially misaligned. Luminance voting is effective in correcting exposure difference, eliminating vignettes, and thus improving image registration. We present results on a variety of images.",
Crawling for domain-specific hidden Web resources,"The Hidden Web, the part of the Web that remains unavailable for standard crawlers, has become an important research topic during recent years. Its size is estimated to 400 to 500 times larger than that of the publicly indexable Web (PIW). Furthermore, the information on the hidden Web is assumed to be more structured, because it is usually stored in databases. In this paper, we describe a crawler which starting from the PIW finds entry points into the hidden Web. The crawler is domain-specific and is initialized with pre-classified documents and relevant keywords. We describe our approach to the automatic identification of Hidden Web resources among encountered HTML forms. We conduct a series of experiments using the top-level categories in the Google directory and report our analysis of the discovered Hidden Web resources.","Crawlers,
Databases,
Information analysis,
Search engines,
Europe,
Web pages,
HTML,
Humans,
Computer science,
Information resources"
Improving the performance of MPI derived datatypes by optimizing memory-access cost,"The MPI Standard supports derived datatypes, which allow users to describe noncontiguous memory layout and communicate noncontiguous data with a single communication function. This feature enables an MPI implementation to optimize the transfer of noncontiguous data. In practice, however, few MPI implementations implement derived datatypes in a way that performs better than what the user can achieve by manually packing data into a contiguous buffer and then calling an MPI function. In this paper, we present a technique for improving the performance of derived datatypes by automatically using packing algorithms that are optimized for memory-access cost. The packing algorithms use memory-optimization techniques that the user cannot apply easily without advanced knowledge of the memory architecture. We present performance results for a matrix-transpose example that demonstrate that our implementation of derived datatypes significantly outperforms both manual packing by the user and the existing derived-datatype code in the MPI implementation (MPICH).","Memory architecture,
Data structures,
Message passing,
Optimization methods"
"Epidaure: A research project in medical image analysis, simulation, and robotics at INRIA","Epidaure is the name of a research project launched in 1989 at INRIA Rocquencourt, close to Paris, France. The research directions of the project were progressively defined around the following topics: volumetric image segmentation, three-dimensional (3-D) shape modeling, image registration, motion analysis, morphometry, and surgery simulation. The author describes and illustrates some of the contributions of the Epidaure team on these different topics.",
A derivation system for security protocols and its logical formalization,"Many authentication and key exchange protocols are built using an accepted set of standard concepts such as Diffie-Hellman key exchange, nonces to avoid replay, certificates from an accepted authority, and encrypted or signed messages. We introduce a basic framework for deriving security protocols from such simple components. As a case study, we examine the structure of a family of key exchange protocols that includes station-to-station (STS), ISO-9798-3, just fast keying (JFK), IKE and related protocols, deriving all members of the family from two basic protocols using a small set of refinements and protocol transformations. As initial steps toward associating logical derivations with protocol derivations, we extend a previous security protocol logic with preconditions and temporal assertions. Using this logic, we prove the security properties of the standard signature based challenge-response protocol and the Diffie-Hellman key exchange protocol. The ISO-9798-3 protocol is then proved correct by composing the correctness proofs of these two simple protocols.","Protocols,
Authentication,
Security,
Cryptography,
Public key,
Sociotechnical systems,
Logic,
Computer science,
Jacobian matrices,
Internet"
Multiresolution fMRI activation detection using translation invariant wavelet transform and statistical analysis based on resampling,"A new method is proposed for activation detection in event-related functional magnetic resonance imaging (fMRI). The method is based on the analysis of selected resolution levels (a subspace) in the translation invariant wavelet transform (TIWT) domain. Using a priori knowledge about the activation signal and trends, we analyze their power in different resolution levels in the TIWT domain and select an optimal set of resolution levels. A randomization-based statistical test is then applied in the wavelet domain for activation detection. This approach suppresses the effects of trends and enhances the detection sensitivity. In addition, since TIWT is insensitive to signal translations, the power analysis is robust with respect to signal shifts. The randomization test alleviates the need for assumptions about fMRI noise. The method has been applied to simulated and experimental fMRI datasets. Comparisons have been made between the results of the proposed method, a similar method in the time domain and the cross-correlation method. The proposed method has shown superior sensitivity compared to the other methods.","Wavelet analysis,
Wavelet transforms,
Statistical analysis,
Signal resolution,
Magnetic analysis,
Wavelet domain,
Signal analysis,
Testing,
Magnetic resonance imaging,
Noise robustness"
A class of photometric invariants: separating material from shape and illumination,"We derive a new class of photometric invariants that can be used for a variety of vision tasks including lighting invariant material segmentation, change detection and tracking, as well as material invariant shape recognition. The key idea is the formulation of a scene radiance model for the class of ""separable"" BRDFs, that can be decomposed into material related terms and object shape and lighting related terms. All the proposed invariants are simple rational functions of the appearance parameters (say, material or shape and lighting). The invariants in this class differ from one another in the number and type of image measurements they require. Most of the invariants in this class need changes in illumination or object position between image acquisitions. The invariants can handle large changes in lighting which pose problems for most existing vision algorithms. We demonstrate the power of these invariants using scenes with complex shapes, materials, textures, shadows and specularities.",
The perception of breast cancers-a spatial frequency analysis of what differentiates missed from reported cancers,"The primary detector of breast cancer is the human eye. Radiologists read mammograms by mapping exogenous and endogenous factors, which are based on the image and observer, respectively, into observer-based decisions. These decisions rely on an internal schema that contains a representation of possible malignant and benign findings. Thus, to understand the hits and misses made by the radiologists, it is important to model the interactions between the measurable image-based elements contained in the mammogram and the decisions made. The image-based elements can be of two types, i.e., areas that attracted the visual attention of the radiologist, but did not yield a report, and areas where the radiologist indicated the presence of an abnormal finding. In this way, overt and covert decisions are made when reading a mammogram. In order to model this decision-making process, we use a system that is based upon the processing done by the human visual system, which decomposes the areas under scrutiny in elements of different sizes and orientations. In our system, this decomposition is done using wavelet packets (WPs). Nonlinear features are then extracted from the WP coefficients, and an artificial neural network is trained to recognize the patterns of decisions made by each radiologist. Afterwards, the system is used to predict how the radiologist will respond to visually selected areas in new mammogram cases.","Frequency,
Humans,
Cancer detection,
Detectors,
Breast cancer,
Decision making,
Visual system,
Wavelet packets,
Feature extraction,
Artificial neural networks"
An analysis of the behavior of simplified evolutionary algorithms on trap functions,"Methods are developed to numerically analyze an evolutionary algorithm (EA) that applies mutation and selection on a bit-string representation to find the optimum for a bimodal unitation function called a trap function. This research bridges part of the gap between the existing convergence velocity analysis of strictly unimodal functions and global convergence results assuming the limit of infinite time. As a main result of this analysis, a new so-called (1 : /spl lambda/)-EA is proposed, which generates offspring using individual mutation rates p/sub i/. While a more traditional EA using only one mutation rate is not able to find the global optimum of the trap function within an acceptable (nonexponential) time, our numerical investigations provide evidence that the new algorithm overcomes these limitations. The analysis tools used for the analysis, based on absorbing Markov chains and the calculation of transition probabilities, are demonstrated to provide an intuitive and useful method for investigating the capabilities of EAs to bridge the gap between a local and a global optimum in bimodal search spaces.",
A group-theoretic approach to fast matrix multiplication,"We develop a new, group-theoretic approach to bounding the exponent of matrix multiplication. There are two components to this approach: (1) identifying groups G that admit a certain type of embedding of matrix multiplication into the group algebra /spl Copf/[G], and (2) controlling the dimensions of the irreducible representations of such groups. We present machinery and examples to support (1), including a proof that certain families of groups of order n/sup 2+o(1)/ support n /spl times/ n matrix multiplication, a necessary condition for the approach to yield exponent 2. Although we cannot yet completely achieve both (1) and (2), we hope that it may be possible, and we suggest potential routes to that result using the constructions in this paper.","Polynomials,
Computer science,
Machinery,
Linear algebra,
Matrix decomposition,
History,
Equations,
Sections,
Mirrors,
Fast Fourier transforms"
Mining emerging substrings,"We introduce a new type of KDD patterns called emerging substrings. In a sequence database, an emerging substring (ES) of a data class is a substring which occurs more frequently in that class rather than in other classes. ESs are important to sequence classification as they capture significant contrasts between data classes and provide insights for the construction of sequence classifiers. We propose a suffix tree-based framework for mining ESs, and study the effectiveness of applying one or more pruning techniques in different stages of our ES mining algorithm. Experimental results show that if the target class is of a small population with respect to the whole database, which is the normal scenario in single-class ES mining, most of the pruning techniques would achieve considerable performance gain.",
Towards a language for coherent enterprise architecture descriptions,"A coherent description of architectures provides insight, enables communication among different stakeholders and guides complicated (business and ICT) change processes. Unfortunately, so far no architecture description language exists that fully enables integrated enterprise modeling. In this paper we focus on the requirements and design of such a language. This language defines generic, organization-independent concepts that can be specialized or composed to obtain more specific concepts to be used within a particular organisation. It is not our intention to re-invent the wheel for each architectural domain: wherever possible we conform to existing languages or standards such as UML. We complement them with missing concepts, focusing on concepts to model the relationships among architectural domains. The concepts should also make it possible to define links between models in other languages. The relationship between architecture descriptions at the business layer and at the application layer (business-IT alignment) plays a central role.","Computer architecture,
Visualization,
Architecture description languages,
Instruments,
Computer science,
Business communication,
Wheels,
Unified modeling language,
Application software,
Software systems"
Remote-access education based on image acquisition and processing through the Internet,"This paper describes a new system for remote education through the Internet based on image processing. By means of an image acquisition system installed in the authors' laboratory, the user may interact with the remote environment and obtain visual information of the task being developed. The images acquired may be processed in order to obtain information concerning the training task. At the end of the exercise, students must answer questions related to key issues of the process and send the answers to a server. Finally, the system automatically evaluates the results. The aim of this system is to provide every element necessary for the students to self-train: theoretical background, lab equipment, and self-evaluation methods. The internet constitutes the ideal way to reach these objectives.",
Scaling an object-oriented system execution visualizer through sampling,"Increasingly, applications are being built by combining existing software components. For the most part, a software developer can treat the components as black-boxes. However, for some tasks, such as when performance tuning, a developer must consider how the components are implemented and how they interact. In these cases, a developer may be able to perform the task more effectively by using dynamic information about how the system executes. In previous work, we demonstrated the utility of a tool, called AVID (Architectural VIsualization of Dynamics), that animates dynamic information in terms of developer-chosen architectural views. One limitation of this earlier work was that AVID relied on trace information collected about the system's execution; traces for even small parts of a system's execution can be enormous, limiting the duration of execution that can be considered. To enable AVID to scale to larger longer-running systems, we have been investigating the visualization and animation of sampled dynamic information. In this paper, we discuss the addition of sampling support to AVID, and we present two case studies in which we experimented with animating sampled dynamic information to help with performance tuning tasks.","Visualization,
Sampling methods,
Animation,
Application software,
Information analysis,
Computer science,
Frequency,
Java,
Conferences"
Efficient flooding in ad hoc networks: a comparative performance study,"The blind flooding can become very inefficient because of redundant, ""superfluous"" forwarding. In fact, superfluous flooding increases link overhead and wireless medium congestion. In a large network, with heavy load, this extra overhead can have severe impact on performance and should be eliminated. Efficient flooding schemes to choose a dominant set of nodes have been recently proposed in ad hoc networks. In this paper, we compare the performance of a set of representative schemes via simulation using as criteria the flooding efficiency and the delivery ratio.",
Dealing with textureless regions and specular highlights - a progressive space carving scheme using a novel photo-consistency measure,"We present two extensions to the space carving framework. The first is a progressive scheme to better reconstruct surfaces lacking sufficient textures. The second is a novel photo-consistency measure that is valid for both specular and diffuse surfaces, under unknown lighting conditions.",
Heterogeneous implementation of an adaptive robotic sensing team,"When designing a mobile robotic team, an engineer is faced with many design choices. This paper discusses the design of a team consisting of two different models of robots with significantly different sensing and control capabilities intended to accomplish a similar task. Two new robotic platforms, the COTS Scout and the MegaScout are described along with their respective design considerations.",
A framework for collective personalized communication,"The paper explores collective personalized communication. For example, in all-to-all personalized communication (AAPC), each processor sends a distinct message to every other processor. However, for many applications, the collective communication pattern is many-to-many, where each processor sends a distinct message to a subset of processors. We first present strategies that reduce per-message cost to optimize AAPC. We then present performance results of these strategies in both all-to-all and many-to-many scenarios. These strategies are implemented in a flexible, asynchronous library with a non-blocking interface, and a message-driven runtime system. This allows the collective communication to run concurrently with the application, if desired. As a result the computational overhead of the communication is substantially reduced, at least on machines such as PSC Lemieux, which sport a co-processor capable of remote DMA. We demonstrate the advantages of our framework with performance results on several benchmarks and applications.","Application software,
Delay,
Computer science,
Cost function,
Runtime library,
Coprocessors,
Scalability,
Bandwidth,
Workstations,
Operating systems"
DIANE - distributed analysis environment for GRID-enabled simulation and analysis of physics data,Distributed analysis environment (DIANE) is the result of R&D in CERN IT Division focused on interfacing semi-interactive parallel applications with distributed GRID technology. DIANE provides a master-worker workflow management layer above low-level GRID services. DIANE is application and language-neutral. Component-container architecture and component adapters provide flexibility necessary to fulfill the diverse requirements of distributed applications. Physical transport layer assures interoperability with existing middleware frameworks based on Web services. Several distributed simulations based on Geant 4 were deployed and tested in real-life scenarios with DIANE.,"Analytical models,
Data analysis,
Physics,
Middleware,
Containers,
Application software,
Research and development,
Computer architecture,
Web services,
Computational modeling"
Performance-impact limited area fill synthesis,"Chemical-mechanical planarization (CMP) and other manufacturing steps in every deep-submicron VLSI have varying effects on device and interconnect features, depending on the local density. To improve manufacturability and performance predictability, area fill features are inserted into the layout to improve uniformity with respect to density criteria. However, the performance impact of area fill insertion is not considered by any fill method in the literature. In this paper, we first review and develop estimates for capacitance and timing overhead of area fill insertions. We then give the first formulations of the Performance Impact Limited Fill (PIL-Fill) problem with the objective of either minimizing total delay impact (MDFC) or maximizing the minimum slack of all nets (MSFC), subject to inserting a given prescribed amount of fill. For the MDFC PIL-Fill problem, we describe three practical solution approaches based on Integer Linear Programming (ILP-I and ILP-II) and the Greedy method. For the MSFC PIL-Fill problem, we describe an iterated greedy method that integrates call to an industry static timing analysis tool. We test our methods on layout testcases obtained from industry. Compared with the normal fill method according to Y. Chen et al.(2002), our ILP-II method for MDFC PIL-Fill problem achieves between 25% and 90% reduction in terms of total weighted edge delay (roughly, a measure of sum of node slacks) impact while maintaining ideal quality of the layout density control and our iterated greedy method for MSFC PIL-II problem also shows significant advantage with respect to the minimum slack of nets on post-fill layout.","Planarization,
Capacitance,
Tiles,
Computer aided manufacturing,
Timing,
Foundries,
Very large scale integration,
Testing,
Delay,
Computer science"
A sketch interface for mobile robots,"In human to human communication, a hand-drawn route map is often sketched to show a desired navigation path. In this paper, we describe a PDA sketching interface that can be used to direct a mobile robot along a specified path. Because sketched route maps are not drawn precisely or necessarily to scale, we do not attempt to analyze precise path information, but rather qualitative route information is extracted. The paper focuses on the front-end sketch understanding and includes a description of the interactive features, such as deleting, moving, and labeling landmarks. Results of a user evaluation are also presented, in which participants report that the sketching interface was as easy as using pencil and paper by a 2:1 margin.","Mobile robots,
Navigation,
Data mining,
Human robot interaction,
Military computing,
Computer science,
Mobile communication,
Personal digital assistants,
Information analysis,
Labeling"
Experiments in free-space triangulation using cooperative localization,"This paper presents a first detailed case study of collaborative exploration of a substantial environment. We use a pair of cooperating robots to test multi-robot environment mapping algorithms based on triangulation of free space. The robots observe one another using a robot tracking sensor based on laser range sensing (LIDAR). The environment mapping itself is accomplished using sonar sensing. The results of this mapping are compared to those obtained using scanning laser range sensing and the scan matching algorithm. We show that with appropriate outlier rejection policies, the sonar-based map obtained using collaborative localization can be as good or, in fact, better than that obtained using what is typically considered to be a superior sensing technology.","Robot sensing systems,
Robot kinematics,
Collaboration,
Collaborative work,
Orbital robotics,
Motion estimation,
Robustness,
Mechanical engineering,
Computer science,
Intelligent robots"
Adaptive minimum bit error rate beamforming assisted receiver for wireless communications,"A novel adaptive beamforming technique is proposed for wireless communication application based on the minimum bit error rate (MBER) criterion. It is shown that the MBER approach provides significant performance gain in terms of smaller bit error rate (BER) over the standard minimum mean square error (MMSE) approach. Using the classical Parzen window estimate of probability density function (p.d.f.), both the block-data and sample-by-sample adaptive implementations of the MBER solution are developed.","Bit error rate,
Wireless communication,
Array signal processing,
Linear antenna arrays,
Niobium,
Mean square error methods,
Adaptive arrays,
Antenna arrays,
Directive antennas,
Computer science"
Analysis of handwriting individuality using word features,,
Influence of the model for random birefringence on the differential group delay of periodically spun fibers,"We consider the two Wai-Menyuk models of birefringence in periodically spun fibers, and we show that the differential group delay differs significantly for the two models when the spin period approaches or exceeds the fiber beat length. When the fiber correlation length is large, we explain this difference quantitatively, and we explain it qualitatively for any fiber correlation length.","Birefringence,
Delay,
Optical fiber polarization,
Optical fiber sensors,
Computer science,
Polarization mode dispersion,
Sensor systems,
Laboratories,
Computational modeling,
Physics computing"
ViewPoints: meaningful relationships are difficult!,"The development of complex systems invariably involves many stakeholders who have different perspectives on the problem they are addressing, the system being developed, and the process by which it is being developed. The ViewPoints framework was devised to provide an organisational framework in which these different. perspectives, and their relationships, could be explicitly represented and analysed The framework acknowledges the inevitability of multiple inconsistent views, promotes separation of concerns, and encourages decentralised specification while providing support for integration through relationships and composition. In this paper, we reflect on the ViewPoints framework, current work and future research directions.","Programming,
Software engineering,
Software development management,
Computer science,
Heart,
Design engineering,
Software prototyping,
Engineering management"
Strategies for combining conflicting dogmatic beliefs,,"Computer science,
Logic,
Books,
US Government,
Cyclic redundancy check,
Costs,
Australia Council"
Software architecture recovery based on pattern matching,"This paper is a summary of the author's thesis that presents a model and an environment for recovering the high level design of legacy software systems based on user defined architectural patterns and graph matching techniques. In the proposed model, a high-level view of a software system in terms of the system components and their interactions is represented as a query, using a description language. A query is mapped onto a pattern-graph, where a component and its interactions with other components are represented as a group of graph-nodes and a group of graph-edges, respectively. Interaction constraints can be modeled by the description language as a part of the query. Such a pattern-graph is applied against an entity-relation graph that represents the information extracted from the source code of the software system. An approximate graph matching process performs a series of graph transformation operations (i.e., node/edge insertion/deletion) on the pattern-graph and uses a ranking mechanism based on data mining association to obtain a sub-optimal solution. The obtained solution corresponds to an extracted architecture that complies with the given query.","Software architecture,
Pattern matching,
Computer architecture,
Software systems,
Data mining,
Documentation,
Pattern analysis,
Reverse engineering,
Software maintenance,
Computer science"
Flow assignment and packet scheduling for multipath routing,"In this paper, we propose a framework to study how to route packets efficiently in multipath communication networks. Two traffic congestion control techniques, namely, flow assignment and packet scheduling, have been investigated. The flow assignment mechanism defines an optimal splitting of data traffic on multiple disjoint paths. The resequencing delay and the usage of the resequencing buffer can be reduced significantly by properly scheduling the sending order of all packets, say, according to their expected arrival times at the destination. To illustrate our model, and without loss of generality, Gaussian distributed end-to-end path delays are used. Our analytical results show that the techniques are very effective in reducing the average end-to-end path delay, the average packet resequencing delay, and the average resequencing buffer occupancy for various path configurations. These promising results can form a basis for designing future adaptive multipath protocols.","Routing,
Delays,
Scheduling algorithms,
Schedules,
Cost function,
Convex functions,
Communication networks"
ASAP: an adaptive QoS protocol for mobile ad hoc networks,"With the increasing widespread use of wireless technologies, the need arises for QoS provisioning mechanisms for multimedia applications in wireless networks. However, to support QoS in mobile ad hoc networks (MANETs) is more challenging than in fixed and last-hop wireless access networks. QoS protocols designed for fixed network, e.g. RSVP, are not applicable in MANETs, because they cannot cope with MANET's highly dynamic topology. Existing QoS frameworks explicitly built for MANETs are not as flexible and efficient as needed. In this paper, we propose a new QoS framework for MANETs-adaptive reservation and preallocation protocol (ASAP). By using two signaling messages, ASAP provides fast and efficient QoS support while maintaining adaptation flexibility and minimizing wasted reservations. Simulation of this framework using AODV C. Perkins, E. Royer (1999) illustrates the features and performance of ASAP, and demonstrates the design concepts.","Mobile ad hoc networks,
Wireless networks,
Bandwidth,
Network topology,
IP networks,
Throughput,
Communication system traffic control,
Wireless application protocol,
Pervasive computing,
Computer science"
The rough set approach to association rule mining,"In transaction processing, an association is said to exist between two sets of items when a transaction containing one set is likely to also contain the other. In information retrieval, an association between two sets of keywords occurs when they cooccur in a document. Similarly, in data mining, an association occurs when one attribute set occurs together with another. As the number of such associations may be large, maximal association rules are sought, e.g., Feldman et al. (1997, 1998). Rough set theory is a successful tool for data mining. By using this theory, rules similar to maximal associations can be found. However, we show that the rough set approach to discovering knowledge is much simpler than the maximal association method.",
Towards empirical evaluation of test-driven development in a university environment,"Test driven development (TDD) is an agile software development technique and it is one of the core development practices of Extreme programming (XP). In TDD, developers write automatically executable tests prior to writing the code they test. We ran a set of experiments to empirically assess different parameters of the TDD. We compared TDD to a more ""traditionally"" oriented iterative test-last development process (ITL). Our preliminary results show that TDD is not substantially different from ITL and our qualitative findings about a development process are different from results obtained from other researches.","Automatic testing,
Writing,
Programming profession,
Information systems,
Information science,
Software testing,
Radio access networks,
Software systems,
Internet,
Software engineering"
Blood pool contrast-enhanced MRA: improved arterial visualization in the steady state,"Blood pool agents (BPAs) for contrast-enhanced magnetic resonance angiography (CE-MRA) allow prolonged imaging during the steady state when the agent is distributed through the complete vascular system. This increases both the spatial resolution and the contrast resolution. However, simultaneous venous and arterial enhancement hampers interpretation. For the pelvic region of the vasculature, it is shown that arterial visualization in this equilibrium phase can be improved if the central arterial axis (CAA) is known. However, manually obtaining this axis is not feasible in clinical practice. Therefore, a method is presented that utilizes images acquired during the first pass of the contrast agent to find the CAA in the steady-state data with minimum user initialization. The accuracy of the resulting CAA is compared with tracings of three observers in six patient datasets. It was found that the mean difference between the semiautomatic method and the manual delineation is 1.32 mm in the steady-state data, and that the resulting CAA was always within the arterial lumen, which is an important prerequisite for both improved visualization and segmentation.","Blood,
Steady-state,
Computer aided analysis,
Biomedical imaging,
Arteries,
Veins,
Angiography,
Spatial resolution,
Data visualization,
Extracellular"
Adaptive course authoring: My Online Teacher,"With the expansion of successful applications of adaptive techniques for telecommunications in education, in particular, for various web-based education and training systems, there is a growing need of creating the respective authoring support. A hypermedia tool, called 'My Online Teacher' (MOT), has been developed and extended at the Eindhoven University of Technology that can be used for authoring adaptive hypermedia courses. With this tool, the subject-matter of the course to be designed can be modeled by means of concept maps from which lessons can be constructed. Concept maps and lessons form the two levels of pre-adaptive content, and they are stored in a database. This structure lays the basis for various types of adaptation, as it uses both the expressivity of metadata annotation and the flexibility of the database structure as will be illustrated. This paper describes this tool's design, implementation and first evaluation remarks. MOT is being presently used for the creation of a variant of the classical Neural Network course for third year students at the Eindhoven University of Technology.","Databases,
Courseware,
Mathematics,
Telecommunication computing,
Information systems,
Performance evaluation,
HTML,
XML,
Collaboration,
Adaptive systems"
The optimal distance measure for object detection,"We develop a multi-class object detection framework whose core component is a nearest neighbor search over object part classes. The performance of the overall system is critically dependent on the distance measure used in the nearest neighbor search. A distance measure that minimizes the misclassification risk for the 1-nearest neighbor search can be shown to be the probability that a pair of input image measurements belong to different classes. In practice, we model the optimal distance measure using a linear logistic model that combines the discriminative powers of more elementary distance measures associated with a collection of simple to construct feature spaces like color, texture and local shape properties. Furthermore, in order to perform search over large training sets efficiently, the same framework was extended to find hamming distance measures associated with simple discriminators. By combining this discrete distance model with the continuous model, we obtain a hierarchical distance model that is both fast and accurate. Finally, the nearest neighbor search over object part classes was integrated into a whole object detection system and evaluated against an indoor detection task yielding good results.","Object detection,
Nearest neighbor searches,
Neural networks,
Shape measurement,
Computer vision,
Testing,
Solid modeling,
Runtime,
Computer science,
Power system modeling"
"Optimizing the number of states, training iterations and Gaussians in an HMM-based handwritten word recognizer","In off-line handwriting recognition, classifiers based on hidden Markov models (HMMs) have become very popular. However, while there exist well-established training algorithms, such as the Baum-Welsh procedure, which optimize the transition and output probabilities of a given HMM architecture, the architecture itself, and in particular the number of states, must be chosen ""by hand"". Also the number of training iterations and the output distributions need to be defined by the system designer. In this paper we examine some optimization strategies for an HMM classifier that works with continuous feature values and uses the Baum-Welch training algorithm. The free parameters of the optimization procedure introduced in this paper are the number of states of a model, the number of training iterations, and the number of Gaussian mixtures for each state. The proposed optimization strategies are evaluated in the context of a handwritten word recognition task.","Gaussian processes,
Hidden Markov models,
Handwriting recognition,
Gaussian distribution,
Vocabulary,
Maximum likelihood estimation,
Computer science,
Electronic mail,
Character recognition,
Speech recognition"
Volume tracking using higher dimensional isosurfacing,"Tracking and visualizing local features from a time-varying volumetric data allows the user to focus on selected regions of interest, both in space and time, which can lead to a better understanding of the underlying dynamics. In this paper, we present an efficient algorithm to track time-varying isosurfaces and interval volumes using isosurfacing in higher dimensions. Instead of extracting the data features such as isosurfaces or interval volumes separately from multiple time steps and computing the spatial correspondence between those features, our algorithm extracts the correspondence directly from the higher dimensional geometry and thus can more efficiently follow the user selected local features in time. In addition, by analyzing the resulting higher dimensional geometry, it becomes easier to detect important topological events and the corresponding critical time steps for the selected features. With our algorithm, the user can interact with the underlying time-varying data more easily. The computation cost for performing time-varying volume tracking is also minimized.",
An adaptive and efficient dimensionality reduction algorithm for high-dimensional indexing,"The notorious ""dimensionality curse"" is a well-known phenomenon for any multidimensional indexes attempting to scale up to high dimensions. One well known approach to overcoming degradation in performance with respect to increasing dimensions is to reduce the dimensionality of the original dataset before constructing the index. However, identifying the correlation among the dimensions and effectively reducing them is a challenging task. We present an adaptive multilevel mahalanobis-based dimensionality reduction (MMDR) technique for high-dimensional indexing. Our MMDR technique has three notable features compared to existing methods. First, it discovers elliptical clusters using only the low-dimensional subspaces. Second, data points in the different axis systems are indexed using a single B/sup +/-tree. Third, our technique is highly scalable in terms of data size and dimensionality. An extensive performance study using both real and synthetic datasets was conducted, and the results show that our technique not only achieves higher precision, but also enables queries to be processed efficiently.","Indexing,
Computer science,
Clustering algorithms,
Euclidean distance,
Degradation,
Multimedia databases,
Shape,
Information retrieval,
Data analysis,
Surface treatment"
Dual multivariate auto-regressive modeling in state space for temporal signal separation,"Many existing independent component analysis (ICA) approaches result in deteriorated performance in temporal source separation because they have not taken into consideration of the underlying temporal structure of sources. In this paper, we model temporal sources as a general multivariate auto-regressive (AR) process whereby an underlying multivariate AR process in observation space is obtained. In this dual AR modeling, the mixing process from temporal sources to observations is the same as the mixture from the nontemporal residuals of the source AR (SAR) process to that of the observation AR (OAR) process. We can therefore avoid the source temporal effects in performing ICA by learning the demixing system on the independently distributed OAR residuals rather than the time-correlated observations. Particularly, we implement this approach by modeling each source signal as a finite mixture of generalized autoregressive conditional heteroskedastic (GARCH) process. The adaptive algorithms are proposed to extract the OAR residuals appropriately online, together with learning the demixing system via a nontemporal ICA algorithm. The experiments have shown its superior performance on temporal source separation.","State-space methods,
Source separation,
Independent component analysis,
Signal processing algorithms,
Signal processing,
Data mining,
Electroencephalography,
Magnetic noise,
Automatic speech recognition,
Computer science"
Rapid configuration and instruction selection for an ASIP: a case study,"We present a methodology that maximizes the performance of Tensilica based Application Specific Instruction-set Processor (ASIP) through instruction selection when an area constraint is given. Our approach rapidly selects from a set of pre-fabricated coprocessors/functional units from our library of pre-designed specific instructions (to evaluate our technology we use the Tensilica platform). As a result, we significantly increase application performance while area constraints are satisfied. Our methodology uses a combination of simulation, estimation and a pre-characterised library of instructions, to select the appropriate co-processors and instructions. We report that by selecting the appropriate coprocessors/functional units and specific TIE instructions, the total execution time of complex applications (we study a voice encoder/decoder), an application's performance can be reduced by up to 85% compared to the base implementation. Our estimator used in the system takes typically less than a second to estimate, with an average error rate of 4% (as compared to full simulation, which takes 45 minutes). The total selection process using our methodology takes 3-4 hours, while a full design space exploration using simulation would take several days.","Application specific processors,
Computer aided software engineering,
Coprocessors,
Process design,
Computer science,
Libraries,
Space exploration,
Time to market,
Architecture description languages,
Flow graphs"
Reduced complexity in-phase/quadrature-phase M-QAM turbo equalization using iterative channel estimation,"A reduced complexity trellis-based turbo equalizer known as the in-phase (I)/quadrature-phase (Q) turbo equalizer (TEQ-IQ) invoking iterative channel impulse response (CIR) estimation is proposed. The underlying principle of TEQ-IQ is based on equalizing the I and Q component of the transmitted signal independently. This requires the equalization of a reduced set of separate I and Q signal components in comparison to all of the possible I/Q phasor combinations considered by the conventional trellis-based equalizer. It was observed that the TEQ-IQ operating in conjunction with iterative CIR estimation was capable of achieving the same performance as the full-complexity conventional turbo equalizer (TEQ-CT) benefiting from perfect CIR information for both 4- and 16-quadrature amplitude modulation (QAM) transmissions, while attaining a complexity reduction factor of 1.1 and 12.2, respectively. For 64-QAM, the TEQ-CT receiver was too complex to be investigated by simulation. However, by assuming that only two turbo equalization iterations were required, which is the lowest possible number of iterations, the complexity of the TEQ-IQ was estimated to be a factor of 51.5 lower than that of the TEQ-CT. Furthermore, at BER = 10/sup -3/ the performance of the TEQ-IQ 64-QAM receiver using iterative CIR estimation was only 1.5 dB away from the associated decoding performance curve of the nondispersive Gaussian channel.","Channel estimation,
Equalizers,
Iterative decoding,
Intersymbol interference,
Bit error rate,
Quadrature amplitude modulation,
Computer science,
Phase shift keying,
Amplitude estimation,
Amplitude modulation"
ACES: an interactive software platform for self-instruction and self-evaluation in automatic control systems,"This paper presents an interactive-, menu-driven prototype software platform, namely automatic control educational software (ACES), for self-instruction and self-evaluation in automatic control systems. ACES is used for enriching instruction in automatic control at Aristotle University of Thessaloniki, Greece, in the Department of Electrical and Computer Engineering. The ACES platform includes theory with hyperlinks, a concept-graph, and a database with exercises. Students' answers to exercises are evaluated automatically ""on-line."" Furthermore, exercises can be proposed automatically by ACES. An instructor/supervisor can support in person the learning effort of a student, monitor the progress of a student, and, also, tailor a course's contents on the modular ACES platform. Two statistical hypothesis tests on both attitude questionnaires and student marks in the final (written) exam confirmed that the employment of ACES in the educational process can improve the performance of students in an automatic control course although the attitude of students toward the course does not change significantly with the use of ACES.","Control engineering education,
Computer aided instruction,
Interactive systems"
Tracking noise via dynamical systems with a continuum of states,"We model noise as a sequence of states of a dynamical system with a continuum of states. Observations generated by such a system are assumed to be related to the state of the system by a functional relation which models clean speech as the corrupting influence on noise. We show how the closed-form representation of such a dynamical system can be rendered tractable and solved iteratively by dynamically sampling the state space, resulting in an estimated noise sequence (sequence of states), which can then be removed from the noisy speech signal by standard methods. Experiments on speech corrupted by various noises show that the proposed algorithm performs better than our best previous algorithm, VTS, which assumes that the noise is stationary.",
The RR/RR CICQ switch: hardware design for 10-Gbps link speed,"The combined input and crossbar queued (CICQ) switch is an input buffered switch suitable for very high-speed networks. The implementation feasibility of the CICQ switch architecture for 24 ports and 10-Gbps link speed is shown in this paper with an FPGA-based design (estimated cost of $30,000 in mid-2002). The bottleneck of a CICQ switch with RR scheduling is the RR poller. We develop a priority encoder based RR poller that uses feedback masking. This design has lower delay than any known design for an FPGA implementation.","Switches,
Hardware,
Costs,
Field programmable gate arrays,
Computer architecture,
Scheduling algorithm,
Computer science,
High-speed networks,
Feedback,
Delay"
Adaptive division of labor in large-scale minimalist multi-robot systems,"A large-scale minimalist multi-robot system (LMMS) is one composed of a group of robots each with limited capabilities in terms of sensing, computation, and communication. Such systems have received increased attention due to their empirically demonstrated performance and beneficial characteristics, such as their robustness to environmental perturbations and individual robot failure and their scalability to large numbers of robots. However, little work has been done in investigating ways to endow such a LMMS with the capability to achieve a desired division of labor over a set of dynamically evolving concurrent tasks, important in many task-achieving LLMS. Such a capability can help to increase the efficiency and robustness of overall task performance as well as open new domains in which LMMS can be seen as a viable alternative to more complex control solutions. In this paper, we present a method for achieving a desired division of labor in a LMMS, experimentally validate it in a realistic simulation, and demonstrate its potential to scale to large numbers of robots and its ability to adapt to environmental perturbations.","Large-scale systems,
Multirobot systems,
Robot kinematics,
Robot sensing systems,
Robustness,
Robust control,
Computer science,
Scalability,
Computational modeling,
Bandwidth"
Analysis of event-related fMRI data using best clustering bases,"We explore a new paradigm for the analysis of event-related functional magnetic resonance images (fMRI) of brain activity. We regard the fMRI data as a very large set of time series x/sub i/(t), indexed by the position i of a voxel inside the brain. The decision that a voxel i/sub 0/ is activated is based not solely on the value of the fMRI signal at i/sub 0/, but rather on the comparison of all time series x/sub i/(t) in a small neighborhood W/sub i(0)/ around i/sub 0/. We construct basis functions on which the projection of the fMRI data reveals the organization of the time series x/sub i/(t) into activated and nonactivated clusters. These clustering basis functions are selected from large libraries of wavelet packets according to their ability to separate the fMRI time series into the activated cluster and a nonactivated cluster. This principle exploits the intrinsic spatial correlation that is present in the data. The construction of the clustering basis functions described in this paper is applicable to a large category of problems where time series are indexed by a spatial variable.","Magnetic analysis,
Wavelet packets,
Magnetic resonance imaging,
Blood,
Image analysis,
Magnetic resonance,
Libraries,
Brain mapping,
Magnetic susceptibility,
Design for experiments"
Fredkin/Toffoli templates for reversible logic synthesis,"Reversible logic has applications in quantum computing, low power CMOS, nanotechnology, optical computing, and DNA computing. The most common reversible gates are the Toffoli gate and the Fredkin gate. Our synthesis algorithm first finds a cascade of Toffoli and Fredkin gates with no backtracking and minimal look-ahead. Next we apply transformations that reduce the size of the circuit. Transformations are accomplished via template matching. The basis for a template is a network with m gates that realizes the identity function. If a sequence in the network to be synthesized matches more than half of a template, then a transformation that reduces the gate count can be applied. In this paper we show that Toffoli and Fredkin gates behave in a similar manner. Therefore, some gates in the templates may not need to be specified-they can match a Toffoli or a Fredkin gate. We formalize this by introducing the box gate. All templates with less than six gates are enumerated and classified. We synthesize all three input, three output reversible functions and compare our results to those obtained previously.",
CBC: clustering based text classification requiring minimal labeled data,"Semisupervised learning methods construct classifiers using both labeled and unlabeled training data samples. While unlabeled data samples can help to improve the accuracy of trained models to certain extent, existing methods still face difficulties when labeled data is not sufficient and biased against the underlying data distribution. We present a clustering based classification (CBC) approach. Using this approach, training data, including both the labeled and unlabeled data, is first clustered with the guidance of the labeled data. Some of unlabeled data samples are then labeled based on the clusters obtained. Discriminative classifiers can subsequently be trained with the expanded labeled dataset. The effectiveness of the proposed method is justified analytically. Our experimental results demonstrated that CBC outperforms existing algorithms when the size of labeled dataset is very small.","Text categorization,
Training data,
Support vector machines,
Support vector machine classification,
Semisupervised learning,
Supervised learning,
Classification algorithms,
Asia,
Computer science,
Clustering algorithms"
Pair-wise test coverage using genetic algorithms,"There has been an emerging trend to develop software using different components. In this way the cost of the software reduces and the developer is able to complete the system efficiently. The components' code may or may not be visible to the developer. Testing, in this case, requires the development of a set of test configurations that can be applied on the software. However, for software that comprises a large number of components, it is infeasible to test each and every test configuration within the limited testing budget and time. In this paper we propose a GA-based technique that identifies a set of test configurations that are expected to maximize pair-wise coverage, with the constraint that the number of test configurations is predefined. Although the paper primarily focuses on the interaction between software components, the idea can be applied to single code component testing. We performed some experiments using our proposed approach. The results were promising.","Genetic algorithms,
System testing,
Software testing,
Java,
Internet,
Costs,
Application software,
Portals,
Computer science,
Petroleum"
GridSAT: A Chaff-based Distributed SAT Solver for the Grid,"We present GridSAT, a parallel and complete satisfiability solver designed to solve non-trivial SAT problem instances using a large number of widely distributed and heterogeneous resources. The GridSAT parallel algorithm uses intelligent backtracking, distributed and carefully scheduled sharing of learned clauses, and clause reduction. Our implementation focuses on dynamic resource acquisition and release to optimize application execution. We show how the large number of computational resources that are available from a Grid can be managed effectively for the application by an automatic scheduler and effective implementation. GridSAT execution speed is compared against the best sequential solver as rated by the SAT2002 competition using a wide variety of problem instances. The results show that GridSAT delivers speed-up for all but one of the test problem instances that are of significant size. In addition, we describe how GridSAT has solved previously unsolved satisfiability problems and the domain science contribution these results make.","Processor scheduling,
Grid computing,
Government,
Computer science,
Testing,
Databases,
Parallel algorithms,
Computational intelligence,
Resource management,
Concurrent computing"
A probabilistic approach to buffer insertion,"This work presents a formal probabilistic approach for solving optimization problems in design automation. Prediction accuracy is very low especially at high levels of design flow. This can be attributed mainly to unawareness of low level layout information and variability in fabrication process. Hence a traditional deterministic design automation approach where each cost function is represented as a fixed value becomes obsolete. A new approach is gaining attention in which the cost functions are represented as probability distributions and the optimization criteria is probabilistic too. This design optimization philosophy is demonstrated through the classic buffer insertion problem. Formally, we capture wirelengths as probability distributions (as compared to the traditional approach which considers wirelength as fixed values) and present several strategies for optimizing the probabilistic criteria. During the course of this work many problems are proved to be NP-Complete. Comparisons are made with the Van-Ginneken ""optimal under fixed wire-length"" algorithm. Results show that the Van-Ginneken approach generated delay distributions at the root of the fanout wiring tree which had large probability (0.91 in the worst case and 0.55 on average) of violating the delay constraint. Our algorithms could achieve 100% probability of satisfying the delay constraint with similar buffer penalty. Although this work considers wirelength prediction inaccuracies, our probabilistic strategy could be extended trivially to consider fabrication variability in wire parasitics.","Design optimization,
Wiring,
Fabrication,
Cost function,
Wire,
Design automation,
Delay estimation,
Educational institutions,
Computer science,
Accuracy"
Toward explicit policy management for virtual organizations,"A virtual organization (VO) is a dynamic collection of distributed resources that are shared by a dynamic collection of users from one or more physical organizations. As grid computing technology is starting to facilitate truly large-scale VOs, issues are being raised regarding the purpose, architecture and operational mechanism of the VO. The emerging approach is essentially to define the VO as a particular set of users, whereby a ""VO server"" issues tokens to humans attesting to their membership in the VO. The problem with this approach is that there is little in the way of rules that describe the operation of the virtual organization or rules that govern the behavior of VO users and resources (and the ramifications of failing to meet the intent of the VO itself). Where such rules exist, they are implicit and therefore difficult to enforce in a consistent or automated manner. We identify two representative policies for existing and future VOs and, more generally, identifies issues and approaches for addressing the practical concerns for implementing any explicit VO policy: utilization measurement, accounting, enforcement conditions, enforcement actions, and security. A prototype implementation using .NET is described.","Humans,
Computer science,
Grid computing,
Large-scale systems,
Computer architecture,
Security,
Prototypes,
Distributed computing,
Collaborative work,
Conferences"
A resolution-like strategy based on a lattice-valued logic,"As the use of nonclassical logics becomes increasingly important in computer science, artificial intelligence and logic programming, the development of efficient automated theorem proving based on nonclassical logic is currently an active area of research. This paper aims at the resolution principle for the Pavelka type fuzzy logic (1979). Pavelka showed that the only natural way of formalizing fuzzy logic for truth-values in the unit interval [0, 1] is by using the Lukasiewicz's implication operator a/spl rarr/b=min{1,1-a+b} or some isomorphic forms of it. Hence, we first focus on the resolution principle for the Lukasiewicz logic L/sub /spl aleph// with [0, 1] as the truth-valued set. Some limitations of classical resolution and resolution procedures for fuzzy logic with Kleene implication are analyzed. Then some preliminary ideals about combining resolution procedure with the implication connectives in L/sub /spl aleph// are given. Moreover, a resolution-like principle in L/sub /spl aleph// is proposed and the soundness theorem of this resolution procedure is also proved. Second, we use this resolution-like principle to Horn clauses with truth-values in an enriched residuated lattice and consider the L-type fuzzy Prolog.","Fuzzy logic,
Logic programming,
Computer science,
Artificial intelligence,
Lattices,
Mathematics,
Expert systems,
Knowledge engineering,
Proposals"
Mobile agent fault tolerance for information retrieval applications: an exception handling approach,"Maintaining mobile agent availability in the presence of agent server crashes is a challenging issue since developers normally have no control over remote agent servers. A popular technique is that a mobile agent injects a replica into stable storage upon its arrival at each agent server. However, a server crash leaves the replica unavailable, for an unknown time period, until the agent server is back online. This paper uses exception handling to maintain the availability, of mobile agents in the presence of agent server crash failures. Two exception handler designs are proposed. The first exists at the agent server that created the mobile agent. The second operates at the previous agent server visited by the mobile agent. Initial performance results demonstrate that although the second design is slower it offers the smaller trip time increase in the presence of agent server crashes.","Mobile agents,
Fault tolerance,
Information retrieval,
Computer crashes,
Web server,
Application software,
Availability,
Computer science,
Internet,
Concurrent computing"
Year,,
Programming of the DSP2 board with the Matlab/Simulink,"The DSP2 board, based on the digital signal processor, has been developed at the Institute of Robotic, FERI, University of Maribor. For the mentioned board a set of Simulink blocks, so called DSP2 library for Simulink, was created under a real-time workshop. These blocks enable easy graphical programming of the different control algorithms under the Matlab/Simulink. At the end, with the real-time workshop and the code composer for the C3x4x digital signal processors, the binary executable code can be generated from the Simulink model and downloaded to the DSP2 board, where it is executed in real time. The DSP2 library for Simulink in combination with the DSP terminal, which was also developed at the at same institute, enables easy online monitoring of the DSP2 variables in the text or graphical mode and the parameter tuning, meanwhile the generated code is executed on the DSP2 board. With the DSP2 library for Simulink, the development time of the different control algorithms that are executed on the DSP2 board is reduced.",
Fast and robust super-resolution,"In the last two decades, many papers have been published, proposing a variety methods of multiframe resolution enhancement. These methods are usually very sensitive to their assumed model of data and noise, which limits their utility. This paper reviews some of these methods and addresses their shortcomings. We propose a different implementation using L/sub 1/ norm minimization and robust regularization to deal with different data and noise models. This computationally inexpensive method is robust to errors in motion and blur estimation, and results in sharp edges. Simulation results confirm the effectiveness of our method and demonstrate its superiority to other robust super-resolution methods.",
Dynamic coding technique for low-power data bus,"Designing chips for lower power applications is one of the most important challenges faced by the VLSI designers. Since the power consumed by I/O pins of a CPU is a significant source of power consumption, work has been done on developing encoding schemes for reducing switching activity on external buses. In this paper we propose a new coding technique, namely, the Dynamic Coding Scheme, for low-power data bus. Our method considers two logical groupings of the bus lines, each being a permutation of the bus lines, and dynamically selects that grouping which yields the minimum number of transitions.","DH-HEMTs,
Capacitance,
Pins,
Energy consumption,
Very large scale integration,
Circuits,
Hamming distance,
Computer science,
Power engineering and energy,
Application software"
Analysis of multirobot localization uncertainty propagation,"This paper deals with the problem of cooperative localization for the case of large groups of mobile robots. A Kalman filter estimator is implemented and tested for this purpose. The focus of this paper is to examine the effect on localization accuracy of the number N of participating robots and the accuracy of the sensors employed. More specifically, we investigate the improvement in localization accuracy per additional robot as the size of the team increases. Furthermore, we provide an analytical expression for the upper bound on the positioning uncertainty increase rate for a team of N robots as a function of N, the odometric and orientation uncertainty for each robot, and the accuracy of a robot tracker measuring relative positions between pairs of robots. The analytical results derived in this paper are validated in simulation for different test cases.","Uncertainty,
Robot sensing systems,
Position measurement,
Upper bound,
Mobile robots,
Testing,
Riccati equations,
Computer science,
Mechanical engineering,
Analytical models"
HIERAS: a DHT based hierarchical P2P routing algorithm,"Routing algorithm has great influence on system overall performance in peer-to-peer (P2P) applications. In current DHT based routing algorithms, routing tasks are distributed across all system peers. However, a routing hop could happen between two widely separated peers with high network link latency which greatly increases system routing overheads. We propose a new P2P routing algorithm - HIERAS to relieve this problem, it keeps scalability property of current DHT algorithms and improves system routing performance by the introduction of hierarchical structure. In HIERAS, we create several lower level P2P rings besides the highest level P2P ring. A P2P ring is a subset of the overall P2P overlay network. We create P2P rings in such a strategy that the average link latency between two peers in lower level rings is much smaller than higher level rings. Routing tasks are first executed in lower level rings before they go up to higher level rings, a large portion of routing hops previously executed in the global P2P ring are now replaced by hops in lower level rings, thus routing overheads can be reduced. The simulation results show HIERAS routing algorithm can significantly improve P2P system routing performance",
Just say no: benefits of early cache miss determination,"As the performance gap between the processor cores and the memory subsystem increases, designers are forced to develop new latency hiding techniques. Arguably, the most common technique is to utilize multi-level caches. Each new generation of processors is equipped with higher levels of memory hierarchy with increasing sizes at each level. In this paper, we propose 5 different techniques that will reduce the data access times and power consumption in processors with multi-level caches. Using the information about the blocks placed into and replaced from the caches, the techniques quickly determine whether an access at any cache level will be a miss. The accesses that are identified to miss are aborted. The structures used to recognize misses are much smaller than the cache structures. Consequently the data access times and power consumption are reduced. Using the SimpleScalar simulator, we study the performance of these techniques for a processor with 5 cache levels. The best technique is able to abort 53.1% of the misses on average in SPEC2000 applications. Using these techniques, the execution time of the applications is reduced by up to 12.4% (5.4% on average), and the power consumption of the caches is reduced by as much as 11.6% (3.8% on average).","Energy consumption,
Delay,
Computer science,
Microprocessors"
Orthogonal rational functions for system identification: numerical aspects,"Recently, there has been a growing interest in the use of orthogonal rational functions (ORFs) in system identification. There are many advantages over more classical techniques. Probably due to a known explicit expression for the basis functions when the orthogonality weight is uniformly equal to 1 (the so called Malmquist basis), the attention has been on the development of methods using this basis. However, for some discrete identification problems, this choice of the orthogonality weight may still lead to serious numerical problems due to the ill conditioning of the linear system of equations to be solved. In this note, we give an algorithm based on a more general system of ORF to overcome the numerical problem and which allows for a fast-order update of the estimate.","System identification,
Polynomials,
Time measurement,
Roundoff errors,
Linear systems,
Equations,
Computer science,
Transfer functions,
Discrete time systems,
Measurement units"
An educational tool for teaching compiler construction,"Compiler construction is a well-developed discipline since there is a long tradition of producing compilers supported by practical underlying theory and a large selection of textbooks. In the compiler construction course, students learn how to write a compiler by hand and how to generate a compiler using tools like lex and yacc. However, these tools usually have little or no didactical value. In this paper, the software tool LISA is described. It facilitates learning and conceptual understanding of compiler construction in an efficient, direct, and long-lasting way. The authors' experience in using the tool shows the following didactical benefits: support for constructive learning, stimulation of exploratory and active learning, support for different learning styles and learning speed, increased motivation for learning, and better understanding of concepts.","Program compilers,
Computer science education,
Student experiments,
Courseware"
Admission control in peer groups,"Security in collaborative peer groups is an active research topic. Most previous work focused on key management without addressing an important pre-requisite: admission control, i.e., how to securely admit a new member. This paper represents an initial attempt to sketch out an admission control framework suitable for different flavors of peer groups and match them with appropriate cryptographic techniques and protocols. Open problems and directions for future work are identified and discussed.","Admission control,
Security,
Peer to peer computing,
Computer science,
Collaborative work,
Cryptographic protocols,
Access control,
Authentication,
Cryptography,
Telephony"
Further analysis of interpolation effects in mutual information-based image registration,"This paper presents an analysis of the mutual information (MI) metric in rigid-body registration of two digital images, in particular, local fluctuations of the MI value due to interpolation. In contrast to existing work in this area, this paper starts with two hypothetical continuous images, based on which both sampling and interpolation effects are analyzed. This analysis indicates that an ""ideal"" interpolator may not be able to completely suppress the undesirable local minima of the MI metric if the sampling effect is not negligible. Several preprocessing methods are discussed for reducing the interpolation effects.","Image analysis,
Information analysis,
Interpolation,
Image registration,
Mutual information,
Image sampling,
Digital images,
Fluctuations,
Pixel,
Gold"
"Interactive hierarchical dimension ordering, spacing and filtering for exploration of high dimensional datasets","Large number of dimensions not only cause clutter in multi-dimensional visualizations, but also make it difficult for users to navigate the data space. Effective dimension management, such as dimension ordering, spacing and filtering, is critical for visual exploration of such datasets. Dimension ordering and spacing explicitly reveal dimension relationships in arrangement-sensitive multidimensional visualization techniques, such as parallel coordinates, star glyphs, and pixel-oriented techniques. They facilitate the visual discovery of patterns within the data. Dimension filtering hides some of the dimensions to reduce clutter while preserving the major information of the dataset. In this paper, we propose an interactive hierarchical dimension ordering, spacing and filtering approach, called DOSFA. DOSFA is based on dimension hierarchies derived from similarities among dimensions. It is scalable multi-resolution approach making dimensional management a tractable task. On the one hand, it automatically generates default settings for dimension ordering, spacing and filtering. On the other hand, it allows users to efficiently control all aspects of this dimension management process via visual interaction tools for dimension hierarchy manipulation. A case study visualizing a dataset containing over 200 dimensions reveals high dimensional visualization techniques.",
Localization for robotic assemblies with position uncertainty,"This paper presents a localization strategy for robotic assemblies with position uncertainty. The assembly of parts whose position uncertainty exceeds assembly clearance has to rely on either visual assistance or searching to achieve parts mating. We present a general strategy, applicable to arbitrary peg-in-hole assemblies, that localizes the misalignment of the mating parts in an efficient manner. The strategy explores the assembly contact configuration space and matches its observations to a pre-acquired C-space map. Simulations and experiments for various assembly scenarios are presented.","Robotic assembly,
Uncertainty,
Sun,
Gears,
Aerospace engineering,
Computer science,
Vehicle dynamics,
Aerodynamics,
Machine vision,
Motion planning"
Analysis of electromagnetic scattering by a plasma anisotropic sphere,"An analytical solution of electromagnetic fields in homogeneous plasma anisotropic media is obtained in this paper. In the source-free plasma anisotropic media, the source-free Maxwell's equations are utilized, where the expansion of plane wave factors is made in terms of the spherical vector wave functions in isotropic media, and the Fourier transformation is then applied. As a result, the field expressions represented using eigenfunctions are obtained in spectral domain. Applying boundary conditions on the spherical interface between air and plasma anisotropy, the electromagnetic fields of the plane wave scattered by a plasma anisotropic sphere are derived. Numerical results for the very general plasma dielectric material media are obtained, and those in a special case are compared between the present method and the Method of Moments (MoM) speeded up with the Conjugate-Gradient Fast-Fourier-Transform (CG-FFT) approach. The formulations in this paper can be generalized to those of more complex cases, such as layered plasma anisotropic sphere, plasma sheath, as well as plasma anisotropic material coated conducting sphere.","Plasmas,
Electromagnetic scattering,
Media,
Manganese,
Wave functions,
Anisotropic magnetoresistance"
Data-flow-based unit testing of aspect-oriented programs,"The current research so far in aspect-oriented software development is focused on problem analysis, software design, and implementation techniques. Even though the importance of software testing is known, it has received little attention in the aspect-oriented paradigm. In this paper, we propose a data-flow-based unit testing approach for aspect oriented programs. Our approach tests two types of units for an aspect-oriented program, i.e., aspects that are modular units of crosscutting implementation of the program, and those classes whose behavior may be affected by one or more aspects. For each aspect or class, our approach performs three levels of testing, i.e., intra-module, inter-module, and intra-aspect or intra-class testing. For an individual module such as apiece of advice, a piece of introduction, and a method, we perform intra-module testing. For a public module along with other modules it calls in an aspect or class, we perform inter-module testing. For modules that can be accessed outside the aspect or class and can be invoked in any order by users of the aspect or class, we perform intra-aspect or intra-class testing. Our approach can handle unit testing problems that are unique to aspect-oriented programs. We use control flow graphs to compute def-use pairs of an aspect or class being tested and use such information to guide the selection of test for the aspect or class.",
List-decoding using the XOR lemma,"We show that Yao's XOR Lemma, and its essentially equivalent rephrasing as a Direct Product Lemma, can be re-interpreted as a way of obtaining error-correcting codes with good list-decoding algorithms from error-correcting codes having weak unique-decoding algorithms. To get codes with good rate and efficient list decoding algorithms, one needs a proof of the Direct Product Lemma that, respectively, is strongly derandomized, and uses very small advice. We show how to reduce advice in Impagliazzo's proof of the Direct Product Lemma for pairwise independent inputs, which leads to error-correcting codes with O(n/sup 2/) encoding length, 0/sup /spl tilde//(n/sup 2/) encoding time, and probabilistic 0/sup /spl tilde//(n) list-decoding time. (Note that the decoding time is sub-linear in the length of the encoding.) Back to complexity theory, our advice-efficient proof of Impagliazzo's hard-core set results yields a (weak) uniform version of O'Donnell results on amplification of hardness in NP. We show that if there is a problem in NP that cannot be solved by BPP algorithms on more than a 1 - 1/(log n)/sup c/ fraction of inputs, then there is a problem in NP that cannot be solved by BPP algorithms on more than a 3/4 + 1/(log n)/sup c/ fraction of inputs, where c > 0 is an absolute constant.","Encoding,
Error correction codes,
Decoding,
Distributed computing,
Computer science,
Complexity theory,
Boolean functions,
Code standards"
Degrees of freedom in underspread mimo fading channels,,"MIMO,
Fading,
Signal to noise ratio,
Receiving antennas,
Uncertainty,
Computer science,
Rayleigh scattering,
Gallium nitride,
Transmitting antennas,
Channel capacity"
Using prosodic and conversational features for high-performance speaker recognition: report from JHU WS'02,"While there has been a long tradition of research seeking to use prosodic features, especially pitch, in speaker recognition systems, results have generally been disappointing when such features are used in isolation and only modest improvements have been seen when used in conjunction with traditional cepstral GMM systems. In contrast, we report here on work from the JHU 2002 Summer Workshop exploring a range of prosodic features, using as testbed the 2001 NIST Extended Data task. We examined a variety of modeling techniques, such as n-gram models of turn-level prosodic features and simple vectors of summary statistics per conversation side scored by k/sup th/ nearest-neighbor classifiers. We found that purely prosodic models were able to achieve equal error rates of under 10%, and yielded significant gains when combined with more traditional systems. We also report on exploratory work on ""conversational"" features, capturing properties of the interaction across conversation sides, such as turn-taking patterns.",
Sensor-centric quality of routing in sensor networks,"Standard embedded sensor network models emphasize energy efficiency and distributed decision-making by considering untethered and unattended sensors. To this we add two constraints - the possibility of sensor failure and the fact that each sensor must tradeoff its own resource consumption with overall network objectives. In this paper, we develop an analytical model of data-centric information routing in sensor networks under all the above constraints. Unlike existing techniques, we use game theory to model intelligent sensors thereby making our approach sensor-centric. Sensors behave as rational players in an N-player routing game, where they tradeoff individual communication and other costs with network wide benefits. The outcome of the sensor behavior is a sequence of communication link establishments, resulting in routing paths from reporting to querying sensors. We show that the optimal routing architecture is the Nash equilibrium of the N-player routing game and that computing the optimal paths (which maximizes payoffs of the individual sensors) is NP-hard with and without data-aggregation. We develop a game-theoretic metric called path weakness to measure the qualitative performance of different routing mechanisms. This sensor-centric concept which is based on the contribution of individual sensors to the overall routing objective is used to define the quality of routing (QoR) paths. Simulation results are used to compare the QoR of different routing paths derived using various energy-constrained routing algorithms.",
Hardware verification using ANSI-C programs as a reference,"We describe an algorithm to verify a hardware design given in Verilog using an ANSI-C program as a specification. We use SAT based bounded model checking in order to reduce the equivalence problem to a bit vector logic decision problem. As a case study, we describe experimental results on a hardware and a software implementation of the data encryption standard (DES) algorithm.","Hardware design languages,
Circuit testing,
Debugging,
Contracts,
Computer science,
Electronic mail,
Algorithm design and analysis,
Logic,
Cryptography,
Software standards"
Process-centric analytical processing of version control data,"We introduce a novel approach to enabling analytical processing of project data. The approach exploits source code repositories for information about project evolution. Furthermore we propose a new perspective on analyzing version control data. It takes up a process-centric viewpoint, addresses related analysis problems like collaboration of programmers and proposes metrics for them. The research has yielded an implementation of the approach, which comprises visualizations that assist in examining the evolution of software process.","Process control,
Data visualization,
Documentation,
Computer science,
Data analysis,
Open source software,
Collaboration,
Programming profession,
Control systems,
Automatic control"
Tripodal schematic design of the control architecture for the Service Robot PSR,"This paper describes a control architecture design and a system integration strategy for the autonomous service robot PSR (Public Service Robot). The PSR is under development at the KIST (Korea Institute of Science and Technology) for service tasks in public spaces such as office buildings and hospitals. The proposed control architecture is designed by tripodal frameworks, which are layered functionality diagram, class diagram, and configuration diagram. The tripodal schematic design clearly points out the way of integrating various hardware and software components. The developed strategy is implemented on the PSR and successfully tested.","Service robots,
Computer architecture,
Hardware,
Control systems,
Space technology,
Communication system control,
Mobile robots,
Infrared sensors,
Buildings,
Hospitals"
A new particle swarm optimiser for linearly constrained optimisation,"A new PSO algorithm, the linear PSO (LPSO), is developed to optimise functions constrained by linear constraints of the form Ax = b. A crucial property of the LPSO is that the possible movement of particles through vector spaces is guaranteed by the velocity and position update equations. This property makes the LPSO ideal in optimising linearly constrained problems. The LPSO is extended to the converging linear PSO, which is guaranteed to always find at least a local minimum.","Constraint optimization,
Particle swarm optimization,
Computer science,
Africa,
Organisms,
Optimization methods,
Topology,
Vectors,
Equations,
Ear"
New directions in instantiation-based theorem proving,"We consider instantiation-based theorem proving whereby instances of clauses are generated by certain inferences, and where inconsistency is detected by proposition tests. We give a model construction proof of completeness by which restrictive inference systems as well as admissible simplification techniques can be justified. Another contribution of the paper are inference systems that allow one to also employ decision procedures for first-order fragments more complex than propositional logic. The decision provides for an approximate consistency test, and the instance generation inference system is a means of successively refining the approximation.",
3D face recognition by profile and surface matching,"In this paper, we presented an approach for automatic face verification from range data. The method consists of profile and surface matching. The profile is extracted on the basis of symmetry of human face, and a global profile matching method based on k-th Hausdorff distance is used to align and compare profiles, without detection of fiducial points that is often unreliable. For each individual, a statistical model of facial surface is built to represent the distinct discriminative capability of the different parts in the facial surface. Then the model is incorporated into a weighted distance function to measure similarity of surfaces. Finally two experts are combined to give a decision. The comparable experimental results are obtained on a database with 180 pieces of range data of 30 individuals.",
Improving MPI-IO output performance with active buffering plus threads,"Efficient collective output of intermediate results to secondary storage becomes more and more important for scientific simulations as the gap between processing power/interconnection bandwidth and the I/O system bandwidth enlarges. Dedicated servers can offload I/O from compute processors and shorten the execution time, but it is not always possible or easy for an application to use them. We propose the use of active buffering with threads (ABT) for overlapping I/O with computation efficiently and flexibly without dedicated I/O servers. We show that the implementation of ABT in ROMIO, a popular implementation of MPI-IO, greatly reduces the application-visible cost of ROMIO's collective write calls, and improves an application's overall performance by hiding I/O cost and saving implicit synchronization overhead from collective write operations. Further, ABT is high-level, platform-independent, and transparent to users, giving users the benefit of overlapping I/O with other processing tasks even when the file system or parallel I/O library does not support asynchronous I/O.","Yarn,
Computational modeling,
File systems,
Concurrent computing,
Distributed computing,
Bandwidth,
Costs,
Libraries,
Writing,
Computer science"
The influence of rendering quality on presence and task performance in a virtual environment,"This user study investigated how rendering quality contributes to the sense of presence and level of task performance of participants in a virtual environment. Fifty-five participants performed the task of dropping four objects in a two-room, virtual environment which was drawn with varying degrees of texture and lighting quality. Their physiological reactions were measured along with their accuracy at dropping objects onto a target. In addition, a series of questionnaires were administered to examine the participant's subjective sense of presence and object recall. An increase in physiological response, particularly heart rate, was demonstrated across all rendering conditions. Gender differences were noted in physiological responses, spatial ability tests, and simulator sickness questionnaires.","Virtual environment,
Heart rate,
Testing,
Virtual reality,
Computer science,
Psychology,
Brain modeling,
Painting,
Lighting,
Packaging"
A shoulder pad insert vibrotactile display,,
Exploring the relationship between parallel application run-time and network performance in clusters,"Highly variable parallel application execution time is a persistent issue in cluster computing environments, and can be particularly acute in systems composed of networks of workstations (NOWs). We are looking at this issue in terms of consistency. In particular, we are focusing on network performance. Before we can use techniques from fault management to attain consistency, this paper presents our preliminary analysis of run-time variability from logs and experiments, exposing important issues related to systemic inconsistency in NOW clusters. The characterization of application sensitivity can be used to set network performance goals, thereby defining operational requirements. Network performance depends on the virtual topology imposed by the scheduler's allocation of nodes and the communication patterns of the set of running applications. Therefore it is important to look at both the network and the cluster's centralized node mapper (scheduler) as critical subsystems.","Runtime,
Intelligent networks,
Application software,
Network topology,
Processor scheduling,
Resource management,
Degradation,
Computer science,
Computer networks,
Workstations"
Ad hoc grids: communication and computing in a power constrained environment,"We introduce ad hoc grids as a hierarchy of mobile devices with different computing and communication capabilities. An ad hoc grid allows a group of individuals to accomplish a mission, often in a hostile environment; examples of applications of ad hoc grids are disaster management, wild-fire prevention, and peacekeeping operations. We are concerned with the interplay between computing and communication in the power-constrained environment of an ad hoc grid.","Grid computing,
USA Councils,
Collaboration,
Topology,
Energy consumption,
Power engineering computing,
Computer science,
Mobile computing,
Mobile communication,
Fault tolerant systems"
Observational studies to accelerate process experience in classroom studies: an evaluation,"Software engineering studies run in classroom environments can and have made important contributions to empirical software engineering. Because the goal of such studies is to improve the state of the practice in industry, researchers must understand and account for the differences between university students and industrial professionals. One major difference identified is the amount of training and practice that students and professional may have when learning a new technique. We propose and test a method of allowing university subjects to cost-effectively gain experience to compensate for this difference. The results show that the proposed method for gaining experience provided subjects with enough experience to improve their effectiveness in some but not all cases. There was also an indication from the results that the proposed method allowed the subjects to become more comfortable with a new technique.","Acceleration,
Software engineering,
Industrial training,
Inspection,
Computer science,
Software quality,
Programming,
Software measurement,
Testing,
Image analysis"
Reducing internal and external fragmentations of OVSF codes in WCDMA systems with multiple codes,"In the 3GPP technical specification, OVSF codes are used as channelization codes. The use of OVSF codes can provide variable data rates to flexibly support applications with different bandwidth requirements. Most works in the literature assume that only one signal OVSF code is used to support one connection. This may sometimes waste the scarce wireless bandwidth since the allocated bandwidth will increase exponentially as the spreading factor decreases, i.e., a user may be ""over-served"". In this paper, we consider the possibility of using multiple OVSF codes to support a connection. We show how using multiple codes can reduce internal fragmentation and external fragmentation of a OVSF code tree. The tradeoff between bandwidth utilization and hardware complexity of a multi-code system is analyzed. The result shows that using 2 or 3 codes will be quite cost-effective. Several multi-code assignment and reassignment strategies, namely random, left-most, crowded-first-space, and crowded-first-code, are proposed based on such environment. Our simulation results show significant increase in code tree utilization and significant reduction in code blocking probability by using the crowded-first-space and crowded-first-code schemes.","Multiaccess communication,
Bandwidth,
Computer science,
Hardware,
3G mobile communication,
Multicarrier code division multiple access,
Chaotic communication,
Application software,
Personal communication networks,
Wireless communication"
Parallel network simulation under distributed Genesis,"We describe two major developments in the general network simulation integration system (Genesis): the support for BGP protocol in large network simulations and distribution of the simulation memory among Genesis component simulations. Genesis uses a high granularity synchronization mechanism between parallel simulations simulating parts of a network. This mechanism uses checkpointed simulation state to iterate over the same time interval until convergence. It also replaces individual packet data for flows crossing the network partitions with statistical characterization of such flows over the synchronization time interval. We had achieved significant performance improvement over the sequential simulation for simulations with TCP and UDP traffic. However, this approach cannot be used directly to simulate dynamic routing protocols that use underlying network for exchanging protocol information, as no packets are exchanged in Genesis between simulated network parts. We have developed a new mechanism to exchange and synchronize BGP routing data among distributed Genesis simulators. The extended Genesis allows simulations of more realistic network scenarios, including routing flows, in addition to TCP or UDP data traffic. Large memory size required by simulation software hinders the simulation of large-scale networks. Based on our new support of distributed BGP simulation, we developed an approach to construct and simulate networks on distributed memory using Genesis simulators in such a way that each participating processor possesses only data related to the part of the network it simulates. This solution supports simulations of large-scale networks on machines with modest memory size.","Computational modeling,
Discrete event simulation,
Computer simulation,
Telecommunication traffic,
Traffic control,
Large-scale systems,
Convergence,
Routing protocols,
Frequency synchronization,
Computer science"
Random flow network modeling and simulations for DDoS attack mitigation,"Recent events show that distributed denial-of-service (DDoS) attack imposes great threat to availability of Internet services. In this paper, we study and evaluate DDoS attacks in a random flow network model, a novel and general approach to DDoS attack prevention and tolerance. The model can be used to evaluate the effectiveness of a DDoS countermeasure framework. Following the random flow network model and state-of-art Internet topology and traffic models, our simulation reveals that general relationship among several metrics derived from the model. Based on the simulation results, we suggest to build a more complete and effective DDoS countermeasure framework using complementary solutions to achieve DDoS attack detection, prevention, and tolerance at same time.","Computer crime,
IP networks,
Web and internet services,
Network topology,
Telecommunication traffic,
Traffic control,
Computational modeling,
Computer simulation,
Discrete event simulation,
Computer science"
Optimal coverage paths in ad-hoc sensor networks,"This paper discusses the computation of optimal coverage paths in an ad-hoc network consisting of n sensors. Improved algorithms, with a preprocessing time of O(n log n), to compute a maximum breach/support path P in optimal (|P|) time or the maximum breach/support value in O(1) time are presented. Algorithms for computing a shortest path that has maximum breach/support are also provided. Experimental results for breach paths show that the shortest path length is on the average 30% less and is not much worse that the ideal straight line path. For applications that require redundancy (i.e., detection by multiple sensors), a generalization of Voronoi diagrams allows us to compute maximum breach paths where breach is defined as the distance to the kth nearest sensor in the field. Extensive experimental results are provided.",
Conservative Scheduling: Using Predicted Variance to Improve Scheduling Decisions in Dynamic Environments,"In heterogeneous and dynamic environments, efficient execution of parallel computations can require mappings of tasks to processors whose performance is both irregular (because of heterogeneity) and time-varying (because of dynamicity). While adaptive domain decomposition techniques have been used to address heterogeneous resource capabilities, temporal variations in those capabilities have seldom been considered. We propose a conservative scheduling policy that uses information about expected future variance in resource capabilities to produce more efficient data mapping decisions. We first present techniques, based on time series predictors that we developed in previous work, for predicting CPU load at some future time point, average CPU load for some future time interval, and variation of CPU load over some future time interval. We then present a family of stochastic scheduling algorithms that exploit such predictions of future availability and variability when making data mapping decisions. Finally, we describe experiments in which we apply our techniques to an astrophysics application. The results of these experiments demonstrate that conservative scheduling can produce execution times that are both significantly faster and less variable than other techniques.","Dynamic scheduling,
Processor scheduling,
Computer science,
Stochastic processes,
Scheduling algorithm,
Government,
Mathematics,
Concurrent computing,
Astrophysics,
Personal communication networks"
Reducing datapath energy through the isolation of short-lived operands,"We present a technique for reducing the power dissipation in the course of writebacks and commitments in a datapath that uses a dedicated architectural register file (ARF) to hold committed values. Our mechanism capitalizes on the observation that most of the produced register values are short-lived, meaning that the destination registers targeted by these values are renamed by the time the results are written back. Our technique avoids unnecessary writebacks into the result repository (a slot within the reorder buffer or a physical register) as well as writes into the ARF by caching (and isolating) short-lived operands within a small dedicated register file. Operands are cached in this manner till they can be safely discarded without jeopardizing the recovery from possible branch mispredictions or reconstruction of the precise state in case of interrupts or exceptions. The power/energy savings are validated using SPICE measurements of actual layouts in a 0.18 micron CMOS process. The energy reduction in the ROB and the ARF is in the range of 20-25% and this is achieved with no increase in the cycle time, little additional complexity and no IPC drop.","Registers,
Power dissipation,
Microarchitecture,
Temperature,
Microprocessors,
Integrated circuit reliability,
Processor scheduling,
Dynamic scheduling,
Computer science,
SPICE"
Structure from motion for scenes without features,"We describe an algorithm for reconstructing the 3D (three-dimensional) shape of the scene and the relative pose of a number of cameras from a collection of images under the assumption that the scene does not contain photometrically distinct ""features"". We work under the explicit assumption that the scene is made of a number of smooth surfaces that radiate constant energy isotropically in all directions, and setup a region-based cost functional that we minimize using local gradient flow techniques.","Layout,
Cameras,
Shape,
Photometry,
Image reconstruction,
Surface reconstruction,
Cost function,
Computer vision,
Computer science,
Geometry"
List decoding from erasures: bounds and code constructions,"We consider the problem of list decoding from erasures. We establish lower and upper bounds on the rate of a (binary linear) code that can be list decoded with list size L when up to a fraction p of its symbols are adversarially erased. Such bounds already exist in the literature, albeit under the label of generalized Hamming weights, and we make their connection to list decoding from erasures explicit. Our bounds show that in the limit of large L, the rate of such a code approaches the ""capacity"" (1 - p) of the erasure channel. Such nicely list decodable codes are then used as inner codes in a suitable concatenation scheme to give a uniformly constructive family of asymptotically good binary linear codes of rate /spl Omega/(/spl epsiv//sup 2//log(1//spl epsiv/)) that can be efficiently list-decoded using lists of size O(1//spl epsiv/) when an adversarially chosen (1 - /spl epsiv/) fraction of symbols are erased, for arbitrary /spl epsiv/ > 0. This improves previous results in this vein, which achieved a rate of /spl Omega/(/spl epsiv//sup 3/log(1//spl epsiv/)).","Decoding,
Linear code,
Hamming weight,
Error correction codes,
Materials science and technology,
Computer science,
Binary codes,
Upper bound,
Veins"
A simple simultaneous geometric and intensity correction method for echo-planar imaging by EPI-based phase modulation,"A technique, based on echo planar imaging (EPI)-based phase modulation factor maps, is described for correction of EPI distortions resulting from field inhomogeneity. In this paper, a phase modulation factor was employed to remove the distortions. The phase modulation factor was obtained experimentally by collecting EPI images with a spin-echo (TE) spacing, /spl Delta/TE, equal to the inter-echo time interval, T/sub i/. Then, the distortions resulting from the field inhomogeneity were removed by modulating the k-space data with the phase modulation factor. One of the advantages of this method is that it requires only a few extra scans to collect the information on field inhomogeneity. The proposed method does not require a phase unwrapping procedure for field inhomogeneity correction and, hence, is easier to implement, compared to other techniques. In addition, it corrects geometric distortion as well as intensity distortions simultaneously, which is robust to external noise or estimation error in severely distorted images. In this work, we also compared the proposed technique with others including, a) interpolation method with EPI-based displacement maps, and b) modulation method with phase modulation factor maps generated from spin-echo images. The results suggest the proposed technique is superior in correcting severely distorted images.","Phase modulation,
Nonuniform electric fields,
High-resolution imaging,
Phase distortion,
Biomedical imaging,
Interpolation,
Magnetic resonance imaging,
Encoding,
Chemicals,
Pixel"
Wrapping legacy codes for Grid-based applications,"This paper describes a process for the semi-automatic conversion of numerical and scientific routines written in the C programming language into Triana-based computational services that can be used within a distributed service-oriented architecture such as that being adopted for Grid computing. This process involves two separate but related tools, JACAW and MEDLI. JACAW is a wrapper tool based on the Java Native Interface (JNI) that can automatically generate the Java interface and related files for any C routine, or library of C routines. The MEDLI tool can then be used to assist the user in describing the mapping between the Triana and C data types involved in calling a particular routine. In this paper we describe both JACAW and MEDLI, and demonstrate how they are used in practice to convert legacy code into Grid services.","Wrapping,
Java,
Grid computing,
Application software,
Software libraries,
Graphical user interfaces,
Packaging,
Computer science,
Physics,
Astronomy"
Transformation-based diagnosis of student programs for programming tutoring systems,"A robust technology that automates the diagnosis of students' programs is essential for programming tutoring systems. Such technology should be able to determine whether programs coded by a student are correct. If a student's program is incorrect, the system should be able to pinpoint errors in the program as well as explain and correct the errors. Due to the difficulty of this problem, no existing system performs this task entirely satisfactorily, and this problem still hampers the development of programming tutoring systems. This paper describes a transformation-based approach to automate the diagnosis of students' programs for programming tutoring systems. Improved control-flow analysis and data-flow analysis are used in program analysis. Automatic diagnosis of student programs is achieved by comparing the student program with a specimen program at the semantic level after both are standardized. The approach was implemented and tested on 525 real student programs for nine different programming tasks. Test results show that the method satisfies the requirements stated above. Compared to other existing approaches to automatic diagnosis of student programs, the approach developed here is more rigorous and safer in identifying student programming errors. It is also simpler to make use of in practice. Only specimen programs are needed for the diagnosis of student programs. The techniques of program standardization and program comparison developed here may also be useful for research in the fields of program understanding and software maintenance.",
"Coarse, inexpensive, infrared tracking for wearable computing",,"Wearable computers,
Optical receivers,
Testing,
Computer science,
User interfaces,
Azimuth,
Augmented reality,
Costs,
Taxonomy,
Radio transmitters"
Enhancements on local outlier detection,"Outliers, commonly referred to as exceptional cases, exist in many real-world databases. Detection of such outliers is important for many applications. In this paper, we focus on the density-based notion that discovers local outliers by means of the local outlier factor (LOF) formulation. Three enhancement schemes over LOF are introduced, namely LOF' and LOF"" and GridLOF. Thorough explanation and analysis is given to demonstrate the abilities of LOF' in providing simpler and more intuitive meaning of local outlier-ness; LOF"" in handling cases where LOF fails to work appropriately; and GridLOF in improving the efficiency and accuracy.","Databases,
Clustering algorithms,
Optical noise,
Data engineering,
Statistical distributions,
Computer science,
Application software,
Failure analysis,
Telecommunications,
Credit cards"
A diversity-controlling adaptive genetic algorithm for the vehicle routing problem with time windows,"This paper presents an adaptive genetic algorithm (GA) to solve the vehicle routing problem with time windows (VRPTW) to near optimal solutions. The algorithm employs a unique decoding scheme with the integer strings. It also automatically adapts the crossover probability and the mutation rate to the changing population dynamics. The adaptive control maintains population diversity at user-defined levels, and therefore prevents premature convergence in search. Comparison between this algorithm and a normal fixed parameter GA clearly demonstrates the advantage of population diversity control. Our experiments with the 56 Solomon benchmark problems indicate that this algorithm is competitive and it paves way for future research on population-based adaptive genetic algorithm.","Genetic algorithms,
Routing,
Decoding,
Genetic mutations,
Adaptive control,
Computational modeling,
Biological cells,
Intelligent vehicles,
Computer science,
Vehicle dynamics"
"A component architecture for an extensible, highly integrated context-aware computing infrastructure","Ubiquitous context-aware computing systems present several challenges in their construction. Principal among them is the tradeoff between easily providing new context-aware services to users and the tight integration of those services, as demanded by the small form factor of the devices typically found in ubiquitous computing environments. Performance issues further complicate the management of this tradeoff. Mechanisms have been proposed and toolkits developed for aiding the construction of context-aware systems, but there has been little consideration of how to specialize, organize, and compose these mechanisms to meet the above requirements. We motivate and describe a software architecture that provides the desired integration and extensibility of services in a context-aware application infrastructure. A key result is the fissioning of intuitive class organizations, both across layers and within layers, to achieve the required integration of services and separation of concerns.","Component architectures,
Context-aware services,
Ubiquitous computing,
Computer architecture,
Buildings,
Computer science,
Software architecture,
Application software,
Navigation,
Sensor systems"
Interagent communication and synchronization support in the DaAgent mobile agent-based computing system,"This paper describes the design, implementation, and evaluation of interagent communication and synchronization models in the DaAgent mobile-agent based computing system. Based on the requirements of some sample Internet computing applications, eight system-level models of interagent communication and synchronization are proposed. A new synchronization mechanism called location synchronization that is relevant for interacting mobile agents is also proposed. This paper evaluates the eight models based on their utility, performance, level of communication and synchronization support, and applicability in the Internet computing environment. A prototype implementation and detailed performance evaluation of these models based on two interacting, multiagent applications are also presented.","Mobile communication,
Mobile computing,
Internet,
Mobile agents,
Distributed computing,
Operating systems,
Power system modeling,
Computer science,
Computer applications,
Prototypes"
Binocular Helmholtz stereopsis,"Helmholtz stereopsis has been introduced recently as a surface reconstruction technique that does not assume a model of surface reflectance. In the reported formulation, correspondence was established using a rank constraint, necessitating at least three viewpoints and three pairs of images. Here, it is revealed that the fundamental Helmholtz stereopsis constraint defines a nonlinear partial differential equation, which can be solved using only two images. It is shown that, unlike conventional stereo, binocular Helmholtz stereopsis is able to establish correspondence (and thereby recover surface depth) for objects having an arbitrary and unknown BRDF and in textureless regions (i.e., regions of constant or slowly varying BRDF). An implementation and experimental results validate the method for specular surfaces with and without texture.","Surface reconstruction,
Image reconstruction,
Reflectivity,
Stereo vision,
Computer science,
Surface texture,
Shape,
Partial differential equations,
Light sources,
Cameras"
World embedded interfaces for human-robot interaction,"Human interaction with large numbers of robots or distributed sensors presents a number of difficult challenges including supervisory management, monitoring of individual and collective state, and apprehending situation awareness. A rich source of information about the environment can be provided even with robots that have no explicit representations or maps of their locale. To do this, we transform a robot swarm into a distributed interface embedded within the environment. Visually, each robot acts like a pixel within a much larger visual display space so that any robot need only communicate a small amount of information from its current location. Our approach uses augmented reality techniques for communicating information to humans from large numbers of small-scale robots to enable situation awareness, monitoring, and control for surveillance, reconnaissance, hazard detection, and path finding.","Orbital robotics,
Robot sensing systems,
Monitoring,
Human robot interaction,
Information resources,
Displays,
Augmented reality,
Surveillance,
Reconnaissance,
Hazards"
The dynamic modeling and analysis for an omnidirectional mobile robot with three caster wheels,"Recently quite a few applications of an omnidirectional mobile robot have been reported. However, understanding some fundamental issues still remains as further study. One of the issues is the exact dynamic model. Previous studies very often ignore the wheel dynamics of the mobile robot and suffer from algorithmic singularity. Thus, actuator sizing or control algorithms based on the incomplete plant model does not guarantee the control performance of the system. This paper deals with the singularity-free, exact dynamic modeling and analysis of an omnidirectional mobile robot with three caster wheels. Initially, the exact dynamic model of the mobile robot including the wheel dynamics is introduced. A natural orthogonal complement approach is also introduced. The joint-space and operational-space dynamic models are derived as analytical forms. Through simulation, the discrepancy of the incomplete dynamic model is shown by comparison with the exact dynamic model. Furthermore, the useful aspect of operational dynamics in terms of impact geometry is also discussed.","Mobile robots,
Wheels,
Kinematics,
Roads,
Computer science,
Instruments,
Computer industry,
Industrial control,
Application software,
Actuators"
An agent-based approach to multisensor coordination,"This paper presents an automated system for multiple sensor placement based on the coordinated decisions of independent, intelligent agents. The problem domain is such that a single sensor system would not provide adequate information for a given sensor task. Hence, it is necessary to incorporate multiple sensors in order to obtain complete information. The overall goal of the system is to provide the surface coverage necessary to perform feature inspection on one or more target objects in a cluttered scene. This is accomplished by a group of cooperating intelligent sensors. In this system, the sensors are mobile, the target objects are stationary and each agent controls the position of a sensor and has the ability to communicate with other agents in the environment. By communicating desires and intentions, each agent develops a mental model of the other agents' preferences, which is used to avoid or resolve conflict situations. In this paper we utilize cameras as the sensors. The experimental results illustrate the feasibility of the autonomous deployment of the sensors and that this deployment can occur with sufficient accuracy as to allow the inspection task to be performed.","Intelligent sensors,
Cameras,
Sensor systems,
Layout,
Intelligent agent,
Inspection,
Art,
Sensor phenomena and characterization,
Control systems,
Cognitive science"
Adaptive intrusion detection with data mining,"A major constraint of an anomaly-based intrusion detection system (IDS) lies in its inability to adapt to distinguish these changes from intrusive behavior. To overcome these obstacles, the normal profile must be updated at regular intervals. The naive approach of exhaustively recomputing the normal profile is often not viable and can incorporate patterns of intrusive behavior as normal. We address technical issues and present an adaptive data mining framework for anomaly detection. We employ a sliding window approach and use only the audit data inside that sliding window to update the profile. Instead of performing an exhaustive update, we use some heuristics to decide when to update. Experimental results using real network traffic data (containing simulated intrusion attacks) demonstrate the effectiveness of the proposed framework.","Intrusion detection,
Data mining,
Association rules,
Computer science,
Bridges,
Buffer overflow,
Floods,
Telecommunication traffic,
Laboratories,
Databases"
SPADES - a distributed agent simulation environment with software-in-the-loop execution,"Simulations are used extensively for studying artificial intelligence. However, the simulation technology in use by and designed for the artificial intelligence community often fails to take advantage of much of the work by the larger simulation community to produce distributed, repeatable, and efficient simulations. We present the system for parallel agent discrete event simulation, (SPADES), which is a simulation environment for the artificial intelligence community. SPADES focuses on the agent as a fundamental simulation component. The thinking time of an agent is tracked and reflected in the results of the agents' actions by using a software-in-the-loop mechanism. SPADES supports distributed execution of the agents across multiple systems, while at the same time producing repeatable results regardless of network or system load. We discuss the design of SPADES and give experimental results. SPADES is flexible enough for a variety of application domains in the artificial intelligence research community.","Computational modeling,
Artificial intelligence,
Discrete event simulation,
Computer simulation,
Design engineering,
Concurrent computing,
Distributed computing,
Computer science,
Machine learning,
Educational institutions"
A dual-channel location estimation system for providing location services based on the GPS and GSM networks,"A dual channel system, which is based on the GPS and the GSM network, is being developed to compensate the problem of the loss of GPS signals in providing location services to mobile users in urban areas. In this design, when GPS signals are being blocked in blind spot areas, GSM positioning algorithms would be used as an alterative method to provide location estimations. This research is an investigation in search of a set of location estimation algorithms based on signal attenuation to work with GPS, so as to develop a dual channel positioning system. With the technical support from a local mobile operator we have constructed and conducted several real world experiments for our investigation and results are promising.","Global Positioning System,
GSM,
FCC,
Mobile handsets,
Cities and towns,
Satellites,
Telephone sets,
Cellular phones,
Base stations,
Computer science"
Optimal rotor design for brushless doubly fed reluctance machines,"This paper examines the impact of the rotor design on the torque and power factor of the brushless doubly fed reluctance machine (BDFRM) with an axially laminated rotor and radially laminated rotor. The performance of this type of machine depends on the ability of the rotor to couple the field of the primary winding to the secondary and vice versa. Models to date have been based on a first order approach ignoring higher order harmonics. This paper aims to evaluate the theoretical model of this machine by examining higher order harmonics in the gap field distribution, with an aim to determine an optimal rotor design for torque generation and machine power factor.","Reluctance machines,
Machine windings,
Torque,
Rotors,
Magnetic analysis,
Computer science,
Reactive power,
Steady-state,
Tensile stress,
Power system harmonics"
Visualization in atomistic and spin simulations,"Certain basic demands from visualization code are common to all atomistic or spin systems; some are also relevant to more general visualization needs. Many commercial and homegrown visualization codes could meet these demands, but obviously the specific code used is less important than the fact that a visualization is made. The author describes in detail one specific public-domain code that meets these basic demands: the AViz package developed at the Technion.","Visualization,
Costs,
Computational modeling,
Hardware,
Animation,
Libraries,
Collaboration,
Switches,
Open source software,
Linux"
On zero-error coding of correlated sources,"The problem of separate zero-error coding of correlated sources is considered. Inner and outer single-letter bounds are established for the achievable rate region, and conditions for their coincidence are investigated. It is shown that successive encoding combined with time sharing is not always an optimal coding strategy. Conditions for its optimality are derived. The inner bound to the achievable rate region follows as a special case of the single-letter characterization of a generalized zero-error multiterminal rate-distortion problem. The applications of this characterization to a problem of remote computing are also explored. Other results include (i) a product-space characterization of the achievable rates, (ii) bounds for finite block length, and (iii) asymptotic fixed-length rates.","Entropy,
Rate-distortion,
Information theory,
Materials science and technology,
Encoding,
Time sharing computer systems,
Decoding,
Laboratories,
Source coding,
Memoryless systems"
A multi-layer model for anomaly intrusion detection using program sequences of system calls,,"Intrusion detection,
Hidden Markov models,
Databases,
Automata,
Machine learning,
Data mining,
Power system reliability,
Computer science,
Information technology,
Delay"
Modeling lobed leaves,"In contrast to the extensively researched modeling of plant architecture, the modeling of plant organs largely remains an open problem. We propose a method for modeling lobed leaves. This method extends the concept of sweeps to branched skeletons. The input of the model is a 2D leaf silhouette, which can be defined interactively or derived from a scanned leaf image. The algorithm computes the skeleton (medial axis) of the leaf and approximates it using spline curves interconnected into a branching structure (sticky splines). The leaf surface is then constructed by sweeping a generating curve along these splines. The orientation of the generating curve is adjusted to properly capture the shape of the leaf blade near the extremities and branching points of the skeleton, and to avoid self-intersections of the surface. The leaf model can be interactively modified by editing the shape of the silhouette and the skeleton. It can be further manipulated in 3D using functions that control turning, bending, and twisting of each lobe.","Brain modeling,
Skeleton,
Spline,
Shape control,
Turning,
Computer graphics,
Deformable models,
Computer science,
Computer architecture,
Blades"
LLVA: a low-level virtual instruction set architecture,"A virtual instruction set architecture (V-ISA) implemented via a processor-specific software translation layer can provide great flexibility to processor designers. Recent examples such as Crusoe and DAISY, however, have used existing hardware instruction sets as virtual ISAs, which complicates translation and optimization. In fact, there has been little research on specific designs for a virtual ISA for processors. This paper proposes a novel virtual ISA (LLVA) and a translation strategy for implementing it on arbitrary hardware. The instruction set is typed, uses an infinite virtual register set in static single assignment form, and provides explicit control-flow and dataflow information, and yet uses low-level operations closely matched to traditional hardware. It includes novel mechanisms to allow more flexible optimization of native code, including a flexible exception model and minor constraints on self-modifying code. We propose a translation strategy that enables offline translation and transparent offline caching of native code and profile information, while remaining completely OS-independent. It also supports optimizations directly on the representation at install-time, runtime, and offline between executions. We show experimentally that despite its rich information content, virtual object code is comparable in size to native machine code, virtual instructions expand to only 2-4 ordinary hardware instructions on average, and simple translation costs under 1% of total execution time except for very short runs.","Hardware,
Instruction sets,
Computer architecture,
Process design,
Microarchitecture,
Computer aided instruction,
Operating systems,
Application software,
Computer science,
Registers"
Modeling and simulation best practices for wireless ad hoc networks,"This paper calls attention to important practices in the modeling and the simulation of wireless ad hoc networks. We present three case studies to highlight the importance of following well-established simulation techniques, of carefully describing experimental study scenarios, and, finally, of understanding assumptions sometimes unstated in the framework of a simulator. The first case addresses the initial transient problem inherent to mobility and traffic generation sub-models. We quantitatively demonstrate how these transients can affect the simulation. Our second case illustrates the fact that strong scientific contributions can only be made via simulation studies when the models used are unambiguously specified. The example we use are simulations with and without a model for the ARP protocol. Finally, our third case discusses the importance of understanding the simulation tool and any default values used for model parameters. The example used relates to the use of the limited interference model.","Best practices,
Ad hoc networks,
Computational modeling,
Protocols,
Mobile ad hoc networks,
Computer simulation,
Computer science,
Educational institutions,
Mathematical model,
Wireless networks"
General-Purpose Computation with Neural Networks: A Survey of Complexity Theoretic Results,"We survey and summarize the literature on the computational aspects of neural network models by presenting a detailed taxonomy of the various models according to their complexity theoretic characteristics. The criteria of classification include the architecture of the network (feedforward versus recurrent), time model (discrete versus continuous), state type (binary versus analog), weight constraints (symmetric versus asymmetric), network size (finite nets versus infinite families), and computation type (deterministic versus probabilistic), among others. The underlying results concerning the computational power and complexity issues of perceptron, radial basis function, winner-take-all, and spiking neural networks are briefly surveyed, with pointers to the relevant literature. In our survey, we focus mainly on the digital computation whose inputs and outputs are binary in nature, although their values are quite often encoded as analog neuron states. We omit the important learning issues.",
A framework for transfer colors based on the basic color categories,"Usually, paintings are more appealing than photographic images. This is because paintings have styles. This style can be distinguished by looking at elements such as motif, color, shape deformation and brush texture. We focus on the effect of ""color"" element and devise a method for transforming the color of an input photograph according to a reference painting. To do this, we consider basic color category concepts in the color transformation process. By doing so, we achieve large but natural color transformations of an image.","Painting,
Brushes,
Image color analysis,
Information science,
Shape,
Paints,
Brightness,
Rendering (computer graphics),
Milling machines,
Lamps"
Robotics in the classroom,"This article presents an integrated approach for incorporating robotics into secondary education with the objective of further engaging students through an exciting application of math, computers, and science. One of the principal objectives of this university-based outreach effort is to promote the creation of independent programs at the secondary-school level in New Mexico by providing a structured set of resources from which the schools may begin. These resources include an adaptable mobile robot kit with detailed do-it-your-self online instructions, various Web-based course offerings with material directed to both secondary students and teachers, and a personnel support structure, including college engineering faculty, secondary teachers, practicing engineers, undergraduate engineering students, and junior-high and high-school students. An outreach program to integrate robotics into secondary education.",
"Displaying aggregate data, interrelated quantities, and data trends in electric power systems","This paper describes a number of effective techniques for visualizing some of the more complex data relationships that characterize an electric power system in real time. Power systems are large, dynamic physical entities that are constantly changing. While SCADA systems capture the quantitative aspects of these changes, visualizing their magnitudes, pinpointing their locations, and interpreting their collective significance for the current and future security of the interconnection pose tremendous challenges for system operators. This paper describes how advanced visualization techniques such as area tie diagrams, calculated data analogs, historical trend animations, and three-dimensional views clarify the complex relationships, aggregate subsystem characteristics, and emerging trends that describe the current state of the interconnection and help predict its future evolution. The paper provides a number of illustrations that demonstrate the effectiveness of the proposed techniques.","Aggregates,
Data visualization,
Power system security,
Personnel,
Computer displays,
Power system interconnection,
Data security,
Information security,
Hardware,
Power system dynamics"
A multi-agent simulation using cultural algorithms: the effect of culture on the resilience of social systems,"Explanations for the collapse of complex social systems including social, political, and economic factors have been suggested. Here we add cultural factors into an agent-based model developed by Kohler for the Mesa Verde Prehispanic Pueblo region. We employ a framework for modeling cultural evolution, cultural algorithms developed by Reynolds (1979). Our approach investigates the impact that the emergent properties of a complex system will have on its resiliency as well as on its potential for collapse. That is, if the system's social structure is brittle, any factor that is able to exploit this fragility can cause a collapse of the system. In particular, we will investigate the impact that environmental variability in the Mesa Verde had on the formation of social networks among agents. Specifically we look at how the spatial distribution of rainfall impacts the systems structure. We show that the distribution of agricultural resources is conducive to the generation of so called ""small world"" networks that require ""conduits"" or some agents of larger interconnectivity to link the small worlds together. Experiments show that there is a major decrease in these conduits in early 1200 A.D. This can have s serious potential impact on the networks resiliency. While the simulation shows an upturn near the start of the 14th century it is possible that the damage to the network had already been done.",
Using service utilization metrics to assess the structure of product line architectures,"Metrics have long been used to measure and evaluate software products and processes. Many metrics have been developed that have lead to different degrees of success. Software architecture is a discipline in which few metrics have been applied, a surprising fact given the critical role of software architecture in software development. Software product line architectures represent one area of software architecture in which we believe metrics can be of especially great use. The critical importance of the structure defined by a product line architecture requires that its properties be meaningfully assessed and that informed architectural decisions be made to guide its evolution. To begin addressing this issue, we have developed a class of closely related metrics that specifically target product line architectures. The metrics are based on the concept of service utilization and explicitly take into account the context in which individual architectural elements are placed. We define the metrics, illustrate their use, and evaluate their strengths and weaknesses through their application on three example product line architectures.",
pFilter: global information filtering and dissemination using structured overlay networks,"The exponential data growth rate of the Internet makes it increasingly difficult for people to find desired information in a timely fashion. Information filtering and dissemination systems allow users to register persistent queries called user profiles, and notify users when relevant files become available. Existing such systems, however, either are not scalable, or do not support matching of unstructured documents (e.g., text, HTML, image, audio or video files) that account for a significant percentage of Internet contents. We propose pFilter a global-scale, decentralized information filtering and dissemination system for unstructured documents. To handle potentially billions of documents for millions of subscribers, pFilter connects a large number of computers into a structured peer-to-peer overlay network. Computers in the overlay collectively publish or collect documents, build indices, register profiles, filter and disseminate documents. Profiles and documents are distributed through the network according to their semantics such that they can be matched efficiently and accurately without excessive flooding. pFilter employs scalable application-level multicast to deliver matching documents to a large number of interested parties efficiently.","Information filtering,
Internet,
Peer to peer computing,
Information filters,
Search engines,
HTML,
Computer networks,
Registers,
Distributed computing,
Computer science"
Continuous path planning with multiple constraints,"We examine the problem of planning a path through a low dimensional continuous state space subject to upper bounds on several additive cost metrics. For the single cost case, previously published research has proposed constructing the paths by gradient descent on a local minima free value function. This value function is the solution of the Eikonal partial differential equation, and efficient algorithms have been designed to compute it. In this paper we propose an auxiliary partial differential equation with which we can evaluate multiple additive cost metrics for paths which are generated by value functions; solving this auxiliary equation adds little more work to the value function computation. We then propose an algorithm which generates paths whose costs lie on the Pareto optimal surface for each possible destination location, and we can choose from these paths those which satisfy the constraints. The procedure is practical when the sum of the state space dimension and number of cost metrics is roughly six or below.","Path planning,
Cost function,
State-space methods,
Partial differential equations,
Integral equations,
Computer science,
Algorithm design and analysis,
Additives,
Rough surfaces,
Surface roughness"
Real time pattern matching using projection kernels,"A novel approach to pattern matching is presented, which reduces time complexity by two orders of magnitude compared to traditional approaches. The suggested approach uses an efficient projection scheme which bounds the distance between a pattern and an image window using very few operations. The projection framework is combined with a rejection scheme which allows rapid rejection of image windows that are distant from the pattern. Experiments show that the approach is effective even under very noisy conditions. The approach described here can also be used in classification schemes where the projection values serve as input features that are informative and fast to extract.","Pattern matching,
Kernel,
Euclidean distance,
Computer science,
Application software,
Pixel,
Image processing,
Computer vision,
Performance evaluation,
Computational complexity"
An adaptive QoS routing protocol with dispersity for ad-hoc networks,"In this paper, we present the design and simulation of a bandwidth constrained multiple path on-demand routing protocol for ad-hoc networks to support end-to-end quality of service, which is known as the adaptive dispersity QoS routing (ADQR) protocol. We propose a route discovery algorithm to find multiple disjoint paths with longer-lived connections, where each path also specifies associated network resource information. This provides an efficient approach for network resource reservation combined with data dispersion and for route maintenance. A longer-lived path generally involves less route maintenance, resulting in an increase of bandwidth utilization. We propose a route maintenance algorithm to proactively monitor network topology changes, providing an interface to network resource management protocols. We also propose a fast rerouting algorithm to significantly reduce data flow and QoS disruptions. Rerouting is proactively carried out before path unavailability occurs. Simulation results show that ADQR reduces data disruptions and provides end-to-end QoS, by employing efficient route discovery and maintenance mechanisms with network resource information.","Routing protocols,
Ad hoc networks,
Quality of service,
Bandwidth,
Resource management,
Network topology,
Computational modeling,
Computer simulation,
Monitoring,
Measurement"
2DOF controller parametrization for systems with a single I/O delay,This note puts forward a parametrization of all stabilizing two-degrees-of-freedom controllers for (possibly unstable) processes with dead-time. The proposed parametrization is based on a doubly coprime factorization of the plant and takes the form of a generalized Smith predictor (dead-time compensator) feedback part and a finite-dimensional feedforward part (prefilter). Some alternative dead-time compensation schemes and disturbance attenuation limitations are also discussed.,"Control systems,
Delay,
Automatic control,
Supervisory control,
Computer science,
Computational complexity,
Discrete event systems,
Process control,
Automata,
Observability"
Network analysis of Counter-strike and Starcraft,"Network games are becoming increasingly popular, but their traffic patterns have received little attention from the academic research community. A better understanding of network game traffic can lead to more effective network architectures and more realistic network simulations. We gathered traffic traces from live Internet games of Counter-strike and Starcraft, representative games from two different gaming genres. We analyzed the network traffic patterns and bandwidth consumption to better understand how such games impact the network. Both games were found to have very low bandwidth usage and packets significantly smaller than typical Internet applications. Starcraft games showed consistent traffic behavior across games while the traffic patterns for Counter-strike varied widely, especially for the server. In particular, Counter-strike clients and servers often send bursts of very small packets. Since current Internet routers are typically designed for large transfers with large packets, there may be opportunities to improve network architectures to better manage and support game traffic.","Telecommunication traffic,
IP networks,
Internet,
Bandwidth,
Computer networks,
Computer architecture,
Network servers,
Web server,
Toy industry,
Computer science"
Using program transformation to secure C programs against buffer overflows,,"Buffer overflow,
Programming profession,
Memory management,
Runtime,
Costs,
Linux,
Domain Name System,
Computer science,
Educational institutions,
Computer security"
Automatic grading of student's programming assignments: an interactive process and suite of programs,"A system for automatic grading of programming assignments is described here. This grading system consists of a suite of Perl and Java programs, linked by a database, and driven by an interactive, user-grader controlled grading process. The rationale for the process, the process itself-and the process's interaction-with the programs, the database, and the user himself are discussed. This grading system has the ability to discover and accommodate unanticipated solutions by means of a grading process that is highly interactive with its user-the grader. It also has the ability to automatically accommodate a wide range of simple student errors, which can easily befuddle a more naive grading system. This system has been used since the Spring 2000 semester at Rutgers University, as a core part of the students' performance evaluation activities, in the Introduction to computer science course, where it routinely grades some 300-600 weekly assignments.","Automatic programming,
Java,
Databases,
Education,
Automatic control,
Process control,
Control systems,
Computer errors,
Computer science,
Error analysis"
Efficient on-the-fly data race detection in multithreaded C++ programs,"Data race detection is highly essential for debugging multithreaded programs and assuring their correctness. Nevertheless, there is no single universal technique capable of handling the task efficiently, since the data race detection problem is computationally hard in the general case. Thus, all currently available tools, when applied to some general case program, usually result in excessive false alarms or in a large number of undetected races. Another major drawback of currently available tools is that they are restricted, for performance reasons, to detection units of fixed size. Thus, they all suffer from the same problem - choosing a small unit might result in missing some of the data races, while choosing a large one might lead to false detection. We present a novel testing tool, called MultiRace, which combines improved versions of Djit and Lockset - two very powerful on-the-fly algorithms for dynamic detection of apparent data races. Both extended algorithms detect races in multithreaded programs that may execute on weak consistency systems, and may use two-way as well as global synchronization primitives. By employing novel technologies, MultiRace adjusts its detection to the native granularity of objects and variables in the program under examination. In order to monitor all accesses to each of the shared locations, MultiRace instruments the C++ source code of the program. It lets the user fine-tune the detection process, but otherwise is completely automatic and transparent. This paper describes the algorithms employed in MultiRace, gives highlights of its implementation issues, and suggests some optimizations. It shows that the overheads imposed by MultiRace are often much smaller (orders of magnitude) than those obtained by other existing tools.","Multithreading,
Computer science,
Debugging,
Testing,
Heuristic algorithms,
Object detection,
Monitoring,
Instruments,
Parallel programming,
Parallel processing"
Estimating the photorealism of images: distinguishing paintings from photographs,"Automatic classification of an image as a photograph of a real-scene or as a painting is potentially useful for image retrieval and Web site filtering applications. The main contribution of the paper is the proposition of several features derived from the color, edge, and gray-scale-texture information of the image that effectively discriminate paintings from photographs. For example, we found that paintings contain significantly more pure-color edges, and that certain gray-scale-texture measurements (mean and variance of Gabor filters) are larger for photographs. Using a large set of images (12000) collected from different Web sites, the proposed features exhibit very promising classification performance (over 90%). A comparative analysis of the automatic classification results and psychophysical data is reported, suggesting that the proposed automatic classifier estimates the perceptual photorealism of a given picture.","Painting,
Computer graphics,
Application software,
Psychology,
Layout,
Humans,
Cities and towns,
Computer science,
Image retrieval,
Information filtering"
Power constrained test scheduling with dynamically varied TAM,"In this paper we present a novel scheduling algorithm for testing embedded core-based SoCs. Given test conflicts, power consumption limitation and top level test access mechanism (TAM) constraint, we handle the constrained scheduling in a unique way that adaptively assigns the cores in parallel to the TAMs with variable width and concurrently executes the test sets by dynamic test partitioning, thus reducing the test cost in terms of the overall test time. Through simulation, we show that up to 30% of SoC testing time reduction can be achieved by using our scheduling approach.","Dynamic scheduling,
System testing,
Job shop scheduling,
Costs,
Processor scheduling,
Scheduling algorithm,
Bandwidth,
Computer science,
Power engineering and energy,
Energy consumption"
Towards adaptive autonomous robots in autism therapy: varieties of interactions,"This paper reports results deriving from the Aurora project (www.aurora-project.com) where we have pioneered research into the possible use of robots in autism therapy. Autistic children have difficulties in social interaction, communication and fantasy and imagination. As part of the project we run trials where autistic children are playing with a small, non-humanoid mobile robot that can engage children in simple interaction games. In our project we focus on the behavioural, rather than the affective level of robots used in therapy. In this paper we first discuss in more detail varieties of interactions where one child, or two children simultaneously, play with a robot. We then outline a new research direction in the project which studies how a mobile robot can adapt to individual children. Quantitative examples of activity levels in child-robot interactions are included. The paper concludes by outlining future research directions for adaptive robots in autism therapy.","Autism,
Medical treatment,
Mobile robots,
Wires,
Software systems,
Tellurium,
Adaptive systems,
Computer science,
Mobile communication,
Robot sensing systems"
Early Web size measures and effort prediction for Web costimation,"Size measures for Web costimation proposed in the literature are invariably related to implemented Web applications. Even when targeted at measuring functionality based on function point analysis, researchers only considered the final Web application, rather than requirements documentation generated using any existing Web development methods. This makes their usefulness as early effort predictors questionable. In addition, it is believed that company-specific data provide a better basis for accurate estimates. Many software engineering researchers have compared the accuracy of company-specific data with multiorganisation databases. However the datasets employed were comprised of data from conventional applications. To date no similar comparison has been adopted for Web project datasets. It has two objectives: The first is to present a survey where early size measures for Web costimation were identified using data collected from 133 Web companies worldwide. All companies included in the survey used Web forms to give quotes on Web development projects, based on gathered size measures. The second is to compare the prediction accuracy of a Web company-specific data with data from a multiorganisation database. Both datasets were obtained via Web forms, used as part of a research project called Tukutuku. Our results show that best predictions were obtained for company-specific dataset, for the two estimation techniques employed.","Size measurement,
Application software,
Java,
Costs,
Databases,
Computer science,
HTML,
Software engineering,
Predictive models,
XML"
Tracking the evolution of Web traffic: 1995-2003,"Understanding the nature and structure of Web traffic is essential for valid simulations of networking technologies that affect the end-to-end performance of HTTP connections. We provide data suitable for the construction of synthetic Web traffic generators and in doing so retrospectively examine the evolution of Web traffic. We use a simple and efficient analysis methodology based on the examination of only the TCP/IP headers of one-half (server-to-client) of the HTTP connection. We show the impact of HTTP protocol improvements such as persistent connections as well as modern content structure that reflect the influences of ""banner ads,"" server load balancing, and content distribution networks. Lastly, we comment on methodological issues related to the acquisition of HTTP data suitable for performing these analyses, including the effects of trace duration and trace boundaries.","Traffic control,
Telecommunication traffic,
TCPIP,
Protocols,
Web server,
Load management,
Internet,
Computer science,
Computational modeling,
Computer simulation"
ECE curriculum in 2013 and beyond: vision for a metropolitan public research university,"In this paper, the authors discuss the anticipated curriculum of electrical and computer engineering (ECE) programs in the year 2013 and beyond for a prototypical metropolitan public research university. Assumptions are made regarding the evolution of the relevant technology addressed by the curriculum, the nature of metropolitan public universities in the next decade, and the nature of the learning environment, technology, and educational focus based on present trends. A 128-credit-hour curriculum is proposed that addresses these challenges.","Electrical engineering education,
Computer science education"
Estimation of congestion price using probabilistic packet marking,"One key component of recent pricing-based congestion control schemes is an algorithm for probabilistically setting the Explicit Congestion Notification bit at routers so that a receiver can estimate the sum of link congestion prices along a path. We consider two such algorithms - a well-known algorithm called Random Exponential Marking (REM) and a novel algorithm called Random Additive Marking (RAM). We show that if link prices are unbounded, a class of REM-like algorithms are the only ones possible. Unfortunately, REM computes a biased estimate of total price and requires setting a parameter for which no uniformly good choice exists in a network setting. However, we show that if prices can be bounded and therefore normalized, then there is an alternate class of feasible algorithms, of which RAM is representative and furthermore, only the REM-like and RAM-like classes are possible. For properly normalized link prices, RAM returns an optimal price estimate (in terms of mean squared error), outperforming REM even if the REM parameter is chosen optimally. RAM does not require setting a parameter like REM, but does require a router to know its position along the path taken by a packet. We present an implementation of RAM for the Internet that exploits the existing semantics of the time-to-live field in IP to provide the necessary path position information.",
Probing-based preprocessing techniques for propositional satisfiability,"Preprocessing is an often used approach for solving hard instances of propositional satisfiability (SAT). Preprocessing can be used for reducing the number of variables and for drastically modifying the set of clauses, either by eliminating irrelevant clauses or by inferring new clauses. Over the years, a large number of formula manipulation techniques has been proposed, that in some situations have allowed solving instances not otherwise solvable with state-of-the-art SAT solvers. This paper proposes probing-based preprocessing, an integrated approach for preprocessing propositional formulas, that for the first time integrates in a single algorithm most of the existing formula manipulation techniques. Moreover, the new unified framework can be used to develop new techniques. Preliminary experimental results illustrate that probing-based preprocessing can be effectively used as a preprocessing tool in state-of-the-art SAT solvers.","Data preprocessing,
Application software,
Computer science,
Failure analysis,
Algorithm design and analysis,
Artificial intelligence,
Home appliances"
Maximizing the system value while satisfying time and energy constraints,"Embedded devices designed for various real-time applications typically have three constraints that must be addressed: energy, deadlines, and reward. These constraints play important roles in the next generation of embedded systems, since they provide users with a variety of quality-of-service (QoS) tradeoffs. We propose a QoS model in which applications may have several versions, each with different time and energy requirements, while providing different levels of accuracy (reward). An optimal scheme would allow the device to run the most critical and valuable versions of applications without depleting the energy source, while still meeting all deadlines. A solution is presented for frame-based and periodic task sets. Three algorithms are devised that closely approximate the optimal solution while taking only a fraction of the runtime of an optimal solution.",
Automatic map acquisition for navigation in domestic environments,"An autonomous robot navigating in its environment needs a map representing the large-scale structure of the setting. There are a variety of different types of maps used in mobile robotics: for example, grid maps, maps containing geometrical beacons, or topological maps. Each of those poses its own constraints on the problem of acquiring the map, where its complexity is a major issue. We present a system that is able to build a topological map of large-scale indoor environments in a semi-autonomous way. This means that a person shows the robot around, interacts with it via a basic interface, the platform is following autonomously, and simultaneously creating the map. The acquisition mechanisms are rather simple and computationally inexpensive, due to the low complexity of the topological map. However, experiments show that this map is sufficient to navigate between arbitrary positions in the environment.",
Optimizing clustering algorithm in mobile ad hoc networks using simulated annealing,"In this paper, we demonstrate how simulated annealing algorithm can be applied to clustering algorithms used in ad hoc networks; specifically our recently proposed weighted clustering algorithm (WCA) is optimized by simulated annealing. As the simulated annealing stands to be a powerful stochastic search method, its usage for combinatorial optimization problems was found to be applicable in our problem domain. The problem formulation along with the parameters is mapped to be an individual solution as an input to the simulated annealing algorithm. Input consists of a random set of clusterhead set along with its members and the set of all possible dominant sets chosen from a given network of N nodes as obtained from the original WCA. Simulated annealing uses this information to find the best solution defined by computing the objective function and obtaining the best fitness value. The proposed technique is such that each clusterhead handles the maximum possible number of mobile nodes in its cluster in order to facilitate the optimal operation of the MAC protocol. Consequently, it results in the minimum number of clusters and hence clusterheads. Simulation results exhibit improved performance of the optimized WCA than the original WCA.","Clustering algorithms,
Intelligent networks,
Mobile ad hoc networks,
Simulated annealing,
Computational modeling,
Iterative algorithms,
Computer simulation,
Ad hoc networks,
Computer science,
Search methods"
Physlets for quantum mechanics,"Educators have often pinned their hopes of better instruction on emerging technologies such as television, computers, and the World Wide Web, yet teaching with technology-without a sound pedagogy-yields no significant educational gain. In addition, students often approach typical end-of-chapter textbook problems by finding a formula that contains the variables given in the problem statement, rather than first determining a problem's conceptual foundation. We use the technology of Physlets combined with pedagogical techniques such as just-in-time teaching to create alternative problems that we believe help students better develop their problem-solving ability and deepen their conceptual understanding. Physlets-physics applets-are small, flexible Java applets usable in a wide variety of Web applications. They have attributes that make them especially valuable for science education.","Quantum mechanics,
Animation,
Acceleration,
TV,
Web sites,
Educational technology,
Computer science education,
Physics education,
Watches,
Educational institutions"
Applying TAM to e-services adoption: the moderating role of perceived risk,"Consumer adoption of e-services is an important goal for many service providers, however little is known about how different consumer segments perceive and evaluate them for adoption. The technology acceptance model (TAM) explains information systems evaluation and adoption, however the Internet-delivered e-services context presents additional variance that requires supplemental measures to be added to TAM. This research extends TAM to include a perceived usage risk main effect and also tested whether perceived risk moderated several of TAM's relationships. Results indicate that higher levels of perceived risk deflated ease of user's effect and inflated subjective norm's effect on perceived usefulness and adoption intention.","Information systems,
Customer service,
Predictive models,
Context modeling,
Testing,
Web and internet services,
Protocols,
Computer interfaces,
Human computer interaction,
Privacy"
Optimal resource allocation in overlay multicast,"The paper targets the problem of optimal resource allocation in overlay multicast, which poses both theoretical and practical challenges. Theoretically, resource allocation among overlay flows is not subject to the network capacity constraint but also the data constraint, mainly due to the dual role of end hosts as both receivers and senders. Practically, existing distributed resource allocation schemes assume the network links to be capable of measuring flow rates, calculating and communicating price signals, none of which actually exists in the Internet today. We address these challenges as follows. First, we formalize the problem using nonlinear optimization theory, which incorporates both network constraint and data constraint. Based on our theoretical framework, we propose a distributed algorithm, which is proved to converge to the optimal point, where the aggregate utility of all receivers is maximized. Second, we propose an end-host-based solution, which relies on the coordination of end hosts to accomplish tasks originally assigned to network links. our solution can be directly deployed without any changes to the existing network infrastructure.","Resource management,
Constraint theory,
Aggregates,
Unicast,
Multicast protocols,
Computer science,
Fluid flow measurement,
IP networks,
Constraint optimization,
Distributed algorithms"
Incremental dynamic impact analysis for evolving software systems,"Impact analysis - determining the potential effects of changes on a software system - plays an important role in helping engineers revalidate modified software. In previous work we presented a new impact analysis technique. PathImpact, for performing dynamic impact analysis at the level of procedures, and we showed empirically that the technique can be cost-effective in comparison to prominent prior techniques. A drawback of that approach as presented, however, is that when attempting to apply the technique to a new version of a system as that system and its test suite evolves, the process of recomputing the data required by the technique for that version can be excessively expensive. In this paper, therefore, we present algorithms that allow the data needed by PathImpact to be collected incrementally. We present the results of a controlled experiment investigating the costs and benefits of this incremental approach relative to the approach of completely recomputing prerequisite data.","Software systems,
System testing,
Information analysis,
Computer science,
Performance analysis,
Software testing,
Risk analysis,
Software measurement,
Costs,
Time factors"
PDA interface for a field robot,"Operating robots in an outdoor setting poses interesting problems in terms of interaction. To interact with the robot there is a need for a flexible computer interface. In this paper a PDA-based (personal digital assistant, i.e. a handheld computer) approach to robot interaction is presented. The system is designed to allow non-expert users to utilise the robot for operation in an urban exploration setup. The basic design is outlined and a first set of experiments are reported.","Robot sensing systems,
Personal digital assistants,
Mobile robots,
Psychology,
Human factors,
Marine vehicles,
Chemical elements,
Collision avoidance,
Numerical analysis,
Computer science"
Indoor target intercept using an acoustic sensor network and dual wavefront path planning,"This paper presents an approach that enables a mobile ""interceptor"" robot to intercept targets in an indoor environment using information from a distributed acoustic sensor network. The approach assumes the indoor environment has been previously mapped and that the sensor nodes know their position in the map. The targets are localized in the sensor network based upon local maxima of the acoustic volume. The current target localization information is reported to an interceptor robot, which utilizes a dual wavefront path planner to move from its current location to a location that is within visibility range of a target. Results of the complete implementation of this approach using 70 sensor net robots in the player/stage multi-robot simulator are reported, as well as implementation results to date on a team of physical robots. To our knowledge, this is the first implementation of a multi-robot system that combines the use of an acoustic sensor net for target detection with an interceptor robot that can efficiently reach the moving position of the detected target in indoor environments.",
Cooperative control for UAV's searching risky environments for targets,"In this paper a model and algorithmic solution are created to control the path planning decision processes of multiple cooperating autonomous aerial vehicles engaged in a search of an uncertain and risky environment. Methods to incorporate a priori and dynamic information about the targets and to incorporate threats in a three-dimensional environment into a computationally feasible dynamic programming approach are provided. Some basic simulation results demonstrate the functionality of this approach, and provide a baseline for future work.","Unmanned aerial vehicles,
Path planning,
Dynamic programming,
Remotely operated vehicles,
Mobile robots,
Computer science,
Vehicle dynamics,
Computational modeling,
Contracts,
Heuristic algorithms"
Honeypot: a supplemented active defense system for network security,"A honeypot is a supplemented active defense system for network security. It traps attacks, records intrusion information about tools and activities of the hacking process, and prevents attacks outbound the compromised system. Integrated with other security solutions, a honeypot can solve many traditional dilemmas. We expatiate key components of data capture and data control in a honeypot, and give a classification for honeypots according to security goals and application goals. We review the technical progress and security contribution of production honeypots and research honeypots. We present typical honeypot solutions and predict the technical trends of integration, virtualization and distribution for future honeypots.","Information security,
Intrusion detection,
Data security,
Computer crime,
Power system security,
Control systems,
Electrical equipment industry,
Taxonomy,
Educational institutions,
Computer science"
3D model retrieval with morphing-based geometric and topological feature maps,"Recent advancement in 3D digitization techniques have prompted the need for 3D object retrieval. Our method of comparing 3D objects for retrieval is based on 3D morphing. It computes, for each 3D object, two spatial feature maps that describe the geometry and topology of the surface patches on the object, while preserving the spatial information of the patches in the maps. The feature maps capture the amount of effort required to morph a 3D object into a canonical sphere, without performing explicit 3D morphing. Fourier transforms of the feature maps are used for object comparison so as to achieve invariant retrieval under arbitrary rotation, reflection, and non-uniform scaling o the objects. Experimental results show that our method of retrieving 3D models is very accurate, achieving a precision of above .086 even at a recall rate of 1.0.","Solid modeling,
Topology,
Shape,
Delta modulation,
Image retrieval,
Computer science,
Drives,
Computational geometry,
Information geometry,
Fourier transforms"
Radio wave propagation through vegetation: Factors influencing signal attenuation,"The paper describes an extensive wideband channel sounding measurement campaign to investigate signal propagation through vegetation. The measurements have been conducted at three frequencies (1.3, 2 and 11.6 GHz) at sites with different measurement geometries and tree species. The data have been used to evaluate current narrowband empirical vegetation attenuation models and study the prevailing propagation mechanisms. Evaluation of the modified exponential decay (MED), maximum attenuation (MA) and nonzero gradient (NZG) models show that on a site by site basis, the NZG model gives the best prediction of excess attenuation due to vegetation. The MA model has been found to be the worst of the three models. The studies have shown that the measurement site used to obtain the NZG model parameter values given in International Telecommunication Union (ITU) [2001] is influenced by metal lampposts and passing traffic, and thus was based on corrupted data. The results show that the leaf state, measurement geometry and vegetation density are more important factors influencing signal attenuation than tree species or leaf shape. Generally, the 11.6 GHz signal was attenuated much more than the 1.3 and 2 GHz signals by vegetation in-leaf, but the differences in attenuation were not significant in the out-of-leaf state. A successful excess attenuation model due to vegetation must consider the measurement geometry and vegetation descriptive parameters as well as any contributions from ground reflection and/ or diffraction over the top or round edges of the trees.","Vegetation,
Antenna measurements,
Attenuation,
Vegetation mapping,
Geometry,
Mathematical model,
Frequency measurement"
Profile-based dynamic voltage and frequency scaling for a multiple clock domain microprocessor,"A Multiple Clock Domain (MCD) processor addresses the challenges of clock distribution and power dissipation by dividing a chip into several (coarse-grained) clock domains, allowing frequency and voltage to be reduced in domains that are not currently on the application's critical path. Given a reconfiguration mechanism capable of choosing appropriate times and values for voltage/frequency scaling, an MCD processor has the potential to achieve significant energy savings with low performance degradation. Early work on MCD processors evaluated the potential for energy savings by manually inserting reconfiguration instructions into applications, or by employing an oracle driven by offline analysis of (identical) prior program runs. Subsequent work developed a hardware-based online mechanism that averages 75-85% of the energy-delay improvement achieved via offline analysis. We consider the automatic insertion of reconfiguration instructions into applications, using profile-driven binary rewriting. Profile-based reconfiguration introduces the need for ""training runs"" prior to production use of a given application, but avoids the hardware complexity of online reconfiguration. It also has the potential to yield significantly greater energy savings. Experimental results (training on small data sets and then running on larger, alternative data sets) indicate that the profile-driven approach is more stable than hardware-based reconfiguration, and yields virtually all of the energy-delay improvement achieved via offline analysis.","Dynamic voltage scaling,
Clocks,
Microprocessors,
Power dissipation,
Potential energy,
Frequency synchronization,
Costs,
Computer science,
Frequency conversion,
Degradation"
Experimental evaluation of code properties for WCET analysis,"This paper presents a quantification of the timing effects that advanced processor features like data and instruction cache, pipelines, branch prediction units, and out-of-order execution units have on the worst-case execution time (WCET) of programs. These features are present in processors (e.g. PowerPC) that are being widely used in embedded and real-time systems. We present an experimental evaluation of the execution time of a series of synthetic benchmarks and real-life case studies. The execution time is evaluated using extensive testing and a simple WCET technique. We show that the most important factor in reduction of execution time is cache size (both instruction and data cache). Other factors like branch prediction and out-of-order execution have minimal improvements that are cancelled out by the pessimism of the analysis. We also argue that some of the performance gain of advanced processor features also applies to the worst case and although WCET estimates may be more pessimistic the overall impact is that they result in lower WCET estimates.","Timing,
Pipelines,
Performance analysis,
Out of order,
Real time systems,
Delay,
Computer science,
Benchmark testing,
Performance gain,
Embedded system"
"Maxwell, Hertz, the Maxwellians, and the early history of electromagnetic waves","In 1864, Maxwell conjectured from his famous equations that light is a transverse electromagnetic wave. Maxwell's conjecture does not imply that he believed that light could be generated electromagnetically. In fact, he was silent about electromagnetic waves, and their generation and detection. It took almost a quarter of a century before Hertz discovered electromagnetic waves and his brilliant experiments confirmed Maxwell's theory. Maxwell's ideas and equations were expanded, modified, and made understandable by the efforts of Hertz, FitzGerald, Lodge, and Heaviside, the last three being referred to as the ""Maxwellians."" The early history of electromagnetic waves, up to the death of Hertz in 1894, is briefly discussed. The work of Hertz and the Maxwellians is briefly reviewed in the context of electromagnetic waves. It is found that historical facts do not support the views proposed by some, in the past, that Hertz's epoch-making findings and contributions were ""significantly influenced by the Maxwellians."".","History,
Electromagnetic scattering,
Maxwell equations,
Electromagnetic propagation,
Laboratories,
Gaussian processes,
Computer science,
Electromagnetic fields,
Space exploration,
Production"
Development and evaluation of a model of programming errors,"Models of programming and debugging suggest many causes of errors, and many classifications of error types exist. Yet, there has been no attempt to link causes of errors to these classifications, nor is there a common vocabulary for reasoning about such causal links. This makes it difficult to compare the abilities of programming styles, languages, and environments to prevent errors. To address this issue, this paper presents a model of programming errors based on past studies of errors. The model was evaluated with two observational of Alice, an event-based programming system, revealing that most errors were due to attentional and strategic problems in implementing algorithms, language constructs, and uses of libraries. In general, the model can support theoretical, design, and educational programming research.","Programming profession,
Computer errors,
Runtime,
Debugging,
Vocabulary,
Computer science,
Libraries,
Error analysis,
Humans,
Testing"
Performance-driven mapping for CPLD architectures,"We present a performance-driven programmable logic array mapping algorithm (PLAmap) for complex programmable logic device architectures consisting of a large number of PLA-style logic cells. The primary objective of the algorithm is to minimize the depth of the mapped circuit. We also develop several techniques for area reduction, including threshold control of PLA fanouts and product terms, slack-time relaxation, and PLA packing. We compare PLAmap with a previous algorithm TEMPLA (Anderson and Brown 1998) and a commercial tool Altera Multiple Array MatriX (MAX) + PLUS II (Altera Corporation 2000) using Microelectronics Center of North Carolina (MCNC) benchmark circuits. With a relatively small area overhead, PLAmap reduces circuit depth by 50% compared to TEMPLA and reduces circuit delay by 48% compared to MAX + PLUS II v9.6.","Programmable logic arrays,
Logic devices,
Field programmable gate arrays,
Programmable logic devices,
Delay,
Logic design,
Integrated circuit interconnections,
Table lookup,
Computer science,
Microelectronics"
Constructing shareable learning materials in bioengineering education,"Innovations in learning sciences and technology are opening new opportunities for designing and implementing effective learning materials that can be shared between bioengineering instructors. The successful reuse of instructional materials depends on how easy it is for an instructor to adapt the materials with a similar intent of the original author. Many resource libraries are emerging that provide a searchable database of sharable, Web-based instructional materials. The materials range from lesson plans and teaching techniques to text, video, and interactive simulation resources. These digital libraries are valuable services for teachers and students to access information and participate in a community of science, mathematics, and technology. The VaNTH (Vanderbilt University; Northwestern University; University of Texas at Austin; and Health, Science and Technology at Harvard/MIT) Engineering Research Center (ERC) is exploring a similar goal to design and test learning materials for bioengineering education and construct a technology infrastructure that supports the reuse of these materials in pedagogically appropriate ways. For VaNTH, using a consistent framework that describes the instructional intent of the materials is a critical factor in helping instructors envision how to adapt the materials to their own instruction. Therefore, VaNTH has defined a design process that capitalizes on the insight of learning sciences and best practices in engineering education to construct learning materials that follow a consistent but flexible instructional model.","Biological materials,
Biomedical engineering,
Materials science and technology,
Software libraries,
Appropriate technology,
Educational technology,
Technological innovation,
Databases,
Education,
Mathematics"
Efficient record linkage in large data sets,"This paper describes an efficient approach to record linkage. Given two lists of records, the record-linkage problem consists of determining all pairs that are similar to each other where the overall similarity between two records is defined based on domain-specific similarities over individual attributes constituting the record. The record-linkage problem arises naturally in the context of data cleansing that usually precedes data analysis and mining. We explore a novel approach to this problem. For each attribute of records, we first map values to a multidimensional Euclidean space that preserves domain-specific similarity. Many mapping algorithms can be applied, and we use the FastMap approach as an example. Given the merging rule that defines when two records are Similar a set of attributes are chosen along which the merge will proceed A multidimensional similarity join over the chosen attributes is used to determine similar pairs of records. Our extensive experiments using real data sets show that our solution has very good efficiency and accuracy.","Couplings,
Databases,
Multidimensional systems,
Merging,
Hospitals,
Computer science,
Data analysis,
Data mining,
Joining processes,
Local government"
Characteristics of mobile robotic toys for children with pervasive developmental disorders,"Pervasive developmental disorders (PDD) refers to a group of disorders characterized by delays in the development of multiple basic functions including socialization and communication. Symptoms may include communication problems such as using and understanding language; difficulty relating to people, objects, and events; unusual play with toys and other objects; difficulty with changes in routine or familiar surroundings, and repetitive body movements or behavior patterns. Autism is the most characteristic and best studied PDD. We are investigating the use of mobile robotic toys that can move in the environment and interact in various manners (vocal messages, music, visual cues, movement, etc.) with children with autism. The hypothesis is that mobile robots can serve as an appropriate pedagogical tool to help children with PDD develop social skills because they are more predictable and less intimidating. The objective is to see how such devices can be used to capture the child's attention and contribute to helping him or her develop social skills. This paper outlines the design considerations for such robots, and presents experimental protocols that are being developed to study the impacts of using these robots on the development of the child.","Mobile robots,
Autism,
Mobile communication,
Protocols,
Intelligent robots,
Intelligent sensors,
Natural languages,
Speech,
Shape"
Real-time digital signal processing in the undergraduate curriculum,"An undergraduate digital signal processing (DSP) laboratory at Western Michigan University (WMU) is described. A sequence of hands-on laboratory experiments using Texas Instruments DSP evaluation modules has been incorporated into the existing DSP undergraduate course. The main objectives are to enhance the students' understanding of the theory taught in class, provide experience with DSP implementation issues, and to increase their interest and participation in this important field of electrical and computer engineering. A description of the course, laboratory sessions, and supporting tools are described. To monitor the acceptance and success of the new laboratory, student assessments and evaluations have been collected and are briefly discussed.","Signal processing,
Student experiments,
Real time systems"
Leveraging a common representation for personalized search and summarization in a medical digital library,"Despite the large amount of online medical literature, it can be difficult for clinicians to find relevant information at the point of patient care. We present techniques to personalize the results of search, making use of the online patient record as a sophisticated, preexisting user model. Our work in PERSIVAL, a medical digital library, includes methods for reranking the results of search to prioritize those that better match the patient record. It also generates summaries of the reranked results, which highlight information that is relevant to the patient under the physician's care. We focus on the use of a common representation for the articles returned by search and the patient record, which facilitates both the reranking and the summarization tasks. This common approach to both tasks has a strong positive effect on the ability to personalize information.",
"PRISMA: towards quality, aspect oriented and dynamic software architectures","The development of software systems must be done using platforms that allow the description of quality, complex, distributed, dynamic and reusable architectural models. We present in this paper PRISMA, an architectural modelling approach based on aspects and components that uses a component definition language (components, connectors and systems) to define architectural types at a high abstraction level and a configuration language to design the architecture of software systems. The component definition language increases reuse allowing importation of COTS and reduces complexity by integrating two modern software development approaches: component-based software development and aspect-oriented software development. The configuration language designs the architecture of software systems by creating and interconnecting instances of the defined types including possible imported COTS. PRISMA has a metalevel with reflexive properties for these two languages. For this reason, the types of PRISMA may evolve and the topologies of PRISMA may be reconfigured dynamically.","Software quality,
Computer architecture,
Object oriented modeling,
Software systems,
Programming,
Information systems,
Software architecture,
LAN interconnection,
Application software,
Computer science"
Efficient generation of monitor circuits for GSTE assertion graphs,"Generalized symbolic trajectory evaluation (GSTE) is a powerful, new method for formal verification that combines the industrially-proven scalability and capacity of classical symbolic trajectory evaluation with the expressive power of temporal-logic model checking. GSTE was originally developed at Intel and has been used successfully on Intel's next-generation microprocessors. However, the supporting algorithms and tools for GSTE are still relatively immature. GSTE specifications are given as assertion graphs, an extension of /spl forall/-automata. This paper presents a linear-time, linear-size translation from GSTE assertion graphs into monitor circuits, which can be used with dynamic verification both as a quick ""sanity check"" of the specification before effort is invested in abstraction and formal verification, and also as means to reuse GSTE specifications with other validations methods. We present experimental results using real GSTE assertion graphs for real industrial circuits, showing that the circuit construction procedure is efficient in practice and that the monitor circuits impose minimal simulation overhead.","Formal verification,
Microprocessors,
Construction industry,
Permission,
Circuit testing,
Computerized monitoring,
Computer science,
Computer industry,
Scalability,
Circuit simulation"
Visual composition of Web services,"Composing Web services into a coherent application can be a tedious and error prone task when using traditional textual scripting languages. As an alternative, complex interactions patterns and data exchanges between different Web services can be effectively modeled using a visual language. In this paper we discuss the requirements of such an application scenario and we present the design of the BioOpera Flow Language. This visual composition language has been fully implemented in a development environment for Web service composition with usability features emphasizing rapid development and visual scalability.","Web services,
Application software,
Web and internet services,
XML,
Monitoring,
Computer science,
Computer errors,
LAN interconnection,
Proposals,
Runtime environment"
Some multiobjective optimizers are better than others,"The No-Free-Lunch (NFL) theorems hold for general multiobjective fitness spaces, in the sense that, over a space of problems which is closed under permutation, any two algorithms will produce the same set of multiobjective samples. However, there are salient ways in which NFL does not generally hold in multiobjective optimization. Previously we have shown that a 'free lunch' can arise when comparative metrics (rather than absolute metrics) are used for performance measurement. Here we show that NFL does not generally apply in multiobjective optimization when absolute performance metrics are used. This is because multiobjective optimizers usually combine a generator with an archiver. The generator corresponds to the 'algorithm' in the NFL sense, but the archiver filters the sample generated by the algorithm in a way that undermines the NFL assumptions. Essentially, if two multiobjective approaches have different archivers, their average performance may differ. We prove this, and hence show that we can say, without qualification, that some multiobjective approaches are better than others.","Computer science,
Measurement,
Filters,
Qualifications,
Algorithm design and analysis,
Particle swarm optimization,
Simulated annealing,
Modems"
XVCL: XML-based variant configuration language,"XVCL (XML-based Variant Configuration Language) is a meta-programming technique and tool that provides effective reuse mechanisms [2]. XVCL is an open source software (http://fxvcl.sourceforge.net) developed at the National University of Singapore. Being a modem and versatile version of Bassett's frames [1], a technology that has achieved substantial gains in industry, the underlying principles of the XVCL have been thoroughly tested in practice. Unlike original frames, XVCL blends with contemporary programming paradigms and complements other design techniques. XVCL uses ""composition with adaptation"" rules to generate a specific program from generic, reusable meta-components. Program generation rules are 100% transparent to a programmer, who retains full control over fine-tuning the generated code. Despite its simplicity, XVCL can effectively manage a wide range of program variants from a compact base of metacomponents, structured for effective reuse.","Application software,
XML,
Computer architecture,
Computer science,
Information processing,
Open source software,
Testing,
Programming profession,
Hardware,
Asset management"
Metamorphic testing and beyond,"When testing a program, correctly executed test cases are seldom explored further, even though they may carry useful information. Metamorphic testing proposes to generate follow-up test cases to check important properties of the target function. It does not need a human oracle for output prediction and comparison. In this paper, we highlight the basic concepts of metamorphic testing and some interesting extensions in the areas of program testing, proving, and debugging. Future research directions are also proposed.","Software testing,
System testing,
Information technology,
Computer science,
Information systems,
Humans,
Debugging,
Australia Council,
Cryptography,
Arithmetic"
On energy-minimizing paths on terrains for a mobile robot,"In this paper we discuss the problem of computing optimal paths on terrains for a mobile robot. The cost of a path is defined to be the energy expended due to both friction and gravity. The model allows for ranges of impermissible traversal directions caused by overturn danger or power limitations. This model is interesting and challenging as it incorporates constraints found in realistic situations and these constraints affect the computation of optimal paths. We give some upper and lower bound results on the combinatorial size of energy-minimizing paths on terrains. We also present an efficient approximation algorithm that computes for two given points a path whose cost is within a user-defined relative error ratio. Compared to previous results with the same approach, this algorithm improves the time complexity by using: (1) a discretization with reduced size, and (2) an improved discrete algorithm for finding optimal paths in the discretization. We present some preliminary experimental results to demonstrate the efficiency of our algorithm. We also provide a similar discretization for the same model but under less restricted assumptions.","Mobile robots,
Friction,
Approximation algorithms,
Costs,
Rivers,
Path planning,
Anisotropic magnetoresistance,
Sun,
Computer science,
Information systems"
The TCP SACK-aware snoop protocol for TCP over wireless networks,"TCP continues to be the most important transport layer communication protocol. Several solutions have been proposed to address the known problems that TCP faces when running over wireless networks. Of these solutions, the Snoop protocol, a link layer retransmission strategy, has been shown to be the most effective. However, not all TCP versions have been analyzed using the Snoop protocol. In fact, we recently showed that TCP Vegas and TCP SACK exhibit opposite performance results when utilized with and without the Snoop protocol. In this paper we analyze this behavior and introduce the TCP SACK-aware Snoop protocol for wired cum wireless networks. We show how to make the Snoop protocol TCP SACK-aware and why using the Snoop protocol is in fact worse than not using any mechanism at all. The TCP SACK-aware protocol improves the performance of TCP SACK by around 30% compared to the plain Snoop protocol and by about 8% in an environment where no TCP enhancing mechanism is in place.","Wireless application protocol,
Wireless networks,
Transport protocols,
Computer science,
Communication system control,
Stability,
Internet,
Satellites,
Mobile ad hoc networks,
Bit error rate"
A registration-based approach to quantify flow-mediated dilation (FMD) of the brachial artery in ultrasound image sequences,"Flow-mediated dilation (FMD) offers a mechanism to characterize endothelial function and, therefore, may play a role in the diagnosis of cardiovascular diseases. Computerized analysis techniques are very desirable to give accuracy and objectivity to the measurements. Virtually all methods proposed up to now to measure FMD rely on accurate edge detection of the arterial wall, and they are not always robust in the presence of poor image quality or image artifacts. A novel method for automatic dilation assessment based on a global image analysis strategy is presented. We model interframe arterial dilation as a superposition of a rigid motion and a scaling factor perpendicular to the artery. Rigid motion can be interpreted as a global compensation for patient and probe movements, an aspect that has not been sufficiently studied before. The scaling factor explains arterial dilation. The ultrasound sequence is analyzed in two phases using image registration to recover both transformation models. Temporal continuity in the registration parameters along the sequence is enforced with a Kalman filter since the dilation process is known to be a gradual physiological phenomenon. Comparing automated and gold standard measurements (average of manual measurements) we found a negligible bias (0.05%FMD) and a small standard deviation (SD) of the differences (1.05%FMD). These values are comparable with those obtained from manual measurements (bias=0.23%FMD, SD/sub intra-obs/=1.13%FMD, SD/sub inter-obs/=1.20%FMD). The proposed method offers also better reproducibility (CV=0.40%) than the manual measurements (CV=1.04%).","Brachytherapy,
Arteries,
Ultrasonic imaging,
Image sequences,
Ultrasonic variables measurement,
Image edge detection,
Image sequence analysis,
Measurement standards,
Cardiovascular diseases,
Robustness"
A perspective on distortions,"A framework for analyzing distortions in non-single viewpoint imaging systems is presented. Such systems possess loci of viewpoints called caustics. In general, perspective (or undistorted) views cannot be computed from images acquired with such systems without knowing scene structure. Views computed without scene structure will exhibit distortions, which we call caustic distortions. We first introduce a taxonomy of distortions based on the geometry of imaging systems. Then, we derive a metric to quantify caustic distortions. We present an algorithm to compute minimally distorted views using simple priors on scene structure. These priors are defined as parameterized primitives such as spheres, planes and cylinders with simple uncertainty models for the parameters. To validate our method, we conducted extensive experiments on rendered and real images. In all cases our method produces nearly undistorted views even though the acquired images were strongly distorted. We also provide an approximation of the above method that warps the entire captured image into a quasi single viewpoint representation that can be used by any ""viewer"" to compute near-perspective views in real-time.",
Bounded-concurrent secure two-party computation in a constant number of rounds,"We consider the problem of constructing a general protocol for secure two-party computation in a way that preserves security under concurrent composition. In our treatment, we focus on the case where an a-priori bound on the number of concurrent sessions is specified before the protocol is constructed. (a.k.a. bounded concurrency). We make no setup assumptions. Lindel (STOC 2003) has shown that any protocol for bounded-concurrent secure two-party computation, whose security is established via black-box simulation, must have round complexity that is strictly larger than the bound on the number of concurrent sessions. In this paper, we construct a (non black-box) protocol for realizing bounded-concurrent secure two-party computation in a constant number of rounds. Our constructions rely on the existence of enhanced trapdoor permutations, as well as on the existence of hash functions that are collision-resistant against subexponential sized circuits.","Concurrent computing,
Cryptographic protocols,
Computer science,
Computational modeling,
Laboratories,
Computer security,
Circuit simulation,
Random variables,
Polynomials,
Interleaved codes"
Quasi-perfect Lee distance codes,"A construction of perfect/quasiperfect Lee distance codes in Z/sub K//sup 2/ is introduced. For this class of codes, a constant time encoding scheme is defined, the minimum code distance is derived, and the maximum covering radius is calculated. Efficient decoding schemes are investigated and developed. In general, a code of this class can be decoded in O(t/sub 1/), where t/sub 1/ is the number of errors that can be corrected. Special cases, however, can be decoded in constant time.",
Sensor network-based multi-robot task allocation,"We present DINTA, distributed in-network task allocation - a novel paradigm for multi-robot task allocation (MRTA) where tasks are allocated implicitly to robots by a pre-deployed, static sensor network. Experimental results with a simulated alarm scenario show that our approach is able to compute solutions to the MRTA problem in a distributed fashion. We compared our approach to a strategy where robots use the deployed sensor network for efficient exploration. The data show that our approach outperforms such an exploration-only algorithm. The data also provide evidence that the proposed algorithm is more stable than the exploration-only algorithm.","Robot sensing systems,
Navigation,
Computer networks,
Distributed computing,
Embedded system,
Mobile robots,
Laboratories,
Computer science,
Sensor systems,
Computational modeling"
Packet scheduling for MIMO cellular systems,"Packet scheduling is investigated for the downlink of a multiple-input multiple-output (MIMO) cellular system. The conventional round robin scheduling (RRS) scheme does not utilize the benefits of multiuser environments, although it provides fair channel access chance to users. We propose an antenna-assisted round robin scheduling (AA-RRS) scheme to improve the conventional RRS scheme. The AA-RRS scheme provides fair channel access chance to users as the RRS scheme, whereas it increases the system capacity through the effective use of multiple antennas in achieving a diversity effect from multiple users. Computer simulations are conducted to compare the RRS and AA-RRS schemes in terms of the expected and outage capacities. It is shown that the AA-RRS scheme provides significant capacity gain over the RRS scheme, especially in terms of the outage capacity. We also discuss the effects of power control, signal-to-noise ratio (SNR), and spatial correlation on the system capacities.",
Routing anomaly detection in mobile ad hoc networks,"Intrusion detection systems (IDSs) for mobile ad hoc networks (MANETs) are necessary when we deploy MANETs in reality. In this paper, focusing on the protection of MANET routing protocols, we present a new intrusion detection agent model and utilize a Markov chain based anomaly detection algorithm to construct the local detection engine. The details of feature selection, data collection, data preprocess, Markov chain construction, classifier construction and parameter tuning are provided. Based on the routing disruption attack aimed at the dynamic source routing protocol (DSR), we study the performance of the algorithm at different mobility levels. Simulation results show that our algorithm can achieve low false positive ratio, high detection ratio, and small MTFA (mean time to the first alarm), especially when the mobility is low. Detailed analysis of simulation results is also presented.","Intelligent networks,
Mobile ad hoc networks,
Intrusion detection,
Computer science,
Routing protocols,
Detection algorithms,
Engines,
Communication system security,
Sun,
Protection"
Incremental low-discrepancy lattice methods for motion planning,"We present deterministic sequences for use in sampling-based approaches to motion planning. They simultaneously combine the qualities found in many other sequences: i) the incremental and self-avoiding tendencies of pseudo-random sequences, ii) the lattice structure provided by multiresolution grids, and iii) low-discrepancy and low-dispersion measures of uniformity provided by quasi-random sequences. The resulting sequences can be considered as multiresolution grids in which points may be added one at a time, while satisfying the sampling qualities at each iteration. An efficient, recursive algorithm for generating the sequences is presented and implemented. Early experiments show promising performance by using the samples in search algorithms to solve motion planning problems.","Lattices,
Sampling methods,
Random sequences,
Computer science,
Urban planning,
Mesh generation,
Dispersion,
Volume measurement"
An ethnographic study of music information seeking: implications for the design of a music digital library,"At present, music digital library systems are being developed based on anecdotal evidence of user needs, intuitive feelings for user information seeking behavior, and a priori assumptions of typical usage scenarios. Emphasis has been placed on basic research into music document representation, efficient searching, and audio-based searching, rather than on exploring the music information needs or information behavior of a target user group. We focus on eliciting the 'native' music information strategies employed by people searching for popular music (that is, music sought for recreational or enjoyment purposes rather than to support a 'serious' or scientific exploration of some aspect of music). To this end, we conducted an ethnographic study of the searching/browsing techniques employed by people in the researchers' local communities, as they use two common sources of music: the public library and music stores. We argue that the insights provided by this type of study can inform the development of searching/browsing support for music digital libraries.","Multiple signal classification,
Software libraries,
Music information retrieval,
Computer science,
Multimedia systems,
Multimedia computing,
Indexing,
Usability,
Grounding"
A parallel shuffled paging strategy under delay bounds in wireless systems,"In sequential paging schemes, the paging process is considered on per user basis. When an incoming call arrives to a mobile terminal (MT), the associated location area is divided into several paging areas (PAs) and PAs are paged one by one until the MT is found. Even though sequential paging algorithms can reduce the paging cost compared to the blanket-paging scheme, they introduce extra and unnecessary delay due to the fact that, during each paging cycle, unpaged cells are idle and unused in terms of paging. In this letter, a simple parallel shuffled paging strategy is proposed to reduce delay and improve performance. In the proposed scheme, multiple MTs can be paged in difference PAs in parallel. Our study shows that the proposed scheme outperforms both the sequential paging and the blanket paging in terms of discovery rate and queueing delay.","Paging strategies,
Costs,
Delay systems,
System performance,
Land mobile radio cellular systems,
Bandwidth,
Computer science"
Distributed pair programming on the Web,"Pair programming is an extreme programming practice, where two programmers working sided by side on a single computer produce a software artifact. This technique has demonstrated to produce higher quality code in less time it would take an individual programmer. We present the COPPER system, a synchronous source code editor that allows two distributed software engineers to write a program using pair programming. COPPER implements characteristics of groupware systems such as communication mechanism, collaboration awareness, concurrency control, and a radar view of the documents, among others. It also incorporates a document presence module, which extends the functionality of instant messaging systems to allow users to register documents from a Web server and interact with them in a similar fashion as they do with a colleague. We report results from a preliminary evaluation of COPPER which provide evidence that the system could successfully support distributed pair programming.","Collaborative work,
Programming profession,
Copper,
Collaborative tools,
Collaborative software,
Software quality,
Computer science,
Software testing,
Collaboration,
Concurrency control"
Inter-domain dynamic routing in multi-layer optical transport networks,"Next-generation optical transport networks will automatically and dynamically provision end-to-end connections. In this paper, we study the problem of inter-domain dynamic routing under a multi-layer multi-domain network model, which allows the end-to-end connections to be set up not only across multiple routing domains but also through two transport layers: the optical layer and the digital layer. In this model, a connection can traverse the domain boundary either through optical bypass or through optical-electrical-optical (O/E/O) processing. We propose an inter-domain dynamic routing scheme with modest time complexity to address the problem from an algorithmic perspective.","Intelligent networks,
Optical fiber networks,
Wavelength routing,
Telecommunication traffic,
Internet,
Protocols,
Next generation networking,
Optical interconnections,
Computer science,
Optical design"
Real-time acquisition and tracking for GPS receivers,"Current GPS receivers spend much time in base-band processing, performing acquisition and tracking. This is due to the large number of required operations in the software-based signal processing. This paper presents a novel signal acquisition and tracking method that reduces the number of operations, simplifies hardware implementation and decreases the acquisition time. The implementation of this method in an FPGA provides very fast processing of incoming GPS samples that satisfies real-time requirements.","Global Positioning System,
Flexible printed circuits,
Correlators,
Frequency,
Signal processing algorithms,
Field programmable gate arrays,
Digital signal processing,
Computer science,
Signal processing,
Hardware"
An Efficient Data Location Protocol for Self.organizing Storage Clusters,"Component additions and failures are common for large-scale storage clusters in production environments. To improve availability and manageability, we investigate and compare data location schemes for a large self-organizing storage cluster that can quickly adapt to the additions or departures of storage nodes. We further present an efficient location scheme that differentiates between small and large file blocks for reduced management overhead compared to uniform strategies. In our protocol, small blocks, which are typically in large quantities, are placed through consistent hashing. Large blocks, much fewer in practice, are placed through a usage-based policy, and their locations are tracked by Bloom filters. The proposed scheme results in improved storage utilization even with non-uniform cluster nodes. To achieve high scalability and fault resilience, this protocol is fully distributed, relies only on soft states, and supports data replication. We demonstrate the effectiveness and efficiency of this protocol through trace-driven simulation.","Protocols,
Peer to peer computing,
Image storage,
Application software,
Local area networks,
Large-scale systems,
Production,
Availability,
Permission,
Computer science"
"Voxel-guided morphometry (""VGM"") and application to stroke","Monitoring of cerebral diseases associated with a change of morphology (e.g., stroke) requires unprecedented accuracy for quantification of its morphological progression for each voxel. The purpose of this paper is to provide a technique [voxel-guided morphometry (VGM)] to quantify macroscopic anatomical differences. VGM consists of four steps: 1) coarse linear alignment by the extended principle axes theory (ePAT) generalized to affine movements; 2) a cross-correlation-based technique using a matrix-norm for fine linear alignment; 3) the applied high-dimensional multiresolution full multigrid method determines the nonlinear deformations, thereby achieving a complete exploitation of information and effective processing. The method measures a gray-value-guided movement of each voxel from source to target. The resulting high-dimensional deformation field is further processed by 4) determination of volume alterations for each voxel. Furthermore, the effect of linear registration errors on final morphometric measurements is discussed and the conditions for a bijective correspondence of voxels assuming small alterations are derived. To illustrate the technique the changing morphology of different subjects suffering from cerebral infarction is presented by using commonly available T/sub 1/-weighted magnetic resonance volumes. VGM visualizes that ischemic as well as remote regions are affected by stroke.","Morphology,
Magnetic field measurement,
Motion measurement,
Magnetic resonance imaging,
Diseases,
Multigrid methods,
Magnetic resonance,
Visualization,
Magnetic analysis"
On exploiting long range dependence of network traffic in measuring cross traffic on an end-to-end basis,"In this paper we present three theoretically grounded methods: prediction, reconstruction and interpolation, for measuring cross traffic on the bottleneck link of an end-to-end path. The objective is to infer cross traffic as accurately as possible, while not injecting a significant amount of probe packets into the network. In the prediction-based method, we take advantage of the LRD characteristic of the cross traffic to predict the future traffic based on the recent information obtained by probe packets. In the reconstruction method, we rebuild the entire cross traffic process with the information obtained by probe packets. In the interpolation method, we periodically send closely-spaced probe packet pairs to sample cross traffic of the bottleneck link, and infer cross traffic between two sampling points using interpolation. The simulation study indicates that (i) the prediction-based and reconstruction methods can give good mean measurement of cross traffic, while the interpolation method usually captures the instantaneous value of cross traffic better; and (ii) all three methods are adaptive to the dynamic change of cross traffic and are quite robust in the presence of multiple bottleneck links on an end-to-end path.","Telecommunication traffic,
Intelligent networks,
Traffic control,
Probes,
Time measurement,
Interpolation,
Reconstruction algorithms,
Fractals,
Helium,
Computer science"
Frequency layered color indexing for content-based image retrieval,"Image patches of different spatial frequencies are likely to have different perceptual significance as well as reflect different physical properties. Incorporating such concept is helpful to the development of more effective image retrieval techniques. We introduce a method which separates an image into layers, each of which retains only pixels in areas with similar spatial frequency characteristics and uses simple low-level features to index the layers individually. The scheme associates indexing features with perceptual and physical significance thus implicitly incorporating high level knowledge into low level features. We present a computationally efficient implementation of the method, which enhances the power and at the same time retains the simplicity and elegance of basic color indexing. Experimental results are presented to demonstrate the effectiveness of the method.",
Web-based peer assessment in learning computer programming,"Peer assessment is a method of motivating students in learning computer programming, involving student's marking and providing feedback on other student's work. We report on the design and implementation of a novel Web-based peer assessment system, and discuss its deployment on a large programming module. The results indicate that this peer assessment system has successfully helped students to develop their understanding of computer programming.","Programming,
Feedback,
Computer science,
Electronic mail,
Displays,
Operating systems,
Computer languages,
Automatic testing"
Multisensory perception: beyond the visual in visualization,"Mapping information onto more than one sensory modality might let us increase human bandwidth for understanding complex, multivariate data. Researchers have done significant work to explore the effective use of a single non-visual sense for data display. Unfortunately, few researchers have examined the question of what data is best expressed in what way. Lacking a theory of multisensory perception and processing of information, the critical issue is determining what data ""best"" maps onto what sensory input channel. Consider the problem of hydrocarbon reservoirs. Most reservoir engineers would agree that the number of variables required to characterize a reservoir is large (perhaps 30 to 50). If you depend only on visual displays limited to seven variables at a time, then it might require as many as seven such displays to cover the full range of necessary variables. You then must either mentally integrate across those seven displays or go through a process of variable selection and redisplay to achieve a specific goal.","Auditory displays,
Haptic interfaces,
Three dimensional displays,
Sea surface,
Virtual environment,
Data visualization,
Olfactory,
Acceleration,
Workstations,
Frequency"
A model for replica placement in content distribution networks for multimedia applications,"The distribution of multimedia files brings new challenges to the problem of replica placement in content distribution networks (CDN) and invalidates several assumptions underlying the existing solutions. In this paper we formulate a new model for the problem of replica placement to accommodate these new characteristics. We perform a theoretical analysis of the cost of distributing multimedia files over CDNs and find out that, contrary to the intuition, deploying as many replicas as possible is not always a good strategy. We then propose several replica placement algorithms that can determine the optimal number of replicas we should select from a given set of potential sites. By simulation we demonstrate that the performance of clients may degrade if we choose too many sites for replica placement.","Intelligent networks,
Costs,
Network servers,
Application software,
Performance analysis,
Web server,
Computer science,
Degradation,
Internet,
Explosives"
Quantifying locality effect in data access delay: memory logP,"The application of hardware-parameterized models to distributed systems can result in omission of key bottlenecks such as the full cost of inter-node communication in a shared memory cluster. However, inclusion in the model of message characteristics and complex memory hierarchies may result in impractical models. Nonetheless, the growing gap between memory and CPU performance combined with the trend toward large scale clustered shared memory platforms implies an increased need to consider the impact of local memory communication on parallel processing in distributed systems. We present a simple and useful model of point-to-point memory communication to predict and analyze the latency of memory copy, pack and unpack. We use the model to isolate contributions of hardware, middleware, and software to data transfers on Intel- and MIPS-based platforms.","Delay effects,
Costs,
Hardware,
Space technology,
Buffer storage,
Computer science,
Application software,
Bandwidth,
Kirk field collapse effect,
Data engineering"
Empirical studies for effective near-field haptics in virtual environments,"This paper presents results from two experiments into the use of vibrotactile cues for near-field haptics in virtual environments. In one experiment, subjects were tested on their ability to identify the location of a one-second vibrotactile stimulus presented to a single tactor of a 3-by-3 array on their back. We recorded an 84% correct identification rate. In a second experiment, subjects were asked to match the intensity of a vibrotactile stimulus presented at one location with the intensity at another location. We found that subjects could match the intensities to within 7Hz if the reference and adjustable stimuli were presented at the same location, but only to within 18Hz otherwise.","Haptic interfaces,
Back,
Virtual environment,
Humans,
DC motors,
Feedback,
Control systems,
Computer science,
Information science,
Testing"
A hierarchical particle swarm optimizer,"A hierarchical version of the particle swarm optimization method called H-PSO is introduced. In H-PSO the particles are arranged in a dynamic hierarchy that is used to define a neighborhood structure. Depending on the quality of their so far best found solution the particles move up or down the hierarchy so that good particles have a higher influence on the swarm. Moreover, the hierarchy is used to define different search properties for the particles. Several variants of H-PSO are compared experimentally with variants of the standard PSO.","Particle swarm optimization,
Iterative algorithms,
Parallel processing,
Computer science,
Optimization methods,
Tree data structures,
Iterative methods,
Multidimensional systems,
Velocity control"
"An investigation into the source of power for AIRS, an artificial immune classification system","The AIRS classifier, based on metaphors from the field of artificial immune systems, has shown itself to be an effective general purpose classifier across a broad spectrum of classification problems. This research examines the new classifier empirically, replacing one of the two likely sources of its classification power with alternative modifications. The results are slightly less effective, but not statistically significantly so. We conclude that the modifications, which are computationally somewhat more efficient, provide fast test versions of AIRS for users to experiment with. We also conclude that the chief source of classification power of AIRS must lie in its replacement and maintenance of its memory cell population.","Artificial immune systems,
Testing,
Classification algorithms,
Psychology,
Computer science,
Power engineering and energy,
Laboratories"
Min-cost flow-based algorithm for simultaneous pin assignment and routing,"Macroblock pin assignment and routing are important tasks in physical design. Existing algorithms for these problems can be classified into two categories: 1) a two-step approach where pin assignment is followed by routing and 2) a net-by-net approach where pin assignment and routing for a single net are performed simultaneously. However, none of the existing algorithms is ""exact"" in the sense that they may fail to route all of the nets even though a feasible solution exists. This remains to be true even if only two-pin nets with fixed pins between two blocks are concerned. In this paper, we consider the problem of two-pin net connections from one macroblock to all of the other blocks, and present the first polynomial-time exact algorithm for simultaneous pin assignment and routing for all of the two-pin nets between one block (source block) and all of the other blocks. In addition to finding a feasible solution whenever one exists, it guarantees to find a pin-assignment/routing solution with minimum cost /spl alpha//spl middot/W+/spl beta//spl middot/V, where W is the total wire length and V is the total number of vias. Our algorithm has various applications. 1) It is suitable in engineering change order (ECO) situations where the existing solution is modified incrementally. 2) Given any pin assignment and routing solution obtained by any existing method, our algorithm can be used to increase the number of routed nets and reduce the routing cost. Furthermore, it provides an efficient algorithm for the pin assignment and routing problem of all of the blocks. The method is applicable to both global and detailed routing with arbitrary routing obstacles on multiple layers. Experimental results demonstrate its efficiency and effectiveness.","Routing,
Pins,
Costs,
Polynomials,
Wire,
Large-scale systems,
Runtime,
Computer science,
Silicon"
Flux driven fly throughs,"We present a fast, robust and automatic method for computing central paths through tubular structures for application to virtual endoscopy. The key idea is to utilize a medial surface algorithm, which exploits properties of the average outward flux of the gradient vector field of a Euclidean distance function the boundary of the structure of interest. The algorithm is modified to yield a collection of 3D curves, each of which is locally centered. The approach requires no user interaction, and is virtually parameter free and has low computational complexity. We illustrate the approach on segmented colon and vessel data.","Endoscopes,
Image segmentation,
Biomedical imaging,
Euclidean distance,
Blood vessels,
Rendering (computer graphics),
Computer vision,
Computer science,
Biomedical engineering,
Robustness"
Tele-coordinated control of multi-robot systems via the Internet,"The coordination of multi-robots is required in many scenarios for efficiency and task completion. Combined with teleoperation capabilities, coordinating robots provide a powerful tool. Add to this the Internet and now it is possible for multi-experts at multi-remote sites to control multi-robots in a coordinated fashion. For this to be feasible there are several hurdles to be crossed including Internet type delays, uncertainties in the environment and uncertainties in the object manipulated. In addition, there is a need to measure and control the quality of tele-coordination. This paper proposes a measure for the quality of tele-coordination, referred to as the coordination index, and details the design procedure that ensures a system performs at a required index. The theory developed was tested by bilaterally tele-coordinating two mobile manipulators via the Internet. The experimental results confirmed the theory presented.","Control systems,
Multirobot systems,
Internet,
Robot kinematics,
Centralized control,
Robotics and automation,
Manipulators,
Delay effects,
Automatic control,
Computer science"
Multi-objective and MGG evolutionary algorithm for constrained optimization,"This paper presents a new approach to handle constrained optimization using evolutionary algorithms. The new technique converts constrained optimization to a two-objective optimization: one is the original objective function, the other is the degree function violating the constraints. By using Pareto-dominance in the multi-objective optimization, individual's Pareto strength is defined. Based on Pareto strength and minimal generation gap (MGG) model, a new real-coded genetic algorithm is designed. The new approach is compared with some other evolutionary optimization techniques on several benchmark functions. The results show that the new approach outperforms those existing techniques in feasibility, effectiveness and generality. Especially for some complicated optimization problems with inequality and equality constraints, the proposed method provides better numerical accuracy.","Constraint optimization,
Evolutionary computation,
Pareto optimization,
Genetic programming,
Computer science,
Genetic algorithms,
Software engineering,
Testing,
Educational institutions,
Helium"
A new set of passive routing attacks in mobile ad hoc networks,"In hostile environments, adversaries can launch passive attacks against interceptable routing information embedded in routing messages and data packets. Allowing adversaries to trace network routes and infer the motion pattern of nodes at the end of those routes may pose a serious threat to covert operations. In this paper we propose a feasible adversary model of such attacks, then present several instantiations and study the principles of designing corresponding countermeasures. We demonstrate that existing ad hoc routing protocols are vulnerable to passive attacks: in the feasible adversary model, (a) the location and motion patterns of mobile nodes can be traced, while (b) proactive and reactive/on-demand ad hoc routes across multiple mobile nodes can be visualized by the adversary. We conclude that ad hoc networks deployed in hostile environments need new countermeasures to resist such passive attacks.","Routing,
Intelligent networks,
Mobile ad hoc networks,
Privacy,
Visualization,
Ad hoc networks,
Telecommunication traffic,
Timing,
Computer science,
Protocols"
Evolution of planning for wireless communication systems,"In this paper we provide a detailed and comprehensive survey of proposed approaches for network design, charting the evolution of models and techniques for the automatic planning of cellular wireless services. These problems present themselves as a trade-off between commitment to infrastructure and quality of service, and have become increasingly complex with the advent of more sophisticated protocols and wireless architectures. Consequently these problems are receiving increased attention from researchers in a variety of fields who adopt a wide range of models, assumptions and methodologies for problem solution. We seek to unify this dispersed and fragmented literature by charting the evolution of centralised planning for cellular systems.",
A missing feature approach to instrument identification in polyphonic music,"Gaussian mixture model (GMM) classifiers have been shown to give good instrument recognition performance for monophonic music played by a single instrument. However, many applications (such as automatic music transcription) require instrument identification from polyphonic, multi-instrumental recordings. We address this problem by incorporating ideas from missing feature theory into a GMM classifier. Specifically, frequency regions that are dominated by energy from an interfering tone are marked as unreliable and excluded from the classification process. This approach has been evaluated on random two-tone chords and an excerpt from a commercially available compact disc, with promising results.",
Energy benefits of a configurable line size cache for embedded systems,"Previous work has shown that cache line sizes impact performance differently for different desktop programs; some programs work better with small line sizes, others with larger line sizes. Typical processors come with a line size that is a compromise, working best on the average for a variety of programs. We analyze the energy impact of different line sizes, for 19 embedded system benchmarks, and we show that tuning the line size to a particular program can reduce memory access energy by 50% in some examples. Our data argues strongly for the need for embedded microprocessors to have configurable line size caches, and for embedded system designers to put effort into choosing the best line size for their programs.","Embedded system,
Microprocessors,
National electric code,
Embedded computing,
Switches,
Computer science,
Power engineering and energy,
Energy consumption,
Batteries,
Wires"
1.14-GHz self-aligned vibrating micromechanical disk resonator,"Micromechanical radial-contour mode disk resonators featuring new self-aligned stems have been demonstrated with record resonance frequencies up to 1.14 GHz and measured Q's at this frequency >1,500 in both vacuum and air. In addition, a 733-MHz version has been demonstrated with Q's of 7,330 and 6,100 in vacuum and air, respectively. For these resonators, self-alignment of the stem to exactly the center of the disk it supports allows balancing of the resonator far superior to that achieved by previous versions (where separate masks were used to define the disk and stem), allowing the present devices to retain high Q while achieving frequencies in the GHz range for the first time. In addition to providing measured results, this paper formulates an equivalent electrical circuit model that accurately predicts the performance of this disk resonator.","Resonance,
Resonant frequency,
Micromechanical devices,
Frequency measurement,
Electric variables measurement,
Equivalent circuits,
Electrodes,
Integrated circuit modeling,
Computer science,
Predictive models"
An improved branch and bound algorithm for exact BDD minimization,"Ordered binary decision diagrams (BDDs) are a data structure for efficient representation and manipulation of Boolean functions. They are frequently used in logic synthesis and formal verification. The size of the BDDs depends on a chosen variable ordering, i.e., the size may vary from linear to exponential, and the problem of Improving the variable ordering is known to be NP-complete. In this paper, we present a new exact branch and bound technique for determining an optimal variable order. In contrast to all previous approaches that only considered one lower bound, our method makes use of a combination of three bounds and, by this, avoids unnecessary computations. The lower bounds are derived by generalization of a lower bound known from very large scale integration design. They allow one to build the BDD either top down or bottom up. Experimental results are given to show the efficiency of our approach.",
Estimating coronary artery lumen area with optimization-based contour detection,"A modified optimization-based contour detection method was presented to compute the lumen area of the coronary artery from intravascular ultrasound (IVUS) video images. First, the search range for the artery inner wall was determined based on the continuity of IVUS video frames. Next, the internal and external energy were calculated to describe the smoothness of the arterial wall and the grayscale variation of ultrasound images, respectively. Here, a novel form of the external energy which combines the gradient and variance of the intensity of image in the radial direction was used. Finally, the minimal energy path based on the optimum contour of the artery wall was obtained using circular dynamic programming (DP). By the comparison with the typical DP procedure using the traditional external energy form, based only on the image gradient, the reliability of this modified method is considerably improved in the measurement of coronary artery lumen area.","Arteries,
Ultrasonic imaging,
Optimization methods,
Dynamic programming,
Ultrasonic variables measurement,
Gray-scale,
Area measurement,
Energy measurement,
Diseases,
Medical treatment"
Facilitating fuzzy association rules mining by using multi-objective genetic algorithms for automated clustering,"We propose an automated clustering method based on multiobjective genetic algorithms (GA); the aim of this method is to automatically cluster values of a given quantitative attribute to obtain large number of large itemsets in low duration (time). We compare the proposed multi-objective GA-based approach with CURE-based approach. In addition to the autonomous specification of fuzzy sets, experimental results showed that the proposed automated clustering exhibits good performance over CURE-based approach in terms of runtime as well as the number of large itemsets and interesting association rules.","Association rules,
Data mining,
Genetic algorithms,
Fuzzy sets,
Itemsets,
Clustering algorithms,
Clustering methods,
Genetic engineering,
Computer science,
Runtime"
Analysis of inconsistency in graph-based viewpoints: a category-theoretical approach,"Eliciting the requirements for a proposed system typically involves different stakeholders with different expertise, responsibilities, and perspectives. Viewpoints-based approaches have been proposed as a way to manage incomplete and inconsistent models gathered from multiple sources. In this paper, we propose a category-theoretical framework for the analysis of fuzzy viewpoints. Informally, a fuzzy viewpoint is graph in which the elements of a lattice are used to specify the amount of knowledge available about the details of nodes and edges. By defining an appropriate notion of morphism between fuzzy viewpoints, we construct categories of fuzzy viewpoints and prove that these categories are (finitely) complete. We then show how colimits can be employed to merge the viewpoints and detect the inconsistencies that arise independent of any particular choice of viewpoint semantics. We illustrate an application of the framework through a case-study showing how fuzzy viewpoints can serve as a requirements elicitation tool in reactive systems.","Fuzzy sets,
Lattices,
Logic,
Merging,
Software engineering,
Computer science,
Application software,
Fuzzy systems,
Information analysis,
Programming"
Minimal energy fixed-priority scheduling for variable voltage processors,"To fully exploit the benefit of variable voltage processors, voltage schedules must be designed in the context of work load requirement. In this paper, we present an approach to finding the least-energy voltage schedule for executing real-time jobs on such a processor according to a fixed priority, preemptive policy. The significance of our approach is that the theoretical limit in terms of energy saving for such systems is established, which can, thus, serve as the standard to evaluate the performance of various heuristic approaches. Two algorithms for deriving the optimal voltage schedule are provided. The first one explores fundamental properties of voltage schedules while the second one builds on the first one to further reduce the computational cost. Experimental results are shown to compare the results of this paper with previous ones.","Processor scheduling,
Voltage,
Real time systems,
Scheduling algorithm,
Timing,
Very large scale integration,
Energy consumption,
Computer science,
Power engineering and energy,
Testing"
Motion planning for humanoid walking in a layered environment,"Motion planning is one of the key capabilities for autonomous humanoid robots. Previous researches have focused on weight balancing, collision detection, and gait generation. Most planners either assume that the environment can be simplified to a 2D workspace or assume that the path is given. In this paper, we propose a motion planning system capable of generating both global and local motions for a humanoid robot in a layered or two and half dimensional environment. The planner can generate a gross motion that moves the humanoid vertically as well as horizontally to avoid obstacles in the environments. The gross motion is further realized by a local planner that determines the most efficient footsteps and locomotion over uneven terrain. If the local planner fails, the failure is feedback to the global planner to consider other alternative paths. The implemented humanoid planning system is an interactive tool that can compute collision-free motions for a humanoid robot in an online manner.",
Building three-dimensional ribonucleic acid structures,"MC-Sym is an intelligent computer system that builds 3D ribonucleic acid structures from low-resolution data by combining symbolic and numerical computations. Specifically, the symbolic step generates all-atom sketches of 3D structures that nonlinear optimization methods can refine in a second step.","Buildings,
RNA,
Intelligent systems,
Intelligent structures,
Nuclear magnetic resonance,
Biochemistry,
Laboratories,
Optimization methods,
Biology computing,
Proteins"
Calibration of a hybrid camera network,"Visual surveillance using a camera network has imposed new challenges to camera calibration. An essential problem is that a large number of cameras may not have a common field of view or even be synchronized well. We propose to use a hybrid camera network that consists of catadioptric and perspective cameras for a visual surveillance task. The relations between multiple views of a scene captured from different cameras can be then calibrated under the catadioptric camera's coordinate system. We address the important issue of how to calibrate the hybrid camera network. We calibrate the hybrid camera network in three steps. First, we calibrate the catadioptric camera using only the vanishing points. In order to reduce computational complexity, we calibrate the camera without the mirror first and then calibrate the catadioptric camera system. Second, we determine 3D positions of some points using as few as two spatial parallel lines and some equidistance points. Finally, we calibrate other perspective cameras based on these known spatial points.","Calibration,
Cameras,
Surveillance,
Mirrors,
Layout,
Computer science,
Computational complexity,
Monitoring,
Target tracking,
Humans"
Dynamic clustering of evolving streams with a single pass,"Stream data is common in many applications, e.g., stock quotes, merchandize sales record, system logs, etc.. It is of great importance to analyze these stream data. As one of the most commonly used techniques, clustering on streams can help to detect and monitor correlations among streams. Due to the unique nature of streaming data, direct application of most existing clustering algorithms fails to deliver efficient results. We introduce a novel model of stream cluster, which employs a weighted distance measure. In addition, we device a novel efficient algorithm which can effectively discover all stream clusters.","Marketing and sales,
Clustering algorithms,
Data analysis,
Computer science,
Application software,
Condition monitoring,
Weight measurement,
Computerized monitoring,
Computer networks,
Resource management"
"Comments on ""The confounding effect of class size on the validity of object-oriented metrics""","It has been proposed by El Emam et al. (ibid. vol.27 (7), 2001) that size should be taken into account as a confounding variable when validating object-oriented metrics. We take issue with this perspective since the ability to measure size does not temporally precede the ability to measure many of the object-oriented metrics that have been proposed. Hence, the condition that a confounding variable must occur causally prior to another explanatory variable is not met. In addition, when specifying multivariate models of defects that incorporate object-oriented metrics, entering size as an explanatory variable may result in misspecified models that lack internal consistency. Examples are given where this misspecification occurs.",
A cluster-based backbone infrastructure for broadcasting in MANETs,"Broadcasting is a fundamental service in mobile ad hoc networks (MANETs). Two categories of algorithms, based on source-independent and source-dependent connected dominating sets (CDSs), are proposed in literature to reduce the broadcast redundancy. In this paper, a cluster-based backbone infrastructure is proposed for broadcasting in MANETs. The backbone of the network takes advantage of the cluster structure and only requires clusterheads and some selected gateways to forward the broadcast packet. The static backbone (cluster-based source-independent CDS) consists of fixed clusterheads and selected source-independent gateways. Each clusterhead individually selects its gateways to connect all the clusterheads in its coverage set. The dynamic backbone (clusterbased source-dependent CDS) consists of fixed clusterheads and dynamically selected gateways. It is constructed step by step as the broadcast packet traverses the network. Each clusterhead selects some gateways to forward the packet when it sends the packet to all the clusterheads in its coverage set. Both the static and dynamic backbone structures have a constant approximation ratio to the minimum CDS. Simulations are conducted to compare both the static and dynamic backbones with another cluster-based source-independent CDS algorithm proposed recently.","Spine,
Broadcasting,
Mobile ad hoc networks,
Clustering algorithms,
Intelligent networks,
Computer science,
Electronic mail,
Base stations,
Routing protocols,
Wireless application protocol"
A new genetic algorithm for nonlinear multiregressions based on generalized Choquet integrals,"This paper gives a new genetic algorithm for nonlinear multiregression based on generalized Choquet integrals with respect to signed fuzzy measures. Unlike the previous work where the values of the signed fuzzy measure are determined by random search in a genetic algorithm with other regression coefficients together; in this new algorithm, they are determined algebraically and, therefore, its complexity is much lower than before.","Genetic algorithms,
Fuzzy sets,
Fuzzy systems,
Biological cells,
Mathematics,
Computer science,
Data mining,
Predictive models,
Power measurement"
Exemplar-based pattern recognition via semi-supervised learning,"The focus of this paper is semi-supervised learning in the context of pattern recognition. Semi-supervised learning (SSL) refers to the semi-supervised construction of clusters during the training phase of exemplar-based classifiers. Using artificially generated data sets we present experimental results of classifiers that follow the SSL paradigm and we show that, especially for difficult pattern recognition problems featuring high class overlap, for exemplar-based classifiers implementing SSL i) the generalization performance improves, while ii) the number of necessary exemplars decreases significantly, when compared to the original versions of the classifiers.","Pattern recognition,
Semisupervised learning,
Subspace constraints,
Computer science,
Shape,
Training data,
Testing,
Neural networks,
Neurons,
Resonance"
Generating small combinatorial test suites to cover input-output relationships,"In this paper, we consider a problem that arises in black box testing: generating small test suites (i.e., sets of test cases) where the combinations that have to be covered are specified by input-output parameter relationships of a software system. That is, we only consider combinations of input parameters that affect an output parameter. We also do not assume that the input parameters have the same number of values. To solve this problem, we revisit the greedy algorithm for test generation and show that the size of the test suite it generates is within a logarithmic factor of the optimal. Unfortunately, the algorithm's main weaknesses are its time and space requirements for construction. To address this issue, we present a problem reduction technique that makes the greedy algorithm or any other test suite generation method more efficient if the reduction in size is significant.","Software testing,
System testing,
Software systems,
Greedy algorithms,
NIST,
Computer science,
Software engineering,
Costs,
Application software,
Performance evaluation"
Induced-current electrical impedance tomography: a 2-D theoretical Simulation,"A reconstruction algorithm, based on the modified Newton-Raphson algorithm, was developed for induced-current electrical impedance tomography and studied in theoretical two-dimensional geometry representing a human thorax. The finite-volume method was applied for the discretization of the physical domain, resulting in a symbolic representation of the Jacobian matrix, which is accurate and fast to construct. Several system configurations, differing in the number of excitation coils and electrodes, were simulated, and the performance in thoracic imaging was studied. It was found that a six-coil system shows a significant 40% improvement of conductivity values reconstruction over the three-coil system (an error of 2.06 /spl Omega//sup -1/ compared with 3.44 /spl Omega//sup -1/). A number of 32 electrodes was found to be sufficient, being the smallest number of electrodes to still provide a reasonable performance (only 4.2% degradation in average conductivity error compared with the maximum possible 106-electrode system).","Impedance,
Tomography,
Electrodes,
Conductivity,
Reconstruction algorithms,
Geometry,
Humans,
Thorax,
Finite volume methods,
Jacobian matrices"
Component-based LDA method for face recognition with one training sample,"Many face recognition algorithms/systems have been developed in the last decade and excellent performances are also reported when there is sufficient number of representative training samples. In many real-life applications, only one training sample is available. Under this situation, the performance of existing algorithms will be degraded dramatically or the formulation is incorrect, which in turn, the algorithm cannot be implemented. We propose a component-based linear discriminant analysis (LDA) method to solve the one training sample problem. The basic idea of the proposed method is to construct local facial feature component bunches by moving each local feature region in four directions. In this way, we not only generate more samples, but also consider the face detection localization error while training. After that, we employ a sub-space LDA method, which is tailor-made for small number of training samples, for the local feature projection to maximize the discrimination power. Finally, combining the contributions of each local feature draws the recognition decision. FERET database is used for evaluating the proposed method and results are encouraging.","Linear discriminant analysis,
Face recognition,
Computer science,
Mathematics,
Degradation,
Facial features,
Face detection,
Spatial databases,
Information technology,
Lighting"
Approximation algorithms for orienteering and discounted-reward TSP,"In this paper, we give the first constant-factor approximation algorithm for the rooted orienteering problem, as well as a new problem that we call the Discounted-Reward TSP, motivated by robot navigation. In both problems, we are given a graph with lengths on edges and prizes (rewards) on nodes, and a start node s. In the orienteering problem, the goal is to find a path that maximizes the reward collected, subject to a hard limit on the total length of the path. In the Discounted-Reward TSP, instead of a length limit we are given a discount factor /spl gamma/, and the goal is to maximize total discounted reward collected, where reward for a node reached at time t is discounted by /spl gamma//sup t/. This is similar to the objective considered in Markov decision processes (MDPs) except we only receive a reward the first time a node is visited. We also consider tree and multiple-path variants of these problems and provide approximations for those as well. Although the unrooted orienteering problem, where there is no fixed start node s, has been known to be approximable using algorithms for related problems such as k-TSP (in which the amount of reward to be collected is fixed and the total length is approximately minimized), ours is the first to approximate the rooted question, solving an open problem based on B. Awerbuch et al. (1999) and E.M. Arkin (1998).","Approximation algorithms,
Robots,
Computer science,
Navigation,
Packaging,
Batteries,
Laboratories,
Traveling salesman problems,
Power supplies,
Path planning"
An obstacle-avoiding minimum variation B-spline problem,"We study the problem of computing a planar curve, restricted to lie between two given polygonal chains, such that the integral of the square of arc-length derivative of curvature along the curve is minimized. We introduce the minimum variation B-spline problem, which is a linearly constrained optimization problem over curves, defined by B-spline functions only. An empirical investigation indicates that this problem has one unique solution among all uniform quartic B-spline functions. Furthermore, we prove that, for any B-spline function, the convexity properties of the problem are preserved subject to a scaling and translation of the knot sequence defining the B-spline.","Spline,
Remotely operated vehicles,
Computer aided manufacturing,
Constraint optimization,
Surface reconstruction,
Cost function,
Shape control,
Computer science,
Mathematics,
Path planning"
On the Infiniband subnet discovery process,"InfiniBand is becoming an industry standard both for communication between processing nodes and I/O devices, and for interprocessor communication. Instead of using a shared bus, InfiniBand employs an arbitrary (possibly irregular) switched point-to-point network. InfiniBand specification defines a basic management infrastructure that is responsible for subnet configuration, activation, and fault tolerance. After the detection of a topology change, management entities collect the current subnet topology. The topology discovery algorithm is one of the management issues that are outside the scope of the current specification. Preliminary implementations obtain the entire topological information each time a change is detected. In this work, we present and analyze an optimized implementation, based on exploring only the region that has been affected by the change.","Computer network management,
Message passing,
Local area networks,
Computer fault tolerance,
Network operating systems,
Communication system software,
Communication standards,
Communication system routing,
Complexity theory,
Optimization methods,
Data communication"
Statistical per-flow service bounds in a network with aggregate provisioning,"Scalability concerns of QoS implementations have stipulated service architectures where QoS is not provisioned separately to each flow, but instead to aggregates of flows. This paper determines stochastic bounds for the service experienced by a single flow when resources are managed for aggregates of flows and when the scheduling algorithms used in the network are not known. Using a recently developed statistical network calculus, per-flow bounds can be calculated for backlog, delay, and the burstiness of output traffic.",
Infrastructures for virtual organizations - where we are,"The design and development of invisible, easy to use and affordable ICT infrastructures is a key prerequisite for the effective large-scale implantation of the collaborative networks organizations such as virtual organizations, professional virtual communities, e-science communities, etc. This paper introduces an overview of the main classes of infrastructure required by different types of virtual organizations and the main approaches being followed in the European projects in the area.","Collaboration,
Collaborative work,
Humans,
Large-scale systems,
Business communication,
Computer networks,
Companies,
Supply chains,
Virtual enterprises,
Context"
Towards a model for trust relationships in virtual enterprises,"Trust is a crucial concept in order to address scalability in managing security tasks such as authentication, authorization and access control. In this paper we discuss the concept of trust by defining a general theoretical model to describe basic trust relationships in heterogeneous environments composed of several Administrative Domains (ADs), i.e. autonomous domains for both security and administrative issues. As a result, we introduce the design of a Trust Management System (TMS) that we plan to deploy into a middleware architecture in order to provide a framework for implementing trust relationships in Virtual Enterprises (VEs).",
Studying the chaos of code development,,"Chaos,
Software systems,
Control systems,
Project management,
Software architecture,
Computer science,
Costs,
History,
Operating systems,
Software development management"
Proactive server roaming for mitigating denial-of-service attacks,"We propose a framework based on proactive server roaming to mitigate the effects of denial-of-service (DoS) attacks. The active server proactively changes its location within a pool of servers to defend against unpredictable and undetectable attacks. Only legitimate clients can follow the active server as it roams. We present algorithms that are secure, distributed, randomized, and adaptive for triggering the roaming and determining the next server to roam to. We propose some modifications to the state recovery process of existing TCP connection-migration schemes to suit roaming. Preliminary experiments in a FreeBSD network show that the overhead of server roaming is small, in terms of response time, in the absence of attacks. Further, during an attack, roaming significantly improves the response time.","Computer crime,
Network servers,
Operating systems,
Telecommunication traffic,
Authentication,
Computer science,
Information science,
Invasive software,
Resource management,
Delay"
On the teaching of distributed software development,"As the software industry moves towards software development projects involving several sites around the world, universities should incorporate this trend into their software engineering curricula. We describe the experiences from the development of a university course in distributed software development. Some of the problems of distributed development make it inherently difficult to transfer this domain to the university environment. Also, the concept of ""distribution "" has penetrated not only the contents of the course but many other levels as well.","Education,
Programming,
Software engineering,
Collaborative software,
Educational products,
Companies,
Cultural differences,
Computer science,
Teamwork,
Information technology"
Photo-consistent 3D fire by Flame-Sheet decomposition,"This paper considers the problem of reconstructing visually realistic 3D models of fire from a very small set of simultaneous views (even two). By modeling fire as a semitransparent 3D density field, we show that fire reconstruction is equivalent to a severely under-constrained computerized tomography problem, for which traditional methods break down. Our approach is based on the observation that every pair of photographs of a semitransparent scene defines a unique density field, called a Flame Sheet, that (1) concentrates all its density on one connected, semitransparent surface, (2) reproduces the two photos exactly, and (3) is the most spatially-coherent density field that does so. From this observation, we reduce fire reconstruction to the convex combination of sheet-like density fields, each of which is derived from the Flame Sheet of two input photos. Experimental results suggest that this method enables high-quality view extrapolation without over-fitting artifacts.","Fires,
Image reconstruction,
Layout,
Computed tomography,
Extrapolation,
Data mining,
Videos,
Combustion,
Computer vision,
Computer science"
Content-trajectory approach for searching video databases,"In the past few years, modeling and querying video databases have been a subject of extensive research to develop tools for effective search of videos. In this paper, we present a hierarchal approach to model videos at three levels, object level (OL), frame level (FL), and shot level (SL). The model captures the visual features of individual objects at OL, visual-spatio-temporal (VST) relationships between objects at FL, and time-varying visual features and time-varying VST relationships at SL. We call the combination of the time-varying visual features and the time-varying VST relationships a Content trajectory which is used to represent and index a shot. A novel query interface that allows users to describe the time-varying contents of complex video shots such as those of skiers, soccer players, etc., by sketch and feature specification is presented. Our experimental results prove the effectiveness of modeling and querying shots using the content trajectory approach.",
Complex division with prescaling of operands,"We adapt the radix-r digit-recurrence division algorithm to complex division. By prescaling the operands, we make the selection of quotient digits simple. This leads to a simple hardware implementation, and allows correct rounding of complex quotient. To reduce large prescaling tables required for radices greater than 4, we adapt the bipartite-table method to multiple-operand functions.","Hardware,
Robustness,
Computer science,
Astronomy,
Radio frequency,
Extraterrestrial measurements,
Algorithm design and analysis,
Interpolation,
Computer architecture"
Supporting QoS with look-ahead window contention resolution in optical burst switched networks,Optical burst switching (OBS) has been proposed as a competitive hybrid switching technology to support the next-generation optical Internet. This paper addresses the problem of contention in OBS networks and introduces a new contention resolution algorithm called look-ahead window contention resolution (LCR) that can also support service differentiation. Simulation results show that the performance of LCR is competitive to existing contention resolution mechanisms in terms of reducing burst loss.,
Always Good Turing: asymptotically optimal probability estimation,"While deciphering the German Enigma code during World War II, I.J. Good and A.M. Turing considered the problem of estimating a probability distribution from a sample of data. They derived a surprising and unintuitive formula that has since been used in a variety of applications and studied by a number of researchers. Borrowing an information-theoretic and machine-learning framework, we define the attenuation of a probability estimator as the largest possible ratio between the per-symbol probability assigned to an arbitrarily-long sequence by any distribution, and the corresponding probability assigned by the estimator. We show that some common estimators have infinite attenuation and that the attenuation of the Good-Turing estimator is low, yet larger than one. We then derive an estimator whose attenuation is one, namely, as the length of any sequence increases, the per-symbol probability assigned by the estimator is at least the highest possible. Interestingly, some of the proofs use celebrated results by Hardy and Ramanujan on the number of partitions of an integer. To better understand the behavior of the estimator, we study the probability it assigns to several simple sequences. We show that some sequences this probability agrees with our intuition, while for others it is rather unexpected.",Computer science
"Evaluating software architectures: development, stability, and evolution",Summary form only given. We survey seminal work on software architecture evaluation methods. We then look at an emerging class of methods that explicates evaluating software architectures for stability and evolution. We define architectural stability and formulate the problem of evaluating software architectures for stability and evolution. We draw attention to the use of Architectures Description Languages (ADLs) for supporting the evaluation of software architectures in general and for architectural stability specifically.,"Computer architecture,
Stability,
Software architecture,
Computer science,
Educational institutions,
Architecture description languages,
Software engineering"
Automated software testing using a metaheuristic technique based on Tabu search,"The use of techniques for automating the generation of software test cases is very important as it can reduce the time and cost of this process. The latest methods for automatic generation of tests use metaheuristic search techniques, i.e. genetic algorithms and simulated annealing. There is a great deal of research into the use of genetic algorithms to obtain a specific coverage in software testing but there is none using the metaheuristic Tabu search technique. In this paper, we explain how we have created an efficient testing technique that combines Tabu search with Korel chaining approach. Our technique automatically generates test data in order to obtain branch coverage in software testing.",
A diskless checkpointing algorithm for super-scale architectures applied to the fast fourier transform,,"Checkpointing,
Fast Fourier transforms,
Fault tolerant systems,
Computer architecture,
Distributed computing,
Delay,
Application software,
Concurrent computing,
Bandwidth,
Computer science"
Horror film genre typing and scene labeling via audio analysis,"We examine localised sound energy patterns, or events, that we associate with high level affect experienced with films. The study of sound energy events in conjunction with their intended affect enable the analysis of film at a higher conceptual level, such as genre. The various affect/emotional responses we investigate in this paper are brought about by well established patterns of sound energy dynamics employed in audio tracks of horror films. This allows the examination of the thematic content of the films in relation to horror elements. We analyse the frequency of sound energy and affect events at a film level as well as at a scene level, and propose measures indicative of the film genre and scene content. Using 4 horror, and 2 non-horror movies as experimental data we establish a correlation between the sound energy event types and horrific thematic content within film, thus enabling an automated mechanism for genre typing and scene content labeling in film.","Layout,
Labeling,
Motion pictures,
Energy measurement,
Computer science,
Frequency measurement,
Modems,
Concrete,
USA Councils,
Pattern analysis"
Electrical and computer engineering curriculum assessment via senior miniproject,"Traditionally, senior capstone projects have been used for comprehensive student assessment. Other mechanisms exist for engineering course assessments, such as teacher/course evaluations, homework and test results, student office visits, and senior exit interviews. Each of these methods has weaknesses if the primary objective is curriculum improvement. The new Accreditation Board for Engineering and Technology (ABET) Engineering Criteria 2000 (EC-2000) accreditation process requires new methods to allow for continuous program improvement. This paper discusses the six-week miniproject for senior students in the Electrical and Computer Engineering program at Bradley University. Use of the miniproject to steer the curriculum design content has been in place for eleven years. The evaluation results have been used successfully to implement course, laboratory, and curriculum modifications. The paper discusses the project, its costs, evaluation procedures, curriculum modifications and improvements, and use of the project in the EC-2000 accreditation process.","Electrical engineering education,
Computer science education"
Estimating volumetric motion in human thorax with parametric matching constraints,"In radiotherapy (RT), organ motion caused by breathing prevents accurate patient positioning, radiation dose, and target volume determination. Most of the motion-compensated trial techniques require collaboration of the patient and expensive equipment. Estimating the motion between two computed tomography (CT) three-dimensional scans at the extremes of the breathing cycle and including this information in the RT planning has been shyly considered, mainly because that is a tedious manual task. This paper proposes a method to compute in a fully automatic fashion the spatial correspondence between those sets of volumetric CT data. Given the large ambiguity present in this problem, the method aims to reduce gradually this uncertainty through two main phases: a similarity-parametrization data analysis and a projection-regularization phase. Results on a real study show a high accuracy in establishing the spatial correspondence between both sets. Embedding this method in RT planning tools is foreseen, after making some suggested improvements and proving the validity of the two-scan approach.","Motion estimation,
Humans,
Thorax,
Computed tomography,
Neoplasms,
Collaboration,
Lesions,
Communications technology,
Uncertainty,
Data analysis"
A multi-resolution data structure for two-dimensional Morse-Smale functions,"We combine topological and geometric methods to construct a multi-resolution data structure for functions over two-dimensional domains. Starting with the Morse-Smale complex, we construct a topological hierarchy by progressively canceling critical points in pairs. Concurrently, we create a geometric hierarchy by adapting the geometry to the changes in topology. The data structure supports mesh traversal operations similarly to traditional multi-resolution representations.","Data structures,
Multiresolution analysis,
Electrostatic measurements,
Spatial resolution,
Solid modeling,
Data analysis,
Image processing,
Computer science,
Scientific computing,
Laboratories"
Almost Boolean functions: the design of Boolean functions by spectral inversion,"The design of Boolean functions with properties of cryptographic significance is a hard task. In this paper, we adopt an unorthodox approach to the design of such functions. Our search space is the set of functions that possess the required properties. It is 'Booleanness' that is evolved.","Boolean functions,
Cryptography,
Algorithm design and analysis,
Zinc,
Jacobian matrices,
Computer science,
Mathematics,
Graphics,
Genetic algorithms,
Simulated annealing"
Towards predicting robot team performance,"In this paper we develop a method for predicting the performance of human-robot teams consisting of a single user and multiple robots. To predict the performance of a team, we first measure the neglect tolerance and interface efficiency of the interaction schemes employed by the team. We then describe a method that shows how these measurements can be used to estimate the team's performance. We validate the performance prediction algorithm by comparing predictions to actual results when a user guides three robots in an exploration and goal-finding mission; comparisons are made for various system configurations.","Human robot interaction,
Robotics and automation,
Prediction algorithms,
Robot control,
Control systems,
Jacobian matrices,
Computer science,
Random processes,
Multirobot systems,
Testing"
Project JXTA-C: enabling a Web of things,"The Web, the collection of all devices connected to the Internet, is on the verge of experiencing a massive evolution from a Web of computers to a Web of things as new devices such as phones, beepers, sensors, wearable computers, telemetry sensors, and tracking agents connect to the Internet. The open-source Project JXTA was initiated a year ago to specify a standard set of protocols for ad hoc, pervasive, peer-to-peer computing as a foundation of the upcoming Web of Things. The following paper presents an up-to-date overview of the JXTA protocols and describes the latest implementation of the JXTA protocols in the ""C"" programming language. The paper discusses the overall architecture and trade off made to allow the JXTA-C implementation to run on a wide range of devices including supercomputers, servers, PCs, and memory-constrained embedded devices. The JXTA-C implementation delivers a small and efficient implementation of the JXTA protocols stack allowing the JXTA protocols to be embedded at the system level rather than the Java Virtual Machine (JVM) level for optimized performance and capabilities.","Protocols,
Internet,
Wearable computers,
Wearable sensors,
Telemetry,
Open source software,
Peer to peer computing,
Computer languages,
Computer architecture,
Supercomputers"
Developing fault predictors for evolving software systems,"Over the past several years, we have been developing methods of predicting the fault content of software systems based on measured characteristics of their structural evolution. In previous work, we have shown there is a significant linear relationship between code churn, a synthesized metric, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. We have begun a new investigation of this relationship with a flight software technology development effort at the jet propulsion laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. Second, we have developed a practical framework for automating the measurement of these faults. we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software technology development effort. Our results indicate that the measures of structural attributes of the evolving software system are suitable for forming predictors of the number of faults inserted into software modules during their development. The new fault standard also ensures that the model so developed has greater predictive validity.","Software systems,
Software measurement,
Standards development,
Propulsion,
Laboratories,
Measurement standards,
Software standards,
Predictive models,
Time measurement,
Computer science"
A multiobjective optimization model for exploring multiprocessor mappings of process networks,"In the Sesame framework, we develop a modeling and simulation environment for the efficient design space exploration of heterogeneous embedded systems. Since Sesame recognizes separate application and architecture models within a single system simulation, it needs an explicit mapping step to relate these models for co-simulation. So far in Sesame, the mapping decision as been assumed to be made by an experienced designer, intuitively. However, this assumption is increasingly becoming inappropriate for the following reasons: already the realistic systems are far too complex for making intuitive decisions at an early design stage where the design space is very large. Likely, these systems will get even more complex in the near future. Besides, there exist multiple criteria to consider, like processing times, power consumption and cost of the architecture, which make the decision problem even harder. The mapping decision problem is formulated as a multiobjective combinatorial optimization problem. For a solution approach, an optimization software tool, implementing an evolutionary algorithm from the literature, has been developed to achieve a set of best alternative mapping decisions under multiple criteria. In a case study, we have used our optimization tool to obtain a set of mapping decisions, some of which were further evaluated by the Sesame simulation framework.","Space exploration,
Embedded system,
Application software,
Signal design,
Computer science,
Computational modeling,
Energy consumption,
Costs,
Power system modeling,
Computer architecture"
Subjective evaluation of a seal robot at the National Museum of Science and Technology in Stockholm,"This paper describes research on mental commit robot that seeks a different direction from industrial robot, and that is not so rigidly dependent on objective measures such as accuracy and speed. The main goal of this research is to explore a new area in robotics, with an emphasis on human-robot interaction. In the previous research, we categorized robots into four categories in terms of appearance. Then, we introduced a cat robot and a seal robot, and evaluated them by interviewing many people. The results showed that physical interaction improved subjective evaluation. Moreover, a priori knowledge of a subject has much influence into subjective interpretation and evaluation of mental commit robot. In this paper, 133 subjects evaluated the seal robot, Paro by questionnaires in an exhibition at the National Museum of Science and Technology in Stockholm, Sweden. This paper reports the results of statistical analysis of evaluation data.",
Frequent-pattern based iterative projected clustering,Irrelevant attributes add noise to high dimensional clusters and make traditional clustering techniques inappropriate. Projected clustering algorithms have been proposed to find the clusters in hidden subspaces. We realize the analogy between mining frequent itemsets and discovering the relevant subspace for a given cluster. We propose a methodology for finding projected clusters by mining frequent itemsets and present heuristics that improve its quality. Our techniques are evaluated with synthetic and real data; they are scalable and discover projected clusters accurately.,"Clustering algorithms,
Character generation,
Data mining,
Itemsets,
Partitioning algorithms,
Databases,
Lungs,
Computer science,
Information systems,
Iterative algorithms"
Web-based information retrieval support systems: building research tools for scientists in the new information age,"The concept of Web-based information retrieval support systems (WIRSS) is introduced. The needs for WIRSS are shown by a detailed case study of existing research article indexing and citation analysis systems, such as current content, DBLP, science citation index and CiteSeer. The objective of WIRSS is to build new and effective research tools for scientists to access, explore and use information on the Web, which may lead to improved research productivity and quality.","Information retrieval,
Libraries,
Indexing,
Bibliographies,
Citation analysis,
Humans,
Information resources,
Data mining,
Computer science,
Productivity"
Rate control for JVT video coding scheme with HRD considerations,"The JVT video coding scheme (MPEG-4 AVC/H.264) is a promising technique due to its high coding efficiency. Hypothetical reference decoder (HRD) is a very important part in JVT video coding, which represents a set of normative requirements on bitstream for the purpose of avoiding buffer overflow and underflow. The problem of HRD requirements can be solved by rate control. This paper proposes an effective rate control scheme for JVT video coding with HRD considerations. First, bit allocation with HRD constraints is presented, and second, based on a simple rate distortion model, a single pass rate control is implemented on both frame level and macroblock level. Experimental results show that the proposed rate control algorithm can achieve the target bit rate with very little bit rate or image quality fluctuation, and meanwhile it can well meet the HRD requirements. Furthermore, the proposed algorithm is so simple that it only introduces little computation complexity. Therefore, it can be used in real time video coding.","Video coding,
Decoding,
Bit rate,
Rate-distortion,
MPEG 4 Standard,
Asia,
Computer science,
Buffer overflow,
Image quality,
Fluctuations"
Maintaining line of sight communications networks between planetary rovers,We present an algorithm designed to solve the problem of maintaining communications within a group of robotic explorers. The rovers we consider are equipped with communication hardware that is effective only over a limited range and requires direct line of sight to function. The paper presents the algorithm used to solve this problem and some details of our implementation. We also present the results of an experimental analysis of the algorithm's performance characteristics in a simulated multi-rover environment.,"Communication networks,
Relays,
Clustering algorithms,
Robot kinematics,
Intelligent robots,
Fires,
Mobile communication,
Protocols,
Computer science,
Hardware"
Convergence of knowledge management and e-learning: the GetSmart experience,"The National Science Digital Library (NSDL), launched in December 2002, is emerging as a center of innovation in digital libraries as applied to education. As a part of this extensive project, the GetSmart system was created to apply knowledge management techniques in a learning environment. The design of the system is based on an analysis of learning theory and the information search process. Its key notion is the integration of search tools and curriculum support with concept mapping. More than 100 students at the University of Arizona and Virginia Tech used the system in the fall of 2002. A database of more than one thousand student-prepared concept maps has been collected with more than forty thousand relationships expressed in semantic, graphical, node-link representations. Preliminary analysis of the collected data is revealing interesting knowledge representation patterns.","Convergence,
Knowledge management,
Electronic learning,
Software libraries,
Management information systems,
Computer science,
Technological innovation,
Computer science education,
Information analysis,
Databases"
High Resolution Forward And Inverse Earthquake Modeling on Terascale Computers,"For earthquake simulations to play an important role in the reduction of seismic risk, they must be capable of high resolution and high fidelity. We have developed algorithms and tools for earthquake simulation based on multiresolution hexahedral meshes. We have used this capability to carry out 1 Hz simulations of the 1994 Northridge earthquake in the LA Basin using 100 million grid points. Our wave propagation solver sustains 1.21 teraflop/s for 4 hours on 3000 AlphaServer processors at 80% parallel efficiency. Because of uncertainties in characterizing earthquake source and basin material properties, a critical remaining challenge is to invert for source and material parameter fields for complex 3D basins from records of past earthquakes. Towards this end, we present results for material and source inversion of high-resolution models of basins undergoing antiplane motion using parallel scalable inversion algorithms that overcome many of the difficulties particular to inverse heterogeneous wave propagation problems.","Earthquakes,
Inverse problems,
USA Councils,
Biomedical engineering,
Predictive models,
Biomedical computing,
Computer science,
Computational modeling,
Geology,
Biomedical measurements"
Group strategy proof mechanisms via primal-dual algorithms,"We develop a general method for turning a primal-dual algorithm into a group strategy proof cost-sharing mechanism. We use our method to design approximately budget balanced cost sharing mechanisms for two NP-complete problems: metric facility location, and single source rent-or-buy network design. Both mechanisms are competitive, group strategyproof and recover a constant fraction of the cost. For the facility location game our cost-sharing method recovers a 1/3rd of the total cost, while in the network design game the cost shares pay for a 1/15 fraction of the cost of the solution.","Costs,
Turning,
Design methodology,
NP-complete problem,
IP networks,
Computer science"
Toward a Formalization of Emergence,"Emergence is a concept widely used in the sciences, the arts, and engineering. Some effort has been made to formalize it, but it is used in various contexts with different meanings, and a unified theory of emergence is still distant. The ultimate goal of a theory of emergence should include using emergence to model, design, or predict the behavior of multiagent systems. The author proposes a formal definition of a basic type of emergence using a language-theoretic and grammar systems approach. It is shown which types of phenomena can be modeled in this sense and what the consequences are for other more complex phenomena.","formal approach,
Emergence,
categorization,
multi-agent system,
grammar system"
Querying imprecise data in moving object environments,"In moving object environments it is infeasible for the database tracking the movement of objects to store the exact locations of objects at all times. Typically the location of an object is known with certainty only at the time of the update. The uncertainty in its location increases until the next update. In this environment, it is possible for queries to produce incorrect results based upon old data. However, if the degree of uncertainty is controlled, then the error of the answers to certain queries can be reduced. More generally, query answers can be augmented with probabilistic estimates of the validity of the answer. We study the execution of such probabilistic nearest-neighbor queries. The imprecision in answers to the queries is an inherent property of these applications due to uncertainty in the data, unlike the techniques for approximate nearest-neighbor processing that trade accuracy for performance.","Nearest neighbor searches,
Uncertainty,
Databases,
Computer science,
Tracking,
Error correction,
Monitoring,
Bandwidth,
Batteries,
Delay effects"
Anonymous publish/subscribe in P2P networks,"One of the most important issues to deal with in peer-to-peer networks is how to disseminate information. In this paper, we use a completely new approach to solving the information dissemination problem. Our approach uses the publish/subscribe paradigm. The publish/subscribe method is the most inclusive strategy to establish communication between the information providers (publishers) and the information consumers (subscribers). We give a formal definition of publish/subscribe systems. We then use the publish/subscribe communication paradigm to design deterministic protocols (topic and content-based) for peer-to-peer networks. Our protocols are designed on top of an innovative information dissemination scheme, and can cope with the anonymity and mobility of both publishers and subscribers, weak-connectivity, and polarization, which are some of the characteristics of peer-to-peer networks. Moreover, in our solutions, every node could play a role of both publisher and subscriber The algorithms are designed completely independent of the underlying routing substrates. The key advantage of our protocols is that they are scalable without additional re-organization cost. To the best of our knowledge, this is the first time the content-based subscription has been addressed in peer-to-peer networks.","Intelligent networks,
Peer to peer computing,
Protocols,
Costs,
Telecommunications,
Computer science,
Polarization,
Algorithm design and analysis,
Routing,
Subscriptions"
On the (In)security of the Fiat-Shamir paradigm,"In 1986, Fiat and Shamir proposed a general method for transforming secure 3-round public-coin identification schemes into digital signature schemes. The idea of the transformation was to replace the random message of the verifier in the identification scheme, with the value of some deterministic hash function evaluated on various quantities in the protocol and on the message to be signed. The Fiat-Shamir methodology for producing digital signature schemes quickly gained popularity as it yields efficient and easy to implement digital signature schemes. The most important question however remained open: are the digital signatures produced by the Fiat-Shamir methodology secure? We answer this question negatively. We show that there exist secure 3-round public-coin identification schemes for which the Fiat-Shamir transformation yields insecure digital signature schemes for any hash function used by the transformation. This is in contrast to the work of Pointcheval and Stern which proved that the Fiat-Shamir methodology always produces digital signatures secure against chosen message attack in the ""Random Oracle Model"" - when the hash function is modeled by a random oracle. Among other things, we make new usage of Barak's technique for taking advantage of nonblack-box access to a program, this time in the context of digital signatures.","Digital signatures,
Protocols,
Computer science,
Law,
Legal factors,
Cryptography,
Design methodology,
Computer security,
Forgery,
Cost function"
Automatic reassembly of document fragments via context based statistical models,"Reassembly of fragmented objects from a collection of randomly mixed fragments is a common problem in classical forensics. We address the digital forensic equivalent, i.e., reassembly of document fragments, using statistical modelling tools applied in data compression. We propose a general process model for automatically analyzing a collection fragments to reconstruct the original document by placing the fragments in proper order. Probabilities are assigned to the likelihood that two given fragments are adjacent in the original using context modelling techniques in data compression. The problem of finding the optimal ordering is shown to be equivalent to finding a maximum weight Hamiltonian path in a complete graph. Heuristics are designed and explored and implementation results provided which demonstrate the validity of the proposed technique.","Context modeling,
Scattering,
Digital forensics,
Data compression,
Operating systems,
File systems,
Information science,
Probability,
Failure analysis,
Random media"
Exploring the trade-off between label size and stack depth in MPLS routing,"Multiprotocol label switching or MPLS technology is being increasingly deployed by several of the largest Internet service providers to solve problems such as traffic engineering and to offer IP services like virtual private networks (VPNs). In MPLS, the analysis of the packet (network layer) header is performed just once, and each packet is assigned a stack of labels, which is examined by subsequent routers when making forwarding decisions. Despite the fact that MPLS is becoming widespread on the Internet, we know essentially very little about the performance one can achieve with it, and about the intrinsic trade-offs in its use of resources. In this paper, we undertake a comprehensive study of the label size versus stack depth trade-off for MPLS routing protocols on lines and trees. We show that in addition to LSP tunneling, label stacks can also be used to dramatically reduce the number of labels required for setting up MPLS LSPs in a network. Based on this observation, we develop routing algorithms and prove lower bounds for two basic problems: (1) fixed label routing: given a fixed number of labels, we want to minimize the stack depth, and (2) fixed stack routing: given a bound on the stack depth, we want to minimize the number of labels used. Our simulation results validate our approach, demonstrating that our novel protocols enable MPLS routing on large trees with few labels and small stack sizes. Thus, our MPLS routing algorithms are applicable to a number of practical scenarios involving the provisioning of VPNs and multicast trees.",
Performance degradation in feedback control due to constraints,"In this note, we present a method to characterize the degradation in performance that arises in linear systems due to constraints imposed on the magnitude of the control signal to avoid saturation effects. We do this in the context of cheap control for tracking step signals.","Degradation,
Feedback control,
Performance analysis,
Control systems,
Costs,
Optimal control,
Steady-state,
Linear systems,
Linear feedback control systems,
Computer science"
Dead reckoning in mobile ad hoc networks,"A predictive model-based mobility tracking method, called dead reckoning, is proposed for mobile ad hoc networks. It disseminates both location and movement models of mobile nodes in the network so that every node is able to predict or track the movement of every other node with a very low overhead. This technique is applied to solve the unicast routing problem by modeling link costs using both link lifetime and geographic distance from the destination to the link egress point. This method presents a much superior routing performance compared to either DSR or AODV, two other popular routing protocols, particularly in terms of delivery fraction and routing load.",
"T-trees, vertical partitioning and distributed association rule mining","We consider a technique (DATA-VP) for distributed (and parallel) association rule mining that makes use of a vertical partitioning technique to distribute the input data, amongst processors. The proposed vertical partitioning is facilitated by a novel compressed set enumeration tree data structure (the T-tree), and an associated mining algorithm (Apriori-T), that allows for computationally effective distributed/parallel ARM when compared with existing approaches.",
Applying projection and B-spline to image authentication and remedy,"Many image authentication methods have been proposed, in recent years, to protect image authenticity and integrity, but most have ignored the dangers of counterfeit attack. In this paper, we propose a new image authentication method, using a vector projection technique, for safety and security in digital systems based on a PKI cryptographic framework. In this method, the vector projective square (VPS) pair of each block can be calculated by a vector projection technique, and we prove that an attacker cannot forge a counterfeit image with the same VPS pair as the original image. In addition to addressing counterfeit attacks, we also propose a new remedial algorithm which applying non-uniform B-spline to repair the modified image. This paper, therefore, has five goals: (1) to verify image authentication, (2) to verify the integrity of an image received, (3) to locate any parts that were illegally modified or counterfeited, (4) to provide security against counterfeit attacks and (5) to repair the illegally modified parts.",
Semantic services,,"Search engines,
Web services,
Programming profession,
Humans,
Natural languages,
Computer science,
Web pages,
Protocols,
Microcomputers,
Art"
Continuous ECG monitoring in the management of pre-hospital health emergencies,"For the last five years, a system for the management and coordination of pre-hospital health emergency has been in regular use at the Emergency Coordination Centre of Heraklion, as part of HYGEIAnet, the Regional Health Information Network of Crete. Approximately 20,000 emergency episodes per year are currently being logged in the system. The need for better support of cardiac cases (about 20% of the total) prompted the extension of the system to include the continuous acquisition of 12-lead ECGs and their real-time transmission to the coordination centre for evaluation by an expert. The 12-lead ECG module has been developed and integrated in the existing system with special attention to the provision of an easy user interface so that the extra work required to the ambulance personnel is negligible. The overall system has been carefully tested and is now in daily use.",
Motion planning for a crowd of robots,"Moving a crowd of robots or avatars from their current configurations to some destination area without causing collisions is a challenging motion-planning problem because the high degrees of freedom involved. Two approaches are often used for this type of problems: decoupled and centralized. The tradeoff of these two approaches is that the decoupled approach is considered faster while the centralized approach has the advantage of being complete. In this paper, we propose an efficient centralized planner that is much faster than the traditional randomized planning approaches. This planner uses a hierarchical sphere tree structure to group robots dynamically. By taking advantage of the problem characteristics on independently moving robots, we are able to design a practical planner with the centralized approach when the number of robots is rather large. We use several simulation examples to demonstrate the efficiency and effectiveness of the planner.",
Analytical treatment of randomly birefringent periodically spun fibers,"Spun fibers are increasingly used in telecommunication systems because their polarization-mode dispersion (PMD) is lower than that in unspun fibers. In this paper, we investigate the effects of a periodic spin on the PMD of fibers with randomly varying birefringence. Numerical simulations show that when the spin period is of the same order as or larger than the beat length, the mean differential group delay of a spun fiber depends on the model used for the random birefringence. We then carry out a general theoretical analysis using the second Wai-Menyuk model, which is the only model of fiber birefringence to date that is consistent with polarization optical time domain reflectometry data. Finally, we consider some particular regimes by means of a perturbative approach.","Birefringence,
Optical fiber polarization,
Polarization mode dispersion,
Delay,
Computer science,
Spinning,
Numerical simulation,
Time domain analysis,
Reflectometry,
Bit rate"
Distributed versus compartment models for PET receptor studies,"Although distributed models are generally accepted as being more realistic than compartment models, use of simpler compartment models is pervasive in nuclear medicine applications, particularly in positron emission tomography (PET). Here, we report on comparisons made between distributed and compartment model outputs to address the question of whether differences between them are sufficient to justify distributed models for analysis of PET receptor experiments. For both two- and three-injection experiments, ""data"" sets were obtained by simulation using a distributed model and a wide range of parameter values. Optimal fits of the compartment model output to these ""data"" were achieved with three strategies in which values of different groups of parameter were estimated. Compartment model outputs yielded good fits to all the distributed model outputs and the values of the corresponding parameters were in close agreement. Given the temporal resolution typically available with PET, the use of a distributed model has no advantage over a compartment model for PET receptor quantification.","Positron emission tomography,
Biomedical engineering,
Spatial resolution,
Nuclear medicine,
Heart,
Context modeling,
Parameter estimation,
Data mining,
Lungs,
Blood"
Generic text summarization using local and global properties of sentences,"With the proliferation of text data on the World-Wide Web, the development of methods for automatically summarizing these data becomes more important. Here, we propose a practical approach for extracting the most relevant sentences from the original document to form a summary. The idea of our approach is to exploit both the local and global properties of sentences. The local property can be considered as clusters of significant words within each sentence, while the global property can be though of as relations of all sentences in the document. These two properties are combined to get a single measure reflecting the informativeness of sentences. Experimental results show that our approach compares favorably to a commercial text summarizer.","Data mining,
Supervised learning,
Personal digital assistants,
Deductive databases,
Information retrieval,
Laboratories,
Computer science,
Machinery,
Natural language processing,
Search engines"
Rectangle size bounds and threshold covers in communication complexity,"We investigate the power of the most important lower bound technique in randomized communication complexity, which is based on an evaluation of the maximal size of approximately monochromatic rectangles, with respect to arbitrary distributions on the inputs. While it is known that the 0-error version of this bound is polynomially tight for deterministic communication, nothing in this direction is known for constant error and randomized communication complexity. We first study a one-sided version of this bound and obtain that its value lies between the MA- and AM- complexities of the considered function. Hence the lower bound actually works for a (communication) complexity class between MA/spl cap/co - MA and AM/spl cap/co - AM, and allows to show that the MA-complexity of the disjointness problem is /spl Omega/(/spl radic/n). Following this we consider the conjecture that the lower bound method is polynomially tight for randomized communication complexity. First we disprove a distributional version of this conjecture. Then we give a combinatorial characterization of the value of the lower bound method, in which the optimization over all distributions is absent. This characterization is done by what we call a bounded error uniform threshold cover, and reduces showing tightness of the bound to the construction of an efficient protocol for a specific communication problem. We then study relaxations of bounded error uniform threshold covers, namely approximate majority covers and majority covers, and exhibit exponential separations between them. Each of these covers captures a lower bound method previously used for randomized communication complexity.","Complexity theory,
Protocols,
Polynomials,
Application software,
Mathematics,
Optimization methods,
Computer science,
Very large scale integration,
Upper bound,
Binary decision diagrams"
A new DMA registration strategy for pinning-based high performance networks,"This paper proposes anew memory registration strategy for supporting Remote DMA (RDMA) operations over pinning-based networks, as existing approaches are insufficient for efficiently implementing Global Address Space (GAS) languages. Although existing approaches often maximize bandwidth, they require levels of synchronization that discourage one-sided communication, and can have significant latency costs for small messages. The proposed Firehose algorithm attempts to expose one-sided, zero-copy communication as a common case, while minimizing the number of host-level synchronizations required to support remote memory operations. The basic idea is to reap the performance benefits of a pin-everything approach in the common case (without the drawbacks) and revert to a rendezvous-based approach to handle the uncommon case. In all cases, the algorithm attempts to amortize the cost of synchronization and pinning over multiple remote memory operations, improving performance over rendezvous by avoiding many handshaking messages and the cost of re-pinning recently used pages. Performance results are presented which demonstrate that the cost of two-sided handshaking and memory registration is negligible when the set of remotely referenced memory pages on a given node is smaller than the physical memory (where the entire working set can remain pinned), and for applications with larger working sets the performance degrades gracefully and consistently outperforms conventional approaches.","Costs,
Bandwidth,
Computer science,
Electronic mail,
Delay,
Degradation,
Protection,
Network interfaces,
Libraries,
Hardware"
Why Johnny can't build [portable scientific software],"The title of this article refers to Rudolph Flesch's famous 1955 book, ""Why Johnny Can't Read"", which called attention to a nationwide decline in reading ability. Here, the author wants to talk about another situation in which an important ability is lacking: the ability to create significant, portable scientific software. The author discusses some of the reasons this problem exists and suggests some approaches to solving it that seem promising.","Process control,
Software tools,
Data structures,
Writing,
Libraries,
Unsolicited electronic mail,
Floods"
Detecting spatial outliers with multiple attributes,"A spatial outlier is a spatially referenced object whose non-spatial attribute values are significantly different from the values of its neighborhood. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and useful spatial patterns for further analysis. Previous work in spatial outlier detection focuses on detecting spatial outliers with a single attribute. In the paper, we propose two approaches to discover spatial outliers with multiple attributes. We formulate the multi-attribute spatial outlier detection problem in a general way, provide two effective detection algorithms, and analyze their computation complexity. In addition, using a real-world census data, we demonstrate that our approaches can effectively identify local abnormality in large spatial data sets.","Testing,
Algorithm design and analysis,
Computer science,
Detection algorithms,
Performance analysis,
Biometrics,
Pattern analysis,
Credit cards,
Voting,
Weather forecasting"
"Probabilistic protocols for node discovery in ad-hoc, single broadcast channel networks","The initial state in an ad-hoc network is a collection of nodes that are unaware of each other's presence. The very first step in joining and building the ad-hoc network is, thus, to discover other nodes. This node discovery procedure is a key step in configuring and optimizing the topology of the network. In spite of its relevance, node discovery has not yet been studied in detail and existing protocols are strongly tied to concrete network implementations. In this paper, we propose a model for node discovery that facilitates the analytical treatment of the problem. We concentrate on networks with a single shared broadcast channel. For these networks, we propose a number of protocols that shed light on the problem of node discovery.","Intelligent networks,
Broadcasting,
Ad hoc networks,
Bluetooth,
Interference,
Broadcast technology,
Computer science,
Access protocols,
Frequency,
Collision avoidance"
Optimal nonlinear line-of-flight estimation in positron emission tomography,"The authors consider detection of high-energy photons in positron emission tomography using thick scintillation crystals. Parallax effect and multiple Compton interactions in such crystals significantly reduce the accuracy of conventional detection methods. In order to estimate the photon line of flight based on photomultiplier responses, the authors use asymptotically optimal nonlinear techniques, implemented by feedforward and radial basis function neural networks. Incorporation of information about angles of incidence of photons significantly improves accuracy of estimation. The proposed estimators are fast enough to perform detection, using conventional computers. Monte Carlo simulation results show that their approach significantly outperforms the conventional Anger algorithm.","Positron emission tomography,
Photonic crystals,
Solid scintillation detectors,
Cameras,
Iterative algorithms,
Single photon emission computed tomography,
Feedforward systems,
Neural networks,
Optical arrays,
Arithmetic"
Using access control for secure information flow in a Java-like language,"Access control mechanisms are widely used with the intent of enforcing confidentiality and other policies, but few formal connections have been made between information flow and access control. Java and C# are object-oriented languages that provide fine-grained access control. An access control list specifies local policy by authorizing permissions for principals (code sources) associated with class declarations; a mechanism called stack inspection checks permissions at run time. An example is given to show how this mechanism can be used to achieve confidentiality goals in situations where a single system call serves callers of differing confidentiality levels and dynamic access control prevents release of high information to low callers. A static analysis is given which applies to such examples. The analysis is shown to ensure a noninterference property formalizing confidentiality.","Access control,
Java,
Permission,
Information security,
Computer security,
Information analysis,
Authorization,
Computer science,
Inspection,
Engineering profession"
Evaluation of SCTP multistreaming over wireless/satellite links,"In this paper, we study the impact of multistreaming on the performance of SCTP over satellite networks. We first show that multistreaming results in higher goodput than single streams when the receiver buffer is constrained as in the case of wireless handheld devices. We then demonstrate that the multistreaming feature of SCTP results in reduced buffer requirements at the receiver in the presence of losses in the satellite network. The above advantages makes SCTP an attractive transport protocol for wireless handheld devices.","Satellites,
Transport protocols,
TCPIP,
Handheld computers,
NASA,
Buffer storage,
Computer science,
Computer architecture,
Fault tolerance,
Physical layer"
An analytical inversion of the nonuniformly attenuated Radon transform with variable focal-length fan-beam collimators,"Single photon emission computed tomography (SPECT) is based on the measurement of radiation emitted by a radiotracer injected into the patient. Because of photoelectric absorption and Compton scatter, the gamma photons are attenuated inside the body before arriving at the detector. A quantitative reconstruction must consider the attenuation, which is usually nonuniform. Novikov has derived an explicit inversion formula for the nonuniformly attenuated Radon transform for SPECT reconstruction of parallel-beam collimated projections. In this paper, we extend his research to variable focal-length fan-beam VFF collimator geometry. A ray-driven analytical formula for VFF reconstruction with nonuniform attenuation was derived. As a unified framework, this formula can be used for parallel-beam, fan-beam, and VFF collimators. Its accuracy is demonstrated by computer simulation experiments.","Collimators,
Single photon emission computed tomography,
Attenuation,
Electromagnetic wave absorption,
Electromagnetic scattering,
Particle scattering,
Gamma ray detection,
Gamma ray detectors,
Geometry,
Computer simulation"
Analysis of routing characteristics in the multicast infrastructure,"As the multicast-capable part of the Internet continues to evolve, important questions to ask are whether the protocols are operating correctly, the topology is well connected, and the routes are stable. A critical step in being able to answer these questions is to monitor the traffic and network operation. In this paper, we analyze characteristics of the multicast infrastructure over the last three years using monitoring data collected from several key routers. Specifically, we focus on analyzing two characteristics of the infrastructure: size and stability. The size analysis focuses on counting the number of connected hosts and networks, and analyzing how the size of the infrastructure has changed over past three years. Second, the stability analysis focuses on examining persistence, prevalence, and visibility of routes across the topology. From our analyses, we identify a number of problems with multicast routing and their effect on the connectivity of certain multicast networks. Moreover, we offer insight into the evolution and future of multicast in the Internet.","Multicast protocols,
Monitoring,
Routing protocols,
Network topology,
Stability analysis,
Web and internet services,
Robust stability,
Computer science,
Telecommunication traffic,
Scalability"
Using discriminant analysis for multi-class classification,Discriminant analysis is known to learn discriminative feature transformations. We study its use in multiclass classification problems. The performance is tested on a large collection of benchmark datasets.,"Scattering,
Linear discriminant analysis,
Covariance matrix,
Chromium,
Character generation,
Computer science,
Face recognition,
Benchmark testing,
Machine learning,
Support vector machines"
On-line detection of faults in carry-select adders,,"Fault detection,
Adders,
Circuit faults,
Electrical fault detection,
Arithmetic,
Computer science,
Electronic mail,
Computer architecture,
Circuit testing,
Hardware"
Efficient data parallel algorithms for multidimensional array operations based on the EKMR scheme for distributed memory multicomputers,"Array operations are useful in a large number of important scientific codes, such as molecular dynamics, finite element methods, climate modeling, atmosphere and ocean sciences, etc. In our previous work, we have proposed a scheme of extended Karnaugh map representation (EKMR) for multidimensional array representation. We have shown that sequential multidimensional array operation algorithms based on the EKMR scheme have better performance than those based on the traditional matrix representation (TMR) scheme. Since parallel multidimensional array operations have been an extensively investigated problem, we present efficient data parallel algorithms for multidimensional array operations based on the EKMR scheme for distributed memory multicomputers. In a data parallel programming paradigm, in general, we distribute array elements to processors based on various distribution schemes, do local computation in each processor, and collect computation results from each processor. Based on the row, column, and 2D mesh distribution schemes, we design data parallel algorithms for matrix-matrix addition and matrix-matrix multiplication array operations in both TMR and EKMR schemes for multidimensional arrays. We also design data parallel algorithms for six Fortran 90 array intrinsic functions: All, Maxval, Merge, Pack, Sum, and Cshift. We compare the time of the data distribution, the local computation, and the result collection phases of these array operations based on the TMR and the EKMR schemes. The experimental results show that algorithms based on the EKMR scheme outperform those based on the TMR scheme for all test cases.","Parallel algorithms,
Multidimensional systems,
Distributed computing,
Concurrent computing,
Algorithm design and analysis,
Phased arrays,
Finite element methods,
Atmospheric modeling,
Atmosphere,
Oceans"
How will bioinformatics impact signal processing research?,"Biomedical research once involved building complex theories upon relatively small amounts of experimental data. The field of bioinformatics has posed many computational problems (bioinformatics can be broadly defined as the interface between biology and computational sciences). The field has stimulated synergetic research and development of state-of-the-art techniques in the areas of data mining, statistics, imaging/pattern analysis, and visualization. By applying these techniques to gene and protein sequence information embedded in biological systems. Signal processing (SP) techniques have been applied most everywhere in bioinformatics and will continue to play an important role in the study of biomedical problems. The goal of this article is to demonstrate to the SP community the potential of SP tools in uncovering complex biological phenomena.","Bioinformatics,
Biomedical signal processing,
Biology computing,
Biomedical computing,
Computer interfaces,
Computational biology,
Research and development,
Data mining,
Statistical analysis,
Biomedical imaging"
Mapping and exploration with mobile robots using coverage maps,"Exploration and mapping belongs to the fundamental tasks of mobile robots. In the past, many approaches have used occupancy grid maps to represent the environment during the map building process. Occupancy grids, however, are based on the assumption that each cell is either occupied or free. In this paper we introduce coverage maps as an alternative way of representing the environment of a robot. Coverage maps store for each cell of a given grid a posterior about the amount the corresponding cell is covered by an obstacle. We also present a model that allows us to update coverage maps upon input obtained from proximity sensors. We furthermore describe how to use coverage maps for a decision theoretic approach to exploration. Finally we present experimental results illustrating that coverage maps can be used to efficiently learn highly accurate models even if noisy sensors such as ultrasounds are used.","Mobile robots,
Robot sensing systems,
Vehicles,
Computer science,
Working environment noise,
Ultrasonic imaging,
Image converters,
Image sensors,
Sensor phenomena and characterization,
Inspection"
Tight lower bounds for the distinct elements problem,"We prove strong lower bounds for the space complexity of (/spl epsi/, /spl delta/)-approximating the number of distinct elements F/sub 0/ in a data stream. Let m be the size of the universe from which the stream elements are drawn. We show that any one-pass streaming algorithm for (/spl epsi/, /spl delta/)-approximating F/sub 0/ must use /spl Omega/(1//spl epsi//sup 2/) space when /spl epsi/ = /spl Omega/(m/sup -1/(9 + k)/), for any k > 0, improving upon the known lower bound of /spl Omega/(1//spl epsi/) for this range of /spl epsi/. This lower bound is tight up to a factor of log log m for small /spl epsi/ and log 1//spl epsi/ for large /spl epsi/. Our lower bound is derived from a reduction from the one-way communication complexity of approximating a Boolean function in Euclidean space. The reduction makes use of a low-distortion embedding from an l/sub 2/ to l/sub 1/ norm.",
Tool-assisted unit test selection based on operational violations,"Unit testing, a common step in software development, presents a challenge. When produced manually, unit test suites are often insufficient to identify defects. The main alternative is to use one of a variety of automatic unit test generation tools: these are able to produce and execute a large number of test inputs that extensively exercise the unit under test. However, without a priori specifications, developers need to manually verify the outputs of these test executions, which is generally impractical. To reduce this cost, unit test selection techniques may be used to help select a subset of automatically generated test inputs. Then developers can verify their outputs, equip them with test oracles, and put them into the existing test suite. In this paper, we present the operational violation approach for unit test selection, a black-box approach without requiring a priori specifications. The approach dynamically generates operational abstractions from executions of the existing unit test suite. Any automatically generated tests violating the operational abstractions are identified as candidates for selection. In addition, these operational abstractions can guide test generation tools to produce better tests. To experiment dynamic approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java unit testing tool). An experiment is conducted to assess this approach.",
Cache optimization for embedded processor cores: an analytical approach,"Embedded microprocessor cores are increasingly being used in embedded and mobile devices. The software running on these embedded microprocessor cores is often a priori known, thus, there is an opportunity for customizing the cache subsystem for improved performance. In this work, we propose an efficient algorithm to directly compute cache parameters satisfying desired performance criteria. Our approach avoids simulation and exhaustive exploration, and, instead, relies on an exact algorithmic approach. We demonstrate the feasibility of our algorithm by applying it to a large number of embedded system benchmarks.",
Selectivity estimation for predictive spatio-temporal queries,"We propose a cost model for selectivity estimation of predictive spatio-temporal window queries. Initially, we focus on uniform data proposing formulae that capture both points and rectangles, and any type of object/query mobility combination (i.e., dynamic objects, dynamic queries or both). Then, we apply the model to nonuniform datasets by introducing spatio-temporal histograms, which in addition to the spatial, also consider the velocity distributions during partitioning. The advantages of our techniques are (i) high accuracy (1-2 orders of magnitude lower error than previous techniques), (ii) ability to handle all query types, and (iii) efficient handling of updates.","Histograms,
Vehicle dynamics,
Computer science,
Weather forecasting,
Typhoons,
Costs,
Predictive models,
Error correction,
Database systems,
Air traffic control"
Symbolic protocol analysis with products and Diffie-Hellman exponentiation,"We demonstrate that for any well-defined cryptographic protocol, the symbolic trace reachability problem in the presence of an Abelian operator (e.g., multiplication) can be reduced to solvability of a particular system of quadratic Diophantine equations. This result enables formal analysis of protocols that employ primitives such as Diffie-Hellman exponentiation, products, and xor, with a bounded number of role instances, but without imposing any bounds on the size of terms created by the attacker. In the case of xor, the resulting system of Diophantine equations is decidable. In the case of a general Abelian group, decidability remains an open equation, but our reduction demonstrates that standard mathematical techniques for solving systems of Diophantine equations are sufficient for the discovery of protocol insecurities.",
Barrier slicing and chopping,"One of the critiques on program slicing is that slices presented to the user are hard to understand. This is partly due to bad user interfaces, but mainly related to the problem that slicing 'dumps' the results onto the user without any explanation. We present an approach that can be used to 'filter' slices. This approach basically introduces 'barriers' which are not allowed to be passed during slice computation. An earlier filtering approach is chopping which is also extended to obey such a barrier. The barrier variants of slicing and chopping provide filtering possibilities for smaller slices and better comprehensibility.","User interfaces,
Filtering,
Reachability analysis,
Programming profession,
Computer science,
Data mining,
Data flow computing,
Iterative methods,
Data analysis,
Conferences"
Delay analysis of the IEEE 802.11 DCF,"With the rising popularity of delay-sensitive real-time multimedia applications (video, voice, data) in wireless local area networks (WLANs), it is becoming important to study the delay performance of WLANs. When the medium access control (MAC) protocol is taken into consideration, the access contention delay is a key problem. In this paper, based on a Markov model, we analyze delay performance of the IEEE 802.11 distributed coordination function (DCF). In addition, through extensive simulations, we calculate the delay performance of both basic access and RTS/CTS access mechanism of the 802.11 protocol.","Delay,
Access protocols,
Media Access Protocol,
Performance analysis,
Collision avoidance,
Throughput,
Wireless sensor networks,
Computer science,
Application software,
Physical layer"
DIVERSE: A Framework for Building Extensible and Reconfigurable Device-Independent Virtual Environments and Distributed Asynchronous Simulations,"We present DIVERSE, a highly modular collection of complimentary software packages designed to facilitate the creation of device-independent virtual environments and distributed asynchronous simulations. DIVERSE is free/open source software, containing both end-user programs and C++ application programming interfaces (APIs). DPF is the DIVERSE graphics interface to OpenGL Performer. A program using the DPF API can run without modification on platforms ranging from fully immersive systems such as CAVEs to generic desktop workstations. The DIVERSE toolkit (DTK) contains all the nongraphical components of DIVERSE, such as networking utilities, hardware device access, and navigational techniques. It introduces a software implementation of networks of replicated noncoherent shared memory. It also introduces a method that seamlessly extends hardware drivers into interprocess and Internet hardware services. We will describe the design of DIVERSE and present a specific example of how it is being used to aid researchers.",
Nodal distance algorithm: calculating a phylogenetic tree comparison metric,"Maintaining a phylogenetic relationship repository requires the development of tools that are useful for mining the data stored in the repository. One way to query a database of phylogenetic information would be to compare phylogenetic trees. Because the only existing tree comparison methods are computationally intensive, this is not a reasonable task. Presented here is the nodal distance algorithm which has significantly less computation time than the most widely used comparison method, the partition metric. When the metric is calculated for trees where one species has been repositioned to a distant part of the tree no further computation is required as is needed for the partition metric. The nodal distance algorithm provides a method for comparing large sets of phylogenetic trees in a reasonable amount of time.","Phylogeny,
Databases,
Partitioning algorithms,
Bioinformatics,
Genomics,
Computer science,
Maintenance engineering,
Data engineering,
Data mining,
Proteins"
Object oriented metrics: precision tools and configurable visualisations,"Software metrics are a valuable tool in helping software engineers to develop large, complex software systems. However, it is vital that transparency and precision are maintained at all stages. We contend that without grammars we cannot define metrics rigorously, without transparent and powerful parsing tools we cannot collect data accurately and without flexible configurable visualisation we cannot exploit the full potential of our data. We report the development of JST, a semantic analyser for Java, and show how it is incorporated into our pipeline-based approach to metrics collection and visualisation. We describe a new visualisation, class clusters, which not only demonstrate the data generated by our tools but also illustrate the value of 3D virtual worlds for visualising software metrics.","Software metrics,
Data visualization,
Software engineering,
Software tools,
Software systems,
Java,
XML,
Object oriented modeling,
Software measurement,
Computer science"
Reducing power consumption for high-associativity data caches in embedded processors,"Modern embedded processors use data caches with higher and higher degrees of associativity in order to increase performance. A set-associative data cache consumes a significant fraction of the total power budget in such embedded processors. This paper describes a technique for reducing the D-cache power consumption and shows its impact on power and performance of an embedded processor. The technique utilizes cache line address locality to determine (rather than predict) the cache way prior to the cache access. It thus allows only the desired way to be accessed for both tags and data. The proposed mechanism is shown to reduce the average L1 data cache power consumption when running the MiBench embedded benchmark suite for 8, 16 and 32-way set-associate caches by, respectively, an average of 66%, 72% and 76%. The absolute power savings from this technique increase significantly with associativity. The design has no impact on performance and, given that it does not have mis-prediction penalties, it does not introduce any new non-deterministic behavior in program execution.","Energy consumption,
Delay,
Hardware,
Computer science,
System performance,
Logic,
Instruction sets,
Testing,
Europe"
Importance of SIMD computation reconsidered,"In this paper, SIMD and MIMD solutions for the real-time database management problem of air traffic control are compared. A real-time database system is highly constrained in a multiprocessor and access to the common database must be made to a limited number of data elements at a time. This MIMD database access is contrasted with the comparable SIMD common database access, which can be several hundred times greater. This is true because the SIMD can simultaneously access thousands of pertinent records instead of the limited number in the MIMD. A relatively simple example is given of a problem that has a polynomial time solution using a SIMD but for which a polynomial time solution using a MIMD is normally impossible. The fact that SIMD can support a polynomial time solution for the air traffic control problem but this problem is normally considered to be intractable for multiprocessors argues against the common belief that MIMD have greater power than SIMD. SIMD are more efficient and powerful for some critically important application areas.",
Evolutionary exploration of dynamic swarm behaviour,"In general, it is difficult to make a highly dynamic swarm system follows explicit behaviour patterns. Multiple, simultaneous interactions among a large number of agents make the non-linear relationship between a parameter change and the corresponding effect on global behaviour non-intuitive and, consequently, hard to control. This paper presents breeding experiments of dynamic swarm behaviour patterns using an interactive evolutionary algorithm. Specifically, eight scalar parameters that influence swarm behaviour dynamics in a 3D swarm simulation are bred to produce agents that collectively fly in line, ring, and figure-eight formations. Our initial examples demonstrate that a 'swarm breeding' system can partly eliminate the manual tuning of control parameters and provides a viable approach to design swarm systems through interactive genetic programming.",
Multiple-resource periodic scheduling problem: how much fairness is necessary?,"The Pfair algorithms are optimal for independent periodic real-time tasks executing on a multiple-resource system. However, they incur a high scheduling overhead by making scheduling decisions in every time unit to enforce proportional progress for each task. In this paper, we will propose a novel scheduling algorithm, boundary fair (BF), which makes scheduling decisions and enforces fairness to tasks only at period boundaries. The BF algorithm is also optimal in the sense that it achieves 100% system utilization. Moreover, by making scheduling decisions at period boundaries, BF effectively reduces the number of scheduling points. Theoretically, the BF algorithm has the same complexity as that of the Pfair algorithms. But, in practice, it could reduce the number of scheduling points dramatically (e.g., up to 75% in our experiments) and thus reduce the overall scheduling overhead, which is especially important for online scheduling.","Optimal scheduling,
Scheduling algorithm,
Resource management,
Processor scheduling,
Real time systems,
Computer science,
Contracts"
Weakly-connected dominating sets and sparse spanners in wireless ad hoc networks,"A set S is dominating if each node in the graph G = (V, E) is either in S or adjacent to at least one of the nodes in S. The subgraph weakly induced by S is the graph G' = (V, E') such that each edge in E' has at least one end point in S. The set S is a weakly-connected dominating set (WCDS) of G if S is dominating and G' is connected G' is a sparse spanner if it has linear edges. In this paper, we present two distributed algorithms for finding a WCDS in O(n) time. The first algorithm has an approximation ratio of 5, and requires O(n log n) messages. The second algorithm has a larger approximation ratio, but it requires only O(n) messages. The graph G' generated by the second algorithm forms a sparse spanner with a topological dilation of 3, and a geometric dilation of 6.","Intelligent networks,
Ad hoc networks,
Spine,
Approximation algorithms,
Mobile ad hoc networks,
Distributed algorithms,
Routing,
Broadcasting,
Linear approximation,
Computer science"
On intervals of partial clones of Boolean partial functions,We describe the interval of all partial clones that contain all monotonic idempotent Boolean partial functions as well as the interval of all partial clones that contain all idempotent self-dual Boolean partial functions.,"Cloning,
Mathematics,
Computer science,
Military computing,
Educational institutions"
Cryptanalysis of Shieh-Lin-Yang-Sun signature scheme,"Due to the special requirements of the mobile code system, Shieh et al. (see IEEE Trans. Veh. Technol., vol.49, p.1464-73, July 2000) proposed some multisignature schemes based on a new digital signature scheme with message recovery. One major characteristic of these schemes is to avoid using one-way hash functions and message redundancy schemes. However, this causes some security flaw. An attack is proposed to show that the underlying signature scheme is not secure. To overcome the attack, the message redundancy schemes may be still used.","Digital signatures,
Authentication,
Forgery,
Security,
Cryptography,
Computer science,
Office automation,
Web and internet services,
Public key"
An evaluation of regular path expressions with qualifiers against XML streams,"We present SPEX, a streamed and progressive evaluation of regular path expressions with XPath-like qualifiers against XML streams. SPEX proceeds as follows. An expression is translated in linear time into a network of transducers, most of them having 1-DPDT equivalents. Every stream message is then processed once by the entire network and result fragments are output on the fly. In most practical cases SPEX needs a time linear in the stream size and for transducer stacks a memory quadratic in the stream depth. Experiments with a prototype implementation point to a very good efficiency of the SPEX approach.",
An experimental card game for teaching software engineering,"The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small ""toy"" software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning, and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in realworld software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.","Education,
Software engineering,
Electrical capacitance tomography,
Time factors,
EMTP,
Remotely operated vehicles,
Knowledge engineering,
Computer industry,
On the job training,
Engineering management"
Teaching image-processing programming in Java,"Image processing (IP) can be taught very effectively by complementing the basic lectures with computer laboratories where the participants can actively manipulate and process images. This offering can be made even more attractive by allowing the students to develop their own IP code within a reasonable time frame. A designed system to be as ""student friendly"" as possible is presented. The software is built around ImageJ, a freely available, full-featured, and user-friendly program for image analysis. The students can walk away from the course with an IP system that is operational. Using the ImageAccess interface layer, they can easily program both ImageJ plug-ins and Internet applets. The system that we have described may also appeal to practitioners as it offers simple, full-proof way of developing professional level IP software.","Education,
Java,
Application software,
Laboratories,
Visualization,
Educational institutions,
Mathematics,
Visual effects,
Engineering students,
Software tools"
Relationship between dialogue acts and hot spots in meetings,"We examine the relationship between hot spots (annotated in terms of involvement) and dialogue acts (DAs, annotated in an independent effort) in roughly 32 hours of speech data from naturally-occurring meetings. Results reveal that four independently-motivated involvement categories (non-involved, disagreeing, amused, and other) show statistically significant associations with particular DAs. Further examination shows that involvement is associated with contextual features (such as the speaker or type of meeting), as well as with lexical features (such as utterance length and perplexity). Finally, we found (surprisingly) that perplexities are similar for involved and non-involved utterances. This suggests that it may not be the amount of propositional content, but rather participants' attitudes toward that content, that differentiates hot spots from other regions in a meeting. Overall, these specific correlations, and their relationships to other features, such as perplexity, could provide useful information for the automatic archiving and browsing of natural meetings.","Computer science,
Speech analysis,
Laboratories,
Audio recording,
Video recording,
NASA,
Space technology,
Telephony"
Teaching a software development methodology: the case of extreme programming,"This article focuses on the teaching of software development methodologies. It presents ten principles of teaching such a topic, while examining each from both a pedagogical and an organizational viewpoint. The teaching principles are demonstrated using the methodology of extreme programming (XP).","Education,
Programming,
Computer aided software engineering,
Computer science,
Software engineering,
Systems engineering and theory,
NIST,
Software quality,
Productivity,
Information systems"
Analytical and empirical analysis of countermeasures to traffic analysis attacks,"We study countermeasures to traffic analysis attacks. A common strategy for such countermeasures is link padding. We consider systems where payload traffic is padded so that packets have either constant inter-arrival times or variable inter-arrival times. The adversary applies statistical recognition techniques to detect the payload traffic rates by using statistical measures like sample mean, sample variance, or sample entropy. We evaluate quantitatively the ability of the adversary to make a correct detection and derive closed-form formulas for the detection rate based on analytical models. Extensive experiments were carried out to validate the system performance predicted by the analytical method. Based on the systematic evaluations, we develop design guidelines for the proper configuration of a system in order to minimize the detection rate","Telecommunication traffic,
Traffic control,
Payloads,
Information analysis,
Guidelines,
Cryptography,
Timing,
Computer science,
Information science,
Entropy"
A real-time world model for multi-robot teams with high-latency communication,"In this paper, we present in detail our approach to constructing a world model in a multi-robot team. We introduce two separate world models, namely an individual world model that stores one robot's state, and a shared world model that stores the state of the team. We present procedures to effectively merge information in these two world models in real-time. We overcome the problem of high communication latency by using shared information on an as-needed basis. The success of our world model approach is validated by experimentation in the robot soccer domain. The results show that a team using a world model that incorporates shared information is more successful at tracking a dynamic object in its environment than a team that does not use shared information.","Robot sensing systems,
Hardware,
Delay,
Computer science,
Observability,
Multirobot systems,
Real time systems,
Fuses,
Context modeling,
Intelligent robots"
Adaptive fuzzy regression clustering algorithm for TSK fuzzy modeling,"The TSK type of fuzzy models has attracted a great attention of the fuzzy modeling community due to their good performance in various applications. Some approaches for modeling TSK fuzzy rules have been proposed in the literature. Most of them define their fuzzy subspaces bases based on the idea of training data being close enough instead of having similar functions. In addition, the fuzzy C-regression model (FCRM) clustering algorithm is proposed to construct TSK fuzzy models. However, this approach does not take into account the data distribution. In this paper, a novel TSK fuzzy modeling approach is presented. In this approach, adaptive fuzzy regression clustering (AFRC) algorithm is proposed to simultaneously define fuzzy subspaces and find the parameters in the consequent parts of TSK rules. In addition, the similarity measure is used to reduce the redundant rules in the clustering process. To obtain a more precise model, a gradient descent algorithm is employed. From the simulation results, the proposed TSK fuzzy model approach indeed showed superior performance.","Clustering algorithms,
Training data,
Cities and towns,
Fuzzy systems,
Educational institutions,
Business,
Computer science,
Application software,
Tellurium,
Supervised learning"
An investigation of a tabu assisted hyper-heuristic genetic algorithm,"This paper investigates a tabu assisted genetic algorithm based hyperheuristic (hyperTGA) for personnel scheduling problems. We recently introduced a hyperheuristic genetic algorithm (hyperGA) with an adaptive length chromosome which aims to evolve an ordering of low-level heuristics in order to find good quality solutions to given problems. The addition of a tabu method, the focus of this paper, extends that work. The aim of adding a tabu list to the hyperGA is to indicate the efficiency of each gene within the chromosome. We apply the algorithm to a geographically distributed training staff and course scheduling problem and compare the computational results with our previous hyperGA.","Genetic algorithms,
Biological cells,
Processor scheduling,
Scheduling algorithm,
Robustness,
Hospitals,
Space exploration,
Computer science,
Personnel,
Algorithm design and analysis"
Synthesizing checkers for on-line verification of System-on-Chip designs,"In modern System-on-Chip (SoC) designs verification becomes the major bottleneck. Since by using state-of-the-art techniques complete designs cannot be fully formally verified, it becomes more and more important to check the correct behaviour during operation. This becomes even more significant in systems that are changed during lifetime, like re-configurable systems. In this paper we present a hardware extension that allows to efficiently synthesize checkers and properties that have been used in the verification process. This allows for an on-line verification of SoC designs. For the verification hardware a regular layout is discussed that can easily be synthesized and has a very low area overhead. The on-line check has (nearly) no effect on the delay of the considered chip.","System-on-a-chip,
Hardware,
Circuit synthesis,
Testing,
Computer science,
Art,
Delay effects,
Modems,
Costs,
Circuit simulation"
High-order solution for the electromagnetic scattering by inhomogeneous dielectric bodies,A high-order method of moment solution with quadrature-point-based sampling is presented for the solution of the volume electric field integral equation for the scattering of inhomogeneous dielectric bodies. The proposed scheme efficiently allows for the material profile to be inhomogeneous within a curvilinear cell. It is demonstrated that the method leads to exponential convergence in both the radar cross section (RCS) and the volume current density. It is also demonstrated that the method can be more efficient than surface field integral equation formulations for thin-material scattering.,"Electromagnetic scattering,
Integral equations,
Dielectrics,
Nonhomogeneous media,
Electric fields"
A HMM based semantic analysis framework for sports game event detection,"Video events detection or recognition is one of important tasks in semantic understanding of video content. Sports game video should be considered as a rule-based sequential signal. Therefore, it is reasonable to model sports events using hidden Markov models. In this paper, we present a generic, scalable and multilayer framework based on HMMs, called SG-HMMs (sports game HMMs), for sports game event detection. At the bottom layer of this framework, event HMMs output basic hypotheses based on low-level features. The upper layers are composed of composition HMMs, which add constraints on those hypotheses of the lower layer. Instead of isolated event recognition, the hypotheses at different layers are optimized in a bottom-up manner and the optimal semantics are determined by top-down process. The experimental results on basketball and volleyball videos have demonstrated the effectiveness of the proposed framework for sports game analysis.","Hidden Markov models,
Event detection,
Games,
Data mining,
Cameras,
Speech,
Clustering algorithms,
Computer science,
Asia,
Information retrieval"
Postprocessing decision trees to extract actionable knowledge,"Most data mining algorithms and tools stop at discovered customer models, producing distribution information on customer profiles. Such techniques, when applied to industrial problems such as customer relationship management (CRM), are useful in pointing out customers who are likely attritors and customers who are loyal, but they require human experts to postprocess the mined information manually. Most of the postprocessing techniques have been limited to producing visualization results and interestingness ranking, but they do not directly suggest actions that would lead to an increase the objective function such as profit. Here, we present a novel algorithm that suggest actions to change customers from an undesired status (such as attritors) to a desired one (such as loyal) while maximizing objective function: the expected net profit. We develop these algorithms under resource constraints that are abound in reality. The contribution of the work is in taking the output from an existing mature technique (decision trees, for example), and producing novel, actionable knowledge through automatic postprocessing.","Decision trees,
Data mining,
Computer science,
Customer profiles,
Industrial relations,
Customer relationship management,
Humans,
Heuristic algorithms,
Mining industry,
Visualization"
Toward an environment for comprehending distributed systems,,"Instruments,
Software systems,
Computer architecture,
Data visualization,
Computer languages,
Runtime,
Computer networks,
Distributed computing,
Computer science,
Educational institutions"
Global regulation of a class of uncertain nonlinear systems using output feedback,,"Nonlinear systems,
Output feedback,
Control systems,
Nonlinear control systems,
Time varying systems,
Linear feedback control systems,
State feedback,
Linear systems,
Computer science,
Uncertain systems"
Survey on QoS management of VoIP,"In this paper, we present a survey on management mechanisms used for ensuring the quality of services (QoS) for voice-over IP (VoIP) applications. We first address the motivations of QoS management for VoIP. We then partition the system into two management planes, data plane and control plane, and describe mechanisms in each plane respectively. In data plane, we cover several important techniques including packet classifier, buffer management, scheduling, loss recovery, and error concealment. In control plane, we describe admission control, resource provisioning, traffic engineering, and connection management etc. As we will see, admission control plays a critical role in QoS for VoIP. Thus, we will discuss methodologies used in admission control in detail. Specifically, we examine parameter-based and measurement-based admission control algorithms, and their use in VoIP. Finally, we discuss limitations of current technologies and future research issues in providing QoS to VoIP applications.",
Towards domain independent speaker clustering,"Speaker clustering is a key component in many speech processing applications. We focus on Broadcast News meta data annotation and speaker adaptation. In this setting, speaker clustering consists of identifying who spoke, and when they spoke in a long news broadcast. Speaker clustering is given a set of short audio segments. Ideally, it will discover how many people are speaking in the broadcast, and when they are speaking. The same problem can be transposed to a different domain. In this paper, we present two techniques that do not require a priori training. The speaker clustering is based on information collected solely on encountered test data. They aim at being portable across domains. The first method is based on a Bayesian information criterion (BIC), with single full-covariance Gaussians. It is fairly primitive but effective. The second method, called speaker triangulation, constructs a coordinate system based on conditional likelihoods of the audio segments. Clusters are located in this coordinate system. We are able to achieve state-of-the-art performance on NIST evaluations across different data sets.","Loudspeakers,
NIST,
Broadcasting,
Speech recognition,
Speech processing,
Streaming media,
Laboratories,
Computer science,
Testing,
Bayesian methods"
Ant colony system with local search for Markov random field image segmentation,"In this paper, we propose a new algorithm for image segmentation based on the Markov random field (MRF) and the ant colony optimization (AGO) metaheuristic. The underlying idea is to take advantage from the ACO metaheuristic characteristics and the MRF theory to develop a novel agents-based approach to segment an image. The proposed algorithm is based on a population of simple agents which construct a candidate partition by a relaxation labeling with respect to the contextual constraints. The obtained results show the efficiency of the new algorithm and that it competes with other global stochastic optimization methods like simulated annealing and genetic algorithm.","Markov random fields,
Image segmentation,
Pixel,
Labeling,
Ant colony optimization,
Partitioning algorithms,
Optimization methods,
Stochastic processes,
Computer science,
Computer vision"
Strategies for finding stable paths in mobile wireless ad hoc networks,"In this paper, we introduce statistical methods to estimate the stability of paths in a mobile wireless ad hoc environment. Identifying stable paths helps to reduce control traffic and the number of connection interruptions. By means of simulation, we analyse the stability of paths chosen according to a variety of strategies, including those used by the well-known routing protocols, AODV and DSR, under a variety of different mobility patterns. This offers new insights into the relation between a path's stability and other characteristics and shows that our statistical metrics are able to identify stable paths in a wide range of scenarios.","Intelligent networks,
Ad hoc networks,
Stability,
Routing protocols,
Propagation delay,
Computer science,
Statistical analysis,
Mobile computing,
Traffic control,
Analytical models"
JavaSplit: a runtime for execution of monolithic Java programs on heterogenous collections of commodity workstations,"This paper presents JavaSplit, a portable runtime for distributed execution of multithreaded Java programs. Java-Split transparently distributes threads and objects of an application among the participating nodes. Thus, it gains augmented computational power and increased memory capacity without modifying the Java multithreaded programming conventions. Java-Split works by rewriting the bytecodes of a given parallel application, transforming it into a distributed application that incorporates all the runtime logic. Each runtime node carries out its part of the resulting distributed computation using nothing but its local standard (unmodified) Java virtual machine (JVM). This is unlike previous Java-based distributed runtime systems, which use a specialized JVM or utilize unconventional programming constructs. Since Java-Split is orthogonal to the implementation of a local JVM, it achieves portability across any existing platform and allows each node to locally optimize the performance of its JVM, e.g., via a just-in-time compiler (JIT).","Object oriented programming,
Virtual computers"
Association rules mining for name entity recognition,We propose a new name entity class extraction method based on association rules. We evaluate and compare the performance of our method with the state of the art maximum entropy method. We show that our method consistently yields a higher precision at a competitive level of recall. This result makes our method particularly suitable for tasks whose requirements emphasize the quality rather than the quantity of results.,"Association rules,
Data mining,
Text recognition,
Entropy,
Thesauri,
Dictionaries,
Costs,
Decision trees,
Computer science,
Information retrieval"
Data integration and information exchange for enhanced control and protection of power systems,"One issue that did not get adequate attention regarding control and protection of power systems in the past is the data integration and information exchange. The traditional approaches assume that each function such as protection, control, monitoring, and maintenance are supported by a separate infrastructure of recording instruments and/or controllers for obtaining and processing data. With introduction of the new computer-based equipment for control and protection in the mid eighties, the integration of data and information exchange were possible, but not explored. This paper indicates what are the improvements and benefits that can be obtained by integrating the data and exchanging information among control and protection as well as system-wide monitoring and control functions.",
Multiple path IEEE floating-point fused multiply-add,We propose optimizations for the IEEE floating-point fused multiply-add operation by considering multiple exclusive parallel computation paths in the implementation. For the proposed design we can show a significant performance improvement over conventional implementations. Considering a variable latency implementation allows for further reduction of the average latency.,"Concurrent computing,
Hardware,
Power dissipation,
Computer science,
Optimization methods,
Computer architecture,
Topology,
Added delay,
Propagation constant"
Piecewise C/sup 1/ continuous surface reconstruction of noisy point clouds via local implicit quadric regression,"This paper addresses the problem of surface reconstruction of highly noisy point clouds. The surfaces to be reconstructed are assumed to be 2-manifolds of piecewise C/sup 1/ continuity, with isolated small irregular regions of high curvature, sophisticated local topology or abrupt burst of noise. At each sample point, a quadric field is locally fitted via a modified moving least squares method. These locally fitted quadric fields are then blended together to produce a pseudo-signed distance field using Shepard's method. We introduce a prioritized front growing scheme in the process of local quadrics fitting. Flatter surface areas tend to grow faster. The already fitted regions will subsequently guide the fitting of those irregular regions in their neighborhood.","Surface reconstruction,
Clouds,
Image reconstruction,
Sampling methods,
Surface fitting,
Computer graphics,
Least squares methods,
Computer vision,
Computer science,
Chromium"
QoS-aware multiple spanning tree mechanism over a bridged LAN environment,"Today's emerging traffic is far removed from the traffic trends seen during the early days of Ethernet technology. As a result, the current IEEE 802.1 standards and its extensions to the spanning tree protocol fall short of providing satisfactory quality of service for traffic which has a significant amount of QoS-sensitive multimedia and VoIP traffic. In the current and near-future scenario of campus-wide networks with significantly large layer-2 clusters and numerous virtual LANs (VLANs), we show significant shortcomings of the basic spanning tree and the multiple spanning tree protocols with regard to QoS. We propose a novel, simple, and yet highly effective enhancement to the multiple spanning tree protocol to achieve a high degree of QoS by keeping in perspective the different characteristics of the various traffic types in the Diffserv framework. We discuss the problems of the current standards and present in detail our proposed extension to overcome them. Our simulation results show good improvements in throughput and significant benefits in delay for all classes of traffic to conclusively prove our claims.",
Optimal framework for low bit-rate block coders,"Block coders are among the most common compression tools available for still images and video sequences. Their low computational complexity along with their good performance make them a popular choice for compression of natural images. Yet, at low bit-rates, block coders introduce visually annoying artifacts into the image. One approach that alleviates this problem is to downsample the image, apply the coding algorithm, and interpolate back to the original resolution. In this paper, we consider the use of optimal decimation and interpolation filters in this scheme. We first consider only optimization of the interpolation filter, by formulating the problem as least-squares minimization. We then consider the joint optimization over both the decimation and the interpolation filters, using the variable projection method. The experimental results presented clearly exhibit a significant improvement over other approaches.","Filters,
Image coding,
Interpolation,
Video sequences,
Computational complexity,
Image resolution,
Performance gain,
Computer science,
Optimization methods,
Discrete cosine transforms"
Property-preserving composition of augmented marked graphs that share common resources,"A large automatic system in manufacturing is usually composed of a set of subsystems sharing the usage of some resources. It is a major design issue to prove the liveness, boundedness and reversibility of the composite system. In this paper, the subsystems are modeled as augmented marked graphs. These augmented marked graphs are then composed into a composite system by merging those places representing the same sources. Conditions are provided under which siphons, traps, liveness, boundedness and reversibility of the subsystems are preserved in the composite system.","Merging,
Interconnected systems,
Mathematics,
Computer science,
Computer aided manufacturing,
Manufacturing automation,
Systems engineering and theory,
Resource management,
Manufacturing processes,
Robots"
Algorithms for supporting compiled communication,"We investigate the compiler algorithms to support compiled communication in multiprocessor environments and study the benefits of compiled communication, assuming that the underlying network is an all-optical time-division-multiplexing (TDM) network. We present an experimental compiler, E-SUIF, that supports compiled communication for High Performance Fortran (HPF) like programs on all-optical TDM networks, and describe and evaluate the compiler algorithms used in E-SUIF. We further demonstrate the effectiveness of compiled communication on all-optical TDM networks by comparing the performance of compiled communication with that of a traditional communication method using a number of application programs.",
Appia vs. Cactus: comparing protocol composition frameworks,"The paper presents and compares Appia and Cactus, two frameworks for protocol composition. The comparison is based on the experience gained in implementing a fault-tolerant atomic broadcast service. The paper also gives preliminary performance results, and concludes with a discussion of the most interesting features of the two frameworks, and suggestions for an improved framework.","Protocols,
Middleware,
Java,
Prototypes,
Computer science,
Broadcasting,
Programming profession,
Intersymbol interference,
Concurrent computing,
Computer languages"
Global resource sharing for synthesis of control data flow graphs on FPGAs,"In this paper we discuss the global resource sharing problem during synthesis of control data flow graphs for FPGAs. We first define the Global Resource Sharing (GRS) problem. Then, we introduce the Global Inter Basic Block Resource Sharing (GIBBS) technique to solve the GRS problem. The first tries to minimize the number of connections between modules, the second considers the area gain, the third uses the criticality of operations assigned to resources as a measure for deciding on merging any given pair of resources, the fourth tries to capture common resource chains and overlap those to minimize both area and delay, and the fifth is the combination of these heuristics. While applying resource sharing, we also consider the execution frequency of the basic blocks. Using our techniques we synthesized several CDFGs representing applications from MediaBench suite. Our results show that, we can reduce the total area requirement by 44% on average (up to 59%) while increasing the execution time by 6% on average.","Resource management,
Flow graphs,
Field programmable gate arrays,
Hardware,
Computer science,
Automatic control,
Permission,
Integrated circuit interconnections,
Multiplexing,
Merging"
Discrete multidirectional wavelet bases,"The application of the wavelet transform in image processing is most frequently based on a separable construction. While simple, such an approach is not capable of capturing properly all 2D properties in images. In this paper, a new truly separable multidirectional transform is proposed with a subsampling method based on lattice theory. Applications are possible in many areas of image processing. Some promising improvements are achieved in nonlinear approximation and denoising of images.","Discrete wavelet transforms,
Wavelet transforms,
Lattices,
Discrete transforms,
Filtering,
Image processing,
Noise reduction,
Computational complexity,
Wavelet analysis,
Computer science"
Discovering hierarchical speech features using convolutional non-negative matrix factorization,"Discovering a representation that reflects the structure of a dataset is a first step for many inference and learning methods. This paper aims at finding a hierarchy of localized speech features that can be interpreted as parts. Non-negative matrix factorization (NMF) has been proposed recently for the discovery of parts-based localized additive representations. The author proposes a variant of this method, convolutional NMF, that enforces a particular local connectivity with shared weights. Analysis starts from a spectrogram. The hidden representations produced by convolutional NMF are input to the same analysis method at the next higher level. Repeated application of convolutional NMF yields a sequence of increasingly abstract representations. These speech representations are parts-based, where complex higher-level parts are defined in terms of less complex lower-level ones.","Convolution,
Unsupervised learning,
Independent component analysis,
Feature extraction,
Psychoacoustic models,
Automatic speech recognition,
Analysis of variance,
Pattern recognition,
Mel frequency cepstral coefficient,
Computer science"
An unsupervised learning approach to content-based image retrieval,"""Semantic gap"" is an open challenging problem in content-based image retrieval. It rejects the discrepancy between low-level imagery features used by the retrieval algorithm and high-level concepts required by system users. This paper introduces a novel image retrieval scheme, CLUster-based rEtrieval of images by unsupervised learning (CLUE), to tackle the semantic gap problem. CLUE is built on a hypothesis that images of the same semantics tend to be clustered. It attempts to narrow the semantic gap by retrieving image clusters based on not only the feature similarity of images to the query, but also how images are similar to each other. CLUE has been tested using examples from a database of about 60,000 general-purpose images. Empirical results demonstrate the effectiveness of CLUE.","Unsupervised learning,
Image retrieval,
Content based retrieval,
Image databases,
Feedback,
National electric code,
Information retrieval,
Spatial databases,
Humans,
Computer science"
Attention coupling as a prerequisite for social interaction,"This paper proposes, ""attention coupling"", that is spatio-temporal coordination of each other's attention, as a prerequisite for human-robot social interaction, where the human interactant attributes mental states to the robot, and possibly vice versa. As a realization of attention coupling we implemented on our robots the capability of eye-contact (mutually looking into each other's eyes) and joint attention (looking at a shared target together). Observation of the interaction with human babies/children showed that the robots with the attention coupling capability facilitated in the babies/children social behavior, including showing, giving, and verbal interactions like asking questions.","Robot kinematics,
Human robot interaction,
Pediatrics,
Humanoid robots,
Robot sensing systems,
Eyes,
Cognitive robotics,
Face detection,
Lips,
Cameras"
A real-time RMI framework for the RTSJ,"The Real-Time Specification for Java (RTSJ) provides a platform for the development of real-time applications. However, the RTSJ does not take the distribution requirements of real-time applications into consideration. As distribution in Java is often implemented using Java's Remote Method Invocation (RMI), a real-time version of RMI between RTSJ implementations can provide a platform for writing distributed real-time systems. This paper describes a Real-Time RMI (RT-RMI) framework that supports timely invocation of remote objects. The thread classes defined by the RTSJ are used to provide the client and server threading mechanisms. The memory model of the RTSJ is considered to ensure that threads correctly use memory areas and avoid memory leaks in the absence of the garbage collector. New classes are developed to control the threads used throughout the invocation and to provide new semantics for remote objects that can be invoked in a timely fashion.","Java,
Yarn,
Real time systems,
Runtime,
Application software,
Writing,
Computer science,
Dispatching,
Virtual machining,
Communication standards"
3D modeling of historic sites using range and image data,"Preserving cultural heritage and historic sites is an important problem. These sites are subject to erosion, vandalism, and as long-lived artifacts, they have gone through many phases of construction, damage and repair. It is important to keep an accurate record of these sites using 3-D model building technology as they currently are, so preservationists can track changes, foresee structural problems, and allow a wider audience to ""virtually"" see and tour these sites. Due to the complexity of these sites, building 3-D models is time consuming and difficult, usually involving much manual effort. This paper discusses new methods that can reduce the time to build a model using automatic methods. Examples of these methods are shown in reconstructing a model of the Cathedral of Ste. Pierre in Beauvais, France.","Buildings,
Computer science,
Cultural differences,
Solid modeling,
Stress,
Educational institutions,
Manuals,
Image reconstruction,
Photometry,
Image coding"
Divisibility properties for covering radius of certain cyclic codes,We are presenting a new method to obtain the covering radius of codes and in particular to prove quasi-perfection in codes. Our techniques combine divisibility results of Ax-Katz and Moreno-Moreno as well as coding theoretic methods. We answer a problem posed by Cohen-Honkala-Litsyn-Lobstein in the book covering radius for Bose-Chaudhuri-Hocquenghem (BCH) codes. We also obtain the covering radius for many new classes of codes.,"Equations,
Codes,
Books,
Galois fields,
Polynomials,
Information theory,
Mathematics,
Computer science"
"Constructing a Gazebo: Supporting Teamwork in a Tightly Coupled, Distributed Task in Virtual Reality","Many tasks require teamwork. Team members may work concurrently, but there must be some occasions of coming together. Collaborative virtual environments (CVEs) allow distributed teams to come together across distance to share a task. Studies of CVE systems have tended to focus on the sense of presence or copresence with other people. They have avoided studying close interaction between us-ers, such as the shared manipulation of objects, because CVEs suffer from inherent network delays and often have cumbersome user interfaces. Little is known about the ef-fectiveness of collaboration in tasks requiring various forms of object sharing and, in particular, the concurrent manipu-lation of objects. This paper investigates the effectiveness of supporting teamwork among a geographically distributed group in a task that requires the shared manipulation of objects. To complete the task, users must share objects through con-current manipulation of both the same and distinct at-tributes. The effectiveness of teamwork is measured in terms of time taken to achieve each step, as well as the impression of users. The effect of interface is examined by comparing various combinations of walk-in cubic immersive projection technology (IPT) displays and desktop devices.",
FloodTrail: an efficient file search technique in unstructured peer-to-peer systems,"Searching efficiency is a decisive factor concerning scalability in large-scale peer-to-peer (P2P) file sharing systems. While flooding is the most commonly used and user-performance oriented method to broadcast query across unstructured P2P networks, it generates a large number of redundant messages. Our study shows that more than 70% of messages are redundant using flooding in a moderately connected network, which imposes an increasingly excessive burden on the underlying infrastructure, hindering the growth and scalability of P2P systems. To reduce the use of flooding as well as its associated overhead, we utilize access trails left by a standard flooding, which is a collection of P2P links used by non-redundant messages. Thus the multiple queries following the flooding can be broadcasted along the trail to achieve two goals: (1) The ability of flooding to achieve short response time is maintained; and (2) the cost of a broadcast is minimized. Though the trail can be partially damaged in an ad hoc system with frequent arrivals and departures of peers, we use repeated trail refreshings and additional trail links to make a trail consistently available for query broadcast. We call this trail-based technique FloodTrail. We have evaluated the performance of FloodTrail on P2P systems for Web contents sharing. Simulation results show that FloodTrail could reduce flooding traffic by up to 57%, while maintaining almost the same search coverage as that of flooding.","Peer to peer computing,
Floods,
Broadcasting,
Scalability,
Computer science,
Educational institutions,
Large-scale systems,
Delay,
Costs,
Traffic control"
A new PROMISE algorithm in networks with shared risk link groups,"Shared risk link group (SRLG) has been widely recognized as an important concept in survivable optical networks. The issues of avoiding the so-called ""traps"" in the path determination phase and maximizing bandwidth sharing are more challenging in providing shared SRLG protection than in providing shared path protection without considering SRLG. In this paper, we extend a algorithm for the scheme of protection with multiple segments (PROMISE) to provide efficient SRLG protection. The proposed algorithm uses a novel dynamic programming technology and achieves a higher bandwidth efficiency and lower request blocking probability.","Intelligent networks,
Protection,
Optical fiber networks,
Physical layer,
Bandwidth,
Optical fiber cables,
Optical switches,
Computer science,
Dynamic programming,
Charge carrier processes"
A method for transporting a team of miniature robots,"The Scouts, developed at the University of Minnesota, are miniature robots designed mainly for reconnaissance and surveillance tasks. The Scout's small size and multiple mobility and sensing modes allow it to efficiently navigate and carry out certain missions in indoor environments and on relatively smooth surfaces. However, like many other small-sized robots, its miniature size and limited battery life become a bottleneck when it comes to cover long distances, particularly on rough, outdoor surfaces. In order to address this issue, we present a method to carry the Scouts to and from their main mission locations by adding a larger robot, a Pioneer 2-AT, to the team. The Pioneer carries a team of Scouts in a color marked box that it holds with its grippers. The Scouts jump out of the box, carry out their mission, and autonomously find the box and jump back in.","Robot kinematics,
Robot sensing systems,
Rough surfaces,
Surface roughness,
Surveillance,
Grippers,
Mobile robots,
Silver,
Computer science,
Design engineering"
A framework for Web-based research support systems,"The objective of research support systems (RSS) is to support and improve research, which may be viewed as a counterpart of decision support systems (DSS) for scientists. Web-based RSS (WRSS) assist scientists in the research process on the Web platform. WRSS are based on the assembling, integration, and adaptation of existing computer technology and information systems for the purpose of research support. A framework of WRSS is presented by focusing on research activities and phases, as well as the technology support needed. The emphasis is on the conceptual formulation of WRSS. Different systems are linked to various research activities, and a pool of support sub-systems is established. As an illustrative example, Web-based information retrieval support systems (WIRSS) are discussed. The results of WRSS may lead to new and viable research tools.","Decision support systems,
Computer science,
Information systems,
Information retrieval,
Software systems,
Data analysis,
Assembly systems,
Decision making,
Application software,
Communication system software"
Management and translation of filtering security policies,"Firewalls are essential elements of security policy enforcement in modern networks. However, managing a filtering security policy, especially for enterprise networks, has become complex and error-prone. Filtering rules have to be carefully written and organized in order to correctly implement the security policy and avoid policy anomalies. In this paper, we present a set of techniques and algorithms that provide (1) automatic anomaly discovery for rule conflicts and potential problems in legacy firewalls, (2) anomaly-free policy editing for rule insertion, modification and removal, and (3) concise translation of filtering rules to high-level textual description for user visualization and verification. These techniques significantly simplify the management of any generic firewall policy written as filtering rules, while minimizing network vulnerability due to filtering policy misconfiguration.","Information security,
Filtering algorithms,
Computer security,
Multimedia systems,
Laboratories,
Computer science,
Management information systems,
Computer network management,
Computer errors,
Visualization"
Oblivious AQM and Nash equilibria,"An oblivious active queue management scheme is one which does not differentiate between packets belonging to different flows. In this paper, we study the existence and the quality of Nash equilibria imposed by oblivious AQM schemes on selfish agents. Oblivious AQM schemes are of obvious importance because of the ease of implementation and deployment, and Nash equilibrium offers valuable clues into network performance under noncooperative user behavior. Specifically, we ask the following three questions: 1) do there exist oblivious AQM schemes that impose Nash equilibria on selfish agents? 2) Are the imposed equilibria, if they exist, efficient in terms of the goodput obtained and the drop probability experienced at the equilibrium? 3) How easy is it for selfish users to reach the Nash equilibrium state? We assume that the traffic sources are Poisson but the users can control the average rate. We show that drop-tail and RED do not impose Nash equilibria. We modify RED slightly to obtain an oblivious scheme, VLRED, that imposes a Nash equilibrium, but is not efficient. We then present another AQM policy, EN-AQM, that can impose an efficient Nash equilibrium. Finally, we show that for any oblivious AQM, the Nash equilibrium imposed on selfish agents is highly sensitive as the number of agents increases, thus making it hard for the users to converge to the Nash equilibrium, and motivating the need for equilibria-aware protocols.","Nash equilibrium,
Traffic control,
Intersymbol interference,
Internet,
Robustness,
Transport protocols,
Game theory,
Computer science"
A parametric error analysis of Goldschmidt's division algorithm,"Back in the 60's Goldschmidt presented a variation of Newton-Raphson iterations for division that is well suited for pipelining. The problem in using Goldschmidt's division algorithm is to present an error analysis that enables one to save hardware by using just the right amount of precision for intermediate calculations while still providing correct rounding. Previous implementations relied on combining formal proof methods (that span thousands of lines) with millions of test vectors. These techniques yield correct designs but the analysis is hard to follow and is not quite tight. We present a simple parametric error analysis of Goldschmidt's division algorithm. This analysis sheds more light on the effect of the different parameters on the error. In addition, we derive closed error formulae that allow to determine optimal parameter choices in four practical settings. We apply our analysis to show that a few bits of precision can be saved in the floating-point division (FP-DIV) microarchitecture of the AMD-K7/spl trade/ microprocessor. These reductions in precision apply to the initial approximation and to the lengths of the multiplicands in the multiplier. When translated to cost, the reductions reflect a savings of 10.6% in the overall cost of the FP-DIV microarchitecture.","Error analysis,
Costs,
Microprocessors,
Delay,
Convergence,
Concurrent computing,
Computer science,
Pipeline processing,
Hardware,
Testing"
Output queued switch emulation by a one-cell-internally buffered crossbar switch,"The output queued (OQ) switching architecture shows optimal performance amongst all queuing approaches. However, OQ switches lack scalability due to high memory-bandwidth constraints. An OQ switch can be exactly emulated by a more scalable crossbar switch (i.e., input-queued - IQ - switch) and a small speedup (Chuang, S. et al., 1998). Unfortunately, this result was not of practical use due to the high complexity of the proposed scheduling scheme. A similar result was shown by B. Magill et al. (see Conf. on Commun. Control and Computing, 2002) and was based on the internally buffered crossbar (IBC) switching architecture. While the latter result seems to overcome the complexity issue, the scheduling scheme presented is costly. We extend our previous work (Mhamdi and Hamdi, IEEE ICC'03, vol.3, p.1659-63, 2003) and prove the same result as Magill et al., but with lower hardware requirements.. In particular, we propose a simple scheduling scheme, named modified current arrival first-lowest TTL (time-to-live) first (MCAF-LTF), that does not require a costly time stamping mechanism. Based on the MCAF-LTF, we prove that, with a speedup of just 2, a one-cell-internally buffered crossbar switch can exactly emulate an OQ switch. The reduced complexity of our proposed scheme makes it of high practical value and allows it to be readily implemented in ultra-high capacity networks.","Switches,
Emulation,
Bandwidth,
Throughput,
Packet switching,
Scalability,
Hardware,
Fabrics,
Computer science,
Computer architecture"
Women and gender in the history of computing,,"History,
Time sharing computer systems,
Scholarships,
Information processing,
Engineering profession,
Chemical technology,
Professional societies,
Cultural differences,
Computer industry,
Computer science"
A digital map/GPS based routing and addressing scheme for wireless ad-hoc networks,"As intelligent transportation systems move towards advanced communication systems, it is fair to assume that every vehicle will be equipped with a digital map database, GPS receiver and 802.11 based radio in the near future. Ad-hoc networks by definition are infrastructure-less and are formed on the fly. Mobile nodes in the form of vehicles can effectively form ad-hoc networks. In this paper we present a routing algorithm and an addressing scheme for such a class of ad-hoc networks, which utilizes digital maps and the geographical position of the nodes. It can be shown that, for this class of ad-hoc networks the proposed scheme will outperform traditional MANET protocols.","Global Positioning System,
Ad hoc networks,
Intelligent transportation systems,
Computer science,
Deductive databases,
Intelligent networks,
Intelligent vehicles,
Receivers,
Routing protocols,
Roads"
Offline Arabic text recognition system,"Optical character recognition (OCR) systems provide human-machine interaction and are widely used in many applications. Much research has already been done on the recognition of Latin, Chinese and Japanese characters. Against this background, it has been experienced that only few papers have specifically addressed to the problem of Arabic text recognition and languages using Arabic script like Urdu and Parsi. This is due to the lack of interest in this field and in part due to the complex nature of the Arabic language. This paper presents a technique for the automatic recognition of Arabic printed text using artificial neural networks. The main features of the system are preprocessing of the text, segmentation of the text to individual characters, feature extraction using moment invariant technique and recognition using RBF network.",
Large-scale porous media and wavelet transformations,"Regardless of whether we're interested in extracting oil or gas from an underground reservoir, or wish to study how a contaminant may spread in an aquifer, we must be able to model flow and transport phenomena in large-scale porous media. However, such models can be predictive and, thus, useful only if we can characterize and model the LSPM's morphology.","Large-scale systems,
Permeability,
Testing,
Morphology,
Grid computing,
Predictive models,
Magnetic recording,
Viscosity,
Uncertainty,
Nonhomogeneous media"
Multi-robot task-allocation through vacancy chains,"This paper presents an algorithm for task allocation in groups of homogeneous robots. The algorithm is based on vacancy chains, a resource distribution strategy common in human and animal societies. We define a class of task-allocation problems for which the vacancy chain algorithm is suitable and demonstrate how reinforcement learning can be used to make vacancy chains emerge in a group of behavior-based robots. Experiments in simulation show that the vacancy chain algorithm consistently outperforms random and static task allocation algorithms when individual robots are prone to distractions or breakdowns, or when task priorities change.","Scheduling algorithm,
Resource management,
Humans,
Learning,
Robot sensing systems,
Laboratories,
Embedded system,
Computer science,
Animals,
Electric breakdown"
JXTA performance study,"Project JXTA is the first peer-to-peer application development infrastructure that includes standard protocols and multi-language implementations. Performance and scalability of JXTA are not well understood, despite its widespread usage in research and increasing popularity in the industry. This paper reports on a performance evaluation of the JXTA protocol implementations in Java. The peer startup and pipe performance are evaluated for the JXTA protocol versions 1.0 and 2.0. The results show that the performance of the latter JXTA version has improved for secure message transfer and multicast propagation, but degraded for several other operations.",
Task clustering and scheduling to multiprocessors with duplication,"Optimal task-duplication-based scheduling of tasks represented by a directed acyclic graph (DAG) onto a set of homogenous distributed memory processors, is a strong NP-hard problem. In this paper we present a clustering and scheduling algorithm with time complexity O(v/sup 3/logv), where v is the number of nodes, which is able to generate an optimal schedule for some specific DAG. For arbitrary DAG, the schedule generated is at most two times as the optimal one. Simulation results show that the performance of TCSD is superb compared to those of four renowned algorithms: PY TDS, TCS and CPFD.","Optimal scheduling,
Processor scheduling,
Clustering algorithms,
Scheduling algorithm,
Costs,
Computational efficiency,
Computer science,
NP-hard problem,
Search methods,
Heuristic algorithms"
High-level synthesis of asynchronous systems by data-driven decomposition,"We present a method for decomposing a high-level program description of a circuit into a system of concurrent modules that can each be implemented as asynchronous pre-charge half-buffer pipeline stages (the circuits used in the asynchronous R3000 MIPS microprocessor). We apply it to designing the instruction fetch of an asynchronous 8051 microcontroller, with promising results. We discuss new clustering algorithms that will improve the performance figures further.","High level synthesis,
Pipelines,
Very large scale integration,
Circuit synthesis,
Cogeneration,
Energy efficiency,
Computer science,
Clustering algorithms,
Permission,
Microprocessors"
Formal methods for Dynamic Power Management,"Dynamic Power Management or DPM refers to the problem of judicious application of various low power techniques based on runtime conditions in an embedded system to minimize the total energy consumption. To be effective, often such decisions take into account the operating conditions and the system-level design goals. DPM has been a subject of intense research in the past decade driven by the need for low power in modern embedded devices. We present an overview of the formal methods that have been explored in solving the system-level DPM problem. We show how formal reasoning frameworks can potentially unify apparently disparate DPM techniques.","Energy management,
Power system management,
Permission,
Computer science,
Power engineering and energy,
Energy consumption,
Embedded computing,
Power engineering computing,
Application software,
Runtime"
Concurrent bilateral negotiation in agent systems,"Bi-lateral negotiations represent an important class of encounter in agent-based systems. To this end, this paper develops and evaluates a heuristic model that enables an agent to participate in multiple, concurrent bi-lateral encounters in competitive situations in which there is information uncertainty and deadlines.",
Scheduling directed a-cyclic task graphs on heterogeneous network of workstations to minimize schedule length,"We evaluate the performance of a non-preemptive heuristic algorithm called heterogeneous critical node first (HCNF) that statically schedules directed a-cyclic task graphs on heterogeneous multiprocessor systems to minimize the makespan. Using simulations on real applications and benchmark graphs we show that HCNF outperforms HEFT (heterogeneous earliest finish time) significantly in schedule length ratio, speedup and efficiency. Schedule length ratio is the ratio of the parallel time to the sum of weights of the critical path tasks on the fastest processor. Speedup is the ratio of the sequential execution time to the parallel execution time. Efficiency is the ratio of the speedup to the number of processors.","Workstations,
Processor scheduling,
Dynamic scheduling,
Scheduling algorithm,
Multiprocessing systems,
Heuristic algorithms,
Computational modeling,
Grid computing,
Vehicle dynamics,
Computer science"
Effects of Handling Real Objects and Self-Avatar Fidelity on Cognitive Task Performance and Sense of Presence in Virtual Environments,"Immersive virtual environments (VEs) provide participants with computer-generated environments filled with virtual objects to assist in learning, training, and practicing dangerous and/or expensive tasks. But does having every object being virtual inhibit the interactivity and level of immersion? If participants spend most of their time and cognitive load on learning and adapting to interacting with virtual objects, does this reduce the effectiveness of the VE? We conducted a study that investigated how handling real objects and self-avatar visual fidelity affects performance and sense of presence on a spatial cognitive manual task. We compared participants' performance of a block arrangement task in both a real-space environment and several virtual and hybrid environments. The results showed that manipulating real objects in a VE brings task performance closer to that of real space, compared to manipulating virtual objects. There was no signifi-cant difference in reported sense of presence, regardless of the self-avatar's visual fidelity or the presence of real objects.",
Heuristics for finding concurrent bugs,"This paper presents new heuristics that increase the probability of manifesting concurrent bugs. The heuristics are based on cross-run monitoring. A contended shared variable is chosen and random context switching is performed at accesses to that variable. The relative strength of the new heuristics is analyzed. In comparison to previous works, our heuristics increase the frequency of bug manifestation. In addition, the new heuristics were able to find bugs that previous methods did not discover.","Computer bugs,
Yarn,
Java,
Interleaved codes,
Testing,
Switches,
Computer science,
System recovery,
Instruments,
Engines"
A computational analysis of the Needham-Schroeder-(Lowe) protocol,"We provide the first computational analysis of the well known Needham-Schroeder-(Lowe) protocol. We show that Lowe's attack to the original protocol can naturally be cast to the computational framework. Then we prove that chosen-plaintext security for encryption schemes is not sufficient to ensure soundness of formal proofs with respect to the computational setting, by exhibiting an attack against the corrected version of the protocol implemented using an ElGamal encryption scheme. Our main result is a proof that, when implemented using an encryption scheme that satisfies indistinguishability under chosen-ciphertext attack, the Needham-Schroeder-Lowe protocol is indeed a secure mutual authentication protocol. The technicalities of our proof reveal new insights regarding the relation between formal and computational models for system security.","Security,
Cryptographic protocols,
Public key cryptography,
Computational modeling,
Computer science,
Drives,
Authentication,
Context modeling,
History,
Engineering profession"
Qualms Regarding the Optimality of Cumulative Path Length Control in CSA/CMA-Evolution Strategies,"Cumulative step-size adaptation (CSA) based on path length control is regarded as a robust alternative to the standard mutative self-adaptation technique in evolution strategies (ES), guaranteeing an almost optimal control of the mutation operator. This paper shows that the underlying basic assumption in CSA — the perpendicularity of expected consecutive steps — does not necessarily guarantee optimal progress performance for (μ/μIλ) intermediate recombinative ES","progress rate,
covariance matrix adaptation,
cumulative step-size adaptation,
evolution strategies,
mutative self-adaptation"
Efficient reconstruction of phylogenetic networks with constrained recombination,"A phylogenetic network is a generalization of a phylogenetic tree, allowing structural properties that are not treelike. With the growth of genomic data, much of which does not fit ideal tree models, there is greater need to understand the algorithmics and combinatorics of phylogenetic networks. We consider the problem of determining whether the sequences can be derived on a phylogenetic network where the recombination cycles are node disjoint. In this paper, we call such a phylogenetic network a ""galled-tree"". By more deeply analysing the combinatorial constraints on cycle-disjoint phylogenetic networks, we obtain an efficient algorithm that is guaranteed to be both a necessary and sufficient test for the existence of a galled-tree for the data. If there is a galled-tree, the algorithm constructs one and obtains an implicit representation of all the galled trees for the data, and can create these in linear time for each one. We also note two additional results related to galled trees: first, any set of sequences that can be derived on a galled tree can be derived on a true tree (without recombination cycles), where at most one back mutation is allowed per site; second, the site compatibility problem (which is NP-hard in general) can be solved in linear time for any set of sequences that can be derived on a galled tree. The combinatorial constraints we develop apply (for the most part) to node-disjoint cycles in any phylogenetic network (not just galled-trees), and can be used for example to prove that a given site cannot be on a node-disjoint cycle in any phylogenetic network. Perhaps more important than the specific results about galled-trees, we introduce an approach that can be used to study recombination in phylogenetic networks that go beyond galled-trees.","Phylogeny,
Bioinformatics,
Genomics,
Genetic mutations,
Combinatorial mathematics,
Computer science,
Environmental factors,
Biological system modeling,
Algorithm design and analysis,
Testing"
Shape simplification based on the medial axis transform,"We present a new algorithm for simplifying the shape of 3D objects by manipulating their medial axis transform (MAT). From an unorganized set of boundary points, our algorithm computes the MAT, decomposes the axis into parts, then selectively removes a subset of these parts in order to reduce the complexity of the overall shape. The result is simplified MAT that can be used for a variety of shape operations. In addition, a polygonal surface of the resulting shape can be directly generated from the filtered MAT using a robust surface reconstruction method. The algorithm presented is shown to have a number of advantages over other existing approaches.","Shape,
Surface reconstruction,
Topology,
Computer graphics,
Image reconstruction,
Solid modeling,
Laboratories,
Computer science,
Robustness,
Reconstruction algorithms"
Language modeling and transcription of the TED corpus lectures,"Transcribing lectures is a challenging task, both in acoustic and in language modeling. In this work, we present our first results on the automatic transcription of lectures from the TED corpus, recently released by ELRA and LDC. In particular, we concentrated our effort on language modeling. Baseline acoustic and language models were developed using respectively 8 hours of TED transcripts and various types of texts: conference proceedings, lecture transcripts, and conversational speech transcripts. Then, adaptation of the language model to single speakers was investigated by exploiting different kinds of information: automatic transcripts of the talk, the title of the talk, the abstract and, finally, the paper. In the last case, a 39.2% WER was achieved.","Loudspeakers,
Natural languages,
Acoustic testing,
Maximum likelihood linear regression,
Broadcasting,
Computer science,
Conference proceedings,
Speech recognition,
Content based retrieval,
Software libraries"
An evolutionary approach to generate fuzzy anomaly (attack) signatures,"We describe the generation of fuzzy signatures to detect some cyber attacks. This approach is an enhancement to our previous work, which was based on the principle of negative selection for generating anomaly detectors using genetic algorithms. The present work includes a different genetic representation scheme for evolving efficient fuzzy detectors. To determine the performance of the proposed approach, which is named Evolving Fuzzy Rule Detectors (EFR), experiments were conducted with three different data sets. One data set contains wireless data, generated using network simulator (NS2) while the other two data sets are publicly available (from Lincoln Lab). Results exhibited that the proposed approach outperformed the previous techniques.","Detectors,
Intrusion detection,
Computer science,
Genetic algorithms,
Fuzzy sets,
Character generation,
Artificial immune systems,
Shape,
Computer networks,
Telecommunication traffic"
Merging the CCA component model with the OGSI framework,"The most important recent development in Grid systems is the adoption of the Web Services model as its basic architecture. The result is called the Open Grid Services Architecture (OGSA). This paper describes a component framework for distributed Grid applications that is consistent with that model. The framework, called XCAT, is based on the U.S. Department of Energy Common Component Architecture (CCA) but with an implementation based on the standard Web Services stack. Using this framework, an application programmer can compose an application from a set of distributed components. The result is a set of Web Services that collectively represent the executing application instance. This paper describes the basic architecture of XCAT and the design issues to be considered for a component to serve as both a CCA and Open Grid Service Infrastructure (OGSI) service.","Merging,
Application software,
Web services,
Grid computing,
Component architectures,
Computer architecture,
Pervasive computing,
Java,
Functional programming,
Computer science"
Analogy of incremental program development and constructivist learning,"During software evolution, programmers add new functionalities and release new versions of software. This complicated work involves not only program development but also learning new knowledge. This paper explores an analogy between incremental program development and constructivist learning, and presents a case study that investigates this analogy. Four types of cognitive processes have been identified. They parallel analogous software engineering activities.","Programming profession,
Software engineering,
Computer science,
Video recording,
Collaborative work,
Algorithm design and analysis,
Testing,
Terminology,
Libraries,
Cognitive informatics"
On predicting rare classes with SVM ensembles in scene classification,"Scene classification is an important technique to infer high-level semantic scene categories from low-level visual features. However, in the real world the positive data for many scenes may be rare, which degrades the performance of many classifiers. In this paper, we propose SVM ensembles to address the rare class problem. Various classifier combination strategies are investigated, including majority voting, sum rule, neural network gater and hierarchical SVMs. We also compare our method with two other common approaches for dealing with the rare class problem. Our experimental results show that hierarchical SVMs can achieve significantly better and more stable performance than other strategies, as well as high computational efficiency.","Support vector machines,
Support vector machine classification,
Layout,
Kernel,
Computer vision,
Testing,
Quadratic programming,
Computer science,
Boosting,
Sampling methods"
AES algorithm implementation - an efficient approach for sequential and pipeline architectures,"We present an efficient implementation of the Rijndael cryptographic algorithm on FPGAs, which is a new advanced encryption standard (AES). The implementation of AES has been carried out in both sequential and pipeline architectures and we are able to compare the results as an area time trade-off. In sequential architecture, the design occupies 2744 CLB slices and achieves a throughput of 258.5 Mbit/s and there is no use of extra memory resources like FPGA BRAMS. On the other hand, our pipeline design occupies a total of 2136 CLB slices and achieved a throughput of 2868 Mbit/s. Both designs were realized on the VirtexE family of devices (XCV812). The performance figures achieved by our implementations are not only efficient in terms of throughput but also areas occupied by them are among the most economical reported to date.","Aerospace and Electronic Systems Society,
Pipelines,
Cryptography,
Strontium,
Field programmable gate arrays,
Throughput,
Computer science,
Standards development,
Hardware,
Matrices"
Learning to use ERP technology: a causal model,"The premise of this paper is that the technically successful implementation of a complex IT does not always result in its effective use. Through the analysis of case study data related to an ERP implementation within a public organization, a causal model is proposed. This model, inductively developed, reveals key factors leading to the construct of ""quality of use."" It suggests that the inclusion of factors relating to learning allows to better understand why ""quality of use"" may vary among individual users. More specifically, factors affecting formal and informal training, and their impact on the extent of learning, are emphasized.","Enterprise resource planning,
Packaging,
Management training,
Databases,
Internet,
Automation,
Warehousing,
Information technology,
Educational institutions,
Dictionaries"
Evaluation of ad-hoc routing protocols under a peer-to-peer application,"Mobile ad hoc networks (MANETs) and peer-to-peer (P2P) applications are emerging technologies based on the same paradigm: the peer-to-peer paradigm. Motivated, respectively, by the necessity of executing applications in environments with no previous infra-structure and the demand for applications that share, in a satisfying manner, files through the Internet, MANETs and P2P applications have brought onto themselves some interest from the community. As a characteristic of the distributed model which they follow, such technologies face a difficult task of routing requests in a decentralized environment. In this paper, we conducted a detailed study of a Gnutella-like application running over a mobile ad-hoc network where three different protocols were considered. The results show that each of the protocols analyzed performed well in some scenarios for some metrics yet had drawbacks in others.","Routing protocols,
Peer to peer computing,
Application software,
Ad hoc networks,
Mobile computing,
Mobile ad hoc networks,
Network servers,
Computer networks,
Computer science,
Internet"
A role-based metamodeling approach to specifying design patterns,"Design patterns describe solutions to recurring design problems in the development of software designs. To encourage the use of design patterns, we are investigating tool support for incorporating patterns into UML models. The development of such tools requires patterns to be specified at the metamodel level. Patterns may be specified using roles, where a role is played by model elements. However, the notion of role in the object-oriented community is strictly based on objects, and does not allow the use of the word ""role"" in any other place where the context is not object-based. In this paper, we propose a notion of role that can be used to specify design patterns at the metamodel level. We survey the characteristics of object-based roles and generalize them. Based on the generalized notion of a role define a new notion of a model role which is played by a model element. We illustrate the use of model roles with a specification of a variant of the Observer design pattern.","Metamodeling,
Object oriented modeling,
Unified modeling language,
Computer science,
Software design,
Computer applications,
Application software"
Early Evaluation of the Cray X1,"Oak Ridge National Laboratory installed a 32 processor Cray X1 in March, 2003, and will have a 256 processor system installed by October, 2003. In this paper we describe our initial evaluation of the X1 architecture, focusing on microbenchmarks, kernels, and application codes that highlight the performance characteristics of the X1 architecture and indicate how to use the system most efficiently.","Laboratories,
Computer science,
Mathematics,
Computer architecture,
High performance computing,
Vector processors,
Government,
Computer applications,
Scalability,
Kernel"
WebSOS: protecting web servers from DDoS attacks,,"Protection,
Web server,
Telecommunication traffic,
Computer crime,
Information filtering,
Information filters,
Network servers,
Cities and towns,
Computer science,
Computer architecture"
Design and prototyping a Fast Hadamard Transformer for WCDMA,"In this paper, the design and implementation of a Fast Hadamard Transformer (FHT) on a field programmable gate array (FPGA) is described. Two possible schemes which use 256 and 16 chip input sequences are compared on a Xilinx Virtex-E XCV1000E FPGA. The results indicate that the 16 chip sequence achieves 90% reduction in hardware resources and more than double the maximum frequency of operation as compared to 256 chip sequences. An application of the proposed FHT design used to perform cell search for Wideband Code Division Multiple Access (WCDMA) system is also presented.",
A dynamic anchor-cell assisted paging with an optimal timer for PCS networks,"An intra location area (intra-LA) location update (LU) scheme is proposed to increase paging accuracy for PCS networks. In the proposed scheme, each mobile terminal (MT) has a valid/invalid anchor-cell, which may be dynamically changed. A MT updates its anchor-cell information only when the status is changed, i.e., from the valid to the invalid or vice versa. Therefore, the intra-LA LU cost is minimized. Whenever the MT enters a cell, a timer is set. The MT realizes that the entered cell is its current anchor-cell if the timer expires before leaving the cell. An analytical model is proposed, and the optimal time threshold is derived explicitly.","Personal communication networks,
Costs,
Paging strategies,
Telephone sets,
Analytical models,
Wireless networks,
Land mobile radio cellular systems,
Bandwidth,
Computer science"
Two families of sequence pairs with zero correlation zone,"A family of sequences with zero correlation zone, which is shortly called ZCZ set, can provide CDMA systems without co-channel interference nor influence of multipath. We present two types of ZCZ sets of nonbinary sequence pairs, which achieve the upper bound of family size for length and zero correlation zone. One, which is produced by use of a perfect complementary pair and an orthogonal code, can change zero correlation zone, while the upper bound is kept. The other, which is generated by use of a newly defined orthogonal pair and an orthogonal code, can offer such CDMA system as a binary ZCZ set is used.","Upper bound,
Multiaccess communication,
Interchannel interference,
Frequency synchronization,
Transmitters,
Autocorrelation,
Hardware,
Computer science,
Systems engineering and theory,
Wireless communication"
Automatic speech summarization based on sentence extraction and compaction,"This paper proposes a new automatic speech summarization method having two stages: important sentence extraction and sentence compaction. Relatively important sentences are extracted based on the amount of information and the confidence measures of constituent words, and the set of extracted sentences is compressed by our sentence compaction method. The sentence compaction is performed by selecting a word set that maximizes a summarization score consisting of the amount of information and the confidence measure of each word, the linguistic likelihood of word strings, and the word concatenation probability. The selected words are concatenated to create a summary. Effectiveness of the proposed method was confirmed by summarizing a spontaneous presentation.","Compaction,
Speech recognition,
Data mining,
Concatenated codes,
Broadcasting,
Text recognition,
Laboratories,
Computer science,
Application software,
Pervasive computing"
Large-scale data collection: a coordinated approach,"In this paper we consider the problem of collecting a large amount of data from several different hosts to a single destination in a wide-area network. Often, due to congestion conditions, the paths chosen by the network may have poor throughput. By choosing an alternate route at the application level, we may be able to obtain substantially faster completion time. This data collection problem is a nontrivial one because the issue is not only to avoid congested link(s), but to devise a coordinated transfer schedule which would afford maximum possible utilization of available network resources. In this paper we present an approach for computing coordinated data collection schedules, which can result in significant performance improvements. We make no assumptions about knowledge of the topology of the network or the capacity available on individual links of the network, i.e., we only use end-to-end information. Finally, we also study the shortcomings of this approach in terms of the gap between the theoretical formulation and the resulting data transfers in wide-area networks. In general, our approach can be used for solving arbitrary data movement problems over the Internet. We use the Bistro platform to illustrate one application of our techniques.","Large-scale systems,
Computer science,
Internet,
Educational institutions,
Job shop scheduling,
Processor scheduling,
Data engineering,
Intersymbol interference,
Throughput,
Network topology"
Content management systems and e-learning systems -a symbiosis?,"Content management systems nowadays are used to manage complex publications far more often than some years ago. The basic principles are the separation of structure, content and presentation, an exactly defined workflow management and the management of content in the form of small units, so called assets. This leads to improved quality, better reusability and reduced costs. We focus on similarities of CMS-systems and e-learning systems and the possibility to transfer gained experiences from the field of CMS to e-learning systems. This leads to a set of demands that can be made on e-learning systems. We conclude with the thesis that transferring the principles of content management systems to the world of e-learning will result in better systems with the improved functionality we already know from current CMS.","Content management,
Symbiosis,
Collision mitigation,
Electronic learning,
Asset management,
Computer science,
Engineering management,
Costs,
Books,
Workflow management software"
Efficient design of SVD-based 2-D digital filters using specification symmetry and order-selecting criterion,"Two-dimensional (2-D) digital filters are widely useful in image processing and other 2-D digital signal processing fields, but designing 2-D filters is much more difficult than designing one-dimensional (1-D) ones. This paper provides a new insight into the existing singular value decomposition (SVD)-based design approach in the sense that the SVD-based design can be performed more efficiently by exploiting the symmetry of the given 2-D magnitude specifications. By using the specification symmetry, only half of the 1-D digital filters (subfilters) need to be designed, which significantly simplifies the design process and reduces the computer storage required for 1-D subfilter coefficients. Another novel point of this paper is that an objective criterion is proposed for selecting appropriate subfilter orders in order to reduce the hardware implementation cost. A design example is given to illustrate the effectiveness of the SVD-based design approach by exploiting specification symmetry and new order-selecting criterion.","Digital filters,
Image processing,
Humans,
Circuits,
Singular value decomposition,
Information science,
Digital signal processing,
Process design,
Signal design,
Hardware"
Power adaptive broadcasting with local information in ad hoc networks,"Network wide broadcasting is an energy intensive function. In this paper we propose a new method that performs transmission power adaptations based on information available locally, to reduce the overall energy consumed per broadcast. In most of the prior work on energy efficient broadcasting it is assumed that the originator of the broadcast has global network information (both topology information as well as the geographical distance between nodes). This can be prohibitive in terms of the consumed overhead. In our protocol, each node attempts to tune its transmit power based on local information (of up to two hops from the transmitting node). We perform extensive simulations to evaluate our protocol. Our simulations take into account the possible loss of packets due to collision effects and the additional re-broadcasts that are necessary due to lower power transmissions. We show that our protocol achieves almost the same coverage as other non power-adaptive broadcast schemes hut with a reduction of approximately 40% in terms of the consumed power as compared to a scheme that does not adapt its power.","Broadcasting,
Intelligent networks,
Ad hoc networks,
Protocols,
Energy consumption,
Energy efficiency,
Network topology,
Performance evaluation,
Computer science,
Power engineering and energy"
An improved key management scheme for large dynamic groups using one-way function trees,"To achieve secure multicast communications, key management is one of the most critical problems that should be solved. So far, many multicast key management schemes have been proposed. In 1999, Balenson, McGrew, and Sherman proposed an efficient multicast key management scheme, the BMS scheme, based on one-way function trees. The number of broadcasts for a key updating operation is roughly proportional to the logarithm of group size. Recently, Horng showed that the BMS scheme is vulnerable to the collusion attack under a certain situation. In this paper, we further analyze the necessary conditions for mounting a collusion attack on the BMS scheme. Additionally, we describe an improved version of the BMS scheme. The improved scheme ensures none of the evictee and the new member can collude to get the group key that they should not know without incurring much additional computational overhead to the system.","Multicast communication,
Broadcasting,
Communication system security,
Wireless networks,
Global Positioning System,
Cryptography,
Computer science,
Engineering management,
Unicast,
Routing"
MPIS: maximal-profit item selection with cross-selling considerations,"In the literature of data mining, many different algorithms for association rule mining have been proposed. However, there is relatively little study on how association rules can aid in more specific targets. One of the applications for association rules - maximal-profit item selection with cross-selling effect (MPIS) problem - is investigated. The problem is about selecting a subset of items, which can give the maximal profit with the consideration of cross-selling. We prove that a simple version of this problem is NP-hard. We propose a new approach to the problem with the consideration of the loss rule - a kind of association rule to model the cross-selling effect. We show that the problem can be transformed to a quadratic programming problem. In case quadratic programming is not applicable, we also propose a heuristic approach. Experiments are conducted to show that both of the proposed methods are highly effective and efficient.","Association rules,
Data mining,
Marketing and sales,
Quadratic programming,
Computer science,
Decision making,
History,
Data engineering,
Application software,
Companies"
User-guided reinforcement learning of robot assistive tasks for an intelligent environment,"Autonomous robots hold the possibility of performing a variety of assistive tasks in intelligent environments. However, widespread use of robot assistants in these environments requires ease of use by individuals who are generally not skilled robot operators. In this paper we present a method of training robots that bridges the gap between user programming of a robot and autonomous learning of a robot task. With our approach to variable autonomy, we integrate user commands at varying levels of abstraction into a reinforcement learner to permit faster policy acquisition. We illustrate the ideas using a robot assistant task, that of retrieving medicine for an inhabitant of a smart home.","Intelligent robots,
Learning,
Medical robotics,
Humans,
Smart homes,
Control systems,
User interfaces,
Switches,
Legged locomotion,
Computer science"
The Livny and Plank-Beck Problems: Studies in Data Movement on the Computational Grid,"Over the last few years the Grid Computing research community has become interested in developing data intensive applications for the Grid. These applications face significant challenges because their widely distributed nature makes it difficult to access data with reasonable speed. In order to address this problem, we feel that the Grid community needs to develop and explore data movement challenges that represent problems encountered in these applications. In this paper, we will identify two such problems that we have dubbed the Livny Problem and the Plank-Beck Problem. We will also present data movement scheduling techniques that we have developed to address these problems.","Grid computing,
Computer science,
Application software,
Permission,
Engineering profession,
US Department of Energy,
File servers,
Collaboration,
Dynamic scheduling,
Processor scheduling"
A width-invariant property of curves based on wavelet transform with a novel wavelet function,"This paper is an improvement on the characterization of edges. Using a novel wavelet function, it is proven that the maximum moduli of the wavelet transform (MMWT) of a curve produces two new symmetrical curves on both sides of the original with the same direction. The distance between the two curves is shown to be independent of the width d of the original curve if the scale s of the wavelet transform satisfies s/spl ges/d. This property provides a novel method of obtaining the skeletons of the curves in an image.","Wavelet transforms,
Image edge detection,
Skeleton,
Pattern recognition,
Machine intelligence,
Spline,
Algorithm design and analysis,
Computer science education,
Scientific computing,
Computer applications"
Backoff-based priority schemes for IEEE 802.11,"Backoff-based priority scheme for IEEE 802.11 are achieved by differentiating the minimum backoff window size, the backoff window-increasing factor, and the maximum backoff stage. An analytical model is proposed to study the performance of backoff-based priority schemes in terms of saturation throughput and saturation delay. Simulations are conduced to validate the analytical results. Results of this paper are beneficial in designing good prioritized QoS parameters.","Throughput,
Analytical models,
Performance analysis,
Access protocols,
Multiaccess communication,
Media Access Protocol,
Space stations,
Counting circuits,
Computer science,
Delay"
Dynamic mapping in a heterogeneous environment with tasks having priorities and multiple deadlines,"In a distributed heterogeneous computing system, the resources have different capabilities and tasks have different requirements. To maximize the performance of the system, it is essential to assign resources to tasks (match) and order the execution of tasks on each resource (schedule in a manner that exploits the heterogeneity of the resources and tasks. The mapping (defined as matching and scheduling) of tasks onto machines with varied computational capabilities has been shown, in general, to be an NP-complete problem. Therefore, heuristic techniques to find a near-optimal solution to this mapping problem are required. Dynamic mapping is performed when the arrival of tasks is not known a priori. In the heterogeneous environment considered in this study, tasks arrive randomly, tasks are independent (i.e., no communication among tasks), and tasks have priorities and multiple deadlines. This research proposes, evaluates, and compares eight dynamic heuristics. The performance of the best heuristics is 83% of an upper bound.","Distributed computing,
Processor scheduling,
Resource management,
Production planning,
Computer science,
Educational institutions,
Asia,
Computer science education,
Systems engineering education,
Educational technology"
Optical tracking using projective invariant marker pattern properties,"We describe a novel optical tracker algorithm for the tracking of interaction devices in virtual and augmented reality. The tracker uses invariant properties of marker patterns to efficiently identify and reconstruct the pose of these interaction devices. Since invariant properties are sensitive to noise in the 2D marker positions, an offline training session is used to determine deviations in these properties. These deviations are taken into account when searching for the patterns once the tracker is used.","Optical noise,
Optical sensors,
Optical devices,
Image reconstruction,
Image processing,
Virtual reality,
Computer vision,
Information systems,
Mathematics,
Computer science"
Knowledge is a terrible thing to waste: using inference in discrete-event control problems,"The role of inference is added to the capabilities of decentralized supervisors in a modal logic setting for discrete-event systems. In previous work, decentralized supervisors made control decisions through formal reasoning, using only information obtained from direct observation of a given system. The framework is extended so that when a supervisor cannot make a definitive control decision based on its own knowledge of the system, the supervisor may reason about whether other supervisors have sufficient knowledge to eventually make the correct control decision.","Control systems,
Discrete event systems,
Logic,
Automatic control,
Process control,
Minutes,
Mathematics,
Computer science,
Niobium,
Humans"
QoS evaluation of VoIP end-points,"We evaluate the QoS of a number of VoIP end-points, in terms of mouth-to-ear (M2E) delay, clock skew, silence suppression behavior and robustness to packet loss. Our results show that the M2E delay depends mainly on the receiving end-point. Hardware IP phones, when acting as receivers, usually achieve a low average M2E delay (45-90 ms) under low jitter conditions. Software clients achieve an average M2E delay from 65 ms to over 400 ms, depending on the actual implementation. All tested end-points can compensate for clock skew, although some suffer from occasional playout buffer underflow. Only a few of the tested end-points support silence suppression. We find that these silence detectors have a relatively long hangover time (> 0.5 sec), and they may falsely detect music as silence. All hardware IP phones we tested support some form of packet loss concealment better than silence substitution. The concealment generally works well for two to three consecutive losses at 20 ms packet intervals, but voice will quickly deteriorate beyond that.","Internet telephony,
Quality of service,
Testing,
Clocks,
Hardware,
Detectors,
Delay estimation,
Computer science,
Circuits,
Information technology"
A voting scheme for estimating the synchrony of moving-camera videos,Recovery of dynamic scene properties from multiple videos usually requires the manipulation of synchronous (simultaneously captured) frames. This paper is concerned with the automated determination of this synchrony when the temporal alignment of sequences is unknown. A cost function characterising departure from synchrony is first evolved for the case in which two videos are generated by cameras that may be moving. A novel voting method is then presented for minimising the cost function in the case where the ratio of the cameras' frame rates is unknown. Experimental results indicate this relatively general approach holds promise.,"Voting,
Videos,
Cameras,
Layout,
Synchronization,
Australia,
Cost function,
Computer science,
Cyclic redundancy check,
Signal processing"
On the effectiveness of distributing information among vehicles using inter-vehicle communication,"Recently, intelligent transportation systems (ITS) are becoming an important research topic. One goal of ITS is to distribute information among vehicles in a timely and efficient manner. In the ITS research community, inter-vehicle communications (IVC) is considered in a way that may be able to achieve this goal. An information network built on top of vehicles using IVC can be viewed as a type of mobile ad hoc networks. Although several information and distribution protocols for mobile ad hoc networks have been proposed, how well they can be applied to an IVC network is still poorly studied. For this reason, based on reasonable vehicle traces generated by a microscopic traffic simulator, this paper studies the effectiveness of distributing information among vehicles using multi-hop inter-vehicle communications.","Mobile ad hoc networks,
Intelligent transportation systems,
Road vehicles,
Telecommunication traffic,
Road transportation,
Mobile communication,
Microscopy,
Traffic control,
Computer science,
Automotive engineering"
Synthesis of open reactive systems from scenario-based specifications,"We propose here live sequence charts with a new, game-based semantics to model interactions between the system and its environment. For constructing programs automatically, we give an algorithm to synthesize either a strategy for the system ensuring that the specification is respected, or, if the specification is unimplementable, a strategy for the environment forcing the system to fail.",
Agent chameleons: agent minds and bodies,"Agent design has to date concerned itself with the issues pertaining to a single body embedded in a single environment, whether virtual or real. This paper discusses the notion of an agent capable of migrating between information spaces (physical worlds, virtual reality, and digital information spaces). An architecture is presented that facilitates agent migration and mutation within such environments. This will in turn support agent evolution the ultimate in agent adaptivity.","Genetic mutations,
Virtual environment,
Virtual reality,
Mobile agents,
Terminology,
Computer science,
Educational institutions,
User centered design,
Europe,
Maximum likelihood estimation"
A temporal-logic extension of role-based access control covering dynamic separation of duties,"Security policies play an important role in today's computer systems. We show some severe limitations of the wide-spread standard role-based access control (RBAC) model, namely that object-based dynamic separation of duty as introduced by Nash and Poland cannot be expressed with it. We suggest to overcome these limitations by extending the RBAC model with an execution history. The natural next step is then to add temporal logic for the specification of execution orders. We show that with this, object-based dynamic separation of duty, as well as other policies, can be adequately specified.",
Micro-benchmark level performance comparison of high-speed cluster interconnects,"In this paper we present a comprehensive performance evaluation of three high speed cluster interconnects: Infini-Band, Myrinet and Quadrics. We propose a set of micro-benchmarks to characterize different performance aspects of these interconnects. Our micro-benchmark suite includes not only traditional tests and performance parameters, but also those specifically tailored to the interconnects advanced features such as user-level access for performing communication and remote direct memory access. In order to explore the full communication capability of the interconnects, we have implemented the micro-benchmark suite at the low level messaging layer provided by each interconnect. Our performance results show that all three interconnects achieve low latency, high bandwidth and low host overhead. However, they show quite different performance behaviors when handling completion notification, unbalanced communication patterns and different communication buffer reuse patterns.","Testing,
High performance computing,
Delay,
Bandwidth,
Personal communication networks,
Information science,
Supercomputers,
Performance evaluation,
Computer applications,
Distributed computing"
Teaching software design with open source software,"When an introductory course on software design and testing was revised, it was decided to use open source software tools as the major examples and objects of study. The goal was to expose students to realistic software systems and give them experience dealing with large quantities of code written by other people. Using open source software also has the beneficial effect of ensuring that students are aware of the open source software movement, and opens up opportunities to discuss topics such as software piracy and ethics.","Education,
Software design,
Open source software,
Reverse engineering,
Software engineering,
Ethics,
Software testing,
Educational institutions,
Software maintenance,
Unified modeling language"
Combinatorial optimization of multicast key management,"There are numerous applications that require secure group communication. Much recent attention has been focused on secure multicasting over the Internet. When such systems are required to manage large groups which undergo frequent fluctuations in group membership, the need for efficient encryption key management becomes critical. This paper presents a combinatorial formulation of the multicast key management problem that is applicable not only to the specific problem of multicast key management, but also to the general problem of managing keys for any type of trusted group communication, regardless of the underlying transmission method between group participants. Specifically, we describe exclusion basis systems, show exactly when they exist, and demonstrate that such systems represent improvements over the current binary tree-based key management systems and other related systems.","Cryptography,
Computer science,
Information security,
Internet,
Fluctuations,
Privacy,
Technology management,
Engineering management,
Management information systems,
Business"
Appearance-based minimalistic metric SLAM,"This paper addresses the problem of simultaneous localization and mapping (SLAM) for the case of very small, resource-limited robots which have poor odometry and can typically only carry a single monocular camera. We propose a modification to the standard SLAM algorithm in which the assumption that the robots can obtain metric distance/bearing information to landmarks is relaxed. Instead, the robot registers a distinctive sensor ""signature"", based on its current location, which is used to match robot positions. In our formulation of this non-linear estimation problem, we infer implicit position measurements from an image recognition algorithm. The iterated form of the extended Kalman filter (IEKF) is employed to process all measurements.","Simultaneous localization and mapping,
Robot kinematics,
Robot sensing systems,
Robot vision systems,
Mobile robots,
Cameras,
Position measurement,
Motion estimation,
Machine vision,
Computer science"
HostCast: a new overlay multicasting protocol,"Though the merits of IP-based multicast are undeniable, the deployment of IP multicast has met many difficulties. In the past several years, lots of research works have been done on overlay multicast. In this paper, we propose a new overlay multicast protocol: HostCast. Besides constructing a data delivery tree, HostCast uses a simple and efficient approach to form an overlay mesh for control and maintenance. The mesh can effectively facilitate the overlay multicasting. HostCast improves the reliability of overlay multicast tree and decreases the convergence time as demonstrated by the results obtained via simulation.","Multicast protocols,
Maintenance,
Traffic control,
Routing protocols,
Stability,
Joining processes,
Computer science,
Telecommunication network reliability,
Convergence,
Communication system traffic control"
"Medical video mining for efficient database indexing, management and access","To achieve more efficient video indexing and access, we introduce a video database management framework and strategies for video content structure and events mining. The video shot segmentation and representative frame selection strategy are first utilized to parse the continuous video stream into physical units. Video shot grouping, group merging, and scene clustering schemes are then proposed to organize the video shots into a hierarchical structure using clustered scenes, scenes, groups, and shots, in increasing granularity from top to bottom. Then, audio and video processing techniques are integrated to mine event information, such as dialog, presentation and clinical operation, from the detected scenes. Finally, the acquired video content structure and events are integrated to construct a scalable video skimming tool which can be used to visualize the video content hierarchy and event information for efficient access. Experimental results are also presented to evaluate the performance of the proposed framework and algorithms.","Indexing,
Data mining,
Video compression,
Layout,
Relational databases,
Videoconference,
Information retrieval,
Content management,
Computer science,
Streaming media"
A self-adapting healthcare information infrastructure using mobile computing devices,"Despite recent improvements in the gathering and sharing of patient medical information among healthcare providers, there remains a gap in the electronic medical record infrastructure. Patient data is not available in some situations, either because the infrastructure is inaccessible (as in a natural disaster) or because there is no way to link the patient to the infrastructure (e.g., the patient cannot supply necessary identification information). We describe the Poket Doktor System, an architecture that allows an individual to carry personal electronic medical information on a wireless handheld device such as a smart card, cell phone, or PDA. Medical workers can obtain this information wirelessly using handheld devices, desktop computers, network access points, etc. In this way, patients play an active role in the medical information infrastructure, resulting in a better healthcare delivery system.","Medical services,
Mobile computing,
Medical treatment,
Handheld computers,
History,
Medical tests,
Computer science,
Computer architecture,
Smart cards,
Cellular phones"
Ad hoc networks: a protocol for supporting QoS applications,"A delay-bounded service in wireless ad hoc networks is challenging, as ad hoc networks do not provide any type of guarantees. Several protocols have been proposed to support applications without timing requirements in ad hoc networks, but the increasing demand of QoS applications, in ad hoc wireless environments, requires delay-bound service. The contribution of this paper is to propose a protocol that provides QoS service, by means of timing guarantees, to the supported applications in ad hoc wireless networks.","Ad hoc networks,
Timing,
Wireless application protocol,
Access protocols,
Mobile ad hoc networks,
Wireless networks,
Application software,
Computer science,
Road accidents,
Delay"
Neighborhood signatures for searching P2P networks,"Overlay networks have received a lot of attention due to the recent widespread use of peer-to-peer (P2P) applications such as SETI, Napster, Gnutella, and Morpheus. Through replications at numerous peers, digital content can be distributed or exchanged with high resilience and availability. However, existing P2P applications incur excessive overhead on network traffic. For example, Gnutella, which broadcasts queries to search shared content, suffers from an overwhelming volume of query and reply messages. In this paper, we investigate the issues of trading-off storage space at peers to reduce network overhead. We propose to use signatures for directing searches along selected network paths, and introduce three schemes, namely complete-neighborhood signature (CN), partial-neighborhood superimposed signature (PN-S), and partial-neighborhood appended signature (PN-A), to facilitate efficient searching of shared content in P2P networks. Extensive simulations are conducted to evaluate the performance of our proposal with existing P2P content search methods, including Gnutella, Random Walk, and Local Index. Results show that PN-A gives much better performance at a small storage cost.","Peer to peer computing,
Network servers,
Telecommunication traffic,
Costs,
Web server,
Network topology,
Computer science,
Electronic mail,
Application software,
Resilience"
An indoor geolocation system for wireless LANs,"With the development of wireless local area networks (WLAN), people are interested in developing the location-based services for WLAN users especially in an indoor environment. The core technology of location-based services is the location-sensing technology. In this paper, we present a location-sensing method which is based on the location fingerprinting approach for WLAN in an indoor environment. This paper focuses on the implementation issues of constructing an indoor location-sensing system. These issues include how to increase accuracy of position, how to reduce the efforts of constructing a fingerprint database and what factors influencing the characteristic of the location fingerprint. The experimental results show that our method can achieve a better accuracy.","Wireless LAN,
Local area networks,
Fingerprint recognition,
Databases,
Computer networks,
Information science,
Indoor environments,
Temperature sensors,
Sampling methods,
Pattern recognition"
Improved nearest neighbor based approach to accurate document skew estimation,"The nearest-neighbor based document skew detection methods do not require the presence of a predominant text area, and are not subject to skew angle limitation. However, the accuracy of these methods is not perfect in general. In this paper, we present an improved nearest-neighbor based approach to perform accurate document skew estimation. Size restriction is introduced to the detection of nearest-neighbor pairs. Then the chains with a largest possible number of nearest-neighbor pairs are selected, and their slopes are computed to give the skew angle of document image. Experimental results on various types of documents containing different linguistic scripts and diverse layouts show that the proposed approach has achieved an improved accuracy for estimating document image skew angle and has an advantage of being language independent.","Nearest neighbor searches,
Histograms,
Text analysis,
Image analysis,
Clustering algorithms,
Layout,
Computer science,
Character recognition,
Graphics"
Program evolution with explicit learning,"In genetic programming (GP) and most other evolutionary computing approaches, the knowledge learned during the evolutionary processing is implicitly encoded in the population. A small family of approaches, known as estimation of distribution algorithms, learn this knowledge directly in the form of probability distributions. In this research, we proposed a new approach for program synthesis - program evolution with explicit learning (PEEL), belonging to this family. PEEL learns probability distributions from previous generations and stochastically generates new populations according to this distribution. PEEL is intrinsically different from GP systems because it abandons conventional GP genetic operators and does not maintain population. On the benchmark problems we have studied, this approach shows at least comparable performance to GP.","Genetic programming,
Australia,
Probability distribution,
Computer science,
Educational institutions,
Stochastic systems,
Ant colony optimization,
Stochastic processes"
Adaptive software rejuvenation: degradation model and rejuvenation scheme,,"Degradation,
Adaptive estimation,
Aging,
Software systems,
Adaptation model,
Application software,
Sun,
Computer science,
Context modeling,
Computer bugs"
Device independence and extensibility in gesture recognition,"Gesture recognition techniques often suffer from being highly device-dependent and hard to extend. If a system is trained using data from a specific glove input device, that system is typically unusable with any other input device. The set of gestures that a system is trained to recognize is typically not extensible, without retraining the entire system. We propose a novel gesture recognition framework to address these problems. This framework is based on a multi-layered view of gesture recognition. Only the lowest layer is device dependent, it converts raw sensor values produced by the glove to a glove-independent semantic description of the hand. The higher layers of our framework can be reused across gloves, and are easily extensible to include new gestures. We have experimentally evaluated our framework and found that it yields comparable performance to conventional techniques, while substantiating our claims of device independence and extensibility.","Sensor systems,
Data gloves,
Sensor phenomena and characterization,
Neural networks,
Recurrent neural networks,
Degradation,
Virtual reality,
Jacobian matrices,
Computer science,
Instruments"
Swarm optimization with instinct-driven particles,"In particle swarm optimization (PSO), each particle stores a candidate solution, and stochastically modifies its candidate over time, based on the best solution found by neighboring particles, and based on the best solution found by the particle itself. We present an enhancement of PSO in which each particle's behavior is also influenced by a third component which is meant to represent the particle's innate instinct-level intelligence. The instinct component is a function of the intrinsic ""goodness"" of each dimension of the particle's candidate solution and has similarity to the goodness measure used in ant colony methods. We apply our modified-PSO to several 100-variable 900-clause instances of weighted max-sat, comparing our performance to standard PSO and to the Walk-Sat algorithms. We use an aging scheme in which the weight of a clause increases gradually if it is not satisfied. We find that our modified-PSO produces significant improvements over standard PSO and yields performance comparable to Walk-Sat.","Particle swarm optimization,
Particle measurements,
Computer science,
Space exploration,
Aging,
Computational intelligence,
Insects,
Educational institutions,
Marine animals"
Policy and enforcement in virtual organizations,"Arguably, the main goal of grid computing is to facilitate the creation of virtual organizations (VOs); however, to date, not enough attention has been placed on the policies and mechanisms by which these VOs will operate. The core of the VO-roughly, the responsibility of each physical organization (PO) in the VO to contribute and not unjustly consume resources in achieving the overall goal of the VO - is at best service-level agreements (SLAs) that lack a concrete connection to the underlying grid software and at worst an implicit ""in-spirit"" agreement. Unfulfilled expectations and obligations on the part of each PO can have dire consequences and can ultimately lead to the demise of the VO itself. We identify three general policies regarding resource utilization by which VOs might operate and present the ramifications of each policy on the VO's day-to-day operations and the VO's ability to actually enforce the policy. A prototype implementation of a VO with the ""you-get-what-you-give"" policy is the basis of a concrete cost/benefit analysis of policy enforcement for this type of VO.",
An active traffic splitter architecture for intrusion detection,"Scaling network intrusion detection to high network speeds can be achieved using multiple sensors operating in parallel coupled with a suitable load balancing traffic splitter. This paper examines a splitter architecture that incorporates two methods for improving system performance: the first is the use of early filtering where a portion of the packets is processed on the splitter instead of the sensors. The second is the use of locality buffering, where the splitter reorders packets in a way that improves memory access locality on the sensors. Our experiments suggest that early filtering reduces the number of packets to be processed by 32%, giving a 8% increase in sensor performance, while locality buffers improve sensor performance by about 10%. Combined together, the two methods result in an overall improvement of 20% while the performance of the slowest sensor is improved by 14%.","Intrusion detection,
Filtering,
Telecommunication traffic,
Sensor systems,
Computer architecture,
Traffic control,
Computer science,
Laboratories,
Computational Intelligence Society,
Load management"
The viewing graph,"The problem we study is: given N views and a subset of the (/sub 2//sup N/) interview fundamental matrices, which of the other fundamental matrices can we compute using only the pre-computed fundamental matrices. This has applications in 3D (three-dimensional) reconstruction and when we want to reproject an area of one view on another, or to compute epipolar lines when the correspondence problem is too difficult to compute between every two views. A complete solution using linear algorithms to compute the missing fundamental matrices are given for up to six views. In many cases problems with more than six views can also be handled.","Tensile stress,
Cameras,
Computer vision,
Computer science,
Computational geometry,
Layout,
Three dimensional displays,
Equations,
Computer Society,
Pattern recognition"
Nonlinear finite-element analysis and biomechanical evaluation of the lumbar spine,"A finite-element analysis (FEA) model of an intact lumbar disc-body unit was generated. The vertebral body of the FEA model consisted of a solid tetrahedral core of trabecular bone surrounded by a cortical shell. The disc consisted of an incompressible nucleus surrounded by nonlinear annulus fibers embedded in a solid ground substance. The purpose was to create a FEA model suitable for clinical purposes as fracture assessment, instrumentation with pedicle screws, and bone remodeling. Testing of the FEA model was performed nonlinear for a number of loading conditions, and the results were compared with experimental data from the literature. The results showed good agreement. The formulation of the FEA model can be justified for the tested loading conditions.","Finite element methods,
Spine,
Ligaments,
Material properties,
Solid modeling,
Cancellous bone,
Surgery,
Mesh generation,
Testing,
Stability"
From cognitive psychology to cognitive informatics,"Cognitive informatics is the transdisciplinary study of cognitive and information sciences that investigates into the internal information processing mechanism and processes of the natural intelligence - human brains and minds. This paper presents an extensive literature review on related research in psychology, or cognitive science, informatics, computer science that led to the emergence of cognitive informatics.","Psychology,
Cognitive informatics,
Cognitive science,
Humans,
Information processing,
Computer science,
Artificial intelligence,
Cognition,
Knowledge acquisition,
Software engineering"
Time and energy efficient service discovery in Bluetooth,"Key challenges in wireless mobile ad hoc networks are computational resource constraints, power limitations, and efficient service discovery techniques. The short range radio network technology Bluetooth suffers from long service discovery delays and high power consumption due to necessary connection establishment between discovering and discovered entity. For improving the efficiency of service discovery in Bluetooth networks we propose two new approaches. By leveraging the implicit broadcast capability of the Bluetooth inquiry procedure, we significantly reduce the service discovery time and hence lower the power consumption. Moreover, we enable the integration of resource limited devices which are incapable of taking part in service discovery themselves. Based on a performance evaluation and a comparison with legacy Bluetooth service discovery protocol (SDP), we present the benefits of our new approaches.","Energy efficiency,
Bluetooth,
Energy consumption,
Protocols,
Broadcasting,
Petroleum,
Computer science,
Mobile ad hoc networks,
Personal area networks,
Temperature sensors"
Revealing class structure with concept lattices,,"Lattices,
Application software,
Computer science,
Entropy,
Reverse engineering,
Gas insulated transmission lines,
Visualization,
Java,
Software systems,
Software engineering"
Implementing configuration dependent gaits in a self-reconfigurable robot,In this paper we examine locomotion in the context of self-reconfigurable robots. Self-reconfigurable robots are robots built from many connected modules. A self-reconfigurable robot can change its shape and configuration by changing the way these modules are connected. The focus of this paper is to understand how several locomotion gaits can be represented in such a robot and how the robot can select one of these gaits depending on its configuration. We implement a control system based on role based control in a physical self-reconfigurable robot built from seven modules. In several experiments we successfully demonstrate that when the robot is manually reconfigured from a chain to a quadruped configuration the robot changes gait from a sidewinder snake gait to a quadruped walking gait. We conclude that role based control is a promising central method for controlling locomotion of self-reconfigurable robots.,"Legged locomotion,
Control systems,
Robot control,
Shape control,
Connectors,
Robustness,
Robust control,
Computer science,
Surface fitting,
Planets"
Tracking the optic nervehead in OCT video using dual eigenspaces and an adaptive vascular distribution model,"Optical coherence tomography (OCT) is a new ophthalmic imaging modality generating cross sectional views of the retina. OCT systems are essentially Michelson interferometers that form images in 1.5 s by directing a superluminescent diode (SLD) beam over the retinal surface. Involuntary eye motions frequently cause incorrect locations to be imaged. This motion may leave no obvious artifacts in the scan data and can easily go undetected. For glaucoma monitoring especially, knowing the measurement path, typically a circle concentric with the nerve head, is crucial. The commercially available OCT system displays a near-infrared video of the retina showing the SLD beam. This paper presents a prototype system to detect the nerve head and SLD beam in the video, and report the true scan path relative to the nerve head. Low image contrast and limited resolution make the reliable detection of retinal features difficult. In an adaptive model construction phase, the system directly detects retinal vasculature and the nerve head and incrementally builds a model of the current subject's vascular pattern relative to the optic disk. The nerve head identification is multitiered, using a novel dual eigenspace technique and a geometric comparison of detected vessel positions and nerve head hypotheses. In its operational phase, a correspondence is achieved between the currently detected vasculature and the model. Using subjects not included in training, the system located the optic nerve head to within 5 pixels (0.07 optic disk diameters, an error well below clinical significance) in 99.75% of 2800 video fields. In current clinical practice, motions as large as 1-2 disc diameters may go undetected, so this is a vast improvement.","Adaptive optics,
Head,
Optical interferometry,
Retina,
Superluminescent diodes,
Structural beams,
Geometrical optics,
Phase detection,
Coherence,
Tomography"
Adaptive pipeline structures for speculation control,"Pipelining is a common method for improving the throughput of a system, especially when the majority of the processing is sequential. Unfortunately when the sequentiality is broken, a pipelined system suffers additional delay and, most importantly for this work, energy waste which is roughly proportional to the pipeline depth. Standard pipelines cannot be modified once they are built so their depth is fixed. This paper proposes a method that allows the dynamic adaptation of the structure of an asynchronous pipeline, so that pipeline stages can be merged and split at run-time, allowing greater flexibility. It is based on novel latch controllers that can be configured dynamically as 'normal' or 'collapsed', i.e. keeping their latches permanently transparent. Using these controllers a model of AMULET3 was designed that is capable of changing its pipeline depth dynamically when branches are anticipated, in order to alleviate the energy loss when the branch finally arrives.","Programmable control,
Adaptive control,
Pipeline processing,
Throughput,
Energy consumption,
Energy efficiency,
Synchronization,
Energy resolution,
Costs,
Computer science"
Uncertainty in the output of artificial neural networks,"Analysis of the performance of artificial neural networks (ANNs) is usually based on aggregate results on a population of cases. In this paper, we analyze ANN output corresponding to the individual case. We show variability in the outputs of multiple ANNs that are trained and ""optimized"" from a common set of training cases. We predict this variability from a theoretical standpoint on the basis that multiple ANNs can be optimized to achieve similar overall performance on a population of cases, but produce different outputs for the same individual case because the ANNs use different weights. We use simulations to show that the average standard deviation in the ANN output can be two orders of magnitude higher than the standard deviation in the ANN overall performance measured by the A/sub z/ value. We further show this variability using an example in mammography where the ANNs are used to classify clustered microcalcifications as malignant or benign based on image features extracted from mammograms. This variability in the ANN output is generally not recognized because a trained individual ANN becomes a deterministic model. Recognition of this variability and the deterministic view of the ANN present a fundamental contradiction. The implication of this variability to the classification task warrants additional study.","Uncertainty,
Intelligent networks,
Artificial neural networks,
Cancer,
Stochastic processes,
Measurement standards,
Neural networks,
Computer aided diagnosis,
Biomedical imaging,
Application software"
"An optimization framework for dynamic, distributed real-time systems","The paper presents a model that is useful for developing resource allocation algorithms for distributed real-time systems that operate in dynamic environments. Interesting aspects of the model include dynamic environments, utility and service levels, which provide a means for graceful degradation in resource-constrained situations and support optimization of the allocation of resources. The paper also provides an allocation algorithm that illustrates how to use the model for producing feasible, optimal resource allocations.","Real time systems,
Resource management,
NASA,
Satellites,
Computer networks,
Space technology,
Geoscience,
Computer science,
Sensor arrays,
Instruments"
Network resource allocation and a congestion game: the single link case,"We explore the properties of a congestion game where users of a congested resource anticipate the effect of their actions on the price of the resource. When users are sharing a single resource, we show existence and uniqueness of the Nash equilibrium, and establish that the aggregate utility received by the users is at least 3/4 of the maximum possible aggregate utility. These results form part of a growing literature on the ""price of anarchy,"" i.e., the extent to which selfish behavior affects system efficiency.","Resource management,
Computer aided software engineering,
Aggregates,
Internet,
Environmental economics,
Nash equilibrium,
Peer to peer computing,
Computer science,
Routing,
Telecommunication traffic"
Model checking guarded protocols,"The parameterized model checking problem (PMCP) is to decide whether a temporal property holds for a uniform family of systems, C||U/sup n/, comprised of a control process, C, and finitely, but arbitrarily, many copies of a user process, U, executing concurrently with interleaving semantics. We delineate the decidability/undecidability boundary of the PMCP for all possible systems that arise by letting processes coordinate using different subsets of the following communication primitives: conjunctive Boolean guards, disjunctive Boolean guards, pairwise rendezvous, asynchronous rendezvous and broadcast actions. Our focus is on the following linear time properties: (p1) LTL/spl bsol/X formulae over C; (p2) LTL formulae over C; (p3) regular properties specified as regular automata; and (p4) /spl omega/-regular automata. We also establish a hierarchy based on the relative expressive power of the primitives by showing that disjunctive guards and pairwise rendezvous are equally expressive, in that we can reduce the PMCP for regular and /spl omega/-regular properties for systems with disjunctive guards and pairwise rendezvous are equally expressive, in that we can reduce the PMCP for regular and /spl omega/-regular properties for systems with disjunctive guards to ones with pairwise rendezvous and vise versa, but that each of asynchronous rendezvous and broadcasts is strictly more expressive than pairwise rendezvous (and disjunctive guards). Moreover, for systems with conjunctive guards, we give a simple characterization of the decidability/undecidability boundary of the PMCP by showing that allowing stuttering sensitive properties bridges the gap between decidability (for p1) and undecidability (for p2). A broad framework for modeling snoopy cache protocols is also presented for which the PMCP for p3 is decidable and that can model all snoopy cache protocols given by Culler and Emerson (1988) thereby overcoming the undecidability results.","Protocols,
Broadcasting,
Process control,
Interleaved codes,
Automata,
Communication system control,
Bridges,
Contracts,
Coherence,
Multiprocessing systems"
Politeness theory and computer-mediated communication: a sociolinguistic approach to analyzing relational messages,"This conceptual paper suggests how politeness theory by P. Brown and S. Levinson (1987) - well known in anthropological and linguistic literatures -can contribute to the study of role relations in computer-mediated communication. Politeness, phrasing things so as to show respect and esteem for the face of others, occurs throughout social interchange. The paper reviews politeness theory and enumerates specific linguistic indices of politeness. It then discusses how recognition of the central role of face-work in social interchange can enhance understanding of why and where emotion-work might occur in CMC, how such emotion-work (in the form of politeness) can be reliably observed and quantitatively measured at a linguistic level of analysis, and how the distribution of politeness phenomena is systematically related to variables of interest in CMC research - such as status, cohesion, impersonality, friendship, and communicative efficiency.","Computer mediated communication,
Business communication,
Psychology,
Humans,
Emotion recognition,
Face recognition,
Computer science,
Bandwidth,
Context modeling,
Displays"
Design memories as evolutionary systems: socio-technical architecture and genetics,"A knowledge base consisting of design cases has been structured into an organizational memory, which has been supporting a community of novices to learn information system (IS) for six years. The paper discusses why the organizational learning process, which is taking place, can be described as a genetic system that evolves towards the best quality attainable, and by means of a simulation examines the role of some components of this socio-technical system (i.e. features of the organizational memory, teacher ability and memetic processes in the community) in steering this evolution. Some indications for the design of socio-technical systems supported by design memories are pointed out.","Genetics,
Sociotechnical systems,
Information systems,
Telecommunications,
Learning systems,
Employment,
Production,
Problem-solving,
Decision making,
Organizing"
Scheduling resources in programmable and active networks based on adaptive estimations,"In active and programmable networks, packet processing could be accomplished in the router within the data path. For efficient resource allocation in such networks, the packet scheduling schemes should consider multiple resources such as CPU and memory in addition to the bandwidth to improve overall performance. The inherent unpredictability of processing times of active packets poses a significant challenge in CPU scheduling. It has been identified that unlike bandwidth scheduling, prior estimation of CPU requirements of a packet is very difficult since it is platform dependent and it also depends on processing load at the time of execution and operating system scheduling etc. This paper presents an adaptive solution for estimating the processing requirements of active flows efficiently and accurately. The estimation process has been used in our composite scheduling algorithm called CBCS/sup WFQ/ to estimate processing requirement. The performances of the estimation process for our composite scheduler have been analyzed through simulation works.",
Resilient functions over finite fields,"Resilient functions play an important role in the art of information security. In this correspondence, we discuss the existence, construction, and enumeration of resilient functions over finite fields. We show that, for each finite field GF(q) with q > 3, we can easily construct a large number of (q, n, 1, n - 1) resilient functions, most of which include mixing terms. We give a general structure for (q, m + 1, m, 1) resilient functions, and present an example which is not of this general structure. We prove that (q, m + 2, m, 2) resilient functions exist for any m such that 1 < m < q when q > 2. We prove that (q, m + t, m, t) resilient functions exist for any (m, t) such that 1 < m < q and 2 < t < q when q > 3. By making some simple generalizations of former results, we also provide some new methods for constructing resilient functions.","Galois fields,
Boolean functions,
Cryptography,
Computer science,
Writing,
Arithmetic,
Error correction codes,
Art,
Information security"
Evolutionary search and constraint violations,"The aim of this work is towards a better understanding of the effect of using constraint violations in guiding evolutionary search for nonlinear programming problems. Different penalty functions, based on constraint violations, create different search biases. However, this bias may be eliminated when treating the nonlinear programming problem as a multiobjective task. The different search behaviors are illustrated using a new artificial test function. The effectiveness of the multiobjective approach is also compared with the standard penalty function method on a number of commonly used benchmark problems. It is shown that in practice multiobjective methods are not an efficient or effective approach to constrained evolutionary optimization.","Constraint optimization,
Evolutionary computation,
Benchmark testing,
Genetic programming,
Computer science,
Probability density function,
Pareto optimization"
Bespoke data broadcasting scheme for popular videos,"Bespoke data broadcasting scheme is the theme of this paper. For employing lower bandwidth, the bespoke scheme requires less local storage, although its waiting time is on the higher side when compared with the harmonic scheme. Specifically, for two video channels, its performance for user's waiting time is about 22% worse whereas for local storage, it performs about 41% better. Further, this scheme initially behaves very similar to the staircase scheme but as the bandwidth is increased, its performance for waiting time improves considerably and it converges to that of the harmonic scheme.","Job production systems,
Broadcasting,
Videos,
Bandwidth,
Multimedia communication,
Buffer storage,
India Council,
Computer science education,
Computer science"
Barriers in using technology for education in developing countries,"The World Conference on Education for All, held in Jomtien (Thailand) in 1990 highlighted the timely significance of catering to the learning needs of all children, youth and adults, who have been excluded and unreached by the existing system of formal and nonformal education. ICTs can support education through various ways. We deal with the barriers in using technology for education in developing countries. The most notable barriers to the use of ICT in education are allocation of proper funds, level of teacher knowledge, government policies, and the gap between the various sections of society. Various recommendations such as the use of FM radios, television lessons, teleconferencing and continous monitoring of ICT projects are suggested to overcome these problems.",
The blue-c distributed scene graph,"We present a distributed scene graph architecture for use in the blue-c, a novel collaborative immersive virtual environment. We extend the widely used OpenGL Performer toolkit to provide a distributed scene graph maintaining full synchronization down to vertex and texel level. We propose a synchronization scheme including customizable, relaxed locking mechanisms. We demonstrate the functionality of our toolkit with two prototype applications in our high-performance virtual reality and visual simulation environment.","Layout,
Virtual reality,
Rendering (computer graphics),
Virtual environment,
Collaboration,
Video sharing,
Identity management systems,
Computer graphics,
Computer science,
Computer architecture"
Evolutionary fault recovery in a Virtex FPGA using a representation that incorporates routing,"Most evolutionary approaches to fault recovery in FPGA focus on evolving alternative logic configurations as opposed to evolving the intra-cell routing. Since the majority of transistors in a typical FPGA are dedicated to interconnect, nearly 80% according to one estimate, evolutionary fault-recovery systems should benefit by accommodating routing. In this paper, we propose an evolutionary fault-recovery system employing a genetic representation that takes into account both logic and routing configurations. Experiments were run using a software model of the Xilinx Virtex FPGA. We report that using four Virtex combinational logic blocks, we were able to evolve a 100% accurate quadrature decoder finite state machine in the presence of a stuck-at-zero fault. Evolutionary experiments with the hardware in the loop have begun and we discuss the preliminary results.","Field programmable gate arrays,
Routing,
Logic,
Redundancy,
Probes,
Space technology,
NASA,
Postal services,
Computer science,
Genetics"
A coded visual marker for video tracking system based on structured image analysis,"This paper proposes a new kind of visual marker for a wearable computer based augmented reality (AR) system. Various marker systems have already been developed for using with an AR system. However, the improvement is needed to use these marker systems on a wearable AR system in terms of the amount of information or the width of the recognizable area. The main feature of our visual marker is a large amount of information that it can possess. Our marker consists of 32 bits data area with error detection code. This feature enables a wearable AR system to be used in various ways.","Image analysis,
Wearable computers,
Augmented reality,
Space technology,
Parity check codes,
Labeling,
Pixel,
Information science,
Computer errors,
Thyristors"
An agent-based architecture for an adaptive human-robot interface,"This paper describes an innovative agent-based architecture for mixed-initiative interaction between a human and a robot that interacts via a graphical user interface (GUI). Mixed-initiative interaction typically refers to a flexible interaction strategy between a human and a computer to contribute what is best-suited at the most appropriate time (Allen, 1999). In this paper, we extend this concept to human-robot interaction (HRI). When compared to pure human-computer interaction, HRIs encounter additional difficulty, as the user must assess the situation at the robot's remote location via limited sensory feedback. We propose an agent-based adaptive human-robot interface for mixed-initiative interaction to address this challenge. The proposed adaptive user interface (UI) architecture provides a platform for developing various agents that control robots and user interface components (UICs). Such components permit the human and the robot to communicate mission-relevant information.","Human robot interaction,
Supervisory control,
User interfaces,
Intelligent robots,
Robot sensing systems,
Graphical user interfaces,
Intelligent systems,
Computer architecture,
Programmable control,
Adaptive control"
Maximal matching scheduling is good enough,"In high-speed switches the input queued (IQ) architecture is very popular due to its low memory-bandwidth requirement compared to the output queued (OQ) switch architecture, which is extremely desirable in terms of performance but requires very high memory-bandwidth. In the past decade researchers and industry people have been trying hard to find good scheduling algorithm for IQ switches. The two main performance criteria for a scheduling algorithm are: (i) throughput, and (ii) delay. There has been a lot of research done to find throughput of scheduling algorithms, but a little has been known about delay performance of algorithms. This paper mainly studies the delay properties of a class of scheduling algorithms known as maximal matching algorithms. It has been known that maximum weight matching (MWM) scheduling algorithm provides the maximum possible throughput, also denoted as 100% throughput [Tassiulas, L., et al., Dec. 1993], [McKneown, N., et al., March 1996], [Dai, J., et al., March 2000]. The delay bounds for MWM algorithm, and a suite of approximations of MWM algorithm, are known under Bernoulli i.i.d. traffic. Unfortunately there are two problems: (i) MWM and its approximations are not implementable, and (ii) delay bounds are very weak compared to the known theoretical lower bounds that can be obtained in terms of performance of an OQ switch. On the other end of spectrum lies simple maximal matching algorithm like iSLIP [McKeown, N., April 1999], which is implemented in commercially available routers. In [Dai, J., et al., March 2000] it was shown that all maximal matching scheduling algorithms are stable at speedup of 2. But nothing is known about their delay performance. In this paper, we obtain bounds on all maximal matching scheduling algorithm running at speedup 2 when traffic is Bernoulli i.i.d. Interestingly, these bounds match the theoretical lower bound very closely and much better than the bounds on MWM. In particular, we show that any CIOQ switch running at speedup 2 with maximal matching schedule as at most 5 times longer queue-sizes on average compared to the OQ switch under Bernolli i.i.d. traffic. This suggests that under assumption of traffic being independent enough, no switch can do better than a simple maximal matching algorithm running at speedup 2. This provides the first theoretical support to ""iSLIP can provide QoS"". We would like to note that any IQ switch architecture that needs to sup- port OQ switch must have speedup 2 as shown in [Chuang, S.-T., et al., 1999], [Prabhakar, P., et al., 1999]. The algorithms proposed in [Chuang, S.-T., et al., 1999], [Prabhakar, P., et al., 1999] are very complex compared to algorithms like !SLIP.","Switches,
Scheduling algorithm,
Delay,
Traffic control,
Throughput,
Job shop scheduling,
Computer architecture,
Processor scheduling,
Computer science,
Bandwidth"
Impact of IPv6 on end-user applications,"With IPv6's maturity increasing, there is a need to evaluate the performance benefits or drawbacks for end users that the new IPv6 protocol will have compared to the well-established IPv4 protocol. Theoretically, the expected overhead between the two different protocols should be directly proportional to the difference in the packet's header size, and therefore the expected performance of IPv6 should be similar to IPv4. According to findings, the empirical performance difference between IPv4 and IPv6 in a real setting is much higher than anticipated. The experiment setup is the key ingredients in the findings since it demonstrates the current performance of IPv6 as delivered by commercial routers supporting IPv6 for performance metrics such as throughput and latency.","IP networks,
Telecommunication traffic,
Computer science,
Routing protocols,
Streaming media,
Quality of service,
Internet,
Application software,
Measurement,
Throughput"
"Design, implementation and evaluation of adaptive recompilation with on-stack replacement","Modern virtual machines often maintain multiple compiled versions of a method. An on-stack replacement (OSR) mechanism enables a virtual machine to transfer execution between compiled versions, even while a method runs. Relying on this mechanism, the system can exploit powerful techniques to reduce compile time and code space, dynamically de-optimize code, and invalidate speculative optimizations. The paper presents a new, simple, mostly compiler-independent mechanism to transfer execution into compiled code. Additionally, we present enhancements to an analytic model for recompilation to exploit OSR for more aggressive optimization. We have implemented these techniques in Jikes RVM and present a comprehensive evaluation, including a study of fully automatic, online, profile-driven deferred compilation.","Optimizing compilers,
Virtual machining,
Voice mail,
Java,
Virtual manufacturing,
Counting circuits,
Runtime,
Computer science,
Power system modeling,
Computer languages"
Design and validation of a performance and power simulator for PowerPC systems,"This paper describes the design and validation of a performance and power simulator that is part of the Mambo simulation environment for PowerPC® systems. One of the most notable features of the simulator, designated as Tempo, is the incorporation of an event-driven power model. Tempo satisfies an important need for fast and accurate performance and power simulation tools at the system level. The power and performance predictions from the simulated model of a PowerPC 405GP (or simply 405GP) were validated against a 405GP-based evaluation board instrumented for power measurements using 42 application/dataset combinations from the EEMBC benchmark suite. The average performance and energy-prediction errors were 0.6% and ࢤ4.1%, respectively. In addition to describing Tempo, we show examples of how well it can predict the runtime power consumption of a 405GP microprocessor during application execution.",
Goal-oriented idea generation method for requirements elicitation,"We present an extended version of goal-oriented analysis methods where an idea generation method is combined to reinforce the support of the step for identifying subgoals by a team of stakeholders. To assess our method, experimental results are also discussed.","Concrete,
Joining processes,
Computer science,
Collaboration"
A heuristic approach to solving the software clustering problem,"This paper provides an overview of the author's Ph.D. thesis (2002). The primary contribution of this research involved developing techniques to extract architectural information about a system directly from its source code. To accomplish this objective a series of software clustering algorithms were developed. These algorithms use metaheuristic search techniques to partition a directed graph generated from the entities and relations in the source code into subsystems. Determining the optimal solution to this problem was shown to be NP-hard, thus significant emphasis was placed on finding solutions that were regarded as ""good enough"" quickly. Several evaluation techniques were developed to gauge solution quality, and all of the software clustering tools created to support this work was made available for download over the Internet.","Clustering algorithms,
Partitioning algorithms,
Software algorithms,
Software maintenance,
Software systems,
Algorithm design and analysis,
Software tools,
Reverse engineering,
Computer science,
Data mining"
Parity-based output compaction for core-based SOCs [logic testing],"SOC test application time is strongly determined by the parallelism attained among core tests. Yet, test bandwidth allocation issues typically impose significant limitations on parallel testing of the cores. In this paper, we propose a response compaction methodology for reducing the required output bandwidth of cores, enabling increased parallelism among core tests and hence reducing overall SOC test time. The proposed methodology is based on judiciously partitioning test responses, with each response fragment being compacted individually. The signature corresponding to the response fragments consists of the parity information which is computed through a cost-effective on-chip space and time compaction mechanism. We show that the proposed technique not only delivers negligible aliasing for any (modeled or unmodeled) fault but that it also provides diagnostic and unknown response value handling capabilities.","Compaction,
Circuit testing,
Bandwidth,
Parallel processing,
Circuit faults,
Channel allocation,
Computer science,
Application software,
Transportation,
Test data compression"
RMIX: a multiprotocol RMI framework for Java,"With the increasing adoption of Java for parallel and distributed computing, there is a strong motivation for enhancing the expressive elegance of the RMI paradigm with flexible and adaptable communication substrates. Java RMI is an especially powerful and semantically comprehensive framework for distributed Java applications - but the default Java RMI implementation is bound to a concrete wire protocol, JRMP, that is neither interoperable nor very efficient. To address the first issue, libraries have been proposed that provide RMI semantics over different wire protocols such as SOAP or IIOP, making Java interoperable with Web Services and CORBA. Similarly, alternative high performance RMI implementations have been developed. However, none of these solutions are designed to work cooperatively, and each imposes specific constraints on developers. This paper describes RMIX: an RMI framework that supports a variety of dynamically pluggable wire transports underlying a common and uniform RMI facade. RMIX facilitates dynamic protocol negotiation in loosely coupled parallel and distributed systems, and enables the development and deployment of applications that are multiprotocol by nature. Additionally, RMIX offers some enhancements to RMI semantics that are particularly useful in multiuser environments. We describe the design and preliminary implementation of RMIX, present two prototype protocol providers based on the JRMP and SOAP protocols, and outline a transition path from legacy RMI applications to RMIX.","Java,
Wire,
Distributed computing,
Libraries,
Simple object access protocol,
Web services,
Computer science,
Dynamic programming,
Concrete,
Prototypes"
Probabilistic noninterference through weak probabilistic bisimulation,"To be practical, systems for ensuring secure information flow must be as permissive as possible. To this end, the author recently proposed a type system for multi-threaded programs running under a uniform probabilistic scheduler; it allows the running times of threads to depend on the values of H variables, provided that these timing variations cannot affect the values of L variables. But these timing variations preclude a proof of the soundness of the type system using the framework of probabilistic bisimulation, because probabilistic bisimulation is too strict regarding time. To address this difficulty, this paper proposes a notion of weak probabilistic bisimulation for Markov chains, allowing two Markov chains to be regarded as equivalent even when one ""runs"" more slowly that the other. The paper applies weak probabilistic bisimulation to prove that the type system guarantees the probabilistic noninterference property. Finally, the paper shows that the language can safely be extended with a fork command that allows new threads to be spawned.","Yarn,
Timing,
Processor scheduling,
Mobile computing,
Computer science,
Computer languages,
Probability distribution,
Application software,
Computer security,
Conferences"
Resolving the directness dilemma in document review sessions with nonnative speakers,"Reviewers of technical documents must often work with nonnative speakers (NNSs) of English. Drawing on research in cross-cultural pragmatics and institutional discourse, we discuss linguistic patterns that document reviewers are likely to use when commenting on NNS writing. We anticipate miscommunications that may arise from some of these linguistic patterns, especially when a reviewer attempts to be both clear (so that the writer understands the comments) and polite (so that the reviewer maintains positive working relations with the writer). We recommend specific linguistic strategies that allow reviewers to balance clarity and politeness most effectively when communicating with NNSs.","Writing,
Cross-cultural communication,
Fellows,
Cultural differences,
Online Communities/Technical Collaboration,
Speech,
Process design,
Computer science,
Mathematics,
Statistics"
Study of distance vector routing protocols for mobile ad hoc networks,"We investigate the performance issues of destination-sequenced distance vector (DSDV) and ad-hoc on-demand distance vector (AODV) routing protocols for mobile ad hoc networks. Four performance metrics are measured by varying the maximum speed of mobile hosts, the number of connections, and the network size. The correlation between network topology change and mobility is investigated by using linear regression analysis. The simulation results indicate that AODV outperforms DSDV in less stressful situations, while DSDV is more scalable with respect to the network size. It is observed that network congestion is the dominant reason for packet drop for both protocols. We propose a new routing protocol, congestion-aware distance vector (CADV), to address the congestion issues. CADV outperforms AODV in delivery ratio by about 5%, while introduces less protocol load. The result demonstrates that integrating congestion avoidance mechanisms with proactive routing protocols is a promising way to improve performance.","Routing protocols,
Mobile ad hoc networks,
Vectors,
Network topology,
Computer networks,
Measurement,
Computer science education,
Computer security,
Information security,
Intelligent networks"
A layered reference model of the brain,"Based on the advances of research in cognitive informatics and related fields, this paper attempts to develop a layered reference model of the brain that explains the functional mechanisms and cognitive processes of the natural intelligence. A variety of life functions and cognitive processes have been identified in cognitive informatics, cognitive science, neuropsychology, and neurophilosophy. In order to formally and rigorously describe a comprehensive and coherent set of mental processes and their relationships, an integrated reference model of the brain is established, which encompasses 37 cognitive processes at six layers known as the sensation, memory, perception, action, meta and higher cognitive layers from bottom up. The reference model of the brain may be applied to explain a wide range of physiological, psychological, and cognitive phenomena in cognitive informatics, particularly relationships between the inherited and the acquired life functions, as well as the subconscious and conscious cognitive processes.","Brain modeling,
Cognitive informatics,
Open systems,
Software engineering,
Psychology,
Protocols,
Computer networks,
Local area networks,
Wide area networks,
Context modeling"
Dusty radio frequency discharges in argon,"Spatial distributions were obtained of the electron and ion densities, the electric potential and field, the space-time averaged electron energy distribution function, and the charge of the dust particles across the discharge interelectrode gap of a radio frequency (RF) discharge in argon by the 1D3V particle-in-cell/Monte Carlo-collisions computer simulations. In addition, the electric current in the external discharge circuit was computed. Obtained results shown that the RF discharge with dust particles has a quasi-neutral central part with a low electric field and nonstationary sheaths with a strong electric field separating the electrodes from the central part. The dust particles essentially influence the spatial distribution of the discharge parameters and the electric current in the external circuit. In particular, an increase of the dust particle density causes an expansion of sheaths as well as a decrease of the current magnitude and an additional shift of the electric current in the external circuit relative to the sustaining external voltage. Secondary electron emission from the electrodes influences the discharge parameters significantly only when the effective secondary-emission yield /spl gamma/ exceeds a value of 0.2.","Radio frequency,
Argon,
Current,
Circuits,
Electrons,
Electrodes,
Electric potential,
Distribution functions,
Computer simulation,
Voltage"
"Beam me up, doctor McCoy [pervasive computing]","Interplanetary explorers in science fiction have long worn communicators to keep in voice contact with team members and deliver crucial information just when needed. Wearers simply ask the badge to contact appropriate individuals by name, function, or location-voila, they are connected. Moreover, the badges respond only to their owners, not to nearby casual conversations. They are always light, small, and casually portable. Best yet they are no longer fiction. To see how this concept translates to a real-life working organization with pressing scheduled and a busy staff, I spoke with representatives from Vocera Communications, which has developed a working communicator badge system for mobile users, and from St. Vincent's Hospital in Birmingham, Alabama, which has deployed the Vocera system. As with other pervasive systems we've covered, this one delivers information directly to mobile workers, at the point of service, thus saving numerous trips to distant telephones, terminals, and PCs to get needed data and messages.","Speech recognition,
Speech synthesis,
Prototypes,
Wireless networks,
Standards development,
Switches,
Engines,
Pressing,
Job shop scheduling,
Mobile communication"
Lectern II: a multimedia lecture capturing and editing system,"Deployment of asynchronous learning systems is still very limited mainly because of the use of expensive video to record and play back classroom lectures. The key advantage of the Lectern II approach is that all important lecture components can be effectively captured without video. Lectern II employs the touch-sensitive screen technology to build a ""digital desk,"" which is shown to be able to effectively support and transparently capture the standard classroom lecturing activity. Recorded lectures can be edited and automatically uploaded to a Web server, and than viewed by students via a standard streaming player. As the total cost of a complete Lectern II system is under $4,000, Lectern II represents the first course lecture recording system that has the potential to be widely deployed in the classrooms of universities and K-12 schools.","Multimedia systems,
Costs,
Web server,
Streaming media,
Hardware,
Video recording,
Modems,
HTML,
Computer science,
Educational institutions"
Building UML class diagram maintainability prediction models based on early metrics,"The fact that the usage of metrics in the analysis and design of object oriented (OO) software can help designers make better decisions is gaining relevance in software measurement arena. Moreover, the necessity of having early indicators of external quality attributes, such as maintainability, based on early metrics is growing. In addition to this, the aim is to show how early metrics which measure internal attributes, such as structural complexity and size of UML class diagrams, can be used as early class diagram maintainability indicators. For this purpose, we present a controlled experiment and its replication, which we carried out to gather the empirical data, which in turn is the basis of the current study. From the results obtained, it seems that there is a reasonable chance that useful class diagram maintainability models could be built based on early metrics. Despite this fact, more empirical studies, especially using data taken form real projects performed in industrial settings, are needed in order to obtain a comprehensive body of knowledge and experience.",
Dynamic data management for location based services in mobile environments,"We characterize the dynamic data management problem for location based services (LBS) in mobile environments and devise a cost model for servicing both location independent and location dependent data. The cost analysis leads to a set of dynamic data management strategies that employs judicious caching, proactive server pushing and neighborhood replication to reduce service cost and improve response time under changing user mobility and access patterns. Simulation results suggest that different strategies are effective for different types of data in response to different patterns of movement and information access.","Environmental management,
Costs,
Mobile computing,
Data engineering,
Pattern analysis,
Delay,
Databases,
Computer science,
Engineering management,
Data analysis"
Self adjustable CHOKe: an active queue management algorithm for congestion control and fair bandwidth allocation,"Queue management and congestion control are very important to the robustness and fairness of the Internet. In this paper, a new queue management algorithm, termed self adjustable choke (SAC) is proposed to achieve both fairness and congestion control in an Internet router. It is based on the well-known RED algorithm and a recently proposed CHOKe algorithm. The SAC scheme has kept the advantages of simplicity and lower processing cost of RED and CHOKe, while solving their unfair bandwidth allocation properties. In particular, the SAC scheme treats TCP and UDP flows differently, and can adaptively adjust its parameters according to the current traffic status. As a result, using the SAC, bandwidth is distributed evenly among different flows, no matter how different these flows are. This will be demonstrated and compared to CHOKe using experimental results under various traffic scenarios.",
A simplified maximum likelihood detection scheme for MIMO systems,"A multiple transmit and multiple receive antenna system, referred to as a multiple-input multiple-output (MIMO) system, is known to achieve large channel capacity without bandwidth expansion. We propose a simplified maximum likelihood (ML) detection scheme for a MIMO system which has reduced computational complexity compared to the ML detection scheme. According to simulation results, the bit error rate (BER) performance of the proposed scheme is shown to be similar to that of the ML detection scheme with significant reduction in the computational complexity.","Maximum likelihood detection,
MIMO,
Computational complexity,
Interference cancellation,
Transmitting antennas,
Receiving antennas,
Bit error rate,
Computer science,
Channel capacity,
Bandwidth"
A study of pattern recovery in recurrent correlation associative memories,"In this paper, we analyze the recurrent correlation associative memory (RCAM) model of Chiueh and Goodman (1990, 1991). This is an associative memory in which stored binary memory patterns are recalled via an iterative update rule. The update of the individual pattern-bits is controlled by an excitation function, which takes as its argument the inner product between the stored memory patterns and the input patterns. Our contribution is to analyze the dynamics of pattern recall when the input patterns are corrupted by noise of a relatively unrestricted class. We show how to identify the excitation function which maximizes the separation (the Fisher discriminant) between the uncorrupted realization of the noisy input pattern and the remaining patterns residing in the memory. The excitation function which gives maximum separation is exponential when the input bit-errors follow a binomial distribution. We develop an expression for the expectation value of bit-error probability on the input pattern after one iteration. We show how to identify the excitation function which minimizes the bit-error probability. The relationship between the excitation functions which result from the two different approaches is examined for a binomial distribution of bit-errors. We develop a semiempirical approach to the modeling of the dynamics of the RCAM.","Associative memory,
Error analysis,
Pattern analysis,
Closed-form solution,
Pattern recognition,
Computer science,
Stress,
Large scale integration,
Very large scale integration"
Using Easy Java Simulations to create scientific simulations in Java,"Easy Java Simulations, Ejs, is a software authoring tool that helps create scientific simulations in Java. We show in this paper a sequence of sample simulations that endorse the claim that is the starting point of Ejs: creating simulations of scientific phenomena that can be a challenging yet attainable task, suitable for science and engineering students. We'll also try to show the pedagogical attractive that creating simulations can have, as long as we let the students engage in model description and not get distracted by computer specific issues, which are of course needed in order to create a modern simulation, but that Ejs automatically takes care of.","Java,
Computational modeling,
Computer simulation,
Equations,
Software tools,
Engineering students,
Writing,
Computer displays,
Physics computing,
Terminology"
"Towards a semantic, deep archival file system","In essence, computers are tools to help us with our daily lives. CPUs are extension to our reasoning capability whereas disks are extensions to our memory. But the simple hierarchical namespace of existing file systems is inadequate in managing files today that have rich semantics. In this paper, we advocate the need for integrating semantic information into a storage system. We propose ""Sedar"", a deep archival file system. Sedar is one of the the first archival file systems that integrates semantic storage and retrieval capabilities. In addition, Sedar introduces several novel features: the notion of ""semantic-hashing"" to reduce the storage consumption that is robust against misalignment of documents; ""virtual snapshot"" of namespace, and ""conceptual deletions"" of files and directories. It exposes a semantic catalog that allows other semantic-based tools (e.g., visualization and statistical analysis) to be built. It uses a decentralized peer-to-peer storage utility enabling horizontal scalability.","File systems,
Humans,
Laboratories,
Milling machines,
Content based retrieval,
Computer science,
Robustness,
Visualization,
Statistical analysis,
Peer to peer computing"
Ascending frequency ordered prefix-tree: efficient mining of frequent patterns,"Mining frequent patterns is a fundamental and important problem in many data mining applications. Many of the algorithms adopt the pattern growth approach, which is shown to be superior to the candidate generate-and-test approach significantly. We identify the key factors that influence the performance of the pattern growth approach, and optimize them to further improve the performance. Our algorithm uses a simple while compact data structure-ascending frequency ordered prefixtree (AFOPT) to organize the conditional databases, in which we use arrays to store single branches to further save space. We traverse our prefix-tree structure using a top-down strategy. Our experiment results show that the combination of the top-down traversal strategy and the ascending frequency item ordering method achieves significant performance improvement over previous works.","Frequency,
Itemsets,
Data mining,
Transaction databases,
Space exploration,
Computer science,
Spatial databases,
Councils,
Costs"
Linking perception and action in a control architecture for human-robot domains,"Human-robot interaction is a growing research domain; there are many approaches to robot design, depending on the particular aspects of interaction being focused on. In this paper we present an action-based framework that provides a natural means for robots to interact with humans and to learn from them. Perception and action are the essential means for a robot's interaction with the environment; for successful robot performance it is thus important to exploit this relation between a robot and its environment. Our approach links perception and actions in a unique architecture for representing a robot's skills (behaviors). We use this architecture to endow the robots with the ability to convey their intentions by acting upon their environment and also to learn to perform complex tasks from observing and experiencing a demonstration by a human teacher. We demonstrate these concepts with a Pioneer 2DX mobile robot, learning various tasks from a human and, when needed, interacting with a human to get help by conveying its intentions through actions.",
On the computational complexity of decidable fragments of first-order linear temporal logics,"We study the complexity of some fragments of first-order temporal logic over natural numbers time. The one-variable fragment of linear first-order temporal logic even with sole temporal operator /spl square/ is EXPSPACE-complete (this solves an open problem of J. Halpern and M. Vardi (1989)). So are the one-variable, two-variable and monadic monodic fragments with Until and Since. If we add the operators O/sup n/, with n given in binary, the fragment becomes 2EXPSPACE-complete. The packed monodic fragment has the same complexity as its pure first-order part - 2EXPTIME-complete. Over any class of flows of time containing one with an infinite ascending sequence - e.g., rationals and real numbers time, and arbitrary strict linear orders - we obtain EXPSPACE lower bounds (which solves an open problem of M. Reynolds (1997)). Our results continue to hold if we restrict to models with finite first-order domains.","Computational complexity,
Logic,
Educational institutions,
Computer science,
Upper bound"
Computer automated multi-paradigm modelling: meta-modelling and graph transformation,"Computer automated multi-paradigm modelling based on meta-modelling and graph transformation is presented. The syntax of a class of models of interest is graphically meta-modelled in an appropriate formalism such as entity-relationship diagrams. From this abstract syntax, augmented with concrete (visual) information, an interactive, visual modelling environment is generated. As the abstract syntax of all models is graph-like, graph rewriting is used to perform model transformation. Graph grammar models thus allow for model transformation specification. Graph rewriting provides a rigourous basis for specifying and analyzing model transformations such as simplification, simulation, and code generation. AToM/sup 3/, a tool for multi-formalism and meta-modelling, is introduced. Meta-modelling and graph transformation concepts are introduced through a simple reactive system example: a timed automata model of a traffic light. Meta-modelling, generating the visual modelling environment, and modelling transformations as graph grammars, as well as executing them, are performed in AToM/sup 3/. The model transformations include simulation, transformation into timed transition Petri nets, and code generation.","Traffic control,
Computational modeling,
Analytical models,
Automata,
Petri nets,
Computer science,
Concrete,
Computer simulation,
Unified modeling language,
Computer interfaces"
A load balancing mobility management for multilevel hierarchical mobile IPv6 networks,"In order to reduce the number of binding updates (BU) when the number of connecting mobile hosts (MH) is limited for higher level mobility anchor point (MAP), a scheme adopting distributed location management (DLM) to multilevel hierarchical mobile IPv6 (M-HMIPv6) is proposed. In the proposed scheme, to avoid extreme load for higher level MAP, a threshold of the number of connecting MHs is set. When a MAP receives a BU and the number of connecting MHs exceeds the threshold, the MAP forwards the BU to the next MAP candidate. In addition, to reduce the number of BUs and extend a MAP domain, DLM is adopted in M-HMIPv6. The usage of the distance field in MAP option is modified to distinguish the route redundancy and keep transparency to HMIPv6. Moreover, to realize load balancing when mobility deviation exists, a load balancing by average BU interval in both access router (AR) and MH is adopted. By classifying MHs roughly by the average BU interval in AR, relatively faster MHs select higher MAPs. By performance evaluation using computer simulations, we show that the proposed scheme transparent to HMIPv6 can reduce the number of BUs and realize load balancing without excessive route redundancy.","Load management,
Mobile radio mobility management,
Joining processes,
Mobile computing,
Mobile communication,
Internet,
Delay,
Topology,
Computer science,
Computer network management"
Solving dynamic TSP with evolutionary approach in real time,"Many real world optimization problems are time-dependent and some of them can be modeled by the dynamic TSPs (DTSPs). A DTSP is harder than a general TSP, which is a NP-hard problem, because the city number and the cost matrix of a DTSP are time varying. Although DTSP is a very common and important model in real world systems, few literatures have discussed this related issues. There are many open questions about DTSP urgently needed to be answered. We first give a mathematical model and the optimization objective for DTSP. Then we discuss why evolutionary algorithms (EAs) are effective for solving DTSPs and give some key points for designing efficient DTSP EAs. By defining three dynamic operators, we proposed an evolutionary algorithm for DTSPs. The experiments show the new algorithm is suitable for solving DTSPs. At the end, we also give some preliminary ideas for reinforcing the efficiency of EAs for DTSPs.",
Individual differences in program comprehension strategies in unfamiliar programming systems,"This study examines the effect of individual differences on the program comprehension strategies of users working with an unfamiliar programming system. Participants of varying programming expertise were given a battery of psychological tests, a brief introduction to a statistical programming environment, and a 20-minute debugging task. Our data show three distinct comprehension strategies that were related to programming experience, but individuals with stronger domain knowledge for specific bugs tended to succeed.","Programming profession,
Debugging,
Psychology,
Computer bugs,
Predictive models,
Documentation,
Computer science,
Educational institutions,
Art,
Batteries"
AOTO: adaptive overlay topology optimization in unstructured P2P systems,"Peer-to-peer (P2P) systems are self-organized and decentralized. However, the mechanism of a peer randomly joining and leaving a P2P network causes topology mismatching between the P2P logical overlay network and the physical underlying network. The topology mismatching problem brings great stress on the Internet infrastructure and seriously limits the performance gain from various search or routing techniques. We propose the adaptive overlay topology optimization (AOTO) technique, an algorithm for building an overlay multicast tree between each source node and its direct logical neighbors so as to alleviate the mismatching problem by choosing closer nodes as logical neighbors, while providing a larger query coverage range. AOTO is scalable and completely distributed in the sense that it does not require global knowledge of the whole overlay network when each node is optimizing the organization of its logical neighbors. The simulation shows that AOTO can effectively solve the mismatching problem and reduce more than 55% of the traffic generated by the P2P system itself.","Peer to peer computing,
Network topology,
Computer science,
Internet,
Adaptive systems,
Stress,
Performance gain,
Routing,
Multicast algorithms,
Telecommunication traffic"
A genetic algorithm for tasks scheduling in parallel multiprocessor systems,"The tasks scheduling problem is a key factor for a parallel multiprocessor system to gain better performance. Because a task can be partitioned into a group of subtasks and represented as a DAG (directed acyclic graph), so the problem can be stated as finding a schedule for a DAG to be executed in a parallel multiprocessor system so that the schedule length can be minimized. The tasks scheduling problem is NP-hard in general, except in a few simplified situations. In order to obtain optimal or suboptimal solutions, a large number of scheduling heuristics have been presented in the literature. The most studied heuristics are based on list heuristic. Recently, genetic algorithm has received much attention as a class of heuristic. This paper presents a new genetic algorithm called TEOL (task execution order list based genetic algorithm) to solve the scheduling problem in parallel multiprocessor systems. It guarantees that all feasible schedules can be reached with some probability, and because the TEOL is based on the restrain of predecessor relationship of the DAG only, so other heuristics can be combined into it to improve the performance. Simulation results comparing with two genetic algorithms and a list algorithm, both from the literature, show that TEOL produces encouraging results in terms of quality of solution and execution speed.","Genetic algorithms,
Multiprocessing systems,
Processor scheduling,
Space exploration,
Iterative algorithms,
Genetic mutations,
Educational institutions,
Computer science,
Concurrent computing,
Agriculture"
Adaptive learning technologies for bioengineering education,"Learning technologies employed both inside and outside the classroom are increasingly influencing the nature of teaching and learning. Web-based learning technologies are enabling powerful possibilities for learning activities outside the classroom, both in preparation for in-class activities and in following them up. Of particular importance among these possibilities is the opportunity to address learners as individuals, assessing their strengths and weaknesses and adapting learning activities in response. Over the past few years we have been developing and maturing learning technologies that target these aims in the context of the National Science Foundation's VaNTH (Vanderbilt University; Northwestern University; University of Texas at Austin; and Health, Science and Technology at Harvard/MIT) Engineering Research Center (ERC). In this article we describe these technologies and discuss their roles in bioengineering education. In pursuing adaptive learning technologies for VaNTH, our primary motivation has been to make the means for authoring such activities accessible to bioengineering educators while providing enough expressive power to enable ambitious applications and the ability to incrementally acquire the needed skills. To address this aim we have created an authoring technology called the courseware authoring and packaging environment (CAPE).","Educational technology,
Biomedical engineering,
Object oriented modeling,
Programming profession,
Buildings,
Biological system modeling,
Courseware,
Tagging,
Testing,
Packaging"
Analytical design space exploration of caches for embedded systems,"The increasing use of microprocessor cores in embedded systems, as well as mobile and portable devices, creates an opportunity for customizing the cache subsystem for improved performance. Traditionally, a design-simulate-analyze methodology is used to achieve desired cache performance. Here, to bootstrap the process, arbitrary cache parameters are selected, the cache sub-system is simulated using a cache simulator, based on performance results, cache parameters are tuned, and the process is repeated until an acceptable design is obtained. Since the cache design space is typically very large, the traditional approach often requires a very long time to converge. In the proposed approach, we outline an efficient algorithm that directly computes cache parameters satisfying the desired performance. We demonstrate the feasibility of our algorithm by applying it to a large number of embedded system benchmarks.","Space exploration,
Embedded system,
Microprocessors,
Design methodology,
System-on-a-chip,
Embedded computing,
Mobile computing,
Algorithm design and analysis,
Computer science,
Portable computers"
StegFS: a steganographic file system,"While user access control and encryption can protect valuable data from passive observers, those techniques leave visible ciphertexts that are likely to alert an active adversary to the existence of the data, who can then compel an authorized user to disclose it. We introduce StegFS, a steganographic file system that aims to overcome that weakness by offering plausible deniability to owners of protected files. StegFS securely hides user-selected files in a file system so that, without the corresponding access keys, an attacker would not be able to deduce their existence, even if the attacker is thoroughly familiar with the implementation of the file system and has gained full access to it. Unlike previous steganographic schemes, our construction satisfies the prerequisites of a practical file system in ensuring integrity of the files and maintaining efficient space utilization. We have completed an implementation on Linux, and experiment results confirm that StegFS achieves an order of magnitude improvements in performance and/or space utilization over the existing schemes.","Steganography,
File systems,
Protection,
Cryptography,
Access control,
Permission,
Proposals,
Laboratories,
Information technology,
Computer science"
A shortest path representation for video summarisation,"A novel approach is presented to select multiple key frames within an isolated video shot where there is camera motion causing significant scene change. This is achieved by determining the dominant motion between frame pairs whose similarities are represented using a directed weighted graph. The shortest path in the graph, found using the A* search algorithm, designates the key frames. The overall method can be applied to extract a set of key frames which portray both the video content and camera motions, all of which are useful features for video indexing and retrieval.",
About translations of classical logic into polarized linear logic,"We show that the decomposition of intuitionistic logic into linear logic along the equation A /spl rarr/ B = !A /spl rarr/ B may be adapted into a decomposition of classical logic into LLP, the polarized version of Linear Logic. Firstly, we build a categorical model of classical logic (a control category) from a categorical model of linear logic by a construction similar to the co-Kleisli category. Secondly, we analyze two standard continuation-passing style (CPS) translations, the Plotkin and the Krivine's translations, which are shown to correspond to two embeddings of LLP into LL.","Polarization,
Calculus,
Linearity,
Logic design,
Equations,
Computer science"
Saving computational effort in genetic programming by means of plagues,"A new technique for saving computing resources when using genetic programming is presented in this work. Instead of directly fighting bloat $the main factor explaining the large computational cost required for the evaluation of generations - by acting on individuals, we apply a new operator to the whole population: the plague. By removing some individuals every generation, we compensate for the increase in size of individuals, thus saving computing time when looking for solutions.","Genetic programming,
Computer science,
Evolutionary computation,
Proposals,
Computational efficiency,
Biological cells,
Tree data structures,
Size control,
Data structures"
Performance analysis of distributed search in open agent systems,"In open multi-agent systems agents need resources provided by other agents but they are not aware of which agents provide the particular resources. Most solutions to this problem are based on a central directory that maintains a mapping between agents and resources. However, such solutions do not scale well since the central directory becomes a bottleneck in terms of both performance and reliability. In this paper, we introduce a different approach: each agent maintains a limited size local cache in which it keeps information about k different resources, that is, for each of k resources, it stores the contact information of one agent that provides it. This creates a directed network of caches. We address the following fundamental problem: how can an agent that needs a particular resource find an agent that provides it by navigating through this network of caches? We propose and analytically compare the performance of three different algorithms for this problem, flooding, teeming and random paths, in terms of three performance measures: the probability to locate the resource, the number of steps and the number of messages to do so. Our analysis is also applicable to distributed search in unstructured peer-to-peer networks.","Performance analysis,
Multiagent systems,
Maintenance,
Peer to peer computing,
Computer science,
Navigation,
Algorithm design and analysis,
Costs"
On the self-similarity of synthetic traffic for the evaluation of intrusion detection systems,"The difficulty of quantifying the accuracy of intrusion detection tools against real network data mandates that researchers use simulated attack data for the partial evaluation of such tools. In 1998 and 1999 researchers at MIT Lincoln Labs produced datasets both with and without attack data specifically for use by those interested in developing intrusion detection tools. Because self-similarity has been shown to be a statistical property of real network traffic, this paper examines the attack-free datasets for the presence of self-similarity in various time periods. The results offer insight for researchers who may wish to use specific subsets of the data for testing. Where the results indicate a lack of self-similarity in the data, the likely cause was determined to be either a low activity level or traffic that was dominated by a single protocol, thus forcing the overall distribution to match its own.","Intrusion detection,
Telecommunication traffic,
Traffic control,
Testing,
Force measurement,
Protocols,
Character generation,
Local area networks,
IP networks,
Computer science"
Reactive fuzzy dispatching rule for automated guided vehicles,"The automated manufacturing system (AMS) scheduling complexity increases with products customization, due dates, alternative routes and no foresee demand in the system. One of the important aspects in shop floor is to ensure that the parts will be at the right position and at the defined time. In this sense it's necessary to have a good transportation system, which allows flexibility and integration, considering a designed performance to the manufacturing system. AGVs have been used to reach this performance, but there is a serious problem for the dispatching rule definition. There are some dispatching techniques using single rule and composed ones, where the main idea is to consider a good performance for the entire manufacturing system. This work presents an approach for a definition of an AGV dispatching rule based on fuzzy logic, that considers the actual status of the manufacturing system and takes the decision on real time.","Dispatching,
Vehicles,
Manufacturing systems,
Fuzzy systems,
Transportation,
Input variables,
Productivity,
Computer science,
Computer aided manufacturing,
Manufacturing automation"
Support vector machines for analog circuit performance representation,"The use of Support Vector Machines (SVMs) to represent the performance space of analog circuits is explored. In abstract terms, an analog circuit maps a set of input design parameters to a set of performance figures. This function is usually evaluated through simulations and its range defines the feasible performance space of the circuit. In this paper, we directly model performance spaces as mathematical relations. We study approximation approaches based on two-class and one-class SVMs, the latter providing a better tradeoff between accuracy and complexity avoiding ""curse of dimensionality"" issues with 2-class SVMs. We propose two improvements of the basic one-class SVM performances: conformal mapping and active learning. Finally, we develop an efficient algorithm to compute projections, so that top-down methodologies can be easily supported.","Support vector machines,
Analog circuits,
Computational modeling,
Circuit simulation,
Permission,
Computer science,
Conformal mapping,
Algorithm design and analysis,
Analog integrated circuits,
Signal design"
Challenging formal specifications by mutation: a CSP security example,"When formal modelling is done we must validate both the model and the assumptions. Formal techniques tend to concentrate on the former. We examine how fault injection (specification mutation) and model checking can help address the latter, in particular, the effects of failure. We find that, in contrast with software testing, where they are a problem, ""equivalent mutants"" are valuable for specification validation.","Formal specifications,
Genetic mutations,
System testing,
Software testing,
Power system modeling,
Computer security,
Computer science,
Government,
Information security,
Power system security"
Representing Web graphs,"A Web repository is a large special-purpose collection of Web pages and associated indexes. Many useful queries and computations over such repositories involve traversal and navigation of the Web graph. However, efficient traversal of huge Web graphs containing several hundred million vertices and a few billion edges is a challenging problem. An additional complication is the lack of a schema to describe the structure of Web graphs. As a result, naive graph representation schemes can significantly increase query execution time and limit the usefulness of Web repositories. We propose a novel representation for Web graphs, called an S-Node representation. We demonstrate that S-Node representations are highly space-efficient, enabling in-memory processing of very large Web graphs. In addition, we present detailed experiments that show that S-Node representations can significantly reduce query execution times when compared with other schemes for representing Web graphs.","Search engines,
Web pages,
Internet,
Computer science,
Navigation,
Natural languages,
Proposals,
Performance analysis,
Indexing,
Collaborative work"
Outdoor exploration and SLAM using a compressed filter,,"Simultaneous localization and mapping,
Robot kinematics,
Filters,
Mobile robots,
Robot control,
Vectors,
Covariance matrix,
Numerical analysis,
Computer science,
Layout"
A QoS metamodel and its realization in a CORBA component,This paper presents a generic model driven approach to enable quality of service (QoS) modeling and realization for component based middleware platforms. We describe a QoS metamodel that makes it possible to support multi category QoS instead of introducing a static set of QoS properties into the modeling method. The metamodel is integrated with the UML metamodel and the component implementation framework (CIF) metamodel of the CORBA component model (CCM). Extensions to the CCM are described that are necessary to realize the QoS models in the software components and infrastructure. In addition an example QoS category is presented to demonstrate the application of the presented approach.,"Quality of service,
Middleware,
Contracts,
Unified modeling language,
Application software,
Programming,
Telecommunications,
Computer architecture,
Bandwidth,
Switches"
Robust watermarking of 3D polygonal models based on vertex scrambling,"We propose a robust 3D model watermarking scheme based on distributing the information corresponds to a bit of the watermark over the entire model via vertex scrambling. The scheme also adaptively varies the strength of the watermark signal with reference to the local geometry and embeds watermark information into the model by modifying the length of the vectors that link vertices to the centre of the model. Experiments show that this watermarking scheme is robust against attacks such as mesh simplification, addition of noise, and model cropping.","Watermarking,
Signal processing algorithms,
Solid modeling,
Spatial resolution,
Signal resolution,
Computer science,
Information geometry,
Noise robustness,
Resists,
Data security"
On demand Web services-based business process composition,"In this paper, we present the Web services outsourcing manager framework via a mathematical model for dynamic business processes configuration using existing Web services to meet customers' requirements. An XML-based annotation document is proposed to capture the business requirements and used to dynamically generate search scripts for an advanced Web services discovery engine to find Web services from UDDI registries and Web service inspection language documents. A list of available Web services is returned for further composition and optimization to produce the final business process. This paper proposes a novel mechanism to map a service selection problem into a solution space {0,1} to utilize gold optimization algorithms such as genetic algorithms (GA). A working research prototype has been implemented to demonstrate the feasibility of the on-demand Web services flow composition.","Web services,
Outsourcing,
Genetic algorithms,
Mathematical model,
Prototypes,
Disaster management,
Chaos,
Computer science,
Engineering management,
Engines"
The Subgraph Bisimulation Problem,"We study the complexity of the Subgraph Bisimulation Problem, which relates to Graph Bisimulation as Subgraph Isomorphism relates to Graph Isomorphism, and we prove its NP-Completeness. Our analysis is motivated by its applications to semistructured databases.","Polynomials,
Information retrieval,
Computer science,
Database languages,
Testing,
Hafnium,
Equations"
Audio information access from meeting rooms,"We investigate approaches to accessing information from the streams of audio data that result from multi-channel recordings of meetings. The methods investigated use word-level transcriptions, and information derived from models of speaker activity and speaker turn patterns. Our experiments include spoken document retrieval for meetings, automatic structuring of meetings based on self-similarity matrices of speaker turn patterns and a simple model of speaker activity. Meeting recordings are rich in both lexical and non-lexical information; our results illustrate some novel kinds of analysis made possible by a transcribed corpus of natural meetings.",
"Persuading developers to ""buy into"" software process improvement: a local opinion and empirical evidence","In order to investigate practitioners' opinions of software process and software process improvement, we have collected a large volume of qualitative evidence from 13 companies. At the same time, other researchers have reported investigations of practitioners, and we are interested in how their reports may relate to our evidence. Thus, other research publications can also be treated as a form of qualitative data. In this paper, we review advice on a method, content analysis, which is used to analyse qualitative data. We use content analysis to describe and analyse discussions on software process and software process improvement. We report preliminary findings from an analysis of both the focus group evidence and four publications. Our main finding is that there is an apparent contradiction between developers saying that they want evidence for software process improvement, and what developers will accept as evidence. This presents a serious problem for research: even if researchers could demonstrate a strong, reliable relationship between software process improvement and improved organisational performance, there would still be the problem of convincing practitioners that the evidence applies to their particular situation.","Software quality,
Data analysis,
Costs,
Programming,
Software engineering,
Computer science,
Educational institutions,
Software performance,
Productivity,
Stability"
Hiding program slices for software security,"Given the high cost of producing software, development of technology for prevention of software piracy is important for the software industry. In this paper we present a novel approach for preventing the creation of unauthorized copies of software. Our approach splits software modules into open and hidden components. The open components are installed (executed) on an insecure machine while the hidden components are installed (executed) on a secure machine. We assume that while open components can be stolen, to obtain a fully functioning copy of the software, the hidden components must be recovered. We describe an algorithm that constructs hidden components by slicing the original software components. We argue that recovery of hidden components constructed through slicing, in order to obtain a fully functioning copy of the software, is a complex task. We further develop security analysis to capture the complexity of recovering hidden components. Finally we apply our technique to several large Java programs to study the complexity of recovering constructed hidden components and to measure the runtime overhead introduced by splitting of software into open and hidden components.","Pervasive computing,
Computer crime,
Application software,
Smart cards,
Computer industry,
Runtime,
Mobile computing,
Protection,
Computer science,
Costs"
Making cyclic circuits acyclic,"Cyclic circuits that do not hold state or oscillate are often the most convenient representation for certain functions, such as arbiters, and can easily by produced inadvertently in high-level synthesis, yet are troublesome for most circuit analysis tools. This paper presents an algorithm that generates an acyclic circuit that computes the same function as a given cyclic circuit for those inputs where the cyclic circuit does not oscillate or hold state. The algorithm identifies all patterns on inputs and internal nodes that lead to acyclic evaluation orders for the cyclic circuit, which are represented as acyclic circuit fragments, and then combines these to produce an acyclic circuit that can exhibit all of these behaviors. Experiments results suggest this potentially exponential algorithm is practical for small circuits and may be improved to handle larger circuits. This algorithm should make dealing with cyclic combinational circuits nearly as easy as dealing with their acyclic counterparts.","High level synthesis,
Circuit simulation,
Permission,
Delay,
Computer science,
Circuit analysis computing,
Combinational circuits,
Logic design,
Circuit analysis,
Computational modeling"
Improving expression data mining through cluster validation,Presents several cluster evaluation techniques for gene expression data analysis. Normalisation and validity aggregation strategies are proposed to improve the prediction of the number of relevant clusters. The effect of different intracluster and intercluster distances on this prediction process is studied. This approach is applied to a publicly released medulloblastomas tumour data set The results suggest that it may represent an effective tool to support biomedical knowledge discovery tasks based on gene expression data.,"Data mining,
Gene expression,
Clustering algorithms,
Tumors,
Pharmaceutical technology,
Data analysis,
DNA,
Biomedical measurements,
Algorithm design and analysis,
Computer science"
Iso-splatting: a point-based alternative to isosurface visualization,"We present a new approach to isosurface visualization that we call ""iso-splatting."" We use point primitives for representing and rendering isosurfaces. The method consists of two steps. In the first step, point samples are generated throughout the volumetric domain of a scalar function. In the second step, these points are projected onto the isosurface of interest. We render the resulting point set using a surface splatting algorithm. The method can be extended to out-of-core or parallel environments. Our results show that this method can offer much greater time and space efficiency when compared with standard triangle-based methods, thereby supporting higher levels of interactivity. Parts of the algorithm can be accelerated using graphics hardware. One key advantage of this approach is that, since extraction computations are divided into two smaller phases, work can be distributed to exploit all available resources.",
Approximation algorithms for asymmetric TSP by decomposing directed regular multigraphs,"A directed multigraph is said to be d-regular if the indegree and outdegree of every vertex is exactly d. By Hall's theorem one can represent such a multigraph as a combination of at most n/sup 2/ cycle covers each taken with an appropriate multiplicity. We prove that if the d-regular multigraph does not contain more than /spl lfloor/d/2/spl rfloor/ copies of any 2-cycle then we can find a similar decomposition into 0(n/sup 2/) pairs of cycle covers where each 2-cycle occurs in at most one component of each pair. Our proof is constructive and gives a polynomial algorithm to find such decomposition. Since our applications only need one such a pair of cycle covers whose weight is at least the average weight of all pairs, we also give a simpler algorithm to extract a single such pair. This combinatorial theorem then comes handy in rounding a fractional solution of an LP relaxation of the maximum and minimum TSP problems. For maximum TSP, we obtain a tour whose weight is at least 2/3 of the weight of the longest tour, improving a previous 5/8 approximation. For minimum TSP we obtain a tour whose weight is at most 0.842log/sub 2/ n times the optimal, improving a previous 0.999log/sub 2/ n approximation. Utilizing a reduction from maximum TSP to the shortest superstring problem we obtain a 2.5-approximation algorithm for the latter problem which is again much simpler than the previous one. Other applications of the rounding procedure are approximation algorithms for maximum 3-cycle cover (factor 2/3, previously 3/5) and maximum asymmetric TSP with triangle inequality (factor 10/13, previously 3/4 ).","Approximation algorithms,
Computer science,
Polynomials,
Traveling salesman problems,
Application software,
Algorithm design and analysis,
Biology computing,
Computer applications,
Computational biology"
The ModelCamera: a hand-held device for interactive modeling,"An important goal of automated modeling is to provide computer graphics applications with high quality models of complex real-world scenes. Prior systems have one or more of the following disadvantages: slow modeling pipeline, applicability restricted to small scenes, no direct color acquisition, and high cost. We describe a hand-held scene modeling device that operates at five frames per second and that costs $2,000. The device consists of a digital video camera with 16 laser pointers attached to it. As the operator scans the scene, the pointers cast blobs that are detected and triangulated to provide sparse, evenly spaced depth samples. The frames are registered and merged into an evolving model, which is rendered continually to provide immediate operator feedback.","Layout,
Laser modes,
Computer graphics,
Costs,
Laser feedback,
Application software,
Digital cameras,
Rendering (computer graphics),
Computer science,
Pipelines"
High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system,"Telepresence systems using an omnidirectional image sensor enable us to experience remote sites with rich sensation. An omnidirectional multi-camera system is more useful for acquiring high-resolution omnidirectional images of outdoor scenes than a monocular camera system. However, exact calibration of the camera system is necessary to generate a panoramic movie. In this paper, we describe a panoramic movie generation method based on geometric and photometric calibration of the omnidirectional multi-camera system. A prototype of a telepresence system is also shown as applications of generated panoramic movies. This high-resolution telepresence system has been proven to enable us to experience remote sites with rich presence.",
Saliency-based multifoveated MPEG compression,"Most current foveation strategies are limited to foveating sequences based on a direct measurement or an implicit assumption of the gaze direction. Such approaches often fail in unconstrained environments or when necessary equipment is absent. Alternatively, a computational model of visual attention may be used to predict visually salient locations. We describe such a neurobiological model of attention and its specific application to foveated video compression. The algorithm is demonstrated to be successful in foveating to regions of human interest in a variety of video segments, including synthetic as well as natural scenes, and also gives good compression ratios.","Transform coding,
Humans,
Layout,
Image coding,
Video compression,
Face detection,
Degradation,
Computer science,
Current measurement,
Computational modeling"
A formal experiment comparing extreme programming with traditional software construction,This paper describes an experiment carried out during the Spring/2002 academic semester with computer science students at the University of Sheffield. The aim of the experiment was to assess extreme programming and compare it with a traditional approach. With this purpose the students constructed software for real clients. We observed 20 teams working for 4 clients. Ten teams worked with extreme programming and ten with the traditional approach. In terms of quality and size teams working with extreme programming produced similar final products to traditional teams. The major implication for the current practice of traditional software engineering is that in spite of the absence of design and the presence of testing before coding the product obtained still has similar quality and size. The implication for extreme programming is the possibility of growth and maturation given the fact that it provided results that were as good as those from the traditional approach.,"Computer science,
Team working,
Software systems,
Springs,
Software engineering,
Software testing,
In vitro,
Production systems,
Filters,
Job production systems"
A reliable broadcast algorithm with selected acknowledgements in mobile ad hoc networks,"Mobile ad hoc networks (MANETs) suffer from transmission contention and congestion because of the broadcast nature of radio transmission. The broadcast operation, as a fundamental service in MANETs, will cause the broadcast storm problem if the forward nodes are not carefully managed. It is a major challenge to reduce broadcast redundancy while still providing high delivery ratio for each broadcast packet in a dynamic environment. In this paper, we propose a simple broadcast algorithm to provide high delivery ratio. Among the 1-hop neighbors of the sender, only selected forward nodes will send acknowledgements to confirm their receipt of the packet. Forward nodes are selected in such a way that all the senders 2-hop neighbors are covered. Moreover, no acknowledgment is needed from non-forward 1-hop neighbors, each of which is covered by at least two forward neighbors. The sender waits for the acknowledgements from all of its forward nodes. If not all acknowledgments are received, the sender will resend the packet until the maximum number of retries is reached. Simulation results show that the algorithm has high delivery ratio and low end-to-end delay for a broadcast operation.","Intelligent networks,
Mobile ad hoc networks,
Radio broadcasting,
Storms,
Redundancy,
Computer network reliability,
Computer science,
Reliability engineering,
Delay,
Mobile communication"
TCPW with bulk repeat in next generation wireless networks,"Wireless links are error-prone. In next generation wireless networks, link capacities will still grow. It is known that in such configurations the performance of current TCP variants degrades severely. In this paper we propose a new transmission strategy for TCP in high error situations- bulk repeat (BR). We apply BR to TCP westwood (TCPW). BR has only three sender-side modifications to TCP, i.e. bulk retransmission, fixed retransmission timeout and intelligent window adjustment. BR permits efficient recovery from multiple losses in the same congestion window. To discriminate error from congestion loss, a loss discrimination algorithm (LDA), based on spike and rate gap threshold, is used. Simulation results in wireless network scenarios show that TCPW BR improves throughput performance up to an order of magnitude over TCPW and newreno when the error rate is high (>5%). The results also show that TCPW BR has satisfactory fairness and friendliness to TCP newreno.","Intelligent networks,
Next generation networking,
Wireless networks,
Linear discriminant analysis,
Degradation,
Throughput,
Computer science,
Computer errors,
Error analysis,
Error correction"
Genetic algorithm based test data generator,"Effective and efficient test data generation is one of the major challenging and time-consuming tasks within the software testing process. Researchers have proposed different methods to generate test data automatically, however, those methods suffer from different drawbacks. In this paper we present a genetic algorithm-based approach that tries to generate a test data that is expected to cover a given set of target paths. Our proposed fitness function is intended to achieve path coverage that incorporates path traversal techniques, neighborhood influence, weighting, and normalization. This integration improves the GA performance in terms of search space exploitation and exploration, and allows faster convergence. We performed some experiments using our proposed approach, where results were promising.","Genetic algorithms,
Software testing,
Automatic testing,
Logic testing,
Computer science,
Petroleum,
Minerals,
Space exploration,
System testing,
Convergence"
The DISCIPLE system for collaboration over the heterogeneous Web,"With the proliferation of mobile devices we witness an increasing demand for supporting collaboration among users working in the field and in the office. A key component for collaboration in this domain is sharing and manipulation of information using very different display capabilities on the diverse devices. We present a system based on a distributed repository of shared data objects and a client-server based infrastructure. The system is robust to intermittent connections, and a mixture of slow and fast links. To preserve bandwidth, application-specific data distribution agents decide what data to send to the clients. We also present a framework for building collaborative applications for clients with different display and processing capabilities. We describe example applications implemented both as Java applets to run in Web browsers and as Java spotlets to run on Palm OS based handheld computers. Using these applications we evaluated the framework and the results show that the framework is scaleable, offers good performance and has a high degree of code reusability.","Collaboration,
Collaborative work,
Bandwidth,
Application software,
Java,
Computer displays,
Collaborative software,
Information processing,
Robustness,
Operating systems"
Making the most of two heuristics: breaking transposition ciphers with ants,"Multiple anagramming is a general method for the cryptanalysis of transposition ciphers, and has a graph theoretic representation. Inspired by a partially mechanised approach used in World War II, we consider the possibility of a fully automated attack. Two heuristics based on measures of natural language are used - one to recognise plaintext, and another to guide construction of the secret key. This is shown to be unworkable for cryptograms of a certain difficulty due to random variation in the constructive heuristic. A solver based on an ant colony optimisation (AGO) algorithm is then introduced, increasing the range of cryptograms that can be treated; the pheromone feedback provides a mechanism for the recognition heuristic to correct the noisy constructive heuristic.","Cryptography,
Computer science,
Feedback,
Natural languages,
Ant colony optimization,
Colored noise,
History,
Error correction,
Frequency,
Geographic Information Systems"
Application of mobile agents to robust teleoperation of internet robots in nuclear decommissioning,"Nuclear decommissioning involves the characterisation of hazardous and contaminated environments. Robot characterisation systems have been developed to reduce the risk to human operatives, however their efficiency is limited. Coming decades will see a substantial increase in decommissioning globally as a large number of nuclear facilities are due to reach the end of their useful life. It is desirable that robot characterisation systems meet this increase in demand by becoming more efficient. This paper describes an architecture that makes use of advances in computer science including mobile agent technology, which we believe will offer improved efficiency over existing robot characterisation systems.","Mobile agents,
Robustness,
Internet,
Mobile robots,
Computer architecture,
Computer science,
Humans,
Decontamination,
Waste management,
Pollution measurement"
HMM based handwritten text recognition using biometrical data acquisition pen,Development of new text and graphical input devices is considered to be important part of human-computer interaction by many researchers worldwide. The paper presents our experience with the text recognition methods that we have developed for a new designed electronic pen that produces signals corresponding to the movement of the pen on paper. Signals are described by a set of primitives and hidden Markov models are used for word recognition. Results of tests are discussed as well as other possible application areas of our electronic pen.,"Hidden Markov models,
Text recognition,
Bioinformatics,
Character recognition,
Writing,
Handwriting recognition,
Data acquisition,
Computer science,
Data engineering,
Signal design"
"Deno: a decentralized, peer-to-peer object-replication system for weakly connected environments","This paper presents the design, implementation, and evaluation of the replication framework of Deno, a decentralized, peer-to-peer object-replication system targeted for weakly connected environments. Deno uses weighted voting for availability and pair-wise, epidemic information flow for flexibility. This combination allows the protocols to operate with less than full connectivity, to easily adapt to changes in group membership, and to make few assumptions about the underlying network topology. We present two versions of Deno's protocol that differ in the consistency levels they support. We also propose security extensions to handle a class of malicious actions that involve misrepresentation of protocol information. Deno has been implemented and runs on top of Linux and Win32 platforms. We use the Deno prototype to characterize the performance of the Deno protocols and extensions. Our study reveals several interesting results that provide fundamental insight into the benefits of decentralization and the mechanics of epidemic protocols.","Protocols,
Data security,
Network operating systems,
Replicated databases,
Synchronization"
Certificate management in ad hoc networks,"Various types of certificates are basic tools of modern cryptography and network security. They are used in various protocols, in the form of public key identity certificates, binding a key to its owner or in the form of attribute certificates, being a proof of rights and capabilities of their owner. Management of certificates (creation, distribution, verification, and revocation) is dependent on a certification infrastructure comprising various certification authorities, protocols, and policies. In this paper we consider usage and management of certificates in open, ad hoc networks. Ad hoc networks differ from fixed, wired networks in several important aspects, one of them being that access to the Internet is not always available. This significantly influences certificate management protocols since online access to various certificate system resources (CA certificates, CRL, etc) is not always available. We specify security requirements and constraints in such environments and outline potential solutions for adaptation of certificate management protocols to these new network environments.","Intelligent networks,
Ad hoc networks,
Certification,
Public key,
Access protocols,
IP networks,
Authentication,
File servers,
Computer network management,
Computer science"
"The Nordugrid production grid infrastructure, status and plans","Nordugrid offers reliable grid services for academic users over an increasing set of computing & storage resources spanning through the Nordic countries Denmark, Finland, Norway and Sweden. A small group of scientists has already been using the Nordugrid as their daily computing utility. In the near future we expect a rapid growth both in the number of active users and available resources thanks to the recently launched Nordic grid projects.We report on the present status and short term plans of the Nordic grid infrastructure and describe the available and foreseen resources, grid services and our forming user base.","Production,
Grid computing,
Testing,
Middleware,
Materials science and technology,
Mathematics,
Computer science,
Physics computing,
Large-scale systems,
Data handling"
JIVE: visualizing Java in action demonstration description,"Dynamic software visualization should provide a programmer with insights as to what the program is doing. Most current dynamic visualizations either use program traces to show information about prior runs, slow the program down substantially, show only minimal information, or force the programmer to indicate when to turn visualizations on or off. We have developed a dynamic Java visualizer that provides a view of a program in action with low enough overhead that it can be used almost all the time by programmers to understand what their program is doing while it is doing it.","Java,
Data visualization,
Programming profession,
Yarn,
Libraries,
Packaging,
Displays,
Computer science,
Software systems,
Feedback"
A hybrid approach to news video classification multimodal features,"This paper presents a hybrid approach to the classification of news video story. Most of current works on news story classification utilize the multimodal features in a uniform manner. However, the reliability of audio-visual confidence is much lower than that of text, which may evidently lower-down the performance of the classification. We proposed a decision strategy mainly depends on the evidence from text classifiers with extra assistance of audio-visual clues. In our approach, SVMs for text features and GMMs for audio-visual features are first built for each category and then used to compute text and audio-visual confidence vectors respectively. To make final decision, a text-biased decision strategy is proposed to combine these multimodal confidence vectors. To validate the performance, text-based classification and SVM-based meta-classification methods are compared on large-scale news stories from TV programs, and our proposed hybrid approach achieves the best overall performance.","Feature extraction,
Text categorization,
Large-scale systems,
Multimedia communication,
Support vector machines,
Support vector machine classification,
Computer science,
TV,
Content based retrieval,
Broadcasting"
Transient fairness of optimized end-to-end window control,"The paper is concerned with optimizing end-to-end window controls that achieve proportional fairness in the long run, and then investigating their transient or short-term fairness. An abstracted stochastic model of a bottlenecked connection is employed and the window control is designed to minimize the buffer queue variance while keeping the mean queue level at a target value, taking the round-trip delay into account. A generalized window control aimed at reducing window size fluctuations is also derived. The effects of both the target queue length and a weighting parameter on the short-term fairness, as measured by short-term fairness indices calculated over short time intervals, are investigated using ns-2 simulations.","Proportional control,
Size control,
Computer science,
Fluctuations,
Stability,
Optimal control,
Australia,
Stochastic processes,
Delay,
Length measurement"
A secure service discovery protocol for MANET,"Service discovery technologies are exploited to enable services to advertise their existence in a dynamic way, and can be discovered, configured and used by other devices with minimum manual efforts. It plays an essential role in future network scenarios especially with development of mobile ad hoc network (MANET) and emergence of pervasive computing. Because MANET allows these devices to communicate dynamically without fixed infrastructure and centralized administration, it gives rise to the challenges of the service discovery techniques. In this paper, we present a dynamic service discovery infrastructure that uses XML to describe services and match using the semantic content of service descriptions for MANET. We believe that the architecture we have designed is a necessary component of service discovery in non-infrastructure network by further exploring the secure and performance issues of this infrastructure.","Mobile ad hoc networks,
Routing protocols,
Computer science,
Educational institutions,
Mobile computing,
Computer architecture,
Manuals,
Pervasive computing,
XML,
Security"
A gateway between the SCP-ECG and the DICOM supplement 30 waveform standard,"Nowadays, the large-scale deployment of electronic health record systems and eHealth services has to face with a real multi-vendor environment. Even if each manufacturer supports an existing standard for communication and storage of ECG data, the large number of co-existing ECG standards is a major problem. In this direction, an online service operating as a gateway between SCP-ECG, the European standard for communication and storage of resting ECGs, and the DICOM waveform standard (Supplement 30), a common format for the communication of time series data including vital signs and ECGs, has been developed. The open availability of this service in the OpenECG portal is expected to facilitate digital ECG interoperability and contribute to the harmonization of ECG standards.","DICOM,
Electrocardiography,
Communication standards,
Standards development,
Computer science,
Medical services,
Frequency synchronization,
Large-scale systems,
Manufacturing,
Portals"
Coping with uncertainty,,"Uncertainty,
Collaboration,
Information security,
Mobile computing,
Computer science"
How technological and vocational education can prosper in the 21st century,"With the rapid development of modern electronics and the information-technology industry, Taiwan has entered the ranks of developed countries. It is important that the technological and vocational education system constantly evolves to meet the needs of worldwide economic development, the new demand of highly skilled manpower, the changing industrial structure, the social/cultural changes, and the continued progress of modern technology.","Educational technology,
Educational institutions,
Educational programs,
Manufacturing industries,
Educational products,
Computer science education,
Physics education,
Continuing education,
Globalization,
Switches"
Efficient subsequence matching for sequences databases under time warping,"It has been found that the technique of searching for similar patterns among time series data is very important in a wide range of scientific and business applications. Most of the research works use Euclidean distance as their similarity metric. However, dynamic time warping (DTW) is a more robust distance measure than Euclidean distance in many situations, where sequences may have different lengths or have patterns which are out of phase in the time axis. Unfortunately, DTW does not satisfy the triangle inequality, so spatial indexing techniques cannot be applied. In this paper, we present a method that supports dynamic time warping for subsequence matching within a collection of sequences. Our method takes full advantage of the ""sliding window"" approach and can handle queries of arbitrary length.","Databases,
Euclidean distance,
Data engineering,
Length measurement,
Computer science,
Application software,
Robustness,
Phase measurement,
Time measurement,
Indexing"
"On-line intrusion detection and attack prevention using diversity, generate-and-test, and generalization","We have built a system for protecting Internet services to securely connected, known users. It implements a generate-and-test approach for on-line attack identification and uses similarity rules for generalization of attack signatures. We can immediately protect the system from many variants of previously unknown attacks without debilitating waits for anti-virus updates or software patches. Unique to our approach is the use of diverse process pairs not only for isolation benefits but also for detection. The architecture uses the comparison of outputs from diverse applications to provide a significant and novel intrusion detection capability. With this technique, we gain the benefits of n-version programming without its controversial disadvantages. The isolation of intrusions is mainly achieved with an out-of-band control system that separates the primary and backup system. It also initiates attack diagnosis and blocking, and recovery, which is accelerated by continual repair.",
Supervised classification for video shot segmentation,"In this paper, we explore supervised classification methods for video shot segmentation. We transform the temporal segmentation problem into a multi-class categorization issue. This approach provides a uniform framework for using different kinds of features extracted from the video and for detecting various types of shot boundaries. The approach utilizes manual labeled training data and a simple classification structure, which eliminates arbitrary thresholds and achieves more reliable estimation than previous threshold-based methods. Contrastive experiments on 13 videos (/spl sim/4 hours) show excellent performance on the 2001 TREC video track shot classification task in terms of precision and recall.","Gunshot detection systems,
Streaming media,
Cameras,
Video compression,
Training data,
Supervised learning,
Histograms,
Detectors,
Computer science,
Feature extraction"
A model-driven approach to non-functional analysis of software architectures,"We present an approach to managing formal models using model driven architecture (MDA) technologies that deliver analysis techniques through integration with the design tools and repositories that practitioners use. Expert modeling knowledge is captured in domain-specific languages and meta-model constraints. These are represented using UML (Unified Modeling Language) and collocated with designs and analysis models, providing a flexible and visible approach to managing semantic associations. The approach relies on standards to permit deployment in multiple tools. We demonstrate our approach with an example in which queuing-network models are associated with UML design models to predict average case performance.","Computer architecture,
Unified modeling language,
Object oriented modeling,
Software architecture,
Predictive models,
Performance analysis,
Computer science,
Educational institutions,
Electronic mail,
Technology management"
Building environments for end-user development and tailoring,"Software shaping workshops (SSWs) described in this paper are software environments designed to support various activities of end-user development (EUD) and tailoring. A design methodology to create easy-to-develop-and-tailor visual interactive systems that are organised as SSWs is illustrated. Users of an interactive system are in many cases experts in some domain different from computer science, who need to perform some task with the aid of the computer system. The design methodology allows users to directly collaborate to the system design and tailoring process to face co-evolution of user's and systems. The strategy feasibility is discussed, outlining its implementation through a Web-based prototype.","Application software,
Design methodology,
Software systems,
Collaboration,
Human computer interaction,
Usability,
Software maintenance,
Interactive systems,
Computer science,
Prototypes"
Performance and effectiveness analysis of checkpointing in mobile environments,"Many mathematical models have been proposed to evaluate the execution performance of an application with and without checkpointing in the presence of failures. They assume that the total program execution time without failure is known in advance, under which condition the optimal checkpointing interval can be determined. In mobile environments, application components are distributed and tasks are computed by sending and receiving computational and control messages. The total execution time includes communication time and depends on multiple factor, such as heterogeneous processing speeds, link bandwidth, etc., making it unpredictable during different executions. However, the number of total computational messages received is usually unchanged within an application. Another special factor that should be considered for checkpointing purpose is handoff, which often happens in mobile networks. With these observations, we analyze application execution performance and average effectiveness, and introduce an equi-number checkpointing strategy. We show how checkpointing and handoff affect performance and effectiveness metrics, determine the conditions when checkpointing is beneficial, and calculate the optimal checkpointing interval for minimizing the total execution time and maximizing the average effectiveness in mobile environments.","Performance analysis,
Checkpointing,
Mobile computing,
Distributed computing,
Bandwidth,
Mathematical model,
Computer networks,
Portable computers,
Computer science,
Application software"
A cooperative nearest neighbours topology control algorithm for wireless ad hoc networks,"In this paper, we introduce a simple distributed algorithm that assigns appropriate individual transmission powers to devices in a wireless ad hoc network. In contrast to many other proposed algorithms, it does not depend on special hardware. It requires only local neighbourhood information and therefore avoids flooding information throughout the network. Finally, the cooperative nature of the algorithm avoids that devices cause excessive interference by using unnecessarily high transmission powers. We show by means of simulation that the topologies created by this algorithm without any global knowledge are as effective as topologies resulting from a good choice of a common transmission power (which would require global knowledge) in terms of the achievable throughput.","Network topology,
Ad hoc networks,
Mobile ad hoc networks,
Interference,
Hardware,
Power control,
Computer science,
Distributed algorithms,
Throughput,
Routing"
Pyramidwise structuring for soccer highlight extraction,"Fast browsing video contents not only is an important research issue, but also has a variety of potential applications, especially for sports videos. In this paper, we propose a practical solution to extract highlights from soccer videos, which is based on the structure analysis of broadcast soccer video. First, the broadcast soccer video is structured into a soccer pyramid. Then, soccer highlights can be extracted in a flexible manner by using such a soccer pyramid structure. Besides, in order to obtain soccer pyramid, a condensation approach to soccer video and the corresponding structure extraction methods are also proposed. Experiments have demonstrated the effectiveness, efficiency and robustness of the proposed approaches to soccer video structuring and highlight extraction.","Broadcasting,
Games,
Robustness,
Hidden Markov models,
Multimedia communication,
Event detection,
Motion detection,
Feature extraction,
Computer science,
Educational institutions"
Smart Baton System: a universal remote control system in ubiquitous computing environment,"This paper describes a universal remote control system - Smart Baton System. This system enables users to explicitly and easily choose an appliance with a laser pointer, and provides users with an appropriate user interface that is customized for each appliance. Furthermore, with user authentication, this system allows an appliance to provide differentiated service to each user.","Control systems,
Ubiquitous computing,
Home appliances,
Laser beams,
User interfaces,
Optical control,
Handheld computers,
Information science,
Paper technology,
Laser theory"
On a partnership between software industry and academia,"This paper discusses a role for industry in software engineering education, specifically presenting a university-industry partnership between the Cardiac Rhythm Management (CRM) organization at the Guidant Corporation and Embry-Riddle Aeronautical University (ERAU). The focus of the partnership is technology transition. The partnership involves fostering students' professional development, providing students experience solving realworld problems, and exploring modern directions of software engineering. The critical component of the partnership is a student-oriented research laboratory. After discussing the background and history of the project, we focus on the partnership's accomplishments. These include facilitating the transition of graduates from student to employee by developing in them extended software engineering skills and in-depth understanding of the application domain.","Computer industry,
Software engineering,
Aerospace industry,
Application software,
Humans,
Engineering management,
Medical control systems,
Control systems,
Defense industry,
Personnel"
"Extraction, matching and pose recovery based on dominant rectangular structures","Man-made environments possess many regularities which can be efficiently exploited for image based rendering as well as robotic visual navigation and localization tasks. In this paper we present an approach for automatic extraction of dominant rectangular structures from a single view and show how they facilitate the recovery of camera pose, planar structure and matching across widely separated views. Since in the presented approach the rectangular hypothesis formation is based on a higher level information encoded by the presence of orthogonal vanishing directions, the dominant rectangular structures can be detected and matched despite the presence of multiple repetitive structures often encountered in a variety of buildings. The different stages of the approach are demonstrated on various examples of images of indoors and outdoors structured environments.","Cameras,
Buildings,
Solid modeling,
Humans,
Computer science,
Rendering (computer graphics),
Robotics and automation,
Navigation,
Data mining,
Robot vision systems"
Text mining with information-theoretic clustering,"Motivated by the success of hybrid information-retrieval algorithms, the authors report on the development of their hybrid clustering scheme. Scheme experiments on data in a reduced vector space model indicate a higher performance level over several existing clustering algorithms.","Data mining,
Text mining,
Clustering algorithms,
Partitioning algorithms,
Probability distribution,
Mutual information,
Random variables,
Information retrieval,
Bioinformatics,
Singular value decomposition"
A probabilistic method for foreground and shadow segmentation,"This paper presents a probabilistic method for foreground segmentation that distinguishes moving objects from their cast shadows in monocular indoor image sequences. The models of background, shadow, and edge information are set up and adaptively updated. A Bayesian framework is proposed to describe the relationships among the segmentation label, background, intensity, and edge information. A Markov random field is used to boost the spatial connectivity of the segmented regions. The solution is obtained by maximizing the posterior probability density of the segmentation field.","Image segmentation,
Layout,
Image sequences,
Image edge detection,
Markov random fields,
Video sequences,
Cameras,
Pixel,
Computer science,
Bayesian methods"
Towards a novel framework for the assessment of enterprise application integration packages,"In addressing enterprise integration problems, a diversity of technologies such as CORBA and XML were promoted, yet no single integration technology solves all integration problems. As a result, a new generation of software called enterprise application integration (EAI) is emerging to addresses many integration problems by combining a diversity of integration technologies (e.g. message brokers, adapters, XML). Since EAI is a new research area, there is an absence of literature discussing issues like its adoption, evaluation and implementation. This paper, examines the application of two frameworks for the evaluation of EAI packages in the practical arena. In doing so, the authors use case study strategy to investigate integration issues. Empirical data derived from the case study suggest additions to the two evaluation frameworks. Therefore, the authors revised and extend previous works by proposing a evaluation framework for the assessment of EAI packages. The proposed framework makes contribution at two levels. First, at the conceptual level, as it incorporates criteria identified separately in previous studies as evaluation criteria. The proposed framework can be used as a decision-making tool and, supports management when taking decisions regarding the adoption of EAI. Additionally, it can be used by researchers to analyse and understand the capabilities of EAI packages.","Packaging,
Application software,
Enterprise resource planning,
Information systems,
Diversity reception,
XML,
Software packages,
Marine technology,
Software quality,
Computer networks"
Enhanced dominant pruning applied to the route discovery process of on-demand routing protocols,"Dominant pruning (DP) is a distributed connected dominating-set algorithm that can be used for reducing the impact of flooding in wireless ad hoc networks. We propose an enhanced dominant pruning (EDP) approach to be used in the route discovery process of on-demand routing protocols. To show the benefits of EDP, we integrate EDP into the ad-hoc on-demand distance vector (AODV) protocol. We present detailed simulation results showing that our approach improves standard AODV in most aspects, and that it is simple and easy to implement. Our approach is compared against AODV and OLSR, as good representatives of on-demand and proactive routing for ad-hoc wireless networks.","Routing protocols,
Broadcasting,
Floods,
Wireless networks,
Computer science,
Computer networks,
Distributed computing,
Mobile ad hoc networks,
Relays,
Access protocols"
Monitoring of polyethylene wear in nonmetal-backed acetubular cups by digitized anteroposterior pelvic radiography,"The aim of this study was to assess polyethylene wear in a total hip prosthesis by digitized radiography of the whole pelvis in the anteroposterior (AP) plane. The three-dimensional (3-D) pose of the nonmetal-backed acetubular cup, materialized by its metal ring and the femoral head made of metal or ceramic, was estimated using iterative algebraic algorithms with inner bias correction and bootstrapping for variance reduction. Points of interest were obtained by maximizing the correlation between sampled density profiles and 3-D geometric models degraded by the modulation transfer function (MTF) of the radiographic system and the film scanner. The error in the maximal correlation estimate were inferred from noise power spectra (NPS) and allowed the calculation of the point covariance matrix. Both NPS and MTF were modeled for each stage and estimated using least-square fitting of the overall NPS model to the autospectral density function calculated in stationary regions. Comparison of the radiographic time series was made possible by the high accuracy level and 3-D matching from the cup orientation. The feasibility of the full 3-D measurement, the assumption of negligible lateral wear and its influence on AP wear are discussed on simulated and real radiographic data.","Monitoring,
Polyethylene,
Radiography,
Power system modeling,
Hip,
Prosthetics,
Pelvis,
Ceramics,
Iterative algorithms,
Solid modeling"
Accelerating products under due-date oriented dispatching rules in semiconductor manufacturing,"In semiconductor manufacturing facilities, there is often the need to speed up certain product types. This is usually done by either assigning higher priorities or by reducing due dates. We study the effects of accelerating one product type by a tighter due date on the on-time delivery performance of the other products manufactured. It turns out that the results depend on the considered factory, its load, and the accelerated product. As a consequence, it will be hard for production planners to find simple rules of thumb for the effects of accelerating products. In general, detailed simulation experiments will be required.","Acceleration,
Dispatching,
Semiconductor device manufacture,
Production facilities,
Chromium,
Pulp manufacturing,
Electronics industry,
Virtual manufacturing,
Computer aided manufacturing,
Computer science"
The feature signatures of evolving programs,"As programs evolve, their code increasingly becomes tangled by programmers and requirements. This mosaic quality complicated program comprehension and maintenance. Many of these activities can benefit from viewing the program as a collection of features. We introduce an inexpensive and easily comprehensible summary of program changes called the feature signature and investigate its properties. We find a remarkable similarity in the nature of feature signatures across multiple nontrivial programs, developers and magnitude changes. This indicates that feature signatures are a meaningful notion worth studying. We then show numerous applications of feature signatures to software evolution, establishing their utility.","Batteries,
System testing,
Programming profession,
Software tools,
Documentation,
Computer science,
Application software,
Humans,
Software systems,
Control systems"
A Bluetooth loop scatternet formation algorithm,"Bluetooth is a promising new wireless technology that enables portable devices to form short-range wireless ad hoc networks. In this paper, we present a new, distributed Bluetooth scatternet formation algorithm, called loop scatternet formation, which forms scatternets with slave/slave bridges only. In addition to meeting the criteria of maintaining connectivity, minimizing the number of piconets and the maximum degree of devices, the proposed algorithm formalizes the notion of network diameter and node contention. The loop scatternet thus formed incurs a much smaller network diameter and the number of node pairs for which a device has to serve, as a relay node is significantly smaller than that in the other types of scatternets. To validate the design, we derive the bounds of the number of piconets, the network diameter, and the maximum node contention. We also conduct ns-2 simulation to evaluate the performance of loop scatternets. Both analytical and simulation results validate the desirable features of loop scatternets.","Bluetooth,
Scattering,
Personal area networks,
Bridges,
Protocols,
Mobile ad hoc networks,
Relays,
Analytical models,
Master-slave,
Computer science"
Near-optimal approaches for shared-path protection in WDM mesh networks,"This paper investigates the problem of dynamic shared-path-protected lightpath provisioning in optical mesh networks employing wavelength-division multiplexing (WDM). We prove that the problem of finding an eligible pair of working and backup paths for a new lightpath request requiring shared-path protection under the current network state is NP-complete. We develop a heuristic, called CAFES, to compute a feasible solution and an algorithm, called OPT, to optimize resource consumption for a given solution. The merits of our approaches are that they capture the essence of shared-path protection and approach to optimal solutions without enumerating paths. We evaluate the effectiveness of our heuristics and the results are found to be promising.","Protection,
Intelligent networks,
Wavelength division multiplexing,
WDM networks,
Mesh networks,
Optimized production technology,
Computer networks,
Multiprotocol label switching,
Computer science,
Optical fiber networks"
Coordination in pervasive computing environments,"Computer science and engineering nowadays appears to be challenged (and driven) by technological progress and quantitative growth. Among the technological progress challenges are advances in submicron and system-on-a-chip designs, communication technologies, micro-electro-mechanical systems, nano and materials sciences. The cast pervasion of global networks, the growing availability of wireless communication technologies for the wide, local and personal area, and the evolving ubiquitous use of mobile and embedded information and communication technologies are indicators for accelerated quantitative growth. We perceive a shift from the ""one person with one computer"" paradigm, which is based on explicit human machine interaction, towards a ubiquitous and pervasive computing paradigm, in which implicit interaction and cooperation is the primary mode of computer supported activity. This, however, poses serious challenges to the conceptual architectures of computing, and related engineering disciplines in computer science.","Pervasive computing,
Communications technology,
Computer science,
Materials science and technology,
System-on-a-chip,
Microelectromechanical systems,
Wireless communication,
Acceleration,
Humans,
Computer architecture"
Nash equilibria of a generic networking game with applications to circuit-switched networks,"A generic mechanism for end-user transmission rate control into a differentiated services Internet is formulated and basic results of corresponding Nash equilibria are proved. We consider specific examples of the mechanism including additive increase and multiplicative decrease inspired by present day TCP congestion control. For the example of users sharing access to a bandwidth resource via resizable provisioned label-switched paths (MPLS), we study the equilibria and the performance of the generic mechanism and give analytical results on convergence to equilibria. The fairness of the resulting equilibria when user demands exceed available network resources is also studied.","Circuits,
Bandwidth,
Multiprotocol label switching,
Convergence,
Internet,
Pricing,
Access control,
Application software,
Computer science,
IP networks"
The K-means clustering algorithm based on density and ant colony,"The ant algorithm is a new evolutional method, k-means and the density-cluster are familiar cluster analysis. In this paper, we proposed a new K-means algorithm based on density and ant theory, which resolved the problem of local minimal by the random of ants and handled the initial parameter sensitivity of k-means. In addition it combined idea of density and made the ants searching selectable. With the experiments it was proved that the algorithm we proposed improved the efficiency and precision of cluster.",
A general self-adaptive task scheduling system for non-dedicated heterogeneous computing,"The efforts to construct a national scale grid computing environment has brought unprecedented computing capacity. Exploiting this complex infrastructure requires efficient middleware to support the execution of a distributed application, composed of a set of subtasks, for best performance. This presents the challenge how to schedule these subtasks in shared heterogeneous systems. Current work has several limitations. Most scheduling systems are based on determined estimation of task completion time. Current application-level scheduling algorithms are too closely coupled with application internal structures. The application performance may suffer when some resources represent an abnormal usage pattern during applications execution. To address these issues, we develop a prototype of grid harvest service (GHS) to provide dynamic and self-adaptive task scheduling. Experimental results show GHS outperforms current systems in scheduling large applications in a non-dedicated heterogeneous environment.",Processor scheduling
Programming in a data factory,"Among the advantages of visual dataflow programming is that it can give the user a sense of location for the data in a computation. This can help novices build a mental picture of a program and its execution. This paper presents an experimental programming system called the data factory that uses a manufacturing metaphor to give data an even stronger sense of place. A key affordance for learners is the explicit display of every data object as it moves through a factory. The system provides facilities for handling streams of data and parallel operations, as well as basic operations on numerical values. The data factory supports low-level computations that might be studied by novices, but it also offers novel constructs that might invite the attention of others.","Production facilities,
Parallel processing,
Java,
Animation,
Robots,
Pulp manufacturing,
Cognitive science,
Belts,
Computer aided manufacturing,
Concurrent computing"
Generalized minimum distance iterative decoding of expander codes,"Recently, G. Zemor (see IEEE Trans. Inf. Theory, vol.47, p.835-7, 2001) proposed an improvement on the Sipser-Spielman analysis of expander codes (Sipser, M. and Spielman, D.A., IEEE Trans. Inf. Theory, vol.42 , p.1710-22, 1996) and presented a linear-time iterative decoder that can correct a number of errors up to approximately 1/4 the known lower bound on the minimum distance of the code. We propose an improvement on Zemor's decoder for F=GF(2), with the number of correctable errors becoming close to half the lower bound on the minimum distance. The improvement is obtained by inserting into the decoding algorithm features akin to generalized minimum distance decoding of concatenated codes.","Iterative decoding,
Graph theory,
Error correction,
Eigenvalues and eigenfunctions,
Iterative algorithms,
Computer science,
Computer errors,
Error correction codes,
Concatenated codes,
Hamming weight"
Exploring context switching and cognition in dual-view coordinated visualizations,"Multiple-view visualizations are useful for finding patterns in complex data sets, but little research has been done on how they are used. We performed a controlled experiment to study cognitive strategies and context switching by using a combination of visualizations and different task types as independent variables, and collecting qualitative and quantitative data. To collect the data, paper-based tests, logging of participants' interactions, eye-tracking, think-aloud techniques, and video recordings were used. Unlike suggestions in the literature, our results show that when considering dual-view visualizations, the time cost for context switching may not be significant, and similar visualizations may actually cause more interference. Furthermore, orthogonal combinations appear to aid users in recognizing patterns. Focusing attention and analogical reasoning on spatial relationships are important cognitive abilities as well.","Cognition,
Data visualization,
Video recording,
Guidelines,
Costs,
Pattern recognition,
Time measurement,
Scattering,
Computer science,
Computer industry"
Using destination-set prediction to improve the latency/bandwidth tradeoff in shared-memory multiprocessors,"Destination set prediction can improve the latency/bandwidth tradeoff in shared memory multiprocessors. The destination set is the collection of processors that receive a particular coherence request. Snooping protocols send requests to the maximal destination set (i.e., all processors), reducing latency for cache to cache misses at the expense of increased traffic. Directory protocols send requests to the minimal destination set, reducing bandwidth at the expense of an indirection through the directory for cache to cache misses. Recently proposed hybrid protocols tradeoff latency and bandwidth by directly sending requests to a predicted destination set. We explore the destination set predictor design space, focusing on a collection of important commercial workloads. First, we analyze the sharing behavior of these workloads. Second, we propose predictors that exploit the observed sharing behavior to target different points in the latency/bandwidth tradeoff. Third, we illustrate the effectiveness of destination set predictors in the context of a multicast snooping protocol. For example, one of our predictors obtains almost 90% of the performance of snooping while using only 15% more bandwidth than a directory protocol (and less than half the bandwidth of snooping).","Delay,
Bandwidth,
Broadcasting,
Multicast protocols,
Multiprocessing systems,
Multiprocessor interconnection networks,
Computer science,
Scholarships,
Sun,
Computer architecture"
An FPGA-based re-configurable 24-bit 96kHz sigma-delta audio DAC,"This paper presents a reconfigurable sigma-delta audio Digital-to-Analog Converter (DAC) which is suitable for embedded FPGA applications. The Sigma-Delta Modulator (SDM) design can be configured as a 3rd or 5th order SDM and allows different input word lengths. Different input sampling rates are also entertained by employing a programmable interpolator. The DAC accepts 16-/18-/20-/24-bit PCM data at sampling rates of 32/44.1/48/88.2/96 kHz for applications in CD, SACD and DVD audio.","Delta-sigma modulation,
Finite impulse response filter,
Sampling methods,
Interpolation,
Field programmable gate arrays,
Frequency modulation,
Phase change materials,
Analog-digital conversion,
Computer science,
Consumer electronics"
Capturing a convex object with three discs,"This paper addresses the problem of capturing an arbitrary convex object P in the plane with three congruent disc-shaped robots. Given two stationary robots in contact with P, we characterize the set of positions of a third robot that prevent P from escaping to infinity and show that the computation of this so-called capture region reduces to the resolution of a visibility problem. We present two algorithms for solving this problem and computing the capture region when P is a polygon and the robots are points (zero-radius discs). The first algorithm is exact and has polynomial-time complexity. The second one uses simple hidden-surface removal techniques from computer graphics to output an arbitrarily accurate approximation of the capture region; it has been implemented and examples are presented.","Robot sensing systems,
Robotics and automation,
H infinity control,
Mobile robots,
Computer graphics,
Kinematics,
Computer science,
Polynomials,
Grippers,
Fixtures"
Feedback-free packet loss recovery for video multicast,"In video streaming over multicast networks, error recovery is essential to alleviate the effect of packet loss. In this paper, we study a feedback-free recovery scheme which combines the strength of FEC (namely, a parity packet can repair any lost packet) and pseudo-ARQ (namely, incremental recovery). To account for the receiver heterogeneity, the server multicasts layered video streams for the receivers to join. For each layer, the receivers may join dynamically additional multicast channels of FEC and pseudo-ARQ packets to recover local losses. In order to offer quality video, we address the following issues: 1) ""menu creation"": given a certain maximum error rate after correction (i.e., residual error rate), what is the combination of FEC and pseudo-ARQ packets for the server to send so as to minimize a target receiver's bandwidth; and 2) ""menu selection"": given the server's menu, what's the combination of FEC and pseudo-ARQ packets for the receiver to join so as to minimize its residual error rate given its local probability, bandwidth and loss pattern. We present the analysis of the scheme and show that our scheme can substantially reduce a receiver's residual error rate as compared with pure FEC or pure pseudo-ARQ alone (by cutting it as much as half).","Forward error correction,
Automatic repeat request,
Error analysis,
Streaming media,
Feedback,
Network servers,
Bandwidth,
Videoconference,
Delay,
Computer science"
Deriving process networks from weakly dynamic applications in system-level design,"We present an approach to the automatic derivation of executable process network specifications from weakly dynamic applications. We introduce the notions of dynamic single assignment code, approximated dependence graph, and linearly bounded sets to model and capture weakly dynamic (data-dependent) behavior of applications at the task-level of abstraction. Process networks are simple parallel processing models that match the emerging multiprocessor architectures in the sense that the mapping of process network specifications of applications onto multiprocessor architectures can be done in a systematic and transparent way.","Intelligent networks,
System-level design,
Mathematical model,
Computational modeling,
Application software,
Concurrent computing,
Computer networks,
Computer science,
Embedded computing,
Design automation"
An application of multi-agent coordination techniques in air traffic management,"Air traffic management (ATM) involves collaborative work from several actors: traffic flow managers, air traffic controllers (controllers) and pilots. The ever-increasing demand for commercial air travel poses great challenges to today's airspace-centered ATM system in which each controller only undertakes the responsibility to control the aircraft flying through her/his own airspace (sector). Any aircraft bunching occurring in a sector has the potential to cause the risk of instant traffic overload in another sector. It is therefore essential to further decentralize the system by redistributing the responsibility as well as workload. This paper presents our research to support this redistribution by setting up a methodological framework using multi-agent coordination techniques. A recently identified problem, i.e. real-time traffic synchronization, is chosen as the first application.","Air traffic control,
Aircraft,
Control systems,
Collaborative work,
Aerospace control,
Application software,
Laboratories,
Computer science,
Geometry,
Strategic planning"
Modeling parallel applications performance on heterogeneous systems,"The current technologies have made it possible to execute parallel applications across heterogeneous platforms. However, the performance models available do not provide adequate methods to calculate, compare and predict the applications performance on these platforms. In this paper, we discuss an enhanced performance evaluation model for parallel applications on heterogeneous systems. In our analysis, we include machines of different architectures, specifications and operating environments. We also discuss the enabling technologies that facilitate such heterogeneous applications. The model is then validated through experimental measurements using an agent-based parallel Java system, which facilitates simultaneous utilization of heterogeneous systems for parallel applications. The model provides good evaluation metrics that allow developers to assess and compare the parallel heterogeneous applications performances.","Java,
Application software,
Workstations,
Local area networks,
Computer science,
Predictive models,
Performance evaluation,
Parallel programming,
Analytical models,
Resource management"
Computer vision in undergraduate education: modern embedded computing,"Computer vision has historically been taught as a graduate subject since few examples of the discipline were being practiced in mainstream engineering. In recent years, the incorporation of multimedia into embedded devices has drawn some vision topics into mainstream attention. Examples of consumer products include digital video recorders, cellular phones, and automobile collision-avoidance systems. This paper describes the development of an undergraduate course that incorporates some vision topics into the larger context of embedded computing. Traditional topics, such as processor types, dynamic power management, and real-time scheduling, are taught alongside relevant vision topics, such as codecs, concurrent interfaces, and multimedia signal acquisition, storage, and rendering. In lab work, the students program hardware to operate as a digital video camera. While the primary goal for the course is to teach embedded computing, a secondary goal for the course is to entice students into graduate study in computer vision. However, a major developmental point was to justify the vision content in the context of how it serves the needs of students not opting for graduate study, as well as how the course would impact students working in other related graduate research areas.","Computer science education,
Machine vision,
Multimedia systems"
A cut-based algorithm for reliability analysis of terminal-pair network using OBDD,"In this paper, we propose an algorithm to construct the ordered binary decision diagram (OBDD) representing the cut function of a terminal-pair network. The algorithm recognizes isomorphic sub-problems and thus avoids redundant computations. The system reliability could be efficiently computed by the OBDD. Finally, we propose an approach to compute the importance measures for multiple components by traversing the OBDD only once. The correctness and the effectiveness of our approach are demonstrated by experiments on 30 benchmark networks. The experimental results on a 2-by-100 lattice network, which has 2/sup 99/ paths or 10,000 cuts, show an impressive improvement compared to the previous works using the sum of disjoint products method that have exponential complexity. The CPU time of our method, including the calculation of not only the reliability but also the importance measures, for a 100-stage lattice network is only about 0.24 seconds. Thus, this approach is very helpful for the reliability and sensitivity analysis of large networks.","Algorithm design and analysis,
Boolean functions,
Computer network reliability,
Computer networks,
Councils,
Partitioning algorithms,
Software algorithms,
Computer science,
Reliability engineering,
Power engineering and energy"
Querying distributed data through distributed ontologies: a simple but scalable approach,This framework for peer-to-peer data-sharing systems allows efficient query answering over a network of semantically related peers. A simple class-based language appropriate for practical applications defines peer schemas as hierarchies of atomic classes and mappings as inclusions of logical combinations of atomic classes.,"Ontologies,
Artificial intelligence,
Network servers,
Education,
Computer science,
Delta modulation,
Data mining,
Computer architecture,
Distributed computing,
Vocabulary"
Modular exponentiation using parallel multipliers,"A field programmable gate array (FPGA) semi-systolic implementation of a modular exponentiation unit, suitable for use in implementing the RSA public key cryptosystem is presented. The design is carefully matched with features of the FPGA architecture, utilizing embedded 18/spl times/18-bit multipliers on the FPGA and employing a carry save addition scheme. Using this architecture, a 1024-bit modular exponentiation can operate at 90 MHz on a Xilinx XC2V3000-6 device and perform a 1024-bit RSA decryption in 0.66 ms with the Chinese Remainder Theorem.","Field programmable gate arrays,
Public key cryptography,
Hardware,
Computer architecture,
Parallel processing,
Application specific integrated circuits,
Pipeline processing,
Clocks,
Public key,
Computer science"
Reproducible dependability benchmarking experiments based on unambiguous benchmark setup descriptions,,"Hardware,
Instruction sets,
Videoconference,
Instruments,
Councils,
Physics,
Chemistry,
Computer science,
Casting,
Biological system modeling"
Buffer evaluation in variable bandwidth channelization for videos,"In multimedia applications the storage and bandwidth are crucial resources. The requirement for bandwidth is more critical than storage in the sense that when bandwidth is very high, it can compensate for storage requirement to a large extent, whereas high storage may not compensate for bandwidth, especially, in real time multimedia applications. A new strategy for bandwidth channelization for videos has been proposed that has been named variable bandwidth channelization. The results for buffer requirement in variable bandwidth channelization have been compared with those of constant bandwidth channelization. It was found that the requirement of buffer storage for variable bandwidth channelization is significantly less compared to that for constant bandwidth channelization.","Bandwidth,
Videos,
Buffer storage,
Multimedia communication,
Jitter,
Delay,
Application software,
Broadcasting,
India Council,
Computer science education"
Cultural swarms II: virtual algorithm emergence,"Cultural algorithms (CA) (Reynolds 1994) is an evolutionary model derived from the cultural evolution process. CA has two major components, a population components and a belief component. In a previous paper it was show that certain problem solving phases emerged during the optimization process in a dynamic problem solving environment (Reynolds and Saleem 2003). These phases were labeled coarse grained, fine grained and backtracking respectively. I understand how these phases emerged as a result of the interaction of the five knowledge sources influenced individuals in the population component. It was demonstrated in a companion paper (Reynolds 2003) how individuals in an EP population exhibited swarm-like behavior while under the belief space during the search for an optimum in a cones-world environment. In this paper we examine the behavior of the cultural algorithm at the meta-level and demonstrate how, at that level, an algorithmic interaction of knowledge sources emerge. This interaction in terms of best-first search.","Cultural differences,
Computer science,
Problem-solving,
Genetic programming,
Genetic algorithms,
Particle swarm optimization,
Power generation,
State-space methods,
System testing"
Efficient aggregation over moving objects,"We study two types of aggregation queries over a set S moving point objects. The first asks to count the number of points in S that are dominated by a query point Q at a given time t. The second asks to find the maximum number of points in S that are dominated by a query point at any time. These queries have several applications in the area of Geographic Information Systems and spatiotemporal databases. For the first query and any fixed dimension d, we give two different solutions, one using O (/spl radic/ N) time and O (N) space and another using O (log N) time and O (N/sup 2/ space, where N is the number of moving points. When each of the points in S is moving piecewise linearly along the same line and the total number of pieces is O (N), then we can do the count query in O (/spl radic/ N) time and O (N) space. For the second query, when all objects move along the x-axis, we give a solution that uses O (log N) time and O (N/sup 2/) space in the worst case. Our solutions introduce novel search structures that can have other applications.","Relational databases,
Board of Directors,
Computer science,
Geographic Information Systems,
Spatiotemporal phenomena,
Aggregates,
Manufacturing,
Logic"
An Evaluation of a Framework for the Dynamic Load Balancing of Highly Adaptive and Irregular Parallel Applications,"We present an evaluation of a flexible framework and runtime software system for the dynamic load balancing of asynchronous and highly adaptive and irregular applications. These applications, which include parallel unstructured and adaptive mesh refinement, serve as building blocks for a large class of scientific applications. Extensive study has lead to the development of solutions to the dynamic load balancing problem for loosely synchronous and computation intensive programs; however, these methods are not suitable for asynchronous and highly adaptive applications. We evaluate a new software framework which includes support for an Active Messages style communication mechanism, global name space, transparent object migration, and preemptive decision making. Our results from both a 3-dimensional parallel advancing front mesh generation program, as well as a synthetic microbenchmark, indicate that this new framework out-performs two existing general-purpose, well-known, and widely used software systems for the dynamic load balancing of adpative and irregular parallel applications.",
Wide Field of View Head Mounted Display for Tele-presence with An Omnidirectional Image Sensor,"Recently, the omnidirectional image sensors have been applied to tele-presence systems, because the sensor can capture images with large field of views at video rate. On the other hand, head mount display (HMD) has been generally used as a personal display for virtual reality applications such as a tele-presence. However, almost all HMDs have a problem that the field of view (FOV), about 60 degree horizontally, of its presented image was terribly narrower than that of human. The problem makes reality and immersion lower in these applications. In this paper, we propose high-immersive visualization system that can display 180 degrees horizontal view by using a new catadioptrical HMD and an omnidirectional image sensor. The HMD consists of ellipsoidal and hyperboloidal curved mirrors, and can display 180 degrees horizontal view.","Mirrors,
Image resolution,
Head,
Optics,
Image sensors,
Optical distortion,
Pixel"
The joint behaviors of intelligent agents,"Joint behaviors widely existing in intelligent systems are important to investigate the cooperation among intelligent agents. In this paper, we put forward a comprehensive and novel model of joint behaviors in support of analysis and design of intelligent systems. The model includes two kinds of joint behaviors, their formal semantics are defined, and important properties are discussed, specified and proved.","Intelligent agent,
Intelligent systems,
Competitive intelligence,
Intelligent robots,
Computational intelligence,
Robot kinematics,
Computer science,
Autonomous agents,
Problem-solving,
Glass"
High-performance left-to-right array multiplier design,"We propose a split array multiplier organized in a left-to-right leapfrog (LRLF) structure with reduced delay compared to conventional array multipliers. Moreover, the proposed design shows equivalent performance as tree multipliers for n/spl les/32. An efficient radix-4 recoding logic generates the partial products in a left-to-right order. The partial products are split into upper and lower groups. Each group is reduced using [3:2] adders with optimized signal flows and the carry-save results from two groups are combined using a [4:2] adder. The final product is obtained with a prefix adder optimized to match the non-uniform arrival profile of the inputs. Layout experiments indicate that upper/lower split multipliers have slightly less area and power than optimized tree multipliers while keeping the same delay for n/spl les/32.","Delay,
Logic arrays,
Computer science,
Classification tree analysis,
Wiring,
Capacitance,
Logic design,
Minimization methods,
Time division multiplexing,
Signal design"
Chameleon: an interactive texture-based rendering framework for visualizing three-dimensional vector fields,"In this paper we present an interactive texture-based technique for visualizing three-dimensional vector fields. The goal of the algorithm is to provide a general volume rendering framework allowing the user to compute three-dimensional flow textures interactively, and to modify the appearance of the visualization on the fly. To achieve our goal, we decouple the visualization pipeline into two disjoint stages. First, streamlines are generated from the 3D vector data. Various geometric properties of the streamlines are extracted and converted into a volumetric form using a hardware-assisted slice sweeping algorithm. In the second phase of the algorithm, the attributes stored in the volume are used as texture coordinates to look up an appearance texture to generate both informative and aesthetic representations of the underlying vector field. Users can change the input textures and instantaneously visualize the rendering results. With our algorithm, visualizations with enhanced structural perception using various visual cues can be rendered in real time. A myriad of existing geometry-based and texture-based visualization techniques can also be emulated.","Data visualization,
Rendering (computer graphics),
Computer graphics,
Pipelines,
Shape,
Data flow computing,
Information science,
Tornadoes,
Data mining,
Chromium"
Towards a clone detection benchmark suite and results archive,"Source code clones are copies or near-copies of other portions of code, often created by copying and pasting portions of source code. This working session is concerned with building a communal research infrastructure for clone detection. The intention of this working session is to try to build a consensus on how to continue to build a benchmark suite and results archive for clone- and source comparison-related research and development. The working session is structured to foster discussion and debates over what should be collected in the archive, and how best to do it.","Cloning,
Benchmark testing,
Infrared detectors,
Software testing,
System testing,
Software systems,
Laboratories,
Computer science,
Buildings,
Research and development"
The agile approach in an undergraduate software engineering course project,"The rise in popularity of agile software development methodologies such as extreme programming (XP), crystal, DSDM and feature-driven development has opened an opportunity for the software engineering education community. How can one capitalize on the strengths of agile development models while still appealing to established software engineering practices? The typical introductory software engineering course makes use of a team-based project to reinforce software process activities. The project normally runs for one academic term during which students are led through life-cycle activities using a modified waterfall approach to software development. While useful in teaching software engineering process concepts, this approach limits the team's ability to utilize feedback from downstream process activities. It also limits the students' opportunity to understand process improvement from their own experiences. The ability to respond to project change is also dampened by the fact that teams do not have the time or resources in this format to modify, or refactor the design of a project component let alone incorporate a new or modified customer requirement. Agile methodologies promote an evolutionary approach to development using short incremental release cycles. We report on the experiences of conducting a team project in an introductory software engineering course using agile development techniques at the Rochester Institute of Technology. Teams have the opportunity to experience multiple iterations of the software engineering life cycle and evolve a product design that allows for discoveries made during implementation and through the introduction of changing customer requirements. The project integrates the concept of test-driven development. This agile technique addresses testing early in the development process and reinforces the value of unit testing. The incorporation of agile techniques is not only useful for students in an introductory course, but may also be applied to upper division software engineering courses.","Software engineering,
Programming profession,
Engineering profession,
Object oriented programming,
Engineering education,
Feedback,
Design engineering,
Product design,
Software testing,
Accreditation"
Configuring sessions in programmable networks with capacity constraints,"The provision of advanced computational services within networks is rapidly becoming both feasible and economical. As computational services become popular, it is important to have effective methods for configuring application sessions so that they use resources efficiently. In this paper, we discuss the problem of configuring application sessions that require intermediate processing. The problem was introduced in an earlier paper, where we showed how to optimally configure sessions in programmable networks by reducing the session configuration problem to the problem of finding a shortest path in a special graph constructed for the particular problem. This layered graph method is quite flexible and can handle a variety of specific session configuration problems. However, it does not explicitly model limits on link bandwidth or processing capacity. In this paper, we show that the optimal session configuration problem is NP-hard when capacity is constrained. Nevertheless, we have found efficient heuristics for which the network performance closely approximates the performance that can be achieved with optimal session configurations.","Intelligent networks,
Bandwidth,
Cryptography,
Computer science,
Computer networks,
Application software,
Unicast,
Product development,
Communication system control,
IP networks"
A simulation of attempts to influence crowd dynamics,"An understanding of how to alter crowd dynamics would have a significant impact in a number of scenarios, e.g., during riots or evacuations. The social force model, where individuals are self-driven particles interacting through social and physical forces, is one approach that has been used to describe crowd dynamics. This work uses the framework of the social force model to study the effects of introducing autonomous robots into crowds. Two simple pedestrian flow problems are used as illustrative examples, namely flow in varying width hallways and lane formation in bi-directional pedestrian flow. Preliminary results indicate that robots capable of inducing an attractive social force are effective at improving pedestrian flow in both of these scenarios.","Robots,
Large-scale systems,
Transportation,
Fires,
Jamming,
Computational modeling,
Motion control,
Indoor environments,
Buildings,
Spraying"
PI gain scheduler for load frequency control using spline techniques,"This paper proposes a new gain scheduler for proportional-integral (PI) based load frequency control (LFC) using spline techniques. LFC operation has to comply with the North American Electric Reliability Council (NERC)'s control performance standards. This can be achieved by tuning the control gains of the load frequency controller. Furthermore, excess maneuvering of the generating units can be reduced if the gains are adjusted properly. Various PI gains are generated by a robust control design, where each one yields different control performance. Subsequently, those gains are interpolated and approximated using spline techniques, which finally result in robust gain paths. An appropriate pair of PI gains are scheduled from the gain paths to achieve the LFC objective. Simulation results are shown to illustrate performance of the proposed methods.","Frequency control,
Spline,
Power generation,
Control systems,
Automatic generation control,
Costs,
Pi control,
Processor scheduling,
Computer science,
Councils"
Logics for reasoning about cryptographic constructions,We present two logical systems for reasoning about cryptographic constructions which are sound with respect to standard cryptographic definitions of security. Soundness of the first system is proved using techniques from nonstandard models of arithmetic. Soundness of the second system is proved by an interpretation into the first system. We also present examples of how these systems may be used to formally prove the correctness of some elementary cryptographic constructions.,"Logic,
Cryptography,
Cryptographic protocols,
Computer science,
Acoustical engineering,
Computer security,
Arithmetic,
Authentication,
Programming profession"
Self-organizing resource allocation for autonomic network,"Application-layer networks (ALN) are software architectures that allow the provisioning of services requiring a huge amount of resources by connecting large numbers of individual computers, e.g. grids and P2P-networks. Self-organization, like proposed by the autonomic computing concept, might be the key to controlling these systems. The CATNET project evaluates a decentralized mechanism for resource allocation in ALN, based on the economic paradigm of the Catallaxy. The economic model is based on self-interested maximization of utility and self-interested cooperation between software agents, who buy and sell network services and resources to and from each other.","Resource management,
Application software,
Computer networks,
Control systems,
Software architecture,
Joining processes,
Power generation economics,
IP networks,
Peer to peer computing,
Computer science"
Synthesizing Realistic Computational Grids,"Realistic workloads are essential in evaluating middleware for computational grids. One important component is the raw grid itself: a network topology graph annotated with the hardware and software available on each node and link. This paper defines our requirements for grid generation and presents GridG, our extensible generator. We describe GridG in two steps: topology generation and annotation. For topology generation, we have both model and mechanism. We extend Tiers, an existing tool from the networking community, to produce graphs that obey recently discovered power laws of Internet topology. We also contribute to network topology theory by illustrating a contradiction between two laws and proposing a new version of one of them. For annotation, GridG captures intra- and inter-host correlations between attributes using conditional probability rules. We construct a set of rules, including one based on empirical evidence of OS concentration in subnets, that produce sensible host annotations.","Grid computing,
Network topology,
Mesh generation,
Middleware,
Distributed computing,
Geographic Information Systems,
Permission,
Peer to peer computing,
Network synthesis,
Computer science"
Comprehension of software analysis data using 3D visualization,"The paper presents a software visualization application-framework that utilizes a variety of 3D metaphors to represent large software system and related analysis data. The 3D representation is based on the SeeSoft pixel representation and extends that original metaphor by rendering the visualization in a 3D space. Object-based manipulation methods and simultaneous alternative mappings are available to the user. The visual elements, mappings, and user interactions implemented and used by the framework are described with respect to their support for software understanding tasks. Examples are presented and discussed to demonstrate how the system's current features support the needs of the user.","Data analysis,
Data visualization,
Software systems,
Software tools,
Application software,
Computer architecture,
Computer science,
Simultaneous localization and mapping,
Large-scale systems,
Software maintenance"
System-level modeling and Simulation of the 10G optoelectronic interconnect,"Mixed-signal multidomain systems present a challenge for computer-aided design tools. Optical and electronic simulation tools are available as separate entities. However, to date, successful system-level cosimulation has not been implemented, leading to expensive refabrication. We present a unique system-level simulation tool for mixed electrooptical systems. We apply our tool Chatoyant to the simulation of an optical high-speed free-space interconnect system designed for 10-GHz speeds. The 10G free-space optical interconnect module has optical, optoelectronic, and microwave components and thus is an ideal vehicle to use as a test system. We demonstrate how Chatoyant, a mixed-signal multidomain simulator, has been used to evaluate end-to-end performance of this complex system, including the exploration of design tradeoffs and mechanical tolerancing.","Computational modeling,
Optical interconnections,
High speed optical techniques,
Analytical models,
Integrated circuit interconnections,
Design automation,
Computer science,
Space technology,
Optical design,
Optical devices"
Engineering drawings recognition using a case-based approach,"In this paper, we propose a framework for engineering drawings recognition using a case-based approach. The key idea of our scheme is that, interactively, the user provides an example of one type of graphic object in an engineering drawing, then the system learns the graphical knowledge of this type of graphic object from the example and uses this learned knowledge to recognize or search for similar graphic objects in engineering drawings. The scheme emphasizes the following three distinct characteristics: automatism, run-time-ness, and robustness. We summarized five types of geometric constraints to represent the generic graphical knowledge. We also developed two algorithms for case-based graphical knowledge acquisition and knowledge-based graphics recognition, respectively. Experiments have shown that our proposed framework is both efficient and effective for recognizing various types of graphic objects in engineering drawings.","Engineering drawings,
Knowledge acquisition,
Pattern recognition,
Computer science,
Humans,
Knowledge representation,
Computer graphics,
Robustness,
Image analysis,
Indexing"
Rotational-position-aware real-time disk scheduling using a dynamic active subset (DAS),"Scheduling disk requests with service guarantees has to bring the demand to meet guarantees in line with the need to optimize disk utilization. This paper presents the design, implementation, and experimental evaluation of a disk-scheduling framework which optimizes the disk utilization under the constraints of both hard and statistical service guarantees. The framework is based on two principles: 1) upon each scheduling decision, the calculation of a subset of the outstanding disk requests such that all service guarantees can be enforced under worst-case assumptions; and 2) the scheduling of this subset based on the rotational position of requests in order to improve the disk utilization. Results are presented through an implementation of the scheduling framework in DROPS, the Dresden real-time operating system.","Dynamic scheduling,
Real time systems,
Constraint optimization,
Processor scheduling,
Operating systems,
Streaming media,
Resource management,
Computer science,
Design optimization,
Delay"
A new optimization criterion for generalized discriminant analysis on undersampled problems,"A new optimization criterion for discriminant analysis is presented. The new criterion extends the optimization criteria of the classical linear discriminant analysis (LDA) by introducing the pseudo-inverse when the scatter matrices are singular. It is applicable regardless of the relative sizes of the data dimension and sample size, overcoming a limitation of the classical LDA. Recently, a new algorithm called LDA/GSVD for structure-preserving dimension reduction has been introduced, which extends the classical LDA to very high-dimensional undersampled problems by using the generalized singular value decomposition (GSVD). The solution from the LDA/GSVD algorithm is a special case of the solution for our generalized criterion, which is also based on GSVD. We also present an approximate solution for our GSVD-based solution, which reduces computational complexity by finding subclusters of each cluster, and using their centroids to capture the structure of each cluster. This reduced problem yields much smaller matrices of which the GSVD can be applied efficiently. Experiments on text data, with up to 7000 dimensions, show that the approximation algorithm produces results that are close to those produced by the exact algorithm.",
Multi-Constraint Mesh Partitioning for Contact/Impact Computations,"We present a novel approach for decomposing contact/impact computations in which the mesh elements come in contact with each other during the course of the simulation. Effective decomposition of these computations poses a number of challenges as it needs to both balance the computations and minimize the amount of communication that is performed during the finite element and the contact search phase. Our approach achieves the first goal by partitioning the underlying mesh such that it simultaneously balances both the work that is performed during the finite element phase and that performed during contact search phase, while producing subdomains whose boundaries consist of piecewise axes-parallel lines or planes. The second goal is achieved by using a decision tree to decompose the space into rectangular or box-shaped regions that contain contact points from a single partition. Our experimental evaluation on a sequence of 100 meshes, shows that this new approach can reduce the overall communication overhead over existing algorithms.","Partitioning algorithms,
Military computing,
Computational modeling,
Finite element methods,
Context modeling,
Concurrent computing,
Distributed computing,
Permission,
Deformable models,
Computer science"
Stretched call model for next generation cellular networks,"This paper examines the possibility of implementation of relaying in 3G cellular systems by allowing calls between a mobile terminal and base station to be forwarded by some other mobile device in the cell. This ""stretched"" call provides improvements in throughput, capacity, energy savings and coverage. The main contribution of this paper is to enumerate the problems that are introduced due to relaying, and how they could be resolved. We consider cdma2000, UTRA-FDD and UTRA-TDD technologies of 3G cellular, and discuss stretched models suitable for best throughput and delay metrics.","Next generation networking,
Land mobile radio cellular systems,
Relays,
Batteries,
Throughput,
Cellular networks,
Delay,
Mobile computing,
Base stations,
Computer science"
Factor analysis based anomaly detection,"We propose a novel anomaly detection algorithm based on factor analysis and Mahalanobis distance. Factor analysis is used to uncover the latent structure (dimensions) of a set of variables. It reduces attribute space from a larger number of variables to a smaller number of factors. The Mahalanobis distance is used to determine the ""similarity"" of a set of values from an ""unknown"" sample to a set of values measured from a collection of ""known"" samples. Combined with factor analysis, Mahalanobis distance is extended to examine whether a given vector is an outlier from a model identified by ""factors"" based on factor analysis. We present a factor analysis-based network anomaly detection algorithm and apply it to DARPA intrusion detection evaluation data. The experimental results show that the proposed algorithm is able to detect network intrusions with relatively low false alarms.","Intrusion detection,
Algorithm design and analysis,
Detection algorithms,
Statistics,
Statistical analysis,
Association rules,
Training data,
Neural networks,
Computer networks,
Information science"
Recurrent Neural Networks with Small Weights Implement Definite Memory Machines,"Recent experimental studies indicate that recurrent neural networks initialized with “small” weights are inherently biased toward definite memory machines (Tiňno, Čerňanský, & Beňušková, 2002a, 2002b). This article establishes a theoretical counterpart: transition function of recurrent network with small weights and squashing activation function is a contraction. We prove that recurrent networks with contractive transition function can be approximated arbitrarily well on input sequences of unbounded length by a definite memory machine. Conversely, every definite memory machine can be simulated by a recurrent network with contractive transition function. Hence, initialization with small weights induces an architectural bias into learning with recurrent neural networks. This bias might have benefits from the point of view of statistical learning theory: it emphasizes one possible region of the weight space where generalization ability can be formally proved. It is well known that standard recurrent neural networks are not distribution independent learnable in the probably approximately correct (PAC) sense if arbitrary precision and inputs are considered. We prove that recurrent networks with contractive transition function with a fixed contraction parameter fulfill the so-called distribution independent uniform convergence of empirical distances property and hence, unlike general recurrent networks, are distribution independent PAC learnable.",
Online education expands and evolves,"Convenience has always been a big draw of distance courses, but with the prolonged recession, engineers and other professionals are turning to online courses to maintain their job skills and their employability. Corporations have likewise embraced distance learning to keep their employees trained and motivated. Continued evolution in all sectors of engineering education around the world is irreversible. For engineering, the content and presentation lend themselves quite well to distance learning. Given their technical bent, engineering students seem to take to online courses. Flexibility is another attraction of distance learning for the working professional.","Computer science,
Computer aided instruction,
Streaming media,
Educational institutions,
Programming profession,
Computer science education,
Computer networks,
Home computing,
Maintenance engineering,
Engineering education"
Measurement system of computer input ability of patients with tetraplegia,,"Mice,
Home computing,
System testing,
Clocks,
Sampling methods,
Employment,
Injuries,
Computer science education,
Motion analysis,
Ergonomics"
Uncertainty in the automation of ontology matching,"The exchange of information between two agents over the semantic Web requires a means of translating between the ""vocabularies"" of the agents. Much research has focused on the use of ontologies for specifying an agent's knowledge and for exchanging information between agents. Effective communication between agents using different ontologies, however, requires determining the semantic interoperability, i.e., the agreement between the two agent's ontologies. Ontology matching is essential for the process of merging or aligning ontologies and for effective communication between agents. We present a survey of several proposals for ontology matching and develop a framework for the process of ontology comparison from several different levels and views. The role of fuzzy set theory in measuring the quality of the match between two ontologies is examined","Uncertainty,
Automation,
Ontologies,
Semantic Web,
Communication effectiveness,
Artificial intelligence,
Information retrieval,
Vocabulary,
Software agents,
Computer science"
An evaluation framework for active queue management schemes,"Over the last decade numerous active queue management (AQM) schemes have been proposed in the literature. Many of these studies have been directed towards improving congestion control in best-effort networks. However, there has been a notable lack of standardised performance evaluation of AQM schemes. A rigorous study of the influence of parameterisation on specific schemes and the establishment of common comparison criteria is essential for objective evaluation of the different approaches. A framework for the detailed evaluation of AQM schemes is described in this paper. This provides a deceptively simple user interface whilst maximally exploiting relevant features of the NS2 simulator. Traffic models and network topologies are carefully chosen to characterise the target simulation environment. The credibility of the results obtained is enhanced by vigilant treatment of the simulation data. The impact of AQM schemes on global network performance is assessed using five carefully selected metrics. Thus, a comprehensive evaluation of AQM schemes may be achieved using the proposed framework.","Analytical models,
Telecommunication traffic,
Traffic control,
Internet,
Jitter,
Computer networks,
Telecommunication computing,
Computer science,
Educational institutions,
User interfaces"
Unsupervised segmentation of color textured images using a multilayer MRF model,"Herein, we propose a novel multilayer Markov random field (MRF) image segmentation model which aims at combining color and texture features: each feature is associated to a so called feature layer, where an MRF model is defined using only the corresponding feature. A special layer is assigned to the combined MRF model. This layer interacts with each feature layer and provides the segmentation based on the combination of different features. The model is quite generic and isn't restricted to a particular texture feature. Herein we will test the algorithm using Gabor and MRSAR texture features. Furthermore, the algorithm automatically estimates the number of classes at each layer (there can be different classes at different layers) and the associated model parameters.","Image segmentation,
Clustering algorithms,
Smoothing methods,
Computer science,
Markov random fields,
Testing,
Humans,
Psychology,
Anthropometry,
Image resolution"
Business rule evolution and measures of business rule evolution,"There is an urgent industrial need to enforce the changes of business rules (BRs) to software systems quickly, reliably and economically. Unfortunately, evolving BRs in most existing software systems is both time-consuming and error-prone. In order to manage, control and improve BR evolution, it is necessary that the software evolution community comes to an understanding of the ways in which BRs are implemented and how BR evolution can be facilitated or hampered by the design of software systems. We suggest that new software metrics are needed to allow us to measure the characteristics of BR evolution and to help us to explore possible improvements in a systematic way. A suitable set of BR-related metrics help us to discover the root causes of the difficulties inherent in BR evolution, evaluate the success of proposed approaches to BR evolution and improve the BR evolution process as a whole.",
INSIDE: INstruction Selection/Identification & Design Exploration for extensible processors,"This paper presents the INSIDE system that rapidly searches the design space for extensible processors, given area and performance constraints of an embedded application, while minimizing the design turn-around-time. Our system consists of a) a methodology to determine which code segments are most suited for implementation as a set of extensible instructions, b) a heuristic algorithm to select pre-configured extensible processors as well as extensible instructions (library), and c) an estimation tool which rapidly estimates the performance of an application on a generated extensible processor. By selecting the right combination of a processor core plus extensible instructions, we achieve a performance increase on average of 2.03x (up to 7x) compared to the base processor core at a minimum hardware overhead of 25% on average.","Hardware,
Heuristic algorithms,
Coprocessors,
Computer science,
Australia,
Design engineering,
Application specific integrated circuits,
Digital signal processing,
National electric code,
Laboratories"
The performance of runtime data cache prefetching in a dynamic optimization system,"Traditional software controlled data cache prefetching is often ineffective due to the lack of runtime cache miss and miss address information. To overcome this limitation, we implement runtime data cache prefetching in the dynamic optimization system ADORE (ADaptive Object code Reoptimization). Its performance has been compared with static software prefetching on the SPEC2000 benchmark suite. Runtime cache prefetching shows better performance. On an Itanium 2 based Linux workstation, it can increase performance by more than 20% over static prefetching on some benchmarks. For benchmarks that do not benefit from prefetching, the runtime optimization system adds only 1%-2% overhead. We have also collected cache miss profiles to guide static data cache prefetching in the ORC compiler. With that information the compiler can effectively avoid generating prefetches for loops that hit well in the data cache.","Runtime,
Prefetching,
Software performance,
Application software,
Programming profession,
Optimizing compilers,
Computer science,
Data engineering,
Cities and towns,
Microprocessors"
Memory Encoding by Theta Phase Precession in the Hippocampal Network,"Recent experimental evidence on spike-timing-dependent plasticity and on phase precession (i.e., the theta rhythm dependent firing of rat hippocampalcells) associates the contribution of phase precession to episodic memory. This article aims at clarifying the role of phase precession in memory storage. Computer simulations show that the memory storage in the behavioral timescale varies in timescale of the temporal sequence from half a second to several seconds. In contrast, the memory storage caused by traditional rate coding is restricted to the temporal sequence within 40 ms. During phase precession, memory storage of a single trial experience is possible, even in the presence of noise. It is therefore concluded that encoding by phase precession is appropriate for memory storage of the temporal sequence in the behavioral timescale.",
An SVM-based algorithm for identification of photosynthesis-specific genome features,"This paper presents a novel algorithm for identification and functional characterization of ""key"" genome features responsible for a particular biochemical process of interest. The central idea is that individual genome features are identified as ""key"" features if the discrimination accuracy between two classes of genomes with respect to a given biochemical process is sufficiently affected by the inclusion or exclusion of these features. In this paper, genome features are defined by high-resolution gene functions. The discrimination procedure utilizes the support vector machine classification technique. The application to the oxygenic photosynthetic process resulted in 126 highly confident candidate genome features. While many of these features are well-known components in the oxygenic photosynthetic process, others are completely unknown, even including some hypothetical proteins. It is obvious that our algorithm is capable of discovering features related to a targeted biochemical process.","Genomics,
Bioinformatics,
Organisms,
Computational biology,
Computer science,
Mathematics,
Laboratories,
Genetics,
Pathogens,
Proteins"
Wireless communications systems with spatial diversity: a volumetric approach,"This paper presents a new physical modeling approach for wireless systems with multiple antennas. The fundamental problem of modelling the communications channel if we are given an arbitrary spatial volume for transmitting, an arbitrary spatial volume for receiving, and a set of scattering bodies is studied. We show how to calculate the number of communication modes, both for direct (point-to-point) and for scattering environments. Our work explains the physical parameters, which determine the channel model and its channel capacity.","Wireless communication,
Optical scattering,
Virtual reality,
MIMO,
Transfer functions,
Communication channels,
Eigenvalues and eigenfunctions,
Computer science,
Australia,
Channel capacity"
Understanding the Windows EAL4 evaluation,"On29 October 2002, Microsoft and Science Applications International Corporation issued press releases announcing that Windows 2000 with Service Pack 3 had received Evaluation Assurance Level 4 certification under the Common Criteria evaluation process. The Microsoft certification is sure to come up in many purchasing discussions over the next few years, so the question arises: What does this mean in plain English?.","Protection,
Information security,
Data security,
ISO standards,
Information technology,
Personnel,
Internet,
Operating systems,
Linux,
Computer security"
Comparing pure parallel ensemble creation techniques against bagging,"We experimentally evaluate randomization-based approaches to creating an ensemble of decision-tree classifiers. Unlike methods related to boosting, all of the eight approaches considered here create each classifier in an ensemble independently of the other classifiers. Experiments were performed on 28 publicly available datasets, using C4.5 release 8 as the base classifier. While each of the other seven approaches has some strengths, we find that none of them is consistently more accurate than standard bagging when tested for statistical significance.","Bagging,
Testing,
Decision trees,
Boosting,
Computer science,
Data mining"
Adaptive compressed caching: design and implementation,"We reevaluate the use of adaptive compressed caching in order to improve system performance through reduction of accesses to the backing stores. We propose a new and simple adaptability policy that adjusts the compressed cache size on-the-fly, and evaluate a compressed caching system with this policy through an implementation in a widely used operating system, Linux. We also redesign compressed caching in order to provide performance improvements for all tested workloads and address the problems faced in previous works and implementations that led to nonconclusive results. Among these fundamental modifications, our compressed cache is the first one to also compress file cache pages, to adaptively disable compression of clean pages when necessary and to address applications with poor compressibility. We tested a system with our adaptive compressed cache under many applications and benchmarks, each one with different memory pressures. The results showed performance improvements (up to 171.4%) in all of them if under memory pressure, and minimal overhead (up to 0.39%) when there is very light memory pressure. We show that this adaptive compressed cache design is actually considered as an effective mechanism for improvement in system performance.","System performance,
Operating systems,
Linux,
Kernel,
Computer science,
System testing,
Adaptive systems,
Benchmark testing,
Data compression,
Hard disks"
On the average path length in decision diagrams of multiple-valued functions,"We consider the path length in decision diagrams for multiple-valued functions. This is an important measure of a decision diagram, since this models the time needed to evaluate the function. We focus on the average path length (APL), which is the sum of the path lengths over all assignments of values to the variables divided by the number of assignments. First, we show a multiple-valued function in which the APL is markedly affected by the order of variables. We show upper and lower bounds on the longest path length in a decision diagram of a multiple-valued function. Next, we derive the APL for individual functions, the MAX, ALL-MAX, and MODSUM functions. We show that the latter two functions achieve the lower and upper bound on the APL overall n-variable r-valued functions. Finally, we derive the average of the APL for two sets of functions, symmetric functions and all functions.","Binary decision diagrams,
Data structures,
Boolean functions,
History,
Circuits,
Adders,
Computer science,
Time measurement,
Upper bound,
Minimization"
Comparing different serial and parallel heuristics to design combinational logic circuits,"In this paper, we perform a comparative study of different heuristics used to design combinational logic circuits. The use of local search hybridized with a genetic algorithm and the effect of parallelism are of particular interest in the study conducted. Our results indicate that a hybridization of a genetic algorithm with simulated annealing is beneficial and that the use of parallelism does not only introduce a speedup (as expected) in the algorithms, but also allows one to improve the quality of the solutions found.","Combinational circuits,
Genetic algorithms,
Parallel processing,
Algorithm design and analysis,
Evolutionary computation,
Encoding,
Mathematics,
Computer science,
Circuit simulation,
Simulated annealing"
On training a sensor network,"The networks considered in this paper consist of tiny energy constrained commodity sensors massively deployed, along with one or more sink nodes providing interface to the outside world. Our contribution is to propose a scalable energy-efficient training protocol for nodes that are initially anonymous, asynchronous and unaware of their locations. Training partitions the nodes into clusters where data can be gathered from the environment and synthesized under local control. Further this training provides a virtual tree for efficient communication routing from clusters to the sink. Being energy-efficient, our training protocol can be run on either a scheduled or ad-hoc basis to provide robustness and dynamic reconfiguration.","Sensor arrays,
Energy efficiency,
Costs,
Military computing,
Routing protocols,
Sensor phenomena and characterization,
Sensor systems,
Computer science,
Network synthesis,
Communication system control"
Supporting QoS-based discovery in service-oriented Grids,"In this paper the service abstraction is extended to support quality of service (QoS) attributes - and implemented in the Grid QoS Management (G-QoSM) framework. The framework supports three main functions: (1) providing mechanisms for establishing QoS guarantees via service level agreements (SLAs), (2) enabling QoS management on allocated QoS-aware services, and (3) supporting discovery of services based on QoS attributes. The framework relies on each service offering both a ""functional"" interface and a ""management"" interface. The functional interface provides attributes on how a service is to be invoked and how it returns results, whereas the management interface is used to provide QoS attributes and performance characteristics associated with a service. The focus of this paper is on implementing a QoS-based service discovery system, which utilises an extended version of the universal description, discovery and integration (UDDI) registry.","Quality of service,
Contracts,
Scientific computing,
Visualization,
Context-aware services,
Computer science,
Quality management,
Advertising,
Middleware,
Telemedicine"
On optimal hyperuniversal and rearrangeable switch box designs,"This paper explores theories on designing optimal multipoint interconnection structures, and proposes a simple switch box design scheme which can be directly applied to field programmable gate arrays (FPGAs), switch box designs, and communication switching network designs. We present a new hyperuniversal switch box designs with four sides and W terminals on each side, which is routable for every multipin net-routing requirement. This new design is proved to be optimum for W = 1, ..., 5 and close to optimum for W /spl ges/ 6 with 6.3 W switches. We also give a formal analysis and extensive benchmark experiments on routability comparisons between today's most well-known FPGA switch boxes like disjoint switch blocks (Xilinx XC4000 Type), Wilton's switch blocks, Universal switch blocks, and our Hyperuniversal switch boxes. We apply the design scheme to rearrangeable switching network designs targeting for applications of connecting multiple terminals (e.g., teleconferencing). Simply using a /spl kappa/-sided hyperuniversal switch block with a W /spl times/ W crossbar attached to each side, one can build a three-stage one-sided polygonal switching network capable of realizing every multipoint connection requirement on kW terminals. Besides, due to the fine-grained decomposition property of our design scheme, the new switch box designs are highly scalable and simple on physical layout and routing algorithm implementations.","Switches,
Field programmable gate arrays,
Communication switching,
Routing,
Joining processes,
Computer science,
Algorithm design and analysis,
Area measurement,
Teleconferencing,
Multiprocessor interconnection networks"
Towards load balancing support for I/O-intensive parallel jobs in a cluster of workstations,"While previous CPU- or memory-centric load balancing schemes are capable of achieving the effective usage of global CPU and memory resources in a cluster system, the cluster exhibits significant performance drop under I/O-intensive workload conditions due to the imbalance of I/O load. To tackle this problem, we have developed two simple yet effective I/O-aware load-balancing schemes, which make it possible to balance I/O load by assigning I/O intensive sequential and parallel jobs to nodes with light I/O loads. Moreover, the proposed schemes judiciously take into account both CPU and memory load sharing in the cluster, thereby maintaining a high performance for a wide spectrum of workload. Using a set of real I/O-intensive parallel applications in addition to synthetic parallel jobs, we show that the proposed schemes consistently outperform the existing non-I/O aware load-balancing schemes for a diverse set of workload conditions. Importantly, the performance improvement becomes much more pronounced when the applications are I/O-intensive.","Resource management,
Parallel processing,
Shared memory systems,
Computer input-output"
Disclosure risk measures for microdata,"We define several disclosure risk measures for microdata. We analyze disclosure risk based on the disclosure control techniques applied to initial microdata. Disclosure Control is the discipline concerned with the modification of data containing confidential information about individual entities, such as persons, households, businesses, etc. in order to prevent third parties working with these data from recognizing entities in the data and thereby disclosing information about these entities. In very broad terms, disclosure risk is the risk that a given form of disclosure will occur if a masked microdataset is released. Microdata represents a series of records, each record containing information on an individual unit. The disclosure risk measures presented in the paper are validated in our experiments.","Sampling methods,
Computer science,
Medical services,
Risk analysis,
Loss measurement,
Measurement standards,
Protection,
Databases,
Conference management"
An unsupervised artifact correction approach for the analysis of DNA microarray images,"Image processing for analysis of microarray images is an important and challenging problem because imperfections and fabrication artifacts often impair our ability to measure accurately the quantities of interest in these images. In this paper we propose a microarray image analysis framework that provides a new method that automatically addresses each spot area in the image. Then, a new unsupervised clustering method is used which is based on a Gaussian mixture model (GMM) and the minimum description length (MDL) criterion, that allows the automatic spot area segmentation and the image artifacts isolation and correction to obtain more accurate spot quantitative values. Experimental results demonstrates the advantages of the proposed scheme in efficiently analysing microarrays.","Image analysis,
DNA,
Fluorescence,
Image sequence analysis,
Computer science,
Biomedical imaging,
Image processing,
Fabrication,
Clustering methods,
Image segmentation"
Pre-loaded key based multicast and broadcast authentication in mobile ad-hoc networks,"The nature of mobile ad hoc networks (MANET), demands stringent requirements on primitives that could be used to secure such networks. Mobility imposes restrictions on memory and processor requirements due to limited battery life. The ad hoc nature warrants schemes that could operate for extended periods without referring to a trusted authority (TA). Additionally, any enabling scheme for security should be able to scale well. We introduce a novel key management scheme, RPS - random preloaded subset key distribution - which satisfies all the above requirements. More specifically, RPS is an n-secure r-conference key predistribution scheme. While most of the previously reported key predistribution schemes also meet all the stringent requirements, RPS has many inherent advantages. In this paper we investigate the applicability of RPS in securing MANETs.","Broadcasting,
Authentication,
Intelligent networks,
Ad hoc networks,
Mobile ad hoc networks,
Unicast,
Public key cryptography,
Mobile computing,
Computer science,
Computer networks"
A lower bound for the bounded round quantum communication complexity of set disjointness,"We show lower bounds in the multi-party quantum communication complexity model. In this model, there are t parties where the ith party has input X/sub i/ /spl sube/ [n]. These parties communicate with each other by transmitting qubits to determine with high probability the value of some function F of their combined input (X/sub 1/,...,X/sub t/). We consider the class of Boolean valued functions whose value depends only on X/sub 1/ /spl cap/.../spl cap/ X/sub t/; that is, for each F in this class there is an f/sub F/ : 2/sup [n]/ /spl rarr/ {0,1}, such that F(X/sub 1/,...,X/sub t/) = f/sub F/(X/sub 1/ /spl cap/.../spl cap/ X/sub t/). We show that the t-party k-round communication complexity of F is /spl Omega/(s/sub m/(f/sub F/)/(k/sup 2/)), where s/sub m/(f/sub F/) stands for the monotone sensitivity of f/sub F/' and is defined by s/sub m/(f/sub F/) = /sup /spl utri// max/sub S/spl sube//[n] |{i : f/sub F/(S /spl cup/ {i}) /spl ne/ f/sub F/(S)}|. For two-party quantum communication protocols for the set disjointness problem, this implies that the two parties must exchange /spl Omega/(n/k/sup 2/) qubits. An upper bound of O(n/k) can be derived from the O(/spl radic/n) upper bound due to S. Aaronson and A. Ambainis (2003). For k = 1, our lower bound matches the /spl Omega/(n) lower bound observed by H. Buhrman and R. de Wolf (2001) (based on a result of A. Nayak (1999)), and for 2 /spl les/ k /spl Lt/ n/sup 1/4 /, improves the lower bound of /spl Omega/(/spl radic/n) shown by A. Razborov (2002). For protocols with no restrictions on the number of rounds, we can conclude that the two parties must exchange /spl Omega/(n/sup 1/3/) qubits. This, however, falls short of the optimal /spl Omega/ (/spl radic/n) lower bound shown by A. Razborov (2002). Our result is obtained by adapting to the quantum setting the elegant information-theoretic arguments of Z. Bar-Yossef et al. (2002). Using this method we can show similar lower bounds for the L/sub /spl infin// function considered in Z. Bar-Yossef et al. (2002).","Complexity theory,
Protocols,
Quantum computing,
Upper bound,
Computer science,
Distributed computing,
Career development,
Scholarships,
Combinatorial mathematics,
Books"
Simulating wax crayons,"We present a physically-inspired model of wax crayons, which synthesizes drawings from collections of user-specified strokes. Paper is represented by a height-field texture, and a crayon is modeled with a 2D mask that evolves as it interacts with the paper. The amount of wax deposition is computed based on the crayon contact profile, contact force, and friction. Previously deposited wax is smeared by crayon action, based on wax softness and contact information. The distributed wax is rendered using a simplified Kubelka-Monk model, which approximates light transmittance and scattering effects.","Computational modeling,
Light scattering,
Shape,
Rendering (computer graphics),
Image processing,
Geometry,
Computer science,
Friction,
Petroleum,
Painting"
Shape representation via harmonic embedding,"We present a novel representation of shape for closed planar contours explicitly designed to possess a linear structure. This greatly simplifies linear operations such as averaging, principal component analysis or differentiation in the space of shapes. The representation relies upon embedding the contour on a subset of the space of harmonic functions of which the original contour is the zero level set.","Shape,
Level set,
Principal component analysis,
Computer science,
Space technology,
Nonlinear equations,
Geometry,
Topology,
Laplace equations,
Design engineering"
Must there be so few? Including women in CS,"Women's under-representation in academic computer science is described for the U.S. and internationally. Conditions that contribute to this situation are identified, and motivations for increasing women's participation in computer science are discussed According to recent research in the US., effective interventions at the undergraduate level include: actively recruiting women, encouraging women to persist, and mentoring for the purpose of overcoming under-representation. The latter two practices are easily implemented.","Computer science,
Mathematics,
Data engineering,
Biology,
Recruitment,
Employee welfare,
Statistics,
Databases,
Computer industry,
Computer science education"
Protocol design for anycast communication in IPv6 network,"Although anycast communication supports service-oriented addresses many of its current definitions in IPv6 are unclear. Furthermore, since there are no protocol standards or even consensus on routing control, inter-segment anycast communications are not yet available. In this paper, we first discuss these problems and solutions. Based on our findings, we present the anycast address resolving protocol (AARP) to establish TCP connections with a specific anycast address and propose a routing protocol for inter-segment anycasts. Our proposed architecture makes anycast addresses more useful without (or at most minimum) modifications/extensions to existing applications and/or upper-layer protocols.",
Teaching ethics in the software engineering curriculum,"This paper describes a pilot study conducted to explore the teaching of ethics in software engineering programs. The author reviewed existing literature and then constructed a tentative survey, which was sent to 127 educators. Questions involved ethical topics encountered, methods of delivery, and the use of codes of ethics. Results provide only an informal snapshot of trends but responses are being used to create a revised survey, which will be sent to a larger population.","Ethics,
Software engineering,
Accreditation,
Computer science education,
Educational programs,
Engineering profession,
Software standards,
Computer crime,
Application software,
Computer science"
An open systems architecture for prognostic inference during condition-based monitoring,,"Open systems,
Condition monitoring,
Computer architecture,
Military computing,
Defense industry,
Military aircraft,
Costs,
Machinery,
Procurement,
Aerospace engineering"
Mathematical framework for representing discrete functions as word-level polynomials,"This paper presents a mathematical framework for modeling arithmetic operators and other RTL design modules as discrete word-level functions and proposes a polynomial representation of those functions. The proposed representation attempts to bridge the gap between bit-level BDD representations and word-level representations, such as *BMDs and TEDs.","Polynomials,
Binary decision diagrams,
Boolean functions,
Computer science,
Design engineering,
Digital arithmetic,
Bridges,
Robustness,
Data structures,
Logic functions"
Enabling snap-stabilization,"A snap-stabilizing protocol guarantees that the system always behaves according to its specification provided some processor initiated the protocol. We present how to snap-stabilize some important protocols, like Leader Election, Reset, Snapshot, and Termination Detection. We use a Snap-stabilizing Propagation of Information with Feedback protocol for arbitrary networks as the key module in the above transformation process. Finally, we design a universal transformer to provide a snap-stabilizing version of any protocol (which can be self-stabilized with the transformer of [15]).","Protocols,
Feedback,
Distributed computing,
Broadcasting,
Nominations and elections,
Computer science,
Process design,
Fault tolerant systems,
Fuzzy systems,
Algorithm design and analysis"
Type-based distributed access control,"The key-based decentralized label model (KDLM) is a type system that combines a weak form of information flow control, termed distributed access control in the article, with typed cryptographic operations. The motivation is to have a type system that ensures access control while giving the application the responsibility to secure network communications, and to do this safely. KDLM introduces the notion of declassification certificates to support the declassification of encrypted data.",
A parameterized cost model to order classes for class-based testing of C++ applications,"In this paper we present the design and implementation of a class ordering system that is driven by a parameterized cost model. The parameters to the model assign weights to the edge types that describe the relationships between the classes in the graphical representation of the program. The nodes in the graph are classes and the edges express relationships between the classes. Previous research has included three or four edge types in the graph. However, to accommodate the full complement of C++ language constructs, which include template classes and functions and nested classes, we extend the graph to include six edge types. The parameters to the cost model can be tuned to remove certain types of edges in an attempt to reduce the cost of the testing effort or to reduce the cost of breaking cycles in the graph. Our case study indicates that inclusion of inheritance edges in cycle breaking considerations may reduce the number of edge removals by a factor of two or more.","Costs,
System testing,
Object oriented modeling,
Software testing,
Application software,
Computer science,
Robustness,
Software maintenance,
Large-scale systems,
Performance evaluation"
On-line overlaid-handwriting recognition based on substroke HMMs,,"Hidden Markov models,
Writing,
Character recognition,
Handwriting recognition,
Wearable computers,
Personal digital assistants,
Image segmentation,
Information science,
Keyboards,
Liquid crystal displays"
Using policy gradient reinforcement learning on autonomous robot controllers,"Robot programmers can often quickly program a robot to approximately execute a task under specific environment conditions. However, achieving robust performance under more general conditions is significantly more difficult. We propose a framework that starts with an existing control system and uses reinforcement feedback from the environment to autonomously improve the controller's performance. We use the policy gradient reinforcement learning (PGRL) framework, which estimates a gradient (in controller space) of improved reward, allowing the controller parameters to be incrementally updated to autonomously achieve locally optimal performance. Our approach is experimentally verified on a Cye robot executing a room entry and observation task, showing significant reduction in task execution time and robustness with respect to un-modelled changes in the environment.","Learning,
Robot control,
Orbital robotics,
Feedback,
Switches,
Control systems,
Optimal control,
State-space methods,
Computer science,
Information science"
Improving access to multi-dimensional self-describing scientific datasets,"Applications that query into very large multidimensional datasets are becoming more common. Many self-describing scientific data file formats have also emerged, which have structural metadata to help navigate the multi-dimensional arrays that are stored in the files. The files may also contain application-specific semantic metadata. In this paper, we discuss efficient methods for performing searches for subsets of multi-dimensional data objects, using semantic information to build multidimensional indexes, and group data items into properly sized chunks to maximize disk I/O bandwidth. This work is the first step in the design and implementation of a generic indexing library that will work with various high-dimension scientific data file formats containing semantic information about the stored data. To validate the approach, we have implemented indexing structures for NASA remote sensing data stored in the HDF format with a specific schema (HDF-EOS), and show the performance improvements that are gained from indexing the datasets, compared to using the existing HDF library for accessing the data.","Indexing,
Navigation,
Libraries,
Multidimensional systems,
NASA,
Middleware,
Computer science,
Educational institutions,
Application software,
Bandwidth"
The evolution of user-centered focus in the human-computer interaction field,"About 20 years have passed since the first conferences dedicated to human-computer interaction (HCI) were held. In that time many changes have occurred in how we think about making use of data gathered from users of technology to guide the process of designing and developing new hardware and software systems. Throughout this process there has been a productive dialog among academic and industry-based researchers and usability engineering practitioners. Academic research has provided insights into methods for understanding and modeling user behavior, and industry has provided a wide range of exciting technologies for consideration by researchers in HCI. This paper looks at the evolution of the field from the behavioral science perspective. We consider the evolution of the field within professional groups, such as the Association for Computing Machinery Special Interest Group on Computer-Human Interaction (ACM SIGCHI) and the International Federation for Information Processing Technical Committee (IFIP TC13), academic departments (primarily in computer science departments), and industry (primarily within IBM). In this paper we offer a view of this journey of 20 years, along with some visions and challenges of the future.",
The once and future analog alternative: evolvable hardware and analog computation,"Once-upon-a-time, analog computers co-existed with their digital counterparts and were considered equally useful. For many applications, specifically equation solving and modeling of physical systems, analog computers were often the better choice. The 1970's, however, saw the beginning of the end of this superiority. Advances in digital circuit fabrication and discrete computer algorithms, not to mention significant advantages of economy, generality, and ease of use, precipitated a mass exodus to general-purpose digital computers so complete that there are now many in the current generation who have neither experience with, nor memory of, the analog alternative. The exodus was certainly made for good reasons. However, it may be beneficial, from time to time, to consider if subsequent developments have rendered those reasons less compelling. This first part of this paper will suggest that the emergence of evolvable hardware (EH) is one such development. It will argue that by applying EH methodologies, one might practically restore the benefits of analog computation as well as achieve benefits not possible in earlier times. The second part of this paper will briefly outline a specific program designed to field practical analog EH control devices.",
Application-service interoperation without standardized service interfaces,"To programmatically discover and interact with services in ubiquitous computing environments, an application needs to solve two problems: (1) is it semantically meaningful to interact with a service? If the task is ""printing a file"", a printer service would be appropriate, but a screen rendering service or CD player service would not. (2) If yes, what are the mechanics of interacting with the service - remote invocation mechanics, names of methods, numbers and types of arguments, etc.? Existing service frameworks such as Jini and UPnP conflate these problems - two services are ""semantically compatible"" if and only if their interface signatures match. As a result, interoperability is severely restricted unless there is a single, globally agreed-upon, unique interface for each service type. By separating the two subproblems and delegating different parts of the problem to the user and the system, we show how applications can interoperate with services even when globally unique interfaces do not exist for certain services.","Printers,
Application software,
Pervasive computing,
Standardization,
Ubiquitous computing,
Printing,
Computer science"
Lower bounds for non-black-box zero knowledge,"We show new lower bounds and impossibility results for general (possibly non-black-box) zero-knowledge proofs and arguments. Our main results are that, under reasonable complexity assumptions: 1. There does not exist a constant-round zero-knowledge strong proof (or argument) of knowledge (as defined by Goldreich, 2001) for a nontrivial language; 2. There does not exist a two-round zero-knowledge proof system with perfect completeness for an NP-complete language; 3. There does not exist a constant-round public-coin proof system for a nontrivial language that is resettable zero knowledge. This result also extends to bounded resettable zero knowledge. In contrast, we show that under reasonable assumptions, there does exist such a (computationally sound) argument system that is bounded-resettable zero knowledge.","Cryptography,
Computer science,
Access protocols"
Digit-recurrence algorithms for division and square root with limited precision primitives,"We propose a digit-recurrence algorithm for square root using limited-precision multipliers, adders, and table-lookups. The algorithm, except in the initialization, uses the digit-recurrence algorithm for division with limited-precision primitives reported in (M.D. Ercegovac, et al., (2001)). Consequently, a combined scheme for division and square root is easily realized. We describe the algorithms and discuss a combined division/square-root design. Compared to a conventional implementation with full-precision primitives, the proposed scheme is estimated to have a longer cycle time and a significantly smaller area with a corresponding effect on power dissipation making the scheme interesting for low-power designs. This class of algorithms is suitable for higher radix implementation.","Computer science,
Algorithm design and analysis,
Convolution"
Managing eBusiness on demand SLA contracts in business terms using the cross-SLA execution manager SAM,"It is imperative for a competitive e-business service provider to be positioned to manage the execution of its service level agreement (SLA) contracts in business terms (e.g., minimizing financial penalties for service-level violations, maximizing service-level measurement based customer satisfaction metrics). This paper briefly describes the design rationale of an integrated set of business-oriented service level management (SLM) technologies under development in the SAM project at IBM TJ Watson Research Center. The e-business SLA execution manager SAM, (1) enables the provider to deploy an effective means of capturing and managing contractual SLA data as well as provider-facing non-contractual SLM data; (2) assists service personnel to prioritize the processing of action-demanding quality management alerts as per the provider's SLM objectives; and (3) automates the prioritization and execution management Of approved SLM processes on behalf of the provider, including assigning SLM tasks to service personnel.",
Finding good counter-examples to aid design verification,"Today up to 80% of the design costs for integrated circuits are due to verification. Verification tools guarantee completeness if equivalence of two designs or a property for a design is proven. In the other case, usually only one counter-example is produced. Then debugging has to be carried out to locate the design error. This paper investigates, how debugging can benefit from using more than one counter-example generated by the verification tool. The problem of finding useful counter-examples is theoretically analyzed and proven to be difficult. Heuristics are introduced and their quality is underlined by experimental results. Guidelines how to generate counter-examples are extracted from one of these heuristics.","Circuits,
Boolean functions,
Data structures,
Error correction,
Debugging,
Guidelines,
Formal verification,
Testing,
Computer science,
Costs"
Ontology-based reconfigurable case-based reasoning system for knowledge integration,"Cased-based reasoning (CRB) is a methodology that follows mankind's problem solving strategy. Most traditional CBR systems are built in a fixed index structure and they cannot be easily adapted to fit new environment. In this paper a KM framework is proposed that combines ontology, case templates and case-based reasoning to solve the problems. In this framework, enterprise ontology provides the basis for integration among knowledge resources. Case templates can be reconfigured based on the enterprise's ontology by the user. The knowledge structure and the user-interfaces of the system knowledge structure and the user-interface of the system can be reconfigured so that the system can suit for an enterprise whose knowledge structure and the requirement for knowledge management system change rapidly. Besides, a workflow based knowledge acquisition method is also discussed that assures the quality of the knowledge acquired.","Ontologies,
Knowledge management,
Knowledge engineering,
Computer science,
Problem-solving,
Product design,
Process design,
Statistical analysis,
Knowledge based systems,
Production systems"
EEG Subspace Representations and Feature Selection for Brain-Computer Interfaces,"Electroencephalogram (EEG) signals recorded from a persons scalp have been used to control binary cursor movements. Multiple choice paradigms will require more sophisticated protocols involving multiple mental tasks and signal representations that capture discriminatory characteristics of the EEG signals. In this study, six-channel EEG is recorded from a subject performing two mental tasks. The signals are transformed via the Karhunen-Loéve or maximum noise fraction transformations and classified by quadratic discriminant analysis. In addition, classification accuracy is tested for all subsets of the six EEG channels. Best results are approximately 90% correct when training and testing data are recorded on the same day and 75% correct when training and testing data are recorded on different days.","Electrodes,
Electroencephalography,
Training,
Accuracy,
Testing,
Noise,
Transforms"
An educational taxonomy for learning objects,Current discussions within the standardization process of learning technology are mainly focused on economical opportunities and technical aspects of so called learning objects. Surprisingly little discussion is about instructional or didactical issues. The main purpose is to conceptualize a didactical taxonomy of learning objects and a didactical metadata approach for the facilitation of reusable instructional navigation patterns.,"Taxonomy,
Standardization,
Environmental economics,
Containers,
Navigation,
Computer science education,
Educational products,
Computer network management,
Project management,
Europe"
Effects of handling real objects and avatar fidelity on cognitive task performance in virtual environments,"Immersive virtual environments (VEs) provide participants with computer-generated environments filled with virtual objects to assist in learning, training, and practicing dangerous and/or expensive tasks. But for certain tasks, does having every object being virtual inhibit the interactivity? Further, does the virtual object's visual fidelity affect performance? Overall VE effectiveness may be reduced if users spend most of their time and cognitive capacity learning how to interact and adapting to interacting with a purely virtual environment. We investigated how handling real objects and how self-avatar visual fidelity affects performance on a spatial cognitive task in an immersive VE. We compared participants' performance on a block arrangement task in both a real-space environment and several virtual and hybrid environments. The results showed that manipulating real objects in a VE brings task performance closer to that of real space, compared to manipulating virtual objects.","Avatars,
Virtual environment,
Assembly,
Lubricating oils,
Engines,
Feedback,
Haptic interfaces,
Problem-solving,
Cognitive science,
Libraries"
Increasing packet delivery ratio in DSR by link prediction,"Most existing on-demand mobile ad hoc network routing protocols continue using a route until a link breaks. During the route reconstruction, packets can be dropped, which will cause significant throughput degradation. In this paper, we add a link breakage prediction algorithm to the dynamic source routing (DSR) protocol. The mobile node uses signal power strength from the received packets to predict the link breakage time, and sends a warning to the source node of the packet if the link is soon-to-be-broken. The source node can perform a pro-active route rebuild to avoid disconnection. Experiments demonstrate that adding link breakage prediction to DSR can significantly reduce the total number of dropped data packets (by at least 20%). The tradeoff is an increase in the number of control messages by at most 33.5%. We also found that the proactive route maintenance does not cause significant increase in average packet latency and average route length. Enhanced route cache maintenance based on the link status can further reduce the number of dropped packets.",
Four types of lookback,"We present a classification that groups lookback into four types: direct strong lookback, universal strong lookback, direct weak lookback, and universal weak lookback. They are defined in terms of absolute and dynamic impact times. We discuss relationships between lookback types by considering when rollbacks and/or antimessages are avoided. From different types of lookback, we also derive three optimization techniques for optimistic simulation and point out their advantages over lazy cancellation. Finally, we show that all four types of lookback exist in the PCS network simulation and can be exploited by either lookback-based or optimistic protocols.","Protocols,
Personal communication networks,
Discrete event simulation,
Computer science,
Conferences"
Model-based clustering with soft balancing,"Balanced clustering algorithms can be useful in a variety of applications and have recently attracted increasing research interest. Most recent work, however, addressed only hard balancing by constraining each cluster to have equal or a certain minimum number of data objects. We provide a soft balancing strategy built upon a soft mixture-of-models clustering framework. This strategy constrains the sum of posterior probabilities of object membership for each cluster to be equal and thus balances the expected number of data objects in each cluster. We first derive soft model-based clustering from an information-theoretic viewpoint and then show that the proposed balanced clustering can be parameterized by a temperature parameter that controls the softness of clustering as well as that of balancing. As the temperature decreases, the resulting partitioning becomes more and more balanced. In the limit, when temperature becomes zero, the balancing becomes hard and the actual partitioning becomes perfectly balanced. The effectiveness of the proposed soft balanced clustering algorithm is demonstrated on both synthetic and real text data.","Clustering algorithms,
Partitioning algorithms,
Data mining,
Computer science,
Application software,
Temperature control,
Clustering methods,
Maximum likelihood estimation,
Indexing,
Databases"
"Computer self-efficacy, gender, and educational background in South Africa","Research has demonstrated possible factors for low participation by women, including self-efficacy. This paper considers computer self-efficacy and its relationship to gender and educational background. Self-efficacy is based on self-perception and is defined as the belief an individual has about their ability to perform a particular task. Self-efficacy is important as it influences the choice of activities by an individual, the amount of effort they will expend on a task and how long they will persevere in stressful situations to complete the task. Self-efficacy beliefs about computing may be a factor in whether people choose to get involved in computing. Therefore, self-efficacy is linked to participation rates and hence important to consider in our attempts to understand why people choose to become involved in information technology.","Africa,
Educational institutions,
Computer science,
Information technology,
Software measurement,
Software packages,
Programming profession,
Nominations and elections,
Computer science education"
Restoration schemes with differentiated reliability,"Reliability of data exchange is becoming increasingly important. In addition, applications may require multiple degrees of reliability. The concept of differentiated reliability (DiR) was recently introduced in [A. Fumagalli and M. Tacca, January 2001] to provide multiple degrees of reliability in protection schemes that provision spare resources. With this paper, the authors extend the DiR concept to restoration schemes in which network resources for a disrupted connection along secondary paths are sought upon failure occurrence, i.e., they are not provisioned before the fault. The DiR concept is applied in two dimensions: restoration blocking probability i.e., the probability that the disrupted connection is not recovered due to lack of network resources - and restoration time - i.e., the time necessary to complete the connection recovery procedure. Differentiation in the two dimensions is accomplished by proposing three preemption policies that allow high priority connections to preempt resources allocated to low priority connections. The three policies trade complexity, i.e., number of preempted connections, for better reliability differentiation. Obtained results indicate that by using the proposed preemption policies, it is possible to guarantee a significant differentiation of both restoration blocking probability and restoration time. By carefully choosing the preemption policy, the desired reliability degree can be obtained, while minimizing the number of preempted connections.",
SAT-based techniques in system synthesis,"SAT-based verification of electronic systems has become very popular in recent years. In this paper, we show that SAT-techniques are also applicable and helpful during the synthesis and the optimization of a system. Therefore, we must consider two questions: (i) how to represent specifications; and (ii) how to quantify properties of embedded systems by boolean formulas. Thus, we reduce the well known binding problem to the Boolean satisfiability problem. Next, we show how to quantify the degree of fault tolerance of a system using quantified Boolean formulas (QBFs). These problems correspond to typical subroutines often used during design space exploration. We show by experiment that problem instances of reasonable size are easily solved by the QBF solver QSOLVE.","Boolean functions,
Embedded system,
Computer science,
Fault tolerant systems,
Algorithms,
Space exploration,
Equations,
Fault tolerance"
Single event upset and hardening in 0.15 /spl mu/m antifuse-based field programmable gate array,"The single event effects and hardening of a 0.15 /spl mu/m antifuse FPGA, the AX device, were investigated by beam test and computer simulation. The beam test showed no permanent damage mode. Functional failures were observed and attributed to the upsets in a control logic circuit, the startup sequencer. Clock upsets were observed and attributed to the single event transients in the clock network. Upsets were also measured in the user flip-flop and embedded SRAM. The hardening technique dealing with each upset mode is discussed in detail. SPICE and three-dimensional mixed-mode simulations were used to determine the design rules for mitigating the multiple upsets due to glancing angle and charge sharing. The hardening techniques have been implemented in the newly fabricated RTAXS device. Preliminary heavy-ion-beam test data show that all the hard-wired hardening solutions are working successfully.","Single event upset,
Field programmable gate arrays,
Circuit testing,
Clocks,
Programmable logic arrays,
Computer simulation,
Logic circuits,
Flip-flops,
Random access memory,
SPICE"
Human-network-based filtering: the information propagation model based on word-of-mouth communication,"In the real world, people get filtered information through word-of-mouth communication. This type of information is ""filtered"" because the information from one user to another is filtered by the people who pass the information to other people. We propose an Information Propagation Model, which simulates word-of-mouth communication on a computer network. In this model, certain information of interest to a user is automatically distributed to neighboring users. Therefore, this model works as an information filtering system filtered by a human network. In this paper we describe the concepts of this model, the features found in this model, and the preliminary experiment, which was carried out by real users. Results of the experiment show both the effectiveness of filtering in our model, and also that the value of information correlates closely with the distance from which the information was transferred.","Information filtering,
Information filters,
Collaboration,
Internet,
Mathematical model,
Informatics,
Information processing,
Information science,
Marine technology,
Computational modeling"
"MaTeLo - statistical usage testing by annotated sequence diagrams, Markov chains and TTCN-3","In this paper, we present a general framework for testing time-critical systems and software, as it is proposed in the European IST project MaTeLo. The main focus is on automatically generating a MCUM (Markov chain usage model) starting from an FDT (formal description technique) description in order to derive TTCN-3 (testing and test control notation version 3) compatible test case definitions. Our approach is a combination of statistical usage testing based on a given MCUM and specification-based testing that is using FDT inputs. Within MaTeLo, special attention is given to international standardized FDT notations, specifically ITU-T MSC (message sequence chart). In addition, we make use of annotations to specify selected non-functional requirements to support the automated software testing of the real time systems. We also defined an XML-based representation format called MCML (Markov chain Markup Language) to build a common interface between various parts of the MaTeLo tool set.","Software testing,
System testing,
Automatic testing,
Logic testing,
Software quality,
Application software,
Mobile communication,
Unified modeling language,
Computer science,
Time factors"
On modeling software architecture recovery as graph matching,"This paper presents a graph matching model for the software architecture recovery problem. Because of their expressiveness, the graphs have been widely used for representing both the software system and its high-level view, known as the conceptual architecture. Modeling the recovery process as graph matching is an attempt to identify a sub-optimal transformation from a pattern graph, representing the high-level view of the system, onto a subgraph of the software system graph. A successful match yields a restructured system that conforms to the given pattern graph. A failed match indicates the points where the system violates specific constraints. The pattern graph generation and the incrementality of the recovery process are the important issues to be addressed. The approach is evaluated through case studies using a prototype toolkit that implements the proposed interactive recovery environment.","Software architecture,
Pattern matching,
Software systems,
Computer architecture,
Reverse engineering,
Software maintenance,
Lattices,
Computer science,
Software prototyping,
Laboratories"
Migrating a multiterabyte archive from object to relational databases,"A commercial, object-oriented database engine with custom tools for data-mining the multiterabyte Sloan Digital Sky Survey archive did not meet its performance objectives. We describe the problems, technical issues, and process of migrating this large data set project, to relational database technology.","Relational databases,
Internet,
Astronomy,
Object oriented databases,
Data models,
Pipelines,
Data mining,
Data engineering,
Humans,
Genomics"
Mobility support with REBECA,"Publish/subscribe (pub/sub) proliferates loose coupling and is touted to facilitate mobility. The inherent loose coupling even allows existing applications to be transferred to mobile environments, if an appropriate infrastructure support is available. However existing pub/sub middleware are mostly optimized for static systems where users as well as the underlying system structure is rather fixed. In this paper we analyze the necessary steps to support mobile clients with publish/subscribe middleware. The REBECA content-based pub/sub service is extended to accommodate to physically mobile clients, offering a location transparent access to the middleware without degrading the previously guaranteed quality of service. The transparent access allows existing applications to be seamlessly transferred from a static to a mobile scenario without having to adapt client applications.","Middleware,
Subscriptions,
Application software,
Quality of service,
Distributed databases,
Degradation,
Mobile communication,
Computer science,
Registers,
Concrete"
Continuous compilation: a new approach to aggressive and adaptive code transformation,"Over the past several decades, the compiler research community has developed a number of sophisticated and powerful algorithms for a variety of code improvements. While there are still promising directions for particular optimizations, research on new or improved optimizations is reaching the point of diminishing returns and new approaches are needed to achieve significant performance improvements beyond traditional optimizations. In this paper, we describe a new strategy based on a continuous compilation system that constantly improves application code by applying aggressive and adaptive code optimizations at all times, from static optimization to online dynamic optimization. In this paper, we describe our general approach and process for continuous compilation of application code. We also present initial results from our research with continuous compilation. These initial results include a new prediction framework that can estimate the benefit of applying code transformations without actually doing the transformation. We also describe results that demonstrate the benefit of adaptively changing application code for embedded systems to make trade-offs between code size, performance, and power consumption.","Adaptive coding,
Phase estimation,
Power system planning,
Computer science,
Runtime,
Dynamic programming,
Lifting equipment,
Distributed processing,
Embedded system,
Cost function"
Buffer overflows: attacks and defenses for the vulnerability of the decade,,"Buffer overflow,
Information security,
Buffer storage,
Computer science,
Contracts,
Registers,
Payloads,
Libraries,
Logic"
Specifying and analyzing early requirements: some experimental results,"Formal Tropos is a specification language for early requirements. It is based on concepts from an agent-oriented early requirement model framework (i/sup */) and extends them with a rich temporal specification language. We demonstrated through a small case study how model checking could be used to verify early requirements written in Formal Tropos. We address issues of methodology and scalability for our earlier proposal. In particular, we propose guidelines for producing a Formal Tropos specification from an i/sup */ diagram and for deciding what model checking technique to use when a particular formal property is to be validated. We also evaluate the scope and scalability of our proposal using a tool, the T-Tool, that maps Formal Tropos specifications to a language that can be handled by NUSMV, a state-of-the-art model checker. Our experiments are based on a course management case study.","Specification languages,
Scalability,
Proposals,
Guidelines,
Software systems,
Computer science,
Certification,
Formal specifications,
Software engineering,
Application software"
Similarity search in sets and categorical data using the signature tree,"Data mining applications analyze large collections of set data and high dimensional categorical data. Search on these data types is not restricted to the classic problems of mining association rules and classification, but similarity search is also a frequently applied operation. Access methods/or multidimensional numerical data are inappropriate for this problem and specialized indexes are needed. We propose a method that represents set data as bitmaps (signatures) and organizes them into a hierarchical index, suitable for similarity search and other related query types. In contrast to a previous technique, the signature tree is dynamic and does not rely on hardwired constants. Experiments with synthetic and real datasets show that it is robust to different data characteristics, scalable to the database size and efficient for various queries.","Multimedia databases,
Data mining,
Data analysis,
Multidimensional systems,
Transaction databases,
Search problems,
Computer science,
Information systems,
Application software,
Information analysis"
Incremental rule learning with partial instance memory for changing concepts,"Learning concepts that change over time is important for a variety of applications in which an intelligent system must acquire and use a behavioral profile. Computer intrusion detection, calendar scheduling, and intelligent user interfaces are three examples. An interesting class of methods for learning such concepts consists of algorithms that maintain a portion of previously encountered examples. Since concepts change over time and these methods store selected examples, mechanisms must exist to identify and remove irrelevant examples of old concepts. In this paper, we describe an incremental rule learner with partial instance memory, called AQ 11 -PM+WAH, that uses Widmer and Kubat's heuristic to adjust dynamically the window over which it retains and forgets examples. We evaluated this learner using the STAGGER concepts and made direct comparisons to AQ-PM and to AQ 11 - PM, similar learners with partial instance memory. Results suggest that the forgetting heuristic is not restricted to FLORA2 the learner for which it was originally designed. Overall, result from this study and others suggest learners with partial instance memory converge more quickly to changing target concepts than algorithms that learn solely from new examples.","Application software,
Intelligent systems,
Intrusion detection,
Calendars,
User interfaces,
Change detection algorithms,
Accuracy,
Computer science,
Computer interfaces,
Processor scheduling"
A graph grammar approach to software architecture verification and transformation,"Software architecture and design are usually modeled and represented by informal diagrams, such as architecture diagrams and UML diagrams. While these graphic notations are easy to understand and are convenient to use, they are not amendable to automated verification and transformation. This paper provides graph grammars for architecture and UML class diagrams. These grammars enable a high level of abstraction for the general organization of a class of software architectures, and form a basis for various analysis and transformations. In this approach, software verification is performed through a syntax analyzer. Architecture transformation is achieved by applying predefined transformation rules.","Software architecture,
Software design,
Computer architecture,
Unified modeling language,
Object oriented modeling,
Computer science,
Computer graphics,
Software performance,
Performance analysis,
Writing"
Tolerance of control-flow testing criteria,"Effectiveness of testing criteria is the ability to detect failure in a software program. We consider not only effectiveness of some testing criterion in itself but a variance of effectiveness of different test sets satisfied the same testing criterion. We name this property ""tolerance"" of a testing criterion and show that, for practical using a criterion, a high tolerance is as well important as high effectiveness. The results of empirical evaluation of tolerance for different criteria, types of faults and decisions are presented. As well as quite simple and well-known control-flow criteria, we study more complicated criteria: full predicate coverage, modified condition/decision coverage and reinforced condition/decision coverage criteria.","Software testing,
Logic testing,
Radio control,
Laboratories,
Computer science,
Australia,
System testing,
Flexible printed circuits,
Computer applications,
Application software"
Appearance management and cue fusion for 3D model-based tracking,This paper presents a systematic approach to acquiring model appearance information online for monocular model-based tracking. The acquired information is used to drive a set of complementary imaging cues to obtain a highly discriminatory observation model. Appearance is modeled as a Markov random field of color distributions over the model surface. The online acquisition process estimates appearance-based on uncertain image measurements and is designed to greatly reduce the chance of mapping non-object image data onto the model. Confidences about the different appearance driven imaging cues are estimated in order to adaptively balance the contributions of the different cues. The discriminatory power of the resulting model is good enough to allow long-duration single-hypothesis model-based tracking with no prior appearance information. Careful evaluation based on real and semi-synthetic video sequences shows that the presented algorithm is able to robustly track a wide variety of targets under challenging conditions.,"Target tracking,
Robustness,
Biological system modeling,
Markov random fields,
Maintenance,
Surface texture,
Computer science,
Electronic mail,
Drives,
Measurement uncertainty"
MUVEES: a PC-based multi-user virtual environment for learning,"This paper summarizes our NSF funded project, a PC-based multi-user learning environment: Multi-User Virtual Environment Experiential Simulator (MUVEES). The goal of this project is to create and evaluate graphical multi-user virtual environments that use digitized museum resources to enhance middle school students' motivation and learning about science. Here, we discuss the design, implementation, and applications of MUVEES. We present its structure, efficient approaches that achieve more realistic avatar behaviors, and pedagogical strategies that foster strong learning outcomes across a wide range of individual student characteristics. Our preliminary results indicate that MUVEES is a powerful vehicle for collaboration and learning. We believe that our system and implementation methods will help improve future multi-user virtual environments.","Virtual environment,
Educational institutions,
Collaboration,
Data analysis,
Power generation,
Avatars,
Vehicles,
Educational technology,
Decision making,
Laboratories"
Teaching computer design using virtual prototyping,"The rapid increase in complexity and size of digital systems has reduced the effectiveness of old design methodologies based on physical prototyping. Prototyping via simulation must be used to achieve design cost and time-to-market goals when designing large digital systems. This virtual prototyping design methodology often permits the first physical prototype to be a manufacturable product. A two-course sequence has been developed to introduce students to this design paradigm. These courses teach virtual prototyping techniques and allow the students to use these techniques to develop a simple computer. The students simulate their designs, and then they implement their designs in hardware using field programmable hardware. This allows the students to complete an entire design cycle from idea to actual hardware implementation and compare their physical results to their simulated results.","Computer science education,
Design automation,
Virtual computers"
Approximation via cost-sharing: a simple approximation algorithm for the multicommodity rent-or-buy problem,"We study the multicommodity rent-or-buy problem, a type of network design problem with economies of scale. In this problem, capacity on an edge can be rented, with cost incurred on a per-unit of capacity basis, or bought, which allows unlimited use after payment of a large fixed cost. Given a graph and a set of source-sink pairs, we seek a minimum-cost way of installing sufficient capacity on edges so that a prescribed amount of flow can be sent simultaneously from each source to the corresponding sink. The first constant-factor approximation algorithm for this problem was recently given by Kumar et al.; however, this algorithm and its analysis are both quite complicated, and its performance guarantee is extremely large. In this paper, we give a conceptually simple 12-approximation algorithm for this problem. Our analysis of this algorithm makes crucial use of cost sharing, the task of allocating the cost of an object to many users of the object in a ""fair"" manner. While techniques from approximation algorithms have recently yielded new progress on cost sharing problems, our work is the first to show the converse - those ideas from cost sharing can be fruitfully applied in the design and analysis of approximation algorithms.","Approximation algorithms,
Algorithm design and analysis,
Cost function,
Economies of scale,
Performance analysis,
Computer science,
Chromium"
Color image segmentation using rival penalized controlled competitive learning,"Color image segmentation has been extensively applied to a lot of applications such as pattern recognition, image compression and matching. In the literature, conventional k-means is one common algorithm used in pixel-based image segmentation. However, it needs to pre-assign an appropriate cluster number before performing clustering, which is an intractable problem from a practical viewpoint. In contrast, the recently proposed rival penalization controlled competitive learning (RPCCL) approach (Cheung, 2002) can perform correct clustering without knowing the exact cluster number in analog with the RPCL (Xu et al., 1993). The RPCCL penalizes the rivals with a strength control such that extra seed points are automatically driven far away from the input data set, but without the de-learning rate selecting problem as the RPCL. In this paper, we further investigate the RPCCL on color image segmentation in comparison with the k-means and RPCL algorithms.","Color,
Image segmentation,
Clustering algorithms,
Pixel,
Nominations and elections,
Computer science,
Pattern recognition,
Automatic control,
Application software,
Image coding"
Improved k-means algorithm in the design of RBF neural networks,"We propose an improved version of the normal k-means clustering algorithm to select the hidden layer neurons of a radial basis function (RBF) neural network. The normal k-means algorithm has been modified to capture more knowledge about the distribution of input patterns and to take care of hyper-ellipsoidal shaped clusters. The RBF neural network with the proposed algorithm has been tested with three different machine-learning data sets. The average recognition rate of an RBF neural network over these data sets has been found to be 93.70% using the proposed improved k-means algorithm, whereas in the method using the normal k-means algorithm, the corresponding value is found to be 88.12%. Clearly, the results show that the performance of the RBF neural network using the proposed modified k-means algorithm has been improved.","Intelligent networks,
Algorithm design and analysis,
Neural networks,
Neurons,
Clustering algorithms,
Euclidean distance,
Senior members,
Computer science,
Testing,
Function approximation"
The use of cumulative inter-frame jitter for adapting video transmission rate,"The concept of cumulative inter-frame jitter has recently been proposed by the authors. The concept has been shown to correctly predict the frame loss rate at the playback buffer. In this paper, we present two rate control mechanisms to illustrate the use of cumulative inter-frame jitter as a network control criterion to provide the required frame-level QoS. The mechanisms adjust the transmission rate of MPEG-2 video at the source either by discarding B frames randomly or by dropping all B frames throughout the congestion period. We combine these mechanisms with different error control strategies for handling network losses, i.e. fixed FEC and adaptive FEC. We study the performance of these mechanisms by simulations. The results reveal two insights. Firstly, dropping B frames allows protection of more important I and P frames for the same amount of bandwidth. This results in lower frame loss rate as observed at the user-level. Secondly, the combination between adapting FEC-level and dropping all B frames outperforms other combinations.","Jitter,
Streaming media,
Forward error correction,
Error correction,
Protection,
Bandwidth,
Internet,
Delay effects,
Computer science,
Programmable control"
Sweep A: space-efficient heuristic search in partially ordered graphs,"We describe a novel heuristic search algorithm, called Sweep A*, that exploits the regular structure of partially ordered graphs to substantially reduce the memory requirements of search. We show that it outperforms previous search algorithms in optimally aligning multiple protein or DNA sequences, an important problem in bioinformatics. Sweep A* also promises to be effective for other search problems with similar structure.","Sequences,
Dynamic programming,
Heuristic algorithms,
Proteins,
DNA,
Bioinformatics,
Search problems,
Upper bound,
Computer science,
History"
A wavelet-based neuro-fuzzy system and its applications,This paper addresses a wavelet-based neuro-fuzzy system (WNFS) for non-linear system identification and control. The WNFS combines the traditional Takagi-Sugeno-Kang (TSK) fuzzy model and the wavelet neural network (WNN). Each fuzzy rule corresponding to a WNN consists of single-scaling wavelets. We adopt the non-orthogonal and compactly supported functions as wavelet neural network bases. The on-line structure/parameter learning algorithm is performed concurrently in the WNFS. The several simulation examples have been given to illustrate the performance and effectiveness of the proposed model.,"Fuzzy neural networks,
Neural networks,
Feedforward neural networks,
Function approximation,
Shape,
Vectors,
Application software,
Computer science,
Chaos,
Computer science education"
Dynamic layering and bandwidth allocation for multisession video broadcasting with general utility functions,"For video broadcasting applications in a wireless environment, layered transmission is an effective approach to support heterogeneous receivers with varying bandwidth requirements. There are several important issues that need to be addressed for such layered video broadcasting systems. At the session level, it is not clear how to allocate bandwidth resources among competing video sessions. For a session with a given bandwidth, questions such as how to set up the video layering structure (i.e., number of layers) and how much bandwidth should be allocated to each layer remain to be answered. The solutions to these questions are further complicated by practical issues such as uneven popularity among video sessions and video layering overhead. This paper presents a systematic study to address these issues for a layered video broadcasting system in a wireless environment. Our approach is to employ a generic utility function for each receiver under each video session. We cast the joint problem of layering and bandwidth allocation (among sessions and layers) into an optimization problem of total system utility among all the receivers. By using a simple 2-step decomposition of inter-session and intra-session optimization, we derive efficient algorithms to solve the optimal layering and bandwidth allocation problem. Practical issues for deploying the optimal algorithm in wireless networks are also discussed. Simulation results show that the optimal layering and bandwidth allocation improves the total system utility.","Channel allocation,
Multimedia communication,
Broadcasting,
Bandwidth,
Video compression,
Video sharing,
Streaming media,
Broadcast technology,
Computer science,
Resource management"
Historical awareness support and its evaluation in collaborative software engineering,"The types of awareness relevant to collaborative software engineering are identified and an additional type, ""historical awareness"" is proposed. This new type of awareness is the knowledge of how software artefacts resulting from collaboration have evolved in the course of their development. The types of awareness that different software engineering environment architectures can support are discussed. A way to add awareness support to our existing OSCAR system, a component of the GENESIS software engineering platform, is proposed. Finally ways of instrumenting and evaluating the awareness support offered by the modified system are outlined.","Collaborative software,
Collaborative work,
Computer architecture,
Software engineering,
Environmental management,
Engineering management,
XML,
Computer science,
Instruments,
Context awareness"
Class decomposition via clustering: a new framework for low-variance classifiers,"We propose a preprocessing step to classification that applies a clustering algorithm to the training set to discover local patterns in the attribute or input space. We demonstrate how this knowledge can be exploited to enhance the predictive accuracy of simple classifiers. Our focus is mainly on classifiers characterized by high bias but low variance (e.g., linear classifiers); these classifiers experience difficulty in delineating class boundaries over the input space when a class distributes in complex ways. Decomposing classes into clusters makes the new class distribution easier to approximate and provides a viable way to reduce bias while limiting the growth in variance. Experimental results on real-world domains show an advantage in predictive accuracy when clustering is used as a preprocessing step to classification.","Clustering algorithms,
Accuracy,
Space exploration,
Computer science,
Classification algorithms,
Testing,
Support vector machines,
Support vector machine classification,
Polynomials,
Kernel"
Single-shot MR imaging using trapezoidal-gradient-based Lissajous trajectories,"A novel single-shot trapezoidal-gradient-based Lissajous trajectory is described and implemented on a 3-tesla magnetic resonance (MR) scanner. A feature of this trajectory is that its sampling points are located on a nonequidistant rectangular grid, which permits the usage of one-dimensional optimal algorithms to increase the robustness and speed of image reconstruction. Another advantage of the trajectory is that two images with different effective echo times can be obtained within a single excitation, which might be used for fast T/sub 2//sup */ mapping, in functional MR imaging scanning of brain activity associated with mental processes. Potential artifacts in reconstructed images were investigated and methods for suppressing these artifacts were developed. Experiments on normal subjects at rest and during brain activation were performed to demonstrate the feasibility of the new sequence.","Magnetic resonance imaging,
Image reconstruction,
Neuroimaging,
Image sampling,
Spirals,
Psychiatry,
Biomedical imaging,
Brain,
Laboratories,
Magnetic resonance"
Variable independence in constraint databases,"In this paper, we study constraint databases with variable independence conditions (vics). Such databases occur naturally in the context of temporal and spatiotemporal database applications. Using computational geometry techniques, we show that variable independence is decidable for linear constraint databases. We also present a set of rules for inferring vics in relational algebra expressions. Using vics, we define a subset of relational algebra that is closed under restricted aggregation.","Relational databases,
Spatial databases,
Spatiotemporal phenomena,
Algebra,
Database languages,
Computer science,
Arithmetic,
Computational geometry,
Calculus,
Constraint theory"
An architecture for flexible service discovery in OCTOPUS,"Service discovery has been drawing much attention from researchers and practitioners. The existing service discovery systems, like SLP, Jini, UPnP and Salutation, provide basic infrastructures where services can announce their presence and users can locate these services across the network. However there are several key issues which are partially solved or have not been well addressed - such as scalability, availability, dynamics and support for multiple matching mechanisms. In this paper, we propose a design for a service locating manager (SLM) system which addresses some of these issues. The SLM system adopts a dynamic hierarchical tree structure and service aggregation for scalability, availability and dynamics, and introduces multiple matching mechanisms which contain an attribute-based and a semantic matching engine. It provides a scalable, distributed, dynamic and robust solution to establish a flexible service discovery architecture. We describe our concepts, architecture and implementation, and present a performance study for our prototype.","Vehicle dynamics,
Scalability,
Protocols,
Java,
XML,
Web and internet services,
IP networks,
Computer architecture,
Computer science,
Tree data structures"
Self-routing in pervasive computing environments using smart messages,"Smart messages (SMs) are dynamic collections of code and data that migrate to nodes of interest in the network and execute on these nodes. A key challenge in programming pervasive computing environments using SMs is the ability to route SMs to nodes named by content. This paper describes the SM self-routing mechanism, which allows SMs to route themselves at each hop in the path toward a node of interest. SM applications can choose among multiple content based routing algorithms, switch dynamically the routing algorithm in use, or even implement the best suited routing algorithm for their needs. The main benefits provided by self-routing are high flexibility and resilience to adverse network conditions. To demonstrate these benefits, we present proof-of-concept implementation, simulation results, and analysis for the SM self-routing mechanism using several content-based routing algorithms. We also show preliminary results for SM routing algorithms executed over our SM prototype implementation.",
Measuring the human side of virtual reality,"The quality of experience (QoE) of any virtual environment is important to its users; if its QoE is too low, users will cease using the VE. Measuring QoE requires measuring users' responses to the VE. There are three main methods for assessing humans in a VE: subjective, performance, and physiological. Each method lets the researcher collect different types of information: user attitude towards a VE, whether the user can complete a task, and whether a VE poses a health danger towards the user. While many virtual researchers also measure telepresence, there is some controversy to its real correlation with QoE.",
Analyzing equivalences of UML statechart diagrams by structural congruence and open bisimulations,"We illustrate how UML statechart diagrams as distinct from statecharts are formalized in the /spl pi/-calculus as a number of processes which communicate via a channel-passing interaction paradigm. Different types of equivalences of statechart diagrams, including isomorphism, strong behavioural equivalence and weak behavioural equivalence are defined in terms of structural congruence, strong open bisimulation and weak open bisimulation of the /spl pi/-calculus. Checking equivalence of any two statechart diagrams is transformed to a problem of verifying whether the corresponding /spl pi/-calculus process expressions are equivalent.",
Approaches of wireless TCP enhancement and a new proposal based on congestion coherence,"TCP is known to have poor performance over unreliable wireless links where packet losses due to transmission errors are misinterpreted as indications of network congestion. TCP enhancements proposed in the literature differ in their signalling and data recovery mechanisms, applicable network configurations, traffic scenarios and locations where required changes are made. In this paper we categorize existing enhancements into several approaches. Motivated by these criteria, we propose a new enhancement that requires only local changes, but applies to a broad range of network and traffic configurations. Comparison with existing algorithm show this new enhancement achieves excellent performance.",
Homomorphism closed vs. existential positive,"Preservations theorems, which establish connection between syntactic and semantic properties of formulas, are a major topic of investigation in model theory. In the context of finite-model theory, most, but not all, preservation theorems are known to fail. It is not known, however, whether the Los-Tarski-Lyndon theorem, which asserts that a first-order sentence is preserved under homomorphisms if it is equivalent to an existential positive sentence, holds with respect to finite structures. Resolving this is an important open question in finite-model theory. In this paper we study the relationship between closure under homomorphism and positive syntax for several nonfirst-order existential logics that are of interest in computer science. We prove that the Los-Tarski-Lyndon theorem holds for these logics. The logics we consider are variable-confined existential infinitary logic, Datalog, and various fragments of second-order logic.",
A secure group key management framework: design and rekey issues,"In many secure group communication models, there exists a group manager that creates the group key and distributes it to every group member. Such group manager is responsible for changing and re-distributing (rekeying) the group key whenever it deems necessary. Many applications will require very fast rekeying so that it is not disruptive to their performance. In this paper, we present a generic software model for secure group key management. We present the main components along with their functionality and interactions. With emphasis on the rekey manager, we discuss two issues that critically impact the rekey time: establishment and maintenance of the logical key hierarchy (LKH), and the key packet construction for a changed key. We show that our novel idea of maintaining balanced LKH as B/sup +/ search tree greatly reduces the number of changed keys compared to an unbalanced LKH. In addition, we show that a rekey packet construction using simple XOR operations between keys instead of the usual encryption technique substantially reduces rekey time. We preformed experiments that demonstrate the effectiveness and feasibility of our approaches.",
More accurate breakdown voltage estimation for the new step-up test method,"The step-up method is used to estimate the impulse breakdown voltages when the electrical insulation is not usable after it is broken. This paper analyses the reliability of the estimates of the underlying breakdown probability distribution in the step-up method, when (1) the observed breakdown voltage itself is available and (2) it is not available. The former case has many advantages compared to the latter case such that (i) the confidence intervals of the estimates become smaller and (ii) the estimates can be obtained with higher probability. Consequently, this paper recommends using the estimates of the underlying distribution for the breakdown voltages instead of the nominal breakdown voltages. Some illustrative examples are given.",
Clustering the Chilean Web,"We perform a clustering of the Chilean Web graph using a local fitness measure, optimized by simulated annealing, and compare the obtained cluster distribution to that of two models of the Web graph. Information on Web clusters can be employed both to validate generation models and to study the properties of the graph. Clusters can also be used in semantics-based grouping of Websites or pages e.g. for indexing and browsing.","Simulated annealing,
Indexing,
Length measurement,
Clustering methods,
Laboratories,
Computer science,
Performance evaluation,
Computational modeling,
Measurement standards,
Organizing"
"Configuring Web services, using structuring and techniques from agent configuration","We explore the use of an agent factory for the composition of Web services. Previously we proposed a structuring approach for automated reconfiguration of agents by an agent factory. The question is whether the same approach can be applied to Web service composition, i.e. whether DAML-S descriptions of Web services offer enough structure for automated configuration by the agent factory. An example trace of the agent factory for configuration of DAML-S Web services illustrates this approach.","Web services,
Production facilities,
Microstrip,
Computer science,
Protocols,
Business,
Workflow management software,
Humans,
Databases,
Logic"
Cost-optimization of the IPv4 zeroconf protocol,,"Protocols,
IP networks,
Ad hoc networks,
Home appliances,
Network servers,
Broadcasting,
Probes,
Computer science,
Computer network reliability,
Costs"
Design of a Self-Organizing Learning Array system,This paper discusses a concept of Self-Organizing Learning Array developed for programmable hardware realization. This system is designed for solving an unspecified machine-learning problems such as classification and recognition. Basic design of the array including neurons interconnections and organization is described. Symbolic values assignment method and self-organizing principle are also discussed in this paper.,"Neurons,
Hardware,
Solar power generation,
Organizing,
Training data,
Testing,
Feeds,
Entropy,
Computer science,
Genetic algorithms"
On countermeasures to traffic analysis attacks,"We make three contributions. First, we propose Shannon's perfect secrecy theory as a foundation for developing countermeasures to traffic analysis attacks on information security systems. A system violating the perfect secrecy conditions can leak mission critical information. Second, we suggest statistical pattern recognition as a fundamental technology to test an information system's security. This technology can cover a large category of testing approaches because of statistical pattern recognition's maturity and abundant techniques. Third, researchers have proposed traffic padding as countermeasures to traffic analysis attacks. By applying the proposed information assurance testing framework, we find that constant rate traffic padding does not satisfy Shannon's perfect secrecy conditions because of its implementation mechanism. We design a variant rate traffic padding strategy as an alternative, which is validated by both theoretical analysis and empirical results.","Protection,
Information security,
Pattern recognition,
Payloads,
System testing,
Contracts,
Computer science,
Information analysis,
Mission critical systems,
Cryptography"
"Generating, selecting and prioritizing test cases from specifications with tool support","The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.","Computer aided software engineering,
Software testing,
Classification tree analysis,
System testing,
Computer science,
Software systems,
Automatic testing,
Life testing,
Tree graphs,
Information technology"
An autonomous assistant robot for book manipulation in a library,"This paper presents work in progress towards a complete system working to assist users in a library. With this aim, the system must be capable to looking for a specific book in a shelf, asked by any user, and whether it is found, deliver it as soon as possible to the user. To get its objectives the system integrates automatic object recognition, visually guided grasping, and force feedback, among other advanced capabilities. Implementation details about the main modules developed presently are shown. Finally, after success in preliminary results obtained in our campus, we are encouraged to fallow working in this way to obtain the complete prototype.","Books,
Libraries,
Grasping,
Mobile robots,
Object recognition,
Navigation,
User interfaces,
Computer science,
Force feedback,
Prototypes"
Point pattern matching and applications-a review,"Feature-based methods in vision analysis often encounter the problem of correspondences between features of two related patterns. The features may be points, lines, curves and surfaces/regions. Point pattern matching (PPM) is a primary and essential approach for establishing a correspondence within two related patterns. Applications exist in wide circumstances. Numerous techniques related to PPM have been studied within a rich and extensive literature, encompassing both theoretical and practical problem domains. In this paper, we provide a review of the PPM techniques and their applications from three aspects: 1) PPM under rigid/affine motion, 2) PPM under non-rigid/elastic motion, and 3) PPM in a dynamic sequence. We also anticipate the trend for the future in adapting existing techniques to novel algorithms for non-rigid PPM.","Pattern matching,
Radar tracking,
Pattern recognition,
Pattern analysis,
Image registration,
Computer vision,
Context modeling,
Application software,
Computer science,
Sequences"
Illumination independent color-based face detection,"Computer vision is one out of many areas that want to understand the process of human functionality and copy that process with intention to complement human life with intelligent machines. For better human-computer interaction it is necessary for the machine to see people. This can be achieved by employing face detection algorithms, like the one used in the installation ""15 Seconds of Fame"" (F. Solina et al., 2002). Mentioned installation unites the areas of modern art and technology. Its algorithm is based on skin-color detection. One of the problems this and similar algorithms have to deal with is sensitivity to the illumination conditions under which the input image is captured. Hence illumination sensitivity influences face detection results. This problem is being more or less successfully solved by the use of color compensation and color constancy methods. In this work some of these methods are described, realized and tested. Their basic intention is to eliminate the influence of non-standard illumination from images. Tests that were performed showed that methods apply positive influence on face detection results.",
Audio transmission over the Internet: experiments and observations,"The performance of IP telephony systems is highly dependent on the audio codecs and their reaction to packet loss and instantaneous delays. Understanding the interaction between audio encoding and the dynamic behavior of the Internet is significant for designing adaptive audio transport mechanisms. For this purpose, we conducted a large-scale audio transmission experiment over the Internet in a 12-month period using various Internet sites. As a result of this experiment, we have made a number of new observations to assess the audio quality of G.711 and G.728 codes under different loss and delay conditions. The paper also states a number of recommendations for implementing efficient adaptive FEC and playout mechanisms.","Codecs,
IP networks,
Internet telephony,
Delay,
Humans,
Bandwidth,
Jitter,
Computer science,
Information systems,
Performance loss"
Visualizing design patterns with a UML profile,"In this paper, we present a UML profile which defines new stereotypes, tagged values and constraints for visualizing design patterns in UML diagrams. These new stereotypes and tagged values are attached to a modeling element to explicitly represent the role the modeling element plays in a design pattern so that the user can identify the pattern in a UML diagram.","Visualization,
Unified modeling language,
Software design,
Software systems,
Computer science,
Application software,
Communication system software,
Packaging,
Vocabulary"
Using the defining issues test for evaluating computer ethics teaching,"Since the publication in 1997 of the Australian Computer Society's (ACS) body of knowledge for computing professionals, a higher priority has been given to the teaching of computer ethics in Australia. This paper evaluates an undergraduate computer ethics teaching program using the defining issues test of moral judgment. A ""before-and-after with a control group"" research design was used. For both the experimental and control groups, a general increase in moral judgment development was observed over the semester. The experimental group exhibited a significantly larger increase in moral judgment development than the control group. However, it was found to be the result of an increase in the moral development of the female students rather than the male students. The results are discussed and the implications for studies in an education context are outlined.","Computer science education,
Engineering profession"
Routing with many additive QoS constraints,"A fundamental problem in QoS routing is to find a path between a specified source-destination node pair that satisfies a set of end-to-end quality of service constraints. We study this problem in a communication system where there are multiple additive quality of service parameters associated with each link. It is well-known that the multi-constrained path selection problem (MCPS) is NP-complete. In this paper, we present a fully polynomial time approximation scheme for an optimization version of the MCPS problem. This means that for any given /spl epsi/ > 0, we can compute, in time bounded by a polynomial of the input size of the problem and in 1//spl epsi/, a solution whose cost is at most (1 + /spl epsi/) of that of the optimal solution.","Routing,
Quality of service,
Polynomials,
Delay,
Computer science,
Cost function,
Approximation algorithms,
Bandwidth,
Operations research,
NP-hard problem"
A Low-Loss Distributed 2-Bit W-Band MEMS Phase Shifter,"A low-loss 2-bit wide-band distributed microelectromechanical system transmission line (DMTL) phase shifter has been developed on a 500 μm glass substrate for W-band operation. The coplanar waveguide (CPW) design is periodically loaded by MEMS switches in series with high-Q MAM (Metal-Air-Metal) capacitors, and shows a reflection coefficient better than -11 dB through the whole W-band frequency range (75-110 GHz) and phase shifts of 0°, 89.3°, 180.1°, 272° at 81 GHz with an average insertion losses of -2.2 dB. This is currently the lowest-loss phase shifter at W-band frequencies.","Micromechanical devices,
Phase shifters,
Coplanar waveguides,
Frequency,
Wideband,
Microelectromechanical systems,
Coplanar transmission lines,
Glass,
Microswitches,
Capacitors"
Networked video surveillance using multiple omnidirectional cameras,"Remote surveillance is widely utilized in banks, shops, offices, at home and so on. In most conventional remote surveillance systems, fixed or active cameras with a narrow field of view are generally used in order to acquire an image of the remote site. This paper proposes a new networked surveillance system. The proposed surveillance system, which uses multiple omnidirectional cameras and network, is based on a server/client model: the server computers, each of which is connected to an omnidirectional video camera, are placed in the surveillance area and the client computer is placed on the user side. The servers detect moving objects and estimate their directions from sensors. The client estimates object positions from their directions received from the servers and presents object-centered perspective images to the user. In experiments, the implemented proposed system can do those at real-time.","Video surveillance,
Cameras,
Object detection,
Mirrors,
Network servers,
Information science,
Computer networks,
Lenses,
Shape,
Real time systems"
General Schema Theory for Genetic Programming with Subtree-Swapping Crossover: Part II,"This paper is the second part of a two-part paper which introduces a general schema theory for genetic programming (GP) with subtree-swapping crossover (Part I (Poli and McPhee, 2003)). Like other recent GP schema theory results, the theory gives an exact formulation (rather than a lower bound) for the expected number of instances of a schema at the next generation. The theory is based on a Cartesian node reference system, introduced in Part I, and on the notion of a variable-arity hyperschema, introduced here, which generalises previous definitions of a schema. The theory includes two main theorems describing the propagation of GP schemata: a microscopic and a macroscopic schema theorem. The microscopic version is applicable to crossover operators which replace a subtree in one parent with a subtree from the other parent to produce the offspring. Therefore, this theorem is applicable to Koza's GP crossover with and without uniform selection of the crossover points, as well as one-point crossover, size-fair crossover, strongly-typed GP crossover, context-preserving crossover and many others. The macroscopic version is applicable to crossover operators in which the probability of selecting any two crossover points in the parents depends only on the parents' size and shape. In the paper we provide examples, we show how the theory can be specialised to specific crossover operators and we illustrate how it can be used to derive other general results. These include an exact definition of effective fitness and a size-evolution equation for GP with subtree-swapping crossover.","Standard Crossover,
Genetic Programming,
Schema Theory"
From search engines to question-answering systems the need for new tools,This paper deals with fuzzy logic based approach to question-answering systems. To achieve significant question answering capability it is necessary to develop methods of dealing 1. with the reality 2.with world knowledge-and especially knowledge about underlying probabilities is perception based. In this approach PNL is employed to represent infornation in the world knowledge database (WKD).,"Search engines,
Logic,
Computer science,
Artificial intelligence,
Contracts,
NASA,
Natural languages,
Laboratories,
History,
Expert systems"
Applying bulk insertion techniques for dynamic reverse nearest neighbor problems,"Reverse nearest neighbors queries have emerged as an important class of queries for spatial and other types of databases. The Rdnn-tree is an R-tree based structure that has been shown to perform outstandingly for such kind of queries. However, one practical problem facing it (as well as other type of indexes) is how to effectively construct the index from stretch. In this case, the cost of constructing and maintaining a Rdnn-Tree is about twice the cost of an R-Tree. Normal insertion into a Rdnn-Tree is performed one point at a time, known as single point insertion. The question arises, can insertion be improved there by reducing the construction and maintenance cost. In this paper we propose a bulk-loading technique, which is capable of significantly, improve the performance of constructing the index from stretch, as well as insert a large amount of data. Experiments show that our method outperforms the single point insertion significantly.","Nearest neighbor searches,
Costs,
Spatial databases,
Indexing,
Indexes,
Computer science,
Database systems,
Shape,
Geographic Information Systems,
Buildings"
PYLON: an architectural framework for ad-hoc QoS interconnectivity with access domains,"The unique nature of mobile ad-hoc networks (MANETs) imposes several challenges when providing QoS. Traffic travelling on a MANET can be local traffic arriving from and targeting a node within the ad-hoc network. Traffic that is not local to the MANET is likely to travel over fixed networks that employ DiffServ, IntServ, or a combination of both. This paper investigates the interaction and the relation between a MANET network and a hosting access domain that provide QoS support potentially based on two distinct paradigms (namely IntServ, and DiffServ). The objective is to achieve a high level of autonomy. In this paper we propose a framework solution to the inter-domain agreements and the use of aggregate RSVP for collective resource reservations.","Telecommunication traffic,
Mobile ad hoc networks,
Diffserv networks,
Ad hoc networks,
Traffic control,
LAN interconnection,
Aggregates,
Scalability,
Computer science,
Systems engineering and theory"
An intelligent e-learning system with authoring and assessment mechanism,"In the recent years, with the improvement of network technologies and hardware supports, distance education has been provided to students all over the world. Students are able to study anytime and anywhere through the Internet. Instructors are able to teach students without being in classroom. It seemed that both instructors and students are benefited through internet technology, yet this new style of teaching and learning creates new problems. As an instructor in distance education, one needs to prepare new course materials for students who are on-line learners. Therefore, all instructors have to learn new technology which could be difficult to learn. As a student in distance education, one needs to know how he does during learning period. As a result, a good student assessment algorithm need to be brought out to judge students' performance. In this paper, we proposed a solution for these problems in distance education.","Intelligent systems,
Electronic learning,
Distance learning,
Computer aided instruction,
Educational technology,
Internet,
Computer science education,
Intelligent networks,
Computer science,
Hardware"
Optimal bandwidth reservation schedule in cellular networks,"Efficient bandwidth allocation strategy with simultaneous fulfillment of QoS requirement of a user in a mobile cellular network is still a critical and an important practical issue. We explore the problem of finding the reservation schedule that would minimize the amount of time for which bandwidth has to be allocated in a cell while meeting the QoS constraint. With the knowledge about the arrival and residence time distribution of a user in a cell, the above problem can be optimally solved using a dynamic programming based approach in polynomial time. To be able to use the solution, we provide a mechanism for constructing the arrival/residence time distribution based on the measurement of hand-off events in a cell. The above solution allows us to propose an optimal time based bandwidth reservation and call admission scheme. By being scalable and distributed, the proposed scheme justifies for practical implementation. Simulations results are also presented to show the effectiveness of the scheme to achieve the target QoS level and optimal bandwidth utilization.",
Programmable end system services using SIP,"In Internet telephony, end systems can take a much larger role in providing services than in traditional telephone systems. We analyze the importance of end system services and describe the services and the service logic execution environment (SLEE) implemented in out SIP user agent, SIPC. Since we believe that end system services differ in their requirements from network services, we define a new service creation scripting language call language for end system services (LESS). Compared with other service creation languages, LESS is extensible, can be easily understood by non-programmers and contains commands and events for direct user interaction and the control of media applications.","Internet telephony,
Automatic control,
Control systems,
Logic,
Network servers,
Home appliances,
Computer science,
Web server,
Ethernet networks,
Personal communication networks"
Atomic operations for task scheduling for systems based on communication on-the-fly between SMP clusters,,"Computer architecture,
Processor scheduling,
Communication switching,
Message passing,
Runtime,
Distributed computing,
Computer science,
Proposals,
Scheduling algorithm,
Scalability"
Determination of the signal strength with the computer analysis of the material structure,"At present, there are a number of software packages for the quantitative estimation of the characteristics of metallographic structure. One of the problems of using these software packages, however, is determining the phase to which the microstructure components belong. This paper proposes three methods based on the phase's signal strength: 1) dotted, 2) graphical and 3) visual. In the dotted method, the intensity level is determined by using few points on the different particles of the same phase. In the graphical method, the phase's intensity level is determined based on the graphical dependence of the phase quantity on the intensity level. In the visual method, the change in the intensity level results in the conversion of the phase color until all particles of the desired phase change color. Thus, the upper level for the dark phase and the lower for the light one can be determined.","Signal analysis,
Materials science and technology,
Iron,
Nanostructured materials,
Plastics,
Microstructure,
Etching,
Pressing,
Mice,
Graphics"
Integrating fuzziness into OLAP for multidimensional fuzzy association rules mining,"We contribute to the ongoing research on multidimensional online association rules mining by proposing a general architecture that utilizes a fuzzy data cube for knowledge discovery. Three different methods are introduced to mine fuzzy association rules in the constructed fuzzy data cube, namely single dimension, multidimensional and hybrid association rules mining. Experimental results obtained for each of the three methods on the adult data of the United States census in 2000 show their effectiveness and applicability.","Multidimensional systems,
Association rules,
Data mining,
Fuzzy systems,
Fuzzy sets,
Computer architecture,
Databases,
Computer science,
Data engineering,
Knowledge engineering"
Confluence of parameters in model based tracking,"During the last decade, model based tracking of objects and its necessity in visual servoing and manipulation has been advocated in a number of systems. Most of these systems demonstrate robust performance for cases where either the background or the object are relatively uniform in color. In terms of manipulation, our basic interest is handling of everyday objects in domestic environments such as a home or an office. In this paper, we consider a number of different parameters that effect the performance of a model-based tracking system. Parameters such as color channels, feature detection, validation gates, outliers rejection and feature selection are considered here and their affect to the overall system performance is discussed. Experimental evaluation shows how some of these parameters can successfully be evaluated (learned) on-line and consequently improve the performance of the system.",
Handling Heterogeneity in Shared-Disk File Systems,"We develop and evaluate a system for load management in shared-disk file systems built on clusters of heterogeneous computers. The system generalizes load balancing and server provisioning. It balances file metadata workload by moving file sets among cluster server nodes. It also responds to changing server resources that arise from failure and recovery and dynamically adding or removing servers. The system is adaptive and self-managing. It operates without any a-priori knowledge of workload properties or the capabilities of the servers. Rather, it continuously tunes load placement using a technique called adaptive, non-uniform (ANU) randomization. ANU randomization realizes the scalability and metadata reduction benefits of hash-based, randomized placement techniques. It also avoids hashing's drawbacks: load skew, inability to cope with heterogeneity, and lack of tunability. Simulation results show that our load-management algorithm performs comparably to a prescient algorithm.","File systems,
File servers,
Hardware,
Load management,
Network servers,
Storage area networks,
Scalability,
Permission,
Computer science,
Adaptive systems"
Rank bounds and integrality gaps for cutting planes procedures,"We present a new method for proving rank lower bounds for Cutting Planes (CP) and several procedures based on lifting due to Lovasz and Schrijver (LS), when viewed as proof systems for unsatisfiability. We apply this method to obtain the following new results: first, we prove near-optimal rank bounds for Cutting Planes and Lovasz-Schrijver proofs for several prominent unsatisfiable CNF examples, including random kCNF formulas and the Tseitin graph formulas. It follows from these lower bounds that a linear number of rounds of CP or LS procedures when applied to relaxations of integer linear programs is not sufficient for reducing the integrality gap. Secondly, we give unsatisfiable examples that have constant rank CP and LS proofs but that require linear rank resolution proofs. Thirdly, we give examples where the CP rank is O(log n) but the LS rank is linear. Finally, we address the question of size versus rank: we show that, for both proof systems, rank does not accurately reflect proof size. Specifically, there are examples with polynomial-size CP/LS proofs, but requiring linear rank.","Integer linear programming,
Linear programming,
Polynomials,
Councils,
Integral equations,
Ellipsoids,
Optimization methods,
Instruments,
Computer science"
Ubiquitous recommendation systems,"In many popular visions of ubiquitous computing, the environment proactively responds to individuals who inhabit the space. For example, a display magically presents a personalized advertisement, the most relevant video feed, or the desired page from a secret government document. Such capability requires more than an abundance of networked displays, devices, and sensors; it relies implicitly on recommendation systems that either directly serve the end user or provide critical services to some other application. As recommendation systems evolve to exploit new advances in ubiquitous computing technology, researchers and practitioners from technical and social science disciplines must collaborate to address the challenges to their effective implementation. Although it may be impossible to perfectly anticipate each individual's needs at any place or time, ubiquitous computing will enable such systems to help people cope with an expanding array of choices.","Filtering,
Pervasive computing,
Collaboration,
Ubiquitous computing,
Predictive models,
Navigation,
Motion pictures,
DVD,
Biomedical imaging,
Privacy"
Towards predictable real-time Java object request brokers,"Distributed real-time and embedded (DRE) applications often possess stringent quality of service (QoS) requirements. Designing middleware for DRE applications poses several challenges to object request broker (ORB) developers. This paper provides the following contributions to the study of middleware for DRE applications. First, we outline the challenges present in one of the principal ORB components - the portable object adapter (POA) focusing on predictable and scalable demultiplexing. Second, we describe how these challenges are addressed in ZEN, which is an implementation of Real-time CORBA that runs atop jRate, an ahead-of-time compiler that implements most of the Real-Time Specification for Java (RTSJ). Third, we qualitatively and quantitatively compare ZEN's demultiplexing strategies with those of other popular Java ORBs, including JacORB, Sun JDK ORB, and ORBacus. Our results show that ZEN and jRate incorporate the strategies necessary to enable predictability using standards-based middleware and also provide a baseline for what can be achieved by combining Real-time CORBA and RTSJ.","Java,
Middleware,
Demultiplexing,
Yarn,
Computer science,
Application software,
Quality of service,
Memory management,
Open source software,
Real time systems"
Enhanced smart-card based license management,"In many e-commerce (electronic commerce) situations, the owner of a digital object wants to enforce policies on the object after the object is in the customer's hands. The object can be thought of as being software, because data is often protected by forcing access to it to take place through a particular authorized software (e.g., a ""reader"" for an encrypted media file, in which case a license to view the movie is, in some sense, a ""software license""). One of the ways that were proposed for such policy enforcement is the use of smart cards. We describe an enhanced solution to software license management based on tamper-resistant smart cards. Our public-key protocols for binding software licenses to smart cards improve on previous schemes in that they support flexible and partial transfers of licenses between cards. The license is verified by checking the presence of the associated card. The user can therefore have several software licenses all of which are bound to one card, to avoid juggling several cards in and out of the card reader.","Licenses,
Smart cards,
Protocols,
Computer science,
Protection,
Cryptography,
Motion pictures,
Public key,
Internet,
Contracts"
All-around display for video avatar in real world,"This paper describes the methodology and prototype for an all-around display system for video avatar presentation in the real world. This system enables us to reconstruct a video avatar which users can look at from all around, by spinning a flat panel display which has a small viewing angle and changing the image on the display panel depending on the display's orientation.","Avatars,
Flat panel displays,
Image reconstruction,
Spinning,
Motion pictures,
Two dimensional displays,
Prototypes,
Information science,
Humans,
Virtual environment"
Analysis and interface for instructional video,"We present a new method for segmenting, and a new user interface for indexing and visualizing, the semantic content of extended instructional videos. Using various visual filters, key frames are first assigned a media type (board, class, computer, illustration, podium, and sheet). Key frames of media type board and sheet are then clustered based on contents via an algorithm with near-linear cost. A novel user interface, the result of two user studies, displays related topics using icons linked topologically, allowing users to quickly locate semantically related portions of the video. We analyze the accuracy of the segmentation tool on 17 instructional videos, each of which is from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%.","Indexing,
User interfaces,
Visualization,
Filters,
Image segmentation,
Computer science,
Clustering algorithms,
Costs,
Computer displays,
Content based retrieval"
Network convergence and the NAT/Firewall problems,"Voice over IP technology is fueling the rapid growth on network convergence and we are seeing the successful deployment of converged networks within enterprises. However, most enterprises today sit behind firewalls and also use private IP addressing behind NATs (network address translators). These NATs and firewalls cause significant problems for multimedia over IP to work and function properly. There are presently two standards for VoIP signaling: H.323 (from ITU-T) and SIP (session initiation protocol from IETF). In this paper we present the details of the problems and issues associated with NATs/firewalls and then survey some ways to solve this problem for SIP. There is no single best solution yet. However, this paper discusses how such a simple and elegant solution can be built. This problem remains a significant obstacle for the successful adoption of convergence as security has become even more important to enterprises than adoption of emerging technologies.","Convergence,
Network address translation,
Information science,
Internet telephony,
Protocols,
Security,
Multimedia communication,
Routing,
Protection,
Joining processes"
Discriminative training of Bayesian Chow-Liu multinet classifiers,"Discriminative classifiers such as support vector machines directly learn a discriminant function or a posterior probability model to perform classification. On the other hand, generative classifiers often learn a joint probability model and then use Bayes rules to construct a posterior classifier from this model. In general, generative classifiers are not as accurate as discriminant classifier. However generative classifiers provide a principled way to handle the missing information problems, which discriminant classifiers cannot easily deal with. To achieve good performances in various classification tasks, it is better to combine these two strategies. In this paper, we develop a novel method to iteratively train a kind of generative Bayesian classifier: Bayesian Chow-Liu multinet classifier in a discriminative way. Different with the traditional Bayesian multinet classifiers, our discriminative method adds into the optimization function a penalty item, which represents the divergence between classes as big as possible. We state the theoretical justification, outline of the algorithm and also perform a series of experiments to demonstrate the advantages of our method. The experiments results are promising and encouraging.",
Application-bypass broadcast in MPICH over GM,"Processes of a parallel program can become unsynchronized, or skewed, during the course of running an application. Processes can become skewed as a result of unbalanced or asymmetric rode, or through the use of heterogeneous systems, where nodes in the system have different performance characteristics, as well as random, unpredictable effects such as the processes not being started at exactly the same time, or processors receiving interrupts during computation. Geographically distributed systems may have more severe skew because of variable communication times. Such skew can have a significant impact on the performance of collective communication operations which impose an implicit synchronization. The broadcast operation in MPICH is one such operation. An application-bypass broadcast operation is one which does not depend on the application running at a process to make progress. Such an operation would not be as sensitive to process skew. This paper describes the design and implementation of an application-bypass broadcast operation. We evaluated the implementation and find a factor of improvement of up to 16 for application-bypass broadcast compared to non-application-bypass broadcast when processes are skewed. Furthermore we see that as the system size increases, the effects of skew on non-application-bypass broadcast also increase. The application-bypass broadcast is much less sensitive to process skew which makes it more scalable than the non-application-bypass broadcast operation.","Broadcasting,
Computer networks,
Laboratories,
Intelligent networks,
Information science,
US Department of Energy,
Distributed computing,
System performance,
Delay,
Grid computing"
Ad-hoc association-rule mining within the data warehouse,"Many organizations often underutilize their existing data warehouses. In this paper, we suggest a way of acquiring more information from corporate data warehouses without the complications and drawbacks of deploying additional software systems. Association-rule mining, which captures co-occurrence patterns within data, has attracted considerable efforts from data warehousing researchers and practitioners alike. Unfortunately, most data mining tools are loosely coupled, at best, with the data warehouse repository. Furthermore, these tools can often find association rules only within the main fact table of the data warehouse (thus ignoring the information-rich dimensions of the star schema) and are not easily applied on non-transaction level data often found in data warehouses. In this paper, we present a new data-mining framework that is tightly integrated with the data warehousing technology. Our framework has several advantages over the use of separate data mining tools. First, the data stays at the data warehouse, and thus the management of security and privacy issues is greatly reduced. Second, we utilize the query processing power of a data warehouse itself, without using a separate data-mining tool. In addition, this framework allows ad-hoc data mining queries over the whole data warehouse, not just over a transformed portion of the data that is required when a standard data-mining tool is used. Finally, this framework also expands the domain of association-rule mining from transaction-level data to aggregated data as well.","Data mining,
Data warehouses,
Databases,
Warehousing,
Query processing,
Companies,
Costs,
Computer science,
Software systems,
Association rules"
Indexing weighted-sequences in large databases,"We present an index structure for managing weighted-sequences in large databases. A weighted-sequence is defined as a two-dimensional structure where each element in the sequence is associated with a weight. A series of network events, for instance, is a weighted-sequence in that each event has a timestamp. Querying a large sequence database by events' occurrence patterns is a first step towards understanding the temporal causal relationships among the events. The index structure proposed enables us to efficiently retrieve from the database all subsequences, possibly noncontiguous, that match a given query sequence both by events and by weights. The index method also takes into consideration the nonuniformfrequency distribution of events in the sequence data. In addition, our method finds a broad range of applications in indexing scientific data consisting of multiple numerical columns for discovery of correlations among these columns. For instance, indexing a DNA microarray that records expression levels of genes under different conditions enables us to search for genes whose responses to various experimental perturbations follow a given pattern. We demonstrate, using real-world data sets, that our method is effective and efficient.","Indexing,
Databases,
Sequences,
Data engineering,
Indexes,
DNA,
Data analysis,
Computer science,
Engineering management,
Information retrieval"
Artificial intelligent techniques for intrusion detection,"This paper concerns using support vector machines (SVMs) and artificial neural networks (ANNs) for intrusion detection. We investigate and compare the performance of IDSs using SVMs and ANNs, using a well-known set of intrusion evaluation data gathered by DARPA. Through a variety of comparative experiments, it is found that, with appropriately chosen kernel functions, SVMs outperform ANNs in at least three critical aspects of IDS performance: (1) Accuracy - SVMs achieve very-high accuracy (in the high 90% range) than the best-trained ANNs, (2) Training Time and Testing Time - SVMs' training time and testing time are an order of magnitude faster than ANNs', and (3) Scalability - SVMs scale much better than ANNs. SVMs, therefore, provide suitable tools for building signature-based IDSs. We describe our investigation methodology, report experimental results, and conclude by describing an ongoing effort of a SVM and agents-based IDS that delivers enhanced performance, that possesses enhanced intrusion response capability and that is applicable to wireless networks.","Artificial intelligence,
Intrusion detection,
Support vector machines,
Artificial neural networks,
Testing,
Neural networks,
Machine intelligence,
Computer science,
Scalability,
Intelligent networks"
Incorporating predicate information into branch predictors,"Predicated execution can be used to alleviate the costs associated with frequently mispredicted branches. This is accomplished by trading the cost of a mispredicted branch for execution of both paths following the conditional branch. In this paper we examine two enhancements for branch prediction in the presence of predicated code. Both of the techniques use recently calculated predicate definitions to provide a more intelligent branch prediction. The first branch predictor, called the squash false path filter, recognizes fetched branches known to be guarded with a false predicate and predicts them as not-taken with 100% accuracy. The second technique, called the predicate global update branch predictor, improves prediction by incorporating recent predicate information into the branch predictor. We use these techniques to aid the prediction of region-based branches. A region-based branch is a branch that is left in a predicated region of code. A region-based branch may be correlated with predicate definitions in the region in addition to those that define the branch's guarding predicate.","Computer architecture,
Costs,
Computer science,
Parallel processing,
Delay,
Mathematics,
Filters,
Computer aided instruction,
Concurrent computing,
VLIW"
Local tomography property of residual minimization reconstruction with planar integral data,"Residual minimization with the well-known conjugate gradient (CG) algorithm has been applied to medical image reconstruction for years. The main advantage of this method is its fast convergence rate. In this paper, we point out that this method has another property-local tomography, when this image reconstruction method is applied to planar integral projections. By local tomography we mean the following: The object is relatively large, the entire object is not sufficiently measured, and the projections are truncated due to a small detector size. However, a small region-of-interest (ROI) is sufficiently measured. The small ROI is able to be exactly reconstructed. This local tomographic property is found for planar integral data only and is not found for line-integral measurements. Iterative local tomography has been applied to cos/spl alpha//r weighted planar integral data through computer simulations and phantom experiments. An efficient projector that models the cos/spl alpha//r weighting factor is also developed.","Tomography,
Image reconstruction,
Minimization methods,
Character generation,
Biomedical imaging,
Convergence,
Size measurement,
Object detection,
Detectors,
Computer simulation"
Sound methods and effective tools for engineering modeling and analysis,"Modeling and analysis is indispensable in engineering. To be safe and effective, a modeling method requires a language with a validated semantics; feature-rich, easy-to-use, dependable tools; and low engineering costs. Today we lack adequate means to develop such methods. We present a partial solution combining two techniques: formal methods for language design, and package-oriented programming for function and usability at low cost. We have evaluated the approach in an end-to-end experiment. We deployed an existing reliability method to NASA in a package-oriented tool and surveyed engineers to assess its usability. We formally specified, improved, and validated the language. To assess cost, we built a package-based tool for the new language. Our data show that the approach can enable cost-effective deployment of sound methods by effective tools.","Acoustical engineering,
Packaging,
Reliability engineering,
Usability,
Design engineering,
Computer science,
Functional programming,
Cost function,
NASA,
Educational institutions"
Towards personalised and collaborative learning management systems,"We propose a framework for the integration of personalisation and collaboration in virtual learning environments (VLEs). We describe two models of personalisation, i.e. assessment-based personalised learning via suggestions and personalised space. Collaboration is achieved via learning service, which provide specific collaboration services for learners. Although the proposed system integrates personalisation and collaboration in the learning environment, users have the option of switching off the personalisation so the learning space can act as a collaborative environment.","Collaborative work,
Collaboration,
Space technology,
Environmental management,
Computer science,
Management information systems,
Collaborative tools,
Education,
Heart,
Feedback"
Practical scheduling algorithms for high-performance packet switches,"As buffer-less scheduling algorithms reach their practical limitations due to higher port numbers and data rates, buffered crossbars gained a lot of interest recently because of the great potential they have in solving the complexity and scalability issues faced by their buffer-less predecessors. In particular, the internally buffered switching architecture was shown, through distributed scheduling algorithms, to be able to sustain the current and expected increases in Internet throughput rates. In this paper, we propose a class of distributed scheduling algorithms for the internally buffered crossbar switching architecture. As will be shown, the distributed nature of these algorithms makes them of high practical value. That is, they can be implemented in real-time for high-speed input traffic. In addition, we will demonstrate, through simulation, that these scheduling algorithms outperform state-of-the-art related algorithms in this area.",
Connecting brains with machines: the neural control of 2D cursor movement,"The paper presents a review of our neural prosthesis research program and provides a brief introduction to the field. We focus on four key problems: sensing, neural encoding, neural decoding, and interface design. We explore these problems and present our current solutions which have led to the direct cortical control of unconstrained 2D cursor movement.","Joining processes,
Prosthetics,
Decoding,
Encoding,
Microelectrodes,
Spinal cord injury,
Robot sensing systems,
Image reconstruction,
Electrodes,
Computer science"
Hierarchical clustering for unstructured volumetric scalar fields,"We present a method to represent unstructured scalar fields at multiple levels of detail. Using a parallelizable classification algorithm to build a cluster hierarchy, we generate a multiresolution representation of a given volumetric scalar data set. The method uses principal component analysis (PCA) for cluster generation and a fitting technique based on radial basis functions (RBFs). Once the cluster hierarchy has been generated, we utilize a variety of techniques for extracting different levels of detail. The main strength of this work is its generality. Regardless of grid type, this method can be applied to any discrete scalar field representation, even one given as a ""point cloud"".","Data mining,
Computer graphics,
Data visualization,
Principal component analysis,
Scattering,
Computational geometry,
Solid modeling,
Binary trees,
Image processing,
Computer science"
Multicast delivery using explicit multicast over IPv6 networks,"This article presents an alternative scheme, called Xcast6+, which is an extension of explicit multicast (Xcast) for an efficient delivery of multicast datagrams over IPv6 networks. The mechanism incorporates MLDv2 and a new control plane into existing Xcast6 (Xcast for IPv6) and not only does it provide the transparency of traditional multicast schemes to sources and recipients, but it also enhances the routing efficiency in networks. Since intermediate routers do not have to maintain any multicast states, it results in a more efficient and scalable mechanism to deliver traditional multicast datagrams. Furthermore, the seamless integration of Xcast6+ in Mobile IPv6 can support multicast efficiently for mobile nodes over IPv6 networks by avoiding tunnel avalanches and tunnel convergence. Our simulation results show distinct performance improvements of our approach. This approach can reduce network resources in many ""medium size groups"" multicast, particularly as the number of recipients in a subnet increases (i.e., ""subnet-dense groups"").",
Visualizing evolving networks: minimum spanning trees versus pathfinder networks,"Network evolution is an ubiquitous phenomenon in a wide variety of complex systems. There is an increasing interest in statistically modeling the evolution of complex networks such as small-world networks and scale-free networks. In this article, we address a practical issue concerning the visualizations of co-citation networks of scientific publications derived by two widely known link reduction algorithms, namely minimum spanning trees (MSTs) and pathfinder networks (PFNETs). Our primary goal is to identify the strengths and weaknesses of the two methods in fulfilling the need for visualizing evolving networks. Two criteria are derived for assessing visualizations of evolving networks in terms of topological properties and dynamical properties. We examine the animated visualization models of the evolution of botulinum toxin research in terms of its co-citation structure across a 58-year span (1945-2002). The results suggest that although high-degree nodes dominate the structure of MST models, such structures can be inadequate in depicting the essence of how the network evolves because MST removes potentially significant links from high-order shortest paths. In contrast, PFNET models clearly demonstrate their superiority in maintaining the cohesiveness of some of the most pivotal paths, which in turn make the growth animation more predictable and interpretable. We suggest that the design of visualization and modeling tools for network evolution should take the cohesiveness of critical paths into account.","Data visualization,
Complex networks,
Chaos,
Animation,
Computer graphics,
Citation analysis,
Educational institutions,
Information science,
Computer networks,
Pervasive computing"
Beyond proof-of-compliance: safety and availability analysis in trust management,"Trust management is a form of distributed access control using distributed policy. statements. Since one party may delegate partial control to another party, it is natural to ask what permissions may be granted as the result of policy changes by other parties. We study security properties such as safety, and availability for a family of trust management languages, devising algorithms for deciding the possible consequences of certain changes in policy. While trust management is more powerful in certain ways than mechanisms in the access matrix model, and the security properties considered are more than simple safety, we find that in contrast to the classical HRU undecidability of safety properties, our primary security properties are decidable. In particular, most properties we studied are decidable in polynomial time. Containment, the most complicated security property we studied, is decidable in polynomial time for the simplest TM language in the family. The problem becomes co-NP-hard when intersection or linked roles are added to the language.","Availability,
Security,
Access control,
Safety,
Mechanical factors,
Computer network management,
Computer science,
Polynomials,
Intelligent networks,
Laboratories"
Who watches the security educators?,"Security knowledge in all fields has historically been a double-edged sword. The information that makes it possible to protect a system, an activity, or a person, is also the information that can be used to harm that system, chat activity, that person. How knowledge is used, and the opinions of who ever is judging that use, makes the difference. The debate regarding appropriate teaching philosophies for security educators is a longstanding one, with modern battle lines drawn primarily around two philosophies /sub e/fense assurance and attack understanding. Most educators fall somewhere in between these perspectives.","Watches,
Computer security,
Computer science education,
Information security,
Privacy,
Remuneration,
Investments,
Testing,
Laboratories,
Decision making"
Optimal and efficient speculation-based partial redundancy elimination,"Existing profile-guided partial redundancy elimination (PRE) methods use speculation to enable the removal of partial redundancies along more frequently executed paths at the expense of introducing additional expression evaluations along less frequently executed paths. While being capable of minimizing the number of expression evaluations in some cases, they are, in general, not computationally optimal in achieving this objective. In addition, the experimental results for their effectiveness are mostly missing. This work addresses the following three problems: (1) Is the computational optimality of speculative PRE solvable in polynomial time? (2) Is edge profiling - less costly than path profiling - sufficient to guarantee the computational optimality? (3) Is the optimal algorithm (if one exists) lightweight enough to be used efficiently in a dynamic compiler? In this paper, we provide positive answers to the first two problems and promising results to the third. We present an algorithm that analyzes edge insertion points based on an edge profile. Our algorithm guarantees optimally that the total number of computations for an expression in the transformed code is always minimized with respect to the edge profile given. This implies that edge profiling, which is less costly than path profiling, is sufficient to guarantee this optimality. The key in the development of our algorithm lies in the removal of some non-essential edges (and consequently, all resulting non-essential nodes) from a flow graph so that the problem of finding an optimal code motion is reduced to one of finding a minimal cut in the reduced (flow) graph thus obtained. We have implemented our algorithm in Intel's Open Runtime Platform (ORP). Our preliminary results over a number of Java benchmarks show that our algorithm is lightweight and can be potentially a practical component in a dynamic compiler. As a result, our algorithm can also be profitably employed in a profile-guided static compiler in which compilation cost can often be sacrificed for code efficiency.","Flow graphs,
Dynamic compiler,
Safety,
Computer science,
Australia,
Polynomials,
Algorithm design and analysis,
Runtime,
Java,
Costs"
Experimental evaluation of a new speaker identification framework using PCA,"In a speaker identification system, training speaker models (e.g. Gaussian mixture model, GMM) is computationally expensive, especially when the dimension of feature vectors is large. Principal component analysis (PCA) method is an optimal linear dimension reduction technique in the mean-square sense, which can reduce the computational overhead of the subsequent processing stages. In this paper, a new speaker identification framework is proposed, with PCA embedded in after feature extraction step. Experiments are conducted to investigate PCA de-correlation and dimension reduction properties. The robust ability of PCA transform is also examined. Some promising results are found.","Principal component analysis,
Vectors,
Feature extraction,
Mel frequency cepstral coefficient,
Speech analysis,
Linear predictive coding,
Karhunen-Loeve transforms,
Educational institutions,
Computer science,
Computational modeling"
Computing layered surface representations: an algorithm for detecting and separating transparent overlays,"The biological visual system possesses the ability to compute layered surface representations, in which one surface is represented as being viewed through another. This ability is remarkable because, in scenes involving transparency, the link between surface topology and image topology is greatly complicated by the collapse of the photometric contributions of two distinct surfaces onto image intensity. Previous analysis of transparency has focused largely on the role of different kinds of junctions. Although junctions are important, they are not sufficient to predict layered surface structure. We present an algorithm that propagates local junction information by searching for chains of polarity-preserving junctions with consistent 'sidedness', and then propagates the transparency labeling into interior regions. The algorithm outputs a layered representation specifying: (i) the distinct surfaces, (ii) their depth ordering, and (iii) their surface attributes. We demonstrate the results of the algorithm on a number of images - both synthetic and real. We end by considering implications for related domains, such as shading.","Topology,
Biology computing,
Equations,
Visual system,
Photometry,
Computer vision,
Psychology,
Cognitive science,
Layout,
Surface structures"
3LS - a peer-to-peer network simulator,"Peer-to-peer (p2p) networks are the latest addition to the already large distributed systems family. With a strong emphasis on self-organization, decentralization and autonomy of the participating nodes, p2p-networks tend to be more scalable, robust and adaptive than other forms of distributed systems. The much-publicized success of p2p-networks for file-sharing and cycle-sharing have resulted in an increased awareness and interest into the p2p protocols and applications. However, p2p-networks are difficult to study due to their size and the complex interdependencies between users, application, protocol and network. We present a 3-level simulator designed to study complex p2p networks.","Peer to peer computing,
Protocols,
Computational modeling,
Application software,
Analytical models,
Computer science,
Robustness,
Large-scale systems,
Bandwidth,
Delay"
Extraction of periodic multivariate signals: mapping of voltage-dependent dye fluorescence in the mouse heart,"In many experimental circumstances, heart dynamics are, to a good approximation, periodic. For this reason, it makes sense to use high-resolution methods in the frequency domain to visualize the spectrum of imaging data of the heart and to estimate the deterministic signal content and extract the periodic signal from background noise in experimental data. In this paper, we describe the first application of a new method that we call cardiac rhythm analysis which uses a combination of principal component analysis and multitaper harmonic analysis to extract periodic, deterministic signals from high-resolution imaging data of cardiac electrical activity. We show that this method significantly increases the signal-to-noise ratio of our recordings, allowing for better visualization of signal dynamics and more accurate quantification of the properties of electrical conduction. We visualize the spectra of three cardiac data sets of mouse hearts exhibiting sinus rhythm, paced rhythm and monomorphic tachycardia. Then, for pedagogical purposes, we investigate the tachycardia more closely, demonstrating the presence of two distinct periodicities in the re-entrant tachycardia. Analysis of the tachycardia shows that cardiac rhythm analysis not only allows for better visualization of electrical activity, but also provides new opportunities to study multiple periodicities in signal dynamics.",
Extended simulated annealing for augmented TSP and multi-salesmen TSP,"An extended simulated annealing (ESA), based on grand canonical ensemble (GCE), is proposed. An ESA is used to solve the augmented traveling salesman problems (ATSP) and the multiple traveling salesmen problems. Experimental results show that ESA has salient features such as simplicity and ability to find high-quality solutions as simulated annealing has.",
A new non-recursive algorithm for binary search tree traversal,"Binary tree traversal refers to the process of visiting each node in a specified order. There are two ways to visit a tree: recursively and non-recursively. Most references introduce tree traversal using recursion only. Our literature survey indicated that most references only show the implementations of the recursive algorithms, and only few references address the issue of nonrecursive algorithms. In this paper, we investigate and compare recursive and non-recursive algorithms for in-order, preorder, and post-order traversals. The in-order traversal of a binary search tree is important in searching algorithms, operating systems, and compiler design. In this paper we propose a new non-recursive algorithm for in-order binary search trees that is both efficient and easy to understand. The implementation of this new algorithm was done in Java and the complete algorithm was tested. The new algorithm was found to be faster than other nonrecursive algorithms.",
Sunset scene classification using simulated image recomposition,"Knowledge of the semantic classification of an image can be used to improve the accuracy of queries in content-based image organization and retrieval and to provide customized image enhancement. We developed an exemplar-based system for classifying sunset scenes. However, the performance of such a system depends largely on the size and quality of the set of training exemplars, which can be limited in practice. In addition, variations in scene content, as well as distracting regions, may exist in many testing images to prohibit good matches with the exemplars. We propose using simulated spatial and temporal image recomposition to address such issues. The recomposition schemes boost the recall of sunset images from a reasonably large data set by 10%, while holding the false positive rate constant.","Layout,
Testing,
Sun,
Image retrieval,
Content based retrieval,
Image enhancement,
Computational modeling,
Computer science,
Pattern recognition,
Boosting"
Catching accurate profiles in hardware,"Run-time optimization is one of the most important ways of getting performance out of modern processors. Techniques such as prefetching, trace caching, memory disambiguation etc., are all based upon the principle of observation followed by adaptation, and all make use of some sort of profile information gathered at run-time. Programs are very complex, and the real trick in generating useful run-time profiles is sifting through all the unimportant and infrequently occurring events to find those that are important enough to warrant optimization. In this paper, we present the multi-hash architecture to catch important events even in the presence of extensive noise. Multi-hash uses a small amount of area, between 7 to 16 Kilo-bytes, to accurately capture these important events in hardware, without requiring any software support. This is achieved using multiple hash tables for the filtering, and interval-based profiling to help identify how important an event is in relationship to all the other events. We evaluate our design for value and edge profiling, and show that over a set of benchmarks, we get an average error less than 1%.","Hardware,
Runtime,
Prefetching,
Computer architecture,
System software,
Computer science,
Filtering,
Computer errors,
Software systems,
Software tools"
Social interactions in multi-agent systems: a formal approach,"The paper presents a formal analysis of social interactions within multi-agent systems. The fundamental building blocks of such systems are social agents which can be individuals or aggregations of agents whose structure can be formally characterised in terms of roles and relationships between them. Agents are free to join social agents while in pursuit of their own objectives, but at the same time they have to balance their preferences and their commitments. Stability and regulation of behaviour within a multi-agent system and within social agents is accounted for by means of commitments, obligations and rights.","Multiagent systems,
Teamwork,
Problem-solving,
Stability,
Computer science,
Ontologies,
Design methodology,
Intelligent agent"
Expressing Bayesian fusion as a product of distributions: applications in robotics,"More and more fields of applied computer science involve fusion of multiple data sources, such as sensor readings or model decision. However, incompleteness of the model prevents the programmer from having an absolute precision over their variables. Therefore Bayesian framework can be adequate fro such a process as it allows handling of uncertainty. We will be interested in the ability to express any fusion process as a product, for it can lead to reduction of complexity in time and space. We study in this paper various fusion schemes and propose to add consistency variable to justify the use of a product to compute distribution over the fused variable. We will then show application of this new fusion process to localization of a mobile robot and obstacle avoidance.","Bayesian methods,
Robot sensing systems,
Application software,
Programming profession,
Sensor fusion,
Mobile robots,
Computer science,
Uncertainty,
Distributed computing,
Mobile computing"
Resource- and quality-aware application-level service multicast,"Current multimedia application deployment tends to rely on composable service systems, where a complex multimedia service can be composed dynamically from multiple simpler ones distributed widely in the Internet. Related to such a scenario is the problem of finding efficient service paths that meet end-to-end requirements. Work has been done in discovering unicast service paths. However, considering that resources are limited, for distributed multimedia applications that may have a single sender but multiple heterogeneous end-users, it is demanding to build service trees to minimize resource usages by means of sharing. We present a resource- and quality-aware application-level multicast Service Path Finding protocol (mc-SPF/sup Q/) that constructs, for each application, a more economical service tree instead of independent service paths. Bandwidth and proxy machine resource savings can be achieved by applying the application-layer multicast concept to deliver data through the service tree. mc-SPF/sup Q/ Is resource- and quality-aware, in that service paths are discovered with resource availability and client's quality demand in mind. Our simulation results show that: (1) compared to unicast service path finding solutions, mc-SPF/sup Q/ is superior; (2) the proxy load balancing feature makes the protocol achieve better path finding success rates in the face of resource scarcity; (3) reserving different portions of resources for service requests of different qualities affects the service path finding success rate and the average quality of service.","Web and internet services,
Unicast,
Bandwidth,
Streaming media,
Multimedia systems,
Multicast protocols,
Scalability,
Hardware,
Computer science,
Application software"
Multiparty micropayments for ad hoc networks,"The majority of ad hoc networks and their associated applications have been designed with closed user groups in mind. In such scenarios all the nodes in the network usually belong to a single authority and are configured to cooperate in the relaying of packets within the network. In recent years however, ad hoc networks have also found their way into everyday networking environments, where mobile devices may be under the administrative control of individual users. These users may not necessarily be motivated to provide services for free to others in the network. A typical situation could be where a node may wish to relay packets through a number of intermediate nodes in the ad hoc network, to access services in the fixed network. In this paper we present a lightweight payment scheme based on hash chains, which allows a node to pay others who relay packets on its behalf in real-time. Also due to the dynamic nature of an ad hoc network, the topology of the network can change unpredictably. The design of the payment scheme is flexible enough to be able to cope with such route changes, without the need to contact a trusted third party such as a bank or broker to pay the nodes in the new path.","Ad hoc networks,
Relays,
Computer science,
Application software,
Network topology,
Spread spectrum communication,
Emergency services,
Batteries,
Cellular networks,
IP networks"
Towards a unified framework for rapid 3D computed tomography on commodity GPUs,"The task of reconstructing an object from its projections via tomographic methods is a time-consuming process due to the vast complexity of the data. For this reason, manufacturers of equipment for computed tomography (CT), both medical and industrial, rely mostly on special ASICs to obtain the fast reconstruction times required in clinical, industrial, and security settings. Although modern CPUs have gained enough power in recent years to be competitive for 2D reconstruction, this is not the case for 3D reconstructions, especially not when iterative algorithms must be applied. Incidentally, this has prevented some very effective algorithms to be used in clinical practice, and the need for proprietary reconstruction hardware has also hampered new equipment manufacturers in their effort on entering the market. However, the recent evolution of GPUs has changed the picture in a very dramatic way. We will show how floating point GPUs can be exploited to perform both analytical and iterative reconstruction from X-ray and functional imaging data at clinical rates and good quality. For this purpose, we derive a decomposition of three popular 3D reconstruction algorithms into a common set of base modules. All of these base modules can be executed on the GPU and their output linked internally. The data never leaves the GPU, which eliminates the previous GPU-CPU bottlenecks. Visualization of the reconstructed object is also easily done since the object already resides in the graphics hardware, and one can simply run a visualization module at any time to view the reconstruction results. Our implementation allows speedups at a factor of 20, compared to software implementations, at comparable image quality.","Computed tomography,
Image reconstruction,
Computer industry,
Manufacturing industries,
Iterative algorithms,
Hardware,
Data visualization,
Biomedical imaging,
Data security,
Image analysis"
Mobility management based on the integration of mobile IP and session initiation protocol in next generation mobile data networks,"For a mobile node that uses both mobile IP and session initiation protocol (SIP), there is severe redundant registration overhead because the mobile node has to make location registration separately to the home agent for mobile IP and to the home registrar for SIP. Therefore, we propose two new IP mobility management methods that integrate both mobile IP and SIP. Performance comparisons are made among previous method, which makes separate registration for the mobile IP and SIP without integration, and our two integrated methods. Numerical results show that proposed methods efficiently reduce the amount of signaling messages related to the location registration and handoff.","Mobile radio mobility management,
Protocols,
Intelligent networks,
Next generation networking,
Mobile computing,
TCPIP,
Routing,
Computer science,
Internet,
Research and development"
HPCM: a pre-compiler aided middleware for the mobility of legacy code,"Mobility is a fundamental functionality of the next generation Internet computing. How to support mobility for legacy codes, however, is still an issue of research. The key to solve this outstanding issue is the support of heterogeneous process migration. During the last few years, we have successfully developed mechanisms to support heterogeneous process migration of legacy codes written in C, C++, and Fortran. We present in this paper the design of the high performance computing mobility (HPCM) middleware, the development and implementation of its key components, pre-compiler and its static libraries. Due to the similarity between process migration and checkpointing, the pre-compiler not only makes automatic process migration of legacy codes feasible, but also supports dynamic heterogeneous checkpointing. We perform a set of tests and compare experimental results with Porch, a well-known portable heterogeneous checkpointing system. The experimental results show that our methods are feasible, efficient and very promising.",
Circular motion geometry by minimal 2 points in 4 images,"We describe a new and simple method of recovering the geometry of uncalibrated circular motion or single axis motion using a minimal data set of 2 points in 4 images. This problem has been solved using nonminimal data either by computing the fundamental matrix and trifocal tensor in 3 images, or by fitting conics to tracked points in 5 images. Our new method first computes a planar homography from a minimum of 2 points in 4 images. It is shown that two eigenvectors of this homography are the images of the circular points. Then, other fixed image entities and rotation angles can be straightforwardly computed. The crux of the method lies in relating this planar homography from two different points to a homology naturally induced by corresponding points on different conic loci from a circular motion. The experiments on real image sequences demonstrate the simplicity, accuracy and robustness of the new method.",
Student models in computer-based education,We describe a student model using possibilities in computer-based education. The model's classification is given. The results of a comparable analysis of presently available student models are shown. Analysis was made after parameters that are considered to be essential for teaching process effectiveness. Offers of developing student model are given.,
Closing multiple loops while mapping features in cyclic environments,"In this paper we propose an offline feature mapping algorithm capable of identifying and correctly closing multiple loops in cyclic environments. The proposed algorithm iteratively alternates between a Kalman smoother based localization step and a map features recalculation step. The identification of loops is done during the localization step by a hybrid localization algorithm that generates and tracks hypotheses generated each time the robot visits an already mapped area. The main contribution of this paper lies on the ability of the proposed algorithm to exploit information contained within the hypotheses histories in order to calculate correct maps, regardless of the complexity of the environment and the number of loops in the robot's path.",
Architectural synthesis Integrated with global placement for multi-cycle communication,"Multiple clock cycles are needed to cross the global interconnects for multi-gigahertz designs in nanometer technologies. For synchronous design, this requires the consideration of multi-cycle on-chip communication at the high level. In this paper, we present a new architectural synthesis system integrated with global placement, named MCAS (Multi-Cycle Architectural Synthesis), on top of the recently-proposed Regular Distributed Register (RDR) micro-architecture. The RDR architecture provides a regular synthesis platform for supporting multi-cycle communication. Novel architectural synthesis algorithms that integrate high-level synthesis with global placement have been developed in MCAS, including scheduling-driven placement and distributed controller generation, etc. Experimental results show that our methodology can achieve a clock period improvement of 31% and a total latency improvement of 24% on average compared to the conventional architectural synthesis flow.","Delay,
Clocks,
Control system synthesis,
Design methodology,
Logic circuits,
Synchronization,
Integrated circuit interconnections,
Permission,
Computer science,
Scheduling algorithm"
M-kernel merging: towards density estimation over data streams,"Density estimation is a costly operation for computing distribution information of data sets underlying many important data mining applications, such as clustering and biased sampling. However, traditional density estimation methods are inapplicable for streaming data, which are continuously arriving large volume of data, because of their request for linear storage and square size calculation. The shortcoming limits the application of many existing effective algorithms on data streams, for which the mining problem is an emergency for applications and a challenge for research. In this paper, the problem of computing density functions over data streams is examined. A novel method attacking this shortcoming of existing methods is developed to enable density estimation for large volume of data in linear time, fixed size memory, and without lose of accuracy. The method is based on M-Kernel merging, so that limited kernel functions to be maintained are determined intelligently, The application of the new method on different streaming data models is discussed, and the result of intensive experiments is presented. The analytical and empirical result show that this new density estimation algorithm for data streams can calculate density functions on demand at any time with high accuracy for different streaming data models.","Data mining,
Density functional theory,
Data models,
Algorithm design and analysis,
Computer science,
Data engineering,
Laboratories,
Information processing,
Distributed computing,
Application software"
Further improve circuit partitioning using GBAW logic perturbation techniques,"Efficient circuit partitioning is becoming more and more important as the size of modern circuits keeps increasing. Conventionally, circuit partitioning is solved without altering the circuit by modeling the circuit as a hypergraph for the ease of applying graph algorithms. However, there is room for further improvement on even optimal hypergraph partitioning results, if logic information can be applied for circuit perturbation. Such logic transformation based partitioning techniques are relatively less addressed. In this paper, we present a powerful multiway partitioning technique which applies efficient logic rewiring techniques for further improvement over already superior hypergraph partitioning results. The approach can integrate with any graph partitioner. We perform experiments on two-, three-, and four-way partitionings for MCNC benchmark circuits whose physical and logical information are both available. Our experimental results show that this partitioning approach is very powerful. For example, it can achieve a further 12.3% reduction in cut size upon already excellent pure graph partitioner (hMetis) results on two-way partitioning with an area penalty of only 0.34%. The outperforming results demonstrate the usefulness of this new partitioning technique.",
Using context for supporting users efficiently,"A good software is a software that is invisible for the user. This is possible by making context explicit in the software. The increasingly interest for the notion of context appears through the number of approaches based on it. Initially, context was mainly considered in engineering and cognitive science. Now even in engineering, one sees two sub-approaches considering context at the level of the knowledge or the data, namely context-based and context-aware systems. For context-based systems, most of the non formal approaches focus, in one way or another, on context in relationships with the user, aiming at a good computer system that is invisible for the user. Context-aware systems are concerned indirectly with users through a modeling of their dynamic environment. In this paper, we present, first, different viewpoints on context related to human-machine interaction. We then consider why we need to focus on users through two aspects, on the one hand, on planning and plan execution, and, on the other hand, procedures and practice. The lesson learned is that an improvement of user's support needs a consideration of context.",
Programming on the Univac 1: a woman's account,"Although women's role in the computer field has been fairly well documented, their role in the programming arena has not. The author's account of her programming experience on the Universal Automatic Computer (Univac 1), for the Eckert-Mauchly Computer Corporation reveals much about the early workplace conditions - namely, a workplace surprisingly free of restrictions in an era when most married women with children did not work outside of the home.","Humans,
Home computing,
Employment,
Contracts,
NIST,
Mathematics,
Physics computing,
Programming profession,
Computer science,
Production facilities"
On the e-negotiation of unmatched logrolling views,"An e-negotiation process comprises computer-facilitated tasks, each of which aims at resolving an issue or a collection of co-related issues under negotiation. It involves an iterative decision process in which two or more parties make individual decisions and interact with each other, aiming to arrive at a contract. In each iterative decision process, a party makes offers or counter offers that may reflect tradeoffs of some issues. Tradeoff evaluation often requires logrolling, which refers to the exchange of loss in some issues for gain in others. The set of issues involved in a logrolling process is referred to as logrolling set. A typical negotiation involves multiple logrolling processes. The logrolling sets exercised by a negotiation party in these processes constitute one logrolling view. Problems arise when parties have unmatched logrolling views. Alternative offers or counter offers that refer to one logrolling view can be difficult to interpret under a different logrolling view. In this paper, motivated by an e-negotiation example of a lease contract template, we formulate a meta-model of unmatched logrolling views, examine the involved e-negotiation process, and propose a mechanism for building an e-negotiation support system (eNSS) to facilitate the process, without the need of formulating utility functions. As a result, e-negotiation across parties with unmatched logrolling views can be streamlined.","Contracts,
Business,
Counting circuits,
Proposals,
Computer science,
Technology management,
Engineering management,
Uncertainty,
US Government,
System recovery"
A multi-agent based simulated stock market - testing on different types of stocks,"Previously, we have developed a multiagent based simulated stock market where artificial stock traders coevolve by means of individual and social learning and learn to trade stock profitably. We tested our model on a single stock (British Petroleum) from the LSE (London Stock Exchange) where our artificial agents demonstrated dynamic learning behaviours and strong learning abilities. We extend our previous work by testing the model on different types of stocks from different sections of the stock market. The results from the experiments show that the artificial traders demonstrate stable and satisfactory learning abilities during the simulation regardless of the different types of stocks. The results lays the foundation for our future work - developing an efficient portfolio manager from a multiagent based simulated stock market.","Stock markets,
Testing,
Portfolios,
Investments,
Computational modeling,
Computer science,
Petroleum,
Artificial neural networks,
Performance evaluation,
History"
Study of mobile payments system,"Mobile payment is the killer application in mobile commerce. We classify the payment methods according to several standards, analyze and point out the merits and drawbacks of each method. To enable future applications and technologies to handle mobile payment, we provide a general layered framework and a new process for mobile payment. The framework is composed of load-bearing layer, network interface and core application platform layer, business layer, and decision-making layer. And it can be extended and improved by the developers. Then a pre-pay and account-based payment process is described. Our method has the advantages of low cost and technical requirement, high scalability and security.",
Automated TTCN-3 test case generation by means of UML sequence diagrams and Markov chains,"The objective of this paper is to automatically generate a MCUM (Markov chain usage model) starting from an OMG UML-SD (sequence diagram) in order to derive TTCN-3 (testing and test control notation version 3) compatible test case definitions. Our approach is a combination of statistical usage testing and specification-based testing. Within this paper, special attention is given to international standardized FDT notations, specifically UML-SD and MSC. We have also defined an XML-based representation format called MCML (Markov chain markup language) to build a common interface between various parts of the MaTeLo tool set. In the case of UML-SD, we use XMI descriptions in order to generate the desired MCML format.","Software testing,
Markov processes,
Specification languages"
Mykil: a highly scalable key distribution protocol for large group multicast,"This paper describes the design, implementation, and evaluation of Mykil, which is a new key distribution protocol for secure group multicast. Mykil has been designed to be scalable to large group sizes. It is based on a combination of group-based hierarchy and key-based hierarchy systems for group key management. Important advantages of Mykil include a fast rekeying operation for large group sizes, continuous availability of the key management service in a disconnected network environment, an ability to map the group structure to the underlying network infrastructure, robustness, support for user mobility, and support for smaller hand-held devices.","Multicast protocols,
Cryptography,
Cryptographic protocols,
Scalability,
Robustness,
Costs,
Organizing,
Computer science,
Availability,
Environmental management"
An Audio Watermarking Technique That Is Robust Against Random Cropping,,
"Performance evaluation of the post-registration method, a low latency handoff in MIPv4","In this paper, we evaluate a low latency handoff protocol for MIPv4, the post-registration handoff method. This mechanism proposed by the IETF tries to improve the performance of hierarchical mobile IP by decreasing the handoff latency. We give a detailed description of the protocol behavior by means of an ns simulation and propose a simple queuing model to study the influence of various parameters on the protocol performance.","Delay,
Analytical models,
Access protocols,
Computer architecture,
Mathematics,
Computer science,
IP networks,
Degradation,
Mobile radio mobility management,
Routing protocols"
Reasoning about hierarchical storage,"In this paper, we develop a new substructural logic that can encode invariants necessary for reasoning about hierarchical storage. We show how the logic can be used to describe the layout of bits in a memory word, the layout of memory words in a region, the layout of regions in an address space, or even the layout of address spaces in a multiprocessing environment. We provide a semantics for our formulas and then apply the semantics and logic to the task of developing a type system for Mini-KAM, a simplified version of the abstract machine used in the ML Kit with regions.","Data structures,
Logic programming,
Computer languages,
Assembly,
Virtual machining,
Memory management,
Bismuth,
Computer science"
Multi-resolution image inpainting,"Digital inpainting is an image interpolation mechanism, which can automatically restore damaged or partially removed image. Most inpainting mechanisms use a singular resolution approach on the extrapolation or interpolation of pixels. We propose a multi-resolution algorithm, which can take into consideration the different levels of details. The algorithm was tested on 1000 still images, with an evaluation showing the effectiveness of our approach. The demonstration of our work is available at: http://www.mine.tku.edu.tw/demos/inpaint.","Pixel,
Image restoration,
Extrapolation,
Testing,
Interpolation,
Image resolution,
Watermarking,
Reactive power,
Computer science,
Electronic mail"
Performance optimizations for group key management schemes,"Recently, many group key management approaches based on the use of logical key trees have been proposed to address the issue of scalable group rekeying that is needed to support secure communications for large and dynamic groups. In this paper, we present two optimizations for logical key tree organizations that utilize information about the characteristics of group members to further reduce the overhead of group rekeying. First, we propose a partitioned key tree organization that exploits the temporal patterns of group member joins and departures to reduce the overhead of rekeying. Using an analytic model, we show that our optimization can achieve up to 31.4% reduction in key server bandwidth overhead over the unoptimized scheme. Second, we propose an approach under which the key tree is organized based on the loss probabilities of group members. Our analysis shows this optimization can reduce the rekeying overhead by up to 12.1%.","Cryptography,
Multicast algorithms,
Multicast protocols,
Management information systems,
Bandwidth,
Teleconferencing,
IP networks,
Web and internet services,
Data security,
Computer science"
An investigation of phylogenetic likelihood methods,"We analyze the performance of likelihood-based approaches used to reconstruct phylogenetic trees. Unlike other techniques such as Neighbor-joining (NJ) and Maximum Parsimony (MP), relatively little is known regarding the behavior of algorithms founded on the principle of likelihood. We study the accuracy, speed, and likelihood scores of four representative likelihood-based methods (fastDNAml, Mr Bayes, PAUP*-ML, and TREE-PUZZLE) that use either Maximum Likelihood (ML) or Bayesian inference to find the optimal tree. NJ is also studied to provide a baseline comparison. Our simulation study is based on random birth-death trees, which are deviated from ultrametricity, and uses the Kimura 2-parameter +Gamma model of sequence evolution. We find that Mr Bayes (a Bayesian inference approach) consistently outperforms the other methods in terms of accuracy and running time.",
Minimum risk distance measure for object recognition,"The optimal distance measure for a given discrimination task under the nearest neighbor framework has been shown to be the likelihood that a pair of measurements have different class labels [S. Mahamud et al., (2002)]. For implementation and efficiency considerations, the optimal distance measure was approximated by combining more elementary distance measures defined on simple feature spaces. We address two important issues that arise in practice for such an approach: (a) What form should the elementary distance measure in each feature space take? We motivate the need to use the optimal distance measure in simple feature spaces as the elementary distance measures; such distance measures have the desirable property that they are invariant to distance-respecting transformations, (b) How do we combine the elementary distance measures ? We present the precise statistical assumptions under which a linear logistic model holds exactly. We benchmark our model with three other methods on a challenging face discrimination task and show that our approach is competitive with the state of the art.","Object recognition,
Nearest neighbor searches,
Extraterrestrial measurements,
Tin,
Principal component analysis,
Computer science,
Logistics,
Face detection,
Neural networks,
Noise measurement"
A property of the intrinsic mutual information,,"Mutual information,
Random variables,
Quantum computing,
Computer science,
Open wireless architecture,
Particle measurements,
Upper bound,
Distributed computing"
The e-logistics of securing distributed medical data,"HIPAA mandates that all healthcare data accessed or transmitted over open communications systems such as the Internet be encrypted. While encrypting a digital x-ray before storage and decrypting it before viewing will not affect a hospital's workflow, the workflow consequences of encryption on the large medical images (a single 500-slice MRI produces a 68 MB file) typically in use by a radiology department were feared but unknown. We conducted performance studies of four candidate encryption algorithms operating in a .NET environment, and then used those results in a workflow model of our hospital's radiology department to predict the impact of adding encryption to the workflow scenario.","Cryptography,
Biomedical imaging,
Data security,
Information security,
Privacy,
Medical diagnostic imaging,
Medical services,
Radiology,
Authentication,
Computer science"
Benefits of information visualization systems for administrative data analysts,"We report results from a study on the adoption of an information visualization system by administrative data analysts. Despite the fact that the system was neither fully integrated with their current software tools nor with their existing data analysis practices, analysts identified a number of key benefits that visualization systems provide to their work. These benefits for the most part occurred when analysts went beyond their habitual and well-mastered data analysis routines and engaged in creative discovery processes. We analyze the conditions under which these benefits arose, to inform the design of visualization systems that can better assist the work of administrative data analysts.","Data visualization,
Data analysis,
Information analysis,
Computer science,
Software tools,
Laboratories,
Software measurement,
Software maintenance,
Employment,
Information technology"
Statistical characterization of clutter scenes based on a Markov random field model,"The problem of clutter region identification based on Markov random field (MRF) models is addressed. Observations inside each clutter region are assumed homogenous, i.e., observations follow a single probability distribution. Our goal is to partition clutter scenes into homogenous regions and to determine this underlying probability distribution. Metropolis-Hasting and reversible jump Markov chain (RJMC) algorithms are used to search for solutions based on the maximum a posteriori (MAP) criterion. Several examples illustrate the performance of our algorithm.","Layout,
Markov random fields,
Partitioning algorithms,
Image segmentation,
Probability distribution,
Probability density function,
Shape,
Radar clutter,
Computer science,
Pixel"
User-specified adaptive scheduling in a streaming media network,"In disaster and combat situations, mobile cameras and other sensors transmit real-time data, used by many operators or analysis tools. Unfortunately, in the face of limited, unreliable resources, and varying demands, not all users may be able to get the fidelity they require. This paper describes MediaNet, a distributed stream processing system designed with the above scenarios in mind. Unlike past approaches, MediaNet's users can intuitively specify how the system should adapt based on their individual needs. MediaNet uses both local and online global resource scheduling to improve user performance and network utilization, and adapts without requiring underlying support for resource reservations. Performance experiments show that our scheduling algorithm is reasonably fast, and that user performance and network utilization are both significantly improved.","Adaptive scheduling,
Intelligent networks,
Streaming media,
Quality of service,
Cameras,
Computer science,
Computer networks,
Educational institutions,
Mobile computing,
Process design"
Automatic animation skeleton using repulsive force field,"A method is proposed in this paper to automatically generate the animation skeleton of a model such that the model can be manipulated according to the skeleton. With our method, users can construct the skeleton in a short time, and bring a static model both dynamic and alive. The primary steps of our method are finding skeleton joints, connecting the joints to form an animation skeleton, and binding skin vertices to the skeleton. Initially, a repulsive force field is constructed inside a given model, and a set of points with local minimal force magnitude are found based on the force field. Then, a modified thinning algorithm is applied to generate an initial skeleton, which is further refined to become the final result. When the skeleton construction completes, skin vertices are anchored to the skeleton joints according to the distances between the vertices and joints. In order to build the repulsive force field, hundreds of rays are shot radially from positions inside the model, and it leads to that the force field computation takes most of the execution time. Therefore, an octree structure is used to accelerate this process. Currently, the skeleton generated from a typical 3D model with 1000 to 10000 polygons takes less than 2 minutes on a Intel Pentium 4 2.4 GHz PC.","Animation,
Skeleton,
Joints,
Skin,
Computer graphics,
Packaging,
Multimedia communication,
Laboratories,
Computer science,
Joining processes"
"Understanding educator perceptions of ""quality"" in digital libraries","The purpose of the study was to identify educators' expectations and requirements for the design of educational digital collections for classroom use. A series of five focus groups was conducted with practicing teachers, preservice teachers, and science librarians, drawn from different educational contexts (i.e., K-5, 6-12, college). Participants' expect that the added value of educational digital collections is the provision of: (1) 'high quality' teaching and learning resources, and (2) additional contextual information beyond that in the resource. Key factors that influence educators' perceptions of quality were identified: scientific accuracy, bias, advertising, design and usability, and the potential for student distraction. The data showed that participants judged these criteria along a continuum of tolerance, combining consideration of several factors in their final judgements. Implications for collections accessioning policies, peer review, and digital library service design are discussed.","Software libraries,
Educational institutions,
Usability,
Computer science,
Advertising,
Mathematics,
Educational technology,
Geoscience,
Continuing education,
Prototypes"
Role activity diagrams as finite state processes,,"Algebra,
Computer science,
Concurrent computing,
Automation,
Software engineering,
Management information systems,
Formal verification,
Information technology,
Technology management,
Petri nets"
A simple model of real-time flow aggregation,"The IETF's integrated services (IntServ) architecture, together with reservation aggregation, provides a mechanism to support the quality-of-service demands of real-time flows in a scalable way, i.e., without requiring that each router be signaled with the arrival or departure of each new flow for which it forwards data. However, reserving resources in ""bulk"" implies that the reservation does not precisely match the true demand. Consequently, if the flows' demanded bandwidth varies rapidly and dramatically, aggregation can incur significant performance penalties of under-utilization and unnecessarily rejected flows. On the other hand, if demand varies moderately and at slower time scales, aggregation can provide an accurate and scalable approximation to IntServ. We develop a simple analytical model and perform extensive trace-driven simulations to explore the effectiveness of aggregation under a broad class of factors. Example findings include: 1) a simple single-time-scale model with random noise can capture the essential behavior of surprisingly complex scenarios; 2) with a two-order-of-magnitude separation between the dominant time scale of demand and the time scale of signaling and moderate levels of secondary noise, aggregation achieves a performance that closely approximates that of IntServ.","Aggregates,
Scalability,
Quality of service,
Computer science,
Intserv networks,
Bandwidth,
Analytical models,
Noise level,
Traffic control,
Diffserv networks"
Scaling secure group communication systems: beyond peer-to-peer,"This paper proposes several integrated security architecture designs for client-server group communication systems. In an integrated architecture, security services are implemented in servers, in contrast to a layered architecture where the same services are implemented in clients. We discuss the performance and accompanying trust issues of each proposed architecture and present experimental results that demonstrate the superior scalability of an integrated architecture.","Peer to peer computing,
Communication system security,
Scalability,
Information security,
Computer science,
Authentication,
Data security,
Computer architecture,
Collaborative software,
Access control"
Autonomous cooperation technologies for achieving real time property and fault tolerance in Service Oriented Community system,"The advancement of wireless communication and mobile telecommunication has made mobile commerce possible. In the retail business under the evolving market, the users would like to utilize the appropriate services based on their preference and situation continuously. The service providers need to grasp the current requirements of the majority of the users in the local service area in order to provide the effective service for the current local majority of the service area. However, these local but familiar services cannot be realized through global information services on the Internet. The Service Oriented Community is proposed in order to satisfy both users' and service providers' requirements. In the Community, the members cooperate with each other in order to get mutual benefits like social community. The users provide their requirements to service providers while utilizing the services, and service providers utilize the users marketing information to provide appropriate services. This marketing information should be collected in the flexible area based on services in real time. Hence flexibility, real-time property, and fault tolerance are required. Here, Time Distance Oriented Information Service System has been proposed to achieve flexibility in collecting the users' marketing information in the area based on services. In the community system, the key idea is autonomous cooperation among nodes to satisfy the requirements. Here, Autonomous Synchronization Technique and Autonomous Cooperation Technique are proposed to satisfy real time property and fault tolerance respectively. The effectiveness of these technologies is shown at the end.","Real time systems,
Fault tolerant systems,
Business,
Fault tolerance,
Web and internet services,
Computer science,
Mobile communication,
Tellurium,
Wireless communication,
Mobile handsets"
Architectures for packet classification caching,"Emerging network applications require packet classification at line speed on multiple header fields. Fast packet classification requires a careful attention to memory resources due to the size and speed limitations in SRAM and DRAM memory used to implement the function. In this paper, we investigate a range of memory architectures that can be used to implement a wide range of packet classification caches. In particular, we examine their performance under real network traces in order to identify features that have the greatest impact. Through experiments, we show that a cache's associativity, replacement policy, and hash function all contribute in varying magnitudes to the cache's overall performance. Specifically, we show that small levels of associativity can result in enormous performance gains, that replacement policies can give modest performance improvements for under-provisioned caches, and that faster, less complex hashes can improve overall cache performance.","Random access memory,
Delay,
Hardware,
Computer architecture,
Educational institutions,
Computer networks,
Computer science,
Application software,
Memory architecture,
Performance gain"
A compact planar folded-dipole antenna for wireless applications,"A new miniaturized printed antenna is presented as a dual structure to the miniature folded slot antenna. It is referred to as a miniaturized printed folded-dipole. This antenna requires neither a ground plane, nor any matching network. Considering its size, this resonant antenna has a fairly wide bandwidth. If a large enough ground plane is readily available, the miniaturized slot antenna exhibits superior efficiency compared to its printed dual.","Dipole antennas,
Slot antennas,
Application software,
Bandwidth,
Electric resistance,
Laboratories,
Computer science,
Mobile antennas,
Frequency,
Costs"
A programmable routing framework for autonomic sensor networks,"This paper proposes a programmable routing framework that promotes the adaptivity in routing services for sensor networks. This framework includes a universal routing service and an automatic deployment service. The universal routing service allows the introduction of different services through its tunable parameters and programmable components. The deployment service completes the configuration of the universal routing service throughout a sensor network in an automatic and energy-efficient way. With this deployment service, a self-configuring ability is realized for sensor routing services. With the changeable parameters and programmable components of the universal routing service, the self-optimizing as well as other autonomic abilities can be explored in an experimental sensor network conforming to the proposed framework.","Routing,
Chemical sensors,
Intelligent sensors,
Energy efficiency,
Large-scale systems,
Condition monitoring,
Helium,
Computer science,
Wireless communication,
Centralized control"
Bounded nondeterminism and alternation in parameterized complexity theory,"We give machine characterisations and logical descriptions of a number of parameterized complexity classes. The focus of our attention is the class W[P], which we characterise as the class of all parameterized problems decidable by a nondeterministic fixed-parameter tractable algorithm, whose use of nondeterminism is bounded in terms of the parameter. We give similar characterisations for AW[P], the ""alternating version of W[P]"", and various other parameterized complexity classes. We also give logical characterisations of the classes W[P] and AW[P] in terms of fragments of least fixed-point logic, thereby putting these two classes into a uniform framework that we have developed in earlier work. Furthermore, we investigate the relation between alternation and space in parameterized complexity theory. We prove that the compact Turing machine computation problem, shown to be hard for the class AW[SAT] in (K. A. Abrahamson et al., 1995) is complete for the class uniform-XNL.","Complexity theory,
Artificial intelligence,
Computer science,
Laboratories,
Logic,
Algorithm design and analysis,
Databases,
Computational biology,
Polynomials,
Biology computing"
Collaboration platforms for virtual student communities,"Since its inception, the Internet has served as a virtual meeting place for people sharing common interests. These electronically supported interest groups are called virtual communities. Networked computers are used to support direct communication and indirect information exchange, and to provide awareness and matchmaking services. The use of member profiles adds a new dimension to the support of interaction between community members. Information about members is needed to introduce them to each other and empower individualized services. The paper describes the research design of two empirical platform projects, one carried out in Germany, and the other in German-speaking Switzerland. Both projects are targeted at students and their respective needs for information, interaction and exchange.","Collaboration,
Knowledge management,
Sections,
Internet,
Context,
Computer science,
Computer networks,
Electronic commerce,
Economies of scale,
Communications technology"
Structuring lecture videos for distance learning applications,"We present an automatic and novel approach in structuring and indexing lecture videos for distance learning applications. By structuring video content, we can support both topic indexing and semantic querying of multimedia documents. our aim is to link the discussion topics extracted from the electronic slides with their associated video and audio segments. Two major techniques in our proposed approach include video text analysis and speech recognition. Initially, a video is partitioned into shots based on slide transitions. For each shot, the embedded video texts are detected, reconstructed and segmented as high-resolution foreground texts for commercial OCR recognition. The recognized texts can then be matched with their associated slides for video indexing. Meanwhile, both phrases (title) and keywords (content) are also extracted from the electronic slides to spot the speech signals. The spotted phrases and keywords are further utilized as queries to retrieve the most similar slide for speech indexing.","Videos,
Computer aided instruction,
Streaming media,
Indexing,
Gunshot detection systems,
Text recognition,
Joining processes,
Application software,
Computer science,
Text analysis"
Frequency plot and relevance plot to enhance visual data exploration,"We present two techniques aiming at exploring databases through multivariate visualizations. Both techniques intend to deal with the problem caused by the limited amount of elements that can be presented simultaneously in traditional visual exploration procedures. The first technique, the Frequency Plot, combines data frequency with interactive filtering to identify clusters and trends in subsets of the database. Thus, graphical elements (lines, pixels, icons, or graphical marks) are color differentiated proportionally to how frequent the value being represented is, while interactive filtering allows the selection of interesting partitions of the database. The second technique, the Relevance Plot, corresponds to assigning different levels of color distinguishably to visual elements according to their relevance to a user's specified data properties set, which can be chosen visually and dynamically.","Frequency,
Data visualization,
Visual databases,
Data analysis,
Layout,
Image databases,
Filtering,
Humans,
Computer science,
Information retrieval"
An extended ASLD trading system to enhance portfolio management,"An adaptive supervised learning decision (ASLD) trading system has been presented by Xu and Cheung (1997) to optimize the expected returns of investment without considering risks. In this paper, we propose an extension of the ASLD system (EASLD), which combines the ASLD with a portfolio optimization scheme to take a balance between the expected returns and risks. This new system not only keeps the learning adaptability of the ASLD, but also dynamically controls the risk in pursuit of great profits by diversifying the capital to a time-varying portfolio of N assets. Consequently, it is shown that: 1) the EASLD system gives the investment risk much smaller than the ASLD one; and 2) more returns are gained through the EASLD system in comparison with the two individual portfolio optimization schemes that statically determine the portfolio weights without adaptive learning. We have justified these two issues by the experiments.","Portfolios,
Investments,
Supervised learning,
Signal processing,
Computer science,
Neural networks,
Education,
Labeling,
Input variables,
Risk management"
A fuzzy clustering approach for mining diagnostic rules,"In this paper an approach for automatic discovery of transparent diagnostic rules from data is proposed. The approach is based on a fuzzy clustering technique that is defined by three sequential steps. First, our Crisp Double Clustering algorithm is applied on available symptoms measurements, to provide a set of representative multidimensional prototypes that are further clustered onto each one-dimensional projection. The resulting clusters are used in the second step, where a set of fuzzy relations are defined in terms of transparent fuzzy sets. As a final step, the derived fuzzy relations are employed to define a set of fuzzy rules, which establish the knowledge base of a fuzzy inference system that can be used for fuzzy diagnosis. The approach has been applied to the Aachen Aphasia dataset as a real-world benchmark and compared with related work.","Fuzzy set theory,
Multidimensional systems,
Fuzzy sets,
Medical diagnosis,
Computer science,
Clustering algorithms,
Fuzzy systems,
Humans,
Diseases,
Bridges"
Color analysis for Chinese car plate recognition,"License plate recognition (LPR) has many applications in traffic systems. In this paper, a color analysis method for Chinese car plate recognition is proposed. A neural network is used as a classifier to identify pixel color. The color analysis method can remove error candidate car plate regions effectively.","Licenses,
Image color analysis,
Neural networks,
Vehicles,
Character recognition,
Data mining,
Computer science,
Application software,
Computer errors,
Monitoring"
Yet another framework for supporting mobile and collaborative work,"This paper presents the design of YACO (Yet Another framework for Collaborative work), a framework for supporting mobile collaborative work. Mobile collaborative work has been increasing in popularity in business domain. Coworkers cooperate and share expertise across sites and domains, employees may move from a location to another carrying devices (such as PDAs and laptops) in which they store documents. The YACO framework that we have designed aims at exploiting the capabilities provided by an event-based system with support for mobile application in order to offer services to users that collaborates each other in a corporate domain.","Collaborative work,
Mobile computing,
Computer science,
Personal digital assistants,
Portable computers,
Application software,
Wireless networks,
Pervasive computing,
Java,
Message service"
Crossing the interdisciplinary barrier: a baccalaureate computer science option in bioinformatics,"Bioinformatics is a new and rapidly evolving discipline that has emerged from the fields of experimental molecular biology and biochemistry, and from the artificial intelligence, database, pattern recognition, and algorithms disciplines of computer science. Largely because of the inherently interdisciplinary nature of bioinformatics research, academia has been slow to respond to strong industry and government demands for trained scientists to develop and apply novel bioinformatic techniques to the rapidly growing freely available repositories of genetic and proteomic data. While some institutions are responding to this demand by establishing graduate programs in bioinformatics, the entrance barriers for these programs are high, largely because of the significant amount of prerequisite knowledge in the disparate fields of biochemistry and computer science required for sophisticated new approaches to the analysis and interpretation of bioinformatics data. The authors present an undergraduate-level bioinformatics curriculum in computer science designed for the baccalaureate student. This program is designed to be tailored easily to the needs and resources of a variety of institutions.","Computer science education,
Biomedical engineering education"
Exploiting redundancy to implement multiobjective behavior,"Teams of robots can be redundant with respect to a given task. This redundancy can be exploited to pursue additional objectives during the execution of the task. In this paper, we describe a control-based method to exploit such redundancy for the execution of additional behavior, leading to the improvement of overall performance. The control-based method provides a suitable mechanism for combining controllers with different objectives. The mechanism ensures that the subordinate controllers do not interfere with the superior controllers. Thus it allows to build controllers exhibiting complex behavior from simple primitives, while maintaining their provable performance characteristics. The effectiveness of the framework is demonstrated by experiments with a multi-robot exploration task.","Distributed control,
Robot control,
Laboratories,
Computer science,
Modular construction,
Process control,
State estimation,
Robot sensing systems,
Algorithm design and analysis,
Learning"
Gradient field distributions for the registration of images,"This paper introduces a new method to register images that are rotated and translated with respect to each other. The method works by transforming each image to a gradient distribution space. This space represents the likelihood of finding a particular gradient in the image and is invariant to translation. Once transformed the rotation between the images is efficiently found using correlation. Unlike Fourier based methods, phase information is retained in the gradient distribution space, thus a larger class of images can be accurately registered. The method is computationally efficient and does not require nonlinear optimization or iterative methods. Furthermore, large rotations and translations can easily be handled.",
Reconfigurable real-time address trace compressor for embedded microprocessors,"Address trace compression represents that the address data, which were generated from instruction fetch stage of microprocessor, can be retrieved for later observation and analysis. In this paper, we present how to design and implement this real-time address trace compressor (RTATC), which can be used to collect the address trace information of FPGA Prototyping system based on embedded microprocessors. Address trace compressor is able to monitor address bus of microprocessor, and to get operation workload under the analysis of profiling software. Address trace compressor is also allowed to perform accurate, successive trace collection in an unlimited length and can be used in different embedded microprocessors. At last, by the help of FPGA, this silicon intelligent property can own the abundant reconfigurable parameters, the removable modules and the reusable ability.",
Edit distance from graph spectra,"We are concerned with computing graph edit distance. One of the criticisms that can be leveled at existing methods for computing graph edit distance is that it lacks the formality and rigour of the computation of string edit distance. Hence, our aim is to convert graphs to string sequences so that standard string edit distance techniques can be used. To do this we use graph spectral seriation method to convert the adjacency matrix into a string or sequence order. We pose the problem of graph-matching as maximum a posteriori probability alignment of the seriation sequences for pairs of graphs. This treatment leads to an expression for the edit costs. We compute the edit distance by finding the sequence of string edit operations, which minimise the cost of the path traversing the edit lattice. The edit costs are defined in terms of the a posteriori probability of visiting a site on the lattice. We demonstrate the method with results on a data-set of Delaunay graphs.",
eXtreme programming at universities - an educational perspective,"To address the problems of traditional software development, recent years have shown the introduction of more light-weight or ""agile"" development processes (eXtreme Programming being the most prominent one). These processes are intended to support early and quick production of working code by structuring the development into small release cycles and focus on continual interaction between developers and customers. As such software development processes become more popular there is a growing demand from industry to introduce agile development practices in tertiary education. This is not a straightforward task as the corresponding practices may run counter to educational goals or may not be adjusted easily to a learning environment. In this paper, we discuss some of these issues and reflect on the problems of teaching agile processes in tertiary education.","Programming profession,
Educational institutions"
Wavelet based segmentation of hyperspectral colon tissue imagery,"Segmentation is an early stage for the automated classification of tissue cells between normal and malignant types. We present an algorithm for unsupervised segmentation of images of hyperspectral human colon tissue cells into their constituent parts by exploiting the spatial relationship between these constituent parts. This is done by employing a modification of the conventional wavelet based texture analysis, on the projection of hyperspectral image data in the first principal component direction. Results show that our algorithm is comparable to other more computationally intensive methods which exploit the spectral characteristics of the hyperspectral imagery data","Image segmentation,
Hyperspectral imaging,
Colon,
Cancer,
Hyperspectral sensors,
Pathology,
Layout,
Infrared image sensors,
Computer science,
Humans"
Practical non-parametric density estimation on a transformation group for vision,"It is now common practice in machine vision to define the variability in an object's appearance in a factored manner, as a combination of shape and texture transformations. In this context, we present a simple and practical method for estimating non-parametric probability densities over a group of linear shape deformations. Samples drawn from such a distribution do not lie in a Euclidean space, and standard kernel density estimates may perform poorly. While variable kernel estimators may mitigate this problem to some extent, the geometry of the underlying configuration space ultimately demands a kernel, which accommodates its group structure. In this perspective, we propose a suitable invariant estimator on the linear group of non-singular matrices with positive determinant. We illustrate this approach by modeling image transformations in digit recognition problems, and present results showing the superiority of our estimator to comparable Euclidean estimators in this domain.","Shape,
Kernel,
Deformable models,
Costs,
Independent component analysis,
Machine vision,
Computer science,
Information geometry,
Image recognition,
Probability"
Establishing timing requirements and control attributes for control loops in real-time systems,"Advances in scheduling theory have given designers of control systems greater flexibility over their choice of timing requirements. Advances in scheduling theory have given designers of control systems greater flexibility over their choice of timing requirements. This could lead to systems becoming more responsive, more flexible and more maintainable. However, experience has shown that engineers find it difficult to exploit these advantages due o the difficulty in determining the ""real"" timing requirements of systems and therefore the techniques have delivered less benefit than expected. Part of the reason for this is that the models used by engineers when developing systems do not allow for emergent properties such as timing. This paper presents an approach and framework for addressing the problem of identifying an appropriate and valid set of timing requirements and their corresponding control parameters based on a combination of static analysis and simulation.","Control systems,
Timing,
Real time systems,
Mathematical model,
Sampling methods,
Automatic control,
Maintenance engineering,
Job shop scheduling,
Computer science,
Processor scheduling"
EDUCOSM - personalized writable Web for learning communities,"Many of the possibilities of Web-based education are still unexplored. It seems that novel ways of thinking about both learning and technology are needed to get beyond the limitations of the traditional classroom setting. In this paper we introduce EDUCOSM, which focusses on the possibilities of collaboration and the open-ended nature of the Web. Its main features include sharing and annotation of arbitrary Web-pages, and an adaptive desktop for accessing the evolving contents of the system. EDUCOSM has been used in a real Web-based course, and the experiences are discussed along with a description of the tool. Although the approach requires both the teacher and the students to rethink their roles, the feedback received so far has been encouraging.","Information technology,
Collaboration,
Collaborative work,
Computer science,
Computer science education,
Feedback,
Organizing,
Building materials,
Context,
Discussion forums"
Applied neural network for location prediction and resources reservation scheme in wireless networks,"In this paper, NPS (neural network prediction scheme) is proposed to provide high accuracy location prediction of mobile host (MH) in target cell. Multimedia communications is urgently expected in wireless networks. One of the most important and complicated issues is quality of service guarantees in third-generation (3G) wireless networks. In other words, the problem to maintain the continuity of multimedia playing during the handoff is hard to solve. In order to avoid too early or over reservation resulting in a waste of resources, in this study, we present a method called TTRR (three times resource reservation scheme), to let reserved resource become really active upon MH entering into tire-2 (C.H. Choi, et al., Nov. 2000) area of cell. NSP and TTRR can efficiently improve the accuracy of MHs trajectory prediction, increase the success probability of resource reservation, and enhance bandwidth utilization.","Neural networks,
Intelligent networks,
Wireless networks,
Bandwidth,
Wireless communication,
Quality of service,
Trajectory,
Telephony,
Computer science,
Accuracy"
Software systems integration and architectural analysis - a case study,"Software systems no longer evolve as separate entities but are also integrated with each other. The purpose of integrating software systems can be to increase user-value or to decrease maintenance costs. Different approaches, one of which is software architectural analysis, can be used in the process of integration planning and design. This paper presents a case study in which three software systems were to be integrated. We show how architectural reasoning was used to design and compare integration alternatives. In particular, four different levels of the integration were discussed (interoperation, a so-called enterprise application integration, an integration based on a common data model, and a full integration). We also show how cost, time to delivery and maintainability of the integrated solution were estimated. On the basis of the case study, we analyze the advantages and limits of the architectural approach as such and conclude by outlining directions for future research: how to incorporate analysis of cost; time to delivery; and risk in architectural analysis, and how to make architectural analysis more suitable for comparing many aspects of many alternatives during development. Finally we outline the limitations of architectural analysis.","Software systems,
Computer aided software engineering,
Risk analysis,
Costs,
Application software,
Companies,
Corporate acquisitions,
Computer architecture,
Computer science,
Process planning"
Adaptive token bucket algorithm for fair bandwidth allocation in DiffServ networks,"We propose an adaptive token bucket algorithm for achieving proportional sharing of bandwidth among aggregate flows in differentiated service (DiffServ) networks. By observing the simulation results obtained in a study of the throughput of TCP flows in a DiffServ network, we note that the aggregate flow with a lower target rate occupies more bandwidth than its fair share, while the aggregate flow with a higher target rate gets less than its fair share. The proposed algorithm solves this unfairness problem by adjusting the target rate according to the edge-to-edge feedback information. This algorithm does not require any additional signaling protocol or measurement of pen-flow states, since it can be implemented in a distributed manner using only two-bit feedback information carried in the TCP acknowledgement. Using ns-2 simulations, we show that the proposed algorithm provides fair bandwidth sharing under various network conditions.","Channel allocation,
Intelligent networks,
Bandwidth,
Aggregates,
Diffserv networks,
Throughput,
Harmonic analysis,
Telecommunication traffic,
Traffic control,
Computer science"
Unsupervised texture segmentation using multiresolution hybrid genetic algorithm,"This work approaches the texture segmentation problem by incorporating genetic algorithm and k-mean clustering method within a multiresolution structure. First, a quad-tree structure is constructed and the input image is partition into blocks at different resolution levels. Texture features are then extracted from each block. Based on the texture features, a hybrid genetic algorithm is employed to perform the segmentation. The crossover operator of traditional genetic algorithm is replaced with k-means clustering method while the mutate and select operators are adopted. In the final step, the boundaries and the segmentation result of the current resolution level are propagated down to the next level to act as contextual constraints and the initial configuration of the next level, respectively.","Genetic algorithms,
Image segmentation,
Biological cells,
Feature extraction,
Clustering methods,
Image resolution,
Markov random fields,
Data mining,
Clustering algorithms,
Computer science"
Locally testable cyclic codes,"Cyclic linear codes of block length n over a finite field F/sub q/ are the linear subspaces of F/sub q//sup n/ that are invariant under a cyclic shift of their coordinates. A family of codes is good if all the codes in the family have constant rate and constant normalized distance (distance divided by block length). It is a long-standing open problem whether there exists a good family of cyclic linear codes based on F.J. MacWilliams and N.J.A. Sloane (1977). A code C is r-testable if there exist a randomized algorithm which, given a word x /spl isin/ F/sub q//sup n/, adaptively selects r positions, checks the entries of x in the selected positions, and makes a decision (accept or reject x) based on the positions selected and the numbers found, such that (i) if x /spl isin/ C then x is surely accepted; (ii) if dist(x,C) /spl ges/ /spl epsi/n then x is probably rejected (dist refers to Hamming distance). A family of codes is locally testable if all members of the family are r-testable for some constant r. This concept arose from holographic proofs/PCPs. O. Goldreich and M. Sudan (2002) asked whether there exist good, locally testable families of codes. In this paper we address the intersection of the two questions stated.","Testing,
Computer science"
Using a taxonomy tool to identify changes in OO software,"In this paper, we present a taxonomy that allows the maintainer to catalog OO classes based on the characteristics of the class. The characteristics of a class include the properties of data items and methods, as well as the relationships with other classes in the application. We construct a tool to track changes across multiple releases of software applications containing hundreds of classes, providing information about each changed class. Our tool identifies class changes in terms of the characteristics exhibited by classes with the same name in different releases of an application.","Taxonomy,
Software tools,
Application software,
Software maintenance,
Unified modeling language,
Software quality,
Computer science,
Software testing,
Programming,
Debugging"
Mapping nominal values to numbers for effective visualization,"Data sets with a large number of nominal variables, some with high cardinality, are becoming increasingly common and need to be explored. Unfortunately, most existing visual exploration displays are designed to handle numeric variables only. When importing data sets with nominal values into such visualization tools, most solutions to date are rather simplistic. Often, techniques that map nominal values to numbers do not assign order or spacing among the values in a manner that conveys semantic relationships. Moreover, displays designed for nominal variables usually cannot handle high cardinality variables well. This paper addresses the problem of how to display nominal variables in general-purpose visual exploration tools designed for numeric variables. Specifically, we investigate (1) how to assign order and spacing among the nominal values, and (2) how to reduce the number of distinct values to display. We propose that nominal variables be pre-processed using a distance-quantification-classing (DQC) approach before being imported into a visual exploration tool. In the distance step, we identify a set of independent dimensions that can be used to calculate the distance between nominal values. In the quantification step, we use the independent dimensions and the distance information to assign order and spacing among the nominal values. In the classing step, we use results from the previous steps to determine which values within a variable are similar to each other and thus can be grouped together. Each step in the DQC approach can be accomplished by a variety of techniques. We extended the XmdvTool package to incorporate this approach. We evaluated our approach on several data sets using a variety of evaluation measures.",
The softer side of custom software development: working with the other players,"Many university programs in software engineering educate students about various technical topics, programming, methodology, software, applications as well as other topics that are considered 'hard' subjects. However, in the practical industrial world of developing custom software applications, there are various other matters confronting the software engineer that involve the other stakeholders in the development setting. These other stakeholders might be from the 'softer' subjects: sales, marketing, graphical arts, management, etc. This article presents an overview of the other stakeholders and suggests, to the software engineer, how to interact with them effectively.",
Ambient computing applications: an experience with the SPREAD approach,"Today, we assist to the explosive development of mobile computing devices like PDAs and cellphones, the integration of embedded intelligence (like Web server) in more and more common devices, and the proliferation of wireless communication technologies (IRdA, Bluetooth, IEEE 802.11, GPRS). All these trends contribute to move us closer to the ubiquitous computing world described by Mark Weiser. But while the technology is here, applications, and more important, models and tools for designing future ambient computing systems are still rare. One of the first innovative concepts of ubiquitous computing, context-awareness is still hard to use and understand from a programming perspective. We think that the problem resides in the lack of system support: in traditional computing, operating system offers simple to use and easy to understand abstractions of computational resources. Ubiquitous computing involves an integration of ""computing"" into the real-world, which is a radically different environment for applications. We think that this environment requires new operating system services and abstractions. Because the real world is made of physical entities, ""living"" in the physical space, ambient computing software should be able to use abstraction representing such objects, in a simple way. In this paper, we present a light framework to design ubiquitous computing software, called SPREAD. Unlike many approaches which hides too much of the real-world behind traditional computing abstraction, SPREAD defines programming abstraction based on the properties of the physical space. Hence, physical properties, like relative proximity, are used as implicitly in SPREAD as variable addressing in a computer memory. In SPREAD, application (or process) behavior can be ""mechanically"" driven, in the sense that actions flow can be directly dependent of physical mobility. To support this concept, we introduce a programming and execution model allowing designing computing and information systems driven directly by arranging and moving physical objects in the space. We demonstrate the use of the model to implement a few practical applications, highlighting its simplicity and expression power.",
A non-Markovian coupling for randomly sampling colorings,"We study a simple Markov chain, known as the Glauber dynamics, for randomly sampling (proper) k-colorings of an input graph G on n vertices with maximum degree /spl Delta/ and girth g. We prove the Glauber dynamics is close to the uniform distribution after O(n log n) steps whenever k > (1 + /spl epsiv/)/spl Delta/, for all /spl epsiv/ > 0, assuming g /spl ges/ 9 and /spl Delta/ = /spl Omega/(log n). The best previously known bounds were k > 11/spl Delta//6 for general graphs, and k > 1.489/spl Delta/ for graphs satisfying girth and maximum degree requirements. Our proof relies on the construction and analysis of a non-Markovian coupling. This appears to be the first application of a non-Markovian coupling to substantially improve upon known results.","Sampling methods,
Character generation,
Computer science,
Physics,
Chromium,
Polynomials,
Mathematics,
Computational modeling,
Computer simulation,
Antiferromagnetic materials"
Enabling partial cache line prefetching through data compression,"Hardware prefetching is a simple and effective technique for hiding cache miss latency and thus improving the overall performance. However, it comes with addition of prefetch buffers and causes significant memory traffic increase. We propose a new prefetching scheme which improves performance without increasing memory traffic or requiring prefetch buffers. We observe that a significant percentage of dynamically appearing values exhibit characteristics that enable their compression using a very simple compression scheme. The bandwidth freed by transferring values from lower levels in memory hierarchy to upper levels in compressed form is used to prefetch additional compressible values. These prefetched values are held in vacant space created in the data cache by storing values in compressed form. Thus, in comparison to other prefetching schemes, our scheme does not introduce prefetch buffers or increase the memory traffic. In comparison to a baseline cache that does not support prefetching, on average, our cache design reduces the memory traffic by 10%, reduces the data cache miss rate by 14%, and speeds up program execution by 7%","Prefetching,
Data compression,
Bandwidth,
Hardware,
Delay,
Computer science,
Pipelines,
High performance computing,
Pollution,
Parallel processing"
NC-SRAM - a low-leakage memory circuit for ultra deep submicron designs,"In this paper, we present a novel N-controlled SRAM (NC-SRAM) design for reducing the subthreshold leakage in cache and embedded memories using a dual-V/sub t/ process. We combine the use of high V/sub t/ transistors in the leakage path and gating the supply voltage to reduce leakage in unused SRAM cells. This circuit-level technique overcomes the potential limitations in the existing techniques for reducing leakage in memory circuits. In this design, the data stored in the cell is retained even when the memory is put in the stand-by mode, with no additional complexity or circuit overhead. Simulation results indicate that NC-SRAM has better leakage savings as compared to other techniques. In addition, our results on 100 nm and 70 nm processes show 21% and 18% reduction in total power and 77% and55 % reduction in leakage power, respectively, with very minimal impact on performance and area, as compared to a conventional 6T-SRAM.",
Optimal software release time incorporating fault correction,"The ""stopping rule"" problem which involves determining an optimal release time for a software application at which costs justify the stop test decision has been addressed by several researchers. However, most of these research efforts assume instantaneous fault correction, an assumption that underlies many software reliability growth models, and hence provide optimistic predictions of both the cost at release and the release time. In this paper, we present an economic cost model which takes into consideration explicit fault correction in order to provide realistic predictions of release time and release cost. We also present a methodology to compute the failure rate of the software in the presence of fault correction, which is necessary in order to apply the cost model. We illustrate the utility of the cost model to provide realistic predictions of release time and cost with a case study.","Cost function,
Software reliability,
Software testing,
Predictive models,
Application software,
Software debugging,
Computer science,
Economic forecasting,
Programming,
Delay"
Degenerate primer design via clustering,"This paper describes a new strategy for designing degenerate primers for a given multiple alignment of amino acid sequences. Degenerate primers are useful for amplifying homologous genes. However, when a large collection of sequences is considered, no consensus region may exist in the multiple alignment, making it impossible to design a single pair of primers for the collection. In such cases, manual methods are used to find smaller groups from the input collection so that primers can be designed for individual groups. Our strategy proposes an automatic grouping of the input sequences by using clustering techniques. Conserved regions are then detected for each individual group. Conserved regions are scored using a blocksimilarity score, a novel alignment scoring scheme that is appropriate for this application. Degenerate primers are then designed by reverse translating the conserved amino acid sequences to the corresponding nucleotide sequences. Our program, DePiCt, was written in BioPerl and was tested on the Toll-Interleukin Receptor (TIR) and the nonTIR family of plant resistance genes. Existing programs for degenerate primer design were unable to find primers for these data sets.",
An XACML-based policy management and authorization service for globus resources,We describe our approach to a policy management system and a policy enforcement point which is integrated into the globus toolkit middleware. Our system enables the specification and modification of resource policies by administrative parties through a graphical user interface and the secure association with and transport of these policies to the policy decision components.,"Resource management,
Authorization,
Access control,
XML,
Graphical user interfaces,
Markup languages,
Grid computing,
Permission,
Information security,
Computer science"
Control flow driven splitting of loop nests at the source code level,"This paper presents a novel source code transformation for control flow optimization called loop nest splitting which minimizes the number of executed if-statements in loop nests of embedded multimedia applications. The goal of the optimization is to reduce runtimes and energy consumption. The analysis techniques are based on precise mathematical models combined with genetic algorithms. Due to the inherent portability of source code transformations, a very detailed benchmarking using 10 different processors can be performed. The application of our implemented algorithms to three real-life multimedia benchmarks leads to average speed-ups by 23.6%-62.1% and energy savings by 19.6%-57.7%. Furthermore, our optimization also leads to advantageous pipeline and cache performance.","Energy consumption,
Pipelines,
Runtime,
MPEG 4 Standard,
Computer science,
Application software,
Algorithm design and analysis,
Mathematical model,
Genetic algorithms,
Biomedical image processing"
Accelerating Bluetooth inquiry for personal area networks,"The recent industry standard Bluetooth promises low cost replacement of communication cabling with moderate symbol-rate, short-range wireless links. The same specification also addresses the establishment of point-to-multipoint piconets and the interconnection of several of these piconets into scatternets, enabling Bluetooth to be used as a technology for realizing personal area networks. Establishing Bluetooth piconets requires nodes to discover each other by completing an inquiry phase. This paper investigates the inquiry phase; shows the shortcomings of the current inquiry procedure in multi node-PAN scenarios, and outlines and analyses a backwards Bluetooth compliant modification to accelerate Bluetooth inquiry. Extensive simulations comparing the original and the proposed Bluetooth inquiry schemes show improvements of more than an order of magnitude in device discovery times.","Acceleration,
Bluetooth,
Personal area networks,
Communication cables,
Frequency,
Computer science,
Wireless sensor networks,
Master-slave,
Communication industry,
Computer industry"
Does the difference between information and scientific visualization really matter?,Is it necessary to continue to define a difference between information and scientific visualization? Is the determined need for these differences creating confusion rather than helping investigators understand how to effectively apply visual display techniques to data and information? The author considers how cartographic and geographic information techniques seem to span both scientific and information visualization. She discusses the future directions in bioinformatics visualization.,"Data visualization,
Bioinformatics,
Genomics,
Computer graphics,
Telecommunication computing,
Computational modeling,
Remote sensing,
Spatial databases,
Visual databases,
Accidents"
A fuzzy feature clustering with relevance feedback approach to content-based image retrieval,"The increasing number of digitized images required an efficient image retrieval system. In this paper, we demonstrate the fundamental principles, implementation methods, performance evaluations, and experimental results from the proposed model. We present a region-based prototype image retrieval system named FuzzyImage. The system is characterized by feature vectors. First, we segment an image into regions depending on clustering similar feature vectors by fuzzy c-means. Next, a similar measurement is used to evaluate the similarity between the query image and incorporated regions. The users can select the most interesting regions from 5 sample images that pop-up, and by feedback to the system. Based on the selected individual regions of query images, the overall similarity helps filter out irrelevant images in a database after relevance feedback and enables a simple user-oriented query interface for a region-based image retrieval system. This algorithm is implemented and tested on general-purpose images. This project makes three main contributions to a region-based CBIR system. First, a region segmentation method is employed in the FuzzyImage system. Second, this system takes the user's intuition into consideration and designs a user-oriented interface to directly search the database. Thirdly, we evaluate retrieval precision of the system to support this theoretical claim.","Feedback,
Image retrieval,
Content based retrieval,
Image databases,
Information retrieval,
Spatial databases,
Feature extraction,
Computer science,
Electronic mail,
Cellular neural networks"
Mass lesion detection with a fuzzy neural network,"This paper presents a novel fuzzy neural network (FNN) approach to detect malignant mass lesions on mammograms. The mammograms were obtained from the digital database for screening mammography (DDSM) at the University of South Florida. Six-hundred-seventy regions of interest (ROIs) were extracted from 100 mammograms and are randomly divided into two groups: training and testing sets. Entropy, uniformity, contrast, and maximum co-occurrence matrix elements are calculated at sizes of 256/spl times/256 and 768/spl times/768, respectively. The differences of these features (feature differences) from these two image sets with the above mentioned sizes are computed for each feature, and they are discriminant in differentiating between malignant masses and normal tissues regardless of lesion shape, size, and subtlety. After training, the FNN can correctly detect all malignant masses on mammograms in the testing group. The true positive fraction (TPF) is 0.92 when the number of false positives (FP) is 1.33 per mammogram; and 1.0 when the FP is 2.15 per mammogram.",
Modeling and simulation in high school education - two European examples,"Discrete simulation is a suitable application area for several disciplines in high schools. One such discipline is computer science. Other disciplines are e.g. mathematics and business. The main principles of modeling information can be practiced in a suitable simulation project. For the implementation of the model on the computer, two different versions of integrated development systems for GPSS have been developed mainly for the purpose of use in high schools. In Germany the version is WinGPSS, dedicated to Windows, and in Sweden WebGPSS, first implemented on the Web. Both systems use the same micro-GPSS simulator kernel. We discuss the goals with, and the experience gained from, the use of these GPSS versions in high schools. Some of the most recent developments of WinGPSS and WebGPSS are also presented.","Computer science,
Object oriented modeling,
Computational modeling,
Computer science education,
Educational institutions,
Educational programs,
Computer simulation,
Application software,
Computer graphics,
Mathematics"
Web mining research,"Web mining is a cross point of database, information retrieval and artificial intelligence. Web content mining (WCM), Web structure mining (WSM) and Web usage mining (WUM) buildup the whole Web mining. The research issues, techniques and development efforts are presented in this paper.","Web mining,
Search engines,
Data mining,
Pattern analysis,
Web pages,
Computer science,
Web sites,
Internet,
Multimedia systems,
Computational intelligence"
Mobile peer membership management to support multimedia streaming,"A CHUM (Cooperating ad Hoc networking to sUpport Messaging) network is an ad hoc network in which mobile devices cooperate to reduce connection costs when accessing wireless global networks. We study how members of a CHUM network cooperate in order to download multimedia data, such as on-air TV programs. Each peer in a CHUM takes turns serving as a proxy, which makes an Internet connection and downloads multimedia data for the other members. Then, the proxy distributes the data to other peers using its ad hoc connection, which we call chumcast. The proxy maintains and updates peer membership information and determines the next CHUM proxy. The CHUM network includes a recovery procedure when a proxy fails or when a CHUM network partitions.","Streaming media,
Internet,
IP networks,
Ad hoc networks,
Telecommunication traffic,
Wireless LAN,
TV,
Cost function,
Bluetooth,
Computer science"
Using of a cost-based unit commitment algorithm to assist bidding strategy decisions,"The paper describes a procedure developed to assist a generating company in choosing the most convenient bidding strategies for a day-ahead electricity energy market. According to the proposed method, the profit maximization problem is transformed into a minimization problem that can be solved by a traditional hydro-thermal unit commitment program after implementing a few modifications. The paper describes the modifications introduced in a unit commitment program based on the Lagrangian relaxation approach and on a disaggregated Bundle method for the solution of the dual problem. It also presents some results obtained for a realistic data set of hydro-thermal power plants. The results arc discussed in order to emphasize how the method can be applied to assess the bidding strategy choke of a given company.","Power generation,
Production,
Reservoirs,
Electricity supply industry,
Cost function,
Minimization methods,
Lagrangian functions,
Councils,
Computer science,
Water resources"
Combining software and hardware monitoring for improved power and performance tuning,"By anticipating when resources will be idle, it is possible to reconfigure the hardware to reduce power consumption without significantly reducing performance. This requires predicting what the resource requirements will be for an application. In the past, researchers have taken one of two approaches: design hardware monitors that can measure recent performance, or profile the application to determine the most likely behavior for each block of code. This paper explores a third option which is to combine hardware monitoring with software profiling to achieve lower power utilization than either method alone. We demonstrate the potential for this approach in two ways. First, we compare hardware monitoring and software profiling of IPC for code blocks and show that they capture different information. By combining them, we can control issue width and ALU usage more effectively to save more power. Second, we show that anticipating stalls due to critical load misses in the L2 cache can enable fetch halting. Again, hardware monitoring and software profiling must be used together to effectively predict misses and criticality of loads.","Software performance,
Hardware,
Monitoring,
Application software,
Energy consumption,
Sampling methods,
Iris,
Power engineering and energy,
Educational institutions,
Cognitive science"
The role of agents in distributed data mining: issues and benefits,"The increasing demand to extend data mining technology to data sets inherently distributed among a large number of autonomous and heterogeneous sources over a network with limited bandwidth has motivated the development of several approaches to distributed data mining and knowledge discovery, of which only a few make use of agents. We briefly review existing approaches and argue for the potential added value of using agent technology in the domain of knowledge discovery, discussing both issues and benefits. We also propose an approach to distributed data clustering, outline its agent-oriented implementation, and examine potential privacy violating attacks in which agents may incur.","Data mining,
Distributed decision making,
Delta modulation,
Data privacy,
Data warehouses,
Computer science,
Bandwidth,
Databases,
Distributed computing,
Business"
Distributed queueing in scalable high performance routers,"This paper presents and evaluates distributed queueing algorithms for regulating the flow of traffic through large, high performance routers. Distributed queueing has a similar objective to crossbar-scheduling mechanisms used in routers with relatively small port counts, and shares some common high level characteristics. However, the need to minimize communication overhead rules out the iterative methods that are typically used for crossbar scheduling, while the ability to sub-divide the available bandwidth among different ports provides a degree of freedom that is absent in the crossbar scheduling context, where inputs must be matched to outputs. Our algorithms are based on four ideas (1) backlog-proportional-allocation of output bandwidth, (2) urgency-proportional-allocation of input bandwidth, (3) dynamic reallocation of bandwidth and (4) deferred underflow. Our algorithms guarantee congestion-free operation of the switch fabric. Our performance results show that for uniform random traffic, even a very modest speedup is sufficient to reduce the loss of output link bandwidth due to sub-optimal rate allocation to negligible levels, and that even under extreme conditions, a speedup of two is sufficient to eliminate such bandwidth loss.","Switches,
Traffic control,
Bandwidth,
Fabrics,
Packet switching,
Telecommunication traffic,
Iterative algorithms,
Processor scheduling,
Computer science,
Iterative methods"
Neighborhood systems: mathematical models of information granulations,"The notion of neighborhood systems (NS), which is an abstraction of ""near"" or ""negligible distance,"" is used to model information granulation. A granule at a point (object) p consists of a clump (crisp/fuzzy set) of data that are related or near the point p. Mathematically, NS, by its definition, includes topological spaces (TS) and rough sets (RS) as special cases. The deeper part of NS is its ability in representing the context dependent notion of near-ness or approximation (via small granule). We apply the idea to formulate the notion of qualitative fuzzy sets.","Mathematical model,
Mathematics,
Set theory,
Relational databases,
Computer science,
Fuzzy sets,
Fuzzy systems,
Topology,
Fuzzy control,
Information retrieval"
Modification Point Depth and Genome Growth in Genetic Programming,"The evolutionary computation community has shown increasing interest in arbitrary-length representations, particularly in the field of genetic programming. A serious stumbling block to the scalability of such representations has been bloat: uncontrolled genome growth during an evolutionary run. Bloat appears across the evolutionary computation spectrum, but genetic programming has given it by far the most attention. Most genetic programming models explain this phenomenon as a result of the growth of introns, areas in an individual which serve no functional purpose. This paper presents evidence which directly contradicts intron theories as applied to tree-based genetic programming. The paper then uses data drawn from this evidence to propose a new model of genome growth. In this model, bloat in genetic programming is a function of the mean depth of the modification (crossover or mutation) point. Points far from the root are correspondingly less likely to hurt the child's survivability in the next generation. The modification point is in turn strongly correlated to average parent tree size and to removed subtree size, both of which are directly linked to the size of the resulting child.","Crossover Point,
Introns,
Inviable Code,
Code Bloat,
Genetic Programming"
FC-Min: a fast multi-output Boolean minimizer,"We present a novel heuristic algorithm for two-level Boolean minimization. In contrast to the other approaches, the proposed method firstly finds the coverage of the on-sets and from that it derives the group implicants. No prime implicants of the single functions are being computed; only the necessary implicants needed to cover the on-sets are produced. This reverse approach makes the algorithm extremely fast and minimizes the memory demands. It is most efficient for functions with a large number of output variables, where the other minimization algorithms (e.g. ESPRESSO) are too slow. It is also very efficient for highly unspecified functions, i.e. functions with only few terms defined.",
Comparison of sampling sizes for the co-evolution of cooperative agents,"The evolution of heterogeneous team behaviour can be a very demanding task. In order to promote the greatest level of specialization team members should be evolved in separate populations. The greatest complication in the evolution of separate populations is finding suitable partners for evolution at trial time. If too few combinations are tested, the genetic algorithm loses its ability to recognize possible solutions and if too many combinations are tested the algorithm becomes too computationally expensive. In previous work a method of punctuated anytime learning was employed to test all combinations of possible partners at periodic generations to reduce the number of evaluations. In further works, it was found that by varying the number of combinations tested, the sample size, the GA could produce an accurate and even less computationally expensive solution. In this paper, we compare different sampling sizes to determine the most effective approach to finding the solution. We use a box pushing task to compare these different sampling sizes.","Sampling methods,
Testing,
Collaboration,
Genetic algorithms,
Robots,
Intelligent agent,
Computer science,
Educational institutions,
Computational efficiency,
Displays"
Quantifying the properties of SRPT scheduling,"This paper uses a probe-based sampling approach to study the behavioural properties of Web server scheduling strategies, such as processor sharing (PS) and shortest remaining processing time (SRPT). The approach is general purpose, in that it can be used to estimate the mean and variance of the job response time, for arbitrary arrival processes, service time distributions, and scheduling policies. In the paper, we apply the approach to trace-driven simulation of Web server scheduling to compare and contrast the PS and SRPT scheduling policies. We identify two types of unfairness, called endogenous and exogenous unfairness. We quantify each, focusing on the mean and variance of slowdown, conditioned on job size, for a range of system loads. Finally, we confirm recent theoretical results regarding the asymptotic convergence of scheduling policies with respect to slowdown, and illustrate typical performance results for a practical range of job sizes from an empirical Web server workload.",
Community collective efficacy: structure and consequences of perceived capacities in the Blacksburg Electronic Village,"Bandura's social cognitive construct ""perceived self-efficacy"" has been used widely to understand individual behavior as a function of domain-specific beliefs about personal capacities. Collective efficacy is the extension of the self-efficacy construct to organizations and groups; it refers to beliefs about collective capacities in specific domains. Our research is investigating the use of collective efficacy in understanding attitudes and behaviors of members of proximal residential communities with respect to issues like attachment, engagement, and sociality, specifically as modulated by use of the Internet and community networks. This paper describes our analysis of the structure and external validity of the collective efficacy construct.","Educational institutions,
IP networks,
Collaboration,
Marketing and sales,
Databases,
Testing,
Application software,
Computer network management,
Processor scheduling,
Collaborative work"
Harmonicity and dynamics based audio separation,"Audio signal source separation is an interesting task performed by humans. In this paper, we present a frequency grouping algorithm based on principles of harmonicity and dynamics: frequency components with a harmonic relation and similar dynamics belong to the same source. The grouping is demonstrated for a variety of sound mixtures.","Source separation,
Humans,
Loudspeakers,
Psychology,
Image analysis,
Signal processing,
Layout,
Frequency domain analysis,
Independent component analysis,
Computer science"
What should graduating software engineers be able to do?,"This paper is concerned with trying to characterise the skills that students should develop during the course of a degree programme in software engineering. It is based on a generic framework that has been developed within the UK to describe the abilities that engineering students should possess on graduation, and the paper discusses how this framework could be applied to software engineering graduates at the levels of both bachelor's and masters degrees. The discussion covers the kinds of systems that graduates should be capable of developing, the process model within which this development can be described as taking place, and the levels of ability that could be expected for each of the activities within this process model.","Software engineering,
Educational programs,
Knowledge engineering,
Accreditation,
Educational technology,
Computer science,
Engineering students,
Programming,
Packaging,
Engineering profession"
The limitations of limitations,"The authors consider human-centered computing, and argue that human factors and applied cognitive psychologists have not just been selective in regarding certain human characteristics as limitations, but have also selected the wrong things and for the wrong reasons.",
Fault injection for verifying testability at the VHDL level,,"Circuit faults,
Circuit testing,
System testing,
Circuit simulation,
Logic testing,
Logic circuits,
Computer science,
Design engineering,
Digital circuits,
Digital systems"
Scalable and continuous media streaming on peer-to-peer networks,"With the growth of computing power and the proliferation of broadband access to the Internet, media streaming has widely diffused. Although the proxy caching technique is one method to accomplish effective media streaming, it cannot adapt to the variations of user locations and diverse user demands. By using the P2P communication architecture, media streaming can be expected to smoothly react to network conditions and changes in user demands for media-streams. We propose efficient methods to achieve continuous and scalable media streaming system. In our mechanisms, a media stream is divided into blocks for efficient use of network bandwidth and storage space. We propose two scalable search methods and two algorithms to determine an optimum provider peer from search results. Through several simulation experiments, we show that the FLS method can perform continuous media play-out while reducing the amount of search traffic to 1/6 compared with full flooding.","Streaming media,
Peer to peer computing,
Network servers,
Bandwidth,
Telecommunication traffic,
Web and internet services,
Information science,
Computer architecture,
Search methods,
Computational modeling"
Recovering internet symmetry in distributed computing,"This paper describes two systems to recover the Internet connectivity impaired by private networks and firewalls. These devices cause asymmetry in the Internet, making peer-to-peer computing difficult or even impossible. The Condor system is one of those that are severely impaired by the asymmetry. Compared to normal peer-to-peer computing applications, Condor has stricter requirements, which are representative to any grid computing. To make Condor seamlessly work across private networks and over firewalls, we designed and implemented Dynamic Port Forwarding (DPF) and Generic Connection Brokering (GCB). Both DPF and GCB satisfy the representative requirements. Furthermore DPF supports dedicated large clusters very well because it is simple, efficient, and highly scalable. On the other hand, GCB perfectly supports non-dedicated or personal clusters because it is independent to private network or firewall technologies and does not require airy administrative power to deploy it. In this paper, we describe the implementations of DPF and GCB and analyze them with respect to performance, deployability, security, and scalability.",
Server-based scheduling of the CAN bus,"In this paper we present a new share-driven server-based method for scheduling messages sent over the controller area network (CAN). Share-driven methods are useful in many applications, since they provide both fairness and bandwidth isolation among the users of the resource. Our method is the first share-driven scheduling method proposed for CAN. Our server-based scheduling is based on earliest deadline first (EDF), which allows higher utilization of the network than using CAN's native fixed-priority scheduling approach. We use simulation to show the performance and properties of server-based scheduling for CAN. The simulation results show that the bandwidth isolation property is kept, and they show that our method provides a quality-of-service (QoS), where virtually all messages are delivered within a specified time.","Processor scheduling,
Bandwidth,
Automotive engineering,
Timing,
Real time systems,
Communication system traffic,
Broadcasting,
Computer science,
Application software,
Design optimization"
Memory hierarchy design for a multiprocessor look-up engine,"We investigate the implementation of IP look-up for core routers using multiple microengines and a tailored memory hierarchy. The main architectural concerns are limiting the number of and contention for memory accesses. Using a level compressed trie as an index, we show the impact of the main parameter, the root branching factor, on the memory capacity and number of memory accesses. Despite the lack of locality, we show how a cache can reduce the required memory capacity and limit the amount of expensive multibanking. Results of simulation experiments using contemporary routing tables show that the architecture scales well, at least up to 16 processors, and that the presence of a small on-chip cache increases throughput significantly, up to 65% over an architecture with the same number of processors but without a cache, all while reducing the amount of required off-chip memory.","Engines,
Routing,
Throughput,
Internet,
Computer science,
Network-on-a-chip,
Costs,
Microprocessors,
Application software,
IP networks"
Speculative register promotion using advanced load address table (ALAT),"The pervasive use of pointers with complicated patterns in C programs often constrains compiler alias analysis to yield conservative register allocation and promotion. Speculative register promotion with hardware support has the potential to more aggressively promote memory references into registers in the presence of aliases. This paper studies the use of the advanced load address table (ALAT), a data speculation feature defined in the IA-64 architecture, for speculative register promotion. An algorithm for speculative register promotion based on partial redundancy elimination is presented. The algorithm is implemented in Intel's open research compiler (ORC). Experiments on SPEC CPU2000 benchmark programs are conducted to show that speculative register promotion can improve performance of some benchmarks by 1% to 7%.",
Fixed point in fractal image compression as watermarking,"Conventional approaches to embedding digital watermarks I. Cox et al. (1997), are done by adjusting spatial pixel values or frequency coefficients. We propose a novel way to achieve attack detection by calculating the image's fractal compression fixed point which is an approximation to the original image. After such transformation the fixed point image survives fractal image compression and is extremely sensitive to manipulations. A method to embed digital signatures into the fixed point is also presented in order to provide authentication.","Fractals,
Image coding,
Watermarking,
Authentication,
Digital signatures,
Frequency,
Image reconstruction,
Computer science,
Noise robustness,
Digital images"
Using collaborative knowledge base to realize adaptive message filtering in collaborative virtual environment,"In collaborative virtual environment (CVE), users need to do some collaborative work, which is highly synchronous. To achieve effective collaboration, the level of user behavior's simulation accuracy (LOA) in CVE must be high. But if we adopt static high LOA, system's scalability will be greatly decreased. So scalability request must compromise with collaborative request. This paper analyses CVE system's scalability request and collaborative request, and presents a classification of collaborative work and a definition of collaborative knowledge base (CKB). Based on CKB, a knowledge-based adaptive message filtering technique, which improves CVE system's scalability, is put forward.","Collaboration,
Adaptive filters,
Filtering,
Virtual environment,
Collaborative work,
Scalability,
Computer science,
Computer networks,
Avatars,
Displays"
Implicit similarity: a new approach to multi-sensor image registration,"This paper presents an implicit similarity-based approach to registration of significantly dissimilar images, acquired by sensors at different modalities. The proposed algorithm introduces a robust matching criterion by aligning the locations of gradient maxima. The alignment is formulated as a parametric variational optimization problem, which is solved iteratively by considering the intensities of a single image. The location of the maxima of the second image's gradient are used as initialization. We are able to robustly estimate affine and projective global motions using 'coarse to fine' processing, even when the images are characterized by complex space varying intensity transformations. Finally, we present the registration of real images, which were taken by multi-sensor and multi-modality using affine and projective motion models.",
The graduation thesis in the computer engineering program at UnicenP,"This paper describes the graduation thesis course (CT) of the computer engineering program at UnicenP, at University in Curitiba, south of Brazil. The main objective of this course is the consolidation of the several contents and concepts handled throughout the entire engineering program. The graduation thesis is an interdisciplinary activity, obligatorily outlined for software and hardware development, the two great subject areas of the program. Each student develops his own project or in two students' teams during the school year. A professor is assigned for each student's supervision. The supervision begins still in the project's proposal definition phase and lasts until its defense before the board of examiners of the GT. The projects' evaluation is carried through by the Board of Examiners, which is composed by the faculty, either supervisors or assigned by the program's chair. The student, however, proposes the work themes subject to the approval, adaptation or substitution on the part of the commission of the course.","Hardware,
Prototypes,
Educational institutions,
Proposals,
Guidelines,
Computer science education,
Educational programs,
Research and development,
Production engineering,
Concurrent engineering"
Sizing up smart dust,"The name started out as something of a joke. ""Everyone was talking about smart houses, smart buildings, smart bombs, and I thought that it was funny to talk about smart dust,"" remembers Kris Pister. Though he might have named his invention partly in jest, ""smart dust"" is now part of the technical lexicon. The tiny, wireless sensors that started out in his University of California Berkeley, office can now be found in laboratories around the country, where scientists and engineers across many disciplines are eagerly devising applications for them. With possible uses in the military, the home, and the environment-and a new commercial company (Dust, Inc.) devoted solely to its development-today more than ever, smart dust is no joke. How Pister's company came to be, and the means through which smart dust is entering American industry, make for an interesting case study in modem technology commercialization.","Wireless sensor networks,
Intelligent networks,
Databases,
Engines,
Cellular phones,
Hardware,
Embedded system,
Laboratories,
Costs,
Instruments"
A modulo p checked self-checking carry select adder,In this paper a new self-checking carry select adder is proposed. The duplicated adder blocks which are inherent to a carry select adder without error detection are checked modulo 3. Compared to a carry select adder without error detection the delay of the MSB of the sum of the proposed adder does not increase. Compared to a self-checking duplicated carry select adder the area is reduced by 20%. No restrictions are imposed on the design of the adder blocks.,"Delay,
Computer science,
Fault tolerance,
Computer errors,
Redundancy,
Concrete,
Design automation,
Libraries,
Testing"
Self-stabilizing autonomic recoverer for eventual Byzantine software,"We suggest to model software package flaws (bugs) by assuming eventual Byzantine behavior of the package. In particular, the package has been tested by the manufacturer for limited length scenarios when started in a predefined initial state; the behavior beyond the tested scenario may be Byzantine. Restarts (reboots) are useful for recovering such systems. We suggest a general yet practical framework and paradigm, based on a theoretical foundation, for the monitoring and restarting of systems. An autonomic recoverer that monitors and restarts the system is proposed, where: the autonomic recoverer is designed to handle different tasks given specific task requirements in the form of predicates and actions. DAG subsystem hierarchy structure is used by a consistency monitoring procedure in order to achieve gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a kernel resident (anchor) process, and the design of the process to be self-stabilizing. The autonomic recoverer uses new scheme for liveness assurance via online monitoring that complements known schemes for online ensuring safety.","Computer bugs,
Safety,
Computer industry,
Software testing,
Software systems,
Software packages,
Packaging,
Monitoring,
Fault tolerant systems,
Computer science"
Design of the server cluster to support avatar migration,"We identified an important issue when supporting a large scale networked virtual environment (NVE) with a server cluster. This issue is similar to the process migration issue on the parallel computing study and we refer it as the avatar migration problem. That is, when an avatar of an NVE is moving from one region managed by a server to another region managed by a different server, the client site may perceive abrupt screen change due to the different contents managed by these two servers. This paper proposes equations to solve this problem and elaborates the proposed avatar migration mechanism with state diagrams. The implementing architecture is also given in this paper. Our experiments that successfully show the efficiency of the proposed mechanism are given at the last.","Avatars,
Network servers,
Content management,
Bandwidth,
Electronic mail,
Scalability,
Filters,
Computer networks,
Information science,
Large-scale systems"
"Information extraction and integration from heterogeneous, distributed, autonomous information sources - a federated ontology-driven query-centric approach","This paper motivates and describes the data integration component of INDUS (intelligent data understanding system) environment for data-driven information extraction and integration from heterogeneous, distributed, autonomous information sources. The design of INDUS is motivated by the requirements of applications such as scientific discovery, in which it is desirable for users to be able to access, flexibly interpret, and analyze data from diverse sources from different perspectives in different contexts. INDUS implements a federated, query-centric approach to data integration using user-specified ontologies.","Data mining,
Ontologies,
Knowledge acquisition,
Artificial intelligence,
Intelligent systems,
Biology,
Biological information theory,
Decision making,
Computer science,
Application software"
Document clustering using hierarchical SOMART neural network,"Availability of large full-text document collections in electronic form has created a need for tools and techniques that assist users in organizing these collections. Document clustering is one of the popular methods used for this purpose. In this paper, we propose the neural network based document clustering method by using a hierarchically organized network built up from independent Self-Organizing Map (SOM) and Adaptive Resonance Theory (ART) neural networks. We present clustering results using the REUTERS corpus and show an improvement in clustering performance using both entropy and F-measure as evaluation measures.","Neural networks,
Subspace constraints,
Artificial neural networks,
Organizing,
Clustering methods,
Computer science,
Automatic control,
Entropy,
Information retrieval,
Search engines"
Experiences in threading UML throughout a computer science program,"The Department of Electrical Engineering and Computer Science of the United States Military Academy at West Point, NY, decided to standardize its computer science program on the unified modeling language (UML) for all software design representations. Converting the appropriate courses to support formal teaching or reinforcement of UML concepts was planned as a phased approach over four academic years. Formal UML instruction was planned for Computer Science 1 courses and the senior two-course software engineering capstone sequence. Reinforcement of UML would be in the intervening courses. Once implementation began, it became apparent that a prolonged period between formal blocks of instruction was insufficient. Instead, some additional courses were redesigned to support formal UML instruction. The end result was a richer and deeper exposure to UML than anticipated over the same timeframe.","Computer science education,
Specification languages,
Software engineering"
A tamper-resistant framework for unambiguous detection of attacks in user space using process monitors,"Replication and redundancy techniques rely on the assumption that a majority of components are always safe and voting is used to resolve any ambiguities. This assumption may be unreasonable in the context of attacks and intrusions. An intruder could compromise any number of the available copies of a service resulting in a false sense of security. The kernel based approaches have proven to be quite effective but they cause performance impacts if any code changes are in the critical path. We provide an alternate user space mechanism consisting of process monitors by which such user space daemons can be unambiguously monitored without causing serious performance impacts. A framework that claims to provide such a feature must itself be tamper-resistant to attacks. We theoretically analyze and compare some relevant schemes and show their fallibility. We propose our own framework that is based on some simple principles of graph theory and well-founded concepts in topological fault tolerance, and show that it can not only unambiguously detect any such attacks on the services but is also very hard to subvert. We also present some preliminary results as a proof of concept.","Fault detection,
Availability,
Security,
Computer displays,
Redundancy,
Voting,
Fault tolerance,
Computer network management,
Computer science,
Laboratories"
Directional wavelet approach to remove document image interference,,
User identification based on the analysis of the forces applied by a user to a computer mouse,"This paper describes the framework for a branch of augmented cognition research performed at the Adaptive Multimodal Laboratory at the University of Hawaii and a spec application involving the identification of a computer user based on the forces applied to a computer mouse (i.e., click signature) during a task. Data was collected from six people during a pilot study. Two methods used to identify users were a back propagation neural network and discriminant analysis. Results indicate that the discriminant analysis was slightly better at identifying users than the neural network, but its primary advantage was that it required less data preparation. Continuous identification of the user is possible with either method. Successful, identification of the user is a useful first step to proceed to the next stage of the research framework, which is to identify the user's cognitive state for implementation in an augmented cognition system.","Mice,
Cognition,
Sensor phenomena and characterization,
Laboratories,
Neural networks,
Application software,
Displays,
Control systems"
Revisiting SRT quotient digit selection,"The quotient digit selection in the SRT division algorithm is based on a few most significant bits of the remainder and divisor, where the remainder is usually represented in a redundant representation. The number of leading bits needed depends on the quotient radix and digit set, and is usually found by an extensive search, to assure that the next quotient digit can be chosen as valid for all points (remainder, divisor) in a set defined by the truncated remainder and divisor, i.e., an ""uncertainty rectangle"". We present expressions for the number of bits needed for the truncated remainder and divisor, thus eliminating the need for a search through the truncation parameter space for validation. We also present simple algorithms to properly map truncated negative divisors and remainders into nonnegative values, allowing the quotient selection function only to be defined on the smaller domain of nonnegative values.","Mathematics,
Computer science,
Councils,
Uncertainty,
Upper bound,
Digital arithmetic"
An enhanced buyer seller watermarking protocol,"Digital watermarks are helpful about the possession identification as well as copyright and intellectual property protection for tons and tons of multimedia data transmitted through the Internet. In a buyer-seller watermarking protocol, both seller and buyer have to insert their watermarks into a copy. If unauthorized parties have drifted into market, the watermark detection and extraction algorithm will both identify exactly who has the legal ownership of that copy and trace back to the illegal reseller. In Memon and Wong's protocol, the seller is responsible for watermarks insertion, but she/he has no idea what the buyer's watermark is. As a result, the seller has no way to forge the buyer's watermark. However, the seller doesn't have to use her/his own private information in the watermark insertion process. This may bring about the problem of man in the middle attack. Besides, in their protocol, if there is a dispute, a judge is a must. To improve those shortcomings, we shall propose a new scheme where the seller has to use her/his own private key to do the watermarking insertion job. In our scheme, everyone can be the judge if there is a quarrel over the copyright. This provides a simple and fair solution to the judgment of copy deterrence.","Watermarking,
Protocols,
Fingerprint recognition,
Cryptography,
Internet,
Data mining,
Law,
Legal factors,
Copyright protection,
Computer science"
Content-based search and annotations in multimedia digital libraries,"This paper describes a solution for the organization and management of multimedia collections in digital libraries. Video U-DL-A (VUDLA) is an extension to a digital library that allows for storage, indexing and annotation of multimedia documents. It functions in such way that text- and image-based queries can be issued in order to retrieve specific scenes from digital video collections. Technologies such as image and speech processing, video streaming, multimedia databases, information retrieval and graphical user interfaces are integrated to produce a novel multimedia, multimodal environment which re-evaluates text as an important medium for knowledge transmission. We have developed a fully operational testbed to explore multimedia data properties and organization possibilities as well as a wide range of practical applications.","Software libraries,
Streaming media,
Multimedia databases,
Image retrieval,
Indexing,
Layout,
Speech processing,
Information retrieval,
Graphical user interfaces,
Testing"
Link layer support for streaming MPEG video over wireless links,"Streaming video as a form of media is becoming increasingly popular on the Internet. Real-time media such as video requires delay constraints from the network to ensure good quality at the receiver. While watching a video stream on his portable device connected to the Internet through the last-hop wireless link, the mobile user of tomorrow will expect a good experience. But, the time-varying nature of the wireless link can cause video frames to be dropped/delayed, affecting the quality of video at the receiver. In this paper, we propose a link layer approach to improve the quality of MPEG video streaming over a wireless link. We use Bluetooth as the wireless technology on which to test our scheme. Our results show that the quality of streaming video can be substantially improved with our scheme, particularly in bad channel conditions.","Streaming media,
Internet,
Delay,
Bluetooth,
Video compression,
MPEG 4 Standard,
Telecommunication traffic,
Forward error correction,
PSNR,
Computer science"
"On the advantages of approximate vs. complete verification: bigger models, faster, less memory, usually accurate","We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications. The cost of an approximate strategy is the potential for inaccuracy. If complete algorithms terminate, they find all the features they are searching for. On the other hand, by its very nature, randomized search can miss important features. Our experiments suggest that this inaccuracy problem is not too serious. In the case studies presented here and elsewhere, LURCHS random search usually found the correct results. Also, these case studies strongly suggest that LURCH can scale to much larger models than standard model checkers like NuSMV and SPIN. The two case studies presented in this paper are selected for their simplicity and their complexity. The simple problem of the dining philosophers has been widely studied. By making the dinner more crowded, we can compare the memory and runtimes of standard methods (SPIN) and LURCH. When hundreds of philosophers sit down to eat, both LURCH and SPIN can find the deadlock case. However, SPINS memory and runtime requirements can grow exponentially while LURCHS requirements stay quite low. Success with highly symmetric, automatically generated problems says little about the generality of a technique. Hence, our second example is far more complex: a real-world flight guidance system from Rockwell Collins. Compared to NuSMV, LURCH performed very well on this model. Our random search finds the vast majority of faults (close to 90%); runs much faster (seconds and minutes as opposed to hours); and uses very little memory (single digits to 10s of megabytes as opposed to 10s to 100s of megabytes). The rest of this paper is structured as follows. We begin with a theoretical rationale for why random search methods like LURCH can be incomplete, yet still successful. Next, we note that for a class of problems, the complete search of standard model checkers can be overkill. LURCH is then briefly introduced and our two case studies are presented.","State-space methods,
Explosions,
Search problems,
Runtime,
Computer science,
Computer networks,
Telecommunication computing,
Hardware,
Computer security,
Protocols"
Performance of IP address fragmentation strategies for DDoS traceback,"Distributed denial-of-service (DDoS) attacks are among the most difficult and damaging security problems that the Internet currently faces. The component problems for an end-system that is the victim of a DDoS attack are: determining which incoming packets are part of the attack (intrusion detection); tracing back to find the origins of the attack (i.e., ""traceback""); taking action to mitigate or stop the attack at the source by configuring firewalls or taking some kind of punitive measures. The preferable solution to these problems operates in real time so that a DDoS attack can be mitigated before the victim is seriously harmed. The paper focuses on the technique of packet marking/overloading for automated DDoS traceback which is a complex problem simply because attackers can use spoof source IP addresses in their attacking packets. A new packet marking strategy is proposed and is shown to yield better results in terms of complexity and performance.",
Time series forecasting using massively parallel genetic programming,"In this paper we propose a massively parallel GP model in hardware as an efficient, flexible and scaleable machine learning system. This fine-grained diffusion architecture consists of a large amount of independent processing nodes that evolve a large number of small, overlapping subpopulations. Every node has an embedded CPU that executes a linear machine code GP representation at a rate of up to 20,000 generations per second. Besides being efficient, implementing the system in VLSI makes it highly portable and makes it possible to target mobile, on-line applications. The SIMD-like architecture also makes the system scalable so that larger problems can be addressed with a system with more processing nodes. Finally, the use of GP representation and VHDL modeling makes the system highly flexible and easy to adapt to different applications. We demonstrate the effectiveness of the system on a time series forecasting application.","Genetic programming,
Genetic algorithms,
Hardware,
Computer architecture,
Centralized control,
Topology,
Computer science,
Learning systems,
Very large scale integration,
Biological system modeling"
An effective buffer generation method in GIS,"Buffer Analysis is one of the most important functions of spatial analysis in GIS. This paper uses rotation transform point formula and recursion method to further improve on the vector buffer generation algorithm of double parallel lines and circular arcs, simplifies the process of the generation of parallel lines and the circular correction of sharp angles, and finds a better solution to intersection problem of borderlines of buffer zone.","Geographic Information Systems,
Equations,
Signal to noise ratio,
Remote sensing,
Content addressable storage,
Algorithm design and analysis,
Data structures,
Organizing,
Buffer storage,
Computer science"
Real-time support for mobile robotics,"Coordinated behavior of mobile robots is an important emerging application area. Different coordinated behaviors can be achieved by assigning sets of control tasks, or strategies, to robots in a team. These control tasks must be scheduled either locally on the robot or distributed across the team. An application may have many control strategies to dynamically choose from, although some may not be feasible, given limited resource and time availability. Thus, dynamic feasibility checking becomes important as the coordination between robots and the tasks that need to be performed evolves with time. This paper presents an online algorithm for finding a feasible strategy given a functionally equivalent set of strategies for achieving an application's goals. We present two heuristics for feasibility checking. Both consider communication cost and utilization bound to make allocation (of tasks to execution sites) and scheduling decisions. Extensive experimental results show the effectiveness of the approaches, especially in resource-tight environments. We also demonstrate the application of our approach to real-world scenarios involving teams of robots and show how feasibility analysis also allows the prediction of the scalability of the solution to large robot teams.","Mobile robots,
Robot kinematics,
Communication system control,
Real time systems,
Mobile communication,
Computer science,
Application software,
Processor scheduling,
Availability,
Costs"
Using multiple SLAM algorithms,"Simultaneous localisation and map building (SLAM) is one of the most important and challenging areas of mobile robotics. Unfortunately, the optimal Kalman filter solution incurs computational costs that scale quadratically with the number of beacons, which is prohibitive for many real time and large scale applications. Consequently, there is a significant practical need for more efficient approaches. The challenge is to develop methods that are both efficient and mathematically rigorous. In this paper we show that the full SLAM problem can be decomposed into two distinct mathematical operations. One is the maintenance of global state information for both the vehicle and the beacons, and the other is the maintenance of relative state information. These operations are distinct because the former is an unobserservable estimation problem while the latter is not. We argue that solutions to these two problems can be applied as scaffolding for the development of a wide variety of specialized SLAM algorithms. As a practical demonstration of the power of the two operations when applied as a generic solution to the SLAM problem, we provide empirical results for a scenario requiring the real-time construction and maintenance of a map containing 1.1 million beacons.","Simultaneous localization and mapping,
Vehicles,
State estimation,
Mobile robots,
Covariance matrix,
Computational efficiency,
Computer industry,
Cities and towns,
Computer science,
Mobile computing"
On the capacity of the partially coherent additive white gaussian noise channel,,"Additive white noise,
Gaussian noise,
Rayleigh channels,
AWGN channels,
Quadrature phase shift keying,
Computer science,
Computer errors,
Random variables,
Phase locked loops,
Bandwidth"
Aggressive test power reduction through test stimuli transformation,"Excessive switching activity during shift cycles in scan-based cores imposes considerable test power challenges. To ensure rapid and reliable test of SOCs, we propose a scan chain modification methodology that transforms the stimuli to be inserted to the scan chain through logic gate insertion between scan cells, reducing scan chain transitions. We introduce a novel matrix band algebra to formulate the impact of scan chain modifications on test stimuli transformations. Based on this analysis, we develop algorithms for transforming a set of test vectors into power-optimal test stimuli through cost-effective scan chain modifications. Experimental results show that scan-in power reductions exceeding 90% for test vectors and 99.5% for test cubes can be attained by the proposed methodology.","Logic testing,
Logic gates,
Power dissipation,
Algorithm design and analysis,
Inverters,
Matrices,
Algebra,
Computer science,
Reliability engineering,
Power engineering and energy"
Towards a dichotomy theorem for the counting constraint satisfaction problem,"The Counting Constraint Satisfaction Problem (#CSP) over a finite domain can be expressed as follows: given a first-order formula consisting of a conjunction of predicates, determine the number of satisfying assignments to the formula. #CSP can be parametrized by the set of allowed constraint predicates. In this paper we start a systematic study of subclasses of #CSP restricted in this way. The ultimate goal of this investigation is to distinguish those restricted subclasses of #CSP which are tractable, i.e. solvable in polynomial time, from those which are not. We show that the complexity of any restricted #CSP class on a finite domain can be deduced from the properties of polymorphisms of the allowed constraints, similar to that for the decision CSP. Then we prove that if a subclass of the #CSP is tractable, then constraints allowed by the class satisfy some very restrictive condition: it has to have a Mal'tsev polymorphism, that is a ternary operation m(x, y, z) such that m(x, y, y) = m(y, y, x) = x. This condition uniformly explains all existing complexity results for particular cases of #CSP, and allows us to obtain new results and to conjecture a criterion distinguishing tractable counting CSPs. We also obtain a dichotomy theorem for the complexity of #CSP with a 3-element domain and give new simpler proofs of the dichotomy results for the problem of counting graph homomorphisms.","Constraint theory,
Artificial intelligence,
Computer science,
Laboratories,
Polynomials,
Search problems,
Logic,
Graph theory,
Physics,
Prototypes"
General Schema Theory for Genetic Programming with Subtree-Swapping Crossover: Part I,"This is the first part of a two-part paper which introduces a general schema theory for genetic programming (GP) with subtree-swapping crossover. The theory is based on a Cartesian node reference system which makes it possible to describe programs as functions over the space N2 and allows one to model the process of selection of the crossover points of subtree-swapping crossovers as a probability distribution over N4. In Part I, we present these notions and models and show how they can be used to calculate useful quantities. In Part II we will show how this machinery, when integrated with other definitions, such as that of variable-arity hyperschema, can be used to construct a general and exact schema theory for the most commonly used types of GP","Schema Theory,
Genetic Programming,
Node Reference Systems,
Models of Crossover"
A method for the computation of the interruption costs caused by supply voltage dips and outages in small industrial plants,"This paper concerns relatively small industrial users, fed at the MV (10-20 kV) distribution level. Recent surveys show that, today, at least one third of these users are sensitive to voltage dips and have significant costs due to supply voltage disturbances and outages, but only some of them are able to quantify these costs with sufficient accuracy. Based on the information got from several Italian small industrial users surveyed by the authors, a pattern for computation of the interruption costs in these plants is developed, with the aim to provide MV users with a reference tool for a correct costs calculation.","Costs,
Voltage fluctuations,
Electronics packaging,
Industrial plants,
Production,
Computer industry,
Distributed computing,
Industrial electronics,
Computer science,
Electronics industry"
Handling Heterogeneity in Networked Virtual Environments,"The availability of inexpensive and powerful graphics cards as well as fast Internet connections make networked virtual environments viable for millions of users and many new applications. It is therefore necessary to cope with the growing heterogeneity that arises from differences in computing power, network speed, and users' preferences. This paper describes an architecture that accommodates the heterogeneity while allowing a manager to define systemwide policies. One of the main objectives of our scheme is to allow slower nodes to participate in the session by preventing fast nodes from flooding slow nodes with too many messages. Policies and users' preferences can be expressed as simple linear equations forming a model that describes the system as a whole as well as its individual components. When solutions to this model are mapped back to the problem domain, viable solutions that accommodate heterogeneity and system policies are obtained. For example, slower nodes may receive less frequent updates than faster ones for one or several information streams. The results of our experiments with a proof-of-concept system are described.",
d-Agent: an approach to mobile agent planning for distributed information retrieval,"Owing to advances in home networking, Internet, and mobile computing technology, the retrieval of information that is required in a timely manner has increased in importance for both the home and business user. This type of information can usually be delivered to users within the required time. Therefore, a retrieval service needs to be able to supply the required information to users within a specified time given by the user, and at the same time, it needs to have minimum system overheads from network traffic when performing the retrieval task for an anticipated increasing number of users. In this paper, we propose a new agent-planning algorithm, called d-Agent, for distributed information retrieval. This planning algorithm has two goals: (1) to guarantee a given turn-around time; (2) to schedule mobile agents (the number of agents and each agent's itinerary) optimally while maintaining low system overheads. Although the algorithm tends to slightly increase the planning cost because of these two requirements, a simulation study shows that our algorithm is practical and realistic, and can be directly applied to distributed information retrieval with deadline constraints.","Mobile agents,
Information retrieval,
Distributed computing,
Mobile computing,
Internet,
Telecommunication traffic,
Bandwidth,
Technology planning,
Delay,
Computer science"
Prefetching for visual data exploration,"Modern computer applications, from business decision support to scientific data analysis, utilize data visualization tools to support exploratory activities. Visual exploration tools typically do not scale well when applied to huge data sets, partially because being interactive necessitates real-time responses. However, we observe that interactive visual explorations exhibit several properties that can be exploited for data access optimization, including locality of exploration, contiguous queries, and significant delays between user operations. We thus apply semantic caching of active query sets on the client side to exploit some of the above characteristics. We also introduce several prefetching strategies, each exploiting characteristics of our visual exploration environment. We have incorporated caching and prefetching strategies into XmdvTool, a public-domain tool for visual exploration of multivariate data sets. Experimental studies using synthetic as well as real user traces are conducted. Our results demonstrate that these proposed optimization techniques achieve significant performance improvements in our exploratory analysis system.","Prefetching,
Data visualization,
Navigation,
Delay,
Computer science,
Computer applications,
Data analysis,
Performance analysis,
Stock markets,
Marketing and sales"
Secure routing in ad hoc networks and a related intrusion detection problem,"The intrinsic nature of wireless ad hoc networks makes them vulnerable to various passive or active attacks. Thus, there is no guarantee that a routed communication path is free of malicious nodes that will not comply with the employed protocol and attempt to interfere the network operations. In this paper, we survey the problem of secure routing in ad hoc wireless networks, and discuss the related techniques of cryptographic key distribution. However, no matter how secure the routing protocol is, it is still possible that some nodes are compromised and become malicious. The presence of compromised nodes, especially in nodes that are communication bottlenecks, limit the effectiveness of the described secure routing protocols. We therefore consider the problem of intrusion detection for such nodes. The intrusion detection problem and some solutions are described in detail for a concrete queueing model of medium access. The extensions of the solutions to address the problem in more general scenarios are also discussed.","Intelligent networks,
Ad hoc networks,
Intrusion detection,
Routing protocols,
Cryptography,
Cryptographic protocols,
Wireless networks,
Authentication,
Computer science,
Mobile ad hoc networks"
A proof theory for generic judgments: an extended abstract,"A powerful and declarative means of specifying computations containing abstractions involves meta-level, universally quantified generic judgments. We present a proof theory for such judgments in which signatures are associated to each sequent (used to account for eigenvariables of sequent) and to each formula in the sequent (used to account for generic variables locally scoped over the formula). A new quantifier, /spl nabla/, is introduced to explicitly manipulate the local signature. Intuitionistic logic extended with /spl nabla/ satisfies cut-elimination even when the logic is additionally strengthened with a proof theoretic notion of definitions. The resulting logic can be used to encode naturally a number of examples involving name abstractions, and we illustrate using the /spl pi/-calculus and the encoding of object-level provability.",
Document copy detection based on kernel method,"We present semantic sequence kernel (SSK) to detect document plagiarism, which is derived from string kernel (SK) and word sequence kernel (WSK). SSK first finds out semantic sequences in documents, and then it uses a kernel function to calculate their similarity. SK and WSK only calculate the gap between the first word and the last one. SSK takes into account each common word's position information. We believe SSK contains both local and global information so that it makes a great progress in small partial plagiarism detection. We compare SSK with relative frequency model and semantic sequence model, which is a word frequency based model. The results show that SSK is excellent on nonrewording corpus. It is also valid on rewording corpus with some impairment on the performance.",
Planning multi-goal tours for robot arms,"This paper considers the following multi-goal motion planning problem: a robot arm must reach several goal configurations in some sequence, but this sequence is not given. Instead, the robot's planner must compute an optimal or near-optimal path through the goals. This problem occurs, for instance, in spot-welding, inspection, and measurement tasks. It combines two computationally hard sub-problems: the shortest-path and traveling-salesman problems. This paper describes a greedy algorithm that operates under the assumption that the number of goals is relatively small (a few dozen at most) and the computational cost of finding a good path between two goals dominates that of finding a good tour in a graph with edges of given costs. Although the algorithm computes a quadratic number of goal-to-goal paths in the worst case, it is much faster in practice.","Robots,
Manipulators,
Welding,
Computer science,
Inspection,
Robotic assembly,
Orbital robotics,
Motion planning,
Greedy algorithms,
Computational efficiency"
Integration of spatial and temporal contexts for action recognition by self organizing neural networks,"We present a neural architecture which learns to recognize object-directed actions by visually observing examples. Our architecture learns to extract spatial (e.g. object relationships) and movement contexts, self-organizes symbolic representations of them, and integrates them in a temporal context, producing self-organized symbolic action classes. Each of the above functions is realized by a self organizing neural network module. A preprocessing module takes a video input and feeds object and movement features to the modules. The System can learn to recognize simple grasp-transfer-place actions performed by a human hand in 2D scenes by simply observing example performances. Intermediate and top level categorical representations are self organized without explicit external supervisory signals.","Organizing,
Neural networks,
Layout,
Biological neural networks,
Intelligent robots,
Intelligent systems,
Robot programming,
Hidden Markov models,
Information science,
Data mining"
Power-aware route maintenance protocol for mobile ad hoc networks,"Most power-aware routing protocols proposed for mobile ad hoc networks are ""proactive"" protocols where power consumption state of nodes are exchanged periodically among nodes. For on-demand reactive routing protocols, however, new routes have to be acquired periodically to better reflect the current power states of nodes. We propose a power-aware route maintenance protocol in order to prolong the network lifetime, when applied to on-demand reactive routing protocols without periodic route recovery. This is achieved by using two threshold power levels to evenly distribute power dissipation among nodes. Through simulations, we proved that our protocol can be applied to on-demand routing protocols without the need to perform periodic route recovery.","Mobile ad hoc networks,
Routing protocols,
Peer to peer computing,
Batteries,
Power engineering and energy,
Ad hoc networks,
Mobile communication,
Computer science,
Mobile computing,
Energy consumption"
Performance evaluation of routing algorithms in RHiNET-2 cluster,"System area networks (SANs), which usually accept irregular topologies, have been used to connect nodes in PC/WS clusters or high-performance storage systems. A lot of deadlock-free routings for SANs have been proposed, and their evaluation on simulations have been widely done. However, these simulation results may differ from that of real PC clusters, since hosts, network interfaces and switches used in the simulation are simplified for achieving enough simulation speed. In this paper, we implement deadlock-free routings on a high-performance PC cluster called RHiNET-2, and evaluate their performance. Execution results show that the DL routing and the structured channel pools achieve almost the same total bandwidth and execution time of the barrier synchronization. Compared with the simple Up*/Down* routing, they improve 51% of total bandwidth and 29% improvement on execution time of the barrier synchronization.","Parallel processing,
Communication system routing,
System recovery"
Do class comments aid Java program understanding?,"This paper describes an experiment that investigates the effects of class and method comments on Java program understanding among beginning programmers. Each of the 103 students from CS1 class at Oregon Slate University was given one of four versions (no comments, only method comments, only class comments, and both method and class comments) of a Java database program and answered questions about the program. The results indicated that method comments do increase low-level program understanding, while class comments did not increase high-level understanding. This raises questions about the role of class comments in object-oriented programs, as well as the kind of commenting guidelines that should be used in teaching CS1 classes.","Java,
Guidelines,
Programming profession,
Databases,
Books,
Testing,
Debugging"
Situation-aware contract specification language for middleware for ubiquitous computing,"Ubicomp applications are characterized as situation-aware, frequently-and-ephemerally-communicated and QoS-properties-associated. Using middleware to provide multiple QoS support for these ubicomp applications will enhance the development of the ubicomp applications. To satisfy the different QoS requirements of various applications in ubicomp environments, which are heterogeneous and resource-variant, it is important for the underlining middleware to adapt to different QoS requirements and environments. Situation-Aware Contract Specification Language (SA-CSL) specifies the QoS requirements of the applications. The specification includes requirements in situation-awareness, real-time constraints and security properties. This specification is used to customize the middleware architecture to better satisfy these requirements. SA-CSL is based on the Separation of Concern (SoC) discipline used in the Aspect-Oriented Software Development (AOSD). It specifies the crosscutting aspects of situation-awareness, real-time constraints and security property separately. Because of the object-oriented design, SA-CSL is open for incorporating new QoS properties specification.","Contracts,
Specification languages,
Middleware,
Ubiquitous computing,
Security,
Pervasive computing,
Quality of service,
Application software,
Context,
Computer science"
Creating a digital-vehicle proving ground,"This installment presents the state of the art of ITS research in China, particularly the facilities and the proving ground for testing automated vehicles. To combine their strengths, in 2002 the ITSC, the Chinese Academy of Sciences, and the University of Arizona agreed to conduct joint research on a digital automobile proving ground (DAPG) for automated-vehicle driving tests based on their Beijing and Tucson facilities. The paper describes this international collaboration's status and progress.","Vehicles,
System testing,
Communication system control,
Automobiles,
Automatic testing,
Transportation,
Humans,
Wireless communication,
Control systems,
Job shop scheduling"
Profile-based Pottery Reconstruction,"A major obstacle to the broader use of 3D object reconstruction and modeling is the extent of manual intervention needed. Such interventions are currently extensive and exist throughout every phase of a 3D reconstruction project: collection of images, image management, establishment of sensor position and image orientation, extracting the geometric information describing an object, and merging geometric, texture and semantic data. We present a fully automated approach to pottery reconstruction based on the fragment profile, which is the cross-section of the fragment in the direction of the rotational axis of symmetry. We demonstrate the method and give results on synthetic and real data.",
A generic distributed broadcast scheme in ad hoc wireless networks,"We propose a generic framework for distributed broadcasting in ad hoc wireless networks. The approach is based on selecting a small subset of hosts (also called nodes) to form a forward node set to carry out a broadcast process. The status of each node, forwarding or non-forwarding, is determined either by itself (self-pruning) or by other nodes (neighbor-designating). Node status can be determined at different snapshots of network state along time (called views) without causing problems in broadcast coverage. A sufficient condition, called coverage condition, is given for a node to take the non-forward status. Such a condition can be easily checked locally around the node. Several existing broadcast algorithms can be viewed as special cases of the generic framework with k-hop neighborhood information. A comprehensive comparison among existing algorithms is conducted. Simulation results show that new algorithms, which are more efficient than existing ones, can be derived from the generic framework. This work is an extension to an early work in which only self-pruning methods are discussed [16].","Broadcasting,
Intelligent networks,
Wireless networks,
Ad hoc networks,
Network topology,
Computer science,
Sufficient conditions,
Routing,
Floods,
Wireless communication"
"Solving sparse, symmetric, diagonally-dominant linear systems in time O(m/sup 1.31/","We present a linear-system solver that, given an n-by-n symmetric positive semi-definite, diagonally dominant matrix A with m non-zero entries and an n-vector b, produces a vector x/spl tilde/ within relative distance /spl epsi/ of the solution to Ax = b in time O(m/sup 1.31/log(n//spl epsi/)b/sup O(1)/), where b is the log of the ratio of the largest to smallest non-zero entry of A. If the graph of A has genus m/sup 2/spl theta// or does not have a K/sub m/spl theta// minor, then the exponent of m can be improved to the minimum of 1 + 5/spl theta/ and (9/8)(1 + /spl theta/). The key contribution of our work is an extension of Vaidya's techniques for constructing and analyzing combinatorial preconditioners.","Linear systems,
Symmetric matrices,
Iterative methods,
Chebyshev approximation,
Computer science,
Sparse matrices,
Gradient methods,
Transmission line matrix methods,
Mathematics,
Artificial intelligence"
Mining frequent itemsets in distributed and dynamic databases,"Traditional methods for frequent itemset mining typically assume that data is centralized and static. Such methods impose excessive communication overhead when data is distributed, and they waste computational resources when data is dynamic. We present what we believe to be the first unified approach that overcomes these assumptions. Our approach makes use of parallel and incremental techniques to generate frequent itemsets in the presence of data updates without examining the entire database, and imposes minimal communication overhead when mining distributed databases. Further, our approach is able to generate both local and global frequent itemsets. This ability permits our approach to identify high-contrast frequent itemsets, which allows one to examine how the data is skewed over different sites.","Data mining,
Itemsets,
Distributed databases,
Transaction databases,
Distributed computing,
Frequency,
Information science,
Computer science,
Computer networks,
Parallel algorithms"
Design and implementation of an agent-oriented expert system of loan risk evaluation,"How to effectively control the risks of loans by means of modern risk management is becoming an increasingly important issue for the development of Chinese banking industry. In addition to considering loan risk probabilities, the expert system of loan risk evaluation introduces two more factors: the price of the loan and the risk preferences of the decision-makers. With the help of the knowledge obtained from experts at evaluating risks of loan, the system establishes a loan risk evaluation index system to control the risks of loans comprehensively. We introduce the method of implementation of an agent-oriented expert system of loan risk evaluation, and discusses the overall structure and function features of the system. The knowledge base of the system is studied, together with the inference engine and the knowledge obtainer.","Expert systems,
Risk management,
Banking,
Industrial control,
Control systems,
Engines,
Knowledge management,
USA Councils,
Chaos,
Computer science"
Reusable active learning system for improving the knowledge retention and better knowledge management,"It has been proven that students are able to learn and retain knowledge better by actively participating rather than learning passively. In line with this thrust, the objective is to address the problem of knowledge retention and reuse of implicit and explicit knowledge through use of an active learning system. In particular we aim to develop a learning module in a database course which would encompass lessons learnt from mistakes, tacit and explicit knowledge from both experts and students. In the process, it endeavors efficient use of students and teacher's time by maximum reuse of knowledge derived from experience and faster access to knowledge with the help of an active learning system.","Learning systems,
Knowledge management,
Education,
Databases,
Costs,
Management information systems,
Error correction,
Feedback,
Wheels"
Using supertasks to improve processor utilization in multiprocessor real-time systems,"We revisit the problem of supertasking in Pfair-scheduled multiprocessor systems by presenting a generalized ""reweighting"" algorithm. The generalized algorithm we present breaks new ground by permitting tasks to have noninteger execution costs, by incorporating blocking terms into the analysis, and by assuming a more flexible global-scheduling model. To demonstrate the efficacy of the supertasking approach, we present an experimental evaluation of our algorithm that suggests that reweighting may often result in almost no schedulability loss in practice.","Real time systems,
Processor scheduling,
Scheduling algorithm,
Costs,
Multiprocessing systems,
Computer science,
Algorithm design and analysis,
Partitioning algorithms,
Fasteners,
Quantum mechanics"
SBARC: A supernode based peer-to-peer file sharing system,"Peer-to-peer (P2P) system has become one of the hottest research topics, its excellent characteristics of fully decentralized control and self-organizing make it attractive for some particular applications. However, it faces more technical problems than client/server architecture. In this paper, we propose SBARC,a new P2P file sharing system which takes into account the tremendous resource difference among peers to improve system performance. SBARC divides the peers into supernodes and ordinary nodes and most workloads are taken into supernodes. The main contributions of SBARC are (1). A supernode based routing algorithm which can reduce the average routing latency; (2) Routing information are cached to further reduce the routing cost; (3). A coordinate file caching scheme which achieves efficient utilization of free storage space. Our simulation results show SBARC routing and caching schemes can achieve better performance than previous approaches.","Peer to peer computing,
Routing,
System performance,
Computer networks,
Computer science,
Distributed control,
Application software,
Delay,
Costs,
Computer architecture"
Comparing representations and recombination operators for the multi-objective 0/1 knapsack problem,"The multiple knapsack problem (MKP) is a popular test-bed for researchers developing new Pareto-based multiobjective evolutionary algorithms. We explore a range of different representations and operators for the MKP, which have been adapted from the single objective case. Results indicate that order-based approaches are superior to binary representations for the problem instances considered here.","Testing,
Evolutionary computation,
Computer science,
Computer industry,
Industrial relations,
Genetics,
Degradation"
Optimality and stability study of timing-driven placement algorithms,"This work studies the optimality and stability of timing-driven placement algorithms. The contributions of this work include two parts: 1) We develop an algorithm for generating synthetic examples with known optimal delay for timing driven placement (T-PEKO). The examples generated by our algorithm can closely match the characteristics of real circuits. 2) Using these synthetic examples with known optimal solutions, we studied the optimality of several timing-driven placement algorithms for FPGAs by comparing their solutions with the optimal solutions, and their stability by varying the number of longest paths in the examples. Our study shows that with a single longest path, the delay produced by these algorithms is from 10% to 18% longer than the optima on the average, and from 34% to 53% longer in the worst case. Furthermore, their solution quality deteriorates as the number of longest paths increases. For examples with more than 5 longest paths, their delay is from 23% to 35% longer than the optima on the average, and is from 41% to 48% longer in the worst case.","Delay,
Timing,
Circuit stability,
Upper bound,
Permission,
Computer science,
Character generation,
Field programmable gate arrays,
Integrated circuit interconnections,
Circuits and systems"
Application of particle filtering in navigation system for blind,"The navigation system for the blind, equipped with the GPS receiver, digital map and dead-reckoning sensors, is described. The problem of estimation of the pedestrian position, based on information from different sources, is solved using the approach known as particle filtering. The particle clustering and convex region mapping techniques are used to guarantee that at all times the position estimates are feasible, i.e. that they comply with the constraints imposed by the digital map of the traversed area.","Filtering,
Global Positioning System,
Sensor systems,
Sensor phenomena and characterization,
Legged locomotion,
Satellite navigation systems,
Position measurement,
Application software,
Computer science,
Character recognition"
Applying Guided Evolutionary Simulated Annealing to cost-based abduction,"Guided Evolutionary Simulated Annealing (GESA) is a parallel simulated annealing (SA) technique that is based on competition among a population of independent SA chains. In each chain, each current state, called the parent state, iteratively, generates a number of child states using a domain-dependent neighborhood operator. The most fit child is deterministically determined, and then is allowed to replace the parent with a logistic probability. The number of child states that each parent is allowed to generate in each iteration is dependent on the quality of the solutions produced by this chain in the past. We show how this technique can be applied to cost-based abduction (CBA), an important AI formalism for representing knowledge under uncertainty. Performance is evaluated using a suite of 50 randomly generated CBA instances, containing 50 hypotheses and 70 rules.","Simulated annealing,
Computational modeling,
Computer simulation,
Uncertainty,
Computer science,
Logistics,
Artificial intelligence,
Temperature,
Computational intelligence,
Evolutionary computation"
Extremal properties of polynomial threshold functions,"We give new extremal bounds on polynomial threshold function (PTF) representations of Boolean functions. Our results include the following: 1) Almost every Boolean function has PTF degree at most n/2+O(/spl radic/(n log n)). Together with results of Anthony and Alon, we establish a conjecture of Wang and Williams [1991] and Aspnes, Beigel, Furst, and Rudich [1994] up to lower order terms. 2) Every Boolean function has PTF density at most (1-1/O(n))2/sup n/. This improves a result of Gotsman [1989]. 3) Every Boolean function has weak PTF density at most O(1)2/sup n/. This gives a negative answer to a question posed by Saks [1993]. 4) PTF degree /spl lfloor/log/sub 2/m/spl rfloor/+1 is necessary and sufficient for Boolean functions with sparsity m. This answers a question of Beigel [2000].","Polynomials,
Boolean functions,
Computer science,
Computational complexity,
Decision trees,
Complexity theory,
Circuits,
Upper bound,
Mathematics,
Binary decision diagrams"
Fast depth of field rendering with surface splatting,"We present a new fast algorithm for rendering the depth-of-field effect for point-based surfaces. The algorithm handles partial occlusion correctly, it does not suffer from intensity leakage and it renders depth-of-field in presence of transparent surfaces. The algorithm is new in that it exploits the level-of-detail to select the surface detail according to the amount of depth-blur applied. This makes the speed of the algorithm practically independent of the amount of depth-blur. The proposed algorithm is an extension of the elliptical weighted average (EWA) surface splatting. We present a mathematical analysis that extends the screen space EWA surface splatting to handle depth-of-field rendering with level-of-detail, and we demonstrate the algorithm on example renderings.","Rendering (computer graphics),
Mathematical analysis,
Focusing,
Lenses,
Filtering algorithms,
Computer science,
Image generation,
Humans,
Layout,
Cameras"
Improving the connectivity of PRM roadmaps,"In this paper we investigate how the coverage and connectedness of PRM roadmaps can be improved by adding a connected component (CC) connection step to the general PRM framework. We provide experimental results establishing that significant roadmap improvements can be obtained relatively efficiently by utilizing a suite of CC connection methods, which include variants of existing methods such as RRT and a new ray tracing based method. The coordinated application of these techniques is enabled by methods for selecting and scheduling pairs of nodes in different CCs for connection attempts. In addition to identifying important and/or promising regions of C-space for exploration, these methods also provide a mechanism for controlling the cost of the connection attempts. In our experiments, the time required by the improvement phase was on the same order as the time used to generate the initial roadmap.","Space exploration,
Motion planning,
Robot kinematics,
Joining processes,
Computer science,
Ray tracing,
Carbon capture and storage,
Costs,
Application software,
Orbital robotics"
Collision-free path planning of a telerobotic manipulator based on swept volume of teleoperated manipulator,"A new approach to collision-free path planning for a telerobotic manipulator is proposed. Using the swept volume of a slave manipulator teleoperated by a human operator, an online transition to autonomous tele-operation from master-slave manipulation is achieved without any geometric model of the environment. This feature enables a wider application to unstructured environments.",
Evaluation of TnT Tagger for Spanish,"Part of speech (POS) tagger is a necessary module in many natural language text processing tasks. A POS tagger is a program that accepts an unprepared raw text in input and to each word adds a tag specifying its grammatical properties, such as part of speech, number, person, etc. One of popular POS taggers - TnT tagger - has been extensively tested for English and some other languages. This paper reports on its evaluation for Spanish language. Error analysis is reported, explaining how some specific features of Spanish language affect tagger performance. It is reported that on Spanish texts TnT shows overall tagging accuracy between 92.5% and 95.84%, specifically, between 95.47% and 98.56% on known words and between 75.57% and 83.49% on unknown words. Results show that TnT has reached a good level of maturity and is helpful enough for NLP tasks.","Natural languages,
Text processing,
Testing,
Tagging,
Speech processing,
Error analysis,
Text recognition,
Character recognition,
Speech recognition,
Mood"
Online Pen-Based Recognition of Music Notation with Artificial Neural Networks,,
Probabilistic treatment of MIXes to hamper traffic analysis,"The goal of anonymity providing techniques is to preserve the privacy of users, who has communicated with whom, for how long, and from which location, by hiding traffic information. This is accomplished by organizing additional traffic to conceal particular communication relationships and by embedding the sender and receiver of a message in their respective anonymity sets. If the number of overall participants is greater than the size of the anonymity set and if the anonymity set changes with time due to unsynchronized participants, then the anonymity technique becomes prone to traffic analysis attacks. We are interested in the statistical properties of the disclosure attack, a newly suggested traffic analysis attack on the MIXes. Our goal is to provide analytical estimates of the number of observations required by the disclosure attack and to identify fundamental (but avoidable) 'weak operational modes' of the MIXes and thus to protect users against a traffic analysis by the disclosure attack.","Telecommunication traffic,
Protection,
Privacy,
Drives,
Computer science,
Organizing,
Broadcasting,
GSM,
ISDN,
IP networks"
VTLN-based voice conversion,"In speech recognition, vocal tract length normalization (VTLN) is a well-studied technique for speaker normalization. As voice conversion aims at the transformation of a source speaker's voice into that of a target speaker, we want to investigate whether VTLN is an appropriate method to adapt the voice characteristics. After applying several conventional VTLN warping functions, we extend the piecewise linear function to several segments, allowing a more detailed warping of the source spectrum. Experiments on voice conversion are performed on three corpora of two languages and both speaker genders.","Speech recognition,
Frequency,
Piecewise linear techniques,
Parameter estimation,
Computer science,
Training data,
Target tracking,
Smoothing methods"
Out-of-core isosurface extraction of time-varying fields over irregular grids,"In this paper, we propose a novel out-of-core isosurface extraction technique for large time-varying fields over irregular grids. We employ our meta-cell technique to explore the spatial coherence of the data, and our time tree algorithm to consider the temporal coherence as well. Our one-time preprocessing phase first partitions the dataset into meta-cells that cluster spatially neighboring cells together and are stored in disk. We then build a time tree to index the meta-cells for fast isosurface extraction. The time tree takes advantage of the temporal coherence among the scalar values at different time steps, and uses BBIO trees as secondary structures, which are stored in disk and support I/O-optimal interval searches. The time tree algorithm employs a novel meta-interval collapsing scheme and the buffer technique, to take care of the temporal coherence in an I/O-efficient way. We further make the time tree cache-oblivious, so that searching on it automatically performs optimal number of block transfers between any two consecutive levels of memory hierarchy (such as between cache and main memory and between main memory and disk) simultaneously. At run-time, we perform optimal cache-oblivious searches in the time tree, together with I/O-optimal searches in the BBIO trees, to read the active meta-cells from disk and generate the queried isosurface efficiently. The experiments demonstrate the effectiveness of our new technique. In particular, compared with the query-optimal main-memory algorithm by Cignoni et al. (1997) (extended for time-varying fields) when there is not enough main memory, our technique can speed up the isosurface queries from more than 18 hours to less than 4 minutes.","Isosurfaces,
Data mining,
Data visualization,
Computer graphics,
Grid computing,
Information science,
Spatial coherence,
Clustering algorithms,
Partitioning algorithms,
Runtime"
A national virtual specimen database for early cancer detection,"Access to biospecimens is essential for enabling cancer biomarker discovery. The National Cancer Institute's (NCI) Early Detection Research Network (EDRN) comprises and integrates a large number of cancer research institutions into a network in order to establish a collaborative scientific environment to discover and validate disease markers. The diversity of both the institutions and the collaborative focus has created the need for establishing cross-disciplinary teams focused on integrating expertise in cancer research, computational and biostatistics, and computer science. Given the collaborative design of the network, the EDRN needed an informatics infrastructure. The Fred Hutchinson Cancer Research Center, the National Cancer Institute, and NASA's Jet Propulsion Laboratory (JPL) teamed up to build an informatics infrastructure creating a collaborative, science-driven research environment despite the geographic and morphologic differences of the information systems that existed within the diverse network. EDRN investigators identified the need to share biospecimen data captured across the country managed in disparate databases. As a result, the informatics team initiated an effort to create a virtual biospecimen database whereby scientists could search and retrieve details about specimens located at collaborating institutions. Each database, however, was locally implemented and integrated into collection processes and methods unique to each institution. This meant that efforts to integrate databases needed to be done in a manner that did not require redesign or re-implementation of existing systems.",
Two-version based concurrency control and recovery in real-time client/server databases,"While there has been a significant amount of research in real-time concurrency control, little work has been done in logging and recovery for real-time databases. This paper proposes a two-version approach which considers both real-time concurrency control and recovery. We propose a network-server-based architecture and algorithms which can not only reduce the blocking time of higher-priority transactions and improve the response time of client-side read-only transactions, but also provide a diskless runtime logging mechanism and an efficient and predictable recovery procedure. The performance of the algorithms was verified by a series of simulation experiments by comparing the algorithms with the well-known Priority Ceiling Protocol (PCP), the Read/Write PCP, the New PCP, and the 2-version two-phase locking protocol, for which we have very encouraging results. The schedulability of higher-priority transactions and the response time of client-side read-only transactions were all greatly improved.","Concurrency control,
Transaction databases,
Delay,
Frequency,
Access protocols,
Computer science,
Runtime,
Real time systems,
Reliability,
Partitioning algorithms"
Combining classifiers for face recognition,"Current two-dimensional face recognition approaches can obtain a good performance only under constrained environments. However, in the real applications, face appearance changes significantly due to different illumination, pose, and expression. Face recognizers based on different representations of the input face images have different sensitivity to these variations. Therefore, a combination of different face classifiers which can integrate the complementary information should lead to improved classification accuracy. We use the sum rule and RBF-based integration strategies to combine three commonly used face classifiers based on PCA, ICA and LDA representations. Experiments conducted on a face database containing 206 subjects (2,060 face images) show that the proposed classifier combination approaches outperform individual classifiers.","Face recognition,
Principal component analysis,
Linear discriminant analysis,
Fingerprint recognition,
Independent component analysis,
Image databases,
Robustness,
Computer science,
Pattern recognition,
Automation"
Learning to optimize mobile robot navigation based on HTN plans,"High-level symbolic representations of actions to control the working of autonomous robots are used in all hybrid (reactive and deliberative) robot control architectures. Abstract action representations serve several purposes, such as structuring the control code, optimizing the robot performance, and providing a basis for reasoning about future robot action. The paper presents results about re-designing the RHINO navigation system by introducing an HTN plan layer. Besides yielding a more structured robot control software, this layer is used as a basis for optimizing the navigation performance by plan transformations. We show how a robot can learn to select plan transformations based on projections of its intended behavior. Our experimental evaluation shows that the overall robot navigation performance is increased by almost 42 % when using learned projective models to select plan transformations.","Mobile robots,
Navigation,
Robot control,
Computer science,
Software performance,
Terminology,
Programming profession,
Robustness"
ORB middleware evolution for networked embedded systems,"Standards-based COTS (common-off-the-shelf) middleware has been shown to be effective in meeting a range of functional and QoS (quality of service) requirements for distributed real-time and embedded (DRE) systems. Each standard makes limiting assumptions, often implicit, about the fundamental set of system capabilities and constraints typical of the domain to which the standard applies. When the characteristics of a particular class of systems violates a standard's assumptions, it may be appropriate to modify or extent the standard and its conforming implementations to better match the actual characteristics of that class of systems while still exploiting the capabilities of the standard. In this paper, we argue that key assumptions upon which even the more advanced middleware standards are based, e.g., Real-Time CORBA (RT-CORBA), are violated by an important class of DRE systems characterized by the following properties: (1) highly connected networks of (2) numerous memory-constrained endsystems, with (3) stringent timeliness requirements, and (4) support for adaptive reconfiguration of computation and communication elements and their associated timeliness requirements. We describe our recent work on nORB, a small footprint ORB middleware framework for the Boeing Open Experimental Platform (OEP) under the DARPA Nest program, to meet this entire set of requirements by adapting, unifying, and extending patterns and techniques from earlier related research on COTS middleware frameworks, such as UBI-core, ACE, Kokyu, and TAO.",
EMMA: an e-mail management assistant,"In this paper, we describe EMMA (E-Mail Management Assistant), an e-mail system that addresses the process of e-mail management, from initially sorting messages into virtual folders, to prioritizing, reading, replying (automatically or semi-automatically), archiving and deleting mail items. EMMA attains a high degree of accuracy on e-mail classification by using a rule-based approach known as Ripple Down Rules (RDR) as the basis of rule construction. In contrast to traditional rule-based systems, RDR systems provide extensive help to the user in defining rules and maintaining the consistency of a rule base, making EMMA easy to use. We discuss the results of evaluating the usability of EMMA on a trial of independent users.",
Proxy location problems and their generalizations,"In this paper we suggest a new angle to look on proxy location problems. We view the proxy as a service and generalize the problem of optimally placing proxies to optimally placing any service in the network. We differentiate between two cases: transparent services, such as transparent caches (TERCs), and non-transparent services where the end stations, such as hosts using a cache, need to be configured with respect to the service locations. This formulation enables us to study in a unified framework the location problems related to active networks, overlay peer-to-peer networks, as well as, CDNs and caching. For both cases we formalize the problem of optimally placing network servers and introduce approximation algorithms. We present simulation results of approximations and heuristics. We also present an optimal solution for the location problem for a special topology. We show, through a series of examples, that our approach can be applied to a variety of different services.","Costs,
Network servers,
Web server,
Telecommunication traffic,
Traffic control,
Peer to peer computing,
Approximation algorithms,
Network topology,
Computer science,
Authentication"
Improving migration by diversity,"We present an improvement to distributed GAs based on migration of individuals between several concurrently evolving populations. The idea behind our improvement is to not only use the fitness of an individual as criterion for selecting the individuals that migrate, but also to consider the diversity of individuals versus the currently best individual. We experimentally show that a distributed GA using a weighted sum of fitness and a diversity measure for selecting migrating individuals finds the known optimal solutions to benchmark problems from literature (that offer a lot of local optima) on average substantially faster than the distributed GA using only fitness for selection. In addition, the run times of several runs of the distributed GA to the same problem instance vary much less with our improvement than in the base case, thus resulting in a more stable behavior of a distributed GA of this type.","Computer science,
Genetic algorithms,
Content addressable storage,
System performance,
Context-aware services,
Genetic programming,
Cows"
Optimal power allocation for MISO systems and complete characterization of the impact of correlation on the capacity,"We study the optimal transmission strategy of a multiple-input single-output (MISO) wireless communication link. The receiver has perfect channel state information while the transmitter has only long-term channel state information in regard to the channel covariance matrix. It was recently shown that the optimal eigenvectors of the transmit covariance matrix correspond with the eigenvalues of the channel covariance matrix. However, the optimal eigenvalues are difficult to compute. We develop a new characterization of the optimum power allocation. Furthermore, we apply this result to develop a simple algorithm which computes the optimum power allocation. In addition to this, we study the impact of correlation on the ergodic capacity of the MISO system with different channel state information (CSI) schemes. We show that the ergodic capacity with perfect CSI and without CSI at the transmitter is Schur-concave. Additionally, we show that the ergodic capacity with covariance knowledge at the transmitter is Schur-convex with respect to the correlation properties. Finally, we illustrate all theoretical results by numerical simulations.","Covariance matrix,
Channel state information,
Transmitters,
Transmitting antennas,
Wireless communication,
Fading,
Array signal processing,
Symmetric matrices,
Computer science,
Mobile communication"
A model of synchronous collaborative information visualization,"We describe a model of the process by which people solve problems using information visualization systems. The model was based on video analysis of forty dyads who performed information visualization tasks in an experiment. We examined the following variables: focused questions vs. free data discovery, remote vs. collocated collaboration, and systems judged to have high and low transparency. The model describes the stages of reasoning and generating solutions with visual data. We found the model to be fairly robust across task type, collaborative setting, and system type, though subtle differences were found. We propose that system transparency can support some stages of the process, and that support is needed in the last stage to help users translate their findings from visual to written representations.",
A framework for adaptive voice communication over wireless channels,"Time-varying channel error conditions of a wireless link do not allow for providing a steady service quality. Adaptability is one way to go around this problem since the quality of service (QoS) requirements can be changed as the channel status changes. In this paper, we propose a framework for adaptive voice over one-hop wireless communication between a source and a destination. Adaptability is achieved at two levels: (1) changing the voice-encoding rate, and (2) changing the signal modulation scheme. In addition to reacting to the channel conditions and hence achieving better quality, adaptation, will allow for the support of more of the offered load (i.e., higher multiplexing gain). This will introduce some issues and problems; the goal was to reach to a near-optimal operating point, where quality is not traded for bandwidth efficiency. We simulated a special case of the framework. Results show that the adaptive approach has a much less degradation in voice quality (DVQ) and allows for more voice communications to be supported.","Wireless communication,
Delay,
Telecommunication traffic,
Quality of service,
Jitter,
Wireless networks,
Fading,
Computer science,
Computer errors,
Bandwidth"
MIP3S: algorithms for power-conserving multicasting in static wireless ad hoc networks,"In a static ad hoc wireless network, given a distinguished source node, and a subset of nodes called multicast group members, the minimum-energy multicast problem is to assign appropriate power levels to nodes in the network so that all group members are reachable from the source, and that the total power usage is as small as possible. In the centralized version of the problem, one finds such power assignment given the entire network topology. In the distributed version, a power assignment is found by exchanging information between neighboring nodes. In this paper, we proposed new algorithms based on the idea of multicast incremental power with potential power saving (MIP3S). Simulations show that the new algorithms work better than all known algorithms. Different versions of this idea, when made distributive, are of different time and message complexities, imposing an interesting trade-off in total saving in power and other complexity measures.","Multicast algorithms,
Intelligent networks,
Ad hoc networks,
Broadcasting,
Wireless sensor networks,
Mobile ad hoc networks,
Mobile communication,
Computer science,
Power engineering and energy,
Wireless networks"
Fast PNN-based clustering using k-nearest neighbor graph,"Search for nearest neighbor is the main source of computation in most clustering algorithms. We propose the use of nearest neighbor graph for reducing the number of candidates. The number of distance calculations per search can be reduced from O(N) to O(k) or where N is the number of clusters, and k is the number of neighbors in the graph. We apply the proposed scheme within agglomerative clustering algorithm known as the PNN algorithm.","Nearest neighbor searches,
Clustering algorithms,
Vector quantization,
Tree graphs,
Computer science,
Costs,
Distortion measurement,
Iterative algorithms,
Optimization methods,
Mean square error methods"
ReCoNet: modeling and implementation of fault tolerant distributed reconfigurable hardware,"Recent research was mainly focused on the OS support for a single reconfigurable chip. This paper presents a general approach to manage fault tolerant distributed reconfigurable hardware. In order to run such a system, three basic tasks must be implemented: (i) rerouting to compensate line errors, (ii) rebinding to compensate node failures, and (iii) hardware reconfiguration to allow the optimization of these systems during runtime. This paper proposes first ideas and solutions of these management functions. Furthermore, a prototype implementation consisting of four fully connected FPGAs is presented.","Fault tolerance,
Hardware,
Field programmable gate arrays,
Prototypes,
Energy consumption,
Delay,
Computer science,
Operating systems,
Automotive engineering,
Body area networks"
An adaptive rational filter for interpretation of spectrometric data,"The computer-based interpretation of spectrometric data is of great importance for applications of various kinds of spectroscopy in science, biomedical engineering, environmental engineering, and industry. It is based on the use of algorithms of (generalized) deconvolution for correcting spectrometric data, i.e., compensating for the instrumental effects and natural bandwidth effects distorting those data. Among many sophisticated methods currently used for this purpose, no one method is able to properly deal with the irregularities in the data such as the variable shape of peaks of which the interpreted spectrum is composed. In this paper, a flexible and efficient algorithm of generalized deconvolution is proposed, viz. an adaptive rational filter. It is proven to satisfactorily solve this problem by recursive adaptation of its parameters to the widths of the consecutive peaks of which the interpreted spectrum is composed, as well as to the relative distances between peaks along the wavelength axis.","Adaptive filters,
Spectroscopy,
Biomedical engineering,
Deconvolution,
Biomedical computing,
Application software,
Data engineering,
Computer industry,
Instruments,
Bandwidth"
Kernel-based fuzzy clustering incorporating spatial constraints for image segmentation,"The 'kernel method' has attracted great attention with the development of support vector machine (SVM) and has been studied in a general way. In this paper, we present a kernel-based fuzzy clustering algorithm that exploits the spatial contextual information in image data. The algorithm is realized by modifying the objective function in the conventional fuzzy c-means algorithm using a kernel-induced distance metric and a spatial penalty term that takes into account the influence of the neighboring pixels on the centre pixel. Experimental results on both synthetic and real MR images show that the proposed algorithm is more robust to noise than the conventional fuzzy image segmentation algorithms.","Image segmentation,
Kernel,
Clustering algorithms,
Machine learning algorithms,
Support vector machines,
Magnetic noise,
Noise robustness,
Magnetic resonance imaging,
Machine learning,
Computer science"
Constructions of codes from number fields,"We define number-theoretic error-correcting codes based on algebraic number fields, thereby providing a generalization of Chinese remainder codes akin to the generalization of Reed-Solomon codes to algebraic-geometric codes. Our construction is very similar to (and in fact less general than) the one given by Lenstra (1986), but the parallel with the function field case is more apparent, since we only use the non-Archimedean places for the encoding. We prove that over an alphabet size as small as 19 there even exist asymptotically good number field codes of the type we consider. This result is based on the existence of certain number fields that have an infinite class field tower in which some primes of small norm split completely.","Error correction codes,
Encoding,
Poles and towers,
Decoding,
Vectors,
Linear code,
Galois fields,
Algebra,
Australia,
Computer science"
Multiobjective-based concepts to handle constraints in evolutionary algorithms,"This paper presents the main multiobjective optimization concepts that have been used in evolutionary algorithms to handle constraints in global optimization problems. A review of some approaches developed under these concepts is provided. Additionally, a comparison of four representative techniques using well-known test functions is shown. Finally, the analysis of the results obtained, based on three main points (quality, consistency and diversity) and some conclusions and future trends are also provided.","Evolutionary computation,
Constraint optimization,
Algorithm design and analysis,
Benchmark testing,
Computer science,
Linear programming,
Vectors"
Middleware support for multicast-based data dissemination: a working reality,"Multicasting is an effective method to guarantee scalability of data transfer. Multicast applications range from the relief of Internet hot spots to healthcare alert systems. Much research has focused on isolated data management issues that arise in a multicast environment, including our previous work on caching, scheduling, indexing, hybrid schemes, and consistency maintenance. This paper discusses the integration of these research contributions and the transition to a working software distribution that provides the middleware support of a data management layer to applications. Our middleware is flexible, can be shared across applications, and operates on top of existing and upcoming implementations of multicast protocols. The middleware benefits distributed applications with a uniform, efficient, scalable, and state-of-the-art support for critical data management functionality.","Middleware,
Multicast protocols,
Application software,
Satellite broadcasting,
Scalability,
Environmental management,
Network servers,
Computer science,
Indexing,
Multicast algorithms"
Securing your data in agent-based P2P systems,"Peer-to-peer (P2P) technology can be naturally integrated with mobile agent technology in Internet applications, taking advantage of the autonomy, mobility, and efficiency of mobile agents in accessing and processing data. We address the problem of protecting critical information in agent-based P2P Internet applications under two different scenarios. First, we assume the route of a mobile agent in the P2P system is fixed. Under this assumption, we propose the usage of an efficient parallel dispatch model where the agent's route is signcrypted at the first step and dispatched to each new peer to collect information. Then, we assume the route is not specified and we propose the usage of a modified multi-signcryption scheme to guarantee protection. Based on this second approach, a mobile agent determines the next peer to communicate with independently and information is collected dynamically in one round of visiting a group of peers. Security issues under the two proposed models are then discussed.","Mobile agents,
Peer to peer computing,
Data security,
Internet,
Protection,
Information security,
Distributed computing,
Computer science,
Data engineering,
Drives"
Interactive imagination: Tapping the emotions through interactive story for compelling simulations,"Most compassionate parents, teachers, or coaches will echo the belief that you must spark the imagination or touch the heart to teach the mind, train the body, or inspire a sense of wonder. This is also the storyteller's craft. How can we use story within interactive simulations to better teach, train, or inspire? Now that science and technology can make simulations more realistic, how can art make them more compelling through interactive fiction? The key is to use story to tap the depths of emotions, engaging the user's desire for exploration, learning, challenge, and adventure. In the new domain of training, story becomes the means more than the end. Can the compelling art of story transition from the passive media of motion pictures to the nonlinear interactivity of simulation? This is venturing beyond the reactive branching of cause-and-effect games or choose-your-own-ending adventure stories. This process is about the unpredictable expressiveness of audiences exchanging discourse with the author mediated through the digital media - the story engine.","Virtual reality,
Books,
Computational modeling,
Engines,
Space technology,
Investments,
Computer simulation,
Humans,
Computational intelligence,
Layout"
Learning communities: connectivity and dynamics of interacting agents,Intelligent agents need to learn how the communication structure evolves within interacting groups and how to influence the groups overall behavior. We are developing methods to automatically and unobtrusively learn the social network structure that arises within a human group based on wearable sensors. Computational models of group interaction dynamics are derived from data gathered using wearable sensors. The questions we are exploring are: Can we tell who influences whom? Can we quantify this amount of influence? How can we modify group interactions to promote better information diffusion? The goal is real-time learning and modification of social network relationships by applying statistical machine learning techniques to data obtained from unobtrusive wearable sensors.,
Overmodulation strategy for a three-phase four-leg voltage source converter,"A three-phase four-leg voltage source converter (VSC) can produce three line to neutral voltages independently with one additional leg compared with three-phase three-leg VSC system. That is, the system has three-degrees of freedom in producing a d-q-o output voltage arbitrarily within a polyhedron of twelve sides, whose vertices are composed of 14 nonzero voltage switching vectors in the d-q-o reference frame. In this paper a novel overmodulation strategy for three-phase four-leg VSC is proposed. It focuses on the full utilization of the entire capability of the VSC system and simultaneously on the maximization of a d-q component voltage rather than zero sequence voltage. The proposed overmodulation strategy can be implemented by the triangular carrier-based PWM method using an offset voltage concept like a three-phase three-leg VSC system. So it is implemented by limiting four pole voltages with four saturation functions on software without any hardware burden. Finally, to show the feasibility of the proposed overmodulation strategy, simulations and experiments have been performed. The experimental results reveal that the utilization of the entire capability of the four-leg VSC is achieved by the proposed overmodulation strategy.","Voltage,
Power conversion,
Space vector pulse width modulation,
Pulse width modulation,
Pulse width modulation converters,
Leg,
Computer science,
Hardware,
Ellipsoids,
Power distribution"
An integrated performance model of disk arrays,"All enterprise storage systems depend on disk arrays to satisfy their capacity, reliability, and availability requirements. Performance models of disk arrays are useful in understanding the behavior of these storage systems and predicting their performance. We extend prior disk array modeling work by developing an analytical disk array model that incorporates the effects of workload sequentiality, read-ahead caching, write-back caching, and other complex optimizations incorporated into most disk arrays. The model is computationally simple and scales easily, making it potentially useful to performance engineers.","Analytical models,
Computational modeling,
Quality of service,
Performance analysis,
Computer science,
Availability,
Predictive models,
Adaptive arrays,
Prefetching,
Computer simulation"
Hue fields and color curvatures: a perceptual organization approach to color image denoising,"The denoising of color images is an increasingly studied problem whose state-of-the-art solutions employ a variety of diffusion schemes. Specifying the correct diffusion is difficult, however, in part because of the subtleties of color interactions. We address this difficulty by proposing a perceptual organization approach to color denoising based on the principle of good continuation. We exploit the periodic chromatic (hue) component of the color in its representation as a frame field. We derive two hue curvatures and use them to construct a local model for the behavior of the color, which in turn specifies consistency constraints between nearby color measurements. These constraints are then used to replace noisy pixels by examining their spatial context. Such a contextual analysis (combined with standard methods to handle the scalar channels, saturation and lightness), results in a robust noise removal process that preserves discontinuities, singularities, and fine chromatic structures, including those that diffusion processes are prone to distort. We demonstrate our approach on a variety of synthetic and natural images.","Color,
Noise reduction,
Computer science,
Computed tomography,
Colored noise,
Psychology,
Noise robustness,
Diffusion processes,
Computer vision,
Image processing"
Author's reply [to comments on 'A proposal to improve the accuracy of linguistic modelling'],"In our opinion, there are two main concerns in Roubos and Babugka's note, that are summarized as follows. 1) The kinds of problems used in our paper to test the algorithm proposed in ""A proposal to improve the accuracy of linguistic modeling"" and other studies. The authors claim that they are very simple to be considered as benchmarks for nonlinear modeling techniques. 2) The interpretability of the different kinds of models considered. Roubos and Babuska think that there is no difference between the interpretability of fuzzy linguistic models, Takagi-Sugeno-Kang (TSK) fuzzy models, and mathematical formulations (linear models, in this case). We agree with some of the opinions of the authors of the note but not with some others.","Benchmark testing,
Mathematical model,
Fuzzy systems,
Fuzzy neural networks,
Databases,
Nonlinear dynamical systems,
Humans,
Modeling,
Neural networks,
Computer science"
Weighted feature extraction using a genetic algorithm for intrusion detection,"The objective of this paper is to investigate the use of a genetic algorithm for weighted feature extraction with specific application to intrusion detection data. In order to achieve this, we have implemented a simple genetic algorithm which evolves weights for the features of the data set. A k-nearest neighbour classifier was used for the fitness function of the GA as well as to evaluate the performance of the new weighted feature set. The results shown in this paper indicate that evolving a weighted set of features for a particular class of data can provide an increase in intrusion detection accuracy.","Feature extraction,
Genetic algorithms,
Intrusion detection,
Computer networks,
Information science,
Noise level,
Telecommunication traffic,
Information processing,
Information analysis,
Data mining"
Autonomous reactive control for simulated humanoids,"We present a framework for composing motor controllers into autonomous composite reactive behaviors for bipedal robots and autonomous, physically-simulated humanoids. A key contribution of our composition framework is an explicit model of the ""pre-conditions"" under which motor controllers are expected to function properly. Pre-conditions may be determined manually or learned automatically by algorithms based on support vector machine (SVM) learning theory. We demonstrate controller composition and evaluate our composition framework using a family of controllers capable of synthesizing basic actions such a balance, protective stepping when balance is disturbed, protective arm reactions when falling, and multiple ways of regaining an upright stance after a fall.","Automatic control,
Object oriented modeling,
Support vector machines,
Protection,
Legged locomotion,
Computer science,
Motion control,
Humanoid robots,
Machine learning,
Humans"
An innovative low-power high-performance programmable signal processor for digital communications,"We describe an innovative, low-power, high-performance, programmable signal processor (DSP) for digital communications. The architecture of this processor is characterized by its explicit design for low-power implementations, its innovative ability to jointly exploit instruction-level parallelism and data-level parallelism to achieve high performance, its suitability as a target for an optimizing high-level language compiler, and its explicit replacement of hardware resources by compile-time practices. We describe the methodology used in the development of the processor, highlighting the techniques deployed to enable application/architecture/compiler/implementation co-development, and the optimization approach and metric used for power-performance evaluation and tradeoff analysis. We summarize the salient features of the architecture, provide a brief description of the hardware organization, and discuss the compiler techniques used to exercise these features. We also summarize the simulation environment and associated software development tools. Coding examples from two representative kernels in the digital communications domain are also provided. The resulting methodology, architecture, and compiler represent an advance of the state of the art in the area of low-power, domain-specific microprocessors.",
Speed estimation for induction machines using imaginary power,"This paper presents a novel algorithm for sensorless speed estimation for induction machines. The algorithm uses the instantaneous imaginary power flowing into the machine as the basis of the estimation technique. The algorithm is simple and requires minimal parameter knowledge. Furthermore, being based on imaginary power, it does not suffer from accuracy problems at low speeds due to the influence of stator resistance. The paper outlines the algorithm and then presents simulation and experimental results on an induction machine drive system.","Induction machines,
Signal processing algorithms,
Sensor systems and applications,
Stators,
Rotors,
Computer science,
Costs,
Robustness,
Frequency estimation,
Electrical resistance measurement"
"Low-correlation, large linear span sequences from function fields",A general method of generating families of binary sequences with low correlation as well as large linear span is presented. The lower bound on the linear span is on the order of the square root of the period of each sequence within the family. The design makes use of the theory of function fields. Two example applications of this method are presented in which the underlying function fields are the rational and elliptic function fields respectively.,"Polynomials,
Mathematics,
Gold,
Chaos,
Binary sequences,
Random sequences,
Galois fields,
Computer science,
Linear feedback shift registers"
A behaviour coordination manager for a mobile manipulator,"In this paper a behaviour coordination manager for a mobile manipulator is proposed. The manager activates and coordinates purposive perception-action units, so-called behaviours, of a mobile manipulator based on a task provided by a deliberative planning system. As a decision making mechanism for the behaviour coordination Bayesian belief networks are applied. The Bayesian belief networks are trained to learn the influence of the behaviours on the manipulators movement in each situation. After development, training, and testing in a virtual environment, the implemented module is transferred to a real mobile manipulator and evaluated there. Finally, experimental results are presented.","Bayesian methods,
Robot kinematics,
Manipulators,
Management training,
Testing,
Sensor phenomena and characterization,
Computer science,
Mobile computing,
Decision making,
Service robots"
Amplitude and permutation indeterminacies in frequency domain convolved ICA,"In this paper a novel approach to solve the permutation indeterminacy in the separation of convolved mixtures in frequency domain is proposed. A fixed-point algorithm in complex domain to perform the separation of the signals for each frequency domain is used. To obtain the frequency bins a short time Fourier transform on a set of fixed frames, is considered. To solve the ambiguity of the amplitude dilation a simple method is proposed. The permutation indeterminacy is solved using an approach based on the Hungarian algorithm that solves an assignment problem and an algorithm of dynamic programming. To obtain the distances in the assignment problem, a Kullback-Leibler divergence is adopted. We shall see that this approach presents a good performance and permits to obtain a clear separation of the signals.","Frequency domain analysis,
Independent component analysis,
Finite impulse response filter,
Convolution,
Fourier transforms,
IIR filters,
Mathematics,
Computer science,
Deconvolution,
Data models"
Development of a hand-held real-time decision support aid for critical care nursing,"In the current health care environment, nurse clinicians must work ""faster and smarter"" making complex decisions on almost a continual basis. Evidence-based knowledge and standardized guides, such as clinical algorithms, can support clinical nursing decisions, however; effective real-time access is limited. This paper outlines research addressing this problem. In this research, current clinical knowledge is delivered to the clinician via an off-the-shelf handheld computer using wireless access to a central server and data repository. Innovative minimal-set database, data mining and knowledge discovery algorithms using a combination of case based and rule based learning with added confidence measures permitting bi-directional (forward and backwards) inferencing based on individual client data are developed and presented for the hand held device. The technology provides real-time decision support for the multiple cases and sequential decisions characterizing present critical care nursing practice. Nurses will be able to consider a full range of alternative explanations, determine additional data needs, find, isolate and examine patient case outliers for additional diagnostic data or verify the appropriateness of a selected strategy. Once fully developed the system will have the capacity to maintain a history of a series of decisions and outcomes thereby over time improving the case base and rule bases used for decision support. Outcomes of the real time decision support aid include more timely health care, less biased decisions, and improved patient outcomes.","Medical services,
Decision making,
Real time systems,
Handheld computers,
Decision support systems,
Expert systems,
Data mining,
Information processing,
Data engineering,
Databases"
Towards stealthy behaviors,"This paper describes a behavior-based approach to stealth. The specific problem we consider is that of making stealthy traverses, i.e., transiting from one point to another while remaining hidden from an observer. Since we assume that the robot has no a priori model of the environment, our stealthy traverse behavior makes opportunistic use of terrain features to hide from the observer. This behavior has been evaluated in both real and simulated experiments, comparing it against a regular goal-seeking/obstacle-avoidance behavior. These experiments show a clear improvement in the stealthiness of the robot.","Observability,
Orbital robotics,
Robot sensing systems,
Object detection,
Laboratories,
Computer science,
Ray tracing,
Robot control,
Global Positioning System,
Simultaneous localization and mapping"
H/sub /spl infin// control of discrete linear repetitive processes,Repetitive processes are a distinct class of 2D systems (i.e. information propagation in two independent directions) of both systems theoretic and applications interest. They cannot be controlled by direct extension of existing techniques from either standard (termed 1D here) or 2D systems theory. Here we give new results on the relatively open problem of the design of physically based control laws using an H/sub /spl infin// setting. These results are for the sub-class of so-called discrete linear repetitive processes which arise in applications areas such as iterative learning control.,"Iterative algorithms,
Control systems,
Optimal control,
Stability,
Linear systems,
State-space methods,
Control engineering computing,
Computer science,
Systems engineering and theory,
Application software"
Using quantum trajectories and adaptive grids to solve quantum dynamical problems,"Techniques using quantum trajectories and dynamic adaptive grids offer possible solutions to quantum mechanics' hydrodynamical equations, facilitating the design of new computational methods to predict quantum system evolution.","Quantum mechanics,
Quantum computing,
Equations,
Hydrodynamics,
Trajectory,
Fluid dynamics,
Joining processes,
Grid computing,
Monitoring,
Chemicals"
A case for network-centric buffer cache organization,"The emergence of clustered and networked storage architecture gives rise to a new type of server which acts as a data conduit over the network for remotely stored data. These servers, which we call pass-through servers, are mainly responsible for passing the data through them without interpreting it in any way. In this paper, we put forward a scheme of network-centric buffer cache management. This scheme can facilitate the data transmission through pass-through servers by avoiding redundant data copying, and by caching the data in a network-ready form, while having no modifications to the existing buffer cache organization. The performance measurement on an NFS server, using iSCSI storage, running on Linux, with this scheme shows throughput improvement of more than 50% compared to an NFS server on common Linux, while consuming about 40% less CPU resource.",
Empirical probability based QoS routing,"We study the quality-of-service (QoS) schemes that make routing decisions based on empirical resource availability probability information. These empirical probability based routing schemes offer better performance than the traditional schemes that make routing decisions based on resource availability information when the global network state information is imprecise. We investigate variations of empirical probability based QoS routing, present a number of schemes to explicitly maintain the resource availability probability information, and evaluate the performance of the routing schemes. We conclude that the performance of empirical probability based routing is insensitive to the frequency of probability information updates and that empirical probability based routing can achieve good performance without introducing excessive overheads.","Routing,
Availability,
Bandwidth,
Quality of service,
Frequency,
Computer science,
Propagation delay,
Network topology,
Probability distribution,
Performance evaluation"
Using media processors for low-memory AES implementation,"Most performance studies of AES make traditional space versus time tradeoffs by allowing large lookup tables to accelerate operations that would normally be calculated by the processor. However, AES is a versatile algorithm and can also be optimised for low-memory use in constrained environments. We investigate the possibility of getting the best of both worlds - an application specific hardware and software solution that has a low dependency on memory yet still executes fast enough to consider for use in production systems. The resulting software is attractive in high level design since it allows AES to be more easily deployed as a composable element in larger systems and scale better as processor speed increases.","Video sharing,
Hardware,
Acceleration,
Resource management,
Computer science,
Table lookup,
Constraint optimization,
Application software,
Production systems,
Software systems"
On using ZENTURIO for performance and parameter studies on cluster and Grid architectures,"Over the last decade, a dramatic increase has been observed in the need for generating and organising data in the course of large parameter studies, performance analysis, and software testing. We have developed the ZENTURIO experiment management tool for performance and parameter studies on cluster and Grid architectures. In this paper we describe our experience with ZENTURIO for performance and parameter studies of a material science kernel, a three-dimensional particle-in-cell simulation, a fast Fourier transform, and a financial modeling application. Experiments have been conducted on an SMP cluster with Fast Ethernet and Myrinet communication networks, using PBS (Portable-Batch System) and GRAM (Globus Resource Allocation Manager) as job managers.","Computer architecture,
Performance analysis,
Resource management,
Data visualization,
Software testing,
Application software,
Measurement,
Software performance,
Mathematics,
Photonics"
Time-aware utility-based QoS optimization,"This paper presents a time-aware admission control and resource allocation scheme in the context of a future generation mobile network. The quality levels (and their respective utility) of the different connections are specified using discrete resource-utility (R-U) functions. The scheme uses these R-U functions for allocating and reallocating bandwidth to connections, aiming to maximize the accumulated utility of the system. However, different applications react differently to resource reallocations Therefore at each allocation timepoint we take into account the following factors: the age of the connection, a drop (disconnection) penalty and the sensitiveness to reallocation frequency. Finally, we show the superior performance of our approach compared to a recent adaptive bandwidth allocation scheme.","Bandwidth,
Resource management,
Quality of service,
Admission control,
Channel allocation,
Availability,
Degradation,
Switches,
Computer networks,
Information science"
On conditions for self-healing in distributed software systems,"This paper attempts to identify one of the necessary conditions for self-healing, or self-repair, in complex systems, and to propose means for satisfying this condition in heterogeneous distributed software. The condition identified here is the following: For a system with a wide and open range of possible configurations to be self healing, it must possess suitable regularities, which can be relied upon to be satisfied by all possible configurations of the system, and which must be invariant of its failures. We observe that self-healing in physical artifacts, as well as in biological systems, are largely based on regularities engendered by the laws of nature. But since laws of nature have no effective sway over the behavior of software, we propose means for imposing artificial laws over a given distributed system, which are designed to induce desired regularities in them. We demonstrate the efficacy of the proposed approach by applying it to a simple example of electronic purchasing in enterprise systems.",
A tool-mediated cognitive apprenticeship approach for a computer engineering course,Teaching database engineers involves a variety of learning activities. A strong focus is on practical problems that go beyond the acquisition of knowledge. Skills and experience are equally important. We propose a virtual apprenticeship model for the knowledge- and skills-oriented Web-based education of database students. We adapt the classical cognitive apprenticeship theory to the Web context utilising scaffolding and activity theory. The choice of educational media and the forms of student interaction with the media are central success criteria.,"Data engineering,
Computer applications,
Computer science education,
Programming profession,
Knowledge engineering,
Mediation,
Software tools,
Database systems,
Spine,
Collaboration"
Coupling on-line and off-line profile information to improve program performance,"In this paper we describe a novel execution environment for Java programs that substantially improves execution performance by incorporating both on-line and off-line profile information to guide dynamic optimization. By using both types of profile collection techniques, we are able to exploit the strengths of each constituent approach: profile accuracy and low overhead. Such coupling also reduces the negative impact of these approaches when each is used in isolation. On-line profiling introduces overhead for dynamic instrumentation, measurement, and decision making. Off-line profile information can be inaccurate when program inputs for execution and optimization differ from those used for profiling. To combat these drawbacks and to achieve the benefits from both online and off-line profiling, we developed a dynamic compilation system (based on JikesRVM) that makes use of both. As a result, we are able improve Java program performance by 9% on average, for the programs studied.","Java,
Virtual machining,
Instruments,
Dynamic compiler,
Degradation,
Adaptive systems,
Instrumentation and measurement,
Computer science,
Decision making,
Internet"
Synchronous Consensus for dependent process failures,"We present a new abstraction to replace the t of n assumption used in designing fault-tolerant algorithms. This abstraction models dependent process failures yet it is as simple to use as the t of n assumption. To illustrate this abstraction, we consider Consensus for synchronous systems with both crash and arbitrary process failures. By considering failure correlations, we are able to reduce latency and enable the solution of Consensus for system configurations in which it is not possible when forced to use algorithms designed under the t of n assumption. We show that, in general, the number of rounds required in the worst case when assuming crash failures is different from the number of rounds required when assuming arbitrary failures. This is in contrast with the traditional result under the t of n assumption.","Algorithm design and analysis,
Computer crashes,
Fault tolerance,
Fault tolerant systems,
Failure analysis,
Computer science,
Design engineering,
Drives,
Delay,
Distributed algorithms"
Sensitivity of software usage to changes in the operational profile,"In this paper we present a methodology for uncertainty analysis of the software operational profile suitable for large complex component-based applications and applicable throughout the software life cycle. Within this methodology, we develop a method for studying the sensitivity of software usage to changes in the operational profile based on perturbation theory. This method is then illustrated on three case studies: software developed for the European Space Agency, an e-commerce application, and real-time control software. Results show that components with small execution rates are the most sensitive to the changes in the operational profile. This observation is very important due to the fact that rarely executed components usually handle critical functionalities such as exception handling or recovery.","Software reliability,
Uncertainty,
Application software,
Frequency estimation,
Software systems,
Computer science,
Software quality,
Aerospace control,
Space vehicles,
Process control"
Improving genetic classifiers with a boosting algorithm,"We present a boosting genetic algorithm for classification rule discovery. The method is based on the iterative rule learning approach to genetic classifiers. The boosting mechanism increases the weight of those training instances that are not classified correctly by the new rules, so that in the next iteration the algorithm focuses the search on those rules that capture the misclassified or uncovered instances. We show that the boosted genetic classifier has higher accuracy for prediction, or from an alternative and perhaps more important perspective, uses less computational resources for similar accuracy, than the original genetic classifier.","Boosting,
Genetic algorithms,
Iterative algorithms,
Learning systems,
Voting,
Computer science,
Iterative methods,
Accuracy,
Predictive models,
Classification tree analysis"
Realizing personality in audio-visually triggered non-verbal behaviors,"Controlling robot behaviors becomes more important recently as active perception for robot, in particular active audition in addition to active vision, has made remarkable progress. We are studying how to create social humanoids that perform actions empowered by real-time audio-visual tracking of multiple talkers. In this paper, we present personality as means of controlling on-verbal behaviors. It consists of two dimensions, dominance vs. submissiveness and friendliness vs. hostility, based on the interpersonal theory in psychology. The upper-torso humanoid SIG equipped with real-time audio-visual multiple-talker tracking system is used as a testbed for social interaction. As a companion robot, with friendly personality, it turns toward new sound source in order to show its attention, while with hostile personality, it turns away a new sound source. As a receptionist robot with dominant personality, it focuses its attention on the current customer, while with submissive personality, its attention to the current customer is interrupted by a new one.","Humanoid robots,
Human robot interaction,
Intelligent sensors,
Acoustic sensors,
Informatics,
Symbiosis,
Computer science,
Laboratories,
Psychology,
Real time systems"
Initial experiences of ALAN-K: an Advanced LeArning Network in Kyoto,"This paper describes the start-up phase of the ALAN-K (Advanced LeArning Network in Kyoto) project, which is part of a major effort to create new learning environments for elementary, junior high and high school students in Kyoto, Japan. Our visions of digitally fluent and creative citizens are largely influenced by the visions, ideas, and experiences of Alan Kay and his colleagues. We view the Squeak system (Ingalls et al., 1997; Guzdial and Rose, 2002) as powerful learning tools and media for enabling both active independent learning and collaboration. A series of Squeak workshops were designed and conducted at two elementary schools, which will be the basis for designing further activities.","Intelligent networks,
Application software,
Educational institutions,
Collaborative tools,
Informatics,
Collaborative work,
Computer science education,
Cities and towns,
Costs,
Information technology"
Source Viewer 3D (sv3D) - a framework for software visualization,"Source Viewer 3D is a software visualization framework that uses a 3D metaphor to represent software system and analysis data. The 3D representation is based on the SeeSoft pixel metaphor. It extends the original metaphor by rendering the visualization in a 3D space. New, object-based manipulation methods and simultaneous alternative mappings are available to the user.",
Self-awareness and adaptivity for quality of service,"Network self-awareness is the ability of a network to observe its own behavior using internal probing and measurement mechanisms, and to make effective autonomous use of these observations for self-management. Experiments are conducted to evaluate the goal's impact on observed QoS for the user's payload. In addition to packet loss due to congestion, we also introduce an artificial packet loss at certain nodes to represent failures or other undesirable events. We see that just using delay in the QoS goal is a good way to reduce delay and loss if losses are only the result of congestion. However, as one would expect, using loss in the user's QoS goal is seen to be useful if the paths, which are selected by SPs, are to avoid nodes where packet losses are occurring for reasons other than congestion. In general we see good correlation between the QoS goal that the SPs use to find paths, and the resulting QoS observed by DPs.","Quality of service,
Payloads,
Routing,
Testing,
Delay estimation,
Computer science,
Electric variables measurement,
Telecommunication traffic,
Packet switching,
Learning"
ARP considered harmful: manycast transactions in ad hoc networks,"ARP handles neighbor discovery and address resolution in infrastructure networks, but is inadequate for mobile ad hoc networks (MANETs). Thus, many MANET routing protocols include a neighbor discovery mechanism. This separation of neighbor discovery and address resolution is a fundamental design problem that causes packet loss, particularly when the communication is manycast, a novel variant of multicast communication. Our approach, automatic address resolution, moves address resolution into the routing protocol along with neighbor discovery, correcting these problems.","Intelligent networks,
Ad hoc networks,
Mobile ad hoc networks,
Routing protocols,
Network topology,
Network servers,
Computer science,
Mobile communication,
Multicast communication,
Frequency"
THMR-V: an effective and robust high-speed system in structured road,"Many systems have been developed which can drive autonomously in structured road, but little works have been reported that describe the specific system solution in a high-speed (more than 120 km/h) situation. This paper presents THMR-V (Tsinghua mobile robot V), a system that performs well with a speed up to 150 km/h. Multi-region and bi-threshold edge extraction is used for image processing and an ATN-based approach is used for lane line location.","Robustness,
Control systems,
Roads,
Pixel,
Computer science,
Mobile robots,
Intelligent robots,
Remotely operated vehicles,
Drives,
Robot kinematics"
Regression testing for Web applications based on slicing,"Web applications have rapid developing speed and changeable user demands, so the regression testing is much important. Since the changed demands result in different versions of Web applications, and the faults usually hiding in the adjusted contents, the regression testing must cover all the related pages. In order to carry through the regression testing quickly and effectively, we make the simplification with the method of slicing. Firstly, we analyze the possible changes in the Web applications and the influences produced by these changes, discussing in the direct-dependent and indirect-dependent way; next, we give the regression testing method based on slicing emphasized on the indirect-dependent among data, i.e., obtaining the dependent set of changed variables by forward and backward search method and generating the testing suits; conclusion remarks and future work are given at last.","Application software,
Software testing,
System testing,
Computer science,
Search methods,
Software quality,
Usability,
Computer science education,
Educational programs,
Laboratories"
The sub-key theorem on credibility measure space,"In 1970s, Vladimir N. Vapnik proposed statistical learning theory. The theory is considered as optimum theory on small samples statistical estimation and prediction learning. It has more systematically investigated the rational conditions of the empirical risk minimization discipline and the relations between the empirical risk and the expected risk on finite samples. In fact, the key theorem of learning theory plays an important role in statistical learning theory. Its importance results in paving the way for the subsequent theories and applications. However, some theories and definitions only suit to fixed probability measure. These restricted conditions reduce the applied range of theorem. In this paper, we will generalize the applied range by means of changing the probability measure space into credibility measure space. In new measure space, we give new concepts and new theorem on classical theoretical foundation.","Extraterrestrial measurements,
Statistical learning,
Risk management,
Probability,
Convergence,
Machine learning,
Mathematics,
Computer science,
Sufficient conditions,
Maximum likelihood estimation"
Visually lossless adaptive compression of medical images,"A method for encoding medical images to visually lossless quality is proposed. Visually lossless coding refers to the compression of images without evoking any perceptible degradation in image quality through the suppression of visible distortions. The proposed coding structure is based on the set partition in hierarchical trees (SPIHT) coder (A. Said et al., June 1996). Added to this is an adaptive vision model designed to identify and removal visually irrelevant information in images prior to encoding. Simulation results of the proposed coder have shown compression improvements over the LOCO (M. Weinberger et al., 1996) coder without any deterioration in image fidelity.","Image coding,
Biomedical imaging,
Medical diagnostic imaging,
Frequency,
Ultrasonic imaging,
Telemedicine,
Humans,
Neurons,
Image decomposition,
Computer science"
Fine-grained device management in an interactive media server,"The use of interactive media has already gained considerable popularity. Interactivity gives viewers VCR controls like slow-motion, pause, fast-forward, and instant replay. However, traditional server-based or client-based approaches for supporting interactivity either consume too much network bandwidth or require large client buffering; and hence they are economically unattractive. We propose the architecture and design of an interactive media proxy (IMP) server that transforms noninteractive broadcast or multicast streams into interactive ones for servicing a large number of end users. For IMP to work cost-effectively, it must carefully manage its storage devices, which are needed for caching voluminous media data. In this regard, we propose a fine-grained device management strategy consisting of three complementary components: disk profiler, data placement, and IO scheduler. Through quantitative analysis and experiments, we show that these fine-grained strategies considerably improve device throughput under various workload scenarios.","Network servers,
Streaming media,
Video on demand,
Bandwidth,
Interactive systems,
Video recording,
Web server,
TV broadcasting,
Computer science,
Throughput"
Minimizing the number of one-paths in BDDs by an evolutionary algorithm,"Ordered binary decision diagrams (BDDs) are used in VLSI CAD, especially for the canonical representation of Boolean functions. In the last decade, the method of choice for optimizing this data structure was minimizing the number of nodes in the associated graph. However, recent works have shown that the number of paths is also important. In this work, minimizing the number of one-paths of BDDs is accomplished by an evolutionary algorithm (EA) acting on the permutation of variables. The optimal operator weights for the EA were determined by a parameter study. Experimental results demonstrate the efficiency of our approach.","Data structures,
Boolean functions,
Evolutionary computation,
Binary decision diagrams,
Very large scale integration,
Computer science,
Optimization methods,
Formal verification,
Genetics,
Embedded system"
A data-mining approach for optimizing performance of an incremental crawler,"Crawlers visit the Web to maintain a local repository of Web pages up to date. We introduce another perspective to build an effective incremental crawler. Based on previous work in this field, we study how we can improve the performance of a crawler using data-mining. The information collected from the users can help the crawler to know which are the popular pages and to revisit them as soon as possible.","Crawlers,
Search engines,
Uniform resource locators,
Web pages,
Computer science,
Databases,
Data analysis,
Bandwidth,
World Wide Web,
Data mining"
A lattice problem in quantum NP,"We consider coGapSVP/sub /spl radic/n/, a gap version of the shortest vector in a lattice problem. This problem is known to be in AM /spl cap/ coNP but is not known to be in NP or in MA. We prove that it lies inside QMA, the quantum analogue of NP. This is the first non-trivial upper bound on the quantum complexity of a lattice problem. The proof relies on two novel ideas. First, we give a new characterization of QMA, called QMA+ formulation allows us to circumvent a problem which arises commonly in the context of QMA: the prover might use entanglement between different copies of the same state in order to cheat. The second idea involves using estimations of autocorrelation functions for verification. We make the important observation that autocorrelation functions are positive definite functions and using properties of such functions we severely restrict the prover's possibility to cheat. We hope that these ideas will lead to further developments in the field.","Lattices,
Quantum computing,
Upper bound,
Autocorrelation,
Polynomials,
Computer science,
Cryptography,
Algorithm design and analysis,
Quantum mechanics"
Design and implementation of home automated telemanagement in chronic obstructive pulmonary disease,"The Internet-based home automated telemanagement (HAT) system has been designed to support a multi-disciplinary approach in patient self-management which includes regular patient assessment, disease-specific education, control of patient compliance with treatment plans, implementation of health behavior change models and social support both for patients and caregivers. The goals of this project were: (1) To refine the HAT program to fully implement the multidisciplinary model for telemanagement of COPD patients; (2) To evaluate in a pilot study feasibility and acceptance of HAT by COPD patients. The COPD HAT interface can be operated by a mouse, keyboard or voice. Specific features have been introduced into presentation and navigation of the HAT home unit to accommodate for possible limitations in vision, motion or cognition by the potential HAT users. Feasibility evaluation showed that the COPD patients were able to use the COPD HAT system successfully regardless of their age, education or prior experience in using computers.","Diseases,
Internet,
Educational programs,
Automatic control,
Medical treatment,
Mice,
Keyboards,
Navigation,
Cognition,
Computer science education"
Utility-based multiagent coalition formation with incomplete information and time constraints,"In this paper we propose a coalition formation model for a cooperative multiagent system in which an agent forms sub-optimal coalitions in view of incomplete information about its noisy, dynamic, and uncertain world, and its need to respond to events within time constraints. Our model has two stages: (1) when an agent detects an event in the world, it first compiles a list of coalition candidates that it thinks would be useful (coalition initialization), and (2) then negotiates with the candidates (coalition finalization). A negotiation is an exchange of information and knowledge for constraint satisfaction until both parties agree on a deal or one opts out. Each successful negotiation adds a new member to the agent's final coalition. This paper talks about the steps we have designed to enhance the finalization stage.","Time factors,
Working environment noise,
Uncertainty,
Computer science,
Collaboration,
Cost function,
Signal to noise ratio,
Communication channels,
Waste materials,
Event detection"
Modelling: a neglected feature in the software engineering curriculum,"This paper argues that the concept of modelling, and particularly of software system structures, is not being given sufficient attention within current sources that describe aspects of the software engineering curriculum. The paper describes the scope of modelling as a general concept, and explains the role that the modelling of software system structures plays within it. It discusses the treatment of this role within the various sources, and compares this both with the experience of the role that such modelling plays in the undergraduate curriculum at Sheffield University, and with the practice in other branches of engineering. The idea is examined that modelling should be treated as a recurring concept within the curriculum, and it is shown that this gives rise to a matrix structure for the software engineering curriculum. The paper discusses how such a structure can be mapped into a conventional hierarchical curriculum model, and the relationships that need to be made explicit in doing so. It describes the practical implications of these results for the structures of degree programmes in software engineering.","Software engineering,
Educational programs,
Guidelines,
Computer science,
Software systems,
Curriculum development,
Computer science education,
Educational products,
Software architecture,
Programming"
Performance evaluation of two emerging media processors: VIRAM and Imagine,"This work presets two emerging media microprocessors, VIRAM and Imagine, and compares the implementation strategies and performance results of these unique architectures. VIRAM is a complete system on a chip which uses PIM technology to combine vector processing with embedded DRAM. Imagine is a programmable streaming architecture with a specialized memory hierarchy designed for computationally intensive data-parallel codes. First, we preset a simple and effective approach for understanding and optimizing vector/stream applications. Performance results are then presented from a number of multimedia benchmarks and a computationally intensive scientific kernel. We explore the complex interactions between programming paradigms, the architectural support at the ISA level and the underlying microarchitecture of these two systems. Our long term goal is to evaluate leading media microprocessors as possible building blocks for future high performance systems.","Streaming media,
Computer architecture,
Computer science,
Instruction sets,
Microarchitecture,
Signal processing algorithms,
Kernel,
Parallel processing,
Delay,
Multimedia systems"
Profit-driven service differentiation in transient environments,"We focus on service differentiation policies for Web servers where clients have different QoS requirements and the environment has large fluctuations in the client arrival and service patterns and in the QoS class mix. We propose an adaptive admission control mechanism that offers service differentiation among client classes and maximizes the effectiveness of the server's operation. Using actual traces from the 1998 World Cup web site, we conduct a detailed analysis of the proposed policy and show that it meets the performance challenges of a robust QoS policy.","Web server,
Robustness,
Time measurement,
Delay,
Admission control,
Performance analysis,
Analytical models,
Cryptography,
Computer science,
Educational institutions"
"Comments on ""New method of performance analysis for diversity reception with correlated Rayleigh-fading signals""","The purpose of this paper is to provide a critical discussion of the new method of performance analysis of diversity systems proposed in the abovementioned paper (for original paper see Fang et al., ibid., vol. 49, p. 1807-1812 (2000)). It is shown that this method provides incorrect results for equal-gain and selective combining.",
An efficient index-based protein structure database searching method,"In this paper, we present a novel indexing method called ProtDex to facilitate fast searching in 3-dimensional protein structure database. In ProtDex, we first build an index on the representative properties of all proteins in the database. When evaluating a query, with the help of the index, we filter out a small candidate list of proteins. Then, we can either directly report them, with their respective rankings, to the user, or do the expensive actual alignments on them upon user's request. Preliminary experimental results show that our solution is up to 16 times faster than the popular DALI method for database searching task (without actual alignments), while its overall accuracy is only slightly inferior to that of DALI. The software is available upon request by sending emails to the authors.","Proteins,
Databases,
Polymers,
Indexes,
Shape,
Biology computing,
Computer science,
Drives,
Genomics,
Bioinformatics"
An embedded merging scheme for H.264/AVC motion estimation,"The emerging H.264/AVC video coding standard takes 50% performance improvement than the H.263. One of the key successful factors is to employee the variable block size matching algorithm in the motion compensation stage. However, the highly computational complexity leads the codec become too complex and hard to implement in some real time applications. Again, large memory requirement to save the middle results limits its usage, too. In this paper, we proposed an embedded merging scheme for fast full search algorithm of H.264/AVC. The proposed scheme cannot only reduce the computation complexity but also provides a better way to be implemented in VLSI hardware.","Merging,
Automatic voltage control,
Motion estimation,
Computational complexity,
Motion compensation,
Very large scale integration,
Hardware,
Computer science,
Video coding,
Codecs"
Bluetooth-based ad-hoc networks for voice transmission,"In this paper we inspect the possibilities of using Bluetooth for building ad-hoc networks suitable for transmitting audio and esp. voice data using synchronous SCO links. We analyze the features or problems that Bluetooth offers for transmitting audio data in a multihop network. As the existing MANET routing protocols that emerged out of the work of the IETF MANET WG (like AODV, DSR etc.) cannot be directly used to work with Bluetooth, we present a new routing protocol called Bluetooth scatternet routing (BSR) that is influenced by other MANET routing protocols but pays special attention to the restrictions of Bluetooth (like number of connections, connection setup times etc.). The protocol is also inspired by the channel switching concept of ATM. Some initial results of simulations and real-life tests give an impression of the performance and efficiency this protocol can reach in an application scenario.","Ad hoc networks,
Bluetooth,
Mobile ad hoc networks,
Routing protocols,
Cellular phones,
Scattering,
Testing,
Personal digital assistants,
Human computer interaction,
Multimedia computing"
ARWin - a desktop augmented reality Window Manager,"We present ARWin, a single user 3D augmented reality desktop. We explain our design considerations and system architecture and discuss a variety of applications and interaction techniques designed to take advantage of this new platform.","Augmented reality,
Application software,
Computer displays,
Cameras,
Clocks,
Computer architecture,
Keyboards,
Mice,
Computer science,
Computer graphics"
Is software engineering training enough for software engineers?,"Most software engineering courses focus exclusively on the software development process, often referring to problems related to the complexity of software products and processes. In practice, however, many problems of a complex nature arise in which system engineering and other engineering disciplines are important in the development of systems. In such cases software engineers may have difficulty in coping with the entire problem, in the same way that engineers in other fields may have difficulty in understanding the software part. This suggests that the software engineering education of today is inadequate in certain respects. This paper presents a case study of a software engineering course and discusses the difficulty for computer science students to understand and to develop a system which also requires skills in engineering of a non-software nature.","Software engineering,
Computer science,
Knowledge engineering,
Programming,
Systems engineering and theory,
Computer science education,
Engineering management,
Management training,
Design engineering,
Software design"
Using a distributed snapshot algorithm in wireless sensor networks,"Wireless Sensor Networks (WSNs) have particular characteristics that do not allow to apply traditional distributed algorithms directly to them. In this work we adapt the algorithms Distributed Snapshot, Broadcast and Propagation of Information with Feedback (PIF) to WSNs and apply them to generate the Energy Map of a WSN. This map shows the behavior of such a network and can be used to predict its behavior. We simulate the algorithms proposed and show their number of messages, energy spent and execution time.","Intelligent networks,
Wireless sensor networks,
Chemical sensors,
Temperature sensors,
Batteries,
Distributed algorithms,
Broadcasting,
Wireless communication,
Distributed computing,
Computer science"
Runtime support for a dynamically composable and adaptive wearable system,,"Runtime,
Adaptive systems,
Computer science,
Portable computers,
Wearable computers,
Home appliances,
Space technology,
Pervasive computing,
Physics computing,
Power system reliability"
TreeRank: a similarity measure for nearest neighbor searching in phylogenetic databases,"Phylogenetic trees are unordered labeled trees in which each leaf node has a label and the order among siblings is unimportant. In this paper we propose a new similarity measure, called TreeRank, for phylogenetic trees and present an algorithm for computing TreeRank scores. Given a query or pattern tree P and a data tree D, the TreeRank score from P to D is a measure of the topological relationships in P that are found to be the same or similar in D. The proposed algorithm calculates the TreeRank score in O(M/sup 2/ + N) time where M is the number of nodes appearing in both P and D, and N is the number of nodes in D. We then develop a search engine that, given a query or pattern tree P and a database of trees D, finds and ranks the nearest neighbors of P in D where the ""nearness"" is measured by the proposed similarity function. This structure-based search engine is fully operational and is available on the World Wide Web.","Nearest neighbor searches,
Phylogeny,
Databases,
Search engines,
Information retrieval,
Web sites,
Educational institutions,
Computer science,
Biology,
Data analysis"
Hidden mode HMM using Bayesian network for modeling speaking rate fluctuation,"One of the most important issues in spontaneous speech recognition is how to cope with the degradation of recognition accuracy due to speaking rate fluctuation within an utterance. This paper proposes an acoustic model for adjusting mixture weights and transition probabilities of the HMM for each frame according to the local speaking rate. The proposed model is implemented along with variants and conventional models using the Bayesian network framework. The proposed model has a hidden variable representing variation of the ""mode"" of the speaking rate and its value controls the parameters of the underlying HMM. Model training and maximum probability assignment of the variables are conducted using the EM/GEM and inference algorithms for Bayesian networks. Utterances from meetings and lectures are used for evaluation where Bayesian network-based acoustic models are used to rescore the utterance hypotheses obtained from a first-pass N-best list. In the experiments, the proposed model shows consistently higher performance than conventional models.","Hidden Markov models,
Bayesian methods,
Fluctuations,
Speech recognition,
Computer science,
Degradation,
Inference algorithms,
Text recognition,
Acoustic testing,
Decoding"
A scalable approach to human-robot interaction,"Much of the current research in human-robot interaction is concerned with single systems and single or few users. These systems and their interfaces are generally tightly-coupled and well-defined. For large-scale human-robot applications, the systems may be unknown prior to designing the interface for potential human interaction. This presents a difficult goal for allowing multiple users to interact with many possibly unknown systems. In this paper, we present an interaction infrastructure aligned with providing this interface. It operates in two phases that accommodate both many-to-many interaction and generalized, one-to-one interaction between users and robotic systems. Our previous research has demonstrated the infrastructure to scale to a large number of users and several systems in simulation. The experiments in this paper substantiate these results in a smaller-scale real robotic environment.","Human robot interaction,
Scalability,
Large-scale systems,
Intelligent robots,
Embedded system,
Application software,
Joining processes,
Intelligent systems,
Computer science,
Robot control"
Rapid verification of embedded systems using patterns,"Verification pattern (VP) is a new technique to test embedded systems rapidly, and it has been used to test industrial safety-critical embedded systems successfully. The key concept of this approach is to classify system scenarios into patterns, and use the same code template to test all the scenarios of the same pattern. In this way, testing effort can be greatly reduced. This paper extends VPs so that they can fully interoperate with a formalized scenario model ACDATE. In this way, various static and dynamic analyses can be performed on system scenarios as well as on system patterns. Furthermore, this paper provides a mapping from system scenarios into temporal logic expressions. In this way, a practitioner can specify system constraints in scenarios, and follow the mapping to obtain the temporal logic expressions easily to perform formal model checking. This paper also provides an OO framework to support automated test script development from VPs. In this way, VPs can be used in an integrated process where both semi-formal analyses and formal techniques can be used together to develop mission-critical embedded applications.","Embedded system,
System testing,
Logic,
Object oriented modeling,
Computer science,
Computer industry,
Defense industry,
Pattern analysis,
Performance analysis,
Automatic testing"
Fast retrieval of similar configurations,"Configuration similarity is a special form of content-based image retrieval that considers relative object locations. It can be used as a standalone method, or to complement retrieval based on visual or semantic features. The corresponding queries ask for sets of objects that satisfy some spatio-temporal constraints, e.g., ""find all triplets of objects (v/sub 1/, v/sub 2/, v/sub 3/), such that v/sub 1/ is northeast of v/sub 2/, which is inside v/sub 3/."" Exhaustive processing (i.e., retrieval of the best solutions) of configuration similarity queries, in general, has exponential complexity and fast search for sub-optimal solutions is the only way to deal with the vast amounts of multimedia information in several real-time applications. In this paper we first discuss the utilization of nonsystematic search heuristics, based on genetic algorithms, simulated annealing and hill climbing approaches. An extensive experimentation with real and synthetic datasets reveals that hill climbing techniques are the best for the current problem; therefore, as a subsequent step we study the search space, and develop improved variations of hill climbing that take advantage of the special structure of the problem to enhance speed. The proposed heuristic methods significantly outperform systematic search when there is only limited time for query processing.","Content based retrieval,
Information retrieval,
Computer science,
Image retrieval,
Genetic algorithms,
Simulated annealing,
Query processing,
World Wide Web,
Prototypes"
Collaborative intrusion detection system,"This paper presents an intrusion detection system consisting of multiple intelligent agents. Each agent uses a self-organizing map (SOM) in order to detect intrusive activities on a computer network. A blackboard mechanism is used for the aggregation of results generated from such agents (i.e. a group decision). In addition, this system is capable of reinforcement learning with the reinforcement signal generated within the blackboard and then distributed over all agents which are involved in the group decision making. Systems with various configurations of agents are evaluated for criteria such as speed, accuracy, and consistency. The results indicate an increase in classification accuracy as well as in its constancy as more sensors are incorporated. Currently this system is primarily tested on the data set for KDD Cup '99.","Collaboration,
Intrusion detection,
Data security,
Computer science,
Computer networks,
Frequency estimation,
Velocity measurement,
Intelligent agent,
Learning,
Signal generators"
A case study of integrating knowledge management into the supply chain management process,"To achieve success at supply chain management (SCM), an organization must possess- and share-knowledge about the different facets of the supply chain. Lack of information sharing between members of the supply chain has been shown to significantly affect total profitability. As such, we argue that knowledge management (KM) can enhance the degree of success of existing SCM efforts as well as increase the likelihood of success of new SCM undertakings. While many SCM projects have resulted in improved performance, we believe that additional improvement in performance is possible by coupling KM initiatives with SCM programs. We present a case study of the service parts department of a large automotive dealer that examines the effect of manufacturers' trade promotions on the coordination of decisions in the supply chain. The use of KM to enhance the ordering process for service parts is explored in this paper.","Computer aided software engineering,
Knowledge management,
Supply chain management,
Supply chains,
Automotive engineering,
Costs,
Inventory management,
Manufacturing,
Production,
Transportation"
A configurable middleware framework with multiple quality of service properties for small embedded systems,"Embedded systems have become commonplace in recent years, and are increasingly being networked Middleware offers many advantages to the distributed application programmer, yet there exist very few middleware frameworks for the low end of the embedded systems market. In this paper we describe MicroQoSCORBA. It represents a fundamental, bottom-up rethinking of what middleware can and should support for resource-constrained devices. This middleware is tailorable, with a fine degree of granularity, to both the device and the application program's constraints. We describe the multiple Quality of Service domains that MicroQoSCORBA supports, and present an evaluation of our working framework.","Middleware,
Quality of service,
Embedded system,
Fault tolerance,
Hardware,
Security,
Computer science,
Application software,
Programming profession,
Embedded computing"
Object-oriented middleware infrastructure for distributed augmented reality,"The paper describes design and implementation of software infrastructure for building augmented reality applications for ubiquitous computing environments. Augmented reality is one of the most important techniques to achieve the vision of ubiquitous computing. Traditional toolkits for augmented reality provide the high level abstraction that makes it easy to build augmented reality applications. However, the applications programmers need to contemplate distribution and context-awareness that make the development of applications very hard, but they are necessary to build ubiquitous computing environments. Our infrastructure provides the high level abstraction and hides distribution and context-awareness from programmers. Therefore, the cost to develop augmented reality applications will be reduced dramatically by using our middleware infrastructure.","Middleware,
Augmented reality,
Application software,
Ubiquitous computing,
Pervasive computing,
Programming profession,
Buildings,
Costs,
Computer networks,
Computer science"
Amplifying Head Movements with Head-Mounted Displays,"The head-mounted display (HMD) is a popular form of virtual display due to its ability to immerse users visually in virtual environments (VEs). Unfortunately, the user's virtual experience is compromised by the narrow field of view (FOV) it affords, which is less than half that of normal human vision. This paper explores a solution to some of the problems caused by the narrow FOV by amplifying the head movement made by the user when wearing an HMD, so that the view direction changes by a greater amount in the virtual world than it does in the real world. Tests conducted on the technique show a significant improvement in performance on a visual search task, and questionnaire data indicate that the altered visual parameters that the user receives may be preferable to those in the baseline condition in which amplification of movement was not implemented. The tests also show that the user cannot interact normally with the VE if corresponding body movements are not amplified to the same degree as head movements, which may limit the implementation's versatility. Although not suitable for every application, the technique shows promise, and alterations to aspects of the implementation could extend its use in the future.",
Static specification analysis for termination of specification-based data structure repair,"We have developed a system that accepts a specification of key data structure consistency constraints, then dynamically detects and repairs violations of these constraints. It is possible to write specifications that are not satisfiable or that for other reasons may cause the repair process to not terminate. We present a static specification analysis that determines whether the repair process terminates for a given specification.","Data structures,
Laboratories,
Computer science,
Face detection,
Switches,
Computer errors,
Operating systems,
File systems,
Solids,
Algorithm design and analysis"
Online self-assessment as a learning method,"Algorithms and Programming Languages is a core subject in the BS degree in mathematics at the authors' university. Some of the students are very interested in computer programming but most of them find the subject quite hard. This situation is particularly stressed when concerning theoretical aspects and, in fact, many students view these areas as the main difficulty of the subject. Because of this, the authors decided to explore new ways to improve the student learning of theoretical concepts. Thus, they analyzed the use of online self-assessment tools as a self-learning system. To perform this analysis two different kinds of tools were chosen and the authors developed an experiment to evaluate, on one hand, the possible use of self-assessment tools as self-learning systems and, on the other hand, to compare the tools to each other.","Learning systems,
Computer languages,
Mathematics,
Programming,
Informatics,
Psychology,
Performance evaluation,
Performance analysis,
Navigation"
A hybrid numerical method to compute erythrocyte TMP in low-frequency electric fields,"This paper presents a coupling method of the finite element method and the boundary element method to compute the transmembrane potential (TMP) of an erythrocyte in a low-frequency electric field. We compute an in vitro erythrocyte's TMP induced by external electric fields by this hybrid method. It takes advantage of the homogeneous characteristics from both intracellular region and extracellular region. Moreover, we may use a fine three-dimensional (3-D) mesh around the thin membrane and avoid 3-D meshes in other regions. Numerical results of a spherical cell show that the hybrid method is accurate. The computed threshold of the applied electric field for membrane electric breakdown agrees well with those experimental results. Numerical results can also guide us to locate the maximum induced TMP on the erythrocyte membrane in various electric fields. Some further applications of the hybrid method are also discussed.","Biomembranes,
Finite element methods,
Boundary element methods,
Extracellular,
Information science,
Electromagnetic coupling,
Humans,
In vitro,
Electric breakdown"
Exploiting functional decomposition for efficient parallel processing of multiple data analysis queries,"Reuse is a powerful method for increasing system performance. In this paper, we examine functional decomposition for improving data and computation reuse and, therefore, overall query execution performance in the context of data analysis applications. Additionally, we look at the performance effects of using various projection primitives that make it possible to transform intermediate results generated by a query so that they can be reused by a new query. A satellite data analysis application is used to experimentally show the performance benefits achieved using functional decomposition and projection primitives.","Parallel processing,
Data analysis,
Relational databases,
Query processing,
System performance,
Subcontracting,
Computer science,
Educational institutions,
Biomedical informatics,
Biomedical computing"
The computer science pipeline in urban high schools: access to what? For whom?,"Our research was conceived out of a desire to understand why so few underrepresented students of color are learning computer science at the high school level. High school is a critical time for pre-college preparation and for getting on the right ""track"" for college and future career opportunities. Yet, by college the number of students of color obtaining bachelors degrees in computer science is remarkably small. In this article, we discuss our research on the high school ""computer science pipeline."" We define the ""computer science pipeline"" as the curriculum and opportunities available to students to learn computer science within their schools. Considering the tremendous economic and educational advantages, and the professional opportunities, that are available to students who understand computer science, this is a critical time to understand what factors are influencing African-American and Latino/a students' enrollment decisions.","Computer science,
Pipelines,
Educational institutions,
Computer science education,
Computer networks,
Bridges,
Engineering profession,
Design engineering,
Software standards,
Software design"
Real-time vehicle tracking on highway,"We propose a macroscopic method, which can perform real-time tracking of moving vehicles on highway. In addition to dealing with the problem caused by occlusion and noise, the proposed method can also track a vehicle doing lane change. The proposed method consists of two phases: the detection phase and the tracking phase. In the detection phase, we use entropy-based features to check the existence of vehicles. Then, we use the flux theory, which is commonly used in fluid mechanics to perform the tracking task. By conducting a great number of experiments, we have demonstrated the efficiency as well as the effectiveness of the proposed system.","Road vehicles,
Road transportation,
Entropy,
Vehicle detection,
Phase detection,
Intelligent transportation systems,
Thermodynamics,
Information science,
Laboratories,
Computer science"
Locally-optimal navigation in multiply-connected environments without geometric maps,"In this paper we present an algorithm to build a sensor-based, dynamic data structure useful for robot navigation in an unknown, multiply-connected planar environment. This data structure offers a robust framework for robot navigation, avoiding the need of a complete geometric map or explicit localization, by building a minimal representation based entirely on critical events in online sensor measurements made by the robot. There are two sensing requirements for the robot: it must detect when it is close to the walls, to perform wall-following reliably, and it must be able to detect discontinuities in depth information. It is also assumed that the robot is able to drop, detect and recover a marker. The navigation paths generated are optimal up to the homotopy class to which the paths belong, even though no distance information is measured.","Navigation,
Robot sensing systems,
Event detection,
Performance evaluation,
Error correction,
Mobile robots,
Computer science,
Data structures,
Robustness,
Buildings"
Automatic keywords extraction of Chinese document using small world structure,"Small world structure characterized by short characteristic path length and high clustering coefficient is widely observed in many natural and man-made systems. Inspired by the small word structure found in English, we analyzed Chinese documents and construct cooccurrence networks to representing the correlations of words, where nodes are words and the cooccurrences of words in the same sentence make up links. The cooccurrence networks show obvious small world properties. By utilizing the impact of each node's absence on the characteristic path length of the network, we extract keywords, which can well represent the contents of the document.","Humans,
Natural languages,
Computer science,
Books,
Biological system modeling,
Bridges,
Social network services,
Power grids,
Character generation,
Mesh generation"
Short-pulse radiation by a sequentially excited semi-infinite periodic planar array of dipoles,"This paper deals with the fourth in a sequence of canonical problems aimed toward an understanding of the time domain (TD) behavior of wideband-excited sequentially pulsed planar periodic finite arrays of dipoles, which play an important role in a variety of practical applications. The present investigation of sequentially pulsed semi-infinite planar dipole arrays extends our previous studies of sequentially pulsed infinite and semi-infinite line dipole arrays and of infinite planar dipole arrays. The discrete element-by-element radiations are converted collectively to radiations from a series of Floquet wave (FW)-modulated truncated smooth equivalent aperture distributions, and to corresponding FW-modulated edge diffraction. After a summary of necessary results from the earlier studies, emphasis is placed on the new truncation-induced TD results and interpretations, which are extracted via phenomenology-matched high-frequency asymptotics from rigorous frequency and time domain formulations parameterized in terms of the dispersive FW instantaneous frequencies and wave numbers. As in our previous studies, the outcome is a numerically efficient, physically incisive algorithm whose accuracy is verified preliminarily by application to a pulsed planar strip array of dipoles.","Time-domain analysis,
Phased arrays,
Planar arrays,
Diffraction,
Time-frequency analysis"
Trajectory planning for smooth transition of a biped robot,"This paper presents a third-order spline interpolation based trajectory planning method which is aiming to achieve smooth biped swing leg trajectory by reducing the instant velocity change which occurs at the time of collision of the biped swing leg with the ground. We first characterize the bipedal walking cycle and point out some major issues that need to be addressed to plan a continuous swing leg trajectory by using the concept of the spline interpolation and zero moment points (ZMP). We demonstrate that the impact effects can be avoided at the time of the swing foot's heel touching with the ground. The proposed biped trajectory planning method has been tested on our soccer-playing humanoid robot, Robo-Erectus, which got Second Place at the 2002 RoboCup Humanoid Walk competition.",
Predicting the performance of globus monitoring and discovery service (MDS-2) queries,"Resource discovery and monitoring in a distributed grid environment gives rise to several issues, one of which is the provision of reliable performance and hence, the quality-of-service delivered to grid users. This performance requirement brings about the necessity to know how the monitoring and discovery service (MDS) would respond to various queries. We focus on the performance of the MDS as part of the knowledge needed by a grid entity, to choose a grid index information service (GIIS) for discovering resources. Several performance metrics are defined and the performance achieved by a GIIS is evaluated against that obtained by a GRIS [H.N. Lim Choi Keung et al. (2003)]. Based on the GIIS performance data collected, the values for the performance metrics are predicted using different algorithms. Thus, past performance observations can be used to qualitatively characterise the future performance of the GIIS, allowing grid middleware built on these services to be more predictable.","Middleware,
Grid computing,
Performance analysis,
Computerized monitoring,
Measurement,
Quality of service,
Computer science,
Prediction algorithms,
Collaboration,
Bandwidth"
Requirements engineering for a pervasive health care system,"We describe requirements engineering for a new pervasive health care system for hospitals in Denmark. The chosen requirements engineering approach composes iterative prototyping and explicit environment description in terms of workflow modelling. New work processes and their proposed computer support are represented via a combination of prose, formal models, and animation. The representation enables various stakeholders to make interactive investigations of requirements for the system in the context of the envisioned work processes. We describe lessons learned from collaboration between users and system developers in engineering the requirements for the new system.","Medical services,
Paramagnetic resonance,
Design engineering,
Hospitals,
Prototypes,
Pervasive computing,
Collaborative work,
Navigation,
Unified modeling language,
Computer science"
Inference graphs: a computational structure supporting generation of customizable and correct analysis components,"Amalia is a generator framework for constructing analyzers for operationally defined formal notations. These generated analyzers are components that are designed for customization and integration into a larger environment. The customizability, and efficiency of Amalia analyzers owe to a computational structure called an inference graph. This paper describes this structure, how inference graphs enable Amalia to generate analyzers for operational specifications, and how we build in assurance. On another level, this paper illustrates how to balance the need for assurance, which typically implies a formal proof obligation, against other design concerns, whose solutions leverage design techniques that are not (yet) accompanied by mature proof methods. We require Amalia-generated designs to be transparent with respect to the formal semantic models upon which they are based. Inference graphs are complex structures that incorporate many design optimizations. While not formally verifiable, their fidelity with respect to a formal operational semantics can be discharged by inspection.","Design optimization,
Design engineering,
Computer Society,
Object oriented modeling,
Inspection,
Assembly,
Software design,
Design methodology,
Software engineering,
Computer science"
Evolution of the Chilean Web structure composition,"We present the evolution of the structure of the Chilean Web between 2000 and 2002. Our results show that although the Web grows as expected, also a significant part of it disappears. In addition, some components are much more stable than others. We also compare the expected life cycle of a Web site in the structure with the actual real data.","Computer science,
Predictive models,
Search engines,
Navigation"
A database record encryption scheme using the RSA public key cryptosystem and its master keys,"Two new database encryption systems are presented. Both systems are based on the concept of the RSA (Rivest, Shamir, Adleman) master key. The first proposed system is a field-oriented encryption system with user master keys that correspond to the access rights of multiple fields. The second proposed system is a record-oriented encryption system with a user master key. Using the idea of master keys, we present a method that establishes a correspondence between the subsets of a given set and a set of integers.","Public key cryptography,
Permission,
Protection,
Data security,
Public key,
Chaos,
Computer science,
Data engineering,
Distributed databases,
Polynomials"
Enhancing the fault-tolerance of nonmasking programs,"In this paper we focus on automated techniques to enhance the fault-tolerance of a nonmasking fault-tolerant program to masking. A masking program continually satisfies its specification even if faults occur. By contrast, a nonmasking program merely guarantees that after faults stop occurring, the program recovers to states from where it continually satisfies its specification. Until the recovery is complete, however a nonmasking program can violate its (safety) specification. Thus, the problem of enhancing fault-tolerance from nonmasking to masking requires that safety be added and recovery be preserved. We focus on this enhancement problem for high atomicity programs-where each process can read all variables-and for distributed programs-where restrictions are imposed on what processes can read and write. We present a sound and complete algorithm for high atomicity programs and a sound algorithm for distributed programs. We also argue that our algorithms are simpler than previous algorithms, where masking fault-tolerance is added to a fault-intolerant program. Hence, these algorithms can partially reap the benefits of automation when the cost of adding masking fault-tolerance to a fault-intolerant program is high. To illustrate these algorithms, we show how the masking fault-tolerant programs for triple modular redundancy and Byzantine agreement can be obtained by enhancing the fault-tolerance of the corresponding nonmasking versions. We also discuss how the derivation of these programs is simplified when we begin with a nonmasking fault-tolerant program.","Fault tolerance,
Safety,
Fault tolerant systems,
Software engineering,
Laboratories,
Computer science,
Automation,
Costs,
Redundancy,
Engineering profession"
On list decoding of alternant codes in the hamming and lee metrics,,"Decoding,
Reed-Solomon codes,
Computer science,
Polynomials,
Artificial intelligence,
Cost function"
ANNIDS: intrusion detection system based on artificial neural network,"This paper describes a network intrusion detection system based on artificial neural network (ANNIDS). The advantage of neural network ensures that ANNIDS does not need expert knowledge and it can find unknown or novel intrusions. The key part of ANNIDS is an adaptive resonance theory neural network (ART). ANNIDS can be trained in real-time and in an unsupervised way. A weight hamming distance method is used in detection, which is simple and correct in finding anomalous behavior. A well-trained ANNIDS can monitor the network in real time. The experimental results show that ANNIDS performs best when vigilance parameter is 0.4 to 0.5 and intrusion threshold is 0.4. The false positive error is about 8%, the negative error is about 2%, and the total error is lower 10%.","Intrusion detection,
Artificial neural networks,
Computer networks,
Data mining,
Immune system,
Computer errors,
Hamming distance,
Monitoring,
Educational institutions,
Computer science"
Process-oriented metrics for software architecture evolvability,"Evolution of software systems is almost a natural process. Evolution can occur at different levels of abstraction of software. Evolution at the architectural level, being the highest level of solution, can often times be the most critical to the success and survival of the pertaining software system. Metrics for software architectural evolvability will help determine the extent to which the architectural evolution can take place. We propose a framework called the POMSAE, process-oriented metrics for software architecture evolvability, that will help not only to intuitively develop architectural evolvability metrics but also to trace the metrics back to the evolvability requirements. This will then help analyze the reasons for the strengths/weaknesses in the metrics. POMSAE is partially validated by demonstrating its application to two practical telecom systems.","Software architecture,
Software systems,
Computer architecture,
Computer science,
Application software,
Dynamic programming,
Telecommunications,
Software maintenance,
Genetic programming,
Computer industry"
Radon transform inversion via Wiener filtering over the Euclidean motion group,"In this paper we formulate the Radon transform as a convolution integral over the Euclidean motion group (SE(2)) and provide a minimum mean square error (MMSE) stochastic deconvolution method for the Radon transform inversion. Proposed approach provides a fundamentally new formulation that can model nonstationary signal and noise fields. Key components of our development are the Fourier transform over SE(2), stochastic processes indexed by groups and fast implementation of the SE(2) Fourier transform. Numerical studies presented here demonstrate that the method yields image quality that is comparable or better than the filtered backprojection algorithm. Apart from X-ray tomographic image reconstruction, the proposed deconvolution method is directly applicable to inverse radiotherapy, and broad range of science and engineering problems in computer vision, pattern recognition, robotics as well as protein science.",
Estimation of 3D gazed position using view lines,"We propose a new wearable system that can estimate the 3D position of a gazed point by measuring multiple binocular view lines. In principle, 3D measurement is possible by the triangulation of binocular view lines. However, it is difficult to measure these lines accurately with a device for eye tracking, because of errors caused by (1) difficulty in calibrating the device and (2) the limitation that a human cannot gaze very accurately at a distant point. Concerning (1), the accuracy of calibration can be improved by considering the optical properties of a camera in the device. To solve (2), we propose a stochastic algorithm that determines a gazed 3D position by integrating information of view lines observed at multiple head positions. We validated the effectiveness of the proposed algorithm experimentally.","Humans,
Hardware,
Fingers,
Position measurement,
Calibration,
Cameras,
Stochastic processes,
Head,
Information science,
Optical devices"
A PDA-based interface for a computer supported educational system,"We present a teacher interface for a computer supported educational system that we are developing. This interface was developed over a PDA with wireless network connectivity to be carried by the teacher into the classroom. The first prototype implemented an interface with a virtual keyboard and mouse in order to remotely manage the classroom server and its peripherals. Additionally other features were included, such a remote video projector control and an agenda application. However, users had problems with regard to direct text introduction and mouse sensibility. Knowledge of the domain was acquired from experts in the field of education, and as a result, a new set of project requirements were defined and a second version of the interface has been developed. Its task-oriented design improves usability through minimizing the need for a virtual keyboard and mouse utilization. This system is currently in use in two classrooms in our university.","Computer interfaces,
Personal digital assistants,
User interfaces,
Keyboards,
Mice,
Computer science education,
Distance learning,
Collaborative work,
Home appliances,
Costs"
A novel coefficient scanning scheme for directional spatial prediction-based image compression,"Spatial prediction is a promising technique for image coding. For example, the coming AVC/H.264 standard adopts directional spatial prediction in the intra frame coding. Similar to the traditional DCT-based image coding schemes, it still scan the transform coefficients in a zigzag order, which is inefficient for coding the residual signals predicted from the different directions. To tackle this problem, a new scheme of scanning the transform coefficients by utilizing the spatial prediction information is proposed in this paper. The distribution of transform coefficients from each direction of spatial predictions is fully studied. According to the statistics, the adaptive scan table is derived for each type of spatial predictions, which is indicated by the prediction mode. Experimental results demonstrate that the proposed scheme can always outperform the JVT codec using zigzag scanning. Moreover, it does not introduce any extra computing costs in software implementation.",
Complete and independent sets of axioms of boolean algebra,"We investigate fundamental properties of axioms of Boolean algebra in detail by using the method of indeterminate coefficients, which uses multiple-valued logic. We can prove that four axioms, one of the commutative laws, one of the complementary laws, one of the distributive laws and one of the least element(a), greatest element (b) and the absorption laws are independent from others in the set of fundamental axioms of Boolean algebra. Then we research candidates, including those four axioms and other smaller number of axioms and prove all of those candidates are indeed complete and independent sets of axioms of the algebra.","Boolean algebra,
Absorption,
Computer science,
Logic functions"
Exploring models of development for evolutionary circuit design,"Traditional circuit design does not scale well to large, complex problems. Nature solves the scalability problem by using a complex mapping implicit in the process of biological development. By modelling this process we aim to improve scalability in evolutionary circuit design. Here we extend our earlier work (Gordon and Bentley, 2002) by demonstrating that evolution can learn and encode useful circuit design abstractions in a developmental process. We go on to present enhanced models of development with improved intercellular communication and show how this improves their ability to generate circuits.",
Comparison of plastic and NaI(Tl) scintillators for vehicle portal monitor applications,"Experimental data and computer simulations are presented for gamma-ray detection by vehicle portal monitors for homeland security applications at international borders. The experiments and simulations use spectral processing of gamma rays from various sources (/sup 241/Am, /sup 57/Co, /sup 133/Ba, /sup 137/Cs, /sup 60/Co) and background to provide data for comparing plastic and NaI(Tl) detectors. The effects of gamma-ray scattering in cargo are also examined. Plastic scintillators are well suited for primary screening of gamma-ray sources because of their large size and low cost. Sodium iodide is preferable to plastic for applications of isotope identification based on gamma-ray spectrometry. Some applications may benefit from integrating features from both types of detectors.",
An FPGA implementation of 3D affine transformations,"3D graphics performance is increasing faster than any other computing application. Almost all PC systems now include 3D graphics accelerators for games, Computer Aided Design (CAD) or visualization applications. This paper investigates the suitability of Field Programmable Gate Array (FPGA) devices as a low cost solution for implementing 3D affine trans formations. A proposed solution based on processing large matrix multiplication has been implemented, for large 3D models, on the RC1000-PP Celoxica board based development platform using Handel-C, a C-like language supporting parallelism, flexible data size and compilation of high-level programs directly into FPGA hardware.",
A holistic methodology for network processor design,"The GigaNetIC project aims to develop high-speed components for networking applications based on massively parallel architectures. A central part of this project is the design, evaluation, and realization of a parameterizable network processing unit. In this paper we present a design methodology for network processors which encompasses the research areas from the application software down to the gate level of the chip. Key components of this holistic approach have been successfully applied to characteristic examples of architecture refinements.","Process design,
Application software,
Computer architecture,
Parallel processing,
Hardware,
Computer science,
Memory management,
Network-on-a-chip,
Multiprocessor interconnection networks,
Program processors"
IT on demand - towards an environmental conscious service system for Vienna (AT),"The presented project is aimed at developing strategies of turning the existing system into product service system (PSS) in order to stop the trend of shortening lifetimes of IT products and to significantly reduce the amount of waste out of this sector. A product service system consists of tangible products and intangible services, designed and combined so that they jointly are capable of fulfilling specific customer needs. It tries to reach the goals of a sustainable development, which means improved economic, environmental and social aspects. The goal of this project has been to offer customers in Vienna a service that would provide the same value as the purchase and ownership of a product and to explore the necessary conditions for creation of such service system in Vienna.","Computer industry,
Environmental factors"
Transductive confidence machine for active learning,"This paper describes a novel active learning strategy using universal p-value measures of confidence based on algorithmic randomness, and transconductive inference. The early stopping criterion for active learning is based on the bias-variance tradeoff for classification. This corresponds to that learning instance when the boundary bias becomes positive, and requires one to switch from active to random selection of learning examples. The sign for the boundary and the increase in the classification error are two manifestations of the same phenomena, i.e., over-training. The experimental results presented show the feasibility and usefulness of our novel approach using a non-separable two-class classification problem. Our hybrid learning strategy achieves competitive performance against standard nearest neighbor methods using much fewer training examples.","Machine learning,
Support vector machines,
Switches,
Entropy,
Computer science,
Microwave integrated circuits,
Pattern classification,
Neural networks,
Costs,
Information theory"
Adaptive load distribution over multipath in NEPLS networks,"The emergence of multiprotocol label switching (MPLS) with its efficient support of explicit routing provides basic mechanisms for facilitating traffic engineering. Exploiting this capability of MPLS, we propose an adaptive multipath traffic engineering mechanism named LDM (load distribution over multipath). The main goal of LDM is to enhance the network utilization as well as the network performance by adaptively splitting traffic load among multiple paths. LDM takes pure dynamic approach not requiring any a priori traffic load statistics. Routing decisions are made at the flow level, and traffic proportioning reflects both the length and the load of a path. LDM also dynamically selects a few good label switched paths (LSPs) according to the state of the entire network. We use simulation to compare the performance of LDM with the performance of several representative dynamic load distribution approaches as well as the traditional static shortest path only routing. The numerical results show that LDM outperforms the compared approaches in both the blocking ratio as well as the performance of the accepted traffic flows.","Intelligent networks,
Multiprotocol label switching,
Telecommunication traffic,
Network topology,
Routing,
Statistical distributions,
Delay,
Computer science,
Protocols,
Transportation"
Using FPGAs to solve the Hamiltonian cycle problem,"The Hamiltonian Cycle (HC) problem is an important graph problem with many applications. The general backtracking algorithm normally used for random graphs often takes far too long in software. With the development of field-programmable gate arrays (FPGAs), FPGA-based reconfigurable computing offers promising choices for acceleration. This research exploits the idea of an instance-specific approach and proposes a system design based on a reconfigurable hardware implementation for solving the HC problems. In our implementation, only one FPGA is used, on an internal PCI-based board. The experimental results show that the reconfigurable hardware approach yields significant runtime speedups over the conventional approach, although the clock rate of the FPGA hardware is much slower than that of the workstation running the software solver.","Field programmable gate arrays,
Hardware,
Circuits,
Programmable logic arrays,
Computer science,
Runtime,
Clocks,
Workstations,
NP-complete problem,
Routing"
Cryptanalysis of a fingerprint-based remote user authentication scheme using smart cards,"A remote user authentication scheme is a procedure for a server to authenticate a remote user in a network. Recently, Lee et al. proposed a fingerprint-based remote user authentication scheme to overcome the security flaw in Hwang and Li's scheme. In Lee et al.'s authentication scheme, they store two secret keys and some public elements in a smart card. We first review Lee et al.'s fingerprint-based user authentication scheme. Next, we show that Lee et al.'s scheme still suffers from the impersonation attack.","Fingerprint recognition,
Authentication,
Smart cards,
Network servers,
Law,
Legal factors,
Computer science,
Sun,
Laboratories,
Communication industry"
Rights amplification in master-keyed mechanical locks,"This article examines mechanical lock security from a computer-science and cryptology perspective, focusing on new and practical attacks for amplifying rights in master-keyed mechanical pin tumbler locks.","Computer security,
Cryptography,
Privacy,
Government,
Manufacturing,
Protection,
Upper bound,
Humans,
Testing,
Couplings"
Evaluating on-line learning platforms: a case study,"Our ""information-oriented"" society shows an increasing exigency of life-long learning. In such framework, online learning is becoming an important tool to allow the flexibility and quality requested by such a kind of learning process. In the recent past, a great number of on-line platforms have been introduced on the market showing different characteristics and services. A series of features should be taken into account when one evaluates e-learning platforms, starting from the function and usability of the overall learning system in the context of the human, social and cultural organization within which it is to be used. Obviously, the analysis of the features of a system is not sufficient: it is also important to understand how they are integrated to facilitate learning and training and what principles are applied to guide the way the system is used. To evaluate them both pedagogical and technological aspects must be carefully evaluated. This paper proposes a model for describing and characterizing on-line learning platform component. The model is then used to evaluate the most known existing commercial platforms.","Computer aided software engineering,
Internet,
Computer aided instruction,
Electronic learning,
Usability,
Learning systems,
Humans,
Cultural differences,
Information technology,
Globalization"
License plate surveillance system using weighted template matching,"This paper presents a simple and robust algorithm for vehicle's license plate recognition system. Based on template matching, this algorithm can be applied for real time recognition of license plates for vehicle surveillance system. The working principle is weight feature based hierarchical template evaluation. The performance of the proposed system has been evaluated on images acquired in real traffic conditions.",
Search strategies for ensemble feature selection in medical diagnostics,"The goal of this paper is to propose, evaluate, and compare four search strategies for ensemble feature selection, and to consider their application to medical diagnostics, with a focus on the problem of the classification of acute abdominal pain. Ensembles of learnt models constitute one of the main current directions in machine learning and data mining. Ensembles allow us to get higher accuracy, sensitivity, and specificity, which are often not achievable with single models. One technique, which proved to be effective for ensemble construction, is feature selection. Lately, several strategies for ensemble feature selection were proposed, including random subspacing, hill-climbing-based search, and genetic search. In this paper, we propose two new sequential-search-based strategies for ensemble feature selection, and evaluate them, constructing ensembles of simple Bayesian classifiers for the problem of acute abdominal pain classification. We compare the search strategies with regard to achieved accuracy, sensitivity, specificity, and the average number of features they select.","Medical diagnosis,
Diversity reception,
Abdomen,
Pain,
Medical diagnostic imaging,
Computer science,
Electronic mail,
Data mining,
Bayesian methods,
Educational institutions"
Discrete finger and palmar feature extraction for personal authentication,"A new method of a reliable and real-time authentication is proposed. Finger geometry and feature extraction of the palmar flexion creases are integrated in discrete points of characteristics. A video image of either palm, palm placed freely facing toward a video camera in front of a low-reflective board, is acquired. Fingers are brought together without any constraints. Discrete feature point extraction for each of the four fingers involves: intersection points of the three digital (finger) flexion creases on the finger skeletal line; skeletal lengths of the finger segments between the three creases; distances between the intersection points and the corresponding points of the adjacent fingers. Discrete feature extraction for the palm involves: intersection points of the major palmar flexion creases on the extended finger skeletal line; orientation of the crease at each point of the intersection. These metrics define the feature vectors for matching. Matching results are perfect for 50 subjects so far. This point wise integration of the finger and palmar feature extraction, extracting enough feature from non contacting video image, requiring no time-consumptive palm print image analysis, and requiring less than one second processing time, will contribute to a real-time and reliable authentication.","Fingers,
Feature extraction,
Authentication,
Biometrics,
Fingerprint recognition,
Geometry,
Security,
Shape measurement,
Computer science,
Cameras"
A distributed framework of Web-based telemedicine system,"A distributed telemedicine system is superior to isolated one. As a typical example, separating Web servers and data servers by CORBA technique can expand basic Web-based telemedicine system into a distributed system. As database would be fragmented on distributed sites, query optimization should be employed when possible. In addition, by estimating the resource utilization ratio, it is easy to achieve maximal profits for the distributed telemedicine system.","Telemedicine,
Network servers,
Medical treatment,
Web server,
Data communication,
Medical diagnostic imaging,
Web services,
Computer science,
Educational institutions,
Distributed databases"
Some issues of role-based collaboration,"Roles are a powerful concept for facilitating distributed systems management. It has been accepted as a good solution for information sharing based on the research of KB AC (role-based access control). Although no explicit roles are declared in traditional CSCW (computer-supported-cooperative work) systems, there are implicit roles granted in the design. Consequently, we may infer that ""without roles, there would be no collaboration"". There is, however, a lack of specific, comprehensive research on building collaborative systems with role management and on role-based methods that have been successfully used in access control. This paper aims to clarify the basic problems related to the research of role-based collaboration and to propose a basic method to do that research.","Collaboration,
Collaborative work,
Access control,
Energy management,
Power system management,
Job design,
Computer science,
Educational institutions,
Buildings,
Virtual environment"
A polynomial algorithm for recognizing perfect graphs,"We present a polynomial algorithm for recognizing whether a graph is perfect, thus settling a long standing open question. The algorithm uses a decomposition theorem of Conforti, Cornuejols and Vuskovic. Another polynomial algorithm for recognizing perfect graphs, which does not use decomposition, was obtained simultaneously by Chudnovsky and Seymour. Both algorithms need a first phase developed jointly by Chudnovsky, Cornuejols, Liu, Seymour and Vuskovic.","Polynomials,
Cleaning,
Sun,
Artificial intelligence,
Computer science"
An agent-based approach for grid computing,"We present an agent-based computational grid (ACG), which applies the concept of computational grid to agents. The ACG system is to implement a uniform higher-level management of the computing resources and services on the grid, and provide users with a consistent and transparent interface for accessing such services. All entities in the grid environment including computing resources and services can be represented as agents. Each entity is registered with a grid service manager. An ACG grid service can be a service agent that provides the actual grid service to the other grid member. Grid members communicate with each other by communication space that is an implementation of tuple space. The design and implementation of four grid protocols for grid service discovery are given. Finally, some conclusions are given.","Grid computing,
Distributed computing,
Resource management,
Application software,
Computer displays,
Computer interfaces,
High performance computing,
Instruments,
Computer science,
Environmental management"
On the grid and sensor networks,"The pervasive computing environment and the wired grid infrastructure can be combined to make the information grid truly pervasive. Interesting applications can be built by utilizing the computational abilities of the grid to answer queries on sensor data. We identify some of the research issues and challenges in building an infrastructure to support such applications. We present the design and preliminary implementation of the proposed infrastructure, identify the tradeoffs relating to sensor accuracy consumption, and report experimental results from a sample scenario involving firefighting.","Grid computing,
Sensor phenomena and characterization,
Temperature sensors,
Pervasive computing,
Aggregates,
Computer networks,
Fires,
Sensor systems,
Computer science,
Application software"
Peer-to-peer networks for virtual home environments,"This paper presents an approach to extend OSGi server based home networks with JXTA's peer-to-peer (P2P) technology to a multi-server home environment connecting multiple private home networks to one true virtual environment for communication, media, sharing/exchange, and distributed device control. We elaborate on the basic integration methodology and extend it to a general, security preserving, architectural concept based on a two level integration concept for inhome and interhome networks. The integration is achieved through additional OSGi services for the management of virtual devices, peer-to-peer communication, and authentication. The example of a quality of service based video streaming management demonstrates one of the potential advanced application for which peer-to-peer can be used in addition.","Peer to peer computing,
Home automation,
Network servers,
Joining processes,
Virtual environment,
Distributed control,
Communication system control,
Authentication,
Quality of service,
Streaming media"
Temporal queries in XML document archives and web warehouses,"By storing the successive versions of a document in an incremental fashion, XML repositories and data warehouses achieve: (i) the efficient preservation of critical information; and (ii) the ability of supporting historical queries on the evolution of documents and their contents. In this paper, we present efficient techniques for managing multi-version document histories and supporting powerful temporal queries on such documents. Our approach consists in: (i) concisely representing the successive versions of a document as an XML document that implements a temporally grouped data model; and (ii) using XML query languages, such as XQuery, to express complex queries on the content of a particular version, and on the temporal evolution of the document elements and their contents. We show that the data definition & manipulation framework of XML & XQuery can support temporal models and historical queries significantly better than the traditional framework of Relational Tables & SQL. To demonstrate this point, we investigate how to express complex queries on the history of relational tables published as XML data.","XML,
Management information systems,
Data warehouses,
History,
Software libraries,
Application software,
Computer science,
Energy management,
Data models,
Database languages"
Reflecting skills and personality internally as means for team performance improvement,"Formal evaluation of team performance (e.g. with metrics) is a good and well known instrument for providing feedback about finished projects. If performance was behind expectations a further analysis may include an oral discussion of problems that occurred during the project, that have been treated ineffectively or even have been put aside. Such problems may include choice and quality of engineering practices, the appropriateness of role and responsibility assignments and relationships between team members, which have been conductive or obstructive to the team. This feedback improves engineering behaviour and strengthens the problem and conflict solving capabilities for coming projects, but can't affect any more the project of concern, which already has been finished. This paper proposes the usage of a questionnaire based on skills and personality traits for providing feedback to software project teams during running projects. The result of the assessment should help to improve the performance of the project team by defining roles adequately and putting responsibilities to the right people: It improves the personal relationships within the team, increases the attitude for personal tasks, increases the understanding and respect for the other team members, shows significant misconception of personal strengths and weaknesses and indicates fit or misfit of engineering practices with skills and personalities.","Feedback,
Software engineering,
Cost accounting,
Instruments,
Performance analysis,
Power distribution,
Productivity,
Programming profession,
Monitoring,
Educational programs"
"Effects of team size on participation, awareness, and technology choice in geographically distributed teams","In this paper we investigate the effects that team size has on geographically distributed teams within a large, multi-national manufacturing organization. Survey responses from 109 members of distributed teams indicate that compared to members of larger teams, members of smaller teams participated more actively on their team, were more committed to their team, were more aware of the goals of the team, were better acquainted with other team members' personalities, work roles and communication styles, and reported higher levels of rapport. The data also show that larger teams are more conscientious in preparing meeting agendas compared to smaller teams. Consistent with their emphasis on coordination, larger teams adopted technology to support the coordination of asynchronous work, while smaller teams adopted collaboration technology. Implications for technology adoption are discussed.","Space technology,
Communications technology,
Consumer electronics,
Computer science,
Pulp manufacturing,
Collaborative work,
Computer mediated communication,
Teleworking,
Information technology,
Teamwork"
Minimum cost ring survivability in WDM networks,"Recently, E. Modiano and A. Narula-Tam (see Proc. IEEE INFOCOM'01, 2001) introduced the notion of survivable routing and established a necessary and sufficient condition for the existence of survivable routes for a given logical topology in a physical topology. In two earlier papers, we showed that the survivability problem is NP-complete, both when the nodes of the logical topology are unlabeled (Sen, A. et al., Proc. IEEE Int. Commun. Conf. ICC'02, 2002) or labeled (Sen et al., Proc. IEEE Int. Symp. on Computers and Commun. ISCC'02, 2002). We also gave algorithms to test if survivable routing of a logical ring is possible in a WDM network with arbitrary physical topology. We now address finding the minimum number of links that have to be added to a physical topology so that the survivable routing of a logical ring is possible. We show that, not only is this problem NP-complete, but an /spl epsiv/-approximation algorithm for the problem cannot be found unless P=NP. We provide an ILP formulation for finding the optimal solution of the problem. We also provide an approximate solution of the problem. The approximate algorithm requires only a very small fraction of the time required by the optimal algorithm, but produces results that are close to the optimal solution on two test networks - ARPANET and the Italian National Network.","Costs,
Intelligent networks,
WDM networks,
Network topology,
Routing,
IP networks,
Computer science,
Sufficient conditions,
Logic testing,
ARPANET"
On reducing broadcast redundancy in ad hoc wireless networks,"Unlike in a wired network, a packet transmitted by a node in an ad hoc wireless network can reach all neighbours. Therefore, the total number of transmissions (forward nodes) is generally used as the cost criterion for broadcasting. The problem of finding the minimum number of forward nodes is NP-complete. Among various approximation approaches, dominant pruning by H. Lim and C. Kim (2001) utilizes 2-hop neighbourhood information to reduce redundant transmissions. In this paper, we analyze some deficiencies of the dominant pruning algorithm and propose two better approximation algorithms: total dominant pruning and partial dominant pruning. Both algorithms utilize 2-hop neighbourhood information more effectively to reduce redundant transmissions. Simulation results of applying these two algorithms show performance improvements compared with the original dominant pruning. In addition, two termination criteria are discussed and compared through simulation.","Broadcasting,
Intelligent networks,
Wireless networks,
Floods,
Approximation algorithms,
Costs,
Computer science,
Algorithm design and analysis,
Mobile communication,
Personnel"
Flexible binary space partitioning for robotic rescue,"In domains such as robotic rescue, robots must plan paths through environments that are complex and dynamic, and in which robots have only incomplete knowledge. This will normally require both diversions from planned paths as well as significant re-planning as events in the domain unfold and new information is acquired. In terms of a representation for path planning, these requirements place significant demands on efficiency and flexibility. This paper describes a method for flexible binary space partitioning designed to serve as a basis for path planning in uncertain dynamic domains such as robotic rescue. This approach is used in the 2003 version of the Keystone Fire Brigade a robotic rescue team. We describe the algorithm used, make comparisons to related approaches to path planning, and provide an empirical evaluation of an implementation of this approach.","Orbital robotics,
Path planning,
Navigation,
Mobile robots,
Autonomous agents,
Computer science,
Fires,
Partitioning algorithms,
Target tracking,
Measurement standards"
A motion generation system for humanoid robots - Tai Chi motion,"This paper proposes a static posture based motion generation system for humanoid robots. The system generates a sequence of motion from given several postures, and the motion is smooth and stable in the balance. We have produced all the motions of Tai Chi Chuan by the system. Motion generation for humanoids has been studied mainly based on the dynamics. Dynamic based method has, however, some defects: e.g., numerous parameters which can not be always prepared, expensive computational cost and no guarantee that the motions are stable in balance. We have, thus studied less dependent-on-dynamics approach. A motion is described as a sequence of postures. Our system figure out if we need extra postures to insert stability. This method enables humanoid robot, HOAP-1 to do Tai Chi Chuan.","Humanoid robots,
Computational efficiency,
Motion control,
Character generation,
Intelligent robots,
Computer science,
Stability,
Control theory,
Optimization methods,
Robot sensing systems"
Egomotion estimation of a range camera using the space envelope,"In this paper we present a method to compute the egomotion of a range camera using the space envelope. The space envelope is a geometric model that provides more information than a simple segmentation for correspondences and motion estimation. We describe a novel variation of the maximal matching algorithm that matches surface normals to find correspondences. These correspondences are used to compute rotation and translation estimates of the egomotion. We demonstrate our methods on two image sequences containing 70 images. We also discuss the cases where our methods fail, and additional possible methods for exploiting the space envelope.","Cameras,
Motion estimation,
Robot vision systems,
Feature extraction,
Image segmentation,
Layout,
Computer science,
Solid modeling,
Image sequences,
Image motion analysis"
A protocol with transcoding to support QoS over Internet for multimedia traffic,"The growth of the Internet has brought with it a tremendous volume of multimedia traffic, which is bursty in nature. Providing a required QoS as well as modeling multimedia traffic has been a challenging task. In this work, we transcode multimedia data to cater for low bandwidth availability and different end-user requirements. We propose a protocol architecture which has been developed by the amalgamation of well-known components and that would provide guaranteed multimedia communication over the Internet. We model multimedia traffic using the M/Pareto distribution in an attempt to represent realistic traffic pattern. We use semantics of multimedia data-streams for transcoding to avoid network congestion and to ensure optimal use of network resources. The impact of transcoding the multimedia data to suit it to the network load and the end user requirements is also studied. The simulation results are presented and compared.","Transcoding,
Quality of service,
Traffic control,
Telecommunication traffic,
IP networks,
Diffserv networks,
Multimedia communication,
Web and internet services,
Routing protocols,
Computer science"
Realizing a visual marker using LEDs for wearable computing environment,"In order to utilize the real world via computers by means of camera images in augmented reality systems, the concise coordinate information of the systems must be identified. Image-based approaches are promising for this purpose since they provide direct mapping from the camera images to the coordinate system for virtual objects, and therefore, no camera calibration is necessary. However, the conventional image-based markers are typically stickers, and the information they deliver is limited and susceptible to change dynamically. In this paper, we propose a location marking method, called VCC (visual computer communication), which uses more intelligent markers. In our method, a marker, consisting of 16 LEDs (light-emitting diodes), keeps blinking to provide both coordinate information and attached information such as an address or a URL.","Light emitting diodes,
Wearable computers,
Cameras,
Augmented reality,
Computer graphics,
Computer displays,
Multimedia systems,
Information science,
Computer science,
Calibration"
A zero one law for RP,"We show that if RP has p-measure nonzero then ZPP=EXP. As corollaries, we obtain a zero-one law for RP, and that both probabilistic classes ZPP and RP have the same p-measure. Finally we prove that if NP has p-measure nonzero then NP=AM.","Complexity theory,
Boolean functions,
Turing machines,
Computer science,
Computer errors,
Size measurement"
Cultural swarms: knowledge-driven problem solving in social systems,"In this paper we investigate how diverse knowledge sources interact to direct individuals in a swarm population. We identify three basic phases of problem solving that are generated by the swarm population in the solution of real valued function optimization problems. The question that we are interested in answering is how these phases derive from the interaction of various sources of cultural knowledge in the belief space of a population. We map the central tendency of a subset of individuals that are influenced by each knowledge source over time at the meta-level. The resultant patterns suggest the presence of ""cultural swarms"" where various knowledge sources take turns at leading and following in the exploration and exploitation of the problem space. This suggests that the social interaction of individuals coupled with their interaction with a culture within which they are embedded provides a powerful vehicle for the solution of complex problems.",
On a text-processing approach to facilitating autonomous deception detection,"Current techniques towards information security have limited capabilities to detect and counter attacks that involve different kinds of masquerade and spread of misinformation executed over long time periods to achieve malicious goals. Detection of such deceptive information obtained during online interactions (emails, chat room conversations) is the first step before counter strategies can be developed. With the large-scale use of information technologies as a general communication medium, facilitating deception detection is a key enabler to utilizing information systems to their fullest potential. This article presents a framework for computer-aided deception detection building on the interpersonal deception theory (IDT) of human interpersonal communication research and text-processing techniques for facilitating deception analysis of text-oriented communication. A state-transition diagram based framework is proposed to model the dynamic evolution of an interpersonal conversation between a sender and receiver based on the IDT-based process schemata. The framework is then utilized to develop a deception detection agent to process textual information. Deception detection is defined as a process of model verification. Architecture of a prototype under development and open problems for further research in this area are outlined.","Humans,
Consumer electronics,
Wire,
Counting circuits,
Decision making,
Electronic mail,
Web and internet services,
Radar detection,
Information security,
Large-scale systems"
New flow control schemes of TCP for multimodal mobile hosts,"Two control schemes designed specifically to handle changes in the datalink interface for a mobile host are presented. The future mobile environment is expected to involve multimode connectivity to the Internet and dynamic switching of the connection mode depending on network conditions. However, the existing TCP architecture is unable to maintain stable and efficient throughput as a result of such interface changes. The main issues are how to handle the change in host IP address when the datalink interface changes, and how to ensure reliable and continuous TCP flow. Although architectures addressing the first of these issues have already been proposed, the problem of flow control still remains. The first of the proposed schemes, immediate expiration of timeout timer, detects such an interface change, and begins retransmission immediately without waiting for retransmission timeout as in conventional TCP. The second scheme, bandwidth aware slow start threshold, detects the interface changes, and also estimates the new bandwidth so as to set an appropriate slow start threshold when beginning retransmission. Through simulations, the bandwidth aware slow start threshold scheme is demonstrated to provide marked improvements in performance over conventional TCP, particularly in situations where the bandwidth changes significantly and the interface is changed consecutively at short intervals. The immediate expiration of timeout timer scheme is also shown to be effective when the communication interruption associated with interface change is long.","TCPIP,
Bandwidth,
Mobile computing,
Internet,
Computer science,
Mobile communication,
Switches,
Art,
IP networks,
Maintenance"
Data and metadata on the semantic Grid,"For original paper see ""Integrating Computing and Information on Grids"", ibid., p. 94-96 (2003). We covered the important role that the Grid is expected to have in information systems, focusing primarily on sensor and database grids. we contrasted this with the major original Grid focus of metacomputing, or the Grid's linking together distributed computers. Whatever type of Grid you have, metadata - data about data - is important.",
Similarity discovery in structured P2P overlays,"Peer-to-peer (P2P) overlays are appealing, since they can aggregate resources of end systems without relying on sophisticated infrastructures. Services can thus be rapidly deployed over such overlays. Primitive P2P overlays only support searches with single keywords. For queries with multiple keywords, presently only unstructured P2P systems can support by extensively employing message flooding. We propose a similarity information retrieval system called Meteorograph for structured P2P overlays without relying on message flooding. Meteorograph is fault-resilient, scalable, responsive and self-administrative, which is particularly suitable for an environment with an explosion of information and a large number of dynamic entities. An information item stored in Meteorograph is represented as a vector. A small angle between two vectors means that the corresponding items are characterized by some identical keywords. Meteorograph further stores similar items at nearby locations in the P2P overlay. To retrieve similar items, only nodes in nearby locations are located and consulted. Meteorograph is evaluated with simulation. The results show that Meteorograph can effectively distribute loads to the nodes. Discovering a single item and a set (in size k) of similar items takes O(log N) and (k/c)middotO(log N) messages and hops respectively, where N is the number of nodes in the overlay and c is the storage capacity of anode","Peer to peer computing,
Floods,
Costs,
Aggregates,
Tornadoes,
Power system management,
Computer science,
Information retrieval,
Explosions,
Delay"
Student-centered learning in online and blended education on computer information systems,"The effectiveness of learning and training in many ways depends on increased demand for student's convenience, including his/her time, pace and place of learning. Pedagogical strategies that are based on total use of state-of-the-art technologies such as Internet and the World Wide Web (Web) correspond to the highest level of student's convenience. As a part of the National Science Foundation's grant # 0196015 (2000-2004), Bradley University (BU) and its InterLabs Research Institute (IRI) develop innovative teaching and learning technologies based on four founding principles: 1) student-centered principles of education, 2) Web-lecturing based on streaming multimedia technologies, 3) modularity of learning content based on reusable learning objects (RLO) approach, and 4) equivalence of learning content delivery using traditional in-classroom face-to-face (F2F) education, online education, and blended education. This paper summarizes author's findings on student-centered learning in online and blended education of the BU academic course on computer information systems.","Computer science education,
Information systems,
Educational technology,
Continuing education,
Internet,
Streaming media,
Educational programs,
Systems engineering education,
Economic forecasting,
Educational institutions"
"J-DSP-C, a control systems simulation environement: labs and assessment",,
An efficient method to schedule tandem of real-time tasks in cluster computing with possible processor failures,"Each computer system in operation today is certain to experience faults in its operational lifetime. There is more than one source, which can cause faults, for example environmental factors, communication, interaction with users, or hardware or software design flaws randomly build into the system. A reliable system is the one that would continue operation albeit in a degraded mode despite the presence of faults. In this paper, we introduce an efficient method to schedule a tandem of real-time tasks in cluster computing with possible processor failures. The method is based on developing an objective function that combines different scheduling goals. This objective function is used to guide the search algorithm to find an efficient solution. The proposed algorithm consists of three terms, reliability, deadline and tasks grouping to minimize remote communication among tasks. The proposed method creates a scheduling table based on the objective function then searches this table for a feasible solution. The method is efficient because it reduces communication between different tasks by grouping them together. This grouping process reduces application de-fragmentation as well.","Processor scheduling,
Scheduling algorithm,
Redundancy,
Hardware,
Clustering algorithms,
Real time systems,
Fault tolerance,
Computer science,
Environmental factors,
Software design"
Generalizing interface design knowledge: lessons learned from developing a claims library,"The experience of preparing interface design knowledge to be reusable allows reflection on the process, potential and general challenges of effectively and efficiently using this knowledge in design tasks. With an interest in crafting a catalog for design claims that would implement recent reuse theory in the human-computer interaction field, we developed and implemented a unique process of creating reusable content for notification systems - interfaces used for multitasking. This process, which we describe and illustrate, extends previous work on capturing metadata related to design claims. The data and metadata are stored in the catalog, which is intended to be accessible to other designers for reuse in other domains. The multitask nature of our catalog subject matter highlights a major challenges faced in reuse: the generalization specific and contextual information (claims). The challenge of balancing abstraction with specificity to ensure both meaningful and domain-independent data is also addressed. We believe that our approach can generalize to other reuse projects that strive for cross-domain knowledge application.","Software design,
Application software,
Human computer interaction,
Computer science,
Reflection,
Multitasking,
Software engineering,
Process design,
Software libraries,
Buildings"
A P2P approach for global computing,"We describe a peer-to-peer self-organizing overlay network for our global computing system. The preliminary simulation results show that the network has some small-world characteristics such as higher clustering coefficient and short path length, which leads to an efficient heuristic task scheduling algorithm on which any volunteer peer with limited knowledge about the global network can dispatch its excrescent computation tasks to powerful nodes globally, in a way contrary to the current global computing system in which a global broker is responsible for the task scheduling. We argue that our approach is a starting point to eliminate the broker component which makes current global systems unscalable.","Peer to peer computing,
Computer networks,
Military computing,
Scheduling algorithm,
Processor scheduling,
Computer architecture,
Scalability,
Load management,
Computer science,
Computational modeling"
Towards composable distributed real-time and embedded software,"The complexity of building and validating software is a growing challenge for developers of distributed real-time and embedded (DRE) applications. While DRE applications are increasingly based on commercial off-the-shelf (COTS) hardware and software elements, substantial time and effort are spent integrating these elements into applications. Integration challenges stem largely from a lack of higher level abstractions for composing complex applications. As a result, considerable application-specific ""glue code"" must be rewritten for each successive DRE application. This paper makes three contributions to the study of composing reusable middleware from standard components in DRE applications: it (1) describes the limitations of current approaches in middleware composition, (2) discusses the minimum set of requirements required of reusable middleware components, and (3) presents recurring patterns for software composition as applied to CIAO (Component-Integrated ACE ORB), our open-source component model implementation.","Embedded software,
Application software,
Middleware,
Hardware,
Software standards,
Open source software,
Concurrent computing,
Computer science,
Distributed computing,
Embedded computing"
Distributed task negotiation in self-reconfigurable robots,"A self-reconfigurable robot can be viewed as a network of many autonomous modules. Driven by their local information, the modules can initiate tasks that may conflict with each other at the global level. How the modules negotiate and select a coherent task among many competing tasks is thus a critical problem for the control of self-reconfigurable robots. This paper presents a distributed algorithm called DISTINCT to solve this challenging problem and show that it can be successfully applied to the CONRO self-reconfigurable robots. A discussion how to apply DISTINCT to other types of distributed systems such as sensor network, swarm robots, or multi-agent systems is also given.","Robot sensing systems,
Sensor systems,
Multiagent systems,
Intelligent networks,
Computer science,
Distributed algorithms,
Tail,
Robot control,
Intrusion detection,
Polynomials"
AAR: an adaptive rate control protocol for mobile ad hoc networks,,"Programmable control,
Adaptive control,
Protocols,
Mobile ad hoc networks,
Throughput,
Ad hoc networks,
Error analysis,
Computer science,
Data engineering,
Sun"
The cost of cache-oblivious searching,"Tight bounds on the cost of cache-oblivious searching are proved. It is shown that no cache-oblivious search structure can guarantee that a search performs fewer than lg e log/sub B/N block transfers between any two levels of the memory hierarchy. This lower bound holds even if all of the block sizes are limited to be powers of 2. A modified version of the van Emde Boas layout is proposed, whose expected block transfers between any two levels of the memory hierarchy arbitrarily close to [lg e + O(lg lg B/ lgB)] logB N + O(1). This factor approaches lg e /spl ap/ 1.443 as B increases. The expectation is taken over the random placement of the first element of the structure in memory. As searching in the disk access model (DAM) can be performed in log/sub B/N + 1 block transfers, this result shows a separation between the 2-level DAM and cache-oblivious memory-hierarchy models. By extending the DAM model to k levels, multilevel memory hierarchies can be modeled. It is shown that as k grows, the search costs of the optimal k-level DAM search structure and of the optimal cache-oblivious search structure rapidly converge. This demonstrates that for a multilevel memory hierarchy, a simple cache-oblivious structure almost replicates the performance of an optimal parameterized k-level DAM structure.","Random access memory,
Computer science,
Algorithm design and analysis,
Contracts,
Helium,
Cost function,
Laboratories,
Information science,
Computational modeling,
Read-write memory"
SCTP over satellite networks,"The stream control transmission protocol (SCTP) has recently been standardized as a new transport layer protocol in the IP suite. In addition to the core features of TCP, SCTP incorporates a number of advanced and unique features which are not available in TCP. The paper investigates the suitability of SCTP for data communications over satellite links. We describe SCTP features that allow SCTP to utilize the bandwidth of satellite networks better, while at the same time avoiding congestion collapse in a shared network. Finally, we provide recommendations on the use of SCTP over satellite networks.","Satellite broadcasting,
Transport protocols,
TCPIP,
Data communication,
NASA,
Bandwidth,
Internet telephony,
Propagation losses,
Wireless sensor networks,
Computer science"
Using legos to interest high school students and imtrove k12 stem education,,"Educational institutions,
Manufacturing,
Engineering profession,
Design engineering,
Computer science education,
Books,
Educational technology,
Automotive engineering,
Educational robots,
Mathematics"
Specifying and verifying systems with multiple clocks,"Multiple clock domains are a challenge for hardware specification and verification. We present a method for specifying the relations between multiple clocks, and for modeling the possible behaviors. We can then verify a hardware design assuming that the clocks meet these constraints. We implement our ideas in the context of SAT based bounded model checking (BMC), using ANSI-C programs to specify the functional behavior of the design.","Clocks,
Hardware design languages,
Contracts,
Context modeling,
Signal design,
Circuits,
Computer science,
Specification languages,
Government,
Frequency"
Augmenting SSEs with structural properties for rapid protein structure comparison,"Comparing protein structures in three dimensions is a computationally expensive process that makes a full scan of a protein against a library of known protein structures impractical. To reduce the cost, we can use an approximation of the three dimensional structure that allows protein comparison to be performed quickly to filter away dissimilar proteins. In this paper we present a new algorithm, called SCALE, for protein structure comparison. In SCALE, a protein is represented as a sequence of secondary structure elements (SSEs) augmented with 3D structural properties such as the distances and angles between the SSEs. As such, the comparison between two proteins is reduced to a sequence alignment problem between their corresponding sequences of SSEs. The 3-D structural properties of the proteins contribute to the similarity score between the two sequences. We have implemented SCALE, and compared its performance against existing schemes. Our performance study shows that SCALE outperforms existing methods in terms of both efficiency and effectiveness (measured in terms of precision and recall).",
A safe mobile agent system for distributed intrusion detection,"Some applications of the technology of mobile agent (MA) in intrusion detection system (IDS) have been developed. MA technology can bring IDS flexibility and enhanced distributed detection ability. However, the security of mobile agents and methods of collaboration among mobile agents are important problems cared by many researchers. For that, we designed a security mobile agent system for distributed intrusion detection and implemented a prototype. In this paper, we firstly analyze the shortcomings of current IDSs and discuss the state of the art for applying MA technology in IDS. Then we present our MA-IDS architecture and detail methods of local intrusion detection and distributed intrusion detection. The structure and security architecture of MAs are expatriated emphatically in Section 3. Finally we demonstrate the advantages of our MA-IDS architecture and our future research contents.","Mobile agents,
Intrusion detection,
Application software,
Information security,
Educational institutions,
Venus,
Computer science,
Collaboration,
Prototypes,
Art"
"A new (t, n) threshold image hiding scheme for sharing a secret color image","The objective of this research is to develop a method to hide a secret color image inside a set of meaningful color images. It allows a group of n members to share the secret image such that any t or more members can cooperatively construct the secret color image, but (t-1) or less members cannot. In addition, this technique has some outstanding properties. The embedded image can be 512 colors, and the size can be as big as that of the cover images. On the other hand, prior schemes have a secret weakness that some members may cheat other members to obtain their shares. Our new scheme can withstand the cheating attack. Therefore, the new technique is superior to previous works. Finally, our experimental results will also reveal that the distortion between our stego-image and cover image is inconspicuous.","Color,
Cryptography,
Steganography,
Data mining,
Digital images,
Protection,
Data security,
Information security,
Computer science,
Internet"
Female graduate students and program quality,"Despite rising numbers of women in computer science (CS) graduate programs, women still comprised less than one-third of enrolled students in those programs in recent years. Efforts to raise the level of female representation resulted in mixed success. Departments where faculty reported putting the most effort into recruiting female graduate students generally enrolled women in no greater, and sometimes smaller, percentages than departments putting little into recruiting women. A look at some departmental characteristics and practices, related to women's proportion of graduate enrollment in CS, indicates that women's representation is actually lowest in departments where faculty are of the highest scholarly quality, and that this situation may be affected by some recruitment and admissions procedures.","Recruitment,
Database systems,
Computer science,
Telephony,
Postal services,
Best practices,
Application software,
Data engineering,
Councils,
Springs"
A pattern for an effective class responsibility collaborator (CRC) cards,"Class responsibility collaborator (CRC) cards can be used in developing object-oriented models. They provide a simple and an easy to use way to explore objects in the development of a system. However, different problems might arise while adopting current CRC-Cards in identifying the system objects. In this paper, we discuss the main problems with current CRC-Cards. As a solution, we propose a new look at CRC-cards that try to avoid most of the problems presented in traditional cards.","Collaboration,
Cyclic redundancy check,
Object oriented modeling,
Stability,
Educational programs,
Education,
Computer languages,
Programming profession,
Writing,
Filling"
Automatic Type-Driven Library Generation for Telescoping Languages,"Telescoping languages is a strategy to automatically generate highly-optimized domain-specific libraries. The key idea is to create specialized variants of library procedures through extensive offline processing. This paper describes a telescoping system, called ARGen, which generates high-performance Fortran or C libraries from prototype Matlab code for the linear algebra library, ARPACK. ARGen uses variable types to guide procedure specializations on possible calling contexts. ARGen needs to infer Matlab types in order to speculate on the possible variants of library procedures, as well as to generate code. This paper shows that our type-inference system is powerful enough to generate all the variants needed for ARPACK automatically from the Matlab development code. The ideas demonstrated here provide a basis for building a more general telescoping system for Matlab.","Prototypes,
Optimizing compilers,
Program processors,
Linear algebra,
Computer science,
Power generation,
Research and development,
Production,
Software libraries,
Permission"
Supporting sliding window queries for continuous data streams,"Although traditional databases and data warehouses have been exploited widely to manage persistent data, a large number of applications from sensor network need functional supports for transient data in the continuous data stream. One of the crucial functions is to summarize the data items within a sliding window. A sliding window contains a fixed width span of data elements. The data items are implicitly deleted from the sliding window, when it moves out of the window scope. Several one-dimensional histograms have been proposed to store the succinct time information in a sliding window. Such histograms, however, only handle the data items with attribute values in unary domains. In this paper, we explore the problem of extending the value to a multi-valued domain. A two-dimensional histogram, the hybrid histogram, is proposed to support sliding window queries on a practical multi-valued domain. The basic building block of the hybrid histogram is the exponential histogram. The hybrid histogram is maintained to capture the changes of data distribution. To further compress the exponential histograms, we propose a condensed exponential histogram without losing the error bound. Results of an extensive experimental study are included to evaluate the benefits of the proposed technique.","Histograms,
Databases,
Data warehouses,
Windows,
Wireless sensor networks,
Temperature sensors,
Lightning,
Radar measurements,
Optical reflection,
Computer science"
Global multiprocessor scheduling of aperiodic tasks using time-independent priorities,"We provide a constant time schedulability test for a multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing the tasks in two priority classes based on task utilization: heavy and light. We prove that if the load on the multiprocessor server stays below U/sub threshold/ = 3 - /spl radic/7 = 35.425%, the server can accept incoming aperiodic tasks and guarantee that the deadlines of all accepted tasks will be met. 35.425% utilization is also a threshold for a task to be characterized as heavy. The bound U/sub threshold/ = 3 - /spl radic/7 = 35.425% is easy-to-use, but not sharp if we know the number of processors in the multiprocessor. For a server with m processors, we calculate a formula for the sharp bound U/sub threshold/(m), which converges to Uthreshold from above as m - -. The results are based on a utilization function u/sub m/(x) = 2(1 - x)/(2 + /spl radic/(2 + 2x)) + x/m. By using this function, the performance of the multiprocessor can in some cases be improved beyond U/sub threshold/ (m) by paying the extra overhead of monitoring the individual utilization of the current tasks.",
Fuzzy oscillometric blood pressure classification,"Classification of systolic, mean and diastolic blood pressure profiles using the oscillometric method is a difficult process. Generally, the algorithms aim at extracting some parameters such as height, and ratios of the pulses at certain pressure levels, which are obtained from the cuff pressure. These parameters can be used to form profiles to relate to blood pressures. The effectiveness of the classification depends on many factors, such as environmental noise, white coat effect, heart rate variability and motion artifacts. In this paper, we investigate the effectiveness of a neuro-fuzzy approach to blood pressure classification. We employ feature extraction using principal component analysis, and fuzzy sets to classify pressure profiles.","Blood pressure,
Pressure measurement,
Computer science,
Feature extraction,
Hypertension,
Ultrasonic variables measurement,
Arteries,
Educational institutions,
Working environment noise,
Heart rate variability"
Managing data mappings in the Hyperion project,"We consider the problem of mapping data in peer-to-peer systems. Such systems rely on simple value searches to locate data of interest. However, different peers may use different values to identify or describe the same data. To accommodate this, peer-to-peer systems often rely on mapping tables that list pairs of corresponding values for search domains that are used in different peers. We illustrate how such tables are used in the genomics community by expert curators. We then argue why mapping tables are appropriate for data mapping in a peer-to-peer environment and motivate the problem of managing these tables. The work presented is part of the Hyperion project.","Project management,
Peer to peer computing,
Genomics,
Bioinformatics,
Biological information theory,
Computer science,
Environmental management,
Data privacy,
Data structures,
Databases"
A simulation-based analysis of scheduling policies for multimedia servers,"Multimedia-on-demand (MOD) has grown dramatically in popularity, especially in the domains of education, business, and entertainment. Therefore, the investigation of various alternatives to improve the performance of MOD servers has become a major research focus. The performance of these servers can be enhanced significantly by servicing multiple requests from a common set of resources. The exploited degrees of resource sharing depend greatly on how servers schedule the waiting requests. By scheduling the requests intelligently, a server can support more concurrent customers and can reduce their waiting times for service. In this paper we provide a detailed analysis of existing scheduling policies and propose two new policies, called quantized first-come-first-served (QFCFS) and enhanced minimum idling maximum loss (IML/sup +/). We demonstrate the effectiveness of these policies through simulation and show that they suit different patterns of customer waiting tolerance.","Analytical models,
Resource management,
Broadcasting,
Motion pictures,
Bandwidth,
Computational modeling,
Processor scheduling,
Computer science,
Computer science education,
Communications technology"
Variational expectation-maximization training for Gaussian networks,This paper introduces variational expectation-maximization (VEM) algorithm for training Gaussian networks. Hyperparameters model distributions of parameters characterizing Gaussian mixture densities. The proposed algorithm employs a hierarchical learning strategy for estimating a set of hyperparameters and the number of Gaussian mixture components. A dual EM algorithm is employed as the initialization stage in the VEM-based learning. In the first stage the EM algorithm is applied on the given data set while the second stage EM is used on distributions of parameters resulted from several runs of the first stage EM. Appropriate maximum log-likelihood estimators are considered for all the parameter distributions involved.,"Parameter estimation,
Bayesian methods,
Radial basis function networks,
Uncertainty,
Computer science,
Approximation algorithms,
Kernel,
Probability distribution,
Distributed computing,
Stochastic processes"
An S-band frequency-modulated continuous-wave boundary layer profiler: Description and initial results,"Operating principles of frequency-modulated continuous-wave (FMCW) radar are reviewed, and their measurement limitations for atmospheric targets are discussed. In particular, we outline misregistration errors due to Doppler velocities and target coherence limitations on range resolution. The latter is of importance to volumetric scattering from atmospheric targets. Parallax errors and near-field operation are also considered. A high-resolution S-band FMCW radar developed at the University of Massachusetts is then described, and observations obtained by this system during the 1999 Cooperative Atmosphere-Surface Exchange Study are used to illustrate system performance. In the convective boundary layer, Rayleigh scatterers appear to dominate the observed vertical profile of mean reflectivity at S-band.",
Relation between Kansei words and the room space in digital traditional Japanese crafting system,"In Japan, there are traditional Japanese craftings such as fittings, furniture, textiles, etc., which are closely related to Japanese culture and life. The design of Japanese houses and their rooms is a very important problem to attract people. However, this design is not easy because people have different feelings. In order to deal with this problem, we have developed a Digital Traditional Japanese Crafting System (DTJCS) which provides a presentation space for traditional Japanese crafting. In this paper, we analyze the effect of room change of fittings to people's feelings using the presentation system.","Image retrieval,
Textiles,
Information retrieval,
Shape,
Product design,
Design methodology,
Image databases,
Information science,
Database systems,
Information technology"
Soft(ware) skills in context: corporate usability training aiming at cross-disciplinary collaboration,"Employing user-centered instructional design methodology, we developed a usability training workshop for developers which has remarkable impact on participants' attitudes towards the cross-disciplinary collaboration in the software development team. Based mainly on learning experiences during a simulation game, participants gain insights regarding typical pitfalls and opportunities of collaboration with user experience specialists and other non-technical professionals. Rather than teaching abstract, high-level usability principles, we use the ""reflected practice"" approach as a guiding workshop theme, in order to achieve lasting effects for the succeeding professional practice of software development teams. We include evaluative data based on participants' feedback and in-class statements.","Usability,
Collaboration,
Collaborative work,
Collaborative software,
Design methodology,
Programming,
Software engineering,
Education,
State feedback,
Computational modeling"
Behavioral verification of distributed concurrent systems with BOBJ,"Following a brief introduction to classical and behavioral algebraic specification, this paper discusses the verification of behavioral properties using BOBJ, especially its implementation of conditional circular coinductive rewriting with case analysis. This formal method is then applied to proving correctness of the alternating bit protocol, in one of its less trivial versions. We have tried to minimize mathematics in the exposition, in part by giving concrete illustrations using the BOBJ system.","Algebra,
Protocols,
Concurrent computing,
Logic functions,
Computer science,
Supercomputers,
Mathematics,
Concrete,
Software systems,
Hardware"
Querying spatiotemporal XML using DataFoX,"We describe DataFoX, which is a new query language for XML documents and extends Datalog with support for trees as the domain of the variables. We also introduce for DataFoX a layer algebra, which supports data heterogeneity at the language level, and several algebra-based evaluation techniques.","Spatiotemporal phenomena,
XML,
Database languages,
Algebra,
Markup languages,
Relational databases,
Tree data structures,
Query processing,
Computer science,
Data engineering"
A reservation-based multiple access protocol with collision avoidance for wireless multihop ad hoc networks,"A flexible and effective adaptive acquisition collision avoidance (AACA) multiple access protocol is proposed. It integrates the concept of multichannel and random reservation with piggyback to effectively solve hidden terminal and exposed terminal problems caused by the multihop architecture. In the protocol, every node adaptively reserves an idle traffic channel by request-to-send and clear-to-send (RTS/CTS) dialogue on the common channel. After successful reservation, the packet transmission of related other nodes do not interrupt other nodes. The protocol can use any number of channels. It performs better than the single channel RTS/CTS protocol under the assumption of the same total bandwidth if the number of the channels is not too large experiments.","Access protocols,
Collision avoidance,
Spread spectrum communication,
Ad hoc networks,
Media Access Protocol,
Contracts,
Computer science education,
Educational programs,
Intersymbol interference,
Computer architecture"
Evaluation of Cache-based Superscalar and Cacheless Vector Architectures for Scientific Computations,"The growing gap between sustained and peak performance for scientific applications is a well-known problem in high end computing. The recent development of parallel vector systems offers the potential to bridge this gap for many computational science codes and deliver a substantial increase in comput-ing capabilities. This paper examines the intranode performance of the NEC SX-6 vector processor and the cache-based IBM Power3/4 superscalar architectures across a number of scientific computing areas. First, we present the performance of a microbenchmark suite that examines low-level machine characteristics. Next, we study the behavior of the NAS Parallel Benchmarks. Finally, we evaluate the performance of several scientific computing codes. Results demonstrate that the SX-6 achieves high performance on a large fraction of our applications and often significantly outperforms the cache-based architectures. However, certain applications are not easily amenable to vectorization and would require extensive algorithm and implementation reengineering to utilize the SX-6 effectively.","Computer architecture,
National electric code,
Scientific computing,
High performance computing,
Concurrent computing,
Computational modeling,
Government,
Physics computing,
Laboratories,
Plasma applications"
Parametric image formation using clustering for dynamic cardiac SPECT,"Dynamic cardiac SPECT imaging can provide quantitative and possibly even absolute measures of physiological parameters. However, a dynamic cardiac SPECT study involves a number of steps to obtain estimates of physiological parameters of interest. One of the key steps involves the selection of regions of interest. In the past, this has been done manually or by using a semi-automatic method. We propose to use cluster analysis to segment the data to obtain improved parameter estimates. The algorithm consists of using a standard k-means clustering followed by a blood input fine-tuning procedure using fuzzy k-means performed to obtain a more accurate blood input function. Computer simulations were used to test the algorithm and to compute bias in kinetic rate parameters with and without the use of blood input fine-tuning. This was followed by performing eight studies in three canines and three studies in two patients with a dynamic cardiac perfusion SPECT protocol. The short-axis slice image data were used as input for the cluster analysis program as well as for a previously validated semi-automatic method. All of the time activity curves were fit to a two-compartment model. Parametric images of the wash-in rate parameter were obtained after cluster analysis. The wash-in rate estimates from the selected regions of interest with both of the methods were compared using microsphere derived flows as a gold standard in the case of canine studies. Our results suggest that in regions with low noise, cluster analysis provides parameter estimates comparable to the semi-automatic method in addition to providing improved visual defect localization and contrast. Moreover, the clustered curves have less noise and yield reasonable fits where with the semi-automatic method the fitting routine sometimes failed to converge. The use of clustering also required less manual intervention than the semi-automatic method. These results indicate that use of clustering may bring dynamic cardiac SPECT closer to clinical feasibility.","Parameter estimation,
Blood,
Clustering algorithms,
Image analysis,
Computer simulation,
Testing,
Kinetic theory,
Protocols,
Gold,
Curve fitting"
LSRP: local stabilization in shortest path routing,,"Routing protocols,
Internet,
Computer crashes,
Network topology,
Intelligent networks,
Computer networks,
Information science,
Stability,
Scalability,
Large-scale systems"
Hybrid OVSF code assignment scheme in W-CDMA,This paper focuses on an analysis of an orthogonal-variable-spreading-factor (OVSF) codes assignment scheme called the hybrid OVSF-code assignment (HCA). We propose HCA as an alternative solution for OVSF code assignment in order to mitigate so-called code blocking in OVSF-CDMA and to correspond to each service equally. We present the performance evaluation of HCA by computer simulations.,"Multiaccess communication,
Mobile communication,
Multimedia communication,
Computer science,
Telephony,
Performance evaluation,
Computer simulation,
Traffic control,
Cellular phones,
Multimedia systems"
Fast algorithms for common-multiplicand multiplication and exponentiation by performing complements,"The multiplications of common multiplicands and exponentiations of large integers with large modulus are the primary computation operations in several well-known public key cryptosystems. The Hamming weight of the multiplier or the exponent plays an important role for computation efficiency. By performing complements, the Hamming weight of an integer can be reduced. Based on this concept, we propose efficient algorithms for common-multiplicand multiplications (CMM) and exponentiations. In the average case, it takes k/2+2/spl times/log(k)+5 k-bit additions to compute the CMM. For exponentiation, the proposed method takes 5k/4+2 multiplications on average, but the pre-computation for a modular multiplicative inverse is required. Combining the original CMM, the number of multiplications can further be reduced to 9k/8+2.","Coordinate measuring machines,
Hamming weight,
Public key cryptography,
Costs,
Computer science,
Equations"
Developmental Constraints Aid the Acquisition of Binocular Disparity Sensitivities,"This article considers the hypothesis that systems learning aspects of visual perception may benefit from the use of suitably designed developmental progressions during training. We report the results of simulations in which four models were trained to detect binocular disparities in pairs of visual images. Three of the models were developmental models in the sense that the nature of their visual input changed during the course of training. These models received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-scale-to-multiscale developmental progression, another used a fine-scale-to-multiscale progression, and the third used a random progression. The final model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the two developmental models whose progressions were organized by spatial frequency content consistently outperformed the nondevelopmental and random developmental models. We speculate that the superior performance of these two models is due to two important features of their developmental progressions: (1) these models were exposed to visual inputs at a single scale early in training, and (2) the spatial scale of their inputs progressed in an orderly fashion from one scale to a neighboring scale during training. Simulation results consistent with these speculations are presented. We conclude that suitably designed developmental sequences can be useful to systems learning to detect binocular disparities. The idea that visual development can aid visual learning is a viable hypothesis in need of study.",
Laboratory tools for robotics and automation education,"This paper describes our efforts and plans to develop a Virtual Laboratory for the education in Robotics and Automation. These efforts are characterized by the need of blending R&A subjects into a traditional Computer Science curriculum, thus forcing a specific selection of development topics. In this context, the Robotics Laboratory must provide the basic as well as advanced experiments, to address the needs of students at different education levels. In this paper, we present the development of three main applications, to support Control Systems and Robotics classes, as well as the thesis and dissertation research. Of particular interest is the effort in the area of teleoperation, preliminary to the opening (next year) of a new curriculum on Medical Informatics, in which Computer Assisted Surgery will play an important role.","Laboratories,
Robotics and automation,
Computer science education,
Computer science,
Educational robots,
Application software,
Medical control systems,
Robot control,
Control systems,
Medical robotics"
Real-time scheduling for multi headed placement machine,"This paper proposes a methodology for real-time scheduling to sequence the pickup and placement of component on multi headed placement machines in printed circuit board (PCB) assembly. The latest technology of surface mount device (SMD) placement machines have a smart feeder carrier that automatically identifies the exact location of each component type on the feeder slot, detects component's missing from the component feeders (and continues working with other component types) and allows a component to be reloaded during a pick and place operation without stopping the operation. Assuming that the components on the feeder carrier may be misplaced or some of the required components are missing, we generate an initial schedule using a greedy constructive heuristic by only considering the available placement points. The initial solution can immediately be used to assemble components for the first PCB. While the placement machine is assembling components, we employ the CPU free time (whilst the robot arm is moving) to improve the initial schedule by using a randomised hill climbing search technique. Thus, the subsequent PCB's will use the improved schedule. Our experimental result on two data sets show that we gain 58.79% and 76.69% (respectively) improvement on assembly cycle time over the initial schedule.","Processor scheduling,
Robotic assembly,
Magnetic heads,
Printed circuits,
Optimization methods,
Computer science,
Surface-mount technology,
Robots,
Electronic switching systems,
Electronic components"
Balancing energy saving and QoS in the mobile Internet: an application-independent approach,"The scarcity of energetic resources in mobile computers is a very limiting factor. In this paper we propose a solution that tries to balance energy consumption and QoS requirements. Our solution follows an application-independent approach and, therefore, it can be used concurrently, and without modifications, by any network application. Furthermore, our solution is independent from the sub-network technology. We implemented this solution and we extensively tested it. Experimental results have shown that a relevant energy saving (about 70% on average) can be achieved with respect to the legacy approach based on the TCP/IP protocol stack. Furthermore, these savings are obtained without a significant degradation in the QoS perceived by the user. We also compared our application-independent approach with an application-dependent one (i.e., a solution tailored to Web browsing) which performs (slightly) better. However, the application-independent solution still guarantees significant savings, and fits better a general-purpose mobile environment.","Batteries,
Application software,
Mobile computing,
Energy consumption,
Protocols,
Web and internet services,
Communication system security,
Power system security,
Energy management,
Power system management"
Tool set implementation for scenario-based multithreading of UML-RT models and experimental validation,"This paper presents our tool set implementation for scenario-based multithreading of object-oriented real-time models and an accompanying experimental validation. Our tools enable the automated, schedulability-aware implementation of real-time object-oriented models, exploiting an existing CASE tool. Our implementation is facilitated by (1) our customized runtime system modified to support scenario-based thread execution, (2) a design model template that centralizes the arrival of external inputs, (3) a model analyzer tool, and (4) a model-specific code modifier tool. Our tools simplify design by removing thread-related design concerns from the modeling process, separating design and implementation. We performed validation by conducting experiments that clearly demonstrate the performance improvements that can be gained through our scenario-based implementation: response time improvements for high priority tasks of as much as 70% and a 5-fold decrease in blocking or the elimination of blocking for some tasks.","Multithreading,
Object oriented modeling,
Computer aided software engineering,
Yarn,
Application software,
Laboratories,
Real time systems,
Protocols,
Tree data structures,
Computer science"
"Web services, Web searches, and cultural algorithms","Cultural algorithms, a form of evolutionary programming, employ a dual inheritance mechanism at population and knowledge levels to support problem solving, reasoning, and knowledge extraction. Domain knowledge is extracted and separated from individuals within a population and is placed in a belief space. Hierarchical structures employed in the belief space help to accelerate and guide population evolution. The structure of the cultural algorithm lends itself well to a data rich, but knowledge poor distributed environments. Current research, as reported in this paper, is directed toward implementing the belief and population space components as agents interacting with Web services, Web pages, and users. The cultural algorithm framework becomes a mechanism to evolve and refine meaningful search queries that extract significant and useful primary and peripheral information.","Web services,
Cultural differences,
Machine learning algorithms,
Genetic programming,
Data mining,
Web search,
Genetic algorithms,
XML,
Simple object access protocol,
Computer science"
Exact collision detection of two moving ellipsoids under rational motions,"In this paper, we describe an exact method for detecting collision between two moving ellipsoids under pre-specified rational motions. Our method is based on an algebraic condition that determines the separation status of two static ellipsoids - the condition itself is described by the signs of roots of the characteristic equation of the two ellipsoids. To deal with moving ellipsoids, we derive a bivariate function whose zero-set possesses a special topological structure. By analyzing the zero-set of this function, we are able to tell whether or not two moving ellipsoids under pre-specified rational motions are collision-free; and if not, we can determine the intervals in which they overlap.","Motion detection,
Ellipsoids,
Equations,
Object detection,
Computer science,
Motion analysis,
Orbital robotics,
Computer graphics,
Design automation,
Application software"
Energy minimization of real-time tasks on variable voltage processors with transition energy overhead,"In this paper, we address the problem of minimizing energy consumption of real-time tasks on variable voltage processors whose transition energy overhead is not negligible. Voltage settings with minimum number of transitions are found first and sequences of lower voltage cycles are evaluated to decide voltage for each cycle of every task. Experimental results demonstrate that our approach can reduce energy consumed by transitions from 41% to 8% and save more energy.","Voltage,
Timing,
Energy consumption,
Processor scheduling,
Energy management,
Power system management,
Control systems,
Computer science,
Power engineering and energy,
Multiprocessing systems"
Small-world characteristics of the Internet and multicast scaling,"Recent work has shown that the physical connectivity of the Internet exhibits small-world behavior. Characterizing such behavior is important not only for generating realistic Internet topologies, but also for the proper evaluation of network algorithms and protocols. Along this line, this paper tries to answer how small-world behavior arises in the Internet topologies and how it impacts the performance of multicast techniques. First, we attribute small-world behavior to two possible causes: the variability of vertex degree and the preference of vertices to have local connections. We found that both factors contribute with different relative degrees to the small-world behavior of the AS-level and router-level Internet topologies. For the AS-level topology, we have observed that extremely high variability of vertex degree is sufficient to cause small-world behavior, but for the router-level topology, preference for local connectivity plays a more important role. Second, we propose simple models to generate more realistic small-world Internet topologies. Our models consider both causes of small-world behavior. Third, we demonstrate the significance of our work by studying the scaling behavior of IP multicast tree size. We show that if topology generators capture only the variability of vertex degree, they are likely to underestimate the efficacy of multicast techniques.","Internet,
Power system modeling,
Network topology,
Computer science,
IP networks,
Protocols,
Telecommunication network topology,
Computational modeling,
Character generation,
Multicast algorithms"
GA or GP? That is not the question,"Genetic algorithms (GAs) and genetic programming (GP) are often considered as separate but related fields. Typically, GAs use a fixed length linear representation, whereas GP uses a variable size tree representation. This paper argues that the differences are unimportant. Firstly, variable length actually means variable length up to some fixed limit, so can really be considered as fixed length. Secondly, the representations and genetic operators of GA and GP appear different, however ultimately it is a population of bit strings in the computers memory which is being manipulated whether it is GA or GP which is being run on the computer. The important difference lies in the interpretation of the representation; if there is a one to one mapping between the description of an object and the object itself (as is the case with the representation of numbers), or a many to one mapping (as is the case with the representation of programs). This has ramifications for the validity of the No Free Lunch theorem, which is valid in the first case but not in the second. It is argued that due to the highly related nature of GAs and GP, that many of the empirical results discovered in one field will apply to the other field, for example maintaining high diversity in a population to improve performance.",
A mining algorithm for fuzzy weighted association rules,"The association rule mining is an important research subject of knowledge discovery. Aiming at the common method of mining for attributes of quantitative type in database, we analyze the existing defects and put forward a method of applying fuzzy set theory to association rules mining. Due to the problem that each attribute's importance is different in specific purpose mining, we put forward a solution by assigning corresponding weight to attribute of different importance. Based on this idea, we put forward a mining algorithm using fuzzy weighted association rules and through the given experiment we testify the feasibility of the algorithm, and point out the existing defect of the algorithm demanding improvement in future.","Data mining,
Association rules,
Fuzzy sets,
Computer science,
Itemsets,
Transaction databases,
Knowledge engineering,
Power engineering and energy,
Electronic mail,
Data analysis"
Phylogenetic trees using evolutionary search: initial progress in extending Gaphyl to work with genetic data,"Gaphyl is an application of evolutionary algorithms to phylogenetics, an approach used by biologists to investigate evolutionary relationships among organisms. For datasets larger than 20-30 species, exhaustive search is not practical in this domain. Gaphyl uses an evolutionary search mechanism to search the space of possible phylogenetic trees, in an attempt to find the most plausible evolutionary hypotheses, while typical phylogenetic software packages use heuristic search methods. In previous work, Gaphyl has been shown to be a promising approach for searching for phylogenetic trees using data with binary attributes and Wagner parsimony to evaluate the trees. In the work reported here, Gaphyl is extended to work with genetic data. Initial results with this extension further suggest that evolutionary search is a promising approach for phylogenetic work.","Phylogeny,
Genetics,
Sequences,
Computer science,
Educational institutions,
Drives,
Organisms,
DNA,
Application software,
Evolutionary computation"
Addressing useless test data in core-based system-on-a-chip test,"This paper analyzes the test memory requirements for core-based systems-on-a-chips and identifies useless test data as one of the contributors to the total amount of test data. The useless test data comprises the padding bits necessary to compensate for the difference between the lengths of different chains in multiple scan chain designs. Although useless test data does not represent any relevant test information, it is often unavoidable, and leads to the tradeoff between the test bus width and the volume of test data in multiple scan chain-based cores. Ultimately, this tradeoff influences the test access mechanism design algorithms leading to solutions that have either short test time or low volume of test data. Therefore, in this paper, a novel test methodology is proposed which, by dividing the wrapper scan chains (WSCs) into two or more partitions, and by exploiting automated test equipment memory management features, reduces the amount of useless test data. Extensive experimental results using ISCAS'89 and ITC'02 benchmark circuits are provided to analyze the implications of the number of WSCs in the partition, and the number of partitions on the proposed methodology.","System testing,
System-on-a-chip,
Circuit testing,
Automatic testing,
Costs,
Computer science,
Algorithm design and analysis,
Partitioning algorithms,
Test equipment,
Memory management"
Towards an efficient optimal trajectory planner for multiple mobile robots,"In this paper, we present a real-time algorithm that plans mostly optimal trajectories for multiple mobile robots in a dynamic environment. This approach combines the use of a Delaunay triangulation to discretise the environment, a novel efficient use of the A* search method, and a novel cubic spline representation for a robot trajectory that meets the kinematic and dynamic constraints of the robot. We show that for complex environments the shortest-distance path is not always the shortest-time path due to these constraints. The algorithm has been implemented on real robots, and we present experimental results in cluttered environments.",
Runtime software architecture based software online evolution,"Runtime environment of software are becoming more and more dynamic and changeful, while pervasive computing and Web services further this situation. Software systems are not only becoming larger, more complex, and also more rigid, which make it difficult to evolve software. This paper focuses on online evolution, more exactly, how to make online evolution process convenient and smart, with help of runtime software architecture (RSA). Following issues are discussed in this paper: types of software environment changes, the incarnation of RSA, retrieval and manipulation of RSA, the relation between RSA and the runtime system, and a visual tool to show RSA, and make evolution process more easy and intuitionist.","Software architecture,
Software maintenance,
Software systems,
Runtime environment,
Control systems,
Pervasive computing,
Web services,
Space technology,
Computer science,
Software tools"
Evolving Turing Complete representations,"Standard GP, chiefly concerned with evolving functions, which are mappings from inputs to output, is not Turing Complete. We raise issues resulting from attempts at extending standard GP to Turing Complete representations. Firstly, there is a problem when a contiguous piece of code is moved to a new location (in a different program) by crossover. In general its functionality will be altered if global memory is used, as other parts of the program may access the same piece of memory. Secondly, traditional crossover does not respect modules. Crossover can disrupt a group of instructions that were working together (e.g. in the body of a loop) in one parent, but end up separated in two different offspring after reproduction. A crossover operator is proposed that only operates at the boundaries of modules. The identification of module boundaries is made easy by using a representation in which explicit modules are denned, in contrast with other representations where the module boundaries would have to be identified by some other means. The halting problem is a central issue, however as a consequence of this crossover operator we are more likely to produce self terminating programs, thus saving time when testing.","Genetic programming,
History,
Space exploration,
Computer science,
Code standards,
Ice,
Automatic testing,
Biological information theory,
Genetic mutations,
Computational modeling"
Intrusion behavior detection through visualization,"As computer and network intrusions become more and more of a concern, the need for better capabilities to assist in the detection and analysis of intrusions also increases. We propose a methodology for analyzing network and computer log information visually based on the analysis of user behavior. Each user's behavior is the key to determining their intent and overriding goals, whether they attempt to hide their actions or not. Proficient hackers will attempt to hide their ultimate goal, which hinders the reliability of log file analysis. Visually analyzing the user's behavior, however, is much more adaptable and difficult to counteract. This paper will discuss how user behavior can be exhibited within the visualization techniques, the capabilities provided by the environment, typical characteristics users should look out for (i.e., how unusual behavior exhibits itself), and exploration paradigms effective for identifying the meaning behind the user's behavior.","Visualization,
Intrusion detection,
Forensics,
Computer networks,
Information analysis,
Pattern matching,
Computer science,
Computer hacking,
Computer network reliability,
Computer security"
Efa: an efficient content routing algorithm in large peer-to-peer overlay networks,"An important issue for peer-to-peer application is to locate content within the network. There are many existing solutions to this problem, however, each of them addresses different aspects and each has its deficiencies. We focus on the unstructured peer-to-peer scenario and present a constrained flooding routing algorithm, Efa, which overcomes some of the deficiencies of those existing strategies. Efa performs application level broadcasting in a potentially very large peer-to-peer network overlaid on the Internet. Efa is completely decentralized and self-organized. It is a more scalable alternative to flooding, which is commonly used in unstructured peer-to-peer systems. Utilizing just a small amount of topology info, Efa is almost as simple as flooding, but it is much more efficient and scalable.",
DOpE - a window server for real-time and embedded systems,"A window server used in real-time applications should be able to assure previously agreed-upon redrawing rates for a subset of windows while providing best-effort services to the remaining windows and operations such as moving windows. A window server used in embedded systems should be small and require only minimal operating system support, for example just threads and address spaces as provided by microkernels. In this paper, we present the design and an implementation of the DOpE window server. The key techniques used are to move redrawing responsibility from client applications to the window server and to devise a simple scheduling discipline for the redrawing subtasks.","Real time systems,
Embedded system,
Delay,
Operating systems,
Quality of service,
Bandwidth,
Resource management,
Computer science,
Application software,
Yarn"
A tool for automatic flow analysis of C-programs for WCET calculation,"Bounding the worst case execution time (WCET) of programs is essential for real-time systems. To be able to do WCET calculations, the iteration bounds for loops and recursion must be known. We describe a prototype tool that calculates these bounds automatically, thereby avoiding the need for manual annotations by the programmer. The analysis is based on an intermediate code representation, which means that compiler optimized code is analyzed. The choice of intermediate code also allows the analysis to support the number of programming languages. C programs are targeted. We also show an example of a program analysis using our method.","Real time systems,
Programming profession,
Hardware,
Information analysis,
Prototypes,
Time measurement,
Computer science,
Electronic mail,
Optimizing compilers,
Computer languages"
Asymptotic distribution of the number of isolated nodes in wireless ad hoc networks with Bernoulli nodes,"Nodes in wireless ad hoc networks may become inactive or unavailable due to, for example, internal breakdown or being in the sleeping state. The inactive nodes cannot take part in routing/relaying and thus may effect the connectivity. A wireless ad hoc network containing inactive nodes is then said to be connected if each inactive node is adjacent to at least one active node and all active nodes form a connected network. This paper is the first installment of our probabilistic study of the connectivity of wireless ad hoc networks containing inactive nodes. We assume that the wireless ad hoc network consists of n nodes, which are distributed independently and uniformly in a unit-area disk and are active (or available) independently with probability p for some constant 0 < p /spl les/ 1. We show that if all nodes have a maximum transmission radius r/sub n/ = /spl radic/(ln n+c//spl pi/pn) for some constant c, then the total number of isolated nodes is asymptotically Poisson with mean e/sup -c/ and the total number of isolated active nodes is also asymptotically Poisson with mean pe/sup -c/.","Intelligent networks,
Ad hoc networks,
Mobile ad hoc networks,
Relays,
Computer science,
Isolation technology,
Electric breakdown,
Routing,
Transceivers,
Transmitting antennas"
Augmented sifting of multiple-valued decision diagrams,"Discrete functions are now commonly represented by binary (BDD) and multiple-valued (MDD) decision diagrams. Sifting is an effective heuristic technique which applies adjacent variable interchanges to find a good variable ordering to reduce the size of a BDD or MDD. Linear sifting is an extension of BDD sifting where XOR operations involving adjacent variable pairs augment adjacent variable interchange leading to further reduction in the node count. In this paper, we consider the extension of this approach to MDDs. In particular, we show that the XOR operation of linear sifting can be extended to a variety of operations. We term the resulting approach augmented sifting. Experimental results are presented showing sifting and augmented sifting can be quite effective in reducing the size of MDDs for certain types of functions.","Binary decision diagrams,
Computer science,
Logic,
Cost function,
Councils,
Input variables"
Reinforcement learning for hierarchical and modular neural network in autonomous robot navigation,"This work describes an autonomous navigation system based on a modular neural network. The environment is unknown and initially the system does not have ability to balance two innate behaviors: target seeking and obstacle avoidance. As the robot experiences some collisions, the system improves its navigation strategy and efficiently guides the robot to targets. A reinforcement learning mechanism adjusts parameters of the neural networks at target capture and collision moments. Simulation experiments show performance comparisons. Only the proposed system reaches targets if the environment presents a high risk (dangerous) configuration (targets are very close to obstacles).","Learning,
Neural networks,
Intelligent networks,
Navigation,
Robot kinematics,
Control systems,
Motion planning,
Robot sensing systems,
Intelligent systems,
Computer science"
Microarray gene expression data association rules mining based on JG-Tree,"The main techniques currently employed in analyzing microarray expression data are clustering and classification. In this paper we propose to use association rules to mine the association relationships among different genes under the same experimental condition. These kinds of relations may also exist across many different experiments with various experimental conditions. In this paper, a new approach, called LIS-growth (Large ItemSet growth) tree, is proposed for mining the microarray data. Our approach uses a new data structure, JG-tree (Jiang, Gruenwald), and a new data partition format for gene expression level data. Each data value can be presented by a sign bit, fractions and exponent bits. Each bit at the same position can be organized into a JG-tree. A JG-tree is a lossless and compression tree. It can be built on fly, a kind of real-time compression for bits string. Based on these two new data structures it is possible to mine the association rules efficiently and quickly from the gene expression database. Our algorithm was tested using the real-life datasets from the gene expression database at Stanford University.","Gene expression,
Association rules,
Data mining,
Genomics,
Bioinformatics,
DNA,
Computer science,
Data analysis,
Itemsets,
Data structures"
Software for taking notes in class,"Although digital devices are replacing paper and pencil in ever more domains, class notes have so far resisted this trend. In part this is because class notes and the note-taking process are unique and different from the tasks supported by existing software. A system to support classroom note-taking should embody seven design principles. A prototype system based on these principles was used in the laboratory and in the classroom by three long-term users. Some users prefered note-taking with the system over pencil and paper, suggesting that taking lecture-notes with the computer is feasible.","Military computing,
Personal digital assistants,
Computer science,
Application software,
Hardware,
Educational technology,
Psychology,
Encoding"
An algorithmic approach for generic parallel adders,"Binary addition is the most fundamental and frequently used operation. A well-designed adder should be fast and satisfy the application requirements. We propose an algorithmic approach to generate an irregular parallel-prefix adder, which has minimal delay for a given profile of input signals. It can cover different topologies such as ripple-carry, carry-skip and carry-select adders. Compared with Kogge-Stone and Brent-Kung adders, the results of the proposed approach have the smallest output delay.","Delay,
Arithmetic,
Hardware,
Permission,
Computer science,
Application software,
Application specific integrated circuits,
Algorithm design and analysis,
System performance,
Computer architecture"
Generalized construction of binary bent sequences with optimal correlation property,"We generalize the construction method of the family of binary bent sequences introduced by Olsen, Scholtz, and Welch (1982) to obtain a family of generalized binary bent sequences with optimal correlation and balance property by using the modified trace transform. Then, the conventional binary bent sequence becomes a special case of our construction method. Several families of the generalized binary bent sequences are constructed by using the bent functions on the intermediate field. Using some of the generalized binary bent sequences, new families of binary sequences with optimal correlation and balance property can be constructed by the lifting idea similar to No (1988) sequences, which are referred to as binary bent-lifted sequences.",
A carrying task for nonprehensile mobile manipulators,"Manipulation is an essential capability for mobile robots to perform many useful tasks. Our focus has been on mobile robots with nonprehensile (i.e., nongrasping) manipulators. The robots are equipped with a flat ""palm"" with two degrees of freedom. We have tackled the problem of carrying an object using two such mobile manipulators. Since these manipulators cannot grasp an object, each robot must support one end. However, if errors cause the separation between robots to change, the robots will drop the object. In this paper, we describe an algorithm to maintain the object contact at a nominal position on the palms by performing corrective actions. We first present analysis of the system mechanics, formulate both a centralized and a distributed algorithm for this task, and then show results of our experimental implementation.","Manipulators,
Mobile robots,
Algorithm design and analysis,
Distributed algorithms,
Shape,
Robot sensing systems,
Computer science,
Performance analysis,
Cities and towns,
Planets"
A field study of use of synchronous chat in online courses,"A field study of computer mediated communication (CMC) as used in higher education asks the questions, ""Will students take part in synchronous chat sessions if they are scheduled? "" and ""What do students and faculty perceive to be the problems and the advantages of synchronous chat sessions?"" media mode is the independent variable, characterized by four nominal values derived from the mixture of asynchronous discussion forums, here called asynchronous learning networks (ALN), with various levels of synchronous media use. Data were collected from 29 course sections, for which instructors were interviewed, students were surveyed online to investigate their perceptions of the use of chat in online courses, and university records were used to determine grade distributions. The percentage of students participating in scheduled chat sessions varied from 5% to 50% and many of the instructors report problems with organizing the sessions as well as ideas about how to do it better ""next time."" Instructors were nevertheless generally positive about the potential usefulness of synchronous sessions in terms of their ability to bring the students closer to the instructor. They report some small success in their first chat session and the experience leads to better facilitation in subsequent sessions. Students significantly find chat more 'rewarding' and less 'complex' in classes that scheduled sessions two or more times than students in asynchronous-only classes. The implication is that when students actually use chat they do find it 'rewarding' and not 'complex.' Given the problems with implementation of chat sessions, however, it is not surprising that its use is not significantly related to predicted improvements in outcomes for courses.",
Individualized e-learning systems enabled by a semantically determined adaptation of learning fragments,"Nowadays e-learning systems are very popular. Several applications have already been implemented and many project initiatives have been started. Although such systems come with interesting advantages, there are still many unsolved problems. Enriching common learning content by applying multimedia did not meet the general expectation to decrease drop out rates of e-learners using such systems. Additionally, most e-learners complain about a ""one-size-fits-all"" philosophy, a resulting cognitive overload and consequently the lack of personalization of existing applications. In this paper, a user-centric approach is presented in order to improve the usability and acceptance, thus, making e-learning systems more successful. Focusing the e-learners' requirements learning fragments are introduced. Depending on the user's skills, learning styles and learning strategies these learning fragments are individually combined to an e-learning system. The necessary user profile is incrementally determined by observing the users learning activities. Based on these observation results the e-learning system is dynamically adapted to the current user's profile.","Electronic learning,
Psychology,
Multidimensional systems,
Education,
Multimedia systems,
Usability,
Computer science,
Humans,
Investments,
Conferences"
A learning objects approach to teaching programming,"The goal of this paper is to describe a new approach to a content creation and delivery mechanism for a programming course. This approach is based on the concept of creating a large repository of learning objects, each of which consists of the core material, code examples, supplementary notes, and review questions. A learning object will be uniquely described by a XML document and presents an interface for future search, retrieval and updating, as well as for potential connection to external assessment tools. Furthermore, we describe a new teaching, learning and authoring tool (called adaptive book) that allows users to add new learning objects, modify current ones, and discuss concepts using a variety of representation models. With the adaptive book, an instructor will be able to design his or her very own course using a large repository of material, which will target a particular audience or a customized syllabus. In addition, the electronic adaptive book will serve as an interactive, continuously up-to-date learning environment for students. It will allow students to create personal learning profiles that are embedded into the core content of the course. We believe that in the near future, this electronic adaptive book will have the potential to replace or enhance traditional paper textbooks. In all physicality, it is impossible to have a personal instructor for each student, but with the adaptive book, we believe we are one step closer to realizing the dream of individualized instructions for each and every student.","Education,
Books,
Computer science,
Mathematical programming,
XML,
Potential well,
Object oriented modeling,
Java,
Information systems,
Systems biology"
The evolution of blackjack strategies,"We investigate the evolution of a blackjack player. We utilise three neural networks (one for splitting, one for doubling down and one for standing/hitting) to evolve blackjack strategies. Initially a pool of randomly generated players play 1000 hands of blackjack. An evolutionary strategy is used to mutate the best networks (with the worst networks being killed). We compare the best evolved strategies to other well-known strategies and show that we can beat the play of an average casino player. We also show that we are able to learn parts of Thorpe's basic strategy.",
On the number of multilinear partitions and the computing capacity of multiple-valued multiple-threshold perceptrons,"We introduce the concept of multilinear partition of a point set V/spl sub/R/sup n/ and the concept of multilinear separability of a function f:V/spl rarr/K={0,...,k-1}. Based on well-known relationships between linear partitions and minimal pairs, we derive formulae for the number of multilinear partitions of a point set in general position and of the set K/sup 2/. The (n,k,s)-perceptrons partition the input space V into s+1 regions with s parallel hyperplanes. We obtain results on the capacity of a single (n,k,s)-perceptron, respectively, for V/spl sub/R/sup n/ in general position and for V=K/sup 2/. Finally, we describe a fast polynomial-time algorithm for counting the multilinear partitions of K/sup 2/.","Computational modeling,
Logic functions,
Computer science,
Logic devices,
Power system modeling,
Polynomials,
Partitioning algorithms,
Information technology,
Computer simulation,
Transfer functions"
PET: enhancing TCP performance over 3G & beyond networks,"TCP flow control algorithms have been designed for reliable links where losses are primarily due to congestion. However, wireless networks suffer from significant packet losses due mainly to bit errors and handoffs. When wireless service is merged with wireline networks in 3G and beyond, TCP's performance will deteriorate further. To mitigate this problem, in this paper we propose a performance enhancing TCP (PET) that decouples loss recovery from congestion control using the enhanced active queue management (AQM) and explicit congestion notification (ECN). PET not only keeps the end-to-end semantics of TCP, but also is capable of performing well over both wireline and wireless links. PET works in two modes: distributed mode and integrated mode, depending on whether all routers in the path are ECN-Capable. We conduct extensive simulations to show that PET provides comparable performance as the standard TCP does over large bandwidth product networks and significantly improves the performance of 3G and beyond networks in the presence of burst errors.",
An emerging marketplace for digital advertising based on amalgamated digital signage networks,"Digital signage networks are a newly emerging form of electronic advertising technology that is rapidly growing in popularity but has received little attention in e-commerce (electronic commerce) literature. We motivate the cost reduction and increased effectiveness of advertising that is displayed and managed using digital signage networks as opposed to traditional signage. We introduce a partially automated intermediary, namely the digital signage exchange that forms a virtual marketplace for the purchase and sale of display time on the digital signage network. We present a concise representation for a customer's order for display time on the digital signage network and an informal business transaction management model for the exchange. This model includes support for advance bulk purchase of blocks of display time. It can also make a counter-offer to the customer when an order cannot be completely fulfilled as requested.","Advertising,
Costs,
Displays,
Marketing and sales,
Internet,
Manufacturing industries,
Airports,
Computer science,
Consumer electronics,
Customer service"
A zero-value prediction technique for fast DCT computation,"The paper proposes a new computationally efficient technique for DCT operation. Unlike related research, the technique reduces the number of computations by predicting the effect of quantization on DCT and avoiding calculations of those DCT values which lead to zero elements in the block after quantization. Experimental evaluation on a number of video benchmarks shows that our method is able to reduce the total number of computations by 29% for DCT and by 59% for quantization while maintaining high image quality.","Discrete cosine transforms,
Quantization,
Image coding,
Transform coding,
Videoconference,
Partitioning algorithms,
Arithmetic,
Computer science,
Image quality,
Discrete transforms"
Performance of UWB communications with imperfect channel estimation,"In this paper, we examine the performance of diversity combining with imperfect channel estimation when the channel is comprised of diffuse as well as specular, or nonfading, components. It is shown that the presence of specular components under certain conditions can reduce the combining loss incurred by large-bandwidth systems when employing imperfect channel estimates and combining a significant number of resolvable paths.","Channel estimation,
Bandwidth,
Signal resolution,
Energy resolution,
Rician channels,
Nakagami distribution,
Fading,
Delay,
Computer science,
Diversity reception"
Jumbo: run-time code generation for Java and its applications,"Run-time code generation is a well-known technique for improving the efficiency of programs by exploiting dynamic information. Unfortunately, the difficulty of constructing run-time code-generators has hampered their widespread use. We describe Jumbo, a tool for easily creating run-time code generators for Java. Jumbo is a compiler for a two-level version of Java, where programs can contain quoted code fragments. The Jumbo API allows the code fragments to be combined at run-time and then executed. We illustrate Jumbo with several examples that show significant speedups over similar code written in plain Java, and argue further that Jumbo is a generalized software component system.","Runtime,
Java,
Program processors,
Automatic programming,
Application software,
Computer science,
Software systems,
Dynamic compiler,
Assembly,
Testing"
Compiler-directed program-fault coverage for highly available Java internet services,,"Program processors,
Java,
Web and internet services,
Software testing,
Operating systems,
Fault diagnosis,
System testing,
Hardware,
Software engineering,
Computer science"
Melting and flowing of viscous volumes,"We present a simple, linear 3D cellular automata approach for animating the melting process of solid volumetric models. Accurate modelling of object melting usually requires complicated physical simulations of heat transfer and phase transition from solid to liquid. Instead, we propose a simplified model to describe the melting behaviors of highly viscous objects, such as wax, lava, plastic, metal and chocolate. We simulate the process by which a volumetric solid transforms into a viscous liquid as the amount of heat it accumulates on its surface reaches a certain temperature. We then animate smooth fluid behavior using a cellular automata. The dynamic volume data is rendered directly on texture mapping hardware to achieve an interactive speed.","Animation,
Solid modeling,
Computational modeling,
Heat transfer,
Microscopy,
Fluid flow,
Temperature,
Equations,
Computer science,
Plastics"
The Web of governance and democratic accountability,"Developments in e-government are resulting in fundamental reorganizations of the ways in which democratic governments operate as well as in the ways in which citizens relate to their own and other governments and to each other. Of special relevance here are the manners in which institutions and citizens are becoming interconnected into a complex 'Web of governance' via largely uncoordinated information networks. This paper examines how this Web of governance is simultaneously producing changes in individual citizen's senses of identity and challenges to conventional notions of accountability in liberal democratic systems. Together, it is argued, these suggest moving focus from e-government (the institutions of government) to e-governance (the larger Web of formal and informal institutions, organizations, norms, traditions, authority structures, groups and behaviors within which individuals and groups live their lives). Such a refocusing holds the promise of developing citizen capacity and identity in balance with formal governmental transformations. Specific illustrative examples are provided including Seoul Metropolitan Government's OPEN System.","Electronic government,
Costs,
US Government,
Computers,
Productivity,
Open systems,
Information technology,
Licenses,
Auditory system,
Internet"
Tracker - a tool for change propagation in Java,"During software evolution, programmers add new functionalities and release new versions of software. This is complicated work, particularly in large applications, and tools are needed to deal with it. In this paper we introduce a tool named JTracker that helps programmers implement change propagation in Java applications. We conducted a case study of a change in open source application JMeter, in which we used JTracker.","Java,
Programming profession,
Open source software,
Application software,
Databases,
Software tools,
Computer science,
Feedback,
Software maintenance"
SEE: a service execution environment for edge services,"The increasing mismatch between the low-bandwidth, resource characteristics of wireless mobile devices and the high-bandwidth expectations of many content-rich services drives the demand for deploying content-oriented services along the data path between the end users and the content servers. We argue that the idea of extending existing caching proxies to support these services is promising. This suggests extending the proxy caches for more than just their original intended purpose, that is the creation of an execution environment within them, which allows the execution of services locally and remotely. We describe the design, implementation and evaluation of a service execution environment (SEE) in the context of the CONCA proxy cache. We also compare the performance of Simple Object Access Protocol (SOAP) and Internet Content Adaptation Protocol (ICAP) by using them as call-out protocols between SEE and the service providers.","Simple object access protocol,
Web and internet services,
Mobile computing,
IP networks,
Privacy,
Scalability,
Computer science,
Drives,
Context-aware services,
Access control"
A method for the evaluation of behavioral fault models,"Many fault models have been proposed which attempt to capture design errors in behavioral descriptions, but these fault models have never been quantitatively evaluated. The essential question which must be answered about any fault model is, ""If all faults in this model are detected, is the design guaranteed to be correct?"" In this paper we present a method to examine the degree to which an arbitrary fault model can ensure the detection of all design errors. The method involves comparing fault coverage to error coverage as defined by a practical design error model which we describe. We have employed our method to perform a limited analysis of the statement and branch coverage fault models.","Fault detection,
Automatic testing,
Costs,
Performance evaluation,
Computer science,
Computer errors,
Error correction,
Performance analysis,
Analytical models,
System testing"
Evaluating distributed checkpointing protocols,"This paper presents an objective measure, called overhead ratio, for evaluating distributed checkpointing protocols. This measure extends previous evaluation schemes by incorporating several additional parameters that are inherent in distributed environments. In particular, we take into account the rollback propagation of the protocol, which impacts the length of the recovery process, and therefore the expected program run-time in executions that involve failures and recoveries. The paper also analyzes several known protocols and compares their overhead ratio.","Checkpointing,
Protocols,
Application software,
Distributed computing,
Costs,
Computer science,
Coordinate measuring machines,
Runtime,
Fault tolerant systems,
Debugging"
Implementation of a calendar application based on SyD coordination links,"System on devices (SyD) is a specification for a middleware to enable heterogeneous collections of information, databases, or devices (such as hand-held devices) to collaborate with each other. This paper illustrates the advantages of SyD by describing a prototype calendar of meetings application. This application highlights some of the technical merits of SyD by exploiting the use of coordination links. Based on the underlying event-and-trigger mechanism, these links allow automatic updates as well as real-time enforcements of global constraints and interdependencies, not available with existing calendar applications. Additionally, the calendar application illustrates coordination among heterogeneous devices and databases, formation and maintenance of dynamic groups, mobility support through proxies, and performance group transactions across independent data stores.","Calendars,
Middleware,
Application software,
Handheld computers,
Databases,
Collaboration,
Prototypes,
Energy management,
Computer science,
Educational institutions"
A self-stabilizing distributed algorithm for minimal total domination in an arbitrary system graph,"In a graph G = (V, E), a set S /spl sube/ V is said to be total dominating if every v /spl isin/ V is adjacent to some member of S. When the graph represents a communication network, a total dominating set corresponds to a collection of servers having a certain desirable backup property, namely, that every server is adjacent to some other server. Self-stabilization, introduced by Dijkstra (1974, 1986), is the most inclusive approach to fault tolerance in distributed systems. We propose a new self-stabilizing distributed algorithm for finding a minimal total dominating set in an arbitrary graph. We also show how the basic ideas behind the proposed protocol can be generalized to solve other related problems.","Distributed algorithms,
Network servers,
Artificial intelligence,
Protocols,
Communication networks,
Fault tolerant systems,
Jacobian matrices,
Computer science,
Propagation delay,
Scheduling algorithm"
Finite-time stabilization of the high-order chained system,"This research focuses on the problem of stabilization of high-order chained-form nonholonomic system. We propose a finite-time discontinuous controller for the chained system. The controller is derived by using a high-order homogeneous finite-time control technique and high-order sliding mode control approach. The proposed method has no feed-forward factors and no singular points where the controller inputs become infinite values. Moreover, we designed the controller for the simplest high-order chained system and confirmed the superior performance of the controlled system by computer simulation.",
An implementation of a hierarchical IP traceback architecture,"The IP traceback technique detects sources of attack nodes and the paths traversed by anonymous DDoS (distributed denial of service) flows with spoofed source addresses. We propose a hierarchical IP traceback architecture, which decomposes the Internet-wide traceback procedure into inter-domain traceback and intradomain traceback. Our proposed method is different from existing approaches in that our method is independent from a single IP traceback mechanism, and domain decomposition is based on existing operational models of the Internet. Moreover, it has the capability of being used for not only the IPv4 network, but also the IPv6 network.","Computer crime,
Web and internet services,
IP networks,
Costs,
Information science,
Information filtering,
Information filters,
Joining processes,
Educational institutions,
Proposals"
On using discretized Cohen-Grossberg node dynamics for model-free actor-critic neural learning in non-Markovian domains,"We describe how multi-stage non-Markovian decision problems can be solved using actor-critic reinforcement learning by assuming that a discrete version of Cohen-Grossberg node dynamics describes the node-activation computations of neural network (NN). Our NN is capable of rendering the process Markovian implicitly and automatically in a totally model-free fashion without learning by how much the state apace must be augmented so that the Markov property holds. This serves as an alternative to using Elman or Jordan-type function as a history memory in order to develop sensitivity to non-Markovian dependencies. We shall demonstrate our concept using a small-scale non-Markovian deterministic path problem, in which our actor-critic NN finds an optimal sequence of actions, although it needs much iteration due to the nature of neural model-free learning. This is, in spirit, a neuro-dynamic programming approach.","Neural networks,
Learning,
Recurrent neural networks,
History,
Dynamic programming,
Neurons,
Signal processing,
Computer science,
Computer networks,
State-space methods"
A role-based conflict resolution method for a collaborative system,"Conflict resolution is one of the most important problems that must be solved in building a collaborative system. In many CSCW (computer-supported-cooperative work) applications or systems, roles were granted in the design and application of these systems. Consequently, we may infer that ""without roles, there would be no collaboration"". Based on the success of RBAC, we found that role-based methods are very useful in building collaborative systems. However, there is little research on conflict resolution based on roles. This paper, briefly introduces an object model for collaborative systems OMCS and a multimedia co-authoring system MCAS, and mainly discusses the role management and a role-based conflict avoidance and resolution method in the system MCAS. It emphasizes the importance of roles in conflict avoidance and resolution. In the last section, it summarizes the paper and draws a conclusion is drawn that a role-based method can help greatly to resolve conflicts in collaborative systems.","Collaboration,
Collaborative work,
Multimedia systems,
Buildings,
Application software,
Computer science,
Mathematics,
Computer applications,
Multiagent systems,
Sun"
Proxy ecology - cooperative proxies with artificial life,"In this paper, we propose a novel P2P cooperative proxy cache system using an individual-based model. We borrow the idea from an ecological system as well as economic systems to manage the cooperative proxies through data and information exchange among individual proxies. The data flow among proxy nodes creates artificial life for the cooperative proxies. The proxy servers with artificial life can automatically configure themselves into a virtual proxy graph. The aggregate effect of caching actions by individual peer proxies automatically distributes the Web document closer to the clients and balances the workload. Our simulation results show that the proposed proxy caching scheme tremendously improves the system performance. In addition, the individual-based design model ensures the simplicity and scalability of the cache system.","Environmental factors,
Network servers,
Web server,
Biological system modeling,
Scalability,
Computer science,
Peer to peer computing,
Internet,
Delay,
Robust stability"
Reengineering a PC-based system into the mobile device product line,"There is a growing demand to port existing PC-based software systems to mobile device platforms. Systems running on mobile devices share basic characteristics with their PC-based counterparts, but differ from them in details of user interfaces, application models, etc. Systems running on mobile devices must also perform well using less memory than PC-based systems. Mobile devices themselves are different from each other in many ways, too. We describe how we made an existing PC-based City Guide System available on a wide range of mobile devices, in a cost-effective way. We applied ""reengineering into a product line architecture"" approach to achieve the goal. Our product line architecture facilitates reuse via generation. We generate specific City Guide Systems for target platforms including PC, Pocket PC and other mobile devices, from generic meta-components that form the City Guide System product line architecture. In our project, we used a meta-programming technique of XVCL to build a product line architecture for City Guide Systems.","Cities and towns,
Java,
Computer architecture,
Mobile handsets,
Personal communication networks,
Mobile computing,
Software systems,
User interfaces,
Computer science,
Application software"
Location privacy in the Alipes platform,"An increasing number of systems use contextual information about their users. Such contextual information can be used to design applications that survey usage and adapt thereafter, or simply just use context information to optimize presentation. Context information could therefore be used to create applications for the benefit of the users of the system, but the same information could cause serious violations of personal integrity if misused. Locality may be the most widely used, but also the most sensitive contextual information. The Alipes platform makes it easy to create location-based services while enforcing user privacy and integrity. The platform handles privacy through rules that describe how and under what circumstances a user's context may be distributed to other users, for example rules describing limitations concerning the user's context, a certain time period, the number of queries and the type of applications. This paper presents how location privacy is enforced in the Alipes platform.","Privacy,
Application software,
Switches,
Computer science,
Design optimization,
Home computing,
Protection,
Sugar,
Physics computing,
Mobile computing"
Parallel mining of maximal frequent itemsets from databases,"In this paper, we propose a parallel algorithm for mining maximal frequent itemsets from databases. A frequent itemset is maximal if none of its supersets is frequent. The new parallel algorithm is named parallel max-miner (PMM), and it is a parallel version of the sequential max-miner algorithm by R.J. Bayardo (1998). Most of existing mining algorithms discover the frequent k-itemsets on the kth pass over the databases, and then generate the candidate (k + 1)-itemsets for the next pass. Compared to those level-wise algorithms, PMM looks ahead at each pass and prunes more candidate itemsets by checking the frequencies of their supersets. We implemented PMM on a cluster of workstations, and evaluated its performance for various cases. PMM demonstrated better performance than other sequential and parallel algorithms, and its performance is quite scalable, even when there are large maximal frequent itemsets (i.e. long patterns) in databases.","Data mining,
Itemsets,
Association rules,
Parallel algorithms,
Transaction databases,
Computer science,
Data engineering,
Clustering algorithms,
Frequency,
Workstations"
An efficient scheduling algorithm for CIOQ switches with space-division multiplexing expansion,"Recently, CIOQ switches have attracted interest from both academic and industrial communities due to their ability of achieving 100% throughput and perfectly emulating OQ switch performance with a small speedup factor S. To achieve a speedup factor S, a conventional CIOQ switch requires the switch matrix and the memory to operate S times faster than the line rate. In this paper, we propose to use a CIOQ switch with space-division multiplexing expansion and grouped inputs/outputs (SDMG CIOQ switch for short) to achieve speedup while only requiring the switch matrix and the memory to operate at the line rate. The cell scheduling problem for the SDMG CIOQ switch is abstracted as a maximum bipartite k-matching problem. Using fluid model, we prove that any maximal size k-matching algorithm on an SDMG CIOQ switch with an expansion factor 2 can achieve 100% throughput assuming input arrivals satisfy the strong law of large numbers and no inputs/outputs are oversubscribed. We further propose an efficient and starvation-free maximal size k-matching scheduling algorithm, kFRR, for the SDMG CIOQ switch. Simulation results show that kFRR achieves 100% throughput with an expansion factor 2 under two SLLN traffic models, uniform traffic and polarized traffic, confirming our analysis.","Scheduling algorithm,
Switches,
Throughput,
Job shop scheduling,
Traffic control,
Packet switching,
Impedance matching,
Iterative algorithms,
Computer science,
Computer industry"
Optimal strategies to track and capture a predictable target,"We present an O(nlog/sup 1+/spl epsiv// n)-time algorithm for computing the optimal robot motion that maintains line-of-sight visibility between a target moving inside a polygon with n vertices which may contain holes. The motion is optimal for the tracking robot (the observer) in the sense that the target either remains visible for the longest possible time, or it is captured by the observer in the minimum time when feasible. Thus, the algorithm maximizes the minimum time-to-escape. Our algorithm assumes that the target moves along a known path. Thus, it is an off-line algorithm. Our theoretical results for the algorithm's runtime assume that the target is moving along a shortest path from its source to its destination. This assumption, however is not required to prove the optimality of the computed solution, hence the algorithm remains correct for the general case.","Target tracking,
Computer vision,
Motion planning,
Service robots,
Computer science,
Runtime,
Application software,
Visual servoing,
Surgery,
Strategic planning"
Evaluation of multiple-output logic functions using decision diagrams,"This paper shows four different methods to evaluate multiple-output logic functions using decision diagrams: Shared BDD (SBDD), Multi-Terminal BDD (MTBDD), BDD for characteristic functions (CF), and BDDs for Encoded Characteristic Function for Non-zero outputs (ECFNs). Methods to compute average evaluation time for each type of decision diagrams are presented. By experimental analysis using benchmark functions, the number of nodes and average evaluation time are compared. Our results show that BDDs for ECFNs outperform MTBDDs, BDDs for CFs, and SBDDs with respect to both number of nodes and computation time. The sizes of BDDs for ECFNs are smaller than for MTBDDs, BDDs for CFs, and SBDDs.","Logic functions,
Binary decision diagrams,
Computer science,
Time measurement,
Microelectronics,
Input variables,
Data structures,
Length measurement"
Integrating safety analysis into formal specification of dependable systems,This paper presents and validates a novel approach to a formal specification of software for dependable systems. The approach incorporates results of statecharts and failure mode and effect analysis (FMEA) in the development of formal specifications of fail-safe systems. We use the action system formalism as our specification framework. Within the framework we define a general model of a safety-critical fail-safe system. Statecharts facilitate construction of a formal specification by structuring informal functional requirements and formalizing safety requirements resulted from FMEA. The approach is validated by a case study - a derivation of formal specification of a conveyor system.,"Safety,
Formal specifications,
Fault detection,
Software systems,
Failure analysis,
Protection,
Control systems,
Computer science,
Control system synthesis,
Distributed computing"
Chinese prosodic phrasing with extended features,"Prosodic phrasing is an important component in modern TTS systems, which inserts natural and reasonable breaks into long utterance. This paper reports the study of prosodic phrasing in unrestricted Chinese text. A text corpus of 500 sentences is collected from our speech database and manually labeled with syntactic structure and prosodic structure. Features and target prosody labels are extracted from the corpus and used as training examples for a rule-learning program. The acquired rules are evaluated on unseen sentences. The experiments show that the tree-level syntactic features are the most effective ones for Chinese prosodic phrasing. And chunk-level features can also help to improve the prediction accuracy.","Hidden Markov models,
Speech synthesis,
Recurrent neural networks,
Computer science,
Spatial databases,
Accuracy,
Intelligent systems,
Humans,
Classification tree analysis,
Regression tree analysis"
On computational complexity of non-reducible descriptors,"We present a supervised pattern recognition model that uses Boolean formulas for non-reducible descriptors. This model leads to computational problem which is shown to be NP-complete. In the paper, we identify two open combinatorial problems in the construction of non-reducible descriptors that can be applied to a large set of applications.","Computational complexity,
Pattern recognition,
Computer science,
Educational institutions,
Electronic mail,
Computational modeling,
Pattern analysis,
Medical diagnosis,
Medical diagnostic imaging,
Mathematical model"
Scoped and approximate queries in a relational grid information service,"We are developing a grid information service, RGIS, that is based on the relational data model. RGIS supports complex queries written in SQL that search for compositions (using joins) of resources. For example, we might ask it to find a Linux cluster with a certain bisection bandwidth and total memory. Such queries can be expensive to execute, however, and so we have developed several approaches that leverage our CIS schema to let us trade off between the number of results returned and the execution time. We describe two of them: scoped queries and approximate queries. Scoped queries constrain search to a network neighborhood, returning all matching results in the neighborhood. Approximate queries reduce the number of joins done by replacing collective constraints with constraints on individual resources, returning a subset of all the possible results in the grid. Scoping, approximation, and nondeterminism (described elsewhere), can be combined. We describe scoped and approximate queries, how they are implemented, and present performance evaluations for two examples. The evaluation suggests that scoping and approximation can greatly reduce query times while still returning a useful number of results.","Geographic Information Systems,
Data models,
Grid computing,
Distributed computing,
Computer science,
Linux,
Bandwidth,
Large-scale systems,
Time sharing computer systems,
Monitoring"
Awareness and agility for autonomic distributed systems: platform-independent and publish-subscribe event-based communication for mobile agents,"We propose using the publish-subscribe model to complement proprietary or standard agent communication languages, useful particularly in the case of exchanging and disseminating short communications about events and consequently, reducing complexity of multi-agent messaging. We describe how we implemented the event notification mechanism for mobile agents, and analyse experiments that demonstrate the Elvin-based event notification mechanism for communication between Grasshopper agents and aglets. We also discuss message loss due to agent migration and offer solutions to reliable communication between mobile agents using events.","Publish-subscribe,
Mobile communication,
Mobile agents,
Computer science,
Software engineering,
Australia,
Communication standards,
Software standards,
Standards publication,
Internet"
Exploring the impacts of knowledge (re)use and organizational memory on the effectiveness of strategic decisions: a longitudinal case study,"Two forces that dramatically affect the sustainability of firms' competitive advantage in the new competitive landscape have been identified as globalization and information and communication technologies (ICTs), such as the Internet and intranets (e.g., Castells, 2000; Porter, 2001). Organizations often rely on acquired knowledge from past experiences to make higher quality decisions on business strategies for better future performance. In this context, knowledge management (KM) and organizational memory (OM) become a central issue to the effectiveness of strategic decision-making and organizational performance. This paper examines the relationship between the (re)use of knowledge/organizational memory (OM) and the effectiveness of strategic decision-making in devising corporate strategies. As part of an exploratory case study approach, a number of interviews are being conducted among top executives at a multinational firm. As a framework, the components of the modified version of McLean's IS success model by Jennex & Olfman (2002) are being used to examine for the impact of knowledge strategy and technological resources, along with the impact of individuals and members from wider organizational context on strategic decision making processes. These components are then analyzed within Galliers' (2002) IS strategy framework of emergent and deliberate strategizing. The analysis accounts for the inter-subjectivity of the concept of KM. Results from a continuous longitudinal study have clearly shown the significance of culture and human-driven knowledge requirements along side the use of an ERP system as part of an OMS. On-going findings of this study aim to contribute to a richer understanding of the impact of knowledge and OM/OMS on organizational learning (OL) and the effectiveness of managerial decision processes. In the context of the IS success model, this paper highlights the intermingled approaches to organizational knowledge management practices due to the contextual nature of knowledge and the human need for social interaction.","Computer aided software engineering,
Knowledge management,
Decision making,
Information systems,
Globalization,
Memory management,
Context modeling,
Humans,
Business communication,
Communications technology"
Breaking a time-and-space barrier in constructing full-text indices,"Suffix trees and suffix arrays are the most prominent full-text indices, and their construction algorithms are well studied. It has been open for a long time whether these indices can be constructed in both O(n log n) time and O(n log n)-bit working space, where n denotes the length of the text. In the literature, the fastest algorithm runs in O(n) time, while it requires O(n log n)-bit working space. On the other hand, the most space-efficient algorithm requires O(n)-bit working space while it runs in O(n log n) time. This paper breaks the long-standing time-and-space barrier under the unit-cost word RAM. We give an algorithm for constructing the suffix array which takes O(n) time and O(n)-bit working space, for texts with constant-size alphabets. Note that both the time and the space bounds are optimal. For constructing the suffix tree, our algorithm requires O(n log/sup /spl epsi//n) time and O(n)-bit working space for any 0 < /spl epsi/ < 1. Apart from that, our algorithm can also be adopted to build other existing full-text indices, such as Compressed Suffix Tree, Compressed Suffix Arrays and FM-index. We also study the general case where the size of the alphabet A is not constant. Our algorithm can construct a suffix array and a suffix tree using optimal O(n log |A|)-bit working space while running in O(n log log |A|) time and O(n log/sup /spl epsi//n) time, respectively. These are the first algorithms that achieve 0(n log n) time with optimal working space, under a reasonable assumption that log |A| = o(log n).",
Real-time cooperative multi-target tracking by communicating active vision agents,,"Cameras,
Target tracking,
Object detection,
Computer networks,
Computer vision,
Real time systems,
Machine vision,
Information science,
Informatics,
Surveillance"
Implicit deregistration in 3G cellular networks,"In a 3G cellular network, the visitor location registers (VLRs), the gateway location registers (GLRs), and the home location registers (HLRs) from a three-level mobility database structure. When users leave a GLR/VLR service area, deregistration with GLR/VLR is required. Deregistration may create significant traffic in the network, especially the traffic between GLR and a HLR, which is the remote/international traffic. In this paper, we propose a hierarchical implicit deregistration scheme with a first/subsequent registration in 3G cellular networks to effectively eliminate deregistration traffic. An analytic model is proposed to carry out the performance of the proposed scheme. Our study shows that the proposed scheme not only reduces the local deregistration traffic between the GLR and the VLR, but also reduces the remote/international deregistration traffic between the HLR and the GLR. This is especially true when the ratio of the cost of the remote/international traffic between the GLR and the HLR to the cost of local traffic between the VLR and the GLR is high.","Intelligent networks,
Land mobile radio cellular systems,
Telecommunication traffic,
Mobile handsets,
Traffic control,
Databases,
Registers,
Costs,
GSM,
Computer science"
Diagnosing architectural degeneration,"Software systems evolve over time and undergo changes that can lead to a degeneration of the systems' architecture. Degeneration may eventually reach a level where a complete redesign of the software system is necessary, which is a task that requires significant effort. In this paper, we start by presenting examples of such degeneration and continue with an analysis of technologies that can be used to diagnose degeneration. These technologies can be employed in identifying, degeneration so that it can be treated as early as possible, before it is too late and the system has to undergo a costly redesign.","Computer architecture,
Software systems,
Software engineering,
Open source software,
Programming profession,
Software maintenance,
Computer science,
Computer industry,
Delay,
Telecommunications"
Affective communication for implicit human-machine interaction,A novel implicit communication framework in human-machine interaction that is sensitive to human affective states is presented in this paper. The focus is to achieve detection and recognition of human affect based on physiological signals. This involves building an affect recognition system that accepts as input various physiological parameters and predicts the probable related affective state. Both decision tree and fuzzy logic methodologies have been applied to this problem. This paper presents the results of the two methods and discusses their comparative merit. Three human subject experiments were designed and trials were conducted with six participants. The experimental results demonstrate the feasibility of the proposed implicit human-machine interaction framework.,"Man machine systems,
Humans,
Intelligent robots,
Biomedical monitoring,
Psychology,
Machine intelligence,
Computer science,
Decision trees,
Decision making,
Rehabilitation robotics"
Discovering direct and indirect matches for schema elements,"Automating schema matching is challenging. Previous approaches to automating schema matching focus on computing direct element matches between two schemas. Schemas, however rarely match directly. Thus, to complete the task of schema matching, we must also compute indirect element matches. In this paper we present a framework for generating direct as well as many indirect element matches between a source schema and a target schema. Recognizing expected data values associated with schema elements and applying schema-structure heuristics are the key ideas to computing indirect matches. Experiments we have conducted over several real-world application domains show encouraging results, yielding over 90% precision and recall for both direct and indirect element matches.","Ontologies,
Multilevel systems,
Computer science,
Query processing,
XML,
Database systems"
Study on requirement specifications for personalized multimedia summarization,"The ability to summarize and abstract information is an essential part of intelligent behavior in consumer devices. However, even for the emerging area of video content analysis, user preferences on summarization have not been explored. This paper reports on a panel which asked: who, why and when summarization is needed; what information should be summarized; and what forms should summaries take. In particular, we investigated the requirements with sensitivity to user needs, user context, media content, device capabilities, and the methods by which user and environment profiles can be assembled and exploited. We organize our findings as a wish list containing four major user questions, and we report the current and near-term states of the art and the major technical challenges in satisfying them. Our study suggests that user preferences should be derived from explicit user statements and from implicit trends inferred from viewing histories and summary usage.","Videoconference,
Multimedia systems,
TV,
Computer science,
Assembly,
History,
Humans,
Speech,
Computational linguistics,
Recommender systems"
Use of multiple I/sub DDQ/ test metrics for outlier identification,"With increasing circuit complexity and reliability requirements, screening outlier chips is an increasingly important test challenge. This is especially true for I/sub DDQ/ test due to increased spread in the distribution. In this paper, the concept of current ratio is extended to exploit wafer-level spatial correlation. Two metrics - current ratio and neighbor current ratio - are combined to screen outliers at the wafer level. We demonstrate that a single metric alone cannot screen all outliers, however, their combination can be used for effectively screening outlier chips. Analyses based on industrial test data are presented.","Chromium,
Circuit faults,
Circuit testing,
Leakage current,
Computer science,
Complexity theory,
Production,
Circuit topology,
Fluctuations,
Current measurement"
A HOTLink/networked PC data acquisition and image reconstruction system for a high-resolution whole-body PET with respiratory or ECG-gated performance,"An ultrahigh-resolution positron emission tomography (PET) camera in whole-body scanning or gated imaging study needs super computer-processing power for creating a huge sinogram as well as doing image reconstruction. A fast HOTLink serial bus attached to networked cluster personal computers (PC) has been developed for this special purpose. In general, the coincidence data from a PET camera is unidirectional; therefore, an additional daisy-chain bus using high-speed HOTLink (400 Mb/s, Cypress Semiconductor, inc.) transmitters and receivers is designed to carry the coincidence data to the entire networked (LAN) computers (PCs), the data from HOTLink are interfaced to a PC through a fast PCI I/O board (80 Mbyte/s). The overall architecture for the image acquisition and reconstruction computing system for a whole-body PET scanning is a pipeline design. One PC will acquire sinogram data for one bed position, and after completion of data acquisition that PC will begin to reconstruct the image. Meanwhile, another PC in the network will start data acquisition for the next bed position. The image results from the previous PC will be sent to a master computer for final tabulation and storage through the standard network, and then it will be free for processing a new bed position.","Data acquisition,
Image reconstruction,
Whole-body PET,
Cameras,
Computer networks,
Image storage,
Positron emission tomography,
High-resolution imaging,
Microcomputers,
Transmitters"
Placement with symmetry constraints for analog layout using red-black trees,"The traditional way of approaching placement problems in computer-aided design (CAD) tools for analog layout is to explore an extremely large search space of feasible or unfeasible placement configurations, where the cells are moved in the chip plane (being even allowed to overlap) by a stochastic optimizer. This paper presents a novel analog placement technique operating on the set of tree representations of the layout, where the typical presence of an arbitrary number of symmetry groups of devices is directly taken into account during the exploration of the solution space. The computation times exhibited by this novel approach are typically 3-6 times better than those of the algorithms using the traditional exploration strategy. This superior efficiency is due to the use of red-black trees, a data structure introduced by Guibas and Sedgewick to support operations on dynamic sets of intervals.","Encoding,
Design automation,
Simulated annealing,
Computer science,
Constraint optimization,
Design optimization,
Tree data structures,
Cost function,
Cryptography,
Computational modeling"
Bluetooth communication employing antenna diversity,"In order to cope with the environmental impacts on Bluetooth data transmission, diversity techniques were examined. A Bluetooth antenna diversity demonstrator was developed. Extensive experimental investigations documenting the significant improvements of data communication employing diversity have been done. It was shown that the effects of multipath and fading could be combated to a large extent by antenna diversity. Thus, antenna diversity was found to be a powerful technique for improving the quality of Bluetooth links.",
Mutual anonymity protocols for hybrid peer-to-peer systems,"In a hybrid peer-to-peer (P2P) system, some operations are intentionally centralized, such as indexing of peers' files. We present several protocols to achieve mutual communication anonymity between an information requester and a provider in a hybrid P2P information-sharing environment with trusted index servers such that neither the requester, nor the provider can identify each other and no other peers can identify the two communicating parties with certainty. Some existing protocols provide solutions to achieve mutual anonymity in pure P2P systems without any trusted central controls. Compared with two representative protocols, our proposed mutual anonymity protocols improve efficiency by utilizing trusted third parties and aiming at both reliability and low-cost. We show that with some limited central support, our protocols can accomplish the goals of anonymity, efficiency, and reliability. We have evaluated our techniques in a browser-sharing environment. We show that the average increase in response time caused by our protocols is trivial, and these protocols show advantages over existing protocols in a hybrid P2P system.","Protocols,
Peer to peer computing,
Indexing,
Computer science,
Delay,
Resists,
Laboratories,
Educational institutions,
Centralized control,
Control systems"
Voxels on fire [computer animation],We introduce a method for the animation of fire propagation and the burning consumption of objects represented as volumetric data sets. Our method uses a volumetric fire propagation model based on an enhanced distance field. It can simulate the spreading of multiple fire fronts over a specified isosurface without actually having to create that isosurface. The distance field is generated from a specific shell volume that rapidly creates narrow spatial bands around the virtual surface of any given isovalue. The complete distance field is then obtained by propagation from the initial bands. At each step multiple fire fronts can evolve simultaneously on the volumetric object. The flames of the fire are constructed from streams of particles whose movement is regulated by a velocity field generated with the hardware-accelerated Lattice Boltzmann Model (LBM). The LBM provides a physically-based simulation of the air flow around the burning object. The object voxels and the splats associated with the flame particles are rendered in the same pipeline so that the volume data with its external and internal structures can be displayed along with the fire.,"Fires,
Computational modeling,
Lattice Boltzmann methods,
Computer graphics,
Visualization,
Animation,
Isosurfaces,
Fuels,
Computer science,
Rendering (computer graphics)"
ELF: efficient location forwarding in ad hoc networks,"Recently, a new family of protocols has been introduced for large scale ad hoc networks that makes use of the approximate location of nodes in the network for geography-based routing. Location management plays an important role in such protocols, and previous work in this area has shown that the asymptotic overhead of location management is heavily dependant on the service primitives (location registration, maintenance and discovery) supported by a location management protocol. Currently, SLALoM (C.T. Cheng et al., 2002), which is a grid-based protocol optimized for large node movements, achieves the best known upper bound on the asymptotic worst case overhead of location management. However, the location registration cost in SLALoM dominates other costs for all practical purposes, and thus novel schemes need to be designed to limit this control traffic. In this work, we use the idea of location forwarding to devise a new scheme called ELF that limits the signalling traffic, and thus enhances the scalability of location management in large ad hoc networks. We find that, while the asymptotic overhead cost by such an improvisation matches that of SLALoM, ELF outperforms SLALoM in average case scenarios.","Ground penetrating radar,
Geophysical measurement techniques,
Intelligent networks,
Ad hoc networks,
Communication system traffic control,
Costs,
Routing protocols,
Large-scale systems,
Computer science,
Upper bound"
HPC.NET - are CLI-based Virtual Machines Suitable for High Performance Computing?,"The Common Language Infrastructure is a new, standardized virtual machine that is likely to become popular on several platforms. In this paper we review whether this technology has any future in the high-performance computing community, for example by targeting the same application space as the Java-Grande Forum. We review the technology by benchmarking three implementations of the CLI and compare those with the results on Java virtual machines.",
Indexing of time series by major minima and maxima,"We describe a technique for fast compression of time series and indexing of compressed series. We have tested it on three data sets: stock prices, air and sea temperatures, and wind speeds.","Indexing,
Computer science,
Testing,
Ocean temperature,
Wind speed,
Technological innovation,
Sea measurements,
Data mining,
Databases,
Information retrieval"
Completeness of a fact extractor,,
A load balancing tool for distributed parallel loops,,"Load management,
Concurrent computing,
Degradation,
Processor scheduling,
Application software,
Computational modeling,
Computer science,
Large-scale systems,
Libraries,
Linux"
Stability and volatility in the Linux kernel,"Packages are the basic units of release and reuse in software development. The contents and boundaries of packages should therefore be chosen to minimize change propagation and maximize reusability. This suggests the need for a predictive measure of stability at the package level. We observed the rates of change of packages in Linux, a large open-source software system. We compared our empirical observations to a theoretical 'stability metric' proposed by Martin. In this case, we found that Martin's metric has no predictive value.",
OdysseyShare: an environment for collaborative component-based development,"Automated support such as the one provided by software development environments (SDEs) is a key requirement for the systematization of large-scale component-based software development. However, to provide a component-based SDE, adequate software development process, methods and tools that consider component-based development (CBD) activities must be previously defined. Moreover, CBD can be a highly distributed and collaborative activity that needs group interaction support. In this paper, we describe OdysseyShare environment, a collaborative component-based SDE under development at the Computer Science Department of COPPE/UFRJ. It supports activities involved in modeling, construction, reuse and group interaction by providing an integrated set of tools and a repository of reusable components.","Collaboration,
Programming,
Collaborative work,
Collaborative software,
Collaborative tools,
Software engineering,
Application software,
Engines,
Large-scale systems,
Computer science"
An application of online learning algorithm for Bayesian network parameter,"Bayesian network is a graphical model that encodes probabilistic relationships among nodes of interest. The automated creation of Bayesian networks can be separated into two tasks. Structure learning, which consists of creating the structure of the Bayesian networks from the collected data and parameter learning, which consists of calculating the numerical parameters for a given structure. A Voting EM algorithm which is based EM be discussed and applied in the online parameter learning in flood decision Bayesian networks in this paper. Both EM and Voting EM algorithm are applied in flood decision Bayesian networks to compared their performance. The result indicates that the Voting EM can be used in the online learning for Bayesian network parameter and it also has more precisely that general EM algorithm.","Bayesian methods,
Voting,
Computer science,
Graphical models,
Application software,
Expert systems,
Large-scale systems,
Database systems,
Cybernetics"
Hardware implementation of the binary method for exponentiation in GF(2/sup m/),"Exponentiation in finite or Galois fields, GF(2/sup m/), is a basic operation for several algorithms in areas such as cryptography, error-correlation codes and digital signal processing. Nevertheless the involved calculations are very time consuming, especially when they are performed by software. Due to performance and security reasons, it is often more convenient to implement cryptographic algorithms by hardware. In order to overcome the well-known drawback of little or inexistent flexibility associated to traditional application specific integrated circuits (ASIC) solutions, we propose an architecture using field programmable gate arrays (FPGA). A cheap but still flexible modular exponentiation can be implemented using these devices. We provide the VHDL description of an architecture for exponentiation in GF(2/sup m/) based in the square-and-multiply method, called binary method, using two multipliers in parallel previously developed by ourselves. Our structure, compared with other designs reported earlier, introduces an important saving in hardware resources.","Hardware,
Signal processing algorithms,
Cryptography,
Application specific integrated circuits,
Field programmable gate arrays,
Galois fields,
Digital signal processing,
Software performance,
Security,
Computer architecture"
SPRiNG: synchronized random numbers for wireless security,"Motivated by the problem of wireless data link layer security, this paper proposes SPRiNG, a simple protocol for secure point-to-point communication. SPRiNG uses synchronized pseudo random number generation to generate authenticator variables and fresh encryption keys on a per frame basis. A major design goal for SPRiNG was simplicity and compatibility with the existing 802.11b WEP protocol. Though motivated by wireless security, SPRiNG is generic and can be applied anywhere secure point-to-point communication is required. Unlike many proposed security schemes, SPRiNG demonstrates that security does not always necessitate complexity.","Springs,
Communication system security,
Protocols,
Data security,
Wireless LAN,
Random number generation,
Cryptography,
Contracts,
Computer science,
Computer security"
Specification coverage aided test selection,"Here, we consider test selection strategies in formal conformance testing. As the testing conformance relation we use the ioco relation, and extend the previously presented on-the-fly test generation algorithms for ioco to include test selection heuristic based on a specification coverage metric. The proposed method combines a greedy test selection with randomization to guarantee completeness. As a novel implementation technique we employ bounded model checking for lookahead in greedy test selection.","Laboratories,
Computer science,
Protocols,
Software testing,
Data communication,
Computer bugs,
Java,
Event detection,
System testing,
Greedy algorithms"
A 8192 complex point FFT/IFFT for COFDM modulation scheme in DVB-T system,"In this paper, we propose an implementation method for a single-chip 8192 complex point FFT/IFFT in terms of sequential data processing. In order to reduce the required chip area for the sequential processing of 8 K complex data, a DRAM-based pipelined commutator architecture is used, and this brings 60% chip size reduction over the flip-flop approach. The 8192-point FFT/IFFT consists of cascaded blocks with six stages of radix-4 and one stage of radix-2. Since each stage requires rounding of the resulting bits while maintaining the proper S/N ratio, the convergent block floating point (CBFP) algorithm is used for the effective internal bit rounding. For the IFFT operation, we a use control signal that changes the radix-4 butterfly operator and coefficients saved in ROM without additional blocks. The proposed FFT/IFFT architecture was fabricated using a 0.35 /spl mu/m standard CMOS process.","Digital video broadcasting,
Frequency synchronization,
Demodulation,
Silicon compounds,
Read only memory,
TV broadcasting,
Europe,
Digital audio broadcasting,
Computer science,
Data processing"
The power of a question: a case study of two knowledge capture systems,"Designers have many options for how to encode knowledge, although most are based on declarative representations. This paper explores the use of questions to represent knowledge. Practitioner experiences implementing two knowledge resources using a question-based representation are described. In both resources, the use of ""questions"" was chosen as both a nonthreatening way of engaging users and for its value in initiating thinking processes. Both systems have succeeded in capturing the interest of users and serve as valuable components of the organization's knowledge capture program. This paper describes the systems, the underlying design approach, and results from system evaluation. Since the goal of any knowledge resource is to facilitate the reuse of knowledge, it is important to understand the impact that different knowledge representations could have on system acceptance. This study raises several research issues based on experiences using the unusual representation of ""questions"".","Computer aided software engineering,
Knowledge transfer,
Technological innovation,
Knowledge based systems,
Propulsion,
Laboratories,
Knowledge representation,
Personnel,
Research and development,
Knowledge acquisition"
Effective schema-based XML query optimization techniques,"Use of path expressions is a common feature in most XML query languages, and many evaluation methods for path expression queries have been proposed recently. However, there are few researches on the issue of optimizing regular path expression queries. In this paper, two kinds of path expression optimization principles are proposed, named path shortening and path complementing, respectively. The path shortening principle reduces the querying cost by shortening the path expressions with the knowledge of XML schema. While the path complementing principle substitutes the user queries with the equivalent lower-cost path expressions. The experimental results show that these two techniques can largely improve the performance of path expression query processing.",
Interactive simulation of burning objects,"In this paper, we describe a fast and interactive method for simulating and controlling the combustion process together with the decomposition of burning solids. The combustion and decomposition processes are integrated to create a physically-based model that runs at interactive rates.","Fires,
Fuels,
Combustion,
Heat transfer,
Computational modeling,
Solid modeling,
Computer science,
Fluid dynamics,
Computer graphics,
Space heating"
A transmission range reduction scheme for power-aware broadcasting in ad hoc networks using connected dominating sets,"In ad hoc wireless networks, due to node mobility, broadcasting is expected to be more frequent to update global route/network information. A straightforward broadcasting by flooding is usually very costly and will result in substantial redundancy and large energy consumption. Power consumption is an important issue since most mobile hosts operate on battery. Broadcasting based on a connected dominating set (CDS) is a promising approach, where only nodes in the CDS need to relay the broadcast message. A set is dominating if all the nodes in the system are either in the set or neighbors of nodes in the set. In general, nodes in the CDS consume more energy to handle various bypass traffic than nodes outside the set. To prolong the life span of each node and, hence, the network, nodes should be alternated when possible to form a CDS. In this paper, we further extend the power-aware broadcasting by dynamically reducing the transmission range of each node during the broadcast process without sacrificing the broadcast coverage. The effectiveness of the proposed methods in prolonging the life span of the network is confirmed through simulation.",
Recent methods for image-based modeling and rendering,"A long-standing goal in image-based modeling and rendering is to capture a scene from camera images and construct a sufficient model to allow photo-realistic rendering of new views. With the confluence of computer graphics and vision, the combination of research on recovering geometric structure from un-calibrated cameras with modeling and rendering has yielded numerous new methods. Yet, many challenging issues remain to be addressed before a sufficiently general and robust system could be built to (for instance) allow an average user to model their home and garden from camcorder video. This tutorial aims to give researchers and students in computer graphics a working knowledge of relevant theory and techniques covering the steps from real-time vision for tracking and the capture of scene geometry and appearance, to the efficient representation and real-time rendering of image-based models. It also includes hands-on demos of real-time visual tracking, modeling and rendering systems.","Rendering (computer graphics),
Computer science,
Layout,
Cameras,
Computer graphics,
Solid modeling,
Computational geometry,
Real time systems,
Hardware,
Robots"
Teaching students to design secure systems,"The pedagogy used to teach computer security to students new to the field is usually handled by a one- or two-course augmentation to an existing curriculum. Furthermore, the courses tend to be technology-centric and often do not uncover the underlying processes that students can transfer to new situations. In this article, we look more closely at determining an appropriate scope and sequence for information assurance (IA) and briefly describe a project whose goal is the articulation of an IA curriculum.","National security,
Information security,
Computer security,
Educational programs,
Privacy,
Computer science education,
Information systems,
Computer Society,
Design engineering,
Systems engineering and theory"
Feature subset selection for blood pressure classification using orthogonal forward selection,"Systolic and diastolic pressures are used to define cardiac related health in general medicine. In our present work, we investigate blood pressure classification based on pressure waveforms using a relatively large non-invasively collected database. Feature selection is required to reduce redundant features in the data set for a better classification. Therefore, a selection method based on an orthogonal transform is used.","Blood pressure,
Feature extraction,
Spatial databases,
Space technology,
Matrix decomposition,
Computer science,
Extraterrestrial measurements,
Covariance matrix,
Testing,
Hypertension"
Multibaseline stereo using a single-lens camera,"In stereopsis, correspondence is a central problem that is known to suffer from several inherent difficulties. Single-lens stereo systems have brought a geometrical solution to some of the problems of stereopsis. The plenoptic camera is one of those systems that we found to simplify the process of depth recovery, but whose main limitation lies in its small baseline. For this particular reason, matches in the plenoptic camera are obtained more easily, however, this trades off against the accuracy with which they are recovered. In this paper, we show how its lack of accuracy due to a small baseline can be made up for to some extent, by using a multibaseline estimate in lieu of the commonly used equal-baseline estimate.","Cameras,
Lenses,
Layout,
Geometry,
Computer science,
Stereo vision,
Yield estimation,
Frequency,
Optical design,
Solid modeling"
Localized prediction of continuous target variables using hierarchical clustering,"We propose a novel technique for the efficient prediction of multiple continuous target variables from high-dimensional and heterogeneous data sets using a hierarchical clustering approach. The proposed approach consists of three phases applied recursively: partitioning, localization and prediction. In the partitioning step, similar target variables are grouped together by a clustering algorithm. In the localization step, a classification model is used to predict which group of target variables is of particular interest. If the identified group of target variables still contains a large number of target variables, the partitioning and localization steps are repeated recursively and the identified group is further split into subgroups with more similar target variables. When the number of target variables per identified subgroup is sufficiently small, the third step predicts target variables using localized prediction models built from only those data records that correspond to the particular subgroup. Experiments performed on the problem of damage prediction in complex mechanical structures indicate that our proposed hierarchical approach is computationally more efficient and more accurate than straightforward methods of predicting each target variable individually or simultaneously using global prediction models.","Predictive models,
Clustering algorithms,
Partitioning algorithms,
Manufacturing processes,
Computer science,
Mechanical engineering,
Scientific computing,
Laboratories,
Large-scale systems,
Data analysis"
An application of fuzzy support vectors,"Support Vector Machines (SVMs) are a recently introduced Machine Learning technique. SVMs approach binary classification by attempting to find a hyperplane that separates the two categories of training vectors. This hyperplane is expressed as a function of a subset of the training vectors. These vectors are called support vectors. In this paper, we present a method of fuzzifying support vectors based off of the results of an SVM induction. We then propose a method of enhancing SVM induction using these fuzzy support vectors. We finish by presenting a computational example using the IRIS data set.","Support vector machines,
Support vector machine classification,
Computer science,
Kernel,
Engines,
Application software,
Milling machines,
Educational institutions,
Machine learning,
Iris"
A novel approach to separate handwritten connected digits,,"Computer science,
Image recognition,
Handwriting recognition,
Postal services,
Writing,
Joining processes,
Text analysis,
Image analysis,
Credit cards,
Feature extraction"
The key theorem of learning theory about fuzzy examples,"After the presence of the key theorem of learning theory, statistical learning theory has been formed. But the examples which it has studied are all random vectors, in theory there is no the case of fuzzy random vectors. In other words, when the characters are very difficult to distract (namely the characters are fuzzy) , we have to use fuzzy random vectors to be the examples. In this paper, we give one method to deal with this kind of problems, and give the feasibility of this method.","Random variables,
Machine learning,
Mathematics,
Statistical learning,
Risk management,
Computer science,
Convergence,
Fuzzy sets,
Cybernetics,
Bismuth"
A self-tuning structure for adaptation in TCP/AQM networks,"Congestion control in TCP/AQM networks is expected to perform well for a wide-range of conditions, but recent advances in modeling and analysis indicate that present AQM (active queue management) schemes need an extra dose of adaptability to cope. The paper answers the call and proposes a self-tuning structure wherein AQM parameters are automatically tuned in response to on-line estimation of link capacity and traffic load. This approach is applicable to any AQM scheme that is parameterizable in terms of link capacity and TCP load. We describe this self-tuning structure, illustrate its application to PI (proportional-integral) and RED (random early detection) AQMs, provide stability analysis, and conduct ns simulations to compare with both fixed AQM schemes and the recently proposed adaptive RED.","Intelligent networks,
Propagation delay,
Computer science,
Performance analysis,
TCPIP,
Pi control,
Proportional control,
Degradation,
Communication system traffic control,
Traffic control"
Novel feature vector for image authentication,"As the dissemination of images grows across the Internet, authentication of images is becoming an important research issue. Image watermarking techniques are the most used forms to authenticate an image, however they introduce distortions in the original image and are vulnerable to a multitude of attacks. In this paper we present a novel method to authenticate images. We propose that instead of watermarking images, we can authenticate an image by calculating an invariant feature vector which models the image based on its shape and illumination characteristics. We propose an algorithm to calculate such a robust feature vector which withstands a variety of attacks and hence can be a good alternative to watermarking techniques.","Authentication,
Watermarking,
Lighting,
Internet,
Shape,
Mirrors,
Gray-scale,
Computer science,
Robustness,
Protection"
A general encryption scheme based on MDS code,"In this paper, we propose to combine cryptographically strong random key stream generators (including those used in stream ciphers) with simple block encoders to achieve both high cryptographic strength and low computation complexity of encryption and decryption. In particular, we show proper error-correcting codes can be used as block encoders. In general, encoding and decoding of a proper block error (or more accurately erasure) correcting code is much faster than encryption and decryption of block ciphers.","Cryptography,
Streaming media,
Random number generation,
Decoding,
Error correction codes,
Encoding,
Computer science,
USA Councils,
Application software,
Wireless communication"
Applying multiple query optimization in mobile databases,"We apply multiple query optimization to batches of pull (on-demand) requests in a mobile database system. The resulting view can answer several queries at once, and it is broadcast on a view channel dedicated to common answers of multiple queries rather than over individual downlink channels. A performance study is conducted that simulates different query workloads. The results indicate a significant savings in channel bandwidth usage and a reduction in average wait time for a multi-query approach compared to a traditional pull-based approach.","Query processing,
Broadcasting,
Downlink,
Bandwidth,
Network servers,
Information retrieval,
Database systems,
Cellular phones,
Personal digital assistants,
Portable computers"
Partitioned vector quantization: application to lossless compression of hyperspectral images,"A novel design for a vector quantizer that uses multiple codebooks of variable dimensionality is proposed. High dimensional source vectors are first partitioned into two or more subvectors of (possibly) different length and then, each subvector is individually encoded with an appropriate codebook. Further redundancy is exploited by conditional entropy coding of the subvectors indices. This scheme allows practical quantization of high dimensional vectors in which each vector component is allowed to have different alphabet and distribution. This is typically the case of the pixels representing a hyperspectral image. We present experimental results in the lossless and near-lossless encoding of such images. The method can be easily adapted to lossy coding.","Vector quantization,
Image coding,
Hyperspectral imaging,
Pixel,
Spatial resolution,
Principal component analysis,
Application software,
Entropy,
Distortion measurement,
Computer science"
SACCS: scalable asynchronous cache consistency scheme for mobile environments,"In this paper, we propose a novel cache consistency maintenance scheme, called Scalable Asynchronous Cache Consistency Scheme (SACCS), for mobile environments. It relies on the following three key features: (1) Use of flag bits at server and MU's cache to maintain cache consistency; (2) Use of an identifier (ID) for each entry in MUs cache after its invalidation in order to maximize the broadcast bandwidth efficiency; (3) Rendering all valid entries of MUs cache to uncertain state when it wakes up. These three features make the SACCS a highly scalable algorithm with minimum database management overhead. Comprehensive simulation results show that the performance of SACCS is superior to those of existing algorithms.","Mobile computing,
Bandwidth,
Broadcasting,
Data communication,
Wireless communication,
Delay,
Computer science,
Spatial databases,
Computational modeling,
Telecommunication traffic"
Dual-gate (FinFET) and tri-Gate MOSFETs: simulation and design,"The continued downward scaling of silicon MOSFET device dimensions below one tenth micron has presented new and serious challenges for future integrated circuit applications. Accordingly, new MOSFET structures, such as the dual-gate (FinFET) and the tri-Gate transistor, have been proposed to replace the conventional planar MOSFET. These devices are compatible with conventional silicon integrated circuit processing, but offer superior performance as the device is scaled into the nanometer range. However, the physics of the MOSFET's operation in these new device structures is somewhat different. This study aims to investigate the differences in performance of these two devices and their device design using a commercial, three-dimensional numerical simulator ATLAS from Silvaco International.","FinFETs,
MOSFETs,
Silicon,
Computational modeling,
Circuit simulation,
Threshold voltage,
Computer science,
Application specific integrated circuits,
Application software,
Nanoscale devices"
Reliable and rapidly-converging ICP algorithm using multiresolution smoothing,"Autonomous range acquisition for 3D modeling requires reliable range registration, for both the precise localization of the sensor and combining the data from multiple scans for view-planning computation. We introduce and present a novel approach to improve the reliability and robustness of the ICP (iterative closest point) 3D shape registration algorithm by smoothing the shape's surface into multiple resolutions. These smoothed surfaces are used in place of the original surface in a coarse-to-fine manner during registration, which allows the algorithm to avoid being trapped at local minima close to the global optimal solution. We used the technique of multiresolution analysis to create the smoothed surfaces efficiently. Besides being more robust, convergence is generally much faster, especially when combined with the point-to-plane error metric of Chen and Medioni. Since the point-to-plane error metric has no closed-form solution, solving it can be slow. We introduce a variant of the ICP algorithm that has convergence rate close to it but still uses the closed-form solution techniques (SIT) or unit quaternion methods) of the original ICP algorithm.","Iterative closest point algorithm,
Smoothing methods,
Shape,
Iterative algorithms,
Closed-form solution,
Robustness,
Iterative methods,
Multiresolution analysis,
Quaternions,
Computer science"
Network intrusion early warning model based on D-S evidence theory,"Application of data fusion technique in intrusion detection is the trend of next-generation intrusion detection system (IDS). In network security, adopting security early warning technique is feasible to effectively defend against attacks and attackers. To do this, correlative information provided by IDS must be gathered and the current intrusion characteristics and situation must be analyzed and estimated. This paper applies D-S evidence theory to distributed intrusion detection system and propose a early warning model which fuses information from detection centers, makes clear intrusion situation and improves the early warning capability and detection efficiency of the IDS.","Intrusion detection,
Information security,
Data security,
Sensor fusion,
Information analysis,
Computer network management,
Computer networks,
Mathematics,
Computer science,
Electronic mail"
An application-level QoS comparison of inter-destination synchronization schemes for continuous media multicasting,"The paper presents an application-level QoS comparison of three inter-destination synchronization schemes: the master-slave destination scheme; the synchronization maestro scheme; the distributed control scheme. The inter-destination synchronization adjusts the output timing among destinations in a multicast group for live audio and video streaming over the Internet/intranets. We compare the application-level QoS of these schemes by simulation, in which we assume an intranet. From the comparison, we clarify their features and find the best scheme in the environment. The simulation result shows that the best scheme depends on the network configurations and the acceptable inter-destination synchronization quality of the applications.",
Application-bypass reduction for large-scale clusters,"Process skew is an important factor in the performance of parallel applications, especially in large-scale clusters. Reduction is a common collective operation which, by its nature, introduces implicit synchronization between the processes involved in the communication and is therefore highly susceptible to performance degradation due to process skew. A collective operation with application-bypass does not require the application to block in order for the operation to make progress. Application-bypass collective operations are therefore highly tolerant of skew. In this paper we describe the design and implementation of an application-bypass version of the reduction operation in MPICH over GM. We evaluate our implementation on a 16-node cluster. Under conditions of process skew we find a factor of improvement of up to 3.3 for our application-bypass reduction versus the default MPICH implementation. In addition, we see that this factor of improvement increases with system size, indicating that the application-bypass implementation is more scalable and skew-tolerant than the default non-application-bypass version. This framework promises design and development of high-performance and scalable collective communication libraries for next-generation large-scale clusters.",
How to apply the Bloom taxonomy to software engineering,The Bloom taxonomy is used in the SWEBOK to specify the expected level of understanding of each topic within its knowledge areas (KA) for a 'graduate plus four years of experience'. This paper discusses how Bloom's taxonomy could be expanded to be more useful not only for education but also for industry. A new taxonomy that is more applicable to engineering is proposed at the end of this paper. This paper is the result of a workshop held in Amsterdam in September 2003 during the Software Technology and Engineering Practice Conference (STEP 2003).,
Supporting collaborative exercises for distance education,"At the German Distance Learning University, collaborative synchronous exercises have been recently identified by students and teachers as an important future form of collaborative learning in the university's virtual learning space. The main requirements of collaborative exercises in a distance learning university include support for preparation of exercises, learning group management, collaborative learning sessions, and learning management. We support such collaborative exercises in the FUB system by providing groupware tools for each phase of collaborative exercises. Especially important is the support of complex problem solving processes. Results of a trial use indicate that our approach works, and identify some needs further improvement. The implications of our approach and our experiences for the design of next generation learning platforms are discussed.",
Iterative Relief,"Feature weighting algorithms assign weights to features according to their relevance to a particular task. Unfortunately, the best-known feature weighting algorithm, ReliefF, is biased. It decreases the relevance of some features and increases the relevance of others when irrelevant attributes are added to the data set. This paper presents an improved version of the algorithm, Iterative Relief, and shows on synthetic data that it removes the bias found in ReliefF. This paper also shows that Iterative Relief outperforms ReliefF on the task of cat and dog discrimination, using real images.","Distance measurement,
Training,
Classification algorithms,
Noise,
Computer vision,
Conferences,
Pixel"
Integrating computing and information on Grids,"We look in depth at some of the different capabilities Grids must support. However, because there are several definitions of Grids, we adopt two views of them: those that support e-Science and those that provide an infrastructure with the ability to dynamically link managed resources. These descriptions emphasize a high-level user requirement and the system capability, respectively. We are integrating resources, managing them, and using them to support distributed collaborative engineering and science. Grids must embrace multiple heterogeneous resources and then integrate them. We discuss how this requires the meeting of three worlds: computers, databases, and sensors.","Grid computing,
Spatial databases,
Web services,
Relational databases,
Distributed databases,
XML,
Information filtering,
Information filters,
Information resources,
World Wide Web"
Efficient parallel out-of-core isosurface extraction,"A new approach for large dataset isosurface extraction is presented. The approach's aim is efficient parallel isosurfacing when the dataset cannot be processed entirely in-core. The approach focuses on reducing the memory requirement and optimizing disk I/O while achieving a balanced load. In particular, an accurate model of isosurface extraction time is exploited to evenly distribute work across processors. The approach achieves processing efficiency by also avoiding unnecessary processing for portions of the dataset that are not intersected by the isosurface. To reduce the redundant computations and the storage requirements, a flexible, variably-granular data structure is utilized, thereby achieving excellent time and space performance.","Isosurfaces,
Data mining,
Concurrent computing,
Data structures,
Computer science,
Data visualization,
Acceleration,
Chromium,
Computational geometry,
Solid modeling"
Matching software practitioner needs to researcher activities,"We present an approach to matching software practitioners' needs to software researchers' activities. It uses an accepted taxonomical software classification scheme as intermediary, in terms of which practitioners express needs, and researchers express activities. A decision support tool is used to combine these expressions of needs/activities, and to assist in studying the implications of that combined knowledge. This enables identification of fruitful connections between researchers and practitioners, of areas of common interest among researchers, and practitioners, and of ""gaps"": areas of unfulfilled needs or unmotivated research. We discuss the software engineering underpinning this approach, illustrating its utility by reporting on experiments with a real-world dataset gathered from researchers and practitioners. We also suggest that this same approach would be applicable to understanding the distribution of interests represented by presenters and attendees of a conference such as APSEC.",
Predicting intrusions with local linear models,"Intrusion Detection Systems are typically deployed for real time operation, but are limited to identifying attacks once initiated. In this work we instead investigate the potential for predicting an attack before it occurs. To do so, a two-stage process is employed with a classification stage following that of a predictor. Predictors are based on the SOM and classifier on an SVM. Training and test is conducted using the 'TCP' connection features from the DARPA KDD competition data set. In spite of the simplicity of the model, the system is able to provide false positive and false negative rates of 23.8% and 7.1% respectively for one step-ahead prediction.","Predictive models,
Support vector machines,
Support vector machine classification,
Intrusion detection,
Testing,
Organizing,
Computer science,
Real time systems,
Pattern analysis,
Statistical analysis"
Improving project planning/tracking for student software engineering projects through SOPPTS,"The Student Online Project Planning and Tracking System, SOPPTS, is an online system designed and implemented to enhance the communication avenues and the project planning/tracking requirements of student projects for the Ball State University (BSU) software engineering classes. This paper presents the design and assessment of this tool SOPPTS has been designed and field-tested to provide real-time feedback from faculty on student project progress, to offer online guidance for project planning and to produce automated tracking of student projects. The tool assessment included interviews of both students at the undergraduate and graduate level and faculty. The interview was a set of specific questions chosen to document each participant's experience and impressions of utilizing SOPPTS. Data evaluation consisted of compiling the reoccurring themes during the interview process. The major themes that emerged are the increased efficiency in developing, recording and tracking of student project plans, the visibility and immediate accessibility of this information and the improved and timely communication among the student team members, faculty and client partners. With the improved access to information and facilitated communication through SOPPTS, the project planning and tracking skills for the software development teams improved. Moreover, the informal aspects of team communication and synergy, factors that can be as important as the technical aspects, were enhanced.","Software engineering,
Programming,
Computer science,
Project management,
Feedback,
Job shop scheduling,
Design engineering,
Software systems,
Documentation,
Educational products"
Hybrid evolutionary ridge regression approach for high-accurate corner extraction,"Corner measurement is of main concern within the following tasks: camera calibration, image matching, object tracking, recognition and reconstruction. This paper presents a hybrid evolutionary ridge regression approach for the problem of corner modeling. We search model parameters characterizing L-corner models by means of fitting the model to the image data. As the model fitting relies on an initial parameter estimation, we use a global approach to find the global minimum. Experimental results applied to an L-corner using several levels of noise show the advantages and disadvantages of our evolutionary algorithm compared to down-hill simplex and simulated annealing.","Computer vision,
Parametric statistics,
Parameter estimation,
Feature extraction,
Computer science,
Physics,
Cameras,
Calibration,
Image matching,
Image recognition"
Temporal task clustering for online placement on reconfigurable hardware,"Partial reconfiguration allows for mapping and executing several tasks on an FPGA during run time. One of the challenging problems in multitasking systems is high amount of communication costs. In this paper, we present two clustering methodologies that temporally cluster real-time tasks for a partially reconfigurable hardware and reduce communication overhead. The first algorithm aims at efficient use of resources by clustering close run-time tasks, and the second one makes the clustering with respect to a trade off between inter-task communication and resource utilization efficiency. The results show significant reduction communication costs.","Hardware,
Field programmable gate arrays,
Registers,
Clustering algorithms,
Computer science,
Multitasking,
Resource management,
Costs,
Scheduling algorithm,
Virtual colonoscopy"
Bitmap techniques for optimizing decision support queries and association rule algorithms,"In this paper, we discuss some new bitmap techniques for optimizing decision support queries and association rule algorithm. We first show how to use a new type of predefined bitmap join index (prejoin/spl I.bar/bitmap/spl I.bar/index) to efficiently execute complex decision support queries with multiple outer join operations involved and push the outer join operations from the data flow level to the bitmap level and achieve significant performance gain. Then we discuss a bitmap based association rule algorithm. Our bitmap based association rule algorithm Bit-AssocRule doesn't follow the generation-and-test strategy of a priori algorithm and adopts the divide-and-conquer strategy, thus avoids the time-consuming table scan to find and prune the item sets, all the operations of finding large item sets from the datasets are the fast bit operations. The experimental results show Bit-AssocRule is 2 to 3 orders of magnitude faster than a priori and a priori hybrid algorithms. Our results indicate that bitmap techniques can greatly improve the performance of decision support queries and association rule algorithm, and bitmap techniques are very promising for the decision support query optimization and data mining applications.","Association rules,
Decision support systems,
Itemsets,
Databases,
Query processing,
Delta modulation,
Educational institutions,
Computer science,
Performance gain,
Data mining"
A rerouting technique with minimum traffic disruption for dynamic traffic in WDM networks,,"Telecommunication traffic,
Intelligent networks,
WDM networks,
Wavelength division multiplexing,
Wavelength assignment,
Wavelength routing,
Optical fiber networks,
Computer science,
Heuristic algorithms,
Throughput"
On the relations between BCMP queueing networks and product form solution stochastic petri nets,,"Stochastic processes,
Petri nets,
Computer science,
Cities and towns,
Stochastic systems,
Power system modeling,
Concurrent computing,
Steady-state,
Linear systems,
Equations"
Experiences with the metacognitive skills inventory,"Despite the national surge in interest in technology fields, student performance measures continue to reveal that many students lack the analytical skills to complete college level courses in math and computer science. The development of tools for the measurement and enhancement of metacognitive skills will be examined. The primary measuring tool, the metacognitive skills inventory, was designed to measure metacognitive abilities. The inventory consists of two subscales: decomposition and confidence. The decomposition subscale measures the subjects ' awareness and reported use of the critical problem solving steps. Examples are problem identification, planning of solution strategies, and comparison of these strategies. The confidence subscale, measures the extent to which subjects are confident in their own problem solving ability. The inventory has been used in studies comparing student grades in college courses, problem solving skills at different levels in various college programs, and student performance on various tests. These tests include the SAT (both verbal and math), the MARS-R math anxiety scales, and the general expectancy of success scale-revised. The MSI is currently being used as an integral component of current research dealing with the development of training and education programs to increase metacognitive skills.",
Coronary characterization in multi-slice computed tomography,"We present a 3D extraction method of coronaries in MSCT, which aims at refining the delineating of the vascular inner wall and the calcified contours for quantification purposes. The proposed approach makes use of a two-step process: the first one performs a vessel central axis tracking by applying a semi-automatic 3D geometrical moment-based method. A refinement is then performed, based on a level set approach, to improve the detection accuracy of both contours and calcifications. The level sets were applied first in 2-D space, independently on each slice, then in 3-D to perform the extraction directly in the volume. A comparison between the 2-D and 3-D procedures is provided in term of quality of delineation.","Computed tomography,
Level set,
Degenerative diseases,
Magnetic resonance imaging,
Cardiac disease,
Cardiovascular diseases,
Deformable models,
Computer science,
Cardiology,
Public healthcare"
Residential load control through real-time pricing signals,"The concept of shaping residential loads can be an effective way of controlling the load profile of a distribution company. Flat energy rates do not provide incentives to customers to use power as would be optimal from a utility point of view. With the restructuring of the power industry it is expected that prices will fluctuate in any given day. Actually real-time pricing (RTP) and demand side management have been discussed for quite some time since the 1970s. However, RTP is still a concept, and often misused with time of use. This paper describes how to design an RTP system for residential customers, by looking into some of the current problems in the electricity market, and the potential impact of demand response from residential customers.","Load flow control,
Pricing,
Energy consumption,
Electricity supply industry,
Computer science,
Load management,
Electricity supply industry deregulation,
Power generation,
Power industry,
Power system management"
Parallel explicit state reachability analysis and state space construction,,"Reachability analysis,
State-space methods,
Hardware,
Workstations,
Testing,
Logic design,
Space exploration,
Computer science,
Clustering algorithms,
Load management"
"Comments on ""A carry-free 54 b/spl times/54 b multiplier using equivalent bit conversion algorithm""","For original paper see ibid., vol. 36, no. 10, p. 1538-1545 (Oct. 2001). In the aforementioned paper by Kim et al., a multiplier is presented which produces the result in radix-2 signed-digit representation. It is claimed that this representation can be converted into conventional magnitude representation by an algorithm which has no carry propagation. To the commenters this algorithm seems incorrect. The critical situation is a string which consists of a sequence of zeros followed by a -1; in such a case a carry is needed and the algorithm proposed is deemed incorrect. Consequently, it is pointed out that the proposed algorithm produces a correct multiplication result in conventional magnitude representation only if the signed-digit string does not have a sequence of 0's followed by a -1. The commenters show a multiplication example using the proposed conversion algorithm in which this situation occurs.","Circuit testing,
Circuit simulation,
Circuit faults,
Solid state circuits,
Error correction codes,
Niobium,
Computer science,
Digital arithmetic"
Using SWIG to bind C++ to Python,"An increasingly popular approach to scientific computing is to combine Python and compiled modules. Such an approach merges the high performance typically found in compiled routines with the interface of a flexible, scalable, and easy-to-learn interpreted language. Although using C to hand-code extensions to Python binds the latter to a given compiled asset in C++, programmers who used C++'s more advanced features (until recently) lacked the automated support available in Fortran and C. One tool for creating Python bindings to C the Simplified Wrapper and Interface Generator. SWIG-an open-source application used by a large and ever-expanding community-began as an effort to expose physics packages in a large parallel simulation code to interpreted languages. SWIG preprocesses C and C++ code and generates library bindings in several interpreted languages including Python, Pert, Tcl, and Java. Recent improvements to SWIG provide greater support for binding C++ code. SWIG now creates, for example, bindings for some of C++'s more advanced features such as templates and exceptions. This article explores how SWIG does this by examining a series of small C++ code examples.","Libraries,
Prototypes,
Scientific computing,
Linux"
Data-driven methodology to extending workflows to e-services over the Internet,"E-services refer to the services offered over the Internet. The globalization of economy accelerates the provision of e-services across organizations. Instead of being built from scratch, e-services are mostly extended from existing internal workflows or information systems. In this paper, we examine the requirements of extending a workflow to the provision of e-services, in order to fulfill predefined business processes and data requirements. We also discuss the support of exception handling and asynchronous events across organizational boundaries, through an event publish-and-subscribe mechanism. We demonstrate the feasibility of this approach with the E-ADOME extension layer of our ADOME-WFMS, further illustrated with a system integration example. Our event-driven execution model provides a unified framework for both synchronous execution of workflow and asynchronous handling of events and exceptions. Illustrations are also given on the provision of such e-services over the infrastructure of Web services and Enterprise JavaBeans.",
GLASS: a graphical query language for semi-structured data,"The increase in the use of XML (eXtensible Markup Language) makes the semistructured data more and more important on the Web. To exploit the full power of XML documents, a query language for semistructured data will be a promising and interesting application. However, the XQuery standard released by W3C is too difficult for common users to use. Some XML graphical query languages for semistructured data have been proposed but they are either too complex or too limited in use. We introduce a graphical query language for semistructured data, which we call GLASS. GLASS is developed on the base of ORA-SS data model, a semantically richer data model for semistructured data In GLASS, we combine the advantages of graphs and texts, which make the graphical language much clear and easy to use. The paper presents the notations and basic capabilities of GLASS via a series of examples with increasingly complexity. We also discuss some complex query examples such as order, group entity, negation and IF-THEN statement.","Glass,
Database languages,
XML,
Data models,
Data mining,
Computer science,
Engines,
Energy management,
Information management,
Standards development"
Parallelized file transfer protocol (P-FTP),"Parallelized FTP (P-FTP) approach, attempts to solve the problem of slow downloads of large multimedia files while optimizing the utilization of mirror servers. The approach presented in this paper downloads a single file from multiple mirror servers simultaneously, where each mirror server transfers a portion of the file. P-FTP server calculates the optimum division of the file for efficient transfer. The dynamic monitoring ability of P-FTP maintains the file transfer process at the optimized level no matter how abruptly network and mirror server characteristics change.","Protocols,
File servers,
Mirrors,
Network servers,
Web server,
Bandwidth,
Delay,
Internet,
Peer to peer computing,
Computer science"
"Assessing attitude towards, knowledge of, and ability to apply, software development process","Software development is one of the most economically critical engineering activities. It is unsettling, therefore, that regularly published analyses reveal that the percentage of projects that fail, by coming in far over budget or far past schedule, or by being cancelled with significant financial loss, is considerably greater in software development than in any other branch of engineering. The reason is that successful software development requires expertise in both state of the art (software technology) and state of the practice (software development process). It is widely recognized that failure to follow best practice, rather than technological incompetence, is the cause of most failures. It is critically important, therefore, that (i) computer science departments be able assess the quality of the software development process component of their curricula and that industry be able to assess the efficacy of SPI (software process improvement) efforts. While assessment instruments/tools exist for knowledge of software technology, none exist for attitude toward, knowledge of, or ability to use, software development process. We have developed instruments for measuring attitude and knowledge, and are working on an instrument to measure ability to use. The current version of ATSE, the instrument for measuring attitude toward software engineering, is the result of repeated administrations to both students and software development professionals, post-administration focus groups, rewrites, and statistical reliability analyses. In this paper we discuss the development of ATSE, results, both expected an unexpected, of recent administrations of ATSE to students and professionals, the various uses to which ATSE is currently being put and to which it could be put, and ATSE's continuing development and improvement.","Programming,
Instruments,
Failure analysis,
Job shop scheduling,
Best practices,
Computer science,
Computer industry,
Software quality,
Software tools,
Current measurement"
Exploiting mobility in large scale ad hoc wireless networks,"Mobility is generally viewed as a major impediment in the control and management of large scale wireless networks. However, researchers have recently looked at mobility in a different way, trying to take advantage of it instead of protecting from it. In the literature, mobility is generally modeled as independent random moves by each node. However, in reality, mobile nodes tend to show some degree of correlated (group oriented) motion behavior. This feature, once detected and understood, can be exploited to help improve network performance, in particular, scalability. We present several schemes for exploring mobility patterns, in particular group mobility. We show, via simulations, that the physical group mobility behavior of mobile nodes can be autonomously recognized and discovered by network layer routing schemes. It can then be utilized to reduce routing overhead and achieve good scalability in large scale ad hoc networks.","Intelligent networks,
Large-scale systems,
Wireless networks,
Routing,
Computer science,
Impedance,
Protection,
Scalability,
Internet,
Partitioning algorithms"
An approach to extracting the target text line from a document image captured by a pen scanner,"In this paper, we present a new approach to extracting the target text line from a document image captured by a pen scanner. Given the binary image, a set of possible text lines are first formed by nearest-neighbor grouping of connected components (CC). They are then refined by text line merging and adding the missed CCs. The possible target text line is identified by using a geometric feature based score function and fed to an OCR engine for character recognition. If the recognition result is confident enough, the target text line is accepted. Otherwise, all the remaining text lines are fed to the OCR engine to verify whether an alternative target text line exists or the whole image should be rejected. The effectiveness of the above approach is confirmed by experiments on a testing database consisting of 117 document images captured by C-Pen and ScanEye pen scanners.","Optical character recognition software,
Engines,
Character recognition,
Target recognition,
Text recognition,
Data mining,
Image recognition,
Computer science,
Information systems,
Merging"
Better real-time response for time-share scheduling,"As computing systems of all types grow in power and complexity, it is common to want to simultaneously execute processes with different timeliness constraints. Many systems use CPU schedulers derived from time-share algorithms; because they are based on best-effort policies, these general-purpose systems provide little support for real-time constraints. This paper describes BeRate, a scheduler that integrates best-effort and soft real-time processing using a best-effort programming model in which soft real-time application parameters are inferred from runtime behavior. We show that with no a prior! information about applications, BeRate outperforms Linux when scheduling workloads containing soft real-time applications.","Real time systems,
Processor scheduling,
Delay,
Scheduling algorithm,
Application software,
Runtime,
Degradation,
Quality of service,
Computer science,
Linux"
Combination of lower bounds in exact BDD minimization,"Ordered binary decision diagrams (BDDs) are a data structure for efficient representation and manipulation of Boolean functions. They are frequently used in logic synthesis and formal verification. The size of BDDs depends on a chosen variable ordering, i.e. the size may vary from linear to exponential, and the problem of improving the variable ordering is known to be NP-complete. In this paper we present a new exact branch & bound technique for determining an optimal variable order In contrast to all previous approaches, that only considered one lower bound, our method makes use of a combination of three bounds and by this avoids unnecessary computations. The lower bounds are derived by generalization of a lower bound known from VLSI design. They allow to build the BDD either top down or bottom up. Experimental results are given to show the efficiency of our approach.",
Cultural swarms,"In the previous work it was observed that certain problem solving phases emerged during the optimization process for a real-valued functional surface within a cone-world environment using a cultural algorithm. The cultural algorithm was configured using five knowledge sources in the belief space, and evolutionary programming model in the population space (Reynolds and Saleem, 2003). It turned out that the five knowledge sources exhibited a swarming behavior at the meta level while solving the problem (Iacoban and Reynolds, 2003). In this paper we investigate whether this swarming behavior at the meta level induces swarming at the population level. Our results show that each knowledge source can control interacting flock of individuals in the population space.",
MAC performance analysis and enhancement 100 Mbps data rates for IEEE 802.11,"The IEEE 802.11 specifications provide up to 54 Mbps data rate. Furthermore, the industry is seeking higher data rates (HDR's) over 100 Mbps for IEEE 802.11a extension. In this paper, we explore the overhead of HDR's to find out whether the MAC is good enough for the increasing data rates and what to expect as the industry seeks much higher data rates. We prove that a theoretical throughput upper limit and a theoretical delay lower limit exist for IEEE 802.11 protocols. The existence of such limits indicates that the overhead must be reduced to get good performance for HDR's. In order to reduce overhead, we introduce a burst transmission and acknowledgement (BTA) mechanism, in which, instead of acknowledging each packet, a burst of packets is received first, and then the whole burst is acknowledged one time.","Performance analysis,
Wireless LAN,
Throughput,
Delay,
Media Access Protocol,
Streaming media,
Computer science,
USA Councils,
Data communication,
Ethernet networks"
Toward ubiquitous intelligent robotics,"This paper proposes a concept of robots called ubiquitous intelligent robot and introduces our research efforts toward establishing its fundamental technology. A ubiquitous intelligent robot is a kind of embodied intelligence that encompasses the sphere of people's activities, records these activities, and supports people. Under this concept, we have developed an intelligent environment for capturing our experiences and a wearable event recording system for medical care. These developments mainly focus on recording people's activities along with their environments.","Intelligent robots,
Cities and towns,
Intelligent systems,
Laboratories,
Wearable sensors,
Intelligent networks,
Concrete,
Medical robotics,
Home appliances,
Audio recording"
A content analytic comparison of FTF and ALN case-study discussions,"While much research has shown that ALNs can produce learning equivalent to FTF classrooms, there has been little empirical research that explicitly and rigorously explores similarities and differences between the learning processes that occur in ALN and FTF activities. Transcripts from eight case study discussions, 4 FTF, 4 ALN, were content analyzed. The study used a content analytic framework derived primarily from previous work of Anderson, Archer, Garrison and Rourke. These authors developed a model that studies cognitive, social, and teaching processes in ALN discussions. Based on the work of Aviv (2000), the current scheme also considers characteristics of the discourse process. The findings provide evidence that ALNs generate high levels of cognitive activity, at least equal to, and in some cases superior to, the cognitive processes in the FTF classroom.","Collaborative work,
Collaboration,
Education,
Vehicles,
Writing"
A multimodal learning interface for word acquisition,"We present a multimodal interface that learns words from natural interactions with users. The system can be trained in an unsupervised mode in which users perform everyday tasks while providing natural language descriptions of their behavior. We collect acoustic signals in concert with user-centric multisensory information from non-speech modalities, such as user's perspective video, gaze positions, head directions and hand movements. A multimodal learning algorithm is developed that firstly spots words from continuous speech and then associates action verbs and object names with their grounded meanings. The central idea is to make use of non-speech contextual information to facilitate word spotting, and utilize temporal correlations of data from different modalities to build hypothesized lexical items. From those items, an EM-based method selects correct word-meaning pairs. Successful learning has been demonstrated in the experiment of the natural task of ""stapling papers"".","Hidden Markov models,
Speech recognition,
Humans,
Learning systems,
Computer science,
Man machine systems,
Computer vision,
Computational modeling,
Natural languages,
Pattern recognition"
The cross-course software engineering project at the NTNU: four years of experience,"Many software engineering courses include all-term projects to convey principles relating to large-scale multi-person development. But even such projects will easily be too small and simple, unless a sufficient amount of study time is allocated to them. This time may be hard to find, especially in strictly programmed profession studies where a lot of general theory courses have to be taken. This paper reports on the experiences from a software engineering project where the solution to the above problem has been to have several courses share one project. This had some advantages. First of all, it allows time for a bigger and more complex project with reasonable sacrifices of ""own time"" in each of the participating courses. Equally important, it is possible to show connections between the courses. In spite of these advantages, there have also been problems with the project, still leaving room for improvement.","Software engineering,
Management training,
Information science,
Large-scale systems,
Books,
Calendars,
Conference management,
Information management,
Project management,
Engineering management"
An IP traceback technique against denial-of-service attacks,"Reflector attack [Vern Paxson (2001)] belongs to one of the most serious types of denial-of-service (DoS) attacks, which can hardly be traced by contemporary traceback techniques, since the marked information written by any routers between the attacker and the reflectors will be lost in the replied packets from the reflectors. We propose a reflective algebraic marking scheme for tracing DoS and DDoS attacks, as well as reflector attacks. The proposed marking scheme contains three algorithms, namely the marking, reflection and reconstruction algorithms, which have been well tested through extensive simulation experiments. The results show that the marking scheme can achieve a high performance in tracing the sources of the potential attack packets. In addition, it produces negligible false positives; whereas other current methods usually produce a certain amount of false positives.","Computer crime,
Floods,
Broadcasting,
Computer science,
Reflection,
Reconstruction algorithms,
Testing,
Telecommunication traffic"
Distributed shared memory using the .NET framework,"The paper introduces a software-only object based Distributed Shared Memory (DSM) implementation designed as an extension to the Microsoft .NET Framework, which adds facilities for sharing objects following the Multiple Readers Multiple Writer (MRMW) memory model, including object replication. The presented implementation is facilitated by a memory coherence protocol previously developed by the author, which uses group communication using IP multicasting and delivers causally consistent memory. The paper describes an attempt to implement the DSM wholly in the context of .NET Remoting by extending it with an UDP-based channel service, a proxy implementation for object access interception and threads performing the receiving and sending parts of the coherence protocol, as well as the final implementation approach. The described DSM implementation allows the construction of distributed applications with a simple programming model, that can be deployed in any networked environment where multicast is available.",
Polling an image for circles by random lines,,"Image sampling,
Image edge detection,
Coaxial components,
Testing,
Pixel,
Image retrieval,
Curve fitting,
Collaboration,
Computer science,
Programmable control"
Test adequacy assessment for UML design model testing,"Systematic design testing, in which executable models of behaviors are tested using inputs that exercise scenarios, can help reveal flaws in designs before they are implemented in code. We present a testing method in which executable forms of the Unified Modeling Language (UML) models are tested. The method incorporates the use of test adequacy criteria based on UML model elements in class diagrams and interaction diagrams. Class diagram criteria are used to determine the object configurations on which tests are run while interaction diagram criteria are used to determine the sequences of messages that should be tested. The criteria can be used to define test objectives for UML designs. In this paper, we describe and illustrate the use of the proposed test method and adequacy criteria.","Unified modeling language,
System testing,
Software testing,
Computer science,
Programming,
Fault detection,
Software design,
Collaborative software,
Software development management,
Software standards"
Performance and cost effectiveness of a cluster of workstations and MD-GRAPE 2 for MD simulations,,
TJIDS: an intrusion detection architecture for distributed network,"We present TJIDS (Tianjin intrusion detection system), a network intrusion detection system whose main functionality is to detect and respond to malicious attacks in distributed network. The main novelty in TJIDS is its intelligent distributed agent architecture to enable distributed intrusion detection with dynamic policy change, as the treat pattern changes. We have adopted a multilevel agent technique, and applied genetic algorithm to this agent-based intrusion detection system. The advantage of our architecture is its ability to perform dynamic policy update in intrusion detection system through wireless net gate, and respond intrusions by distributed agents. Key concepts and preliminary results are presented.","Intrusion detection,
Intelligent agent,
Information security,
Communication system security,
Computer architecture,
Sun,
Computer science,
Genetic algorithms,
Information technology,
Computer hacking"
An examination of DSLs for concisely representing model traversals and transformations,"A key advantage for the use of a domain-specific language (DSL) is the leverage that can be captured from a concise representation of a programmer's intention. This paper reports on three different DSLs that were developed for two different projects. Two of the DSLs assisted in the specification of various modeling tool ontologies, and the integration of models across these tools. On another project, a different DSL has been applied as a language to assist in aspect-oriented modeling. Each of these three languages was converted to C++ using different code generators. These DSLs were concerned with issues of traversing a model and performing transformations. The paper also provides quantitative data on the relative sizes of the intention (as expressed in the DSL) and the generated C++ code. Observations are made regarding the nature of the benefits and the manner in which the conciseness of the DSL is best leveraged.",
Billboard advertising detection in sport TV,"Precise visibility measuring of billboard advertising is a key element for organizers and broadcasters to make cost effective their sport live relay. However, this activity currently is very manpower and time consuming as it is manually processed for the moment. In this paper we describe a technique for detection of commercial advertisement in sport TV. Based on some a priori knowledge of sport field and commercial advertisement, our technique makes use of fast Hough transform and text's geometry features in order to extract advertisement from sport TV images. Our experiments show that our technique achieves more than 90% accuracy rate.","Advertising,
Image edge detection,
Image segmentation,
TV broadcasting,
Geometry,
Color,
Filters,
Mathematics,
Computer science,
Broadcast technology"
Minimizing request blocking in all-optical rings,"In all-optical networks that use WDM technology it is often the case that several communication requests have to be blocked, due to bandwidth and technology limitations. Minimizing request blocking is therefore an important task calling for algorithmic techniques for efficient routing and wavelength assignment. Here we study the problem for rings under both the undirected and the directed settings, corresponding to symmetric and one-way communication respectively. The problem in graph-theoretic terms can be formulated as the maximum routing and path coloring problem. We present a chain-and-matching technique for routing requests and coloring the corresponding paths which gives constant approximations for both the undirected and the directed cases. For the undirected problem we obtain a 2/3-approximation algorithm; this corresponds to a considerable increase in the number of satisfied requests compared to the best known algorithm so far, due to Wan and Liu (1998), that achieves a 1 - 1/e ratio using iteratively a maximum edge-disjoint paths algorithm. For the directed case, we also introduce a balanced matching method which, combined with the chain-and-matching technique, gives a 7/11-approximation algorithm. This algorithm also improves upon the (1 $1/e)-approximation algorithm that can be obtained by extending the iterative method of Wan and Liu.","Iterative algorithms,
Routing,
Computer science,
Wavelength division multiplexing,
Approximation algorithms,
All-optical networks,
Iterative methods,
Optical fiber networks,
Mathematics,
Computational Intelligence Society"
A Way-Halting Cache for Low-Energy High-Performance Systems,"We have designed a low power four-way setassociativecache that stores the four lowest-order bits of all way’stags into a fully associative memory, which we call the halt tagarray. The comparison of the halt tag array with the desired tagoccurs concurrently with the address decoding that determineswhich tag and data ways to read from. The halt tag array predeterminesmost tags that cannot match due to their low-orderfour bits mismatching. Further accesses to ways with knownmismatching tags are then halted, thus saving power. Our halttag array has the additional feature of using static logic only,rather than dynamic logic used in highly-associative caches,making our cache consumes even less power. Our result shows55% savings of memory access related energy over a conventionalfour-way set-associative cache. We show nearly 2x energy savingscompared with highly associative caches, while imposing noperformance overhead and only 2% cache area overhead.","Decoding,
Logic arrays,
Circuits,
Cams,
Switches,
Computer science,
Design engineering,
Power engineering and energy,
Embedded computing,
Power engineering computing"
"Formation, routing, and maintenance protocols for the BlueRing scatternet of Bluetooths","The basic networking unit in Bluetooth is piconet, and a larger-area Bluetooth network can be formed by multiple piconets, called scatternet. However, the structure of scatternets is not defined in the Bluetooth specification and remains as an open issue at the designers' choice. It is desirable to have simple yet efficient scatternet topologies with well supports of routing protocols, considering that Bluetooths are to be used for personal-area networks with design goals of simplicity and compactness. In the literature, although many routing protocols have been proposed for mobile ad hoc networks, directly applying them poses a problem due to Bluetooth's special baseband and MAC-layer features. In this work, we propose an attractive scatternet topology called BlueRing which connects piconets as a ring interleaved by bridges between piconets, and address its formation, routing, and topology maintenance protocols. The BlueRing architecture enjoys the following nice features. First, routing on BlueRing is stateless in the sense that no routing information needs to be kept by any host once the ring is formed. This would be favourable for environments such as Smart Homes where computing capability is limited. Second, the architecture is scalable to median-size scatternets easily (e.g., around 50/spl sim/70 Bluetooth units). In comparison, most star- or tree-like scatternet topologies can easily form a communication bottleneck at the root of the tree as the network enlarges. Third, maintaining a BlueRing is an easy job even as some Bluetooth units join or leave the network. To tolerate single-point failure, we propose a protocol-level remedy mechanism. To tolerate multi-point failure, we propose a recovery mechanism to reconnect the BlueRing. Graceful failure is tolerable as long as no two or more critical points fail at the same time. As far as we know, the fault-tolerant issue has not been properly addressed by existing scatternet protocols yet. In addition, we also evaluate the ideal network throughput at different BlueRing sizes and configurations by mathematical analysis. Simulations results are presented, which demonstrate that BlueRing outperforms other scatternet structures with higher network throughput and moderate packet delay.","Routing protocols,
Scattering,
Bluetooth,
Personal area networks,
Network topology,
Computer architecture,
Throughput,
Mobile ad hoc networks,
Baseband,
Bridges"
Trade determination in multi-attribute exchanges,"Electronic exchanges are double-sided marketplaces that allow multiple buyers to trade with multiple sellers, with aggregation of demand and supply across the bids to maximize the revenue in the market. Two important issues in the design of exchanges are (1) trade determination (determining the number of goods traded between any buyer-seller pair) and (2) pricing. W address the trade determination issue for one-shot, multi-attribute exchanges that trade multiple units of the same good. The bids are configurable with separable additive price functions over the attributes and each function is continuous and piecewise linear. We model trade determination as mixed integer programming problems for different possible bid structures and show that even in two-attribute exchanges, trade determination is NP-hard for certain bid structures. We also make some observations on the pricing issues that are closely related to the mixed integer formulations.","Pricing,
Piecewise linear techniques,
Consumer electronics,
Linear programming,
Laboratories,
Computer science,
Automation,
Internet,
Information technology,
Cost function"
Integrative negotiation in complex organizational agent systems,"This paper introduces an integrative negotiation mechanism, which enables agents to choose any attitude from the extremes of self-interested and fully cooperative to those that are partially self-interested and partially cooperative. Experimental work verifies this mechanism and explores the question whether it always improves the social welfare to have an agent be completely cooperative. It is found that it is good for the organization to have agents being partially cooperative in their local negotiation with other agents rather than being fully cooperative, in order to deal more effectively with the uncertainty of not having a more informed view of the state of the entire agent organization.","Multiagent systems,
Uncertainty,
Resource management,
Government,
Supply chains,
Information science,
Computer science,
Laboratories,
Problem-solving,
Contracts"
A novel fuzzy clustering algorithm,"In this paper we proposed a novel fuzzy clustering algorithm, called a fuzzy compactness and separation (FCS), based on a fuzzy scatter matrix. The compactness is measured by a fuzzy within variation and the separation is measured by a fuzzy between variation. The proposed FCS objective function is a modification of the FS validity index proposed by Fukuyama and Sugeno (1989) and also a generalization of the fuzzy c-means (FCM). The FCS algorithm assigns a hard kernel boundary for each cluster such that hard memberships and fuzzy memberships could be co-existed in the clustering results. Thus, FCS can be seen as a clustering algorithm with a novel sense between hard c-means and fuzzy c-means. Some numerical examples are demonstrated to show its properties and effectiveness.","Clustering algorithms,
Scattering,
Partitioning algorithms,
Matrix decomposition,
Mathematics,
Computer science,
Partial response channels"
Mobile instant messaging,"We describe a mobile instant messaging system, MIM, designed for mobile environments. During the design of mobile applications, several new problems and possibilities have to be considered that do not exist with applications targeted at desktop PCs. One example of an application not designed for a mobile environment is the current, very popular, instant messaging, typified by systems such as ICQ, AIM and MSN Messenger. We describe why current systems are not suitable in a mobile environment, and present our architecture for a new system, MIM, and show various implementations for different mobile devices such as PDAs, wearable computers and mobile phones.","Application software,
Mobile computing,
Wearable computers,
Personal communication networks,
User interfaces,
Personal digital assistants,
Mobile handsets,
Switches,
Protocols,
Computer science"
Queue-based cost evaluation of mental simulation process in program comprehension,"We present a method to estimate the cost of mental (hand) simulation of programs. In mental simulation, human short term memory is extensively used to recall and memorize values of variables. When the simulation reaches a variable reference, the simulation can be performed easily if the value is still remembered. However, if not, we have to backtrack the simulation until the value is obtained, which is time consuming. Taking the above observation into consideration, we first present a model, called virtual mental simulation model (VMSM), which exploits a queue representing short term memory. The VMSM takes one of the abstract processes recall or backtrack, depending on whether the variable is currently stored in the queue or not. Then, applying cost functions to the VMSM, we derive four dynamic metrics reflecting the cost of mental simulation. In our empirical study, the proposed VMSM metrics reveal that the backtrack process for nonconstant variables gives a significant impact on the cost of mental simulation. Since the proposed method can be fully automated, it can provide a practical means to estimate the cost of mental simulation, which can be also used as a program comprehension measure.",
FPGA-based SIMD processor,A massively parallel single instruction multiple data stream (SIMD) processor designed specifically for cryptographic key search applications is presented. This design exploits fine grain parallelism and the high memory bandwidth available in an FPGA (field programmable gate array) by integrating 95 simple processors and memory on a single FPGA chip. Performance is compared with a previously reported hardwired design on a RC4 key search application.,"Registers,
Random access memory,
Field programmable gate arrays,
Application software,
Read-write memory,
Process design,
Software performance,
Decoding,
Testing,
Computer science"
Ubiquitous computing with service adaptation using peer-to-peer communication framework,"Mobile devices and wireless network infrastructures will be leading users to seamlessly use peer-to-peer services and ubiquitous computing by the growing of the infrastructures. In order to realize to use peer-to-peer services and ubiquitous services seamlessly, a new framework that enables users to use peer-to-peer services and ubiquitous computing is required. Hence, this paper describes a new ubiquitous computing framework, called VPC on KODAMA, using a peer-to-peer mechanism. Virtual Private Community (VPC) is an execution environment for peer-to-peer services, and provides a framework for definition of peer-to-peer services. Peer-to-peer services in VPC are defined as policy packages that have necessary elements to provide the services. Peer-to-peer services are offered in communities by collaboration among roles that are assigned to users. KODMA provides a network infrastructure for agents. Agents in KODAMA have their own community, and represent the communities. Communities have a hierarchy structure by agents residing in other agent's communities. Agents have message filtering policy, and refuse messages that are against the policy. By unifying VPC and KODAMA, a new framework that enables services to define roles and their behavior and to manage logical relationship among communities is provided. VPC on KODAMA enables users to use peer-to-peer services and appliances seamlessly with their mobile devices.","Ubiquitous computing,
Peer to peer computing,
Home appliances,
Packaging,
Laboratories,
Computer network management,
Engineering management,
Information science,
Mobile computing,
Wireless networks"
Undergraduate teaching of real-time scheduling algorithms by developed software tool,"Theoretical foundations of scheduling in the course Real-Time Systems of the undergraduate studies program are extended by the usage of the developed visual software tool. The tool enables teaching of simulated fixed-priority scheduling algorithms. The user simply sets the simulation and algorithm properties and loads the prepared input task sets from the database. Results are obtained within the program or from external applications. The paper deals with the problem of scheduling the mixture of periodic and aperiodic tasks in a uniprocessor system. Results are displayed in the form of a flow report, table report, and diagrams of dependencies of evaluation parameters on the change of periodic and aperiodic workload. The flow report explains the algorithm behavior and the table form and dependency diagrams enable a more precise scheduling analysis and mutual comparison, respectively. Some of the evaluation parameters are the following: aperiodic response time, number of tasks with missed deadlines, utilization of resource and server tasks, resource-breakdown utilization. The software tool improves teaching of the course and the final success of students regarding relevant exam exercises.","Computer science education,
Real time systems,
Processor scheduling,
Computer aided instruction,
Software tools"
A parallel and fault tolerant file system based on NFS servers,"One important piece of system software for clusters is the parallel file system. All current parallel file systems and parallel I/O libraries for clusters do not use standard servers, thus it is very difficult to use these systems in heterogeneous environments. However why use proprietary or special-purpose servers on the server end of a parallel file system when you have most of the necessary functionality in NFS servers already? This paper describes the fault tolerance implemented in Expand (Expandable Parallel File System), a parallel file system based on NFS servers. Expand allows the transparent use of multiple NFS servers as a single file system, providing a single name space. The different NFS servers are combined to create a distributed partition where files are stripped. Expand requires no changes to the NFS server and uses RPC operations to provide parallel access to the same file. Expand is also independent of the clients, because all operations are implemented using RPC and NFS protocol. Using this system, we can join heterogeneous servers (Linux, Solaris, Windows 2000, etc.) to provide a parallel and distributed partition. Fault tolerance is achieved using RAID techniques applied to parallel files. The paper describes the design of Expand and the evaluation of a prototype of Expand, using the MPI-IO interface. This evaluation has been made in Linux clusters and compares Expand with PVFS.","Fault tolerant systems,
File systems,
File servers,
Fault tolerance,
Linux,
Parallel processing,
Computer architecture,
Space technology,
Libraries,
Computer science"
A multilingual concept mapping tool for a diverse world,"Today's world culture wealth is based on the diversity of language and cultures that are an inherent part of the identity of its countries and regions. Education has a preeminent responsibility to preserve world's cultural heritage. Computer-based learning tools among other issues must also take into account the variety of languages. Some issues about the localisation of the CM-ED Concept Map EDitor are presented. First, some aspects related to software localisation are introduced. Then, the main characteristics of CM-ED are briefly described. Finally, the two levels in which localisation techniques have been applied over CM-ED are showed. CM-ED is localisable not only at user interface level but also regarding the final concept maps it generates.","Natural languages,
Cultural differences,
Computer science education,
User interfaces,
Computer industry,
Educational institutions,
Globalization,
Internet,
Programming,
Educational products"
DCMSIM: didactic cache memory simulator,"We present a functional and structural didactic simulator of cache memory systems developed at the Pontifical Catholic University of Minas Gerais, Brazil. The development occurred during the undergraduate Computer Architecture discipline, in the Computer Science course. Its implementation is one part of a new didactic method, in which developers (students of the Computer Architecture discipline) must learn the concepts and theory of the discipline topics to correctly apply them in the simulator. In our simulator, DCMSim, there are features to allow students to construct and verify knowledge, testing and comparing several different configurations and memory access traces.","Cache memory,
Computer science,
Buildings,
Logic,
Control systems,
Computer architecture,
Data engineering,
Computer science education,
Educational programs,
Memory architecture"
Diversity analysis in cellular and multipopulation genetic programming,This paper presents a study that evaluates the influence of the parallel genetic programming (GP) models in maintaining diversity in a population. The parallel models used are the cellular and the multipopulation one. Several measures of diversity are considered to gain a deeper understanding of the conditions under which the evolution of both models is successful. Three standard test problems are used to illustrate the different diversity measures and analyze their correlation with performance. Results show that diversity is not necessarily synonym of good convergence.,"Genetic programming,
Convergence,
Measurement standards,
Testing,
Performance analysis,
Evolutionary computation,
Genetic mutations,
Costs,
Computer science,
Size measurement"
An inexpensive method to teach hands-on digital communications,"Some of the more subtle concepts related to digital communication systems such as signal constellation imbalance, intersymbol interference, pulse shaping ramifications, and performance with in-band interference are quite difficult for most students to grasp from a textbook or lectures alone. This paper describes an inexpensive approach to providing a real-time, hands-on laboratory experience for the students covering these same topics using the Texas Instruments C6711 digital signal processing starter kit along with custom software the authors have developed in MATLAB and C/C++. Expensive specialized test equipment such as a vector signal generator or a vector signal analyzer is not needed using this method. Our initial teaching experience has shown this new method to be highly successful yet very inexpensive when compared to purchasing traditional dedicated test equipment. The software described herein is freely available to educators and students.","Digital communication,
Intersymbol interference,
Test equipment,
Constellation diagram,
Pulse shaping methods,
Instruments,
Digital signal processing,
MATLAB,
Signal generators,
Signal analysis"
BCFG: a configuration management tool for heterogeneous environments,"Since clusters were first introduced, node counts have increased rapidly. Currently, a variety of clusters with more than one thousand nodes are listed on the TOP500 list. In the next three years, clusters with more than four thousand nodes are expected. Cluster management functionality has lagged behind all areas of system software. In order to effectively manage the clusters of today and tomorrow, the basic cluster management software model must change. Current techniques focus on the management of single nodes, as opposed to complete cluster configurations. This approach typically leads to automatic management of compute nodes, while using ad-hoc techniques to manage service nodes. Configuration management is the process where software configurations on clients are installed, updated and verified. To address these issues, we have begun the development of BCFG, a symbolic configuration management tools for heterogeneous clusters. It uses a multi-tiered configuration description.","Computer network management,
Parallel architectures,
System software,
Network operating systems"
Reducing maintenance overhead in DHT based peer-to-peer algorithms,"DHT based peer-to-peer (P2P) algorithms are very promising for their efficient routing performance. However, most commercial P2P systems do not adapt DHT algorithms and still use central facilities or broadcasting based routing mechanisms. One reason impeding the DHT algorithm popularity is the routing information maintenance overhead in DHT algorithms; it generates considerable network traffic and increases P2P system complexity, especially in a highly dynamic environment. We discuss its effects on DHT routing performance and propose our solution to reduce this overhead.","Peer to peer computing,
Routing,
Data structures,
Impedance,
Computer science,
Broadcasting,
Telecommunication traffic,
Secure storage,
Information management,
Fault tolerance"
Specification matching of object-oriented components,"Object-orientation supports software reuse via features such as abstraction, information hiding, polymorphism, inheritance and redefinition. However, while libraries of classes do exist, one of the challenges that still remains is to locate suitable classes and adapt them to meet the specific requirements of the software developer. Traditional approaches to library retrieval are text-based; it is therefore difficult for the developer to express their requirements in a precise and unambiguous manner. A more promising approach is specification-based retrieval, where library component interfaces and requirements are expressed using a formal specification language. In this case retrieval is based on matching formal specifications. In this paper, we describe how existing approaches to specification matching can be extended to handle object-oriented components.","Software libraries,
Formal specifications,
Software engineering,
Information retrieval,
Computer science,
Information technology,
Specification languages"
Adaptive QoS routing based on prediction of local performance in ad hoc networks,"We propose an adaptive QoS routing scheme based on the prediction of the local performance in ad hoc networks. It is implemented by a link performance prediction strategy. Integrated QoS performance in each local area is estimated based on translating the effects of the lower layer parameters into the link state information. Corresponding to the prediction approach, several mechanisms are built to complete the location information management process (i.e., information monitoring, collecting and updating functions). The node movement is characterized by the probabilities of the link state and the prediction of local QoS performance. The QoS routing proposed is adaptive to its node's mobility, and also scalable due to the distributed structure. Theoretical computation and simulation results are presented and discussed.","Routing,
Intelligent networks,
Ad hoc networks,
Information management,
Computer science,
State estimation,
Monitoring,
Computational modeling,
Sun,
Telecommunication traffic"
A prototype of information requirement elicitation in m-commerce,Information requirement elicitation (IRE) is a context-aware and personalized wireless Web service to elicit user information requirements through interactive choice prompts. The authors developed a prototype of IRE and demonstrated its operations in an imagined m-commerce scenario. This article also gives the result of a preliminary usability study.,"Prototypes,
Cellular phones,
Sun,
Computer science,
Context-aware services,
Web services,
Usability,
Web pages,
Wireless networks,
Internet"
Large multidimensional data visualization for materials science,Materials scientists use scientific visualization to explore very large multidimensional data sets. The Atomsviewer visualization system enables telepresence and provides multimodal views of simulation data.,
Design and implementation of real-time digital video streaming system over IPv6 network using feedback control,"In this paper, we discuss a design of a real-time DV (Digital Video) streaming system, which dynamically adjusts packet transmission rate from the source host according to feedback information from the network. In our DV streaming system, the destination host continuously notifies the source host of network status (e.g., the end-to-end packet transmission delay and the packet loss probability in the network). The source host dynamically adjusts its packet transmission rate by lowering the quality of the video stream using a feedback-based control mechanism. Our DV streaming system achieves an efficient utilization of network resources, and prevents packet losses in the network. Thus, our DV streaming system realizes high-quality and real-time video streaming services on the Internet. By modifying the existing DVTS (Digital Video Transmission System), we implement a prototype of our real-time DV streaming system. Through several experimental results, we demonstrate the effectiveness of our DV streaming system.","Real time systems,
Streaming media,
Feedback control,
Bandwidth,
Video on demand,
Web and internet services,
Application software,
Computer networks,
Public policy,
Information science"
On the limits of bottom-up computer simulation: towards a nonlinear modeling culture,"In the complexity and simulation communities there is growing support for the use of bottom-up computer-based simulation in the analysis of complex systems. The presumption is that because these models are more complex than their linear predecessors they must be more suited to the modeling of systems that appear, superficially at least, to be (compositionally and dynamically) complex. Indeed the apparent ability of such models to allow the emergence of collective phenomena from quite simple underlying rules is very compelling. But does this 'evidence' alone 'prove' that nonlinear bottom-up models are superior to simpler linear models when considering complex systems behavior? Philosophical explorations concerning the efficacy of models, whether they be formal scientific models or our personal worldviews, has been a popular pastime for many philosophers, particularly philosophers of science. This paper offers yet another critique of modeling that uses the results and observations of nonlinear mathematics and bottom-up simulation themselves to develop a modeling paradigm that is significantly broader than the traditional model-focused paradigm. In this broader view of modeling we are encouraged to concern ourselves more with the modeling process rather than the (computer) model itself and embrace a nonlinear modeling culture. This emerging view of modeling also counteracts the growing preoccupation with nonlinear models over linear models.","Computer simulation,
Computational modeling,
Mathematical model,
Predictive models,
Analytical models,
Educational institutions,
Coherence,
Mathematics,
Iron,
Physics"
A semiconductor industry perspective on future directions in ECE education,"This paper looks at changes in the education of engineers made necessary by unprecedented challenges within the semiconductor industry, from the perspective of a not-for-profit consortium of semiconductor manufacturers. To design and implement curricula to serve as a basis for the 40-year career of engineering graduates is a daunting challenge in this rapidly evolving environment. The authors propose ten principles that should be of use in connecting the graduates of today with the engineering careers of tomorrow.",
Benchmark and framework for encouraging research on multi-threaded testing tools,"A problem that has been getting prominence in testing is that of looking for intermittent bugs. Multi-threaded code is becoming very common, mostly on the server side. As there is no silver bullet solution, research focuses on a variety of partial solutions. In this paper (invited by PADTAD 2003) we outline a proposed project to facilitate research. The project goals are as follows. The first goal is to create a benchmark that can be used to evaluate different solutions. The benchmark, apart from containing programs with documented bugs, will include other artifacts, such as traces, that are useful for evaluating some of the technologies. The second goal is to create a set of tools with open API that can be used to check ideas without building a large system. For example an instrumentor will be available, that could be used to test temporal noise making heuristics. The third goal is to create a focus for the research in this area around which a community of people who try to solve similar problems with different techniques, could congregate.",
Improved analysis of D*,"D* is a planning method that always routes a robot in initially unknown terrain from its current location to a given goal location along a shortest presumed unblocked path. The robot moves along the path until it discovers new obstacles and then repeats the procedure. D* has been used on a large number of robots. It is therefore important to analyze the resulting travel distance. Previously, there has been only one analysis of D*, and it has two shortcomings. First, to prove the lower bound, it uses a physically unrealistic example graph which has distances that do not correspond to distances on a real map. We show that the lower bound is not smaller for grids, the kind of map-based graph on which D* is usually used. Second, there is a large gap between the upper and lower bounds on the travel distance. We considerably reduce this gap by decreasing the upper bound on arbitrary graphs, including grids. To summarize, we provide new, substantially tighter bounds on the travel distance of D* on grids, thus providing a realistic analysis for the way D* is actually used.","Robot sensing systems,
Mobile robots,
Prototypes,
Path planning,
Navigation,
Land vehicles,
Educational institutions,
Technology planning,
Motion planning,
Computer science"
Teaching and learning combined (TLC),"Discusses Teaching and Learning Combined (TLC). TLC is how professors learn, so it makes sense that students can learn that way, too!.","Education,
Drives,
Cities and towns,
Libraries,
Solids,
Educational institutions,
Physics"
Website evolution based on statistic data,"Maintenance and evolution are critical for website since the requirements often change, the developing cycle is short, while the life cycle is long. In order to make progress in this area, we focus on the users' responses and attitudes. So we begin our work at the log files on the side of website server, gathering the users' visiting information and the server's responses. Thus we obtain our weighted structure model. Then, we analyze and discuss such information to identify the key pages, the predominate pages and the users' visiting patterns. And we can improve the testing efficiency based on these knowledge. All these will help to improve the structure of the site, fulfill the functionality of the site, and enhance users' visiting efficiency.","Statistics,
Computer science,
Laboratories,
Software engineering,
File servers,
Information analysis,
Data mining,
History,
Uniform resource locators,
Statistical analysis"
Pixel classification through divergence-based integration of texture methods with conflict resolution,"This paper presents a new technique for combining multiple texture feature extraction methods in order to classify the pixels of an input image into a set of texture models of interest. The problem of integrating multiple texture methods for classification purposes is cast as a collaborative decision making problem. Each texture method is considered to be an expert that gives an opinion about the membership of every input image pixel to each texture model, along with a conviction about that judgement. A conviction measure based on the Kullback J-divergence between texture models is proposed, along with an arbitration mechanism that combines those convictions by taking into account conflicts that may occur when different experts disagree with a similar strength. The proposed technique is compared to previous pixel-based texture classifiers by using real textured images.","Pixel,
Intelligent robots,
Feature extraction,
Collaboration,
Decision making,
Robot vision systems,
Computer vision,
Computer science,
Mathematics,
Soil"
Modeling and performance analysis of a multiserver multiqueue system on the grid,"Grid computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and high-performance orientation. A Grid integrates and coordinates resources and users that live within different control domains with the goal of delivering nontrivial quality of service (QoS). Consequently, task management and scheduling is of central importance for the Grid-based systems. In this paper, we model the task dispatching and selecting on the distributed Grid computing system by a multiserver multiqueue (MSMQ) model, and propose a modeling and analysis technique based on Stochastic Petri Net (SPN) methods. An approximate analysis technique is also proposed to reduce the complexity of the model.","Performance analysis,
Processor scheduling,
Grid computing,
Quality of service,
Resource management,
Distributed computing,
Dispatching,
Stochastic systems,
Random access memory,
Computer science"
A comparative study of restoration schemes and spare capacity assignments in mesh networks,"This paper presents the results of a comparative study of spare capacity assignment for original quasipath restoration (OQPR), improved quasipath restoration (IQPR), link restoration (LR), path restoration (PR) and link-disjoint path restoration (LDPR) schemes. Numerical results indicate that the restoration schemes studied can be sorted from most expensive to least expensive (spare capacity assignment cost) in the following order: LR, OQPR, IQPR, LDPR and PR. Since IQPR is computationally very efficient, simpler than PR, scalable, and economical in spare capacity assignment, it provides a good alternative to PR when quick restoration is desired. However, due to the potential difficulty in rapid failure isolation coupled with the increasing importance of restoration speed and simplicity, LDPR is an attractive scheme. A number of networks with different topologies and projected traffic demand patterns are used in the experiments to study the effect of various network parameters on spare capacity assignment cost. The experimental analysis shows that network topology, demand patterns and the average number of hops per primary route have a significant impact on the spare capacity assignment cost savings offered by one scheme over the other.",
Design of kernels for support multivector machines involving the Clifford geometric product and the conformal geometric neuron,This paper presents the design of kernels for nonlinear support vector machines using the Clifford geometric algebra framework. In this study we present the design of kernels involving the Clifford or geometric product making use of nonlinear mappings which map multi-vectors into higher dimensional geometric algebra. We introduce also the conformal geometric neuron for geometric classification. Experiments are given to demonstrate the usefulness of the approach.,
Peer-to-peer policy management system for wearable mobile devices,,"Peer to peer computing,
Resource management,
Ad hoc networks,
Communication system security,
Computer science,
Educational institutions,
Wearable computers,
Mobile computing,
Protection,
Wireless sensor networks"
Application service providing as part of intelligent decision support for supply chain management,"A prominent trend in the software industry in the late 1990s was the development of the application service providing business model. Application service providers (ASP) offer their customers access to software applications via a network instead of installing them on the customer's in-house computer system. ASPs host and manage applications from a central location and charge a fee for their services. Nevertheless, there are currently serious reservations and myths about what ASP can mean for companies and whether it can be applied in the context of business networks. In this paper we investigate the impact of this business model on decision support technologies in the supply chain management field. Furthermore, we provide a survey of some ASP issues regarding supply chain management software.","Supply chain management,
Supply chains,
Application software,
Application specific processors,
Transportation,
Decision support systems,
Electronic mail,
Companies,
Logistics,
Humans"
JSPick-a server pages design recovery tool,"This paper presents the reverse engineering tool JSPick, which recovers page signatures and form types from server pages based presentation layers. A formal semantics of the tool is given in pseudo-evaluation style.","Java,
Safety,
HTML,
Reverse engineering,
Web server,
Computer science,
Documentation,
Specification languages,
Graphical user interfaces,
Couplings"
An intelligent tutoring system prototype for learning to program Java/spl trade/,"The ""Java/spl trade/ Intelligent Tutoring System"" (JITS) research project involves the development of a programming tutor designed for students in their first programming course in Java/spl trade/ at the college and university level. We present an overview of the architectural design, the AI techniques used, and the user interface. This project is a prototype being constructed which will model the domain of a small subset of the Java/spl trade/ programming language in a very specific context. Research is in progress and it is hypothesized that the completed prototype will be sufficient to prove the concept and that a fully developed Java/spl trade/ intelligent tutoring system will provide an interactively-rich learning environment for students resulting in increased achievement. Based on the success of similar intelligent tutoring systems, it is also hypothesized that these students will be able to learn programming skills and knowledge more quickly and effectively than students in traditional educational settings.","Intelligent systems,
Prototypes,
Java,
Programming profession,
Artificial intelligence,
User interfaces,
Feedback,
Educational institutions,
Engines,
Information management"
Text-indicated writer verification using hidden Markov models,"We propose an HMM-based text-indicated writer verification method, which is based on a challenge and response type of authentication process. In this method, a different text including ordinary characters is used on every occasion of verification. This text can be selected automatically by the verification system so as to reflect a specific writer's personal features. The specific writer is accepted only when the same text as indicated by the verification system is inputted, and the system can verify the writer's personal features from the inputted text. Moreover, the characters used in the verification process can be different from those in the enrollment process. This method makes it more difficult to get away with forged handwriting than the previous methods using only signatures. In the proposed method, the characteristics of the indicated text and each writer's personal features are both represented by using hidden Markov models.",
Level set methods in image science,"In this article, we discuss the question ""what level set methods can do for image science"". We examine the presence of these and related methods in image science and introduce some relevant level set techniques that are potentially useful for this class of applications. We will show that image science demands multidisciplinary knowledge and flexible but still robust methods, and that is why the level set method has become a thriving technique in this field.","Level set,
Mathematics,
Robustness,
Image segmentation,
TV,
Digital images,
Image processing,
Computer graphics,
Computer vision,
Linear algebra"
Java mobile agents on project JXTA peer-to-peer platform,"Over the last year peer-to-peer (p2p) implementations have evolved from experimental to sophisticated systems. Mobile agents have a long e-commerce history dating back to the middle 1990's, and have not been generally accepted by the corporate, e-commerce mainstream. This is because of a misfit with the client-server model. With the advent of p2p networks, sub-network, ecosystem like communities, that are more suitable for mobile agent behaviour, now exists. In this paper we discuss the application of mobile agents to e-commerce on peer-to-peer (P2P) networks, as well as an implementation on the project JXTA, P2P platform. This implementation demonstrates both the suitability of mobile agents on p2p networks, and the ease of use of the project JXTA protocols.","Java,
Mobile agents,
Peer to peer computing,
Protocols,
Network address translation,
Security,
Internet,
Computer crime,
Virtual manufacturing,
US Department of Transportation"
Tabbycat: an inexpensive scalable server for video-on-demand,"Tabbycat is a video server prototype demonstrating the benefits of a proactive approach for distributing popular videos on demand to a large customer base. Rather than reacting to individual customer requests, Tabbycat broadcasts the contents of the most popular videos according to a fixed schedule. As a result, the number of customers watching a given video does not affect the cost of distributing it. We found that one workstation with a single ATA disk drive and a fast Ethernet interface could distribute three two-hour videos while achieving a maximum customer waiting time of less than four minutes.","Broadcasting,
Multimedia communication,
Protocols,
Video on demand,
Network servers,
Computer science,
Prototypes,
Ethernet networks,
Scheduling,
Lifting equipment"
An ant-algorithm for the weighted minimum hitting set problem,"We present an ant-based algorithm for finding good, near optimal solutions to the weighted minimum hitting set problem. We compare our results with the ones obtained by a greedy procedure and by an ad hoc genetic algorithm.","Genetic algorithms,
Chemicals,
Greedy algorithms,
Mathematics,
Computer science,
NP-complete problem,
Routing,
Scheduling,
Emergency services,
Virtual colonoscopy"
A cooperative framework for inter-organizational workflow system,"Workflow management systems (WfMSs) are accepted worldwide due to their capability to model and control business processes. With WfMS, enterprises can improve work efficiency and react to the emergent situation quickly. However, most WfMSs adopt the centralized architecture in an organization so that they usually lack of a cooperative communication mechanism across different organizations. In this paper, we propose a cooperative framework for inter-organizational workflow systems. The framework consists of an inter-workflow meta-model, CA-PLAN (Cooperative Agentflow Process LANguage), and a prototype system based on Agentflow, a system developed in our laboratory. The cooperation mechanism between WfMSs in CA-PLAN is modeled as a Remote Call Process (RCP) paradigm. A process service is a mechanism that defines a process to participate in an inter-organizational process among different WfMSs and specifies the associated arguments in and out. A remote process is a proxy mechanism that refers to a process service on another WfMS. RCP provides the mechanism by which the process service and the remote process communicate and pass information back and forth and process monitor mechanism. The mechanism, also allowing dynamic changes and reconfiguration, can adapt dynamic and competitive business environment. Through RCP, the cooperative process across organizations becomes simple, faster, and flexible.",
Virtual Petri nets as a modular modeling method for planning and control tasks of FMS,"The modeling of automated manufacturing systems has been studied to cope with the production planning and control problems. Petri Nets have been applied to model these systems because they provide resources to represent the systems behavior. When the systems are too large or complex, the modeling task is difficult and the elements in the model are too many for a simple under-standing and analyze task. This work proposes an method for modeling these systems in order to minimize the difficulties mentioned. The proposed method introduces the virtual Petri net (VPN) and a modular strategy considering the shared resources and the alternative process planning. Using this method, a system has been modeled, initially considering the systems elements for the modeling job, and then a modules linking way can be used in order to build the complete model.","Petri nets,
Flexible manufacturing systems,
Process planning,
Automatic control,
Production planning,
Joining processes,
Power system modeling,
Manufacturing systems,
Virtual private networks,
Computer science"
High-rate LDPC codes from unital designs,"The paper presents a construction of very high-rate low-density parity-check (LDPC) codes based on incidence matrices of unital designs. Like the projective geometry and oval designs, unital designs exist with incidence matrices which are significantly rank deficient. Thus high-rate LDPC codes with a large number of linearly dependent parity-check equations can be constructed. The LDPC codes from unitals have Tanner graphs free of 4-cycles and perform well with iterative decoding, offering new LDPC codes at rates and lengths not available with existing algebraic LDPC codes.","Parity check codes,
Geometry,
Iterative decoding,
Australia,
Iterative algorithms,
Computer science,
Equations,
Sparse matrices,
Displays,
Error analysis"
Determination of three-dimensional voxel sensitivity for two- and three-headed coincidence imaging,"Recently, SPECT and positron emission tomography imaging modalities have been hybridized so that positron coincidence detection can be accomplished with SPECT systems. Originally, only systems with two opposing camera heads were employed. Recent developments to improve sensitivity include the addition of a third camera head. Several authors have developed methods to calculate line-of-response and voxel sensitivities, known as rotational and geometric weights, respectively. These weights are important for use as normalization factors in iterative image reconstruction, as well as to provide insight into the nonuniformity of voxel sensitivity across the reconstructed field-of-view. Although their formulations use analytic expressions, the equations derived for the voxel sensitivities involve an integral which cannot be computed in closed form; that is, one must use a numerical approximation. This may affect the voxel sensitivity in that the accuracy and speed of such a discrete calculation are heavily dependent on the mesh size used. The authors' alternative approach, that does not rely on the numerical approximation, is to directly calculate the solid angle subtended by each voxel with the detectors over all detector positions. They include results for two camera heads 180/spl deg/ apart (DUAL), three camera heads 120/spl deg/ apart (TRI), and three camera heads 90/spl deg/ and 180/spl deg/ apart (C-SHAPE), with and without axial collimation.","Head,
Cameras,
Image reconstruction,
Detectors,
Positron emission tomography,
Integral equations,
Solids,
Mathematics,
Computer science,
Biomedical imaging"
Project management model for a physically distributed software development environment,"The objective of this study is to propose a project management model which includes the unified process (UP) and UML language, for e-business software development in a physically distributed environment. The research is a qualitative study, aiming to develop methods and models. The main research method is the case study, according to Yin (1992). Initial results point towards a model, which includes the spiral life cycle type, the development process of object-oriented systems (using UML specification language and the unified process as proposed by RUP), and the incorporation of the procedural approach proposed by PMBOK (2000). The proposed model is divided into 6 phases, where each one of these phases contains a set of associated activities. In the future, the intention is to develop a support software for the model and apply this software into the environment in study. This extremely important study line demonstrates that the business world and business practices in the software development area are moving ahead of existent theories and conceptual models in this area. This study, yet in its initial phases, starts to bring significant results that contribute to meet market expectations nowadays.","Project management,
Programming,
Object oriented modeling,
Unified modeling language,
Risk management,
Computer science,
Application software,
Costs,
Productivity,
Scholarships"
Improvements to the *CGA enabling online intrinsic evolution in compact EH devices,"Recently, we proposed a neuromorphic intrinsic online evolvable hardware (EH) system designed to learn control laws of physical devices. Since we intend to eventually build this device using mixed signal VLSI techniques, and because we intend to address control applications in which small size and low power consumption are critical, we are extremely concerned with the design of physically compact devices. This paper focuses on the evolutionary algorithm (EA) portion of our proposed system. We discuss modifications to our previously reported *CGA that significantly increases its performance against dynamic optimization problems without significantly increasing the amount of hardware required for implementation. We demonstrate the efficacy of our improvement by testing against two series of moving peak benchmarks. We conclude with discussions of both the implications of our findings and our plans for future work.","Hardware,
Very large scale integration,
Evolutionary computation,
Engines,
Legged locomotion,
Control systems,
Vehicle dynamics,
Benchmark testing,
Computer science,
Design engineering"
Laser pointer interaction for camera-registered multiprojector displays,Camera-based registration techniques use a camera to automatically align the geometry of large scale displays composed of multiple overlapping light projectors. We present a method to implement a laser-pointer interface for these displays which capitalizes on the presence of this camera in the display environment and its precomputed knowledge about the display geometry. We describe our camera-registration framework and laser pointer integration and detail the functionality provided to applications. Our laser pointer integration supports both single pointer and multipointer interaction.,"Cameras,
Large-scale systems,
Computer displays,
Laser theory,
Laser modes,
Geometrical optics,
Laser applications,
Geometry,
Computer science,
Registers"
IBHIS: integration broker for heterogeneous information sources,"Effective use of heterogeneous, distributed information in a coherent, integrated fashion has long been a ""holy grail"". The UK Health and Social care domain is an example where a global view is needed to facilitate decision making, whilst having ethical and legal concerns. The IBHIS project aims to provide an integrated broker that enables coherent use of a set of distributed, heterogeneous data sources, whilst ensuring trustworthiness and audit. This paper presents a service-oriented federated architecture for the IBHIS broker.","Computer science,
Hospitals,
Distributed computing,
Decision making,
Law,
Legal factors,
Management information systems,
Warehousing,
Large scale integration,
Service oriented architecture"
Maximum-shortest-path (MSP) is not optimal for a general N /spl times/ N torus,"A shortest-path routing is optimal if it maximizes the probability of reaching the destination from a given source, assuming that each link in the system has a given failure probability. An approximation for the shortest-path routing policy, maximum-shortest-path (MSP) routing was proposed by Wu (see ibid., vol.48, no.3, p.247-55, 1999). Wu shows that: MSP is optimal in the mesh and hypercube networks; MSP is at least suboptimal in the torus network; MSP is optimal for 6 /spl times/ 6 and 8 /spl times/ 8 tori; and conjectured that MSP is optimal for 2-D tori in general. This short paper shows that, contrary to the claims by Wu, MSP is not optimal for a general N /spl times/ N torus-specifically, MSP is not optimal for a 12 /spl times/ 12 torus, and its optimal routing depends on the success probability.","Routing,
Computer science,
Hypercubes,
Costs"
A data-type-based approach for identifying corresponding attributes in heterogeneous databases,"In order to support interoperability among databases, the probability and efficiency of using metadata and data contents to automate some aspect of semantic integration become a key challenge. An important task of semantic integration in heterogeneous database is to determine which fields refer to the same data. Although many methods can be employed to automate the process, we argue that the existing methods of identifying corresponding attributes are time-consuming. In this paper we contribute to lower time complexity without reducing the precision ratio and recall ratio by providing a more efficient method, a data-type-based approach, to identify corresponding attributes in heterogeneous databases. The experimental results showed our method is effective.","Databases,
Neural networks,
Computer science,
White spaces,
Statistics,
Machine learning,
Cybernetics"
Conversion degree functions induced by qualitative mapping and fuzzy artificial neurons,"Two definitions of qualitative mapping (QM) and conversion degree function (CDF) are presented in this paper. It is shown that, under the action of some special conversion degree functions, its qualitative criterion is turned into a fuzzy qualitative criterion. A fuzzy artificial neuron can be induced by an m-dimension weight conversion degree function (MWCDF) /spl eta/i(x), and two classifying algorithms based on MWCDF for two nonlinear patterns, XOR and two spirals, are given respectively.","Neurons,
Sugar,
Fuzzy sets,
Fuzzy set theory,
Fuzzy neural networks,
Artificial neural networks,
Blood,
Computer science,
Quality management,
Spirals"
A control algorithm and preliminary user studies for a bone drilling medical training system,"Bone drilling procedures require a high surgeon skill. The required core skills are: recognizing the drilling end-point, ability of applying constant, sufficient, but non-excessive feeding velocity and thrust force. Although several simulators and training systems were developed for different surgery, a bone drilling medical training system does not exist yet. In this paper, a bone drilling medical training system is proposed and a novel control algorithm for the problem is presented. A graphical user interface is developed to complete a medical training system structure. Experimental results for controller performance are satisfactory. Additional experiments are performed to check if the developed system improves the skill of trainees or not. Early results suggest that training in the developed medical training system is a promising way to teach drilling into a bone to medical students.","Medical control systems,
Control systems,
Bones,
Drilling,
Surgery,
Surges,
Medical simulation,
Spine,
Computer science,
Haptic interfaces"
Application of XML Schema and active rules system in management and integration of heterogeneous biological data,"Automating the process of information retrieval and integration of heterogeneous biological data is complex and difficult. This paper describes an approach to solve this problem by using XML technologies such as XML Schema and an XML-based active rules system. Current limitations of active rule system for XML databases are discussed. We then propose a template for defining rules that is consistent with the current XQuery specification, a defacto standard language for querying XML data. Finally, an example scenario is used to illustrate how these techniques can come together in integrating heterogeneous biological data sources.","XML,
Proteins,
Bioinformatics,
Biology,
Information retrieval,
Database languages,
Database systems,
Application software,
Engineering management,
Computer science"
The web-DL environment for building digital libraries from the web,,"Software libraries,
Computer science,
Web pages,
Data mining,
Database systems,
Access protocols,
Proposals,
Environmental management"
Computation of minimal uniform transmission power in ad hoc wireless networks,"Power conservation is a critical issue for ad hoc wireless networks. The main objective of the paper is to find the minimum uniform transmission power of an ad hoc wireless network, where each node uses the same transmission power while maintaining network connectivity. Three different algorithms, binary search, Prim's MST and its extension are developed to solve the problem, and their performance is compared by simulation study together with Kruskal's minimum spanning tree (MST), a known solution proposed by Ramanathan and Rosales-Hain for topology control by transmission power adjustment. Our results show that Prim's MST outperforms both Kruskal's MST and binary search. The performance between Prim's MST implemented with binary heap and Fibonacci heap is fairly close.","Computer networks,
Intelligent networks,
Wireless networks,
Network topology,
Clustering algorithms,
Tree graphs,
Batteries,
Routing,
Power engineering computing,
Computer science"
Initiating a program in nanotechnology through a structured curriculum,"The field of nanotechnology is currently undergoing an exciting period of discoveries. Focusing on the intersection of areas such as physics, biology, engineering, chemistry, computer science and more, nonotechnology, is rapidly expanding. Many researchers in academia and industries have expressed need for an academic program where students from diverse fields of sciences and engineering (molecular computing, quantum physics, chemistry, mechanical, electrical, computer etc.) can come together to learn and discuss the latest advances, with the overall objective of encouraging further development. This will need preparing engineering students with an ability to apply knowledge of science, mathematics, and engineering to develop nanodevices and nanosystems. The challenge is to provide an interdisciplinary education to students through a well-structured curriculum covering broad aspects of basic sciences and engineering.","Nanotechnology,
Physics,
Chemistry,
Quantum computing,
Nanobioscience,
Biology,
Computer science,
Computer industry,
Molecular computing,
Quantum mechanics"
Learning fuzzy logic controller for reactive robot behaviours,Fuzzy logic plays an important role in the design of reactive robot behaviours. This paper presents a learning approach to the development of a fuzzy logic controller based on the delayed rewards from the real world. The delayed rewards are apportioned to the individual fuzzy rules by using reinforcement Q-learning. The efficient exploration of a solution space is one of the key issues in the reinforcement learning. A specific genetic algorithm is developed in this paper to trade off the exploration of learning spaces and the exploitation of learned experience. The proposed approach is evaluated on some reactive behaviour of the football-playing robots.,"Fuzzy logic,
Robot control,
Fuzzy sets,
Orbital robotics,
Computer science,
Genetic algorithms,
Fuzzy systems,
Logic design,
Delay,
Learning"
Exploiting program hotspots and code sequentiality for instruction cache leakage management,"Leakage energy optimization for caches has been the target of much recent effort. In this work, we focus on instruction caches and tailor two techniques that exploit the two major factors that shape the instruction access behavior, namely, hotspot execution and sequentiality. First, we adopt a hotspot detection mechanism by profiling the branch behavior at runtime and utilize this to implement a HotSpot based Leakage Management (HSLM) mechanism. Second, we exploit code sequentiality in implementing a Just-InTime Activation (JITA) that transitions cache lines to active mode just before they are accessed.,We utilize the recently proposed drowsy cache that dynamically scales voltages for leakage reduction and implement various schemes that use different combinations of HSLM and JITA. Our experimental evaluation using the SPEC2000 benchmark suite shows that instruction cache leakage energy consumption can be reduced by 63%, 49% and 29%; on the average, as compared to an unoptimized cache, a recently proposed hardware optimized cache, and a cache optimized using compiler, respectively. Further, we observe that these energy savings can be obtained without a significant impact on performance.","Leakage current,
Energy consumption,
Threshold voltage,
Energy management,
Permission,
Voltage control,
Turning,
Engineering management,
Computer science,
Power engineering and energy"
On-line laboratory for communication systems using J-DSP,"The J-DSP editor was developed at Arizona State University (ASU) to enable students to conduct on-line simulations and experiments in digital signal processing and related subjects. In this paper, we describe a series of J-DSP communications functions and laboratory exercises that have been developed to support the laboratory portion of the ASU communications elective, EEE455. The functionality developed covers both analog and digital communications. The J-DSP communications laboratories include descriptions of relevant J-DSP functions, lab exercises illustrating the key concepts, and a short quiz that captures the main points of the assignment. The J-DSP communication labs have been assigned in EEE 455 and assessment results have been compiled. The students responded to specific questions on the labs and the J-DSP environment in general. Assessment results indicate that the majority of the students responded that the new JDSP functionality and the associated lab exercises complemented well the theory covered in class and helped them develop intuition on the communications concepts covered in these labs.","Laboratories,
Digital communication,
Communication systems,
Digital modulation,
Digital signal processing,
Demodulation,
Internet,
Java,
Signal processing,
Amplitude modulation"
Towards a standard EAI quality terminology,"Since software integration research has yielded a variety of different concepts, we survey and discuss EAI (Enterprise Application Integration terminology, in the domain of the quality properties, known as Quality of Services (QoS) offered by the EAI middleware. The main goal of this work is to establish a correspondence or mapping between the ISO standard definitions and the terminology used in the description of the QoS provided by the EAI middleware. Our aim is to give the EAI software community an easy way to ""standardize"" and unify their terminology using our mapping, without loosing the advantage of using the current notions employed by users and practitioners. A standard quality model, based on the ISO-9126-1 for EAI middleware and integrated information systems (Enterprise Systems) has been defined to achieve this goal. Our result can be used as a basic element or starting point towards an ontology for software quality in the EAI domain.","Terminology,
Middleware,
ISO standards,
Quality of service,
Software quality,
Application software,
Ontologies,
Business process re-engineering,
Information systems,
Computer architecture"
A framework for incremental deployment strategies for router-assisted services,"Incremental deployment of a new network service or protocol is typically a hard problem, especially when it has to be deployed in the routers. First, an incrementally deployable protocol is needed. Second, a study of the performance impact of incremental deployment should be carried out to evaluate deployment strategies. Choosing the wrong strategy can be disastrous, as it may inhibit reaping the benefits of an otherwise robust service, and prevent widespread adoption. Unfortunately, to date there has been no systematic evaluation of incremental deployment for such services. Our research work is focused on the second aspect, namely the performance impact of incremental deployment of router-assisted services. We take the first step to define a framework for evaluating incrementally deployable services, which consists of three parts: (a) selection and classification of deployment strategies; (b) definition of performance metrics; and (c) systematic evaluation of deployment strategies. As a case study for our framework, we evaluate the performance of router-assisted reliable multicast protocols. Although our framework is still evolving, our results clearly demonstrate that the choice of a strategy has a substantial impact on performance, and thus affirms the need for systematic evaluation of incremental deployment. Our case study includes two router-assisted reliable multicast protocols, namely PGM and LMS. We make several interesting observations: (a) the performance of different deployment strategies varies widely; for example, with some strategies, both PGM and LMS approach full deployment performance with as little as 5% of the routers deployed, but with other strategies up to 80% deployment may be needed to approach the same level; (b) our sensitivity analysis reveals relatively small variation in the results in mos","Computer science,
Least squares approximation,
Measurement,
Multicast protocols,
Web and internet services,
Large-scale systems,
Topology,
Helium,
Robustness,
Sensitivity analysis"
Odaies: ontology-driven adaptive Web information extraction system,"This paper proposes an ontology-driven self-adapting approach in the semi-structured Web information extraction field, where ontology provides semantic support and plays a central role during the extraction process. It outperforms traditional wrapper systems in adaptiveness and maintenance. Firstly, we build a domain-dependant ontology. Then we design three templates generating algorithms, which have self-adaptiveness and self-maintenance based on the ontology, to perform Web page information extraction. Experiment results show that our prototype system can achieve 100% recall and 97.64% precision.",
A fuzzy matching method of fuzzy decision trees,"In this paper, we present a matching method that can improve the classification performance of a fuzzy decision tree (FDT). This method takes into consideration prediction strength of leave nodes of a fuzzy decision tree by combining true degrees (CF) of fuzzy rules, generated from a fuzzy decision tree, with membership degrees of antecedent parts of rules when applied to cases for classification. We illustrate the importance of CF through an example. An experiment shows by using this method, we can obtain more accurate results of classification when compared to the original method and to those obtained using the C5.0 decision tree.",
A comparative study of Zernike moments,"Effective image retrieval by content requires that visual image properties are used instead of textual labels to properly index pictorial data. Shape is one of the primary low-level image features. Many shape representations had been proposed. The Zernike moment descriptor is the most suitable for shape similar-based retrieval in terms of computation complexity, compact representation, robustness, and retrieval performance. We study the first 36 Zernike moments and find the dependence relations between them. A new compact representation is proposed to replace the old one. It is not only saving storage capacity but also reducing the execution time of index generation.","Shape,
Image retrieval,
Content based retrieval,
Information science,
Information retrieval,
Robustness,
Object recognition,
Computer displays,
Internet,
Content management"
Evaluating the impact of communication architecture on the performability of cluster-based services,"We consider the impact of different communication architectures on the performability (performance plus availability) of cluster-based servers. In particular, we use a combination of fault-injection experiments and analytic modeling to evaluate the performability of two popular communication protocols, TCP and VIA, as the intra-cluster communication substrate of a sophisticated Web server. Our analysis leads to several interesting conclusions, the most surprising of which is, under the same fault load, VIA-based servers deliver greater availability than TCP-based servers. If we assume higher fault rates for VIA-based servers because the underlying technology is more immature and programming model more complex, we find that packet errors or application faults would have to occur at approximately 4 times the rate in TCP-based servers before their performabilities equalize. We use our results from the study to suggest that high-performance and robust communication layers for highly available cluster-based servers should preserve message boundaries, as opposed to using byte streams, use single-copy transfers, pre-allocate channel resources, and report errors in manner consistent with the network fabric's fault model.","Performance evaluation,
Availability,
Web server,
Protocols,
Network servers,
Robustness,
Operating systems,
Computer architecture,
Computer science,
Performance analysis"
Some applications of an asymmetric subsethood product fuzzy neural inference system,"This paper presents some applications of an asymmetric subsethood product fuzzy neural inference system (ASuP-FuNIS). The ASuPFuNIS model extends SuPFuNIS by permitting signal and weight fuzzy sets to be modeled by asymmetric Gaussian membership functions. The asymmetric subsethood product network admits both numeric as well as linguistic inputs. Numeric inputs are fuzzified prior to their application to the network; linguistic inputs are presented without modification. The network architecture directly embeds fuzzy if-then rules, and connections represent antecedent and consequent fuzzy sets. The model uses mutual subsethood based activation spread and a product aggregation operator that works in conjunction with volume defuzzification in a gradient descent learning framework. The model is economical in terms of the number of rules required to solve difficult problems and is robust against random variations in data sets. Simulation results on three benchmark problems-the Hepatitis diagnosis, Iris data classification and the Narazaki-Ralescu function approximation problem-show that the subsethood based model performs excellently with minimal number of rules.","Fuzzy systems,
Fuzzy sets,
Fuzzy neural networks,
Function approximation,
Iris,
Fuzzy control,
Application software,
Liver diseases,
Physics,
Computer science"
Evolutionary ensemble classifier for lymphoma and colon cancer classification,"Cancer is one of the most dangerous diseases for people. Recently, development of array technologies makes it possible to measure thousands of genes at once, and it can be used to treat cancer. Various methods using array data to classify cancer are proposed, but there are no perfect and general methods. Ensemble method can demonstrate its ability if there are complementary set of individuals. However, it is needed methods to search the optimal ensembles, because there are so many available ensembles. We propose a GA-based method to search the optimal ensemble. In two benchmark datasets, our proposed method has shown significant result in the aspects of performance and time efficiency under the several situations.","Colon,
Gene expression,
DNA,
Diseases,
Computer science,
Modems,
Cancer detection,
Robustness,
Genetic algorithms,
Biological cells"
Lab exercises and learning activities for courses in computer networks,"For today's graduates in computer science and engineering, possessing a working knowledge of networking and data communications has become essential. While most computer science and engineering departments offer an upper-level course in this area, many are exploring ways to leach some of this material earlier in the curriculum. Doing this provides an opportunity for advanced study in this area, but it also presents some challenges. One such challenge is how to create labs and activities for students who may not have the typical upper-division course background. This paper presents activities that are suitable for use in a lower-division computer networks course or that can be used in programs where access to proper lab facilities may be limited. In addition, the projects are designed to use collaborative learning and discovery learning techniques. The structure of these activities is presented and a qualitative discussion of their effectiveness is given.","Intelligent networks,
Computer networks,
Computer science,
Guidelines,
Education,
Data communication,
Operating systems,
Educational programs,
Data structures,
Programming profession"
A rule grouping technique for weight-based TCAM coprocessors [packet classification application],"A crucial issue associated with a TCAM (ternary content addressable memory) coprocessor with weights is that no more rules can be enforced if the weights are exhausted. In this paper, the problem is identified and a rule grouping technique is proposed to solve the problem. The technique allows a virtually unlimited number of rules with arbitrary rule structures to be enforced. It requires no special hardware support and can be readily implemented in a fully programmable network processor and a weight-based TCAM coprocessor.","Coprocessors,
Yarn,
Computer science,
Hardware,
Associative memory,
Clocks"
What is good teaching of computer networks?,"In this paper it is argued, based on theoretical as well as empirical grounds, that a university teacher in computer networks can improve students' learning by being aware of the different ways in which the students understand the concepts that he or she teaches. The distinct, qualitatively different, ways, in which students understand the network protocol TCP (transmission control protocol), have been revealed in a research project, performed with a phenomenographic qualitative approach. The perceptions of TCP held among the students have been evaluated, based on situational appropriateness and richness. The results indicate that all the ways in which TCP is understood in the group are relevant during different phases of a software development project, and with different tasks at hand. Thus, a teacher should encourage students to understand what he or she teaches in different ways and should help them to choose in a relevant way between these perceptions. These results are also related to current research into students' learning, which clearly demonstrate that teaching, based on results of this type, promotes better understanding.","Computer networks,
Protocols,
Lenses,
Computer science education,
Leaching,
Programming,
Guidelines"
Projector-based augmented reality in surgery without calibration,"Augmented reality (AR) is becoming an important tool in surgery to support the surgeon and improve operation quality, safety and duration. However the AR setup with head-mounted display (HMD) and other equipments is often considered cumbersome by surgeons and limits its wide use in the operating room. To reduce this burden, we introduce a new approach to display undistorted image data directly on the patient (skin, bone, surgery linen etc.) without explicit camera and projector calibration. With a single camera used to capture the surgeon's field of view, the calibration is implicitly represented as a mapping establishing the correspondence of each pixel of a camera to a pixel from a projector. After this mapping has been carried out, one can display an image corrected for the surgeon. Results are presented showing the simplicity and potential of the method for an operating room.","Augmented reality,
Surgery,
Calibration,
Surges,
Cameras,
Computer displays,
Skin,
Bones,
Geometry,
Computer science"
Using mobile agents to support unreliable database retrieval operations,"One major drawback in using a client server system in the mobile environment is the redundancy of data transmission in the event of connection failure. In this paper, we present a novel mobile agent architecture which allows minimum redundancy of data transmission in a client server database connection in a frequent disconnections environment. The system supports disconnection events caused by network failure or client power failure. Unlike current approaches, our model uses agent messaging to transfer the data. The messaging reduces the amount of migrations that the mobile agents performed. The architecture was implemented and tested in the Aglets platform.","Mobile agents,
Databases,
Network servers,
Mobile computing,
Computer architecture,
Computer networks,
Computer science,
Software engineering,
Data communication,
Bandwidth"
Animation and visualization of spot prices via quadratized power flow analysis,"This paper presents a new model for efficient calculation of spot prices and animation and visualization of spot price evolution as the system operating point is changing. The computational method is based on the quadratized power flow approach that cast the power flow problem as a set of quadratic equations. The load model consists of constant power, constant impedance and induction motor loads. The electric load time variation is modeled via a small set of independent random variables resulting in a nonconforming electric load model. Constraints of voltage limits as well as circuit loading are imposed. For a specific load point, the operation of the system is determined by the appropriate formulation of the power flow problem and subsequent solution. At this operating point, the spot prices are computed from a linear program defined at the operating point. The results are visualized in a three-dimensional OpenGL display. As the system load evolves, the spot prices are recomputed and the visualization display is updated thus generating an animated evolution of the system spot prices. The paper describes the proposed computational method and discusses the efficiency of the proposed method. The method is also demonstrated on the IEEE RTS system which has been modified to include a model of a spot price market. The presentation of the paper will include a demonstration of the visualization and animation of the spot prices for the IEEE RTS system.","Animation,
Visualization,
Load flow analysis,
Power system modeling,
Load flow,
Load modeling,
Equations,
Impedance,
Induction motors,
Random variables"
Debugging with reverse watchpoint,"Many programmers have had to deal with an overwritten variable resulting for example from an aliasing problem. The culprit is obviously the last write-access to that memory location before the manifestation of the bug. The usual technique for removing such bugs starts with the debugger by (1) finding the last write and (2) moving the control point of execution back to that time by re-executing the program from the beginning. We call the feature to implement this process ""reverse watchpoint"" and believe automating it is important. In this paper we propose a new concept, ""position "", that is, a point in the program execution trace, as needed for step (2) above. The position enables debuggers to automate the control of program execution to support common debugging activities. We have implemented position in Java with a bytecode transformer. Measurements show that position can be provided with an acceptable amount of overhead.","Debugging,
Programming profession,
Computer bugs,
Automatic control,
Java,
Proposals,
Information science,
Position measurement,
Software measurement,
Software testing"
Quantified propositional temporal logic with repeating states,"Quantified Propositional Temporal Logic (QPTL) is a linear temporal logic that allows quantification over propositional variables. In the usual semantics for QPTL, a model is an infinite discrete linear sequence of states, with each state having some propositional interpretation. The effect of this is that the interpretation of a proposition at one point in time is independent from its interpretation at another point in time. In this paper, we examine the expressivity and decidability of a QPTL, given generalizations of the usual semantics that do not have this restriction. We introduce the repeating semantics (QPTL/sup R/), which allows states to be repeated throughout a model. While semantic interpretation does not affect the unquantified fragment of QPTL it significantly increases the expressive power in the presence of propositional quantification. In the main result of this paper, we show that QPTL/sup R/ makes the satisfiability problem highly undecidable through a complicated encoding of a tiling problem. We also investigate two less expressive semantics which still allow states to be repeated. We prove the satisfiability problem for one is undecidable, and decidable for the other.","Logic,
Automata,
Computer science,
Software engineering,
Encoding"
High speed EPICS data acquisition and processing on one VME board,"A custom VME board is being designed at the Los Alamos Neutron Science Center (LANSCE) for high speed signal acquisition and processing. While it is desirable to design around the EPICS/VME platform, it can be difficult to process high-speed signals with long record lengths. The relatively slow data path between the IOC and the general-purpose computer makes real time computations impossible. Commercial VME processor boards can be used, but the data must still flow over the VME backplane in lieu of other traffic. This custom board is designed to overcome this problem by acquiring and processing the signal in one place, with the processed result presented at the VME interface instead of the raw data. The board consists of multiple front-end signal conditioners/digitizer cards plugged into the foundation 6U VME board with an embedded digital signal processor (DSP). The DSP is programmed in C to process the raw signal any way the user wants before writing results into a VME register map. The present front-end conditioner/digitizer cards are being designed with the LANSCE low momentum detector in mind, but other variations on this card could be developed. The architecture is flexible enough to deploy in many accelerator applications.",
Fast good features selection for wide area monitoring,"Recently the surveillance of wide areas has pointed the interest of the research community. The use of active vision seems to be the most effective solutions for these needs. Against the better acquiring resolution there is the problem of the apparent motion inducted by the camera motion known as ego-motion. Feature based methods for ego-motion estimation are widely used in computer vision but they deal with feature recovery and with errors in feature tracking. In this paper, we propose a fast method to extract and select new features during camera motion. This is achieved by adopting a reference map containing well trackable features that is updated at each frame by introducing new good features related to regions appearing in the current image. A new procedure is applied to reject badly tracked features. The current frame and the background after compensation are processed by a change detection method in order to locate mobile objects. Results are presented in the context of a visual-based surveillance system for monitoring outdoor environments.",
Two-stage continuous speech recognition using feature-based models: a preliminary study,"In recent research, we have demonstrated that linguistic features can be used to improve speech recognition for an isolated vocabulary recognition task. This paper addresses two important new research problems in our attempts to build a two-stage speech recognition system using linguistic features. First, through a controlled study we show that our knowledge-driven feature sets perform competitively when compared with similar classes discovered by data-driven approaches. Secondly, we show that the cohort idea can be effectively generalized to continuous speech. Improved recognition results are achieved using this two-stage framework on multiple speech recognition experiments, on conversational telephone quality speech.","Speech recognition,
Automatic speech recognition,
Computer science,
Artificial intelligence,
Laboratories,
Isolation technology,
Search problems,
Access protocols,
Performance analysis,
Humans"
A parallel framework for simplification of massive meshes,"As polygonal models rapidly grow to sizes orders of magnitudes bigger than the memory of commodity workstations, a viable approach to simplifying such models is parallel mesh simplification algorithms. A naive approach that divides the model into a number of equally sized chunks and distributes them to a number of potentially heterogeneous workstations is bound to fail. In severe cases the computation becomes virtually impossible due to significant slow downs because of memory thrashing. We present a general parallel framework for simplification of very large meshes. This framework ensures a near optimal utilization of the computational resources in a cluster of workstations by providing an intelligent partitioning of the model. This partitioning ensures a high quality output, low runtime due to intelligent load balancing, and high parallel efficiency by providing total memory utilization of each machine, thus guaranteeing not to trash the virtual memory system. To test the usability of our framework we have implemented a parallel version of R-Simp [Brodsky and Watson 2000].","Workstations,
Computer science,
Computational intelligence,
Machine intelligence,
Usability,
Partitioning algorithms,
Educational institutions,
Runtime,
Load management,
Testing"
Using a layered approach for interoperability on the semantic Web,"In this paper, we further develop a proposed layered approach for the semantic Web. Our objective is to build a specific solution to the problem of providing data interoperability among different databases, so as to allow for schematic data integration. In particular, we solve the problem of translating queries on a database schema into queries on another database schema, using their relationship with an ontology. We use RDF schema to model the databases and the ontology. A common vocabulary expresses the mappings between each database schema and the ontology.","Semantic Web,
Databases,
Resource description framework,
Ontologies,
Data models,
Vocabulary,
XML,
Operating systems,
Computer science,
Web sites"
A QoS network routing algorithm using multiple pheromone tables,"Quality-of-services routing algorithms that utilize probes are criticized for not being able to achieve optimal path selection for real-time flows. This is because probe-based algorithms do not have a global view of the network. We introduce a new probe-based routing algorithm for packet-switched networks that supports both best-effort and real-time flows. Unlike other probe-based algorithms, probes are routed via routing tables to achieve optimal performance. Simulations were carried out on QColony and other QoS probe-based routing systems under various network traffic loads and irregular network topologies. Simulation results, concerning resource utilization and connection request success rates, show that QColony provides good performance under heavy loads with failure conditions especially for large networks. We also introduce a novel approach for traffic protection in case of network failure conditions.",
A counterexample to Tang and Padubidri's claim about the bisection width of a diagonal mesh,A counterexample is presented to disprove Tang and Padubidri's (1994) claim about the bisection width of a diagonal mesh.,Multiprocessor interconnection
Evolving aesthetic images using multiobjective optimization,"We consider the problem of using evolutionary multiobjective optimization to evolve visual imagery. In our method, images (phenomes) are generated from expressions (genomes), and then color segmented so that they can be evaluated under a number of different aesthetic criteria. Our principal task is to formulate fitness functions that make the best use of these elementary aesthetic components. We demonstrate the benefits obtained from using more than one objective function. We also discuss technical issues that arose as a consequence of treating our computational aesthetics problem as a ""real-world"" application of evolutionary multiobjective optimization.",
Energy-optimal and energy-balanced sorting in a single-hop wireless sensor network,"A large number of sensors networked together form selforganizing pervasive systems that provide the basis for implementation of several applications involving distributed, collaborative computations. Energy dissipation is a critical issue for these networks, as their life-time is limited by the battery power of the sensors. In this paper, we focus on design of an energy-balanced, energy-optimal algorithm for sorting in a single-hop sensor network. Energy optimality implies that the overall energy dissipation in the system is minimized. Energy-balancedness ensures that all the sensors spend asymptotically equivalent amount of energy in the system. Uniform energy dissipation is desirable as it enables the network to remain fully functional for the maximum time. We demonstrate that given a single-hop, singlechannel network of n randomly distributed sensors, sorting can be performed in O(n log n) time and energy, with no sensor being awake for more than O(log n) time steps. In a p-channel network, where p ∓ n1-ϵ for 0 < ϵ ∓ 1, sorting can be performed in O(n = p log n) time and O(n log n) energy with no node being awake for more than O(log n) time steps.","Sorting,
Intelligent networks,
Wireless sensor networks,
Computer networks,
Energy dissipation,
Sensor systems,
Military computing,
Communication switching,
Collaboration,
Pervasive computing"
Hardware-aware communication protocols in low energy wireless sensor networks,"As the energy consumption in sensor nodes is dominated by the radio transmission/reception circuitry, communication protocols must be designed for economy in radio communications. Sensor nodes are generally equipped with short-range radios that have various characteristics including data rate, power consumption in transmit, receive, and sleep modes, and time to switch from one mode to another. These parameters can have significant effects on the performance of communication protocols in low energy sensor networks. In this paper, we consider a protocol called STEM that was proposed in [C. Schurgers et. al., January-March 2002] and optimize the protocol's parameters by considering the radio characteristics.","Wireless application protocol,
Intelligent networks,
Wireless sensor networks,
Radio control,
Circuits,
Monitoring,
Energy consumption,
Sensor phenomena and characterization,
Base stations,
Computer science"
Principles and experiences in using legos to teach behavioral robotics,"This paper describes the application of lego mindstorms and vision command kits as a cost- and time-effective means of reinforcing behavioral robotics principles to students of different disciplines with limited programming skills. As part of a course in robotics, senior undergraduate and first year graduate students in computer science, engineering, and psychology have worked in small groups building and programming robots to perform a variety of tasks, ultimately developing robots for a mock search and rescue operation. This paper discusses the pedagogical principles, the exercises, student reactions, shortcomings, and lessons learned. The laboratory exercises were used to teach students in two locations (Tampa, Florida and Reykjavik, Iceland) with positive student reviews. The laboratory manual is available to teachers by request, along with the instructor's guide to Introduction to AI robotics. Based on our experiences, we recommend their use.","Educational robots,
Laboratories,
Robot vision systems,
Robot programming,
Artificial intelligence,
Costs,
Application software,
Computer science,
Psychology,
Computer vision"
Ants can play prisoner's dilemma,"An ant colony optimization approach towards the development of robust game strategies for iterated prisoner's dilemma (IPD) is presented. It is demonstrated that, ants can play IPD and, in spite of comparably longer running times, provide game strategies of better quality than the genetic algorithms. The success of the developed strategies, compared to the ones developed using genetic algorithms are illustrated by experimental evaluations.","Genetic algorithms,
Ant colony optimization,
Linear matrix inequalities,
Robustness,
Biology,
Computer science,
Biological system modeling,
Application software,
Resists"
A voltage scheduling heuristic for real-time task graphs,,"Processor scheduling,
Energy consumption,
Real time systems,
Dynamic voltage scaling,
Dynamic scheduling,
Computer science,
Application software,
Energy efficiency,
Circuits,
Delay lines"
An overview of systems enabling computer supported collaborative learning requiring immersive presence (CSCLIP),"In this paper, we consider three types of computer-based systems that are employed individually and in pairs to achieve learning objectives: computer-supported learning systems, collaborative systems, and immersive presence systems. When these systems are integrated together, higher-order learning objectives that have typically required co-located interactions can be supported in a distributed fashion. The objective of this work is to report on educational uses of the intersection of these three different types of systems in the combination space we refer to as computer supported collaborative learning requiring immersive presence (CSCLIP). Educational uses of CSCLIP technology and their features are identified. We then provide a discussion and comparison of the features common to CSCLIP systems. Within this discussion, we develop the criteria necessary for comparison of current CSCLIPs and some key requirements of implementation of future CSCLIPs.","Collaborative work,
Computer aided instruction,
Collaboration,
Learning systems,
Space technology,
Distance learning,
Humans,
Virtual reality,
Educational technology,
Education"
A new maze routing approach for path planning of a mobile robot,"A new path planning approach for a mobile robot among obstacles of arbitrary shape is presented. This approach is based on a higher geometry maze routing algorithm. Starting from a top view of a workspace with obstacles, the so-called free workspace is first obtained by virtually expanding the obstacles in the image. After that, the 8-geometry maze routing algorithm is applied to obtain a shortest collision-free path. The proposed method is not only able to search a shortest path with rotation scheme but also capable to rotate the robot configuration to pass a narrow passage intelligently. The time complexity of the algorithm is O(N), where N is the number of pixels in the free workspace. Furthermore, for many researchers who work on dynamic collision avoidance for multiple autonomous robots and optimal path searching among various terrains (weighted regions), the concept of this algorithm can be applied to solve these problems.","Routing,
Path planning,
Mobile robots,
Computer science,
Oceans,
Intelligent robots,
Shape,
Collision avoidance,
Motion planning,
Acceleration"
Knowledge negotiation in asynchronous learning networks,"The negotiation of what is to count as mutually acceptable collaborative knowledge is difficult to conduct when participants cannot interact face-to-face. We review certain related work on negotiation support and develop a concept of ""knowledge negotiation "" that is appropriate for collaborative learning in ALNs used in school classrooms. This concept is situated within the framework of collaborative knowledge building viewed at the group unit of analysis; it contrasts with negotiation as the reconciliation of multiple personal opinions through voting. We then describe an implementation of support for knowledge negotiation in an ALN that is currently being tested in European schools.","Intelligent networks,
Collaborative work,
Buildings,
Educational institutions,
Testing,
Computer networks,
Collaborative software,
Knowledge management,
Information technology,
Voting"
Discovery of high-dimensional inclusion dependencies,"Determining relationships such as functional or inclusion dependencies within and across databases is important for many applications in information integration. When such information is not available as explicit meta data, it is possible to discover potential dependencies from the source database extents. However, the complexity of such discovery problems is typically exponential in the number of attributes. We have developed an algorithm for the discovery of inclusion dependencies across high-dimensional relations in the order of 100 attributes. This algorithm is the first to efficiently solve the inclusion-dependency discovery problem. This is achieved by mapping it into a progressive series of clique-finding problems in k-uniform hypergraphs and solving those. Extensive experimental studies confirm the algorithm's efficiency on a variety of real-world data sets.","Databases,
Redundancy,
Computer science,
Data mining,
Association rules,
Application software,
Companies,
Instruments,
Medical treatment,
Diseases"
The evolving face of telemedicine & e-health: opening doors and closing gaps in e-health services opportunities & challenges,"Despite the growing number of e-health Web sites that are dedicated to health education and information therapy, healthcare provider organizations have been slow in embarking on sign e-commerce ventures to improve the delivery of healthcare services to patients and consumers in geographically dispersed communities. More specifically, hospitals and health provider organizations tend to use static Web sites that supply information, but have not made major investments in interactive technologies to engage patients and healthcare consumers more actively. In this paper, we survey a number of key participants in the e-health marketplace and the technologies that these players have employed to date. Not surprisingly, we found that opportunities abound to change the face of telemedicine and e-health in terms of improving efficiencies, developing new markets, reducing costs, and enhancing the quality and value of health services delivery. Evidently, today's Internet savvy consumers have become more informed as well as more demanding than ever and the healthcare industry must respond in kind. We therefore conclude that the healthcare industry faces sign challenges in closing the gaps of e-health services delivery to meet the desires and needs of a growing population of sophisticated healthcare consumers.","Telemedicine,
Medical services,
Internet,
Hospitals,
Educational technology,
Medical treatment,
Investments,
Costs,
Business,
Computer science"
Recurses!,"The author considers recursion: functions, subroutines, and even whole computer languages defined in terms of themselves. Recursion is a direct and elegant way to translate certain mathematical relations into programs, and it's a great technique for discovering efficient algorithms. Given its utility, it is seldom used. Correctly used, recursion is so valuable you should use it whenever it makes programs clearer or briefer. He explains when recursion is appropriate and when it is a bad idea, and also shows how you might find it useful.",Program control structures
Sequential association mining for video summarization,"In this paper, we propose an association-based video summarization scheme that mines sequential associations from video data for summary creation. Given detected shots of video V, we first cluster them into visually distinct groups, and then construct a sequential sequence by integrating the temporal order and cluster type of each shot. An association mining scheme is designed to mine sequentially associated clusters from the sequence, and these clusters are selected as summary candidates. With a user specified summary length, our system generates the corresponding summary by selecting representative frames from candidate clusters and assembling them by their original temporal order. The experimental evaluation demonstrates the effectiveness of our summarization method.","Video sequences,
Motion pictures,
Computer science,
Gunshot detection systems,
Assembly systems,
Streaming media,
Indexing,
Layout,
Data mining"
Using Golomb rulers for optimal recovery schemes in fault tolerant distributed computing,"Clusters and distributed systems offer fault tolerance and high performance through load sharing. When all computers are up and running, we would like the load to be evenly distributed among the computers. When one or more computers break down the load on these computers must be redistributed to other computers in the cluster. The redistribution is determined by the recovery scheme. The recovery scheme should keep the load as evenly distributed as possible even when the most unfavorable combinations of computers break down, i.e. we want to optimize the worst-case behavior. In this paper we define recovery schemes, which are optimal for a number of important cases. We also show that the problem of finding optimal recovery schemes corresponds to the mathematical problem called Golomb rulers. These provide optimal recovery schemes for up to 373 computers in the cluster.",
Heuristic backtracking algorithms for SAT,"In recent years backtrack search SAT solvers have been the subject of dramatic improvements. These improvements allowed SAT solvers to successfully replace BDDs in many areas of formal verification, and also motivated the development of many new challenging problem instances, many of which too hard for the current generation of SAT solvers. As a result, further improvements to SAT technology are expected to have key consequences informal verification. The objective is to propose heuristic approaches to the backtrack step of backtrack search SAT solvers, with the goal of increasing the ability of the SAT solver to search different parts of the search space. The proposed heuristics to the backtrack step are inspired by the heuristics proposed in recent years for the branching step of SAT solvers, namely VSIDS and some of its improvements. The preliminary experimental results are promising, and motivate the integration of heuristic backtracking in state-of-the-art SAT solvers.","Heuristic algorithms,
Formal verification,
Space technology,
Data structures,
Boolean functions,
NP-complete problem,
Application software,
Computer science,
Design engineering,
Artificial intelligence"
An image fusion based visible watermarking algorithm,"Although the need for visible watermarking for copyright notification is apparent, visible watermarking has received much less attention than invisible watermarking. In this paper, we present a general framework for performing visible watermarking based on some concepts of image fusion. Especially, we introduce a new visible watermarking algorithm in which the watermarked coefficients are computed using global as well as local characteristics of both the host and watermark images.","Image fusion,
Watermarking,
Wavelet domain,
Wavelet transforms,
Image resolution,
Partial response channels,
Pixel,
Computer science,
Cities and towns,
Sun"
Reducing the size of routing tables for large-scale network simulation,"In simulating large-scale networks, due to the limitation of available resources on computers, the size of the networks and the scale of simulation scenarios are often restricted. Especially, routing tables, which indicate the directions to forward packets, are considered to consume memory space. A simple general routing table requires O(N/sup 2/) space where N is the number of nodes. An algorithmic routing approach recently proposed by Heung et al. only requires O(N) space for representing routing tables, however this can be applied in the case that all the routes between two nodes are contained in a spanning tree (i.e. very limited routing strategies are allowed). We propose a new method to reduce the size of routing tables under any routing strategy. Given a general simple routing table, our method represents a routing table as the combination of an algorithmic routing based table and a general routing table, by translating a part of the given general routing table into the algorithmic routing based one. In order to reduce the size of the routing table, we find a (near-optimal)algorithmic routing based table that represents most part of the given routing table. Our experimental results have shown that our method could reduce the size of the table to 10 % of the given routing table in hierarchical networks.",
Constant density displays using diversity sampling,"The Informedia Digital Video Library user interface summarizes query results with a collage of representative keyframes. We present a user study in which keyframe occlusion caused difficulties. To use the screen space most efficiently to display images, both occlusion and wasted whitespace should be minimized. Thus optimal choices will tend toward constant density displays. However, previous constant density algorithms are based on global density, which leads to occlusion and empty space if the density is not uniform. We introduce an algorithm that considers the layout of individual objects and avoids occlusion altogether. Efficiency concerns are important for dynamic summaries of the Informedia Digital Video Library, which has hundreds of thousands of shots. Posting multiple queries that take into account parameters of the visualization as well as the original query reduces the amount of work required. This greedy algorithm is then compared to an optimal one. The approach is also applicable to visualizations containing complex graphical objects other than images, such as text, icons, or trees.","Sampling methods,
Visualization,
Software libraries,
Information retrieval,
Image retrieval,
Computer displays,
Computer science,
User interfaces,
Greedy algorithms,
Tree graphs"
Peer-to-peer over ad-hoc networks: (re)configuration algorithms,"A peer-to-peer network over an ad-hoc infrastructure is a powerful combination that provides users with means to access different kinds of information anytime and anywhere. In this paper we study the (re)configuration issue in this highly dynamic scenario. We propose three (re)configuration algorithms especially concerned with the constraints of the environment presented. The algorithms aim to use the scarce resources of the network in an efficient way, improving the performance and the network lifetime. The algorithms were simulated and used a simple Gnutella-like algorithm as comparison. The results show that the algorithms achieved their goals, presenting a good cost-benefit relation.","Peer to peer computing,
Ad hoc networks,
Network servers,
Algorithm design and analysis,
Scalability,
Routing protocols,
Computer science,
Instruction sets,
Computational modeling,
Mobile computing"
Computational study of millimeter-wave metal-pin photonic bandgap waveguides for use as ultrahigh-speed bandpass wireless interconnects,"Much above present computer clock rates of about 3 GHz, problems with signal integrity, cross-coupling, and radiation may render continuous metallic interconnects impractical. An emerging possibility is to use bandpass wireless interconnects wherein the baseband digital bit stream amplitude-modulates a millimeter-wave carrier. This approach is enabled by the recent development of THz-class silicon transistors (R.F. Service, Science, vol. 293, no. 5531, p. 786, 2001) and advances in understanding the waveguiding physics provided by photonic bandgap (PBG) structures. In principle, bandpass wireless interconnects could transmit increasingly fast digital bit streams in smaller waveguides by modulating higher-frequency RF carriers. This paper is a computational-modeling study of a 2D PBG waveguiding structure which could be used to implement such a bandpass wireless interconnect. Using the finite-difference time-domain (FDTD) method, we study the transmission characteristics of a candidate PBG array of metal rods in air for the TM-polarization case. A frequency-independent surface impedance model (A. Taflove and S.C. Hagness, Computational Electrodynamics: The Finite-Difference Time-Domain Method, 2nd ed. Norwood, MA, Artech House, 2000) is used to permit estimation of the PBG stopband characteristics and the transmission losses of the PBG waveguide.","Optical computing,
Photonic band gap,
Finite difference methods,
Time domain analysis,
Millimeter wave technology,
Millimeter wave transistors,
Clocks,
Baseband,
Silicon,
Physics"
Scheduling methods based on data division for continuous media data broadcast,"Recently, various schemes have arisen to broadcast continuous media data such as audio and video. Some of them have focused on reducing clients' waiting time under the continuity condition, i.e., to play data completely without any interruptions. These schemes usually employ multiple channels to broadcast continuous media data. However, receivers of many existing broadcast systems such as those equipped with wireless LAN or Bluetooth cannot receive data from multiple channels concurrently. In this paper, we propose and evaluate a scheduling scheme to reduce the waiting time of clients with a single channel.",
Three improved fuzzy lattice neurocomputing (FLN) classifiers,"Three novel fuzzy lattice neurocomputing (FLN) classifiers, namely FLN first fit (FLNff), FLN ordered tightest fit (FLNotf), and FLN selective fit (FLNsf), are introduced in this work. Learning is incremental, memory-based, data order dependent, and polynomial O(n/sup 3/) where n is the number of the training data. Convenient geometric interpretations on the plane illustrate the mechanics of the aforementioned FLN classifiers whose capacity is demonstrated in three benchmark classification problems. The classification results compare favorably with the results by alternative classification methods from the literature. In addition, an FLN classifier can both induce rules from the data and it can deal with numeric and/or nominal data including missing attribute values. An important experimental outcome of this work is that the computation of ""smaller than maximal"" lattice intervals can increase considerably the capacity for generalization.","Lattices,
Training data,
Neural networks,
Fuzzy neural networks,
Computer networks,
Artificial intelligence,
Computer science,
Finance,
Computer industry,
Industrial economics"
Location tracking for media appliances in wireless home networks,We present a location detection system for tracking multimedia appliances and users within a wireless local area network (WLAN) enabled home environment. Multimedia appliances communicate with a central gateway via low cost network interface devices called buddies. The gateway can locate the users and appliances within the accuracy of a room and use the location information to automatically redirect media streams to user's presence. We call this paradigm as a media cloud. This paper presents the scheme for collecting proximity information from the buddies and effective techniques to determine their location. We have implemented the location detection system and present an experimental performance evaluation.,"Home appliances,
Intelligent networks,
Home automation,
Streaming media,
Wireless LAN,
Automatic control,
Multimedia systems,
Clouds,
Computer science,
Bandwidth"
Student modeling based on fuzzy inference mechanisms,"The paper presents a competence-based instructional design system and a way to provide a personalization of navigation in the course content. The navigation aid tool builds on the competence graph and the student model, which includes the elements of uncertainty in the assessment of students. An individualized navigation graph is constructed for each student, suggesting the competences the student is more prepared to study. We use fuzzy set theory for dealing with uncertainty. The marks of the assessment tests are transformed into linguistic terms and used for assigning values to linguistic variables. For each competence, the level of difficulty and the level of knowing its prerequisites are calculated based on the assessment marks. Using these linguistic variables and approximate reasoning (fuzzy IF-THEN rules), a crisp category is assigned to each competence regarding its level of recommendation.","Inference mechanisms,
Navigation,
Production,
Testing,
Performance analysis,
Information science,
Signal processing,
Computer science education,
Educational products,
Set theory"
Unsupervised link discovery in multi-relational data via rarity analysis,"A significant portion of knowledge discovery and data mining research focuses on finding patterns of interest in data. Once a pattern is found, it can be used to recognize satisfying instances. The new area of link discovery requires a complementary approach, since patterns of interest might not yet be known or might have too few examples to be learnable. We present an unsupervised link discovery method aimed at discovering unusual, interestingly linked entities in multi-relational datasets. Various notions of rarity are introduced to measure the ""interestingness"" of sets of paths and entities. These measurements have been implemented and applied to a real-world bibliographic dataset where they give very promising results.","Data analysis,
Data mining,
Databases,
Computer science,
Pattern recognition,
Pattern matching,
Contracts,
Event detection,
Association rules,
Performance evaluation"
Neural networks and rule extraction for prediction and explanation in the marketing domain,"This paper contains a case study where neural networks are used for prediction and explanation in the marketing domain. Initially, neural networks are used for regression and classification to predict the impact of advertising from money invested in different media categories. Rule extraction is then performed on the trained networks, using the G-REX method, which is based on genetic programming. Results show that both the neural nets and the extracted rules outperform the standard tool See5. G-REX combines high performance with keeping the rules short to ensure that they really provide explanation and not obfuscation.","Neural networks,
Intelligent networks,
Investments,
Advertising,
Marketing and sales,
Artificial neural networks,
Informatics,
Computer science,
Genetic programming,
Packaging"
The Rosetta meta-model framework,"Heterogeneous systems are naturally complex and their design is a tedious process. The modeling of components that constitute such a system mandates the use of different techniques. This gives rise to the problem of methodology integration that is needed to provide a consistent design. In this paper, we propose a meta-model framework that provides such an integration. The semantics of different computational models can be expressed and used together in the Rosetta framework. We use denotational semantics to define unifying semantic domains, which are themselves extended to provide ontologies for models of computation. Interaction relations defined between models are then used to exploit and analyze model integration. We demonstrate our approach by providing applications where different computational models are used together.",
A color image progressive transmission method by common bit map block truncation coding approach,"This paper proposes a new progressive image transmission (PIT) scheme for the color images. The backbone of the new method is block truncation coding (BTC). The common BTC bit map will be employed to reduce the bit rate and to accelerate the transmission process; moreover, a break cluster method will also be put to use to speed up the division of the block tree. It can be a rather complex job to deal properly with color images transmitted progressively; therefore, to make it easier and faster, we shall count on the BTC method. Our experimental results show that our new algorithm, with a lower bit rate and higher reconstructed image quality, obviously outperforms the BPM and traditional BTC methods. The high PSNR results in most phases can help the user decide whether to wait for more of the current image to come out or to abort the transmission session as quickly as possible.","Color,
Image coding,
Image communication,
Bit rate,
Image reconstruction,
Image recognition,
Transform coding,
Computer science,
Spine,
Acceleration"
LAKER: location aided knowledge extraction routing for mobile ad hoc networks,"In this paper we present a location aided knowledge extraction routing (LAKER) protocol for MANETs, which utilizes a combination of caching strategy in dynamic source routing (DSR) and limited flooding area in location aided routing (LAR) protocol. The key novelty of LAKER is that it can gradually discover knowledge of topological characteristics such as population density distribution of the network. This knowledge can be organized in the form of a set of guiding/spl I.bar/routes, which includes a chain of important positions between a pair of source and destination locations. The guiding/spl I.bar/route information is learned during the route discovery phase, and it can be used to guide future route discovery process in a more efficient manner. LAKER is especially suitable for mobility models where nodes are not uniformly distributed. LAKER can exploit the topological characteristics in these models and limit the search space in route discovery process in a more refined granularity. Simulation results show that LAKER outperforms LAR and DSR in term of routing overhead, saving up to 30% broadcast routing message compared to the LAR approach.","Lakes,
Mobile ad hoc networks,
Routing protocols,
Broadcasting,
Computer science,
Topology,
Relays"
The capstone senior design course: an initiative in partnering with industry,"In the department of computer science and engineering at the University of South Florida, we use the required capstone senior design course to help students make the transition from student to industry professional. The course also plays a key role in achieving departmental ABET EC 2000 outcomes. We have partnered with local industry to bring nonproprietary, real-world hardware and software projects to our students. Industry partners contribute projects, mentor students, and give guest lectures. Students work in teams and have milestones and schedules. Project duration is one semester. Final presentations include a project demonstration, user documentation, press release, and poster. We find that our students perform very well when presented with a project that someone at the end truly ""cares about"" and will use. Evaluation results show that students see this as a very valuable course in the curriculum for preparing them for industry careers.","Design engineering,
Computer science,
Computer industry,
Job shop scheduling,
Knowledge engineering,
Uncertainty,
Hardware,
Documentation,
Engineering profession,
Educational institutions"
Optimal linear identifying codes,"Identifying codes can be used to locate malfunctioning processors. We say that a code C of length n is a linear (1,/spl les/l)-identifying code if it is a subspace of F/sub 2//sup n/ and for all X,Y/spl sube/F/sub 2//sup n/ such that |X|, |Y|/spl les/l and X/spl ne/Y, we have /spl cup//sub x/spl isin/X/(B(x)/spl cap/C)/spl ne//spl cup/y/spl isin/Y(B(y)/spl cap/C). Strongly (1,/spl les/l)-identifying codes are a variant of identifying codes. We determine the cardinalities of optimal linear (1,/spl les/l)-identifying and strongly (1,/spl les/l)-identifying codes in Hamming spaces of any dimension for locating any at most l malfunctioning processors.",
Graphical programming: a vehicle for teaching computer problem solving,"Translating from a problem description given in a natural language to a solution expressed in a programming language requires many complex steps. Though many of these steps can be done mentally for simple problems, the process itself is important when dealing with complicated software. Expressing the process demonstrates not only the complexity of solving a particular problem but also the inherent difficulties in forcing beginners to jump from a problem description to a solution. Our experiences show that using LabVIEW and Alice as graphical foundations, with several carefully designed examples, may help students more quickly learn the process involved in computer-based problem solving than they would with traditional techniques.","Vehicles,
Education,
Problem-solving,
Computer languages,
Computer science,
Instruments,
Parallel programming,
Natural languages,
Software design,
Software engineering"
Performance evaluation of inter-domain IP traceback,"IP traceback is technology used to find the true source address of a Distributed Denial of Service (DDoS) attack with source address spoofing. We focus on IP option traceback (IP-OPT) for inter-domain IP traceback. In the Passive Detection Packet (PDP) method, which is a basic mechanism of IP-OPT, there is a trade off between the amount of trace traffic and the detection time for the path of attack time. However, no analysis of this condition has been made at this time. Thus, we mathematically analyze the tradeoff of PDP, and show that 1.1/spl times/10/sup -4/ is the optimal value of the pacekt generation probability for IP-OPT through numerical experiments.","Computer crime,
Protocols,
Mathematical model,
Information science,
Delta modulation,
Web and internet services,
Protection,
Capacity planning"
Banked multiported register files for high-frequency superscalar microprocessors,"Multiported register files are a critical component of high-performance superscalar microprocessors. Conventional multiported structures can consume significant power and die area. We examine the designs of banked multiported register files that employ multiple interleaved banks of fewer ported register cells to reduce power and area. Banked register files designs have been shown to provide sufficient bandwidth for a superscalar machine, but previous designs had complex control structures that would likely limit cycle time and add to design complexity. We develop a banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank. We present area, delay, and energy numbers extracted from layouts of the banked register file. For a four-issue superscalar processor, we show that we can reduce area by a factor of three, access time by 20%, and energy by 40%, while decreasing IPC by less than 5%.","Microprocessors,
Registers,
Pipelines,
Logic,
Delay,
Bandwidth,
Microarchitecture,
Laboratories,
Computer science,
Limit-cycles"
Test case generation and reduction by automated input-output analysis,"In the software testing process, selecting the test cases and verifying their results requires a lot of subjective decisions and human intervention. For a program having a large number of inputs, the number of corresponding combinatorial black-box test cases is huge. A method needs to be established in order to limit the number of test cases and to choose the most important ones. In this research effort we present a novel methodology for identifying important test cases automatically. These test cases involve input attributes which contribute to the value of an output and hence are significant. The reduction in the number of test cases is attributed to identifying input-output relationships. A ranked list of features and equivalence classes for input attributes of a given code are the main outcomes of this methodology. Reducing the number of test cases results directly in the saving of software testing resources.",
A novel mechanism for contention resolution in HFC networks,"The Medium Access Control (MAC) scheme proposed by DAVIC/DVB, IEEE 802.14 and DOCSIS for the upstream channel of Hybrid Fiber Coaxial (HFC) access networks is based on a mixable contention-based/contention-less time slot assignment. Contention-less slots are assigned by the head end to end stations according to a reservation scheme. Contention-based slots are randomly accessed by active terminals without any preliminary allocation, so that collisions may occur. To resolve contention, the contention tree algorithm has been widely accepted by the DVB/DAVIC, IEEE 802.14 and DOCSIS standards for MAC because of higher throughput and lower access delay. In this paper we propose a novel contention resolution mechanism and compare its performance with that of existing procedures. The proposed procedure is termed as static arrival slot mechanism. In this mechanism, one slot in each frame is exclusively reserved for new arrivals that wish to access the channel using contention resolution, and at least one slot is reserved for resolving their contention if there was one in the arrival slot. The performance of the proposed mechanism is evaluated through analysis and simulation. The results show that the proposed mechanism outperforms existing contention resolution procedures under heavy traffic.","Intelligent networks,
Hybrid fiber coaxial cables,
Helium,
Media Access Protocol,
Digital video broadcasting,
Access protocols,
Mathematics,
Computer science,
USA Councils,
Delay"
Switch scheduling via randomized edge coloring,"The essence of an Internet router is an n /spl times/ n switch which routes packets from input to output ports. Such a switch can be viewed as a bipartite graph with the input and output ports as the two vertex sets. Packets arriving at input port i and destined for output port j can be modeled as an edge from i to j. Current switch scheduling algorithms view the routing of packets at each time step as a selection of a bipartite matching. We take the view that the switch scheduling problem across a sequence of time-steps is an instance of the edge coloring problem for a bipartite multigraph. Implementation considerations lead us to seek edge coloring algorithms for bipartite multigraphs that are fast, decentralized, and online. We present a randomized algorithm which has the desired properties, and uses only a near-optimal /spl Delta/ + o(/spl Delta/) colors on dense bipartite graphs arising in the context of switch scheduling. This algorithm extends to non-bipartite graphs as well. It leads to a novel switch scheduling algorithm which, for stochastic online edge arrivals, is stable, i.e. the queue length at each input port is bounded at all times. We note that this is the first decentralized switch scheduling algorithm that is also guaranteed to be stable.","Switches,
Scheduling algorithm,
Packet switching,
Bipartite graph,
Communication switching,
Traffic control,
Hardware,
Internet,
Routing,
Stochastic processes"
Study on cross-layer design and power conservation in ad hoc network,"As the use of small portable and mobile computing devices increases, ad hoc networks have become an increasingly popular topic of research and development activities. Cross-layer design and power conservation are two new technologies in this area. Both of them can make the ad hoc network more robust and more adaptive. The relation between them is improvable. A power control based cross-layer architecture is presented.","Cross layer design,
Intelligent networks,
Ad hoc networks,
Protocols,
Network topology,
Routing,
Robustness,
Physical layer,
Bandwidth,
Computer science"
Mockup-driven fast-prototyping methodology for Web requirements engineering,"Web application development differs from the development of traditional software in several significant ways; therefore requirements engineering for Web applications entails new demands accordingly. This paper proposes an extreme Web requirements engineering - mockup-driven fast-prototyping methodology to help elicit and finalize system requirements, as well as facilitate adjustment to quickly changing user requirements typical to Web applications. Supporting the inclusion of customer feedback early in the development process, this strategy minimizes the risk of wasting valuable development efforts because of ambiguous or incomplete specifications. Real-life experiences of the use of the methodology in industry are reported as examples.","Application software,
Software prototyping,
Computer science,
Feedback,
Computer networks,
Costs,
Programming environments,
Software design,
Computer architecture,
Service oriented architecture"
A service robot for automating the sample management in biotechnological cell cultivations,"In this paper we present a mobile robot system that is capable of automating the sample management in a biotechnological laboratory. The system consists of a mobile platform and a robot arm. It can navigate freely in the laboratory and operate standard devices needed for the sample management. The platform uses an extended Kalman filter for localization and A* algorithm for path planning on a tangent graph computed from the laboratory's map. Motion execution has been designed to be as predictable as possible to not irritate, disturb or harm human personnel. The robot arm uses color vision to detect devices and compensate for positioning errors. The parameters and tasks needed to operate the devices are specified in simple scripts to allow quick and easy adaptations to other situations.","Service robots,
Humans,
Personnel,
Mobile robots,
Laboratories,
Robotics and automation,
Bioreactors,
Sampling methods,
Computer science,
Navigation"
"Making e-government happen everyday co-development of services, citizenship and technology","In a joint research project concerning the use and design of IT in public services, we are using a simple figure of on-going design-oriented interactions to highlight shifting foci on relationships of co-development of services, citizenship and technology. We bring together a number of concrete examples of this on-going everyday co-development, presented from the different perspectives that we, as researchers from different disciplines and traditions, represent in the project. The article explores and discusses working relations of technology production and use that we see as central to what is actually making e-government happen - or not happen. The main challenge in this area, as we see it, concerns making visible, and developing supportive infrastructures for, the continuing local adaptation, development and design in use of integrated IT and public services.","Electronic government,
Web and internet services,
Concrete,
Europe,
Rhetoric,
Software engineering,
Computer science,
Humans,
Production,
Procurement"
Two-mode overmodulation in two-level voltage source inverter using principle control between limit trajectories,A two-mode overmodulation in two-level VSI using the control principle between limit trajectories is presented. The control principle between limit trajectories is used to ensure linear modulation from linear range of space vector pulse width modulation to six step mode and two-mode is employed to get better performance of overmodulation. The proposed principle is verified by software simulation.,"Voltage control,
Amplitude modulation,
Pulse width modulation inverters,
Space vector pulse width modulation,
Pulse width modulation,
Table lookup,
Support vector machines,
Space technology,
Computer science,
Nonlinear equations"
Scenario-based test case generation for state-based embedded systems,"To reduce testing cost and effort, the paper proposes a systematic approach to generate test cases for state-based embedded systems. This process first derives a state/event tree based on a scenario specification, with a node of the tree representing a state, and a link a transition between two states. Once the tree is obtained, it is possible to generate test inputs based on partition testing, random testing and boundary value testing. It is also possible to perform various analyses such as completeness and consistency analysis, dependency analysis and relationship analysis. An XML-based tool has been developed to automate many of the steps in the process. Whenever there is a change to the system, the tester needs to modify the state/event tree, and the tool automatically re-generates the new test cases to test those changed parts as well as perform selective regression testing to test those affected parts. To illustrate the ideas, the paper uses a wireless mobile phone system as an example. The system consists of three parts: a mobile station center server, several base station servers, and clients.","System testing,
Computer aided software engineering,
Embedded system,
Performance analysis,
Automatic testing,
Computer science,
Costs,
Performance evaluation,
Regression tree analysis,
Merging"
The research on QoS for grid computing,"Grid computing is a hotspot in recent years. It is believed that every application will be based on grid computing in the near future. This paper analyzes the importance of research on grid quality of service (QoS) and presents many QoS problems. Then, our solution to QoS of open grid services architecture (OGSA) is proposed and sketched. The solution is very flexible and is typical of grid QoS.","Grid computing,
Quality of service,
Application software,
Computer science,
Computer architecture,
Large-scale systems,
Resource management,
Problem-solving,
Web server,
Packaging"
Power-efficiency clustering method with power-limit constraint for sensor networks,"The conventional clustering method has the unique potential to be the framework for power-conserving ad hoc networks. In this environment, studies on energy-efficient strategies, such as sleeping mode and redirection, have been reported, and, recently, some have even been adopted by standards like Bluetooth and IEEE 802.11. However, let us consider sensor networks. The devices employed are power-limited in nature; introducing the conventional clustering approach to the sensor networks provides a unique challenge due to the fact that cluster-heads, which are communication centers by default, tend to be heavily utilized and thus drained of their battery power rapidly. We introduce a re-clustering strategy and a power-limit constraint for cluster-based sensor wireless networks in order to address the power-conserving issues in such networks, while maintaining the merits of a clustering approach. Based on a practical energy model, simulation results show that the improved clustering method can achieve a longer lifetime when compared with the conventional clustering method.",
On the load distribution and performance of meta-computing systems,,"Distributed computing,
Dynamic scheduling,
Scheduling algorithm,
Concurrent computing,
Grid computing,
Computer science,
Educational institutions,
Processor scheduling,
Heuristic algorithms,
High performance computing"
Implementation and validation of multicast-enabled landmark ad-hoc routing (M-LANMAR) protocol,"In this paper, we investigate the performance of multicast-enabled landmark ad-hoc routing (denoted as M-LANMAR) protocol using a Linux implementation as well as a QualNet simulation model. The main objectives are the validation of our implementation and the demonstration of the advantages of M-LANMAR. Using a Linux implementation of ODMRP, we compare the performance of M-LANMAR to that of ODMRP in a small-scale testbed. Through simulation, we show also the scalability of M-LANMAR to large networks.","Multicast protocols,
Routing protocols,
Mobile ad hoc networks,
Wireless application protocol,
Land vehicles,
Unmanned aerial vehicles,
Unicast,
Computer science,
Computational modeling,
Computer simulation"
Image deblurring: I can see clearly now,"Inverse problems are among the most challenging computations in science and engineering because they involve determining the parameters of a system that is only observed indirectly. For example, we might have a spectrum and want to determine the species that produced it as well as their relative proportions, or we may have sonar measurements of a containment tank and want to know whether it has an internal crack. Given a blurred image and a linear model for the blurring, the original image is reconstructed. This linear inverse problem illustrates the impact of ill-conditioning on the choice of algorithms.","Image restoration,
Singular value decomposition,
Matrix decomposition,
Home computing,
Partial differential equations,
Eigenvalues and eigenfunctions,
Numerical analysis,
Knowledge engineering,
Scientific computing,
Educational programs"
An interactive electronic book approach for teaching computer implementation of industrial control systems,"There is an overall consensus on the importance of laboratory work that exposes the students to broader and more practical issues of industrial control systems, such as their implementation by distributed computer systems (DCSs) and programmable logic controllers (PLCs). However, setting up appropriate laboratory facilities to serve this purpose is expensive. For this reason, an interactive learning environment has been developed around the concept of the electronic book. The architecture of the environment allows the integration of hypertext with simulators of DCS, PLC, and process operation. The simulators are specially designed to serve an application-oriented teaching approach, which involves the student in the simulation setup and the running of the application. They are able to simulate not only the execution of the software that realizes the regulatory control algorithms but also the start-up and emergency control strategies of an industrial process, the manual, automatic, and cascade modes of controller operation, and the man-machine interface of a DCS- or PLC-based control system. The applications on which the teaching of DCS and PLC-based control system implementation is based are the interactive advanced control of a distillation column and the pH control of a reactor solution.",
Stochastic stage-structured modeling of the adaptive immune system,"We have constructed a computer model of the cytotoxic T lymphocyte (CTL) response to antigen and the maintenance of immunological memory. Because immune responses often begin with small numbers of cells and there is great variation among individual immune systems, we have chosen to implement a stochastic model that captures the life cycle of T cells more faithfully than deterministic models. Past models of the immune response have been differential equation based, which do not capture stochastic effects, or agent-based, which are computationally expensive. We use a stochastic stage-structured approach that has many of the advantages of agent-based modeling but is much more efficient. Our model can provide insights into the effect infections have on the CTL repertoire and the response to subsequent infections.","Stochastic systems,
Adaptive systems,
Immune system,
Biological system modeling,
Biology computing,
Computer science,
Collaboration,
Mathematical model,
Pathology,
Cells (biology)"
The augmented composer project: the music table,The music table enables the composition of musical patterns by arranging cards on a tabletop. An overhead camera allows the computer to track the movements and positions of the cards and to provide immediate feedback in the form of music and on-screen computer generated images. Musical structure is experienced as a tangible space enriched with physical and visual cues about the music produced.,"Multiple signal classification,
Augmented reality,
Instruments,
Cameras,
Tracking,
Feedback,
Image generation,
Timing,
Information science,
Laboratories"
Enabling the Efficient Use of SMP Clusters: The GAMESS/DDI Model,"An important advance in cluster computing is the evolution from single processor clusters to multi-processor SMP clusters. Due to the increased complexity in the memory model on SMP clusters, new approaches are needed for applications that make use of distributed-memory paradigms. This paper presents new communications software developments that are designed to take advantage of SMP cluster hardware. Although the specific focus is on the central field of computational chemistry and materials science, as embodied in the popular electronic structure package GAMESS (General Atomic and Molecular Electronic Structure System), the impact of these new developments will be far broader in scope. Following a summary of the essential features of the distributed data interface (DDI) in the current implementation of GAMESS, the new developments for SMP clusters are described. The advantages of these new features are illustrated using timing benchmarks on several hardware platforms, using a typical computational chemistry application.","Hardware,
Chemistry,
Games,
Application software,
Programming,
Materials science and technology,
Electronics packaging,
Molecular electronics,
Timing,
Computer applications"
A topology control algorithm for constructing power efficient wireless ad hoc networks,"In this paper, we present a localized algorithm for constructing power efficient topology for wireless ad hoc networks. Each mobile node determines its own transmission power based only on local information. The proposed algorithm first constructs the constrained Gabriel graph from the given unit disk graph and then reduces the total transmission power by allowing each node individually excises some replaceable links. The constructed topology is sparse, has a constant bounded power stretch factor, and the total transmission power is lower than those obtained from other proposed algorithms. In addition, compared with others, our algorithm requires lower time complexity to generate a solution, and can thus further save the energy for each mobile node. We demonstrate the performance improvements of our algorithm through simulations.","Network topology,
Ad hoc networks,
Mobile ad hoc networks,
Computer networks,
Protocols,
Mobile computing,
Euclidean distance,
Energy consumption,
Power engineering computing,
Information science"
Ad hoc on-demand routing protocol setup with backup routes,"The ad hoc backup node setup routing protocol (ABRP) is proposed to focus attention on the intrinsic properties of the ad hoc wireless networks. It more completely considers the quality of routing than the routing protocols proposed in the past. According to the proposed ABRP, many routes can be found to reach a destination in a given period. Those routes, almost more than one, from the source node to the destination node can be analyzed to obtain some good backup routes to support reconnection when a link fails. The backup route information can be saved in a specific on-the-route node. These backup routes can be rapidly found in situations involving disconnection or loss of connection. Furthermore, ABRP provides a backup node mechanism that reconnects quickly as required for ad hoc wireless networks.","Routing protocols,
Wireless networks,
Network topology,
Ad hoc networks,
Energy consumption,
Switches,
Computer science,
Failure analysis,
Radio communication,
Batteries"
A new approach using time-based model for TCP-friendly rate estimation,"For the multimedia streaming applications to apply RTP/RTCP, two significant factors are the performance of long periodic control with RTCP, and the friendliness to TCP on the rate adjustment. To satisfy these two criteria, this paper proposes a rate estimation scheme based on the packets loss ratio and jitter ratio sampling at the RTP/RTCP-like receiver and adopting the time-based TCP model. The time-based model is a rate equation of the variables of time, while the existing model, named packet-based, is another equation of the variables of packet counting. The meaning of rate estimation is that for one connection of a particular sending rate, the receiver can closely estimate the average transmission rate of other TCP flows in competition. The simulation results show that our rate estimation approach conducts good estimation, and can be the basis of rate adjustment and congestion control.",
MEH: modular evolvable hardware for designing complex circuits,"Evolvable hardware adjusts oneself to changeable environments by self-organizing the circuit. Due to its high productivity and creativity for designing circuit, it is widely investigated. However, it is very difficult to apply it to a complicated circuit, because the search space increases exponentially as the complexity of hardware. In this paper, we propose a modular approach to evolving complex hardware circuits effectively. A comparative experiment with the conventional evolutionary approach indicates that the proposed method works 50/spl sim/1000 times faster and yields a more optimized hardware.","Hardware,
Logic functions,
Genetic algorithms,
Digital circuits,
Biological cells,
Fuses,
Computer science,
Design methodology,
Algorithm design and analysis,
Programmable logic devices"
Towards index-based similarity search for protein structure databases,We propose two methods for finding similarities in protein structure databases. Our techniques extract feature vectors on triplets of SSEs (secondary structure elements) of proteins. These feature vectors are then indexed using a multidimensional index structure. Our first technique considers the problem of finding proteins similar to a given query protein in a protein dataset. This technique quickly finds promising proteins using the index structure. These proteins are then aligned to the query protein using a popular pairwise alignment tool such as VAST. We also develop a novel statistical model to estimate the goodness of a match using the SSEs. Our second technique considers the problem of joining two protein datasets to find an all-to-all similarity. Experimental results show that our techniques improve the pruning time of VAST3 to 3.5 times while keeping the sensitivity similar.,"Proteins,
Iterative algorithms,
Atomic measurements,
Dynamic programming,
Spatial databases,
Amino acids,
Heuristic algorithms,
Computer science,
Feature extraction,
Multidimensional systems"
Real-time scheduling of hierarchical reward-based tasks,"A reward-based task typically consists of a mandatory part that must be accomplished before the given deadline, and an optional part that is associated with rewards for partial completion. In this paper we consider a hierarchical framework of reward-based tasks. These types of tasks are characterized by positive rewards, tree-like order-dependency and identical service times. We propose a near-optimal scheduling algorithm for such tasks under hard-real time constraints. In our technique, tasks are pre-sorted by their potential rewards and real-time scheduling can be achieved with no prior knowledge of the hard deadlines. We also demonstrate how this approach could be utilized for uninterrupted transfer of multimedia in varying network conditions, while delivering near-best results.","Optimal scheduling,
Real time systems,
Iris,
Processor scheduling,
Laboratories,
Computer science,
Scheduling algorithm,
Time factors,
Resource management,
Computational modeling"
Improving Delaunay triangulation for application-level multicast,"In recent years, there has been increasing interest in application-level multicast (ALM), where the multicast related functionalities are moved to end-hosts. One of the promising ALM protocols is Delaunay triangulation (DT), which constructs an overlay mesh using 2-D Delaunay triangulation (DT) and makes use of compass routing to forward packets. However, DT protocol as it is originally proposed suffers from several weaknesses: 1) it requires users to input its geographic location, and assumes that the location correlates well with network distance; 2) it tends to form multiple connections across two domains, and hence has a high usage of long delay (interdomain) links; 3) it does not consider the fanout of a host, therefore some less-powerful hosts may serve too many users, leading to degradation of service. To address these problems, we propose to use global network positioning (GNP) for host location estimation and forward delegation to limit the fanout of a host explicitly and efficiently trade off the network resource usage with latency. Using Internet-like topologies, we show that our scheme, as compared to the original DT protocol, can substantially reduce average relative delay penalty, physical link stresses and network resource usage while meeting the processing capability of the hosts in the network.","Economic indicators,
Delay,
IP networks,
Routing protocols,
Multicast protocols,
Computer science,
Degradation,
Network topology,
Stress,
Proposals"
On automatic partial orders,"We investigate partial orders that are computable, in a precise sense, by finite automata. Our emphasis is on trees and linear orders. We study the relationship between automatic linear orders and trees in terms of rank functions that are versions of Cantor-Bendixson rank. We prove that automatic linear orders and automatic trees have finite rank. As an application we provide a procedure for deciding the isomorphism problem for automatic ordinals. We also investigate the complexity and definability of infinite paths in automatic trees. In particular, we show that every infinite path in an automatic tree with countably many infinite paths is a regular language.","Automata,
Scattering,
Computer science,
Tree graphs,
Mathematics,
Lattices,
Logic,
Upper bound"
Using a genetic algorithm optimizer tool to generate good quality timetables,This paper describes a Genetic Algorithm optimizer tool with adaptive parameter control designed to generate a university timetable. In this research we aim to show that by controlling the parameter settings of the genetic operators we can improve the quality of the timetable. This tool was tested on actual data and we present the experimental results.,"Genetic algorithms,
Constraint optimization,
Computer science,
Information technology,
Design optimization,
Programmable control,
Adaptive control,
Algorithm design and analysis,
Testing,
Simulated annealing"
A case study of parallel I/O for biological sequence search on Linux clusters,"In this paper we analyze the I/O access patterns of a widely-used biological sequence search tool and implement two variations that employ parallel-I/O for data access based on PVFS (Parallel Virtual File System) and CEFT-PVFS (cost-effective fault-tolerant PVFS). Experiments show that the two variations outperform the original tool when equal or even fewer storage devices are used in the former. It is also found that although the performance of the two variations improves consistently when initially increasing the number of servers, this performance gain from parallel I/O becomes insignificant with further increase in server number. We examine the effectiveness of two read performance optimization techniques in CEFT-PVFS by using this tool as a benchmark. Performance results indicate: (1) doubling the degree of parallelism boosts the read performance to approach that of PVFS; and (2) skipping hot-spots can substantially improve the I/O performance when the load on data servers is highly imbalanced. The I/O resource contention due to the sharing of server nodes by multiple applications in a cluster has been shown to degrade the performance of the original tool and the variation based on PVFS by up to 10 and 21 folds, respectively; whereas, the variation based on CEFT-PVFS only suffered a two-fold performance degradation.","Biomedical computing,
Sequences,
Computer input-output,
Parallel programming,
Virtual memories"
Formulated silhouettes for sketching terrain,"The mathematical definition of silhouettes within NPAR is based on the assumption that they are the eye-centered projections of occluding contours on an imaging plane. However, occluding contours are insufficient for sketching the silhouettes of hills as drawn by cartographers. The research reported in this paper is based on the proposition that silhouettes are mental visualizations of the outlines of objects that arise from knowledge and experience of the visual world. This paper does not seek to provide an alternative definition of silhouettes. Instead, it tests the proposition that missing silhouette elements should be drawn on those visible surfaces brought into view through adoption of a higher viewpoint.","Computer science,
Computer vision,
Data visualization,
Digital elevation models,
Rendering (computer graphics),
Internet,
Testing,
Information systems,
Concurrent computing,
Permission"
Visual cluster validity (VCV) displays for prototype generator clustering methods,Conventional cluster validity techniques usually represent all the validity information available about a particular clustering by a single number. The display method introduced here is an alternative to standard validity functionals. The proposed approach uses intensity images generated from the results of any prototype generator clustering algorithm as a means for cluster validation. Several numerical examples are given to illustrate the method.,"Prototypes,
Clustering methods,
Clustering algorithms,
Computer displays,
Partitioning algorithms,
Fuzzy logic,
Computer science,
Image generation,
Fuzzy sets,
Shape"
Supporting digital signatures in mobile environments,"Digital signatures, like physical signatures, can verify that a specific user affixed their signature to a document and they can also verify that the document is the same as when the user affixed the digital signature. Digital signature systems (DSS) use public key cryptography methods to create digital signatures. The integrity of the digital signature is tied to the security of the user's private key. As long as the user's private key is secure, then only the user can affix their digital signature to a document. In this paper, we examine methods and risks involved in creating digital signatures on workstations other than the user's primary workstation. The challenge is to allow the user to create a digital signature, which requires their private key, at workstations where we cannot guarantee the key's security.","Digital signatures,
Decision support systems,
Workstations,
Public key cryptography,
Security,
Public key,
Computer science,
Printers,
Protection,
Privacy"
Efficient minimization of multiple-valued decision diagrams for incompletely specified functions,"This paper addresses the problem of finding a small size Multiple-Valued Decision Diagram (MDD) representation of an incompletely specified multiple-valued function. Optimal MDD representation improves performance and flexibility of many applications in logic design and multiple-valued circuit synthesis. We introduce an algorithm which incorporates a new operation on incompletely specified MDDs, called fusion. The diagram is optimized by dynamic variable ordering, graph compaction and minimization. During the optimization the structure of the underlying MDD is modified in a way that only specified values are represented while don't cares are ignored. The results on multiple-valued as well as binary benchmarks with don't cares are given to demonstrate the efficiency and robustness of the algorithm.","Minimization,
Circuit synthesis,
Computer science,
Logic design,
Compaction,
Robustness,
Ultra large scale integration,
Network synthesis,
Field programmable gate arrays,
Data mining"
Human face detection using angular radial transform and support vector machines,"This paper presents a new face detection method. For a potential face pattern, a histogram equalized intensity map and a local intensity variance map are created to normalize the pattern photometrically. We then view these two maps as geometric shapes and apply the angular radial transform (ART) to derive a compact representation of the pattern. The ART transform coefficients are then used as input to a support vector machine (SVM) to determine the presence or absence of a face in the pattern. We also develop a SVM based skin color detection technique as a preprocessing step and only search image regions that contain sufficient large number of skin pixels thus greatly enhancing the detection speed. Experimental results are presented to demonstrate the effectiveness of the new method.",
Spread-spectrum Markovian-code acquisition in asynchronous DS/CDMA systems,SS codes generated by a Markov chain are shown to be superior to LFSR-based codes and IID codes in asynchronous DS/CDMA systems in terms of the following two common performance measures: 1) success rate of acquisition; and 2) bit error rate of the correlator using an accumulated version of the serial search acquisition.,"Spread spectrum communication,
Multiaccess communication,
Bit error rate,
Delay effects,
Delay estimation,
Gold,
Random variables,
Computer science,
Correlators,
Multiple access interference"
An energy efficient framework for mobile target tracking in sensor networks,"Most existing work on sensor networks concentrates on finding efficient ways to forward data from the information source to the data center, and not much work has been done on collecting local data and generating the data report. This paper studies this issue by proposing techniques to track a mobile target and monitor its surrounding region. We introduce a framework called dynamic convoy tree-based collaboration (DCTC), and formalize it as a multiple objective optimization problem which needs to find a convoy tree sequence with high tree coverage and low message complexity. We propose an optimal solution which achieves 100% coverage and minimizes the message complexity under certain ideal situations. Considering the real constraints of sensor networks, we propose two practical solutions: the conservative scheme and the prediction-based scheme. Extensive simulations are conducted to compare these schemes, and the results show that the prediction-based scheme outperforms the conservative scheme, and it can achieve a relatively high coverage and low message complexity close to the optimal scheme.","Energy efficiency,
Target tracking,
Intelligent networks,
Sensor phenomena and characterization,
Collaboration,
Wireless sensor networks,
Monitoring,
Collaborative work,
Computer science,
Data engineering"
Load cell response correction using analog adaptive techniques,"Load cell response correction can be used to speed up the process of measurement. This paper investigates the application of analog adaptive techniques in load cell response correction. The load cell is a sensor with an oscillatory output in which the measurand contributes to response parameters. Thus, a compensation filter needs to track variation in measurand whereas a simple, fixed filter is only valid at one load value. To facilitate this investigation, computer models for the load cell and the adaptive compensation filter have been developed and implemented in PSpice. Simulation results are presented demonstrating the effectiveness of the proposed compensation technique.","Intelligent sensors,
Filters,
Sensor systems,
Signal processing,
Velocity measurement,
Sensor phenomena and characterization,
Transfer functions,
Electrostatic discharge,
Computer science,
Load modeling"
Enterprise application integration using a component-based architecture,"Enterprise Application Integration (EAI) and Business-to-Business integration (B2B) leverage several key technologies including middleware and message brokers. However, an effective integration solution largely depends on the right combination of technologies that provide the glue between disparate applications. To build a technology infrastructure that can adapt to changes as the integration architecture evolves is crucial. The solution to the integration problem is forcing companies to think in distributed terms. In this paper, we show that a formula for successful deployment of an integrated organization is to have a technology infrastructure, consisting of a standard middleware and application server, that incorporates a distributed object infrastructure as well as platform to develop and integrate component-based applications. In particular, the paper evaluates the suitability of CORBA and EJBs as the enablers of EAI, and also the suitability of XML as a data definition and integration language.","Application software,
Computer architecture,
XML,
Business,
Middleware,
Computer applications,
Automation,
Computer science,
Australia,
Electronic mail"
A coordinated plan for teaching software engineering in the Rey Juan Carlos University,"Nowadays both industry and academic environments are showing a lot of interest in the Software Engineering discipline. Therefore, it is a challenge for universities to provide students with appropriate training in this area, preparing them for their future professional practice. There are many difficulties to provide that training. The outstanding ones are: the Software Engineering area is too broad and class hours are scarce, the discipline requires a high level of abstraction; it is difficult to reproduce real world situations in the classroom to provide a practical learning environment; the number of students per professor is very high (at least in Spain); companies develop software with a maturity level rarely over level 2 of the CMM for Software (again, at least in Spain) as opposed to what is taught at the University. Besides, there are different levels and study plans, making more difficult to structure the contents to teach in each term and degree. In this paper we present a plan for teaching Software Engineering trying to overcome some of the difficulties above.","Education,
Software engineering,
Computer science,
Industrial training,
Engineering management,
Systems engineering and theory,
Computer industry,
Coordinate measuring machines,
Management information systems,
Laboratories"
A system model for mobile commerce,"The emergence of wireless and mobile networks has made possible the introduction of electronic commerce to a new application and research subject: mobile commerce. Understanding or constructing a mobile commerce system is an arduous task because the system involves a wide variety of disciplines and technologies. To facilitate understanding and constructing such a system, this article divides a mobile commerce system into six components: (i) mobile commerce applications, (ii) mobile stations, (iii) mobile middleware, (iv) wireless networks, (v) wired networks, and (vi) host computers. Elements in components related to mobile commerce are described in detail and lists of technologies for component construction are also given.","Business,
Mobile computing,
Electronic commerce,
Application software,
Handheld computers,
Computer science,
Wireless networks,
Middleware,
Computer networks,
Databases"
Optimization model for opportunistic replacement policy using genetic algorithm with fuzzy logic controller,"The paper presents a genetic algorithm with fuzzy logic controller for determining opportunistic replacement policy for deteriorating components of an equipment or system. An opportunistic replacement model has been formulated by considering the dynamics of the decision process of such a policy. In order to reduce the computational burden involving complete enumeration of all possible policies, genetic algorithm has been used to find near optimal solution by maximizing net benefit to be gained from an opportunistic replacement. A fuzzy logic controller has been used to automatically adjust the fine-tuning structure of genetic algorithm parameters. The performance of the model and the solution procedure has been evaluated for a number of case problems, which clearly demonstrates that the proposed method is very effective.","Genetic algorithms,
Fuzzy logic,
Chemical technology,
Australia,
Costs,
Mathematical model,
Computer science,
Information technology,
Chemical engineering,
Control systems"
Level-set based geometric colour snake with region support,A novel method is introduced to force a geometric-based snake be more tolerant towards weak edges and noise in images. The method integrates gradient flow forces with region constraints obtained from diffused region segmentation forces. The diffusion is obtained from the region map vector flow field. This extra region force gives the snake a global view of the boundary information within the image. We present results on both graylevel and colour images.,
A scalable hash-based mobile agent location mechanism,"In this paper, we propose a novel mobile agent tracking mechanism based on hashing. To allow our system to adapt to variable workloads, dynamic rehashing is supported The proposed mechanism scales well with both the number of agents and the number of moving and querying operations. We also report on its implementation in the Aglets platform and present performance results.","Mobile agents,
Computer science,
Mobile communication,
Network servers,
Mobile computing,
Real time systems,
Information retrieval,
Open systems,
Merging,
Load management"
PCI data acquisition card for application in radiation imaging systems,An efficient data acquisition system for operation with different types of radiation imaging detectors was developed and tested. This system uses a PCI-416 industrial high-speed analog data acquisition card from DATEL. The card is based on 32-bit PCI bus architecture and operates inside a wide variety of PC compatible computers. The hardware for analog signal interfacing as well as software for error free continuous data collection was developed. A data acquisition system based on the PCI-416 was tested on several projects and different computers running MS Windows NT4.0 OS and MS Windows 2000 OS.,
The Y-architecture: yet another on-chip interconnect solution,"In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0/spl deg/, 60/spl deg/, and 120/spl deg/). To evaluate the efficiency of different interconnect architectures, we assume mesh structures with uniform communication demand and develop a multi-commodity flow (MCF) approach to model the on-chip communication traffic. We also extend the combinatorial MCF algorithm of N. Garg and J. Konemann (Proc. 39th Annual Symp. on Foundations of Comp. Sci., pp. 300-309, 1998) to compute the optimal routing resource allocations for different interconnect architectures. The experiments show that: (1) compared with the Manhattan architecture, the Y-architecture demonstrates a throughput improvement of 30.7% for a square chip. The throughput of the Y-architecture is only 2.5% smaller than that of the X-architecture. (2) A chip with the shape of a convex polygon produces better throughput than a rectangular chip: For Y-architecture, a hexagonal chip provides 41% more throughput than a squared chip using the Manhattan architecture. For Manhattan architecture, a diamond chip achieves a throughput improvement of 19.5%, over the squared chip using the same interconnect architecture. (3) Compared with Manhattan architecture, the Y-architecture reduces the wire length of a randomly distributed two pin net by 13.4% and the average wire length of Y-architecture is only 4.3% more than that of the X-architecture.","Routing,
Integrated circuit interconnections,
Computer architecture,
Throughput,
Wires,
Resource management,
Costs,
Computer science,
Traffic control,
Shape"
Web/spl I.bar/Soc: a Socratic-dialectic-based collaborative tutoring system on the World Wide Web,"A Web/spl I.bar/Soc tutoring system for recursion concept learning is proposed. The system integrates different modes of learning, synchronous and asynchronous, collaborative and individualized, into a World Wide Web (WWW) environment. The paper proposes a collaborative learning protocol. Based on the protocol, the principles of the Socratic dialogue are applied to the collaborative learning of recursion concepts. In order to evaluate the effects on learning of the Web/spl I.bar/Soc system, an educational experiment was conducted. The results show that the effects from the Web/spl I.bar/Soc system were more obvious than merely studying articles.","Internet,
Collaborative work"
Uncertainties and the flexible logics,"How to deal with various uncertainties and evolution has been a critical problem for further development of AI. The well-developed mathematical logic is too rigid and it can only solve certain problems. It is a new challenge for logics to make mathematical logic more flexible and can contain various uncertainties and evolution. This has been studied by the academic community for several years and has made some breakthroughs at some isolated points, but a systematic theory has not come into being. The flexible propositional logics is put forward on the basis of our study of the general logical rules in real world, which can include quite a few kinds of known uncertainty reasoning models, such as probability, belief, likelihood, certainty, possibility and fuzzy reasoning, and so on. Moreover, it lays the theoretical basis for the study of more complex uncertainty problems and evolution problems.","Uncertainty,
Fuzzy logic,
Artificial intelligence,
Fuzzy reasoning,
Machine learning,
Fuzzy sets,
Helium,
Computer science,
Extraterrestrial measurements,
Cybernetics"
Quorum-based asynchronous power-saving protocols for IEEE 802.11 ad hoc networks,"We investigate the power mode management problem for an IEEE 802.11-based mobile ad hoc network (MANET) that allows mobile hosts to tune to the power-saving (PS) mode. We adopt an asynchronous approach proposed in [Y. C. Tseng et al., (2002)] and correlate this problem to the quorum system concept. We identify a rotation closure property for quorum systems. It is shown that any quorum system that satisfies this property can be translated to an asynchronous power-saving protocol for MANETs. We derive a lower bound for quorum sizes for any quorum system that satisfies the rotation closure property. We identify a group of quorum systems that are optimal or near optimal in terms of quorum sizes, which can be translated to efficient asynchronous power-saving protocols. We also propose a new e-torus quorum system, which can be translated to an adaptive protocol that allows designers to trade hosts' neighbor sensibility for power efficiency","Protocols,
Ad hoc networks,
Mobile ad hoc networks,
Energy management,
Batteries,
Computer science,
Power engineering and energy,
Mobile communication,
Spread spectrum communication,
Synchronization"
Approaches to supporting software visual notation exchange,"A wide range of software tools provide software engineers with different views (static and dynamic) of software systems. Much work has focused on software information model exchange. However, most software tools lack support for exchange of information about visualisation notations (both definitions of notations and instances of them). Some basic converters have been developed to support the exchange of notation information between software tools but almost all are custom-built to support specific notations and difficult to maintain. We describe the development of several notation exchange converters for tools supporting software architecture notations. This leads to a unified converter generator framework for notation exchange.","Visualization,
Software tools,
Computer aided software engineering,
Software architecture,
Virtual reality,
Shape,
Software engineering,
Computer science,
Computer architecture,
Runtime"
A dynamic shadow approach for mobile agents to survive crash failures,Fault tolerance schemes for mobile agents to survive agent server crash failures are complex since developers normally have no control over remote agent servers. Some solutions inject a replica into stable storage upon its arrival at an agent server. However in the event of an agent server crash the replica is unavailable until the agent server recovers. This paper presents a failure model and a revised exception handling framework for mobile agent systems. An exception handler design is presented for mobile agents to survive agent server crash failures. A replica mobile agent operates at the agent server visited prior to its master's current location. If a master crashes its replica is available as a replacement. Experimental evaluation is performed and performance results are used to suggest some useful design guidelines.,"Mobile agents,
Computer crashes,
Fault tolerance,
Web server,
Information retrieval,
Performance analysis,
Computer science,
Performance evaluation,
Guidelines,
Internet"
Triangulation based technique for efficient stereo computation in infrared images,"This paper proposes a technique to compute an accurate semi-dense disparity map from infrared stereo image pairs obtained using an uncalibrated stereo rig. First, an initial sparse disparity map is obtained using corner matching methods. This map is then refined using our proposed triangular constraints and the epipolar geometrical constraints to yield a more accurate semidense disparity map. Experimental results obtained using the proposed method are reported along with results obtained with a classical correlation based method as well as a more recent method based on graph-cuts. The proposed method yields good results even with low resolution, low texture infrared images. The proposed method is designed to be part of a vision-based occupant sensing system that will help to control airbag deployment in future vehicles.","Optical computing,
Infrared imaging,
Computer science,
Image resolution,
Gray-scale,
Filters,
Design methodology,
Control systems,
Vehicles,
Lighting"
Trends in software process: the PSP and agile methods,"Water cooler discussions often gravitate to trends-the ""latest"" software process, programming language, or Web development paradigm. Some of them justly deserve the title ""trend"" or ""fad;"" others do not. In this article, two practitioners share their experience with current, yet diverse, software process approaches that are starting to greatly affect the industry. One author describes Science Applications International Corporation's (SAIC) experience with the Personal Software Process. The other author shares Motorola's experience with agile methods, a lighter-weight process approach. It appears that despite their diversity, both approaches have resulted in improved productivity and in similar or even higher-quality levels of the resulting products (from previous releases).","Pluto,
Mars,
Application software,
Productivity,
Graphical user interfaces,
Databases,
Computer languages,
Computer industry,
Unified modeling language,
Scheduling"
Semantic relationship and identification of music,"Automatic search engines for musical documents mostly provide a vast number of relevant and irrelevant hits including multiple appearances of the same documents. Generally, underlying information-processing methods that are based on metadata or simple content-based procedures cause this situation. Therefore we propose a model for the recognition of music that enables identification and comparison of musical documents regardless their actual instantiation. Inherent component of this model is the capability of navigation based on semantic relationship between musical pieces. In order to realize this functionality we introduce a concept that tries to derive deductive music recognition from generic music production as an equivalent counterpart. Hence, some deductive processes of human music perception are simulated. The resulting structures enclose musical meaning and therefore can be used for the estimation of identity and relationship between musical documents. As a byproduct of this functionality, plagiarism and copyright infringements could be detected.","Multiple signal classification,
Humans,
Databases,
Music,
Computer science,
Chemical technology,
Search engines,
Navigation,
Production,
Plagiarism"
Embedded system hardware design course track for CS students,"Qualified software engineers are often in charge of system architecture design, system software design and many hardware-related issues, especially for embedded systems. Nowadays, embedded systems are equipped with fully-functional operating systems, multi-media applications, communication protocols, and so on. Since the portion of software is getting larger and larger than hardware, it is natural that software engineers are more promising in management of system-level design and integration. To supply qualified software engineers, the School of Computer Science and Engineering in Seoul National University offers a series of hardware design courses on embedded systems. They consist of FPGA design, board-level hardware design, microprocessor-based embedded system and system software design. Actual prototype implementations are mandatory in each course. The track ends up with a two-semester design project. The course track produces 20 to 30 CS-background students with intensive experience of hardware design and implementation every year. This paper introduces the outline of the course track and results.","Embedded system,
Hardware,
Design engineering,
Software design,
Embedded software,
System software,
Computer architecture,
Operating systems,
Multimedia systems,
Multimedia communication"
Co-ordinated coscheduling in time-sharing clusters through a generic framework,"In this paper, we attempt to address several key issues in designing coscheduling algorithms for clusters. First, we propose a generic framework for deploying coscheduling techniques by providing a reusable and dynamically loadable kernel module. Second, we implement all prior dynamic coscheduling algorithms (dynamic coscheduling (DCS), spin block (SB) and periodic boost (PB)) and a new coscheduling technique, called co-ordinated coscheduling (CC), using the above framework. Third, with exhaustive experimentation using mixed workloads, we observe that unlike PB, which provided the best performance on a Solaris platform (followed by SB and DCS), the proposed CC scheme outperforms all other techniques on a Linux platform, followed by SB, PB and DCS, in that order. Finally, we argue that due to its modular design, portable implementation on a standard platform, high performance and tolerance to workload mixes, the proposed CC scheme can be a viable scheduling option for time-sharing clusters.","Processor scheduling,
Time-sharing computer systems,
Multiprogramming,
Distributed algorithms"
An accurate scene-based traffic model for MPEG video stream,"In this paper, we propose an accurate scene-based traffic model for MPEG video stream. The autocorrelation structure of video stream is modeled by two Auto-Regress (AR) processes. We also take the correlation in GOP (Group of Picture) into account. According to the size of I frame, the size of B frame and P frame in the same GOP are determined by a lognormal distribution. As for the marginal distribution of the MPEG video stream, we use normal mixture distribution to fit it, and the parameters of normal mixture distribution are estimated through an Expecting Maximum (EM) algorithm. At last, the accuracy of the proposed model is validated with simulation experiments and the application of this model in network QoS (Quality of Service) evaluation.","Traffic control,
Streaming media,
Autocorrelation,
Telecommunication traffic,
Quality of service,
Computer science,
MPEG 4 Standard,
Bit rate,
Probability distribution,
Degradation"
Evaluation of pseudorandom sequences for third generation spread spectrum systems,"Much research has been conducted on pseudorandom sequences used in spread spectrum systems. With the growing complexity of new systems, more sets of sequences need to be evaluated for their suitability in new applications, such as multi-rate code-division multiple access (CDMA) systems. Hence there is still a need for further study of these sequences. In our work, the performance of pseudo random sequences used in 3/sup rd/ generation spread spectrum systems has been analyzed statistically using auto and cross correlation properties of the sequences. The effect of pulse shape altering on the correlation properties has also been characterized.","Random sequences,
Spread spectrum communication,
Multiaccess communication,
Frequency,
Bandwidth,
Computer science,
Application software,
Pulse shaping methods,
Shape,
Filtering"
Systolic blood pressure classification,"To classify systolic, mean and diastolic blood pressure using the oscillometric method heavily depends on the computational algorithms. Generally, the algorithms aim at extracting some parameters such as height, ratios of the pulses at certain pressure levels, which are obtained from the cuff pressure. These parameters can be used to form profiles to attribute to blood pressures. Our algorithms are based on fuzzy sets, whose membership functions are determined by using neural networks. We further employ Gram-Schmidt orthogonal transformation to select appropriate features for classification. The effectiveness of neural network solution to systolic blood pressure classification is the focus of this paper.",
Memoization and DPLL: formula caching proof systems,"A fruitful connection between algorithm design and proof complexity is the formalization of the DPLL approach to satisfiability testing in terms of tree-like resolution proofs. We consider extensions of the DPLL approach that add some version of memoization, remembering formulas the algorithm has previously shown unsatisfiable. Various versions of such formula caching algorithms have been suggested for satisfiability and stochastic satisfiability (S. M. Majercik et al., 1998; F. Bacchus et al., 2003). We formalize this method, and characterize the strength of various versions in terms of proof systems. These proof systems seem to be both new and simple, and have a rich structure. We compare their strength to several studied proof systems: tree-like resolution, regular resolution, general resolution, and Res(k). We give both simulations and separations.","Computer science,
Design engineering,
Inference algorithms,
Drives,
Algorithm design and analysis,
Stochastic processes,
Testing,
Computational complexity,
Runtime,
Bayesian methods"
Modelling TCP Reno with spurious timeouts in wireless mobile environments,"TCP has been found to perform poorly in the presence of spurious timeouts (ST) caused by delay spikes which are especially more frequent in today's wireless mobile networks than in traditional wired network. Because STs are generally considered to represent a transient state in wired networks, previous research did not consider the effects of ST on the steady state performance of TCP. In this paper, we propose an analytical model for the sending rate and throughput of TCP Reno as a function of packet error rate and characteristics of spurious timeouts. The proposed model has been validated against simulation results and has been found to be more accurate than previous models in the presence of spurious timeouts.",
Converse coding theorem for identification via general degraded broadcast channels,,
Formation constrained multi-robot system in unknown environments,"This paper explores the application of the behavior-based approach to path planning for multiple mobile robots performing a formation control task in unknown environments. To predict the positions of moving obstacles for the purpose of collision avoidance, parabola prediction model whose parameters are estimated by the recurrence least square algorithm with restricted scale is adopted. Then, on the basis of the task and environment, we adopt five primitive behaviors and design a series of generation functions to generate control parameters for behaviors' combination. Furthermore, as the outputs of these functions can be adjusted according to the current situation, thus robots can achieve a motion strategy by reasonably combining behaviors and the adaptability to the environment is improved. We illustrate the validity of the approach by the simulations.","Multirobot systems,
Control systems,
Collision avoidance,
Robot control,
Orbital robotics,
Computer science,
Application software,
Path planning,
Mobile robots,
Predictive models"
Fuzzy data domain description using support vector machines,"In this paper, we reformulate the use of a data domain description method, inspired by the fuzzy support vector machine (SVM) by Lin, called the fuzzy data domain description. This data description is suitable for applications in which each input point may not be fully assigned to one class. In this method, different input data can make different contributions to the domain description.","Support vector machines,
Virtual colonoscopy,
Machine learning,
Fuzzy systems,
Training data,
Mathematics,
Computer science,
Application software,
Upper bound,
Error analysis"
On the supervision and assessment of part-time postgraduate software engineering projects,"This paper describes existing practices in the supervision and assessment of projects undertaken by part-time, postgraduate students in Software Engineering. It considers this aspect of the learning experience, and the educational issues raised, in the context of existing literature-much of which is focussed upon the experience of full-time, undergraduate students. The importance of these issues will increase with the popularity of part-time study at a postgraduate level; the paper presents a set of guidelines for project supervision and assessment.","Software engineering,
Guidelines,
Computer science,
International collaboration,
Computer science education,
Computer industry,
Intellectual property,
Reflection,
Heart,
Educational programs"
Portable gamma-ray monitor composed of a compact electrically cooled Ge detector and a mini-MCA system,"A portable gamma-ray monitor composed of a compact electrically cooled germanium detector and a mini-MCA system was developed for in situ gamma-ray monitoring with very fast starting time and easy operation. This detector is suitable for an accident, an emergency, a fast inspection, and an easy analysis since the detector could acquire data after a 51 min cooling time. The size of Ge detector cryostat was 10.2 cm/spl phi//spl times/39.3 cm and the weight 5.2 kg. A small Stirling refrigerator could cool down a 14 cm/sup 3/ planar-type high purity Ge detector with the relative detection efficiency of 1% automatically with low power driving electronics. After cooling down, gamma-ray data were acquired and analyzed repeatedly by a mini-MCA system interfaced to a very small hand held computer. The energy resolution of the detector was 2.46 keV at an energy of 1.33 MeV. Consequently, a user can comfortably analyze a gamma-ray spectrum with radionuclide identification and quantitative analysis by watching the display on the hand held computer.","Monitoring,
Gamma ray detection,
Gamma ray detectors,
Electronics cooling,
Computer displays,
Germanium,
Accidents,
Inspection,
Refrigeration,
Computer interfaces"
Eliminating redundancies in SAT search trees,"Conflict analysis is a powerful paradigm of backtrack search algorithms, in particular for solving satisfiability problems arising from practical applications. Accordingly, most recent Boolean satisfiability solvers implement forms of conflict analysis, at least to some extent. In this paper, a branching criterion initially introduced by Purdom is revisited and extended. Contrary to the author's a priori analysis, it is shown very efficient from a practical point of view in that it allows search trees in SAT solving to be pruned in a significant way while obeying an interesting time and space trade-off. More precisely, we show that redundancies during the search process can be avoided without adding new constraints explicitly. Moreover, the technique can be used not only to prune branches in the search tree, but also to derive implied literals. Extensive experimental results illustrate the feasibility and practical interest of this approach.","Algorithm design and analysis,
Artificial intelligence,
Lenses,
NP-complete problem,
Computer science,
Very large scale integration,
Buildings,
Surges,
Data structures,
Information analysis"
A framework for generic inter-application interaction for 3D AR environments,"We present a generic software framework enabling inter-application interaction in a 3D augmented reality environment. Our framework is built within a 3D AR window manager centered around ARToolkit. The user interface presents users with a simple visual mechanism to establish communications among applications in a generic way. The application interface is designed for ease of development and maximum end-user flexibility. We showcase some novel 3D applications and their interactions within the framework, demonstrating the new possibilities created by 3D interfaces. A classification is presented to categorize the applications' roles in such interactions, to help steer development. Finally, we discuss future possibilities for applications and interactions within such a framework.","Utility programs,
Application software,
Computer science,
Augmented reality,
Computer interfaces,
User interfaces,
Computer graphics,
Rendering (computer graphics),
Data visualization,
Operating systems"
A hierarchical approach to fuzzy segmentation of colour images,"In this paper we introduce a methodology for the segmentation of colour images by means of a nested hierarchy of fuzzy partitions. Colour image segmentation attempts to divide the pixels of an image in several homogeneously-coloured and topologically connected groups, called regions. Our methodology deals with the different (but related) aspects of imprecision that are present in this process. First, the concept of homogeneity in a colour space is imprecise, so a measure of distance/similarity between colours is introduced. As a direct consequence, boundaries between regions are imprecise in general, so it is convenient to define regions as fuzzy subsets of items. The proposed distance in a perceptual colour space is employed to calculate fuzzy regions and membership degrees. In addition, fuzzy segmentation can be different depending on the precision level we consider when looking for homogeneity. Starting from an initial fuzzy segmentation, a hierarchical approach, based on a similarity relation between regions, is employed to obtain a nested hierarchy of regions at different precision levels.","Image segmentation,
Pixel,
Image color analysis,
Computer science,
Artificial intelligence,
Extraterrestrial measurements,
Image processing,
Histograms,
Image analysis,
Algorithm design and analysis"
A multiprocessor real-time process scheduling method,"Multimedia systems like video-on-demand systems require a good scheduling method to improve their services because of their real-time requirements. If such systems consist of multiple processors, then the scheduling problem becomes much important. Scheduling is an important problem for both computer science and operation research. It is proved that the complexity for scheduling problems is NP-complete and sometimes NP-hard depending on the constraints of the problems, implying the difficulties for finding a good scheduling approach. We propose a method for multiprocessor real-time scheduling algorithm applicable for both computer science and operation research. Our method is general enough to solve different scheduling problems such as wafer lot dispatching and scheduling for behaviors of a robot soccer player. There are scheduling problems exist in multimedia systems with real-time constraints, although the scheduling problems for multimedia systems have some unique characteristics differ from process scheduling, we believe the generality nature of our method facilitates the possibility of our scheduling method to be helpful for multimedia systems to solve scheduling problems after some minor modifications.",
A high-throughput whole-body PET scanner using flat panel PS-PMTs,"A new positron emission tomography (PET) scanner for whole-body studies has been developed. The scanner has 720 block detectors, each of which consists of a flat panel position-sensitive photomultiplier and a 16/spl times/8 BGO crystal array. The detector system is composed of 12 layers of block detector rings stacked axially, and each ring consists of a circular array of 60 block detectors. Since each block detector ring contains 8 crystal rings, the total number of detector rings is 96 and the axial field of view (FOV) is 685 mm, which is sufficient to measure a whole-body with two steps of scan. The detector ring diameter is 1,020 mm and the transaxial FOV is 600 mm in diameter. Coarse slice septa are placed between the block detector rings, which are effective to reduce scattered coincidence events while keeping a high detection sensitivity. Parallel 16 personal computers are used for the data acquisition and processing to deal with a large amount of event data. The acquired data in 3D manner are converted to 2D data set with Fourier rebinning algorithm and reconstructed with a fast iterative algorithm ""DRAMA"". The reconstructed images for the whole body are obtained within 5 min after the scan. By using the coarse septa, the scatter fraction measured with NEMA NU2-2001 standard was 31.4% whose value was significantly lower than that of 3D PET without septa. The system sensitivity was 9.72 cps/kBq, and the peak counts of the noise equivalent count rate used with direct random-subtraction was 113.6 kcps at 10.5 kBq/ml.","Whole-body PET,
Sensor arrays,
Event detection,
Scattering,
Iterative algorithms,
Image reconstruction,
Positron emission tomography,
Position sensitive particle detectors,
Photomultipliers,
Microcomputers"
Mission impossible? Automatically assembling agents from high-level task descriptions,"In this paper, we present our notion of automatically assembling agents on-demand given high-level task descriptions. Our approach is based on the concept of the mission, which is represented by a hierarchical structure called the task decomposition diagram (TDD). Tasks in TDD are specified using a task specification language, which defines the functionality and possible implementation of those tasks. We discuss several strategies for executing the TDD, for assembling agents based on the TDD, and present the architecture of our prototype system called eHermes.","Software agents,
Application software,
Prototypes,
Multiagent systems,
Runtime,
Software engineering,
Specification languages,
Assembly systems,
Software prototyping,
Computer science"
Model-checking trace event structures,"Given regular collection of Mazurkiewicz traces, which can be seen as the behaviors of a finite-state concurrent system, one can associate with it a canonical regular event structure. This event structure is a single (often infinite) structure that captures both the concurrency and conflict information present in the system. We study the problem of model-checking such structures against logics such as first-order logic (FOL), monadic second-order logic (MSOL) and a new logic that lies in between these two called monadic trace logic (MTL). MTL is a fragment of MSOL where the quantification is restricted to sets that are conflict-free. While it is known that model-checking such event structures against MSOL is undecidable, our main results are that FOL and MTL admit effective model-checking procedures. It turns out that FOL captures previously known decidable temporal logics on event structures. MTL is more powerful and can express interesting branching-time properties of event structures, and when restricted to a sequential setting, can express the standard logic CTL over trees.",
A low latency handoff scheme using positional information for mobile IP based networks,"In order to realize low handoff latency without increasing overhead in wired channel, we propose a handoff scheme using positional information for mobile IP based networks. In the proposed scheme, foreign agents (FA) exchange IP addresses and positional information. When a distance between FA and mobile host (MH) exceeds a threshold, FA copies packets and forwards them only to MH's next FA. Moreover, in order for MH to receive first advertisement (ADV) message earlier, the transmission interval of ADV message is shortened in next FA. By shortening transmission interval of ADV message, MH can receive forwarded packets earlier. In addition, when new FA receives registration request (REQ) message from MH, FA replies tentative registration (TREG) message to MH. Adding tentative registration, MH can begin receiving forwarded packets without waiting for receiving registration reply (REP) message from home agent (HA). By performance evaluation using both theoretical analysis and computer simulations, we show that the proposed scheme can realize low handoff latency without increasing overhead in both wired and wireless channel.","Delay,
Birth disorders,
Global Positioning System,
Mobile computing,
Internet,
Doped fiber amplifiers,
Computer science,
Performance analysis,
Computer simulation,
Protocols"
Uses of multiagents systems for simulation of MAPK pathway,"Since emergence of molecular biology, one has improved knowledge about intracellular networks controlling cell behavior. In parallel, advances in mathematic and computer science allow one to simulate such complex phenomena. Moreover, most methods need a global resolution of the system which makes it difficult to be created and modified. We proposed, in this study, a distributed approach by multiagent system (MAS), to simulate the MAPK pathway. Our results show that such simulation is possible and allows ""in virtuo"" experimentation, i.e. model perturbation during its execution.","Multiagent systems,
Bioinformatics,
Biomedical engineering"
Optimal opportunistic scheduling in wireless networks,"Scheduling has been extensively studied in various disciplines in operations research and wireline networking. However, the unique characteristics of wireless communication systems - namely, time-varying channel conditions and multiuser diversity - means that new scheduling solutions need to be developed that are specifically tailored for this environment. In this paper, we summarize various opportunistic scheduling schemes that exploit the time-varying nature of the radio environment to improve the spectrum efficiency while maintaining a certain level of satisfaction for each user. We also discuss the advantages and costs associated with opportunistic scheduling, and identify possible future research directions.","Intelligent networks,
Wireless networks,
Throughput,
Scheduling algorithm,
Processor scheduling,
Computer networks,
Wireless communication,
Delay,
Computer science,
Operations research"
Time-varying modifying factor partly continuous fuzzy controller,"By continuing the control surfaces of the time-varying modifying factor analytic expression method, we construct a new algorithm to improve the properties of the fuzzy controller obviously. The algorithm has two merits: (a) Static error is erased. (b) Its program is very simple. Therefore, its operation speed is fast. The excellent quality control and rapid response real time system can be realized easily only by cheap general digital chip-computer. The paper presents theoretic proof, Simulation results illustrate the effectiveness of the approach.","Fuzzy control,
Fuzzy systems,
Control systems,
Computer science,
Algorithm design and analysis,
Quality control,
Real time systems,
Fuzzy reasoning,
Error correction,
Fuzzy sets"
Direct interesting rule generation,"An association rule generation algorithm usually generates too many rules including a lot of uninteresting ones. Many interestingness criteria are proposed to prune those uninteresting rules. However, they work in post-pruning process and hence do not improve the rule generation efficiency. We discuss properties of informative rule set and conclude that the informative rule set includes all interesting rules measured by many commonly used interestingness criteria, and that rules excluded by the informative rule set are forwardly prunable, i.e. they can be removed in the rule generation process instead of post pruning. Based on these properties, we propose a direct interesting rule generation algorithm, DIG, to directly generate interesting rules defined by any of 12 interestingness criteria. We further show experimentally that DIG is faster and uses less memory than Apriori.","Association rules,
Itemsets,
Mathematics,
Data mining,
Computer science"
Reduced-order controllers for the discrete-time H/sub /spl infin// control problem with unstable invariant zeros,"This paper addresses the existence and design methods of reduced-order controllers for the discrete time H/sub /spl infin// control problem with unstable invariant zeros in the state-space realization of the transfer function matrix from the control input to the controlled error or from the exogenous input to the observation output, where the realization is induced from a stabilizable and detectable realization of the generalized plant. This paper presents new controller degree bounds for the H/sub /spl infin// control problem and provides new solutions to designing of reduced-order controllers for the H/sub /spl infin// control problem with invariant zeros located outside of the open unit disk. When such unstable invariant zero exists (even it is a zero on the unit circle), this paper shows that reduced-order controllers with strictly orders less than the generalized plant order exist if the H/sub /spl infin// control problem is solvable. Two LMI based design methods for constructing the reduced-order controllers are given. Furthermore, this paper shows that the results obtained here unify previous special results of reduced-order controllers for the H/sub /spl infin// control problems with infinite zeros, or unstable real transmission zeros, etc.",
Experiments on the automated selection of patients for clinical trials,"When clinicians test a new treatment procedure, they need to identify and recruit patients with appropriate medical conditions. We have developed an expert system that helps clinicians select patients for experimental treatments, and to reduce the number and overall cost of related medical tests. We describe experiments on selecting patients for new treatments at the Moffitt Cancer Center. The experiments have shown that the system can increase the number of selected patients by a factor of three, and that it can also reduce the cost of the selection process.","Clinical trials,
Costs,
Medical tests,
Medical treatment,
Acquired immune deficiency syndrome,
Medical expert systems,
Recruitment,
Breast cancer,
Knowledge based systems,
Computer science"
Reconfigurable architecture requirements for co-designed virtual machines,"This paper addresses the physical hardware requirements necessary for a co-design hardware/software virtual machine to not only exist, but to also provide comparable performance with other implementation techniques for virtual machines. The discussion centers on requirements of the reconfigurable device and it's peripheral connections to main memory and the general-purpose processor.",
Publishing and querying the histories of archived relational databases in XML,"There is much current interest in publishing and viewing databases as XML documents. The general benefits of this approach follow from the popularity of XML and the tool set available for visualizing and processing information encoded in this universal standard. In this paper, we explore the additional and unique benefits achieved by this approach on temporal database applications. We show that XML views combined with XQuery can provide surprisingly effective solutions to the problem of supporting historical queries on past content of database relations and their evolution. Indeed, using XML, the histories of database relations can be naturally represented by temporally grouped data models. Thus, we identify mappings from relations to XML that are most conducive to modeling and querying database histories, and show that temporal queries that would be very difficult to express in SQL can be easily expressed in standard XQuery. Then, we turn to the problem of supporting efficiently the storage and the querying of relational table histories. We present an experimental study of the pros and cons of using native XML databases, versus using traditional databases, where the XML-represented histories are supported as views on the historical tables.","Publishing,
History,
Relational databases,
XML,
Visual databases,
Information systems,
Object oriented databases,
Data models,
Database systems,
Computer science"
Efficient LUT-based FPGA technology mapping for power minimization,"We study the technology mapping problem for LUT-based FPGAs, targeting at power minimization. The problem has been proved to be NP-hard previously. Hence, we present an efficient heuristic to compute low-power mapping solutions. The major distinction of our work from previous ones is that while generating a LUT, we look ahead at the impact of the mapping selection of this LUT on the power consumption of the remaining network. We choose the mapping that results in the least estimated overall power consumption. The key idea is to compute low-power K-feasible cuts by an efficient incremental network flow computation method. Experimental results show that our algorithm reduces both power consumption and area over the previous algorithms reported in the literature.",
Modeling the DOCSIS 1.1/2.0 MAC protocol,"A model of the data over cable (DOCSIS) version 1.1/2.0 MAC and physical layers using the ns simulation package is developed. In this paper we present the results of a performance analysis that we have conducted using the model. The main objective of our study is to examine the performance impact of several key MAC layer system parameters as traffic loads are varied. We focus on the DOCSIS best effort service. We measure the level of ACK compression experienced by a downstream TCP connection and show that even under moderate load levels DOCSIS can cause TCP acknowledgement packets to compress in the upstream direction leading to bursty (and lossy) downstream dynamics. We explore the use of downstream rate control on network performance and find that it does reduce the level of ACK compression for moderate load levels compared to an equivalent network scenario without rate control. However, as the number of active subscribers on a given channel increase, the level of ACK compression grows implying that other mechanisms should be looked at to improve performance during periods of high usage.","Media Access Protocol,
Collision mitigation,
Radio frequency,
Bit rate,
Channel allocation,
Computer science,
Communication cables,
Physical layer,
Internet,
Streaming media"
Auctions with untrustworthy bidders,"The paper analyzes auctions which are not completely enforceable. In such auctions, the winning bidders may fail to carry out their obligations, and parties involved cannot rely on external enforcement or control mechanisms for backing up a transaction. We propose two mechanisms that make bidders directly or indirectly reveal their trustworthiness. The first mechanism is based on discriminating bidding rules that separate trustworthy from untrustworthy bidders. That is, the auctioneer offers two types of auction rules which are designed so that all trustworthy bidders choose one bidding rule, while untrustworthy bidders choose another. This allows the auctioneer to discover trustworthy bidders and to transact with them. The second mechanism is a generalization of the Vickrey auction to the case of untrustworthy bidders. We prove that, if the winner is considered to have the trustworthiness of the second-highest bidder, truthfully declaring one's trustworthiness becomes a dominant strategy. We expect the proposed mechanisms to reduce the cost of trust management and to help agent designers avoid many market failures caused by lack of trust.",
A tutor for counter-controlled loop concepts and its evaluation,"We have developed a Web-based tutor for teaching and testing counter-controlled loop concepts in C++. The tutor is designed to promote problem-based learning. It repeatedly generates problems, grades user's answers and provides feedback about the correct answers. This paper describes the design of the tutor, outlines a test that we used to evaluate its effectiveness, and presents the results of die test. The test confirmed our hypothesis that using the tutor helps improve student learning. The improvement is statistically significant. This tutor can be used for practice or testing in Computer Science I.","Testing,
Computer science,
Programming profession,
Computer science education,
Educational programs,
Computer languages,
Statistics,
Feedback loop,
Educational institutions"
"Notice of Correction

Task modeling and specification for modular sensory based human-machine cooperative systems","This paper is directed towards developing human-machine cooperative systems (HCMS) for augmented surgical manipulation tasks. These tasks are commonly repetitive, sequential, and consist of simple steps. The transitions between these steps can be driven either by the surgeon's input or sensory information. Consequently, complex tasks can be effectively modeled using a set of basic primitives, where each primitive defines some basic type of motion (e.g. translational motion along a line, rotation about an axis, etc.). These steps can be ""open-loop"" (simply complying to user's demands) or ""closed-loop, in which case external sensing is used to define a nominal reference trajectory. The particular research problem considered here is the development of a system that supports simple design of complex surgical procedures from a set of basic control primitives. The three system levels considered are: i) task graph generation which allows the user to easily design or model a task, ii) task graph execution which executes the task graph, and iii) at the lowest level, the specification of primitives which allows the user to easily specify new types of primitive motions. The system has been developed and validated using the JHU Steady Hand Robot as an experimental platform.",
A worst-case model for co-channel interference in the Bluetooth wireless system,"This paper presents a model for the expected throughput in a Bluetooth network in presence of a number of other Bluetooth networks that cause radio interference. The analysis considers unidirectional traffic and makes worst-case assumptions on the effect of radio interference. In order to verify the model, simulation results are presented. The results from this work may be very useful when designing ad-hoc networking functionality for future versions of Bluetooth.",
Handwritten Hangul character recognition with hierarchical stochastic character representation,"In structural character recognition, a character is usually viewed as a set of strokes and the spatial relationships between them. In this paper, we propose a stochastic modeling scheme by which strokes as well as relationships are represented by utilizing the hierarchical characteristics of target characters. Based on the proposed scheme, a handwritten Hangul (Korean) character recognition system is developed. The effectiveness of the proposed scheme is shown through experimental results conducted on a public database.","Character recognition,
Stochastic processes,
Space technology,
Statistical analysis,
Computer science,
Databases,
Optical character recognition software,
Writing,
Shape,
Pattern analysis"
A global description of medical imaging with high precision,"This paper explores our solution aiming to provide efficient retrieval of medical imaging. Depending on the user, the same image can be described through different views. In essence, an image can be described on the basis of either low-level properties, such as texture or color; contextual data, such as date of acquisition or author; or semantic content, such as real-world objects and relations. Our approach consists of providing a multispaced description model capable of integrating different facets (or views) of the medical image. Visual retrieval solutions are recommended and are the most appropriated for noncomputer-science users. However, current visual languages suffer from several problems, especially ambiguities generated by the user and/or the system, and imprecision at different levels of image description. In this paper, we expose our solution and demonstrate how spatial precision of medical image content and ambiguities can be resolved. An implementation called Medical Image Management System (MIMS) has been realized to prove our proposition. A set of tests has been deployed to validate our prototype.","Biomedical imaging,
Image retrieval,
Spatial resolution,
Image resolution,
Prototypes,
Picture archiving and communication systems,
Shape,
Brightness,
Computer science,
Image storage"
An XVCL approach to handling variants: a KWIC product line example,"We developed XVCL (XML-based variant configuration language), a method and tool for product lines, to facilitate handling variants in reusable software assets (such as architecture, code components or UML models). XVCL is a newer version of Bassett's frames (1997), a technology that has achieved substantial productivity improvements in large data processing product lines written in COBOL. Despite its simplicity, XVCL can effectively manage a wide range of product line variants from a compact base of meta-components, structured for effective reuse. We applied XVCL in two medium-size product line projects and a number of smaller case studies. We communicate XVCL's capabilities to support product lines by means of a simple, but still interesting, example of the KWIC system introduced by Parnas in 1970's. We show how we can handle functional variants, variant design decisions and implementation-level variants in a generic KWIC system.","Productivity,
Software reusability,
Software tools,
Computer architecture,
Unified modeling language,
Data processing,
Software quality,
Application software,
Java,
Computer science"
A robust statistic method for classifying color polarity of video text,"Video text extraction and recognition are prerequisite tasks for video indexing and retrieval. Color polarity classification of video text is very important to these tasks. Most existing text extraction methods assume that the text color is always light (or dark). Obviously, this assumption restricts the application of these methods to some specific domains. Only a few methods can detect the color polarity in the condition that the background is clear. However, many real video texts have various appearances and complex backgrounds that existing methods cannot handle. The paper proposes a statistical color polarity classification method that is robust to various background complexities, font styles, stroke widths, and languages. We discover the intrinsic relationships between text edges and background edges, and then develop an efficient measurement to detect the color polarity. The experimental results show that the proposed method achieves a much higher accuracy, 98.5%, than existing methods.","Robustness,
Statistics,
Image edge detection,
Image color analysis,
Text recognition,
Indexing,
Image analysis,
Computer science,
Performance analysis,
Filters"
Inspections and historical data in teaching software engineering project course,"Software engineering project course document inspection data have been collected from meeting minutes. The derived statistics are used in inspections, in project management lectures, and also in software engineering training outside university. The use of historical project data, such as statistics on working hours, and comments from participants in previous courses, have been considered very valuable in teaching, by both the students and course personnel. These are the two specialities that have been used for five years at the Institute of Software Systems. This paper gives some hints to project course personnel, for what kind of useful data could be collected easily.","Inspection,
Education,
Software engineering,
Statistics,
Personnel,
Software systems,
Project management,
Information technology,
Minutes,
Management training"
Performance evaluation of predictive handoff scheme with channel borrowing,"We report a study on a predictive handoff scheme in cellular networks. We present a new channel resource allocation strategy, which combines predictive channel reservation based on real-time positioning of mobile users and dynamic channel borrowing among cells in time of congestion, to provide services to handoff requests with higher priority over new call requests. We also propose a different criterion, threshold time, to improve the accuracy of channel reservation by better determining the right time to send a reservation request, if necessary, for a potential handoff call. Our simulation results indicate that the new scheme outperforms other handoff schemes including guard channel, the original predictive channel reservation, and channel borrowing by significantly reducing the handoff blocking rates with little impact on the throughput of cellular networks. In addition, our data suggest that initiating reservation requests based on an appropriate threshold time introduces fewer false reservations than initiating reservation requests based on a threshold distance.","Land mobile radio cellular systems,
Throughput,
Quality of service,
Protocols,
Telecommunication traffic,
Computer science,
Resource management,
Predictive models,
Channel allocation,
Fluctuations"
Extensible point location algorithm,We present a general walkthrough point location algorithm for use with general polyhedron lattices and polygonal meshes assuming the usage of nothing more than a simple linked list as a data structure to store the polyhedra. The generality of the approach stems from using barycentric coordinates to extract local information about the location of the query point that allows a 'gradient descent'-like walk toward the goal.,
Taking the twinkle out of starlight,"The article discusses the technology called adaptive optics used in improving telescopes, enabling astronomers to capture clear, sharp images of the galaxies, stars, and planets that populate the universe. The technique brings together the latest in computers, materials science, electronic detectors, and digital control in a system that warps and bends a mirror in the telescope to counteract, in real time, the atmospheric distortion. An adaptive optics system installed on the 6.5-meter-diameter telescope, called the MMT telescope, on Mt. Hopkins, south of Tucson, Arizona, takes the technology a step further. The ways by which adaptive optics overcomes the deficiencies of conventional telescopes with the design and structure of the adaptive secondary mirror and the use of nulling interferometry are discussed.","Telescopes,
Adaptive optics,
Space technology,
Mirrors,
Planets,
Materials science and technology,
Detectors,
Digital control,
Real time systems,
Optical distortion"
Image processing and analysis at ipag,,"Image processing,
Image analysis,
Radiology,
Biomedical imaging,
Image motion analysis,
Computed tomography,
Medical diagnostic imaging,
Nuclear medicine,
Art,
Biomedical image processing"
High speed FPGA implementation of RSA encryption algorithm,"In this paper, new structures that implement RSA cryptographic algorithm are presented. These structures are built using a modified Montgomery modular multiplier, where the operations of multiplication and modular reductions are carried out in parallel rather than interleaved as in the traditional Montgomery multiplier. The global broadcast data lines are avoided by interleaving two operations into the same structure, thus making the implementation systolic. The results of implementation in FPGA have shown that the proposed RSA structures outperformed those structures built around a traditional Montgomery multiplier in terms of speed. In terms of area usage, an area-efficient architecture is shown in this paper that has the merit of having a high speed and a reduced area usage when compared with other architectures.","Field programmable gate arrays,
Public key cryptography,
Interleaved codes,
Broadcasting,
Computer architecture,
Informatics,
Computer science,
Security,
Electronic commerce,
Safety"
A fixed time-step approach for multibody dynamics with contact and friction,"We present a fixed time-step algorithm for the simulation of multi-rigid-body dynamics with joints, contact, collision, and friction. The method solves a linear complementarity problem (LCP) at each step. We show that the algorithm can be obtained as the stiff limit of fixed time-step schemes applied to regularized contact models. We do not perform collision detection. Instead, a noninterpenetration constraint is replaced by its linearization, which, together with a judicious choice of the active constraints, guarantees geometrical constraint stabilization without the need to perform a reduction of the time step to detect new collision or stick-slip transition events. Partially elastic collisions are accommodated by a suitable modification of the free term of the LCP.","Friction,
Computational modeling,
Acceleration,
Mathematics,
Computer science,
Computer simulation,
Event detection,
Virtual reality,
Robots,
Differential algebraic equations"
Camera calibration of long image sequences with the presence of occlusions,"Camera calibration is a critical problem in applications such as augmented reality and image based model reconstruction. When constructing a 3D model of an object from an uncalibrated video sequence, large amounts of frames and self occlusions of parts of the object are common and difficult problems. In this paper we present a fast and robust algorithm that uses a divide and conquer strategy to split the video sequence into sub-sequences containing only the most relevant frames. Then a robust stratified linear based algorithm is able to calibrate each of the subsequences to a metric structure and finally the subsequences are merged together and a final nonlinear optimization refines the solution. Examples of real data reconstructions are presented.","Cameras,
Calibration,
Image sequences,
Rendering (computer graphics),
Video sequences,
Layout,
Image reconstruction,
Computer science,
Robustness,
Computational modeling"
Low cost instruction cache designs for tag comparison elimination,"Tag comparison elimination (TCE) is an effective approach to reduce I-cache energy. Current research focuses on finding good tradeoffs between hardware cost and percentage of comparisons that can be removed. For this purpose, two low cost innovations are proposed in this paper. We design a small dedicated TCE table whose size is flexible both horizontally (entry size) and vertically (number of entries). The design also minimizes interactions with the I-cache. For a 64-way 16K cache, the new design reduces the tag comparisons to 4.0% with a fraction only 20% of the hardware cost of the way memoization technique. The result is 40% better compared to a recent proposed low cost design of comparable hardware cost.","Costs,
Technical Activities Guide -TAG,
Hardware,
Computer science,
Energy consumption,
Permission,
Switches,
Power engineering and energy,
Technological innovation,
Modems"
Programming systems for autonomy,"This paper describes a new approach to programming autonomic systems. Autonomic functions are integrated into element objects at design time using a special language called JSpoon. JSpoon extends element classes with management attributes representing configuration, performance, status and fault information. The JSpoon compiler generates respective code and interfaces to instrument the data in a common modeler repository, provided by NESTOR (Yemini at al., 2000) JSpoon programs access and manipulate management data without distinction between ""agent"" and ""manager"" roles. JSpoon further supports integration of plug-in knowledge modules that can interpret and control element operations. These knowledge modules are used to incorporate autonomic operations with elements. This design-time approach offers several substantive advantages over current alternatives. Management is integrated with the element development life-cycle. Instrumentation is compiler-generated and may be flexibly designed by element developers, while being consolidated into a unified global management data model. Knowledge modules can be seamlessly integrated with third party elements augmenting these elements with the logic for autonomic behavior.","Instruments,
Data models,
Knowledge management,
Runtime environment,
Information management,
Computer architecture,
Conferences,
Computer science,
Logic,
Costs"
Motion trajectory based video authentication,"With the relentless digitization of commercial media, it is increasingly susceptible to the violation of its intellectual property rights. We address the issue of authenticating digital video in this paper by presenting a novel technique based on motion trajectory and cryptographic secret sharing. Given a video, we segment it into shots and select the key frames in a shot based on the motion trajectory. We then consider these key frames as shares of a secret-sharing scheme for color images. We then reconstruct the secret corresponding to these shares. This secret frame is deemed as the secret key of this shot. Utilizing the secret keys of all the shots, we hierarchically construct the master key for the entire video by recursively employing the same method of color image sharing. Any modifications in a shot or in the important content of a shot will be reflected as changes in the computed master key. Thus, any video can be authenticated by comparing its computed master key with the original master key. The advantages of the proposed technique are that we take the correlation among the video frames into account, all authentication data is encapsulated within the master key, and this master key authentication information need not be embedded into the video. Thus the video quality is not affected by the auxiliary authentication information.","Authentication,
Video sharing,
Color,
Cryptography,
Monitoring,
Polynomials,
Computer science,
Intellectual property,
Image segmentation,
Image reconstruction"
Design of an artificial immune system as a novel anomaly detector for combating financial fraud in the retail sector,"The retail sector often does not possess sufficient knowledge about potential or actual frauds. This requires the retail sector to employ an anomaly detection approach to fraud detection. To detect anomalies in retail transactions, the fraud detection system introduced in this work implements various salient features of the human immune system. This novel artificial immune system, called CIFD (computer immune system for fraud detection), adopts both negative selection and positive selection to generate artificial immune cells. CIFD also employs an analogy of the self-major histocompatability complex (MHC) molecules when antigen data is presented to the system. These novel mechanisms are expected to improve the scalability of CIFD, which is designed to process gigabytes or more of transaction data per day. In addition, CIFD incorporates other prominent features of the HIS such as clonal selection and memory cells, which allow CIFD to behave adaptively as transaction patterns change.","Artificial immune systems,
Detectors,
Immune system,
Business,
Scalability,
Humans,
Computer crime,
Computer science,
Educational institutions,
Process design"
A fuzzy logic CBIR system,"A fuzzy logic framework is proposed to alleviate two problems in traditional CBIR systems, including the semantic gap and the perception subjectivity. The proposed framework consists of two major parts, including (1) model construction and (2) query comparison. In the model construction part, fuzzy linguistic terms with associated fuzzy membership functions are automatically generated through an unsupervised fuzzy clustering algorithm. The linguistic terms provide a nature way of expressing user's concepts, and the membership functions characterize the mapping between image features and human visual concepts. We also define the syntax and semantics rules of a query description language to unify the query expression of textual descriptions, visual examples, and relevance feedbacks. In the query comparison part, a similarity function is inferred based on user's feedbacks to measure the similarity between the query and each image in the database. The user's preference is also captured and retained in his/her own profile to achieve personalization. Our work provides a unified and comprehensive framework for incorporation a fuzzy approach into CBIR systems. To verify our CBIR framework, we select Tamura features to describe and retrieve texture images. Experimental results show that the proposed framework is indeed effective to alleviate the semantic gap and the perception subjectivity problems.",
A reengineering process for migrating from an object-oriented legacy system to a component-based system,"Computing environments are evolving from mainframe systems to distributed systems. Stand-alone programs that have been developed using object-oriented technology are not suitable for these new environments. However, programs that have been developed using a component-based technology have proven to be more suitable for the new environments due to their granularity and reusability. In this paper, we present a reengineering process for migrating from an object-oriented legacy system to a component-based system. This process consists of two steps: first, to create basic components with the relationship of their constituent classes, second, to refine components using the metrics we propose.","Application software,
Computer science,
Distributed computing,
Internet,
Programming,
Assembly,
Software systems,
Software maintenance,
Reverse engineering,
Wrapping"
A fuzzy logic evaluating system to support Web-based collaboration using collaborative and metacognitive data,"A fuzzy logic-based expert system, namely collaboration/reflection-fuzzy inference system (C/R-FIS,) is presented. By means of interconnected fuzzy inference systems (FIS), it automatically evaluates the collaborative activity, during asynchronous, written, Web-based collaboration. This information is used for the provision of enhanced support during the collaboration. The proposed model extents the evaluation system of a Web-based collaborative tool namely Lin2k, which served as a test-bed for the C/R-FIS experimental use. The results proved the potentiality of the proposed model to significantly contribute to the enhancement of the collaborative activity.","Collaboration,
Fuzzy logic,
Collaborative work,
Fuzzy systems,
Nerve fibers,
Expert systems,
Collaborative tools,
Automation,
Employment,
Computer science education"
Issue tracking,"All programming projects have one thing in common: there is always more to do. Some things that need doing are bug fixes; others are enhancements such as cleaning up and refactorlng existing code, adding tests, and writing documentation. but before your office wall becomes a collage of sticky-note reminders, you might want to try an issue tracker instead. In this article, we describe two open-source issue-tracking software packages: Roundup, an implementation of the winning design in Los Alamos National Laboratory's Software Carpentry contest and Bugzilla (from the GNU project).","Software packages,
Client-server systems"
PPM model cleaning,"The prediction by partial matching (PPM) algorithm uses a cumulative frequency count of input symbols in different contexts to estimate its probability distribution. Compression ratios yielded by the PPM algorithm have not instigated broader use of this scheme mainly because of its high demand for computational resources. An algorithm that improves the memory usage by the PPM model is presented. The algorithm identifies and removes portions of the PPM model, which are not contributing toward better modeling of the input data. As a result, our algorithm improves the average compression ratio up to 7% under the memory limitation constraint at the expense of increased computation. Under the constraint of maintaining the same level of compression ratios, the algorithm reduces the memory usage up to 70%.","Cleaning,
Frequency estimation,
Probability distribution,
Context modeling,
Data compression,
Arithmetic,
Computer science,
Heuristic algorithms,
Memory management,
Compressors"
Evaluating support for improvisation in simulated emergency scenarios,"Technological systems involving hazards are typically managed by experienced personnel guided by well-formulated, pre-determined procedures. These procedures are designed to ensure that operations proceed in a safe and cost-effective manner. Yet normal operations in these systems are exposed to unexpected contingencies that can require personnel to develop and deploy new procedures in real-time. Creative thinking in such situations is therefore necessary in order to prevent degradation of operations, particularly when there is potential for personal injury, economic loss or environmental damage. One approach to addressing these situations is improvisation. The research described here discusses a series of studies conducted to evaluate the efficacy of a computer-based system for supporting improvisation in simulated crisis situations. The design and implementation of the system are first discussed, drawing upon prior work in blackboard-based systems. The experimental design is then reviewed, followed by a discussion of how the studies were run using groups of emergency response personnel from the Port of Rotterdam in The Netherlands. The group task was to address unexpected contingencies in a timely fashion. A number of measures of group decision effectiveness and uniqueness are presented. Results of the studies suggest that availability of decision support may have had an uneven influence on solution effectiveness and no influence on solution uniqueness. Possible implications for the design of group decision support systems for improvisation are then discussed, along with a number of observations on conducting experimentally-based research on group improvisation.","Personnel,
Hazards,
Disaster management,
Technology management,
Real time systems,
Degradation,
Injuries,
Environmental economics,
Computational modeling,
Computer simulation"
Singular: a computer algebra system,"Singular is free software for polynomial computations. Originally designed for research in mathematics, it features one of the fastest implementations of Buchberger's Grobner basis algorithm, which is the core of many symbolic methods for simplifying and solving systems of polynomial equations.","Algebra,
Polynomials,
Equations,
Mathematics,
Numerical analysis,
Computational geometry,
Roundoff errors,
Surface treatment,
Licenses,
System testing"
Modeling and simulation sigma-delta analog to digital converters using VHDL-AMS,"The purpose of this paper is to introduce the approach of modeling and simulation of sigma-delta modulators using VHDL-AMS. In this paper is stressed that fast and efficient modeling and simulation methodology can be very helpful in the process of microsystem design. A few sigma-delta modulator structures are presented. From 1/sup st/ to 4/sup th/ order sigma-delta modulators and also a triple cascade structure are shown. The advantages and disadvantages of the presented structures are discussed. The optimal solution is submitted to use as part of a silicon microsystem. The proposed silicon microsystems are based on two major parts. The first part consists of chemical sensors whereas the second part consists of analog to digital converters, registers, buffers, etc. The analog to digital conversion is performed using the sigma-delta method. Modulators are modeled and tested with the application of a VHDL-AMS simulator. In this paper, the simulation results are presented, and discussion of the advantages of sigma-delta conversion in this kind of measurement is performed.",
AdaVegas: adaptive control for TCP Vegas,"We introduce AdaVegas, an adaptive congestion control mechanism based on TCP Vegas. TCP Vegas has several parameters which control the way it increases the sending rate. While TCP Vegas holds these parameters constant, AdaVegas sets these values dynamically. In this way, AdaVegas is able to change its increment strategy dynamically and better adapt to the current environment. Using simulations, we both evaluate AdaVegas and compare it to TCP Vegas. Our simulations show that AdaVegas achieves significantly better performance than TCP Vegas, all this with a fairly low overhead.",
Automatic feature weight assignment based on genetic algorithm for image retrieval,"Integrating multiple features content-based image retrieval can overcome the problems of single feature, but how to organize these features and feature representation methods is difficult in image retrieval. In this paper, an automatic feature weight assignment approach based on genetic algorithm is proposed. The problem of weight assignment is firstly changed into optimization problem, and genetic algorithm is used for finding the optimization weight in order to get the best retrieval results. The experimental results show that the recall and precision of this proposed approach is better than the others' weight assignment methods. This approach is robust to various kinds of features and feature representation methods, and it can get the best feature combination for image retrieval.","Genetic algorithms,
Image retrieval,
Content based retrieval,
Information retrieval,
Image databases,
Spatial databases,
Information science,
Genetic engineering,
Integrated optics,
Computer applications"
A system theoretic perspective of learning and optimization,"Learning and optimization of stochastic systems is a multi-disciplinary area that attracts wide attentions from researchers in control systems, operations research and computer science. Areas such as perturbation analysis (PA), Markov decision process (MDP), and reinforcement learning (RL) share the common goal. In this paper, we offer an overview of the area of learning and optimization from a system theoretic perspective. We show how these seemly different disciplines are closely related, how one topic leads to the others, and how this perspective may lead to new research topics and new results, and how the performance sensitivity formulas can serve as the basis for learning and optimization.","Stochastic systems,
Performance analysis,
Learning,
Queueing analysis,
State-space methods,
Control systems,
Operations research,
Markov processes,
System performance,
User-generated content"
Language-driven nonverbal communication in a bilingual conversational agent,"This paper describes an animated conversational agent called Kare which integrates a talking head interface with a linguistically motivated human-machine dialogue system. The agent has a range of nonverbal behaviors, which involve a mixture of machine vision, computer animation and natural language processing techniques. The system's architecture couples the agent's nonverbal communicative processes very tightly to its model of verbal interaction. We discuss several consequences of this architecture, in particular the ability to use different nonverbal dialogue management signals when speaking different languages.","Animation,
Tellurium,
Computer architecture,
Natural languages,
Natural language processing,
Computer science,
Magnetic heads,
Man machine systems,
Machine vision,
Computer interfaces"
A new hardware/software codesign environment and senior capstone design project for computer engineering,This paper describes a design environment and platform developed to support senior capstone design projects in computer engineering that incorporates the concept of hardware/software codesign. A proposed capstone design project which utilizes this environment is also presented. This project is being undertaken by senior computer engineering students for the first time this year at the authors' university.,"Hardware,
Design engineering,
Field programmable gate arrays,
Clocks,
Libraries,
Vehicles,
Motion control,
Pins,
Printers,
Computer interfaces"
Homeland security and privacy sensitive data mining from multi-party distributed resources,"Defending the safety of an open society from terrorism or other similar threats requires intelligent but careful ways to monitor different types of activities and transactions in the electronic media. Data mining techniques are playing an increasingly important role in sifting through large amount of data in search of useful patterns that might help us in securing our safety. Although the objective of this class of data mining applications is very well justified, they also open up the possibility of misusing personal information by malicious people with access to the sensitive data. This brings up the following question: Can we design data mining techniques that are sensitive to privacy? Several researchers are currently working on a class of data mining algorithms that work without directly accessing the sensitive data in their original form. This paper considers the problem of mining distributed data in a privacy-sensitive manner. It first points out the problems of some of the existing privacy-sensitive data mining techniques that make use of additive random noise to hide sensitive information. Next it briefly reviews some new approaches that make use of random projection matrices for computing statistical aggregates from sensitive data.","Terrorism,
Data privacy,
Data mining,
Computer science,
Safety,
Additive noise,
Aggregates,
Perturbation methods,
Computerized monitoring,
Social network services"
A multiple heuristic search algorithm for solving traveling salesman problem,"This research was carried out to solve the traveling salesman problem (TSP) using a multiple heuristic search algorithm. Two main operations of complete 2-opt (C2Opt) and smallest square (SS) were combined in a genetic algorithm (GA) and applied to the TSP. The C2Opt is based on the 2-Opt heuristic search, which can remove all crossed edges in the tour if the repeat times are sufficient. The SS selects the shorter edges than the C2Opt. The problem of the SS is that the city orders of the original tour were changed while the SS was applied, hence the crossed edges could not be removed completely. However, a reasonable result is presented by combining the C2Opt and the SS in the GA for the TSP in our experiment. Another two operations the deletion (DL) and the best part collector (BPC) are discussed. The DL was used for removing the duplicates from the population and the BPC was used for collecting the best part among the individuals to the elite individual.","Heuristic algorithms,
Traveling salesman problems,
Cities and towns,
Biological system modeling,
Computer science,
Operating systems,
Genetic algorithms,
Evolution (biology),
Genetic mutations,
Nearest neighbor searches"
Integrating ontological and linguistic knowledge for conceptual information extraction,"Text understanding makes strong assumptions about the conceptualisation of the underlying knowledge domain. This mediates between the accomplishment of the specific task at the one hand and the knowledge expressed in the target text fragments at the other. However, building domain conceptualisations from scratch is a very complex and time-consuming task. Traditionally, the reuse of available domain resources, although not constituting always the best, has been applied as an accurate and cost effective solution. Here, we investigate the possibility of exploiting sources of domain knowledge (e.g. a subject reference system) to build a linguistically motivated domain concept hierarchy. The limitation connected with the use of domain taxonomies as ontological resources will be firstly discussed in the specific light of IE, i.e. for supporting linguistic inference. We then define a method for integrating the taxonomical domain knowledge and a general-purpose lexical knowledge base, like WordNet. A case study, i.e. the integration of the MeSH, Medical Subject Headings, and WordNet, will be then presented as a proof of the effectiveness and accuracy of the overall approach.",
Efficient distributed shared state for heterogeneous machine architectures,"InterWeave is a distributed middleware system that supports the sharing of strongly typed, pointer-rich data structures across heterogeneous platforms. As a complement to RPC-based systems such as CORBA, .NET and Java RMI, InterWeave allows processes to access shared data using ordinary reads and writes. Experience indicates that InterWeave-style sharing facilitates the rapid development of distributed applications, and enhances performance through transparent caching of state. In this paper we focus on the aspects of InterWeave specifically designed to accommodate heterogeneous machine architectures. Beyond the traditional challenges of message-passing in heterogeneous systems, InterWeave (1) identifies and tracks data changes in the face of relaxed coherence models, (2) employs a wire format that captures not only data but also diffs in a machine and language-independent form, and (3) swizzles pointers to maintain long-lived (cross-call) address transparency. To support these operations, InterWeave maintains an extensive set Of metadata structures, and employs a variety of performance optimizations. Experimental results show that InterWeave achieves performance comparable to that of RPC parameter passing when transmitting previously uncached data. When updating data that have already been cached, InterWeave's use of platform-independent diffs allows it to significantly outperform the straightforward use of RPC.",
A parallel IP lookup algorithm for terabit router,"IP address lookup is a key bottleneck for high performance routers because they need to find the longest matching prefix. With traditional memory organization, core routers can hardly improve their performance much with the restriction of memory accessing speed. By analyzing the statistical attribution of the IP prefixes, this paper presents a novel parallel IP lookup algorithm based on a new memory organization, which can achieve a much higher throughput rate while keeping the memory consumption unchanged. With current 5 ns SRAM, the proposed mechanism furnishes approximately 600 million routing lookups per second.","Throughput,
Routing,
Performance analysis,
Random access memory,
Computer science,
Parallel processing,
Engines,
Mathematics,
Mathematical model,
Pipelines"
Stereoscopic video coding based on global displacement compensated prediction,"In this paper, we propose a novel algorithm for coding the stereoscopic video sequence based on the global displacement information between the left- and right-view images. MPEG-4 temporal scalable video coding scheme is employed at the baseline, in which the left- and right-view images are compressed at the base and enhancement layer, respectively. To further improve the coding efficiency, some intuitive properties of stereoscopic videos are utilized. One of the major properties lies in the global displacement between the left- and right-view images. In this paper, we propose coding the right-view image with global motion compensated prediction from the left-view image. A further main property of stereoscopic video arises from the fact that, objects simultaneously existing in the left- and right-view images usually result in the very close motion vectors. Based on the global motion information, the motion vectors of the right-view sequence can be easily predicted from that of the left-view sequence. Rate-distortion optimization is also used to select the coding mode. Experimental results show that, compared to the MPEG-4 temporal scalability algorithm for low bitrate stereoscopic video coding, the proposed algorithm can save the bitrate up to 20%.","Video coding,
Image coding,
MPEG 4 Standard,
Video sequences,
Video compression,
Scalability,
Bit rate,
Layout,
Computer science,
Digital images"
Self-stabilizing smoothing and counting,"A smoothing network is a distributed data structure that accepts tokens on input wires and routes them to output wires. It ensures that however imbalanced the traffic on input wires, the numbers of tokens emitted on output wires are approximately balanced. Prior work on smoothing networks always assumed that such networks were properly initialized. In a real distributed system, however, network switches may be rebooted or replaced dynamically, and it may not be practical to determine the correct initial state for the new switch. Prior analyses do not work under these new assumptions. This paper makes the following contributions. First, we show that some well-known 1-smoothing networks, known as counting networks, when started in an arbitrary initial state (perhaps chosen by an adversary), remain remarkably smooth, degrading from 1-smooth to log(n)-smooth, where n is the number of input/output wires. Second, we show that the same networks can be made eventually 1-smooth by ""piggy-backing"" a small amount of additional information on messages when (and only when) trouble is detected.",
Image orientation detection with integrated human perception cues (or which way is up),"In this paper, we propose a set of human perceptual cues used jointly to automatically detect image orientation. The cues used are: orientation of faces, position of the sky, brighter regions, and textured objects, and symmetry. We combine these cues in a Bayesian framework, and the photo acquiring model has been considered carefully as the prior knowledge of the image orientation. Results on more than a thousand different images provide a compelling argument that our approach is a viable one.","Humans,
Bayesian methods,
Clustering algorithms,
Digital cameras,
Object detection,
Face,
Computer science,
Detectors,
Content management,
Image retrieval"
Together we stand: group projects for integrating software engineering in the curriculum,"Software engineering is done by individuals within teams and in organisations, with all that those words imply. It is crucial to make this fact, and its implications, concrete to students who aspire to be or work with software engineers. Although frequent collaborations are encouraged throughout degree programmes, final-year group projects remain the favoured mechanism for achieving this goal. This paper describes and reflects on our experience of introducing group projects to balance theory, technology and practice into five different degree programmes. A novel facet of our approach has been to locate these projects in the context of a course on software project management in parallel with the preparation of capstone, individual projects. Hence, the final-year group projects are viewed as essential complements to the individual projects and together they encapsulate the theories and systematic practices of software projects we know as software engineering. We argue that this approach injects realism into what might have been seen by students as abstract primarily by providing students with experience of working as part of a team and so enabling them to engage with large and significant projects.","Software engineering,
Computer science education,
Computer science,
Project management,
Information systems,
Concrete,
Collaborative work,
Software systems,
Educational programs,
Visualization"
Design of a parser for real-time process algebra,"The real-time process algebra (RTFA) is a set of new mathematical notations for formally describing software system architectures, and static and dynamic behaviors. To bring RTPA into industrial software development practice, tools are needed for analyzing and visualizing RTPA specifications. The first step to develop the supporting tools is to build a grammar parser for recognizing the RTPA notation system. In this paper, a parser of RTPA is described. The parser takes a textual RTPA specification as input, and generates an abstract syntax tree (AST) as its output. The generated AST represents RTPA tokens and lexical information in a structured format, which provides a foundation for further semantic analysis, code generation, visualization, and validation.","Algebra,
Real time systems,
Computer architecture,
Java,
Software engineering,
Visualization,
Computer science,
Drives,
Software systems,
Computer industry"
A statistical rationalisation of Hartley's normalised eight-point algorithm,"The eight-point algorithm of Hartley occupies an important place in computer vision, notably as a means of providing an initial value of the fundamental matrix for use in iterative estimation methods. In this paper, a novel explanation is given for the improvement in performance of the eight-point algorithm that results from using normalised data. A first step is singling out a cost function that the normalised algorithm acts to minimise. The cost function is then shown to be statistically better founded than the cost function associated with the non-normalised algorithm. This augments the original argument that improved performance is due to the better conditioning of a pivotal matrix. Experimental results are given that support the adopted approach. This work continues a wider effort to place a variety of estimation techniques within a coherent framework.",
An intelligent algorithm for automatic document summarization,"Automatic document summarization is a highly interdisciplinary research area related with computer science as well as cognitive psychology. Here, we introduce an intelligent algorithm, the event indexing and summarization (EIS) algorithm, for automatic document summarization, which is based on taking into account a cognitive psychological model, the event-indexing model, and the roles and importance of sentences and their syntax in document understanding. The EIS algorithm involves syntactic analysis of sentences, clustering and indexing sentences with the five indices from the event-indexing model, and extracting the most prominent content by lexical analysis at phrase and clause levels. After thorough implementation and objective evaluations, the algorithm has now shown good performance in multidocument summarization.","Indexing,
Psychology,
Algorithm design and analysis,
Clustering algorithms,
Data mining,
Computer science,
Natural language processing,
Artificial intelligence,
Information retrieval,
Statistics"
Multicast routing by mobility prediction for mobile hosts,"In this paper, we propose an efficient multicast routing protocol based on mobile IP standard in wireless mobile networks. A mobile host that is located in a foreign network receives a tunneled multicast datagram from a multicast agent, which is located in a remote network or local network. While receiving a tunneled multicast datagram from a remote multicast agent, the local multicast agent starts a multicast tree join operation if the mobile host is expected to remain the network relatively long period of time, while it does not start multicast tree join operation if the mobile host is expected to remain the network relatively short period of time. The proposed protocol tries to maximize the number of unnecessary multicast tree join operations. We examined performance of the proposed protocol by simulation under various environments and we got good performance results.","Multicast protocols,
Routing protocols,
Telecommunication traffic,
Mobile computing,
Computer science,
Data engineering,
Bidirectional control,
Tunneling,
Finishing"
Intrusion detection based on hidden Markov model,"The intrusion detection technologies of the network security are researched, and the technologies of pattern recognition are used to intrusion detection. Intrusion detection rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. Hidden Markov Model (HMM) has been successfully used in speech recognition and some classification areas. Since Anomaly Intrusion Detection can be treated as a classification problem, some basic ideas have been proposed on using HMM to model normal behavior. The experiments have showed that the method based on HMM is effective to detect anomalistic behaviors.","Intrusion detection,
Hidden Markov models,
Computer security,
Handwriting recognition,
Educational institutions,
Computer science,
Data engineering,
Data security,
Pattern recognition,
Speech recognition"
"Which comes first, usability or utility?",,"Usability,
Human computer interaction,
Educational institutions,
Computer interfaces,
Data visualization,
Vehicles,
Motorcycles,
Switches,
Computer science,
Laboratories"
Introducing testing practices into objects and design course,"Though software testing courses are commonly taught as part of software engineering curricula, software testing is still a challenging issue in software engineering education. Students frequently see testing only as something that happens at the end of the development process. Two challenges can be recognized: ""how to make the students recognize the relevance of the testing activity?"" and ""how to motivate the students on using testing ideas in their projects?"". In an attempt to explore the impact of introducing testing practices throughout development, during the past Fall semester we modified the project requirements in a course on object-oriented analysis and design offered to the undergraduate students in computer science at Georgia Institute of Technology. Our idea was to require the students to start thinking about testing as early as possible, by including testing-related practices in all phases of the development process. This paper presents the details of the testing approach used in the course and discusses the results we obtained, in terms of the students' attitudes and learning.","Software testing,
Software engineering,
System testing,
Programming profession,
Educational institutions,
Educational technology,
Computer science,
Software quality,
Object oriented modeling,
Education"
Shot boundary detection based on the knowledge of information theory,"Shot boundary detection servers as the preliminary step to structure the contents of videos. Many research efforts have been devoted to this field. The distributions of this paper are as follows. First, in contrast with former methods, shot boundary detection is performed based on the knowledge of information theory; the threshold is determined automatically. We find that mutual information, joint entropy and conditional entropy all can detect the abrupt changes. Second, we also analyze the gradual transitions and motions. Fade in/out is easy to be detected using the joint entropy. Since the dissolve and motion have different characters in our method, it is easy to distinguish them from each other. The experiments based on the proposed method show better results and further researching is worthwhile.","Gunshot detection systems,
Information theory,
Videos,
Cameras,
Pixel,
Entropy,
Computer science,
Indexing,
Color,
Mutual information"
Combining clustering and partitioning in quadratic placement,"Because of the computation complexity of large circuits, the quadratic placement (Q-Place) cannot solve the placement problem fast enough without any preprocessing. In this paper, a method of combining the MFFC clustering and hMETIS partitioning based quadratic placement algorithm is proposed. Experimental results show it can gain good results but consume long running time. In order to cut down the running time, an improved MFFC clustering method (IMFFC) based Q-place algorithm is proposed in this paper. Comparing with the combining clustering and partitioning based method, it is much fast but with a little increase in total wire length.","Clustering algorithms,
Partitioning algorithms,
Integrated circuit interconnections,
Clustering methods,
Wire,
Quadratic programming,
Large-scale systems,
Computer science,
Optimization methods,
Logic circuits"
Knowledge management and quality certification in a research and development environment,"In this paper, a model that combines knowledge management and quality management approaches is presented. Essential concepts are provided about what must be considered in establishing a quality system for an institution, company or group that is dedicated to research and development (R&D) activities and that needs to fulfill the ISO9001: 2000 standard to obtain certification. The paper describes how certain principles of knowledge management and the requirements of the ISO9001 standard can be aligned with the objectives of an R&D organization and what aspects should be considered in fulfilling the standard. The model and the comments provided will help to an R&D organization to become ISO9001 certified with a minimum of additional efforts as compared with its operation without adhering to the standard.",
On construction of codes for DNA computers,We describe similarities between constraints of codes for DNA computers and constraints which appear in digital magnetic recording. We apply coding techniques developed for digital recording systems to problems on DNA sequences for DNA computers.,
Optimizing binary feature vector similarity measure using genetic algorithm and handwritten character recognition,,"Genetic algorithms,
Character recognition,
Hamming distance,
Computer science,
Text analysis,
Pattern recognition,
Optical character recognition software,
Data mining,
Information systems,
Pattern analysis"
A CC-based security engineering process evaluation model,"Common criteria (CC) provides only the standard for evaluating information security product or system, namely target of evaluation (TOE). On the other hand, SSE-CMM provides the standard for security engineering process evaluation. Based on the CC, TOE's security quality may be assured, but its advantage is that the development process is neglected. SSE-CMM seems to assure the quality of TOE developed in an organization equipped with security engineering process, but the TOE developed in such environment cannot avoid CC-based security assurance evaluation. We propose an effective method of integrating two evaluation methods, CC and SSE-CMM, and develop CC-based assurance evaluation model, CC/spl I.bar/SSE-CMM. CC/spl I.bar/SSE-CMM presents the specific and realistically operable organizational security process maturity assessment and CC evaluation model.","ISO standards,
IEC standards,
Information security,
Reliability engineering,
Personnel,
SPICE,
Computer science,
Information systems,
Computer crime,
Protection"
Model checking and evidence exploration,"We present an algebraic framework for evidence exploration: the process of interpreting, manipulating, and navigating the proof structure or evidence produced by a model checker when attempting to verify a system specification for a temporal-logic property. Due to the sheer size of such evidence, single-step traversal is prohibitive and smarter exploration methods are required. Evidence exploration allows users to explore evidence through smaller, manageable views, which are definable in relational graph algebra, a natural extension of relational algebra to graph structures such as model-checking evidence. We illustrate the utility of our approach by applying the Evidence Explorer, our tool implementation of the evidence-exploration framework, to the Java meta-locking algorithm, a highly optimized technique deployed by the Java Virtual Machine to ensure mutually exclusive access to object monitor queues by threads.","Usability,
Logic,
Formal verification,
Algebra,
Java,
Switches,
Computer science,
Electronic mail,
Navigation,
Virtual machining"
An ontology-extended relational algebra,Heterogeneous relational databases being integrated vary widely in how the same data is represented. We propose the notion of an ontology extended relation (OER). An OER contains an ordinary relation as well as an associated ontology that conveys semantic meaning about the terms being used. We extend the relational algebra to query OERs. The advantage of this is that the OER model can be directly built on top of commercial relational DBMSs. We describe an implementation of the OER model and show (via experiment) that the system scales to handle large data sets.,"Ontologies,
Algebra,
Relational databases,
Educational institutions,
Computer science,
Insurance,
USA Councils,
Costs,
Industrial relations,
Prototypes"
Designing a language for creating conceptual browsing interfaces for digital libraries,,"Software libraries,
Protocols,
Graphical user interfaces,
Middleware,
Design methodology,
Water heating,
Computer science,
Iterative methods,
Prototypes,
Mathematics"
Empowering software engineers in human-centered design,"Usability is about to become the quality measure of today's interactive software including Web sites, and mobile appliances. User-centered design approach emerges from this need for developing more usable products. However, interactive systems are still designed and tested by software and computer engineers unfamiliar with UCD and the related usability engineering techniques. While most software developers may have been exposed with basic concepts such as GUI design guidelines, few developers are able to understand the human/user-centered design (UCD) toolbox at a level that allows them to incorporate it into the software development lifecycle. This paper describes an approach for skilling developers and students enrolled in an engineering program in critical user-centered design techniques and tools. The proposed approach starts from the analysis of the usability and software engineer's work context, identifies critical UCD skills and then associates relevant learning resources with each of the identified skills. Our approach suggests a list of patterns for facilitating the integration the UCD skills into the software engineering lifecycle.","Design engineering,
User centered design,
Usability,
Software measurement,
Software quality,
Home appliances,
Interactive systems,
Software testing,
System testing,
Software tools"
A new paradigm for group cryptosystems using quick keys,"In this paper we introduce a new approach to group key agreement. Our approach is based on the idea of an associative one way function (AOWF). We illustrate how such functions can be used to perform highly dynamic and fully contributory multiparty key agreement in group-oriented cryptosystems. We also show how such schemes could be used to create efficient group digital signature schemes. Since at present, we have no working examples of AOWFs, the protocols proposed here only have theoretical value. A similar scheme was also discussed in [Rabi Muhammad and Sherman Alan, Nov. 15 1993] and our work is an extension to it.","Cryptography,
Protocols,
Internet,
Centralized control,
Computer science,
Digital signatures,
Distributed computing,
Collaborative work,
Robustness,
Security"
"Addendum to ""Strong converse for identification via quantum channels""","The paper discusses Conjecture 21 formulated by Ahlswede and Winter (see ibid., vol.48, p.569-79, Mar. 2002) about finite families of self-adjoint operators A/sub i/ and B/sub i/ noting that for commuting operators equality holds. It adds that the other statements made in connection with this conjecture, though not logically disproved, should be regarded with caution.","Computer science,
Quantum mechanics,
Information theory"
Fuzzy logic applications to environment management systems: case studies,"Water quality management is an important issue of relevance in the context of present times. Water quality indices are computed for classification of water wherein the integration of parametric information on water quality data and the expert's knowledge base on their importance & weights are considered. Considerable uncertainties are involved in the process of defining water quality for specific usage. The paper presents modeling of cognitive uncertainty in perception of experts or consumers and statistical uncertainty in the field data while dealing with these systems with recourse to fuzzy logic. Case study 1 presents fuzzy description of water quality in river Ganga for bathing purpose following partial implementation of pollution control strategies while case study 2 arrives at per capita water consumption of the consumers of the study area in Coimbatore, India for their level of satisfaction.","Fuzzy logic,
Environmental management,
Computer aided software engineering,
Rivers,
Uncertainty,
Water pollution,
Fuzzy systems,
Fuzzy set theory,
Knowledge based systems,
Oxygen"
Identification of deliberately doctored text documents using frequent keyword chain (FKC) model,"Text documents have always been the most dominant source of data available. A number of classification techniques are used to organize these documents and a majority of these classification algorithms use keywords to categorize them. It is possible to mislead such algorithms by inserting keywords ('deliberate doctoring') belonging to a class different from that of the document. Such intentional deception is done in order to rank Web pages higher in searches. As text classification is used to classify e-mails, deliberate doctoring is also done as a spam filter-busting measure. In addition, it may be practiced to avoid detection by security agencies. The cost of such misclassification can be high and it is a serious problem in many scenarios. In this paper we have exhaustively examined the possible methods to doctor a document which may lead to its misclassification. In the study we have concluded that a majority of the ways would involve insertion of a number of misleading keywords in close proximity. We propose the frequent keyword chain model to identify such local concentration of keywords. A tool called the FKCLocater is designed around the model which identifies and highlights FKC's in a document and alerts the user to the possibility of misclassification. The tool is also used to specify various parameters to fine tune the frequency keyword chain model. Experiments on newsgroup data sets show that this model is effective.","Search engines,
Classification algorithms,
Web pages,
Internet,
Classification tree analysis,
Computer science,
Text categorization,
Electronic mail,
Security,
Costs"
Frame difference normalization: an approach to reduce error rates of cut detection algorithms for MPEG videos,"The segmentation of video sequences into shots is the first step towards video content analysis. Two kinds of shot boundaries can be distinguished: abrupt scene changes (""cuts"") and gradual transitions. In this paper, we present a technique to reduce the error rates of cut detection algorithms based on pixel-wise or histogram-based frame difference metrics when operating directly on compressed MPEG video data. The proposed approach, called ""frame difference normalization"" (FDN), intends to eliminate the effects of a specific frame pattern in MPEG streams responsible for causing such errors. Experimental results will be presented to demonstrate the benefits of our proposal and its superiority over a more general noise filter. Furthermore, the proposed method is not limited to a particular algorithm but it is applicable to an entire class of cut detection algorithms.",
Computational methods in ultrafast time-domain optics,"Three computational methods exist for time-domain optics: time domain, frequency domain, and hybrid time-frequency domain. Because of temporal dispersion in optical materials, problems unique to ultrafast optics require well-defined and highly accurate approximation methods.","Optical computing,
Ultrafast optics,
Time domain analysis,
Optical materials,
Tensile stress,
Optical pulses,
Equations,
Current density,
Magnetic materials,
Electromagnetic radiation"
Reducing energy and delay using efficient victim caches,"In this paper, we investigate methods for improving the hit rates in the first level of memory hierarchy. Particularly, we propose victim cache structures to reduce the number of accesses to more power consuming structures such as level 2 caches. We compare the proposed victim cache techniques to increasing the associativity or the size of the level I data cache and show that the enhanced victim cache technique yield better energy-delay and energy-delay-area products. We also propose techniques that predict the hit/miss behavior of the victim cache accesses and bypass the victim cache when a miss can be determined quickly. We report simulation results obtained from SimpleScalar/ARM modeling a representative Network Processor architecture. The simulations show that the victim cache is able to reduce the energy consumption by as much as 17.6% (8.6% on average) while reducing the execution time by as much as 8.4% (3.7% on average) for a set of representative applications.","Delay,
Energy consumption,
Hardware,
Permission,
System buses,
Computer science,
Computer architecture,
Computational modeling,
Application software,
Embedded computing"
Real-time adaptive filtering for projection reconstruction MR fluoroscopy,"Magnetic resonance (MR) imaging has faced a dramatic increase in real-time capabilities over the last years. However, the application of fast pulse sequences still suffers from low signal-to-noise ratios (SNRs), which can be the limiting factor for the actual acquisition speed. In MR fluoroscopy, filtering along the time and/or spatial domain can be applied to increase the image quality. In this paper, a projection-based noise filter is presented that significantly enhances the SNR in projection reconstruction (PR) fluoroscopy without apparent loss of resolution in the reconstructed images. In contrast to an imaged-based approach, this method allows a very efficient computational implementation. The filter algorithm was implemented on a digital signal processor and was applied to real-time processing during PR fluoroscopy. A quantitative analysis of the improvement in SNR and results for different fluoroscopic MR applications are given. Apart from MR fluoroscopy, the proposed technique has the potential to be applied to low dose computed tomography fluoroscopy.","Adaptive filters,
Image reconstruction,
Signal to noise ratio,
Magnetic resonance,
Magnetic resonance imaging,
Limiting,
Magnetic separation,
Filtering,
Image quality,
Spatial resolution"
Split-layer video multicast protocol: a new receiver-based rate-adaptation protocol,"This paper proposes a new receiver-based rate-adaptation protocol for multicasting video, called Split-Layer Video Multicast (SPLIT). Unlike existing receiver-based rate-adaptation protocols, such as Receiver-driven Layered Multicast (RLM), the SPLIT protocol is specifically designed to take advantage of existing packet loss concealment techniques to provide end-users with increased quality of video. In an effort to gauge the performance of the SPLIT protocol, a number of experiments using the NS-2 network simulator were conducted and the results were compared with those of the RLM protocol. The results show that the proposed SPLIT protocol utilizes the available bandwidth in a more efficient way than the RLM protocol does.",
A load balancing mechanism with verification,,"Load management,
Distributed computing,
Delay,
Protocols,
Cost accounting,
Computer science,
Computational modeling,
Algorithm design and analysis,
Resource management,
System performance"
