Title,Abstract,Keywords
Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes,"Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","Shape,
Machine learning,
Solid modeling,
Feature extraction,
Convolution,
Unsupervised learning"
GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data,"The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","Approximation algorithms,
Big Data,
Acceleration,
Training,
Libraries,
Machine learning algorithms,
Clustering algorithms"
A Machine Learning Algorithm for Identifying Atopic Dermatitis in Adults from Electronic Health Records,"The current work aims to identify patients with atopic dermatitis for inclusion in genome-wide association studies (GWAS). Here we describe a machine learning-based phenotype algorithm. Using the electronic health record (EHR), we combined coded information with information extracted from encounter notes as features in a lasso logistic regression. Our algorithm achieves high positive predictive value (PPV) and sensitivity, improving on previous algorithms with low sensitivity. These results demonstrate the utility of natural language processing(NLP) and machine learning for EHR-based phenotyping.","Machine learning algorithms,
Prediction algorithms,
Diseases,
Skin,
Feature extraction,
Gold,
Standards"
Detecting Cognitive Distortions Through Machine Learning Text Analytics,"Machine learning and text analytics have proven increasingly useful in a number of health-related applications, particularly in the context of analyzing online data for disease epidemics and warning signs of a variety of mental health issues. We follow in this tradition here, but focus our attention on cognitive distortion, a precursor and symptom of disruptive psychological disorders such as anxiety, anorexia and depression. We collected a number of personal blogs from the Tumblr API, and labeled them based on whether they exhibited distorted thought patterns. We then used LIWC to extract textual features and applied machine learning to the resulting vectors. Our findings show that it is possible to detect cognitive distortions automatically from personal blogs with relatively good accuracy (73.0%) and false negative rate (30.4%).","Distortion,
Logistics,
Psychology,
Blogs,
Social network services,
Pragmatics,
Decision trees"
Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database,"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.","Electromagnetic compatibility,
Databases,
Prediction algorithms,
Support vector machines,
Training,
Diseases,
Time measurement"
A machine learning approach to characterizing the effect of asynchronous distributed electrical stimulation on hippocampal neural dynamics in vivo,"Asynchronous distributed microelectrode theta stimulation (ADMETS) of the hippocampus has been shown to reduce seizure frequency in the tetanus toxin rat model of mesial temporal lobe epilepsy suggesting a hypothesis that ADMETS induces a seizure resistant state. Here we present a machine learning approach to characterize the nature of neural state changes induced by distributed stimulation. We applied the stimulation to two animals under sham and ADMETS conditions and used a combination of machine learning techniques on intra-hippocampal recordings of Local Field Potentials (LFPs) to characterize the difference in the neural state between sham and ADMETS. By iteratively fitting a logistic regression with data from the inter-stimulation interval under sham and ADMETS condition we found that the classification performance improves for both animals until 90s post stimulation before leveling out at AUC of 0.64 ± 0.2 and 0.67 ± 0.02 when all inter-stimulation data is included. The models for each animal were re-fit using elastic net regularization to force many of the model coefficients to 0, identifying those that do not optimally contribute to the classifier performance. We found that there is significant variation in the non-zero coefficients between animals (p < 0.01), suggesting that the ADMETS induced state is represented differently between subject. These findings lay the foundation for using machine learning to robustly and quantitatively characterize neural state.","Animals,
Electrodes,
Data models,
Hippocampus,
Logistics,
Biological system modeling,
Biomarkers"
Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems,"With the exponential growth of data and complexity of systems, fast machine learning/artificial intelligence and computational intelligence techniques are highly required. Many conventional computational intelligence techniques face bottlenecks in learning (e.g., intensive human intervention and convergence time) [item 1) in the Appendix]. However, efficient learning algorithms alternatively offer significant benefits including fast learning speed, ease of implementation, and minimal human intervention. The need for efficient and fast implementation of machine learning techniques in big data and dynamic varying systems poses many research challenges. This special issue highlights some latest development in the related areas.","Big Data,
Feature extraction,
Support vector machines,
Cybernetics,
Acceleration,
Kernel,
Delays"
Semisupervised Incremental Support Vector Machine Learning Based on Neighborhood Kernel Estimation,"Semisupervised scheme has emerged as a popular strategy in the machine learning community due to the expensiveness of getting enough labeled data. In this paper, a semisupervised incremental support vector machine (SE-INC-SVM) algorithm based on neighborhood kernel estimation is proposed. First, kernel regression is constructed to estimate the unlabeled data from the labeled neighbors and its estimation accuracy is discussed from the analogy with tradition RBF neural network. The incremental scheme is derived to improve the learning efficiency and reduce the computing time. Simulations for manual data set and industrial benchmark-penicillin fermentation process demonstrate the effectiveness of the proposed SE-INC-SVM method.","Support vector machines,
Kernel,
Estimation,
Data models,
Interpolation,
Semisupervised learning,
Training"
Classifying osteosarcoma patients using machine learning approaches,"Metabolomic data analysis presents a unique opportunity to advance our understanding of osteosarcoma, a common bone malignancy for which genomic and proteomic studies have enjoyed limited success. One of the major goals of metabolomic studies is to classify osteosarcoma in early stages, which is required for metastasectomy treatment. In this paper we subject our metabolomic data on osteosarcoma patients collected by the SJTU team to three classification methods: logistic regression, support vector machine (SVM) and random forest (RF). The performances are evaluated and compared using receiver operating characteristic curves. All three classifiers are successful in distinguishing between healthy control and tumor cases, with random forest outperforming the other two for cross-validation in training set (accuracy rate for logistic regression, support vector machine and random forest are 88%, 90% and 97% respectively). Random forest achieved overall accuracy rate of 95% with 0.99 AUC on testing set.","Logistics,
Radio frequency,
Support vector machines,
Benign tumors,
Metabolomics,
Testing"
Supervised Machine Learning to Predict Follow-Up Among Adjuvant Endocrine Therapy Patients,"Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.","Medical diagnostic imaging,
Tumors,
Electronic medical records,
Neural networks,
Drugs,
Breast cancer"
Evaluating machine learning algorithms for applications with humans in the loop,"Applications employing data classification such as smart lighting that involve human factors such as perception lead to non-deterministic input-output relationships where more than one output may be acceptable for a given input. For these so called non-deterministic multiple output classification (nDMOC) problems, the relationship between the input and output may change over time making it difficult for the machine learning (ML) algorithms in a batch setting to make predictions for a given context. In this paper, we describe the nature of nDMOC problems and discuss the Relevance Score (RS) that is suitable in this context as a performance metric. RS determines the extent by which a predicted output is relevant to the user's context and behaviors, taking into account the inconsistencies that come with human (perception) factors. We tailor the RS metric so that it can be used to evaluate ML algorithms in an online setting at run-time. We assess the performance of a number of ML algorithms, using a smart lighting dataset with non-deterministic one-to-many input-output relationships. The results indicate that using RS instead of classification accuracy (CA) is suitable to analyze the performance of conventional ML algorithms applied to the category of nDMOC problems. Instance-based online ML gives the best RS performance. An interesting finding is that the RS keeps increasing with increasing number of samples, even after the CA performance converges.","Measurement,
Prediction algorithms,
Lighting,
Machine learning algorithms,
Electronic mail,
Monitoring,
Mood"
Machine learning for variability aware statistical device design: The case of perpendicular spin-transfer-torque random access memory,"Spin-transfer-torque random access memory (STT RAM) is a promising memory technology due to its scalability, endurance and non-volatility. Addressing the process induced variations during realistic device fabrication process is a challenge, while trying to meet performance specifications, more so with the technology scaling leading to smaller device dimensions. In a simplified picture, the performance parameters of an STT RAM cell such as switching current density for a given pulse length or the switching delay for a given applied current density, depend on a variety of material parameters such as magnetic anisotropy and damping constant of the “free layer” (FL), the information storage layer. Besides material parameters, device dimensions, such as the diameter and the thickness of FL also vary about the target values, due to imperfections during thin-film deposition, lithography and ion-beam etching, among other process steps. To consider process variations in such parameters, Monte-Carlo simulations can be used, where each of the parameters can be, e.g., a random number from a Gaussian distribution about its target value. However, a modest number of 5 parameters and 100 values for each of them would require (102)5 = 1010 device simulations, and would be computationally infeasible for e.g., micromagnetic simulation. A possible route to circumvent such a prohibitively large computational load would be to use a compact model [1, 2]. However, a method like this relies on the so-called macrospin approximation and assumes spins across FL to be parallel to each other at all times during switching and might not be a true representation. Also, for many emerging devices, a compact model still is not available. Machine learning (ML) has been used in the past to model array of resistive random access memory (RRAM) [3]. In this work, we propose an ML driven simulation methodology to take the effect of process variation into account using micromagnetic simulations with reasonable computational effort. We employ support vector regression (SVR), a method used in supervised learning to anticipate the behavior of a system based on previously obtained “training data”, to predict performance of an STT RAM cell. We use STT RAM as a model system, although the proposed scheme should be usable for other devices too.","Random access memory,
Computational modeling,
Switches,
Current density,
Training,
Gaussian distribution,
Probability distribution"
A 3D multi-layer CMOS-RRAM accelerator for multi-layer machine learning,"Fast machine learning is required for future real-time data analytics. This paper introduces a 3D multi-layer CMOS-RRAM accelerator for learning on neural network. Given input of buffered data hold on the layer of a RRAM memory, intensive matrix-vector multiplication can be firstly accelerated on the layer of a digitized RRAM-crossbar. The remaining algorithmic operations such as feature extraction and classifier training can be accelerated on the layer of CMOS ASIC with consideration of parallelism and pipeline. Experiment results have shown that such a 3D accelerator can significantly reduce training time with acceptable accuracy. Compared to 3D-CMOS-ASIC implementation, it can achieve 1.28× smaller area, 2.05× faster runtime and 12.4× energy reduction.","Three-dimensional displays,
Dictionaries,
Flexible printed circuits,
Feedforward neural networks,
Nickel,
Marine vehicles,
Airplanes"
A flexible low-power machine learning accelerator for healthcare applications,"A flexible machine learning accelerator with low power consumption is presented in this paper. Linear classifier, naïve Bayes (NB) classifier and support vector machine (SVM) classifier with three kernel functions (linear, polynomial and radial basis function (RBF)) are realized in this design using the same architecture. A modified coordinate rotation digital computer (CORDIC) unit with expanded convergence range is employed to calculate exponential functions. The supply voltage is decreased to help reduce the power consumption. Re-characterized libraries for TSMC 65 nm LP technology under near-threshold supply voltages are generated for the implementation of the proposed accelerator. At the supply voltage of 0.6 V and the clock frequency of 10 MHz, the accelerator consumes 0.26 μJ for each classification using SVM with RBF function. Using linear classifier, the power consumption is further reduced to 0.41 nJ per classification.","Support vector machines,
Biosensors,
Brain modeling,
Niobium,
Electroencephalography"
Machine Learning Framework for the Detection of Mental Stress at Multiple Levels,"Mental stress has become a social issue and could become a cause of functional disability during routine work. In addition, chronic stress could implicate several psychophysiological disorders. For example, stress increases the likelihood of depression, stroke, heart attack, and cardiac arrest. The latest neuroscience reveals that the human brain is the primary target of mental stress, because the perception of the human brain determines a situation that is threatening and stressful. In this context, an objective measure for identifying the levels of stress while considering the human brain could considerably improve the associated harmful effects. Therefore, in this paper, a machine learning (ML) framework involving electroencephalogram (EEG) signal analysis of stressed participants is proposed. In the experimental setting, stress was induced by adopting a well-known experimental paradigm based on the montreal imaging stress task. The induction of stress was validated by the task performance and subjective feedback. The proposed ML framework involved EEG feature extraction, feature selection (receiver operating characteristic curve, t-test and the Bhattacharya distance), classification (logistic regression, support vector machine and naïve Bayes classifiers) and tenfold cross validation. The results showed that the proposed framework produced 94.6% accuracy for two-level identification of stress and 83.4% accuracy for multiple level identification. In conclusion, the proposed EEG-based ML framework has the potential to quantify stress objectively into multiple levels. The proposed method could help in developing a computer-aided diagnostic tool for stress detection.","Stress,
Electroencephalography,
Feature extraction,
Support vector machines,
Cardiac arrest,
Object recognition,
Psychology"
